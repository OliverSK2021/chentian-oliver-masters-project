,Unnamed: 0,id,code,label
32,32,dbc822e1f1b968ae52075b1eceb1e43293903996,"api/app/admin/examtype.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from app.models.bookings import ExamType
from .base import Base
from flask_login import current_user
from qsystem import db


class ExamTypeConfig(Base):
    roles_allowed = ['SUPPORT']

    def is_accessible(self):
        return current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    def get_query(self):
        return self.session.query(self.model)

    create_modal = False
    edit_modal = False

    column_list = [
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'number_of_minutes',
        'method_type',
        'ita_ind',
        'group_exam_ind',
        'pesticide_exam_ind'
    ]

    column_labels = {
        'exam_type_name': 'Exam Type Name',
        'exam_color': 'Exam Color',
        'number_of_hours': 'Number of Hours',
        'number_of_minutes': 'Number of Minutes',
        'method_type': 'Method Type',
        'ita_ind': 'ITA Exam Flag',
        'group_exam_ind': 'Group Exam Flag',
        'pesticide_exam_ind': 'Pesticide Exam Flag',
    }

    column_searchable_list = {'exam_type_name'}

    form_excluded_columns = [
        'exam'
    ]

    form_create_rules = (
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'number_of_minutes',
        'method_type',
        'ita_ind',
        'group_exam_ind',
        'pesticide_exam_ind'
    )

    form_edit_rules = {
        'exam_type_name': 'Exam Type Name',
        'exam_color': 'Exam Color',
        'number_of_hours': 'Number of Hours',
        'number_of_minutes': 'Number of Minutes',
        'method_type': 'Method Type',
        'ita_ind': 'ITA Exam Flag',
        'group_exam_ind': 'Group Exam Flag',
        'pesticide_exam_ind': 'Pesticide Exam Flag',
    }

    column_sortable_list = [
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'method_type',
        'ita_ind',
        'group_exam_ind',
        'pesticide_exam_ind'
    ]

    column_default_sort = 'exam_type_name'


ExamTypeModelView = ExamTypeConfig(ExamType, db.session)

/n/n/napi/app/admin/invigilator.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from app.models.bookings import Invigilator
from .base import Base
from flask_login import current_user
from qsystem import db


class InvigilatorConfig(Base):
    roles_allowed = ['SUPPORT', 'GA']
    delete_allowed = ['SUPPORT']

    def is_accessible(self):
        return current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    def get_query(self):
        if current_user.role.role_code == 'SUPPORT':
            return self.session.query(self.model)
        elif current_user.role.role_code == 'GA':
            return self.session.query(self.model).filter_by(office_id=current_user.office_id)

    # Check to see whether or not the user can delete records based on their role
    def _handle_view(self, name, **kwargs):
        if current_user.role.role_code in self.delete_allowed:
            self.can_delete = True
        else:
            self.can_delete = False

    create_modal = False
    edit_modal = False

    column_list = [
        'office.office_name',
        'invigilator_name',
        'contact_phone',
        'contact_email',
        'contract_number',
        'contract_expiry_date',
        'invigilator_notes'
    ]

    form_excluded_columns = [
        'bookings'
    ]

    column_labels = {'office.office_name': 'Office Name'}

    column_searchable_list = {'invigilator_name'}

    form_create_rules = (
        'office',
        'invigilator_name',
        'contact_phone',
        'contact_email',
        'contract_number',
        'contract_expiry_date',
        'invigilator_notes'
    )

    form_edit_rules = (
        'office',
        'invigilator_name',
        'contact_phone',
        'contact_email',
        'contract_number',
        'contract_expiry_date',
        'invigilator_notes'
    )

    column_sortable_list = [
        'invigilator_name',
        'contact_email',
        'contract_number',
        'contract_expiry_date'
    ]

    column_default_sort = 'invigilator_name'


InvigilatorModelView = InvigilatorConfig(Invigilator, db.session)
/n/n/napi/app/admin/office.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''


from app.models.theq import Office
from .base import Base
from flask_login import current_user
from qsystem import db


class OfficeConfig(Base):
    roles_allowed = ['SUPPORT']

    def is_accessible(self):
        return current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    create_modal = False
    edit_modal = False
    can_delete = False
    form_create_rules = ('office_name', 'office_number', 'sb', 'services', 'deleted', 'exams_enabled_ind',
                         'appointments_enabled_ind', 'timezone')
    form_edit_rules = ('office_name', 'office_number', 'sb', 'services', 'deleted', 'exams_enabled_ind',
                       'appointments_enabled_ind', 'timezone')
    column_labels = {'sb': 'Smartboard', 'timezone.timezone_name': 'Timezone Name'}
    column_sortable_list = ['office_name', 'sb', 'deleted', 'exams_enabled_ind']
    column_list = ['office_name',
                   'sb',
                   'services',
                   'deleted',
                   'exams_enabled_ind',
                   'appointments_enabled_ind',
                   'timezone.timezone_name',
                   ]

    form_excluded_columns = ('citizens',
                             'csrs',
                             'exams',
                             'rooms',
                             'invigilators'
                             )

    form_create_rules = ('office_name',
                         'office_number',
                         'sb',
                         'services',
                         'deleted',
                         'exams_enabled_ind',
                         'appointments_enabled_ind',
                         'timezone'
                         )

    form_edit_rules = ('office_name',
                       'office_number',
                       'sb',
                       'services',
                       'deleted',
                       'exams_enabled_ind',
                       'appointments_enabled_ind',
                       'timezone'
                       )

    column_labels = {'sb': 'Smartboard',
                     'timezone.timezone_name': 'Timezone Name',
                     'exams_enabled_ind': 'Exams Enabled',
                     'appointments_enabled_ind': 'Appointments Enabled',
                     }

    column_sortable_list = ['office_name',
                            'sb',
                            'deleted',
                            'exams_enabled_ind',
                            'exams_enabled_ind',
                            'appointments_enabled_ind',
                            ]

    column_default_sort = 'office_name'


OfficeModelView = OfficeConfig(Office, db.session)
/n/n/napi/app/admin/room.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from app.models.bookings import Room
from .base import Base
from flask_login import current_user
from qsystem import db


class RoomConfig(Base):
    roles_allowed = ['SUPPORT', 'GA']

    def is_accessible(self):
        return current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    def get_query(self):
        if current_user.role.role_code == 'SUPPORT':
            return self.session.query(self.model)
        elif current_user.role.role_code == 'GA':
            return self.session.query(self.model).filter_by(office_id=current_user.office_id)

    create_modal = False
    edit_modal = False
    can_delete = False

    column_list = [
        'office.office_name',
        'room_name',
        'capacity',
        'color'
    ]

    form_excluded_columns = [
        'sb',
        'booking'
    ]

    column_labels = {'office.office_name': 'Office Name'}

    form_create_rules = (
        'office',
        'room_name',
        'capacity',
        'color'
    )

    form_edit_rules = (
        'office',
        'room_name',
        'capacity',
        'color'
    )

    column_sortable_list = [
        'room_name',
        'capacity',
        'color'
    ]


RoomModelView = RoomConfig(Room, db.session)/n/n/napi/app/models/bookings/exam_type.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from app.models.bookings import Base
from qsystem import db


class ExamType(Base):

    exam_type_id = db.Column(db.Integer, primary_key=True, autoincrement=True, nullable=False)
    exam_type_name = db.Column(db.String(50), nullable=False)
    exam_color = db.Column(db.String(10), nullable=False)
    number_of_hours = db.Column(db.Integer, nullable=False)
    number_of_minutes = db.Column(db.Integer, nullable=True, default=0)
    method_type = db.Column(db.String(10), nullable=False)
    ita_ind = db.Column(db.Integer, nullable=False)
    group_exam_ind = db.Column(db.Integer,  nullable=False)
    pesticide_exam_ind = db.Column(db.Integer, nullable=False)

    exam = db.relationship(""Exam"", lazy=False)

    def __repr__(self):
        return '<Exam Type Name: (name={self.exam_type_name!r})>'.format(self=self)

    def __init__(self, **kwargs):
        super(ExamType, self).__init__(**kwargs)
/n/n/napi/app/resources/bookings/exam/exam_list.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

import logging
from flask import g, request
from flask_restplus import Resource
from sqlalchemy import exc, or_, desc
from app.models.bookings import Exam
from app.models.theq import CSR
from app.schemas.bookings import ExamSchema
from qsystem import api, jwt
from datetime import datetime, timedelta


@api.route(""/exams/"", methods=[""GET""])
class ExamList(Resource):

    exam_schema = ExamSchema(many=True)

    @jwt.requires_auth
    def get(self):
        try:
            csr = CSR.find_by_username(g.jwt_oidc_token_info['preferred_username'])

            ninety_day_filter = datetime.now() - timedelta(days=90)

            if csr.liaison_designate == 1:
                exams = Exam.query.filter(Exam.deleted_date.is_(None))\
                                  .filter(or_(Exam.exam_returned_date.is_(None),
                                              Exam.exam_returned_date > ninety_day_filter))\
                                  .order_by(desc(Exam.exam_id))

            else:
                exams = Exam.query.filter(Exam.deleted_date.is_(None))\
                                  .filter_by(office_id=csr.office_id)\
                                  .filter(or_(Exam.exam_returned_date.is_(None),
                                              Exam.exam_returned_date > ninety_day_filter))\
                                  .order_by(desc(Exam.exam_id))

            search_kwargs = {}

            if request.args:
                for key in request.args:
                    if hasattr(Exam, key):
                        search_kwargs[key] = request.args.get(key)

                exams = exams.filter_by(**search_kwargs)

            result = self.exam_schema.dump(exams)

            return {'exams': result.data,
                    'errors': result.errors}, 200

        except exc.SQLAlchemyError as error:
            logging.error(error, exc_info=True)
            return {""message"": ""api is down""}, 500
/n/n/napi/app/schemas/bookings/exam_type_schema.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from marshmallow import fields
import toastedmarshmallow
from app.models.bookings import ExamType
from qsystem import ma


class ExamTypeSchema(ma.ModelSchema):

    class Meta:
        model = ExamType
        exclude = (""exam"",)
        jit = toastedmarshmallow.Jit

    exam_type_id = fields.Int(dump_only=True)
    exam_type_name = fields.Str()
    exam_color = fields.Str()
    number_of_hours = fields.Int()
    number_of_minutes = fields.Int()
    method_type = fields.Str()
    ita_ind = fields.Int()
    group_exam_ind = fields.Int()
    pesticide_exam_ind = fields.Int()


/n/n/napi/manage.py/n/n""""""Manage the database and some other items required to run the API""""""
from flask_script import Command, Manager, Option # class for handling a set of commands
from flask_migrate import Migrate, MigrateCommand, upgrade
from qsystem import db, application
from app.models import theq
from app.models import bookings
import logging
from datetime import datetime
import pytz

migrate = Migrate(application, db)
manager = Manager(application)

class Bootstrap(Command):

    def run(self):
        print(""Clearing out all models"")
        theq.Period.query.delete()
        theq.PeriodState.query.delete()
        theq.ServiceReq.query.delete()
        theq.SRState.query.delete()
        theq.Citizen.query.delete()
        theq.CitizenState.query.delete()
        theq.CSR.query.delete()
        theq.CSRState.query.delete()
        # theq.OfficeService.query.delete()   #  This needs to be updated.
        bookings.Exam.query.delete()
        bookings.ExamType.query.delete()
        bookings.Room.query.delete()
        bookings.Invigilator.query.delete()
        theq.Office.query.delete()
        theq.SmartBoard.query.delete()
        # theq.RolePermission.query.delete()  #  No data in this table yet.
        theq.Role.query.delete()
        # theq.Permission.query.delete()      #  No data in this table yet.
        theq.Service.query.filter_by(actual_service_ind=1).delete()
        theq.Service.query.delete()
        theq.Channel.query.delete()
        bookings.Booking.query.delete()
        theq.Timezone.query.delete()

        db.session.commit()

        print(""Starting to bootstrap data"")
        #-- Channels --------------------------------------------------------
        print(""--> Channels"")
        channel1 = theq.Channel(
            channel_name=""In Person""
        )
        channel2 = theq.Channel(
            channel_name=""Phone""
        )
        channel3 = theq.Channel(
            channel_name=""Back Office""
        )
        channel4 = theq.Channel(
            channel_name=""Email/Fax/Mail""
        )
        channel5 = theq.Channel(
            channel_name=""CATs Assist""
        )
        channel6 = theq.Channel(
            channel_name=""Mobile Assist""
        )
        db.session.add(channel1)
        db.session.add(channel2)
        db.session.add(channel3)
        db.session.add(channel4)
        db.session.add(channel5)
        db.session.add(channel6)
        db.session.commit()

        #-- Roles -----------------------------------------------------------
        print(""--> Roles"")
        role_csr = theq.Role(
            role_code=""CSR"",
            role_desc=""Customer Service Representative""
        )
        role_ga = theq.Role(
            role_code=""GA"",
            role_desc=""Government Agent""
        )
        role3 = theq.Role(
            role_code=""HELPDESK"",
            role_desc=""Help Desk Functions""
        )
        role4 = theq.Role(
            role_code=""SUPPORT"",
            role_desc=""All Administrative Functions""
        )
        role5 = theq.Role(
            role_code=""ANALYTICS"",
            role_desc=""Analtyics Team to update Services per Office""
        )

        db.session.add(role_csr)
        db.session.add(role_ga)
        db.session.add(role3)
        db.session.add(role4)
        db.session.add(role5)
        db.session.commit()

        #-- Period State ----------------------------------------------------
        print(""--> Period States"")
        period_state1 = theq.PeriodState(
            ps_name=""Waiting"",
            ps_desc=""Waiting in line to see a CSR, after a ticket has been created for them. The time they are in this state is the Citizen Wait Time"",
            ps_number=1
        )
        period_state2 = theq.PeriodState(
            ps_name=""Ticket Creation"",
            ps_desc=""A receptionist is creating a service request / ticket for the citizen. This is the first state a citizen will be in. The time they are in this state is the CSR prep time."",
            ps_number=2
        )
        period_state3 = theq.PeriodState(
            ps_name=""Invited"",
            ps_desc=""Has been called from the waiting area to be served. The time they are in this state is the time it takes them to walk from the waiting area, to the CSR, until the CSR starts to serve them."",
            ps_number=4
        )
        period_state4 = theq.PeriodState(
            ps_name=""Being Served"",
            ps_desc=""Is being servbed by a CSR. The time they are in this state is the Service time."",
            ps_number=7
        )
        period_state5 = theq.PeriodState(
            ps_name=""On hold"",
            ps_desc=""Has been placed on hold be a csr. The time they are in this state is the Hold time"",
            ps_number=11
        )
        db.session.add(period_state1)
        db.session.add(period_state2)
        db.session.add(period_state3)
        db.session.add(period_state4)
        db.session.add(period_state5)
        db.session.commit()

        #-- Smartboard values -----------------------------------------------
        print(""--> Smartboard"")
        smartboard_call_name = theq.SmartBoard(sb_type=""callbyname"")
        smartboard_call_ticket = theq.SmartBoard(sb_type=""callbyticket"")
        smartboard_no_call = theq.SmartBoard(sb_type=""nocallonsmartboard"")
        db.session.add(smartboard_call_name)
        db.session.add(smartboard_call_ticket)
        db.session.add(smartboard_no_call)
        db.session.commit()

        #-- Citizen state values --------------------------------------------
        print(""--> Citizen State"")
        cs1 = theq.CitizenState(
            cs_state_name=""Active"",
            cs_state_desc=""Citizen is active, a ticket is being or has been created for them""
        )
        cs2 = theq.CitizenState(
            cs_state_name=""Received Services"",
            cs_state_desc=""Citizen left after receiving services""
        )
        cs3 = theq.CitizenState(
            cs_state_name=""Left before receiving services"",
            cs_state_desc=""Citizen left, after ticket creation, before service was started for them""
        )
        db.session.add(cs1)
        db.session.add(cs2)
        db.session.add(cs3)
        db.session.commit()

        #-- CSR state values     --------------------------------------------
        print(""--> CSR State"")
        csr_state_logout = theq.CSRState(
            csr_state_name=""Logout"",
            csr_state_desc=""Logged out""
        )
        csr_state2 = theq.CSRState(
            csr_state_name=""Login"",
            csr_state_desc=""Logged in""
        )
        csr_state3 = theq.CSRState(
            csr_state_name=""Break"",
            csr_state_desc=""Currently on break""
        )
        csr_state4 = theq.CSRState(
            csr_state_name=""Serving"",
            csr_state_desc=""Serving a citizen""
        )
        csr_state5 = theq.CSRState(
            csr_state_name=""Back Office"",
            csr_state_desc=""Currently doing back office work""
        )
        db.session.add(csr_state_logout)
        db.session.add(csr_state2)
        db.session.add(csr_state3)
        db.session.add(csr_state4)
        db.session.add(csr_state5)
        db.session.commit()

        #-- Service Request values ------------------------------------------
        print(""--> Service Request states"")
        sr_state1 = theq.SRState(
            sr_code=""Pending"",
            sr_state_desc=""Service Request is pending, citizen has not started receiving services yet.""
        )
        sr_state2 = theq.SRState(
            sr_code=""Active"",
            sr_state_desc=""Service Request is active.  A citizen has started being served.""
        )
        sr_state3 = theq.SRState(
            sr_code=""Complete"",
            sr_state_desc=""The service has been received for this Service Request.""
        )
        db.session.add(sr_state1)
        db.session.add(sr_state2)
        db.session.add(sr_state3)
        db.session.commit()

        #-- Service Category values -----------------------------------------
        print(""--> Categories and Services"")
        category_msp = theq.Service(
            service_code = ""MSP"",
            service_name = ""MSP"",
            service_desc = ""Medical Services Plan"",
            prefix = ""A"",
            display_dashboard_ind = 0,
            actual_service_ind = 0
        )
        category_ptax = theq.Service(
            service_code = ""PTAX"",
            service_name = ""Property Tax"",
            service_desc = ""Property Tax"",
            prefix = ""A"",
            display_dashboard_ind = 0,
            actual_service_ind = 0
        )
        category_back_office = theq.Service(
            service_code = ""Back Office"",
            service_name = ""Back Office"",
            service_desc = ""Back Office"",
            prefix = ""B"",
            display_dashboard_ind = 0,
            actual_service_ind = 0
        )
        category_exams = theq.Service(
            service_code = ""Exams"",
            service_name = ""Exams"",
            service_desc = ""Exams"",
            prefix = ""E"",
            display_dashboard_ind = 0,
            actual_service_ind = 0
        )
        db.session.add(category_msp)
        db.session.add(category_ptax)
        db.session.add(category_back_office)
        db.session.add(category_exams)
        db.session.commit()

        #-- Service values --------------------------------------------------
        service_msp6 = theq.Service(
            service_code = ""MSP - 006"",
            service_name = ""Payment - MSP"",
            service_desc = ""MSP- SC686, SC1089 -Pay direct payment, employer payment"",
            parent_id = category_msp.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_ptax4 = theq.Service(
            service_code = ""PTAX - 004"",
            service_name = ""Other - PTAX"",
            service_desc = ""PTax/RPT - Providing information, forms, searches, tax clearance certificate, address changes, add new owner, extensions, forfeiture status, tax search, etc."",
            parent_id = category_ptax.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_ptax1 = theq.Service(
            service_code = ""PTAX - 001"",
            service_name = ""Deferment Application"",
            service_desc = ""PTax/RPT - Process application - new and renewal, post note, etc."",
            parent_id = category_ptax.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_ptax2 = theq.Service(
            service_code = ""PTAX - 002"",
            service_name = ""Deferment Payment"",
            service_desc = ""PTax/RPT - Full or Partial deferment account payment"",
            parent_id = category_ptax.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_msp1 = theq.Service(
            service_code = ""MSP - 001"",
            service_name = ""Account Enquiry/Update"",
            service_desc = ""MSP-Address or family changes, personal information updates, general status enquiries, billing information from Biller Direct, immigration documents to HIBC, needs PHN, etc."",
            parent_id = category_msp.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_msp2 = theq.Service(
            service_code = ""MSP - 002"",
            service_name = ""BCSC Non Photo"",
            service_desc = ""MSP- SC2607 RAPID ordering , status enquiry, address update, also for the non photo form process when photo eligible, etc."",
            parent_id = category_msp.service_id,
            prefix = ""A"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_bo1 = theq.Service(
            service_code = ""Back Office - 001"",
            service_name = ""Batching"",
            service_desc = ""Batching"",
            parent_id = category_back_office.service_id,
            prefix = ""B"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_bo2 = theq.Service(
            service_code = ""Back Office - 002"",
            service_name = ""Cash Out"",
            service_desc = ""Cash Out"",
            parent_id = category_back_office.service_id,
            prefix = ""B"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        service_exams = theq.Service(
            service_code = ""Exams - 001"",
            service_name = ""Exam Management"",
            service_desc = ""ITA or PEST -Checking for expired Exams, contacting ITA or PEST program, mailing back ITA or shredding expired PEST Exams, etc."",
            parent_id = category_exams.service_id,
            prefix = ""E"",
            display_dashboard_ind = 1,
            actual_service_ind = 1
        )
        db.session.add(service_bo1)
        db.session.add(service_bo2)
        db.session.add(service_msp1)
        db.session.add(service_msp2)
        db.session.add(service_msp6)
        db.session.add(service_ptax1)
        db.session.add(service_ptax2)
        db.session.add(service_ptax4)
        db.session.add(service_exams)
        db.session.commit()

        print(""--> Bookings: Timezones"")

        timezone_one = theq.Timezone(
            timezone_name='America/Vancouver'
        )

        timezone_two = theq.Timezone(
            timezone_name='America/Dawson_Creek'
        )

        timezone_three = theq.Timezone(
            timezone_name='America/Edmonton'
        )

        timezone_four = theq.Timezone(
            timezone_name='America/Creston'
        )

        db.session.add(timezone_one)
        db.session.add(timezone_two)
        db.session.add(timezone_three)
        db.session.add(timezone_four)
        db.session.commit()

        #-- Office values ---------------------------------------------------
        print(""--> Offices"")
        office_test = theq.Office(
            office_name=""Test Office"",
            office_number=999,
            sb_id=smartboard_call_ticket.sb_id,
            exams_enabled_ind=1,
            timezone_id=timezone_one.timezone_id
        )
        office_100 = theq.Office(
            office_name=""100 Mile House"",
            office_number=1,
            sb_id=smartboard_no_call.sb_id,
            exams_enabled_ind=0,
            timezone_id=timezone_one.timezone_id
        )
        office_victoria = theq.Office(
            office_name=""Victoria"",
            office_number=61,
            sb_id=smartboard_call_name.sb_id,
            exams_enabled_ind=0,
            timezone_id=timezone_one.timezone_id
        )
        db.session.add(office_test)
        db.session.add(office_100)
        db.session.add(office_victoria)
        db.session.commit()

        #-- CSR values ------------------------------------------------------
        print(""--> CSRs"")
        cfms_postman_operator = theq.CSR(
            username=""cfms-postman-operator"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=1,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        cfms_postman_non_operator = theq.CSR(
            username=""cfms-postman-non-operator"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        akroon3r = theq.CSR(
            username=""akroon3r"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        sjrumsby = theq.CSR(
            username=""sjrumsby"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        scottrumsby = theq.CSR(
            username=""scottrumsby"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        chrisdmac = theq.CSR(
            username=""ChrisDMac"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        gil0109 = theq.CSR(
            username=""gil0109"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        demo_ga = theq.CSR(
            username=""admin"",
            office_id=office_test.office_id,
            role_id=role_ga.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        demo_csr = theq.CSR(
            username=""user"",
            office_id=office_test.office_id,
            role_id=role_csr.role_id,
            qt_xn_csr_ind=0,
            receptionist_ind=1,
            deleted=None,
            csr_state_id=csr_state_logout.csr_state_id,
            ita_designate=0,
            pesticide_designate=0,
            finance_designate=0,
            liaison_designate=0
        )
        db.session.add(cfms_postman_operator)
        db.session.add(cfms_postman_non_operator)
        db.session.add(demo_ga)
        db.session.add(demo_csr)
        db.session.add(akroon3r)
        db.session.add(sjrumsby)
        db.session.add(scottrumsby)
        db.session.add(chrisdmac)
        db.session.add(gil0109)
        db.session.commit()

        #-- The Office / Services values ------------------------------------
        print(""--> Office Services"")
        office_test.services.append(category_back_office)
        office_test.services.append(category_msp)
        office_test.services.append(category_ptax)
        office_test.services.append(category_exams)
        office_test.services.append(service_bo1)
        office_test.services.append(service_bo2)
        office_test.services.append(service_msp1)
        office_test.services.append(service_msp2)
        office_test.services.append(service_msp6)
        office_test.services.append(service_ptax1)
        office_test.services.append(service_ptax2)
        office_test.services.append(service_ptax4)
        office_test.services.append(service_exams)

        office_victoria.services.append(category_back_office)
        office_victoria.services.append(category_msp)
        office_victoria.services.append(service_bo1)
        office_victoria.services.append(service_bo2)
        office_victoria.services.append(service_msp1)
        office_victoria.services.append(service_msp2)
        office_victoria.services.append(service_msp6)

        office_100.services.append(category_back_office)
        office_100.services.append(category_ptax)
        office_100.services.append(service_bo1)
        office_100.services.append(service_bo2)
        office_100.services.append(service_ptax1)
        office_100.services.append(service_ptax2)
        office_100.services.append(service_ptax4)
        db.session.commit()

        print(""--> Bookings: Rooms"")
        room_one = bookings.Room(
            office_id = office_test.office_id,
            room_name = ""Boardroom 1"",
            capacity = 25,
            color = ""#EFD469""
        )
        room_two = bookings.Room(
            office_id = office_test.office_id,
            room_name = ""Turquoise W-135"",
            capacity = 25,
            color = ""#EFD469""
        )


        db.session.add(room_one)
        db.session.add(room_two)
        db.session.commit()

        print(""--> Bookings: Invigilators"")
        invigilator_one = bookings.Invigilator(
            invigilator_name = ""Homer Simpson"",
            office_id = office_test.office_id,
            invigilator_notes = ""He works in a nuclear power plant."",
            contact_phone = ""2502084247"",
            contact_email = ""homer.j.simpson@gmail.com"",
            contract_number = ""c-000001"",
            contract_expiry_date = ""2018-11-30""
        )

        invigilator_two = bookings.Invigilator(
            invigilator_name = ""Lisa Simpson"",
            office_id = office_test.office_id,
            invigilator_notes = ""She plays the sax-a-ma-phone during exams"",
            contact_phone = ""555-555-5555"",
            contact_email = ""lisasimpsonesq@gmail.com"",
            contract_number = ""c-000002"",
            contract_expiry_date = ""2018-12-31""
        )

        invigilator_three = bookings.Invigilator(
            invigilator_name=""Bart Simpson"",
            office_id=office_test.office_id,
            invigilator_notes=""Loves using chalk boards to communicate to examinees"",
            contact_phone=""555-555-5555"",
            contact_email=""bartwuzhere@gmail.com"",
            contract_number=""c-000003"",
            contract_expiry_date=""2019-01-31""
        )

        db.session.add(invigilator_one)
        db.session.add(invigilator_two)
        db.session.add(invigilator_three)
        db.session.commit()

        print(""--> Bookings: Exam Types"")
        exam_type_one = bookings.ExamType(
            exam_type_name = ""COFQ - 3HR Group Exam"",
            exam_color = ""#FF69B4"",
            number_of_hours = 3,
            method_type = ""Written"",
            ita_ind = 1,
            group_exam_ind = 1,
            pesticide_exam_ind = 0,
        )

        exam_type_two = bookings.ExamType(
            exam_type_name = ""COFQ - 3HR Single Exam"",
            exam_color = ""#FF69B4"",
            number_of_hours = 3,
            method_type = ""Written"",
            ita_ind = 1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_three = bookings.ExamType(
            exam_type_name = ""COFQ - 3HR Single Exam - Own Reader"",
            exam_color = ""#FF69B4"",
            number_of_hours = 3,
            method_type = ""Written"",
            ita_ind = 1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_four = bookings.ExamType(
            exam_type_name=""COFQ - 3HR Single Exam - SBC Reader"",
            exam_color=""#FF69B4"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind= 1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_five = bookings.ExamType(
            exam_type_name=""COFQ - 3HR Single Exam - Time Extension"",
            exam_color=""#FF69B4"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_six = bookings.ExamType(
            exam_type_name=""IPSE - 4HR Group Exam"",
            exam_color=""#FFD701"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=1,
            pesticide_exam_ind=0,
        )

        exam_type_seven = bookings.ExamType(
            exam_type_name=""IPSE - 4HR Single Exam"",
            exam_color=""#FFD701"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_eight = bookings.ExamType(
            exam_type_name=""IPSE - 4HR Single Exam - Own Reader"",
            exam_color=""#FFD701"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_nine = bookings.ExamType(
            exam_type_name=""IPSE - 4HR Single Exam - SBC Reader"",
            exam_color=""#FFD701"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_ten = bookings.ExamType(
            exam_type_name=""IPSE - 4HR Single Exam - Time Extension"",
            exam_color=""#FFD701"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_eleven = bookings.ExamType(
            exam_type_name=""SLE - 3HR Group Exam"",
            exam_color=""#8FBC8F"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=1,
            pesticide_exam_ind=0,
        )

        exam_type_twelve = bookings.ExamType(
            exam_type_name=""SLE - 3HR Single Exam"",
            exam_color=""#8FBC8F"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_thirteen = bookings.ExamType(
            exam_type_name=""SLE - 3HR Single Exam - Own Reader"",
            exam_color=""#8FBC8F"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_fourteen = bookings.ExamType(
            exam_type_name=""SLE - 3HR Single Exam - SBC Reader"",
            exam_color=""#8FBC8F"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_fifteen = bookings.ExamType(
            exam_type_name=""SLE - 3HR Single Exam - Time Extension"",
            exam_color=""#8FBC8F"",
            number_of_hours=3,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_sixteen = bookings.ExamType(
            exam_type_name=""Monthly Session Exam"",
            exam_color=""#FFFFFF"",
            number_of_hours=4,
            method_type=""Written"",
            ita_ind=1,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_seventeen = bookings.ExamType(
            exam_type_name=""Veterinary Exam"",
            exam_color=""#FFFFFF"",
            number_of_hours=2,
            method_type=""written"",
            ita_ind=0,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_eighteen = bookings.ExamType(
            exam_type_name=""Milk Grader"",
            exam_color=""#FFFFFF"",
            number_of_hours=2,
            method_type=""written"",
            ita_ind=0,
            group_exam_ind=0,
            pesticide_exam_ind=0,
        )

        exam_type_nineteen = bookings.ExamType(
            exam_type_name=""Pesticide"",
            exam_color=""#FFFFFF"",
            number_of_hours=2,
            method_type=""written"",
            ita_ind=0,
            group_exam_ind=0,
            pesticide_exam_ind=1,
        )

        exam_type_twenty = bookings.ExamType(
            exam_type_name=""Group Pesticide"",
            exam_color=""#FFFFFF"",
            number_of_hours=2,
            method_type=""written"",
            ita_ind=0,
            group_exam_ind=1,
            pesticide_exam_ind=1,
        )

        db.session.add(exam_type_one)
        db.session.add(exam_type_two)
        db.session.add(exam_type_three)
        db.session.add(exam_type_four)
        db.session.add(exam_type_five)
        db.session.add(exam_type_six)
        db.session.add(exam_type_seven)
        db.session.add(exam_type_eight)
        db.session.add(exam_type_nine)
        db.session.add(exam_type_ten)
        db.session.add(exam_type_eleven)
        db.session.add(exam_type_twelve)
        db.session.add(exam_type_thirteen)
        db.session.add(exam_type_fourteen)
        db.session.add(exam_type_fifteen)
        db.session.add(exam_type_sixteen)
        db.session.add(exam_type_seventeen)
        db.session.add(exam_type_eighteen)
        db.session.add(exam_type_nineteen)
        db.session.add(exam_type_twenty)
        db.session.commit()

        print(""--> Bookings: Exam"")
        exam_one = bookings.Exam(
            exam_type_id = exam_type_one.exam_type_id,
            office_id = office_test.office_id,
            event_id = ""1234abcd"",
            exam_name = ""Carpentry Red Seal"",
            examinee_name = ""Chandler Bing"",
            expiry_date = ""2018-11-29 11:19:53.5"",
            notes = ""This student is extremely s-m-r-t"",
            exam_received_date= ""2018-12-25 9:00:00.000"",
            session_number = 1,
            number_of_students = 1,
            exam_method = ""paper"",
        )

        exam_two = bookings.Exam(
            exam_type_id = exam_type_two.exam_type_id,
            office_id = office_test.office_id,
            event_id = ""e-000001"",
            exam_name = ""Plumbing Red Seal"",
            examinee_name = ""Joey Fatone"",
            expiry_date = ""2018-11-29 11:19:53.5"",
            notes = ""Speak slowly with this student, hearing impaired"",
            exam_received_date=""2018-12-24 9:00:00.000"",
            session_number = 2,
            number_of_students = 12,
            exam_method = ""online"",
        )

        exam_three = bookings.Exam(
            exam_type_id=exam_type_three.exam_type_id,
            office_id=office_test.office_id,
            event_id=""e-000002"",
            exam_name=""Culinary Red Seal"",
            examinee_name=""Anthony Bourdain"",
            expiry_date=""2018-11-29 11:19:53.5"",
            notes=""Student is extremely verbally obscene"",
            exam_received_date=""2018-12-23 9:00:00.000"",
            session_number=3,
            number_of_students=10,
            exam_method=""paper"",
        )

        exam_four = bookings.Exam(
            exam_type_id=exam_type_four.exam_type_id,
            office_id=office_test.office_id,
            event_id=""e-000005"",
            exam_name=""Lyrical Flow Exam"",
            examinee_name=""Celine Dion"",
            expiry_date=""2019-01-31 15:00:00.000"",
            notes=""Not sure if she uses a ghost writer or not"",
            exam_received_date=""2018-12-25 9:00:00.000"",
            session_number=4,
            number_of_students=25,
            exam_method=""online"",
        )

        exam_five = bookings.Exam(
            exam_type_id=exam_type_five.exam_type_id,
            office_id=office_test.office_id,
            event_id=""e-000005"",
            exam_name=""Lyrical Flow Exam"",
            examinee_name=""Luciano Pavarotti"",
            expiry_date=""2019-01-31 15:00:00.000"",
            notes=""Use ear plugs, quite loud"",
            exam_received_date=""2018-12-25 9:00:00.000"",
            session_number=4,
            number_of_students=25,
            exam_method=""paper"",
        )

        db.session.add(exam_one)
        db.session.add(exam_two)
        db.session.add(exam_three)
        db.session.add(exam_four)
        db.session.add(exam_five)
        db.session.commit()

        start_time = datetime(2019, 3, 28, 9, 0, 0)
        end_time = datetime(2019, 3, 28, 9, 15, 0)
        checked_in_time = datetime(2019, 3, 28, 9, 3, 0)

        start_time_aware = pytz.utc.localize(start_time)
        end_time_aware = pytz.utc.localize(end_time)
        checked_in_time_aware = pytz.utc.localize(checked_in_time)

        print(""--> Bookings: Appointments"")

        appointment_one = bookings.Appointment(
            office_id=office_test.office_id,
            service_id=service_exams.service_id,
            start_time=start_time_aware,
            end_time=end_time_aware,
            checked_in_time=checked_in_time_aware,
            comments=""Consultation"",
            citizen_name=""Adam Kroon"",
            contact_information=""adam@olivewood.io""
        )

        db.session.add(appointment_one)
        db.session.commit()


class FetchData(Command):

    def run(self):
        offices = db.session.query(theq.Office).all()
        for o in offices:
            print(o.id, o.name)

class CreateUser(Command):
    option_list = (
        Option('--username', '-u', dest='username'),
        Option('--password', '-p', dest='password'),
        Option('--office_id', '-o', dest='office_id'),
    )

    def run(self, username, password, office_id):

        if username is None or password is None or office_id is None:
            exit(""Error, username, password and office_id are all required"")

        user = theq.User(username, password, office_id)
        db.session.add(user)
        db.session.commit()

class MigrateWrapper(Command):
    def run(self):
        upgrade()

manager.add_command('db', MigrateCommand)
manager.add_command('migrate', MigrateWrapper())
manager.add_command('bootstrap', Bootstrap())
manager.add_command('fetch', FetchData())
manager.add_command('create_user', CreateUser())

if __name__ == '__main__':
    logging.log(logging.INFO, 'Running the Manager')
    manager.run()/n/n/napi/migrations/versions/5b0e8d74bcb8_.py/n/n""""""empty message

Revision ID: 5b0e8d74bcb8
Revises: c6e752f35c56
Create Date: 2019-04-16 14:16:01.939275

""""""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '5b0e8d74bcb8'
down_revision = 'c6e752f35c56'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('examtype', sa.Column('pesticide_exam_ind', sa.Integer(), nullable=False))
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('examtype', 'pesticide_exam_ind')
    # ### end Alembic commands ###
/n/n/napi/migrations/versions/d19d82e2bd34_.py/n/n""""""empty message

Revision ID: d19d82e2bd34
Revises: 5b0e8d74bcb8
Create Date: 2019-04-17 11:58:25.737327

""""""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'd19d82e2bd34'
down_revision = '5b0e8d74bcb8'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('examtype', sa.Column('number_of_minutes', sa.Integer(), nullable=True))
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('examtype', 'number_of_minutes')
    # ### end Alembic commands ###
/n/n/n",0
33,33,dbc822e1f1b968ae52075b1eceb1e43293903996,"/api/app/admin/examtype.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

from app.models.bookings import ExamType
from .base import Base
from flask_login import current_user
from qsystem import db


class ExamTypeConfig(Base):
    roles_allowed = ['SUPPORT']

    def is_accessible(self):
        return  current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    def get_query(self):
        return self.session.query(self.model)

    create_modal = False
    edit_modal = False
    column_list = [
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'method_type',
        'ita_ind',
        'group_exam_ind'
    ]

    column_searchable_list = {'exam_type_name'}

    form_excluded_columns = [
        'exam'
    ]

    form_create_rules = (
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'method_type',
        'ita_ind',
        'group_exam_ind'
    )

    form_edit_rules = (
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'method_type',
        'ita_ind',
        'group_exam_ind'
    )

    column_sortable_list = [
        'exam_type_name',
        'exam_color',
        'number_of_hours',
        'method_type',
        'ita_ind',
        'group_exam_ind'
    ]

    column_default_sort = 'exam_type_name'


ExamTypeModelView = ExamTypeConfig(ExamType, db.session)

/n/n/n/api/app/admin/office.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''


from app.models.theq import Office
from .base import Base
from flask_login import current_user
from qsystem import db


class OfficeConfig(Base):
    roles_allowed = ['SUPPORT']

    def is_accessible(self):
        return current_user.is_authenticated and current_user.role.role_code in self.roles_allowed

    create_modal = False
    edit_modal = False
    can_delete = False
    column_list = ['office_name', 'sb', 'services', 'deleted', 'exams_enabled_ind', 'timezone.timezone_name']
    form_excluded_columns = ('citizens', 'csrs', 'exams', 'rooms', 'invigilators')
    form_create_rules = ('office_name', 'office_number', 'sb', 'services', 'deleted', 'exams_enabled_ind',
                         'appointments_enabled_ind', 'timezone')
    form_edit_rules = ('office_name', 'office_number', 'sb', 'services', 'deleted', 'exams_enabled_ind',
                       'appointments_enabled_ind', 'timezone')
    column_labels = {'sb': 'Smartboard', 'timezone.timezone_name': 'Timezone Name'}
    column_sortable_list = ['office_name', 'sb', 'deleted', 'exams_enabled_ind']
    column_default_sort = 'office_name'


OfficeModelView = OfficeConfig(Office, db.session)
/n/n/n/api/app/resources/bookings/exam/exam_list.py/n/n'''Copyright 2018 Province of British Columbia

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.'''

import logging
from flask import g, request
from flask_restplus import Resource
from sqlalchemy import exc, or_
from app.models.bookings import Exam
from app.models.theq import CSR
from app.schemas.bookings import ExamSchema
from qsystem import api, jwt
from datetime import datetime, timedelta


@api.route(""/exams/"", methods=[""GET""])
class ExamList(Resource):

    exam_schema = ExamSchema(many=True)

    @jwt.requires_auth
    def get(self):
        try:
            csr = CSR.find_by_username(g.jwt_oidc_token_info['preferred_username'])

            ninety_day_filter = datetime.now() - timedelta(days=90)

            if csr.liaison_designate == 1:
                exams = Exam.query.filter(Exam.deleted_date.is_(None))\
                                  .filter(or_(Exam.exam_returned_date.is_(None),
                                              Exam.exam_returned_date > ninety_day_filter))

            else:
                exams = Exam.query.filter(Exam.deleted_date.is_(None))\
                                  .filter_by(office_id=csr.office_id)\
                                  .filter(or_(Exam.exam_returned_date.is_(None),
                                              Exam.exam_returned_date > ninety_day_filter))

            search_kwargs = {}

            if request.args:
                for key in request.args:
                    if hasattr(Exam, key):
                        search_kwargs[key] = request.args.get(key)

                exams = exams.filter_by(**search_kwargs)

            result = self.exam_schema.dump(exams)

            return {'exams': result.data,
                    'errors': result.errors}, 200

        except exc.SQLAlchemyError as error:
            logging.error(error, exc_info=True)
            return {""message"": ""api is down""}, 500
/n/n/n",1
18,18,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
19,19,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
118,118,671a06608c5210f205d93c0c235c94a8783892b9,"hyperion/lib/monitoring/threads.py/n/nfrom threading import Thread, Lock
import logging
import sys
import time
import hyperion.lib.util.config as config
from os import system
from subprocess import call
from psutil import Process, NoSuchProcess
is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as Queue
else:
    import queue as Queue
    

class ComponentMonitorJob(object):
    """"""Abstract class that represents a component monitoring job (local or remote).""""""

    def __init__(self, pid, comp_name):
        """"""Initializes component monitoring job.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""
        self.pid = pid
        self.comp_name = comp_name

    def run_check(self):
        """"""You need to override this function in monitoring subclasses. It is called in the main monitoring thread.

        :return: True on a successful check, otherwise a CrashEvent is generated
        :rtype: bool or CrashEvent
        """"""


class LocalComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a local component monitoring job.""""""

    def __init__(self, pid, comp_name):
        """"""Creates a monitoring job for a local component.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""

        super(LocalComponentMonitoringJob, self).__init__(pid, comp_name)

    def run_check(self):
        """"""Runs a check if the pid exists and has not finished yet.

        :return: True if the component is running, otherwise returns a generated ``LocalCrashEvent``
        :rtype bool or LocalCrashEvent
        """"""
        try:
            proc = Process(self.pid)
            if proc.is_running():
                return True
        except NoSuchProcess:
            pass
        return CrashEvent(self.comp_name)

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for local component %s with pid %s"" % (self.comp_name, self.pid)


class RemoteComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a remote component monitoring job.""""""

    def __init__(self, pid, comp_name, hostname, host_status):
        """"""Creates a remote component monitoring job.

        :param pid: Process id on the remote machine
        :type pid: int
        :param comp_name: Name of the monitored component
        :type comp_name: str
        :param hostname: Name of the host running the component
        :type hostname: str
        """"""

        super(RemoteComponentMonitoringJob, self).__init__(pid, comp_name)
        self.hostname = hostname
        self.host_status = host_status

    def run_check(self):
        """"""Runs a check if a remote process is still running.

        :return: True if the component is still running or the host is not reachable, otherwise a ``RemoteCrashEvent`` is generated.
        :rtype: bool or RemoteCrashEvent
        """"""

        if self.host_status.get(self.hostname):
            cmd = 'ssh -F %s %s ""ps -p %s > /dev/null""' % (config.CUSTOM_SSH_CONFIG_PATH, self.hostname, self.pid)
            if call(cmd, shell=True) == 0:
                return True
            else:
                return RemoteCrashEvent(self.comp_name, self.hostname)
        # Return true because no information can be retrieved. The connection to the host has to be reestablished first.
        return True

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for remote component %s with pid %s on host %s"" % (self.comp_name, self.pid,
                                                                                 self.hostname)


class HostMonitorJob(object):
    """"""Class representing a host monitoring job.""""""
    def __init__(self, pid, hostname, host_status, host_lock):
        """"""Create host monitoring job.

        :param pid: Process id of the ssh connection
        :type pid: int
        :param hostname: Name of the host connected to
        :type hostname: str
        :param host_status: Status of the used hosts
        :type host_status: dict
        :param host_lock: Lock that has to be acquired in order to write to the host status dictionary.
        :type host_lock: Lock
        """"""
        self.pid = pid
        self.hostname = hostname
        self.host_status = host_status
        self.host_lock = host_lock

    def run_check(self):
        try:
            proc = Process(self.pid)
            if proc.is_running() and system('(ping -w2 -c 1 %s) > /dev/null' % self.hostname) is 0:
                return True
        except NoSuchProcess:
            pass

        self.host_lock.acquire()
        self.host_status[self.hostname] = None
        self.host_lock.release()

        return DisconnectEvent(self.hostname)

    def info(self):
        return ""Running ssh host check for %s with pid %s"" % (self.hostname, self.pid)


class CrashEvent(object):
    """"""Superclass to model a component crash.

    Provides the name of the crashed component.""""""

    def __init__(self, comp_name):
        """"""Initializes the crash event assigning the component name

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        self.comp_name = comp_name


class LocalCrashEvent(CrashEvent):
    """"""Crash event subclass for local component crashes.

    Provides the name of the crashed component and a short message.
    """"""

    def __init__(self, comp_name):
        """"""Creates a local crash event class with a component name and generates a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        super(LocalCrashEvent, self).__init__(comp_name)
        self.message = 'Component %s crashed on localhost' % comp_name


class RemoteCrashEvent(CrashEvent):
    """"""Crash event subclass for remote component crashes.

    Provides the name of the crashed component along with the host it ran on and a short message.
    """"""

    def __init__(self, comp_name, hostname):
        """"""Creates a remote crash event with a component name and a host generating a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        :param hostname: Name of the host the component was running on
        :type hostname: str
        """"""

        super(RemoteCrashEvent, self).__init__(comp_name)
        self.hostname = hostname
        self.message = 'Component %s crashed on remote host %s' % (comp_name, hostname)


class DisconnectEvent(object):
    """"""Class representing a disconnect event for remote hosts.""""""

    def __init__(self, hostname):
        """"""Creates a disconnect event with a hostname and generates a short message.""""""
        self.hostname = hostname
        self.message = 'Lost connection to remote host %s' % hostname


class MonitoringThread(Thread):
    """"""This class is monitoring thread that extends the threading.Thread class.""""""

    def __init__(self, queue):
        """"""Initializes the monitoring thread with its input queue.

        :param queue: Input queue the monitor retrieves its jobs from
        :type queue: Queue.Queue
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Initialized thread"")
        super(MonitoringThread, self).__init__()
        self.job_queue = queue
        self.subscribed_queues = []
        self.end = False

    def kill(self):
        """"""Shuts down the thread by signalling the run function to end.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Killing process monitoring thread"")
        self.end = True

    def add_subscriber(self, queue):
        """"""Adds a subscriber to the list of queues to send notifications to.

        :param queue: Subscribing queue that will get notifications by this thread
        :type queue: Queue.Queue
        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Added subscriber"")
        self.subscribed_queues.append(queue)

    def run(self):
        """"""Starts the monitoring thread.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Started run funtion"")
        while not self.end:

            comp_jobs = []
            jobs = []
            already_handleled = {}
            # Get all enqueued jobs for this iteration
            while not self.job_queue.empty():
                mon_job = self.job_queue.get()
                if isinstance(mon_job, HostMonitorJob):
                    jobs.append(mon_job)
                if isinstance(mon_job, ComponentMonitorJob) and mon_job.comp_name not in already_handleled:
                    comp_jobs.append(mon_job)
                    already_handleled[mon_job.comp_name] = True

            # Reorder job list to first check the hosts, then check the components because this makes sense
            jobs.extend(comp_jobs)
            for mon_job in jobs:
                logger.debug(mon_job.info())
                ret = mon_job.run_check()
                if ret is True:
                    logger.debug(""S'all good man"")
                    # If job is ok, put it back for the next iteration
                    self.job_queue.put(mon_job)
                else:
                    # If job is not ok, notify subscribers
                    logger.debug(""Check failed, notifying subscribers"")
                    for subscriber in self.subscribed_queues:
                        subscriber.put(ret)

            time.sleep(1)
/n/n/nhyperion/manager.py/n/n#! /usr/bin/env python
from libtmux import Server, Window
from yaml import load, dump
import logging
import os
import sys
import socket
import uuid
import shutil
from psutil import Process, NoSuchProcess
from subprocess import call, Popen, PIPE
from threading import Lock
from enum import Enum
from signal import SIGTERM
from time import sleep, time
from lib.util.setupParser import Loader
from lib.util.depTree import Node, dep_resolve
from lib.monitoring.threads import LocalComponentMonitoringJob, RemoteComponentMonitoringJob, \
    HostMonitorJob, MonitoringThread
import lib.util.exception as exceptions
import lib.util.config as config

is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as queue
else:
    import queue as queue

logging.basicConfig(level=logging.WARNING, format=config.FORMAT, datefmt='%I:%M:%S')

BASE_DIR = os.path.dirname(__file__)
""""""Path to the directory this file is contained in""""""

SCRIPT_CLONE_PATH = (""%s/bin/start_named_clone_session.sh"" % BASE_DIR)
""""""File path of the 'clone session' script""""""


class CheckState(Enum):
    """"""Enum that provides information about the status of a run check""""""
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4
    UNREACHABLE = 5
    NOT_INSTALLED = 6


class StartState(Enum):
    """"""Enum that provides information about the start state of a component""""""
    STARTED = 0
    ALREADY_RUNNING = 1
    FAILED = 2


###################
# Logging
###################
def setup_log(window, filepath, comp_name):
    """"""Redirect stdout and stderr of window to file.

    Rotate logs and ensure the log directory for component `comp_name` exists, than,
    redirect the outputs of `window` to /dev/tty to undo the case that previous output was already redirected.
    After that redirect outputs to `file`.

    :param window: tmux reference to the window the component is being run in.
    :type window: Window
    :param filepath: filepath of the component log file
    :type filepath: str
    :param comp_name: name of the component being run
    :type comp_name: str
    :return: None
    """"""

    clear_log(filepath)
    ensure_dir(filepath)

    window.cmd(""send-keys"", ""exec > /dev/tty"", ""Enter"")

    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % filepath, ""Enter"")
    # Reroute stdout to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % filepath, ""Enter"")
    # Reroute stdin to log file <== causes remote host to logout, disabled for now
    # window.cmd(""send-keys"", ""exec 0> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\\t$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    """"""If found rename the log at file_path to a uuid.

    :param file_path: log file path
    :type file_path: str
    :return: None
    """"""

    if os.path.isfile(file_path):
        directory = os.path.dirname(file_path)
        os.rename(file_path, ""%s/%s.log"" % (directory, uuid.uuid4().hex))


def ensure_dir(file_path):
    """"""If not already existing, recursively create parent directory of file_path.

    :param file_path: log file path
    :type file_path: str
    :return: None
    """"""

    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)


class AbstractController(object):
    """"""Abstract controller class that defines basic controller variables and methods.""""""

    def __init__(self, configfile):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.config = None
        self.session = None
        self.server = None

    def load_config(self, filename=""default.yaml""):
        """"""Load configuration recursively from yaml file.

        :param filename: path to the configuration file.
        :type filename: str
        :return: None
        """"""
        try:
            with open(filename) as data_file:
                self.config = load(data_file, Loader)
        except IOError as e:
            self.logger.critical(""No such file '%s' found"" % filename)
            raise e

    ###################
    # Component Management
    ###################
    def run_component_check(self, comp):
        """"""Runs the component check defined in the component configuration and returns the exit state.

        :param comp: Component configuration
        :type comp: dict
        :return: Check exit state (fail = False / success = True).
        :rtype: bool
        """"""
        self.logger.debug(""Running specific component check for %s"" % comp['name'])
        if call(comp['cmd'][1]['check'], shell=True) == 0:
            self.logger.debug(""Check returned true"")
            return True
        else:
            self.logger.debug(""Check returned false"")
            return False

    def check_local_component(self, comp):
        """"""Check if a local component is running and return the corresponding CheckState.

        :param comp: Component configuration
        :type comp: dict
        :return: tuple of pid and component status. If the component is not running, the pid is 0.
        :rtype: (int, CheckState)
        """"""
        logger = self.logger

        logger.debug(""Running component check for %s"" % comp['name'])
        check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
        window = self.find_window(comp['name'])

        ret = None
        pid = 0

        if window:
            w_pid = self.get_window_pid(window)
            logger.debug(""Found window pid: %s"" % w_pid)

            # May return more child pids if logging is done via tee (which then was started twice in the window too)
            procs = []
            for entry in w_pid:
                procs.extend(Process(entry).children(recursive=True))

            pids = []
            for p in procs:
                if p.name() != 'tee':
                    pids.append(p.pid)
            logger.debug(""Window is running %s non-logger child processes: %s"" % (len(pids), pids))

            if len(pids) < 1:
                logger.debug(""Main process has finished. Running custom check if available"")
                if check_available and self.run_component_check(comp):
                    logger.debug(""Process terminated but check was successful"")
                    ret = CheckState.STOPPED_BUT_SUCCESSFUL
                else:
                    logger.debug(""Check failed or no check available: returning false"")
                    ret = CheckState.STOPPED
            elif check_available and self.run_component_check(comp):
                logger.debug(""Check succeeded"")
                pid = pids[0]
                ret = CheckState.RUNNING
            elif not check_available:
                logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
                pid = pids[0]
                ret = CheckState.RUNNING
            else:
                logger.debug(""Check failed: returning false"")
                ret = CheckState.STOPPED
        else:
            logger.debug(""%s window is not running. Running custom check"" % comp['name'])
            if check_available and self.run_component_check(comp):
                logger.debug(""Component was not started by Hyperion, but the check succeeded"")
                ret = CheckState.STARTED_BY_HAND
            else:
                logger.debug(""Window not running and no check command is available or it failed: returning false"")
                ret = CheckState.STOPPED

        return pid, ret

    def get_window_pid(self, window):
        """"""Returns pid of the tmux window process.

        :param window: tmux window
        :type window: Window
        :return: pid of the window as list
        :rtype: list of int
        """"""
        self.logger.debug(""Fetching pids of window %s"" % window.name)
        r = window.cmd('list-panes',
                       ""-F #{pane_pid}"")
        return [int(p) for p in r.stdout]

    def get_component_wait(self, comp):
        """"""Returns time to wait after component start (default of 5 seconds unless overwritten in configuration).

        :param comp: Component configuration
        :return: Component wait time
        :rtype: float
        """"""
        self.logger.debug(""Retrieving wait time of component %s"" % comp['name'])
        if 'wait' in comp:
            self.logger.debug(""Found %s seconds as wait time for %s"" % (float(comp['wait']), comp['name']))
            return float(comp['wait'])
        else:
            self.logger.debug(""No wait time for %s found, using default of %s seconds"" %
                              (comp['name'], config.DEFAULT_COMP_WAIT_TIME))
            return config.DEFAULT_COMP_WAIT_TIME

    def get_component_by_name(self, comp_name):
        """"""Return component configuration by providing only the name.

        :param comp_name: Component name
        :type comp_name: str
        :return: Component configuration
        :rtype: dict
        :raises exceptions.WindowNotFoundException: If component was not found
        """"""
        self.logger.debug(""Searching for %s in components"" % comp_name)
        for group in self.config['groups']:
            for comp in group['components']:
                if comp['name'] == comp_name:
                    self.logger.debug(""Component %s found"" % comp_name)
                    return comp
        raise exceptions.WindowNotFoundException('Component %s not found in current configuration' % comp_name)

    ###################
    # TMUX
    ###################
    def kill_session_by_name(self, name):
        """"""Kill tmux session by name.

        :param name: Name of the session to be killed
        :type name: str
        :return: None
        """"""
        self.logger.debug(""Killing session by name %s"" % name)
        session = self.server.find_where({
            ""session_name"": name
        })
        session.kill_session()

    def kill_window(self, window):
        """"""Kill tmux window by reference.

        :param window: Window to be killed
        :type window: Window
        :return: None
        """"""
        self.logger.debug(""Killing window by name %s"" % window.name)
        window.cmd(""send-keys"", """", ""C-c"")
        window.kill_window()

    def start_window(self, window, comp, log_file):
        """"""Execute cmd in window.

        Mainly used to run a component start command in its designated window

        :param window: Window the component will be started in
        :type window: Window
        :param comp: Component configuration
        :type comp: dict
        :param log_file: log file path
        :type log_file: str
        :return: None
        """"""

        cmd = comp['cmd'][0]['start']
        comp_name = comp['name']

        pid = self.get_window_pid(window)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))

        for proc in procs:
            self.logger.debug(""Killing leftover child process %s"" % proc.name())
            os.kill(proc.pid, SIGTERM)

        self.logger.debug(""Rotating log for %s"" % comp_name)
        setup_log(window, log_file, comp_name)
        self.logger.debug(""Running start command for %s"" % comp_name)
        window.cmd(""send-keys"", cmd, ""Enter"")

    def find_window(self, window_name):
        """"""Return window by name (None if not found).

        :param window_name: Window name
        :type window_name: str
        :return: Window with name `window_name`
        :rtype: Window or None
        """"""

        window = self.session.find_where({
            ""window_name"": window_name
        })
        return window

    def send_main_session_command(self, cmd):
        """"""Send command to the main window of the master session.

        `Session.cmd` sends the command to the currently active window of the session, and when issuing commands to the
        session, usually it is not intended to interact with component windows thus this functions fetches the main
        window and calls the `cmd` function on it.

        :param cmd: Command to execute
        :type cmd: str
        :return: None
        """"""
        self.logger.debug(""Sending command to master session main window: %s"" % cmd)
        window = self.find_window('Main')
        window.cmd(""send-keys"", cmd, ""Enter"")


class ControlCenter(AbstractController):
    """"""Controller class that is able to handle a master session.""""""

    def __init__(self, configfile=None, monitor_enabled=False):
        """"""Sets up the ControlCenter

        Initializes an empty node dict, an empty host_list dict, creates a queue for monitor jobs and a monitoring
        thread that is started right away and sets a handler for signals. After that the configuration file is loaded
        and a master session with a main window is created if not already existing.

        :param configfile: Path to the configuration to initialize
        :type configfile: str
        :param monitor_enabled: Whether the monitoring thread should be launched or not
        :type monitor_enabled: bool
        """"""

        super(ControlCenter, self).__init__(configfile)
        self.nodes = {}
        self.host_list = {}
        self.host_list_lock = Lock()
        self.monitor_queue = queue.Queue()
        self.mon_thread = MonitoringThread(self.monitor_queue)
        if monitor_enabled:
            self.mon_thread.start()

        if configfile:
            try:
                self.load_config(configfile)
            except IOError:
                self.cleanup()
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def init(self):
        """"""Initialize the controller.

        Sets up master ssh connections to all used hosts, copies components to them if they are reachable and computes a
        dependency tree for all used components.

        :return: None
        """"""
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            self.setup_ssh_config()

            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        if comp['host'] not in self.host_list:
                            if self.establish_master_connection(comp['host']):
                                self.logger.debug(""Master connection to %s established!"" % comp['host'])
                        if self.host_list.get(comp['host']) is not None:
                            self.copy_component_to_remote(comp, comp['host'])
                        else:
                            self.logger.debug(""Not copying because host %s is not reachable: %s"" %
                                              (comp['host'], self.host_list.get(comp['name'])))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        """"""Parses all components constructing a dependency tree.

        :param exit_on_fail: Whether the program should be exited on an encountered error
        :type exit_on_fail: bool
        :return: None
        """"""
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.add_edge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.add_edge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        self.logger.debug(""exit on fail: %s"" % exit_on_fail)
                        if exit_on_fail:
                            self.cleanup(status=1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except exceptions.CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                self.cleanup(status=1)

    def copy_component_to_remote(self, comp, host):
        """"""Copies `comp` to `TMP_SLAVE_DIR` on the remote host `host`.

        To do so `comp` gets temporarily saved as standalone configfile on the local machine (in `TMP_COMP_DIR`) and
        then scpd to `TMP_SLAVE_DIR` on `host` (after ensuring the containing directory exists via mkdir -p invocation
        over ssh in the main window of the master session).

        :param comp: Component to copy
        :type comp: dict
        :param host: Host to copy the component to
        :type host: str
        :return: None
        """"""

        comp_name = comp['name']

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (config.TMP_COMP_DIR, comp_name))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(comp, outfile, default_flow_style=False)

            self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp_name, host))
            cmd = (""ssh -F %s %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
                   (config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, tmp_comp_path, host,
                    config.TMP_SLAVE_DIR, comp_name))
            self.send_main_session_command(cmd)

    def setup_ssh_config(self):
        """"""Creates an ssh configuration that is saved to `CUSTOM_SSH_CONFIG_PATH`.

        The user config in `SSH_CONFIG_PATH` is copied to `CUSTOM_SSH_CONFIG_PATH` and then appends the lines enabling
        master connections for all hosts to it. This is done in order to use the master connection feature without
        tempering with the users standard configuration.

        :return: None
        """"""
        try:
            self.logger.debug(""Trying to copy ssh config from %s to %s"" % (config.SSH_CONFIG_PATH,
                                                                           config.CUSTOM_SSH_CONFIG_PATH))
            ensure_dir(config.CUSTOM_SSH_CONFIG_PATH)
            ensure_dir('%s/somefile' % config.SSH_CONTROLMASTERS_PATH)
            shutil.copy(config.SSH_CONFIG_PATH, config.CUSTOM_SSH_CONFIG_PATH)
        except IOError:
            self.logger.critical(""Could not copy ssh config! Make sure you have a config in your users .ssh folder!"")
            sys.exit(1)

        try:
            conf = open(config.CUSTOM_SSH_CONFIG_PATH, 'a')
            conf.write(""Host *\n    ControlMaster yes\n    ControlPath ~/.ssh/controlmasters/%C"")
        except IOError:
            self.logger.error(""Could not append to custom ssh config!"")

    def establish_master_connection(self, hostname):
        """"""Create a master ssh connection to host `hostname` in a dedicated window.

        The pid of the ssh session is put into the monitoring thread to have a means to check if the connection still
        exists. Also `host` is added to the list of known hosts with its current status.

        :param hostname: Host to establish a connection with
        :type hostname: str
        :return: Whether establishing the connection was successful or not
        :rtype: bool
        """"""

        self.logger.debug(""Establishing master connection to host %s"" % hostname)

        cmd = 'ssh -F %s %s -o BatchMode=yes -o ConnectTimeout=%s' % (config.CUSTOM_SSH_CONFIG_PATH,
                                                                      hostname, config.SSH_CONNECTION_TIMEOUT)

        is_up = True if os.system('ping -w2 -c 1 %s > /dev/null' % hostname) is 0 else False
        if not is_up:
            self.logger.error(""Host %s is not reachable!"" % hostname)

            self.host_list_lock.acquire()
            self.host_list[hostname] = None
            self.host_list_lock.release()
            return False

        window = self.find_window('ssh-%s' % hostname)
        if window:
            self.logger.debug(""Connecting to '%s' in old window"" % hostname)
            window.cmd(""send-keys"", """", ""C-c"")
        else:
            self.logger.debug(""Connecting to '%s' in new window"" % hostname)
            window = self.session.new_window('ssh-%s' % hostname)
        window.cmd(""send-keys"", cmd, ""Enter"")

        t_end = time() + config.SSH_CONNECTION_TIMEOUT

        pid = self.get_window_pid(window)
        pids = []

        while time() < t_end:
            procs = []
            for entry in pid:
                procs.extend(Process(entry).children(recursive=True))

            for p in procs:
                try:
                    if p.name() == 'ssh':
                        pids.append(p.pid)
                except NoSuchProcess:
                    pass
            if len(pids) > 0:
                break

        if len(pids) < 1:
            self.host_list_lock.acquire()
            self.host_list[hostname] = None
            self.host_list_lock.release()
            return False

        ssh_proc = Process(pids[0])
        # Add host to known list with process to poll from
        self.host_list_lock.acquire()
        self.host_list[hostname] = ssh_proc
        self.host_list_lock.release()

        self.logger.debug(""Testing if connection was successful"")
        if ssh_proc.is_running():
            self.logger.debug(""Adding ssh master to monitor queue"")
            self.monitor_queue.put(HostMonitorJob(pids[0], hostname, self.host_list, self.host_list_lock))
            self.logger.debug(""SSH process still running. Connection was successful"")
            return True
        else:
            self.logger.debug(""SSH process has finished. Connection was not successful. Check if an ssh connection ""
                              ""is allowed or if the certificate has to be renewed"")
            return False

    def reconnect_with_host(self, hostname):
        """"""Re-establish master connection to host `hostname`

        :param hostname: Host to connect to
        :type hostname: str
        :return: Whether establishing the connection was successful or not
        :rtype: bool
        """"""

        # Check if really necessary
        self.logger.debug(""Reconnecting with %s"" % hostname)
        proc = self.host_list.get(hostname)
        if proc is not None:
            self.logger.debug(""Killing off leftover process"")
            proc.kill()

        # Start new connection
        if self.establish_master_connection(hostname):
            # Sync components
            self.logger.debug(""Syncinc components to remote host"")
            for group in self.config['groups']:
                for comp in group['components']:
                    if comp['host'] == hostname:
                        self.copy_component_to_remote(comp, comp['host'])
            return True
        else:
            return False

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        """"""Stop component `comp`.

        Invokes the lower level stop function depending on whether the component is run locally or on a remote host.

        :param comp: Component to stop
        :type comp: dict
        :return: None
        """"""

        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp)
        else:
            window = self.find_window(comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.debug(""Shutting down window..."")
                self.kill_window(window)
                self.logger.debug(""... done!"")

    def stop_remote_component(self, comp):
        """"""Stops remote component `comp`.

        Via ssh Hyperion is executed on the remote host in slave mode with the --kill option.

        :param comp: Component to stop
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']
        # invoke Hyperion in slave kill mode on remote host
        if not self.host_list[host]:
            self.logger.error(""Host %s is unreachable. Can not stop component %s"" % (host, comp_name))
            return

        cmd = (""ssh -F %s %s 'hyperion --config %s/%s.yaml slave --kill'"" % (
            config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, comp_name))
        self.send_main_session_command(cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):
        """"""Invoke dependency based start of component `comp`.

        Traverses the path of dependencies and invokes a call to ``start_component_without_deps`` for all found
        dependencies before calling it for `comp`.

        :param comp: Component to start
        :type comp: dict
        :return: Information on the start process
        :rtype: StartState
        """"""

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    # Wait component time for startup
                    sleep(self.get_component_wait(comp))

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                                state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 10:
                            return StartState.FAILED
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
            return StartState.ALREADY_RUNNING
        else:
            self.start_component_without_deps(comp)
        return StartState.STARTED

    def start_component_without_deps(self, comp):
        """"""Chooses which lower level start function to use depending on whether the component is run on a remote host or not.

        :param comp: Component to start
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']

        if host != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp_name, host))
            self.start_remote_component(comp)
        else:
            log_file = (""%s/%s/latest.log"" % (config.TMP_LOG_PATH, comp_name))
            window = self.find_window(comp_name)

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp_name)
                self.start_window(window, comp, log_file)
            else:
                self.logger.debug(""creating window '%s'"" % comp_name)
                window = self.session.new_window(comp_name)
                self.start_window(window, comp, log_file)

    def start_remote_component(self, comp):
        """"""Start component 'comp' on remote host.

        The remote component is started by invoking Hyperion over ssh in slave mode.

        :param comp: Component to start
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']
        # invoke Hyperion in slave mode on each remote host

        if not self.host_list[host]:
            self.logger.error(""Hot %s is not reachable. Can not start component %s"" % (host, comp_name))
            return

        cmd = (""ssh -F %s %s 'hyperion --config %s/%s.yaml slave'"" % (
            config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, comp_name))
        self.send_main_session_command(cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        """"""Runs component check for `comp` and returns status.

        If `comp` is run locally the call is redirected to ``check_local_component``. If the `comp` is run on a remote
        host the function checks, if the host is reachable and on success issues an ssh command over the master
        connection which starts Hyperion in slave mode with the check option. The return value of the call is then
        interpreted for further processing.

        :param comp: Component to check
        :type comp: dict
        :return: State of the component
        :rtype: CheckState
        """"""
        if self.run_on_localhost(comp):
            ret = self.check_local_component(comp)

            pid = ret[0]
            if pid != 0:
                self.monitor_queue.put(LocalComponentMonitoringJob(pid, comp['name']))
            return ret[1]
        else:
            self.logger.debug(""Starting remote check"")
            if self.host_list.get(comp['host']) is not None:
                p = Popen(['ssh', '-F', config.CUSTOM_SSH_CONFIG_PATH, comp['host'], 'hyperion --config %s/%s.yaml slave -c' %
                           (config.TMP_SLAVE_DIR, comp['name'])], stdin=PIPE, stdout=PIPE, stderr=PIPE)

                while p.poll() is None:
                    sleep(.5)
                pid = int(p.stdout.readlines()[-1])

                if pid != 0:
                    self.logger.debug(""Got remote pid %s for component %s"" % (pid, comp['name']))
                    self.monitor_queue.put(RemoteComponentMonitoringJob(pid, comp['name'], comp['host'], self.host_list))
                rc = CheckState(p.returncode)
                try:
                    return rc
                except ValueError:
                    self.logger.error(""Hyperion is not installed on host %s!"" % comp['host'])
                    return CheckState.NOT_INSTALLED
            else:
                self.logger.error(""Host %s is unreachable. Can not run check for component %s!"" % (comp['host'],
                                                                                                   comp['name']))
                return CheckState.UNREACHABLE

    ###################
    # CLI Functions
    ###################
    def list_components(self):
        """"""List all components used by the current configuration.

        :return: List of components
        :rtype: list of str
        """"""

        return [self.nodes.get(node).comp_name for node in self.nodes]

    def start_by_cli(self, comp_name):
        """"""Interface function for starting component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to start
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')

        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return

        logger.info(""Starting component '%s' ..."" % comp_name)
        ret = self.start_component(comp)
        if ret is StartState.STARTED:
            logger.info(""Started component '%s'"" % comp_name)
            sleep(self.get_component_wait(comp))
            ret = self.check_component(comp)
            logger.info(""Check returned status: %s"" % ret.name)
        elif ret is StartState.FAILED:
            logger.info(""Starting '%s' failed!"" % comp_name)
        elif ret is StartState.ALREADY_RUNNING:
            logger.info(""Aborted '%s' start: Component is already running!"" % comp_name)

    def stop_by_cli(self, comp_name):
        """"""Interface function for stopping component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to stop
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')
        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return
        logger.info(""Stopping component '%s' ..."")
        self.stop_component(comp)
        sleep(2)
        ret = self.check_component(comp)
        logger.info(""Check returned status: %s"" % ret.name)

    def check_by_cli(self, comp_name):
        """"""Interface function for checking component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to check
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')
        logger.info(""Checking component %s ..."" % comp_name)
        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return
        ret = self.check_component(comp)
        logger.info(""Check returned status: %s"" % ret.name)

    def start_clone_session_and_attach(self, comp_name):
        """"""Interface function for show term of component by name `comp_name` from the cli. !!(NYI)!!

        :param comp_name: Name of the component to show
        :type comp_name: str
        :return: None
        """"""

        self.logger.debug(""NYI"")

    def show_comp_log(self, comp_name):
        """"""Interface function for viewing the log of component by name `comp_name` from the cli. !!(NYI)!!

        :param comp_name: Name of the component whose log to show
        :type comp_name: str
        :return: None
        """"""

        self.logger.debug(""NYI"")

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        """"""Get a list of all components that `comp` depends on.

        :param comp: Component to get dependencies from
        :type comp: dict
        :return: List of components
        :rtype: list of Node
        """"""

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        """"""Check if 'hostname' resolves to localhost.

        :param hostname: Name of host to check
        :type hostname: str
        :return: Whether 'host' resolves to localhost or not
        :rtype: bool
        """"""

        try:
            hn_out = socket.gethostbyname('%s' % hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            self.logger.error(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)


    def run_on_localhost(self, comp):
        """"""Check if component 'comp' is run on localhost or not.

        :param comp: Component to check
        :type comp: dict
        :return: Whether component is run on localhost or not
        :rtype: bool
        """"""

        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        """"""Kill tmux session by name 'name' on host 'host'

        :param name: Name of the session to kill
        :type name: str
        :param host: Host that the session runs on
        :type host: str
        :return: None
        """"""

        cmd = ""ssh -F %s -t %s 'tmux kill-session -t %s'"" % (config.CUSTOM_SSH_CONFIG_PATH, host, name)
        self.send_main_session_command(cmd)

    def start_clone_session(self, comp):
        """"""Start a clone session of the master session and open the window of component `comp`.

        Because the libtmux library does not provide functions to achieve this, a bash script is run to automatize the
        process.

        :param comp: Component whose window is to be shown in the cloned session
        :type comp_name: str
        :returns None
        """"""

        comp_name = comp['name']
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, self.session_name, comp_name)
        self.send_main_session_command(cmd)

    def start_remote_clone_session(self, comp):
        """"""Start a clone session of the remote slave session and open the window of component `comp`.

        Same as ``start_clone_session`` only that the bash script is fed into a ssh command issued over the main window
        of the master session.

        :param comp: Component whose window is to be shown in the clone session
        :type comp: dict
        :return:
        """"""

        session_name = 'slave-session'
        comp_name = comp['name']
        hostname = comp['host']

        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh -F %s %s 'bash -s' < %s"" % (config.CUSTOM_SSH_CONFIG_PATH, hostname, remote_cmd)
        self.send_main_session_command(cmd)

    ###################
    # Safe shutdown
    ###################
    def signal_handler(self, signum, frame):
        """"""Handler that invokes cleanup on a received signal.

        :param signum: Signal signum
        :type int
        :param frame:
        :return: None
        """"""
        self.logger.debug(""received signal %s. Running cleanup"" % signum)
        self.cleanup()

    def cleanup(self, full=False, status=0):
        """"""Clean up for safe shutdown.

        Kills the monitoring thread and if full shutdown is requested also the ssh slave sessions and master connections
        and then shuts down the local tmux master session.

        :param full: Whether everything shall be shutdown or not
        :type full: bool
        :return: None
        """"""

        self.logger.info(""Shutting down safely..."")

        self.logger.debug(""Killing monitoring thread"")
        self.mon_thread.kill()

        if full:
            self.logger.debug(""Chose full shutdown. Killing remote and main sessions"")

            for host in self.host_list:
                window = self.find_window('ssh-%s' % host)

                if window:
                    self.logger.debug(""Killing remote slave session of host %s"" % host)
                    self.kill_remote_session_by_name(""slave-session"", host)
                    self.logger.debug(""Close ssh-master window of host %s"" % host)
                    self.kill_window(window)

            self.kill_session_by_name(self.session_name)
        self.logger.info(""... Done"")
        exit(status)


class SlaveLauncher(AbstractController):
    """"""Controller class that performs a single slave execution task.""""""

    def __init__(self, configfile, kill_mode=False, check_mode=False):
        """"""Initializes slave.

        :param configfile: Path to configuration file (component configuration)
        :type configfile: str
        :param kill_mode: Whether the slave was started in kill mode or not
        :type kill_mode: bool
        :param check_mode: Whether the slave was started in check mode or not
        :type check_mode: bool
        """"""

        super(SlaveLauncher, self).__init__(configfile)
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.debug('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.debug('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.debug(""No slave session found on server. Aborting"")
            # Print fake pid
            print(0)
            exit(CheckState.STOPPED.value)

        if configfile:
            try:
                self.load_config(configfile)
            except IOError:
                exit(1)
            self.window_name = self.config['name']
            self.log_file = (""%s/%s/latest.log"" % (config.TMP_LOG_PATH, self.window_name))
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def init(self):
        """"""Runs the mode specified by the slave execution call (start/stop or preparation for check)

        :return: None
        """"""
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = self.find_window(self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.debug(""Shutting down window..."")
                    self.kill_window(window)
                    self.logger.debug(""... done!"")
            elif not self.kill_mode:
                self.logger.debug(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                self.start_window(window, self.config, self.log_file)

            else:
                self.logger.debug(""There is no component running by the name '%s'. Exiting kill mode"" %
                                  self.window_name)

    def run_check(self):
        """"""Run check for the current component.

        :return: Status of the component
        :rtype: CheckState
        """"""
        if not self.config:
            self.logger.error(""Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error(""Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        ret = self.check_local_component(self.config)
        print(ret[0])
        exit(ret[1].value)
/n/n/n",0
119,119,671a06608c5210f205d93c0c235c94a8783892b9,"/hyperion/lib/monitoring/threads.py/n/nfrom threading import Thread, Lock
import logging
import sys
import time
import hyperion.lib.util.config as config
from os import system
from subprocess import call
from psutil import Process, NoSuchProcess
is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as Queue
else:
    import queue as Queue
    

class ComponentMonitorJob(object):
    """"""Abstract class that represents a component monitoring job (local or remote).""""""

    def __init__(self, pid, comp_name):
        """"""Initializes component monitoring job.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""
        self.pid = pid
        self.comp_name = comp_name

    def run_check(self):
        """"""You need to override this function in monitoring subclasses. It is called in the main monitoring thread.

        :return: True on a successful check, otherwise a CrashEvent is generated
        :rtype: bool or CrashEvent
        """"""


class LocalComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a local component monitoring job.""""""

    def __init__(self, pid, comp_name):
        """"""Creates a monitoring job for a local component.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""

        super(LocalComponentMonitoringJob, self).__init__(pid, comp_name)

    def run_check(self):
        """"""Runs a check if the pid exists and has not finished yet.

        :return: True if the component is running, otherwise returns a generated ``LocalCrashEvent``
        :rtype bool or LocalCrashEvent
        """"""
        try:
            proc = Process(self.pid)
            if proc.is_running():
                return True
        except NoSuchProcess:
            pass
        return CrashEvent(self.comp_name)

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for local component %s with pid %s"" % (self.comp_name, self.pid)


class RemoteComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a remote component monitoring job.""""""

    def __init__(self, pid, comp_name, hostname, host_status):
        """"""Creates a remote component monitoring job.

        :param pid: Process id on the remote machine
        :type pid: int
        :param comp_name: Name of the monitored component
        :type comp_name: str
        :param hostname: Name of the host running the component
        :type hostname: str
        """"""

        super(RemoteComponentMonitoringJob, self).__init__(pid, comp_name)
        self.hostname = hostname
        self.host_status = host_status

    def run_check(self):
        """"""Runs a check if a remote process is still running.

        :return: True if the component is still running or the host is not reachable, otherwise a ``RemoteCrashEvent`` is generated.
        :rtype: bool or RemoteCrashEvent
        """"""

        if self.host_status.get(self.hostname):
            cmd = 'ssh -F %s %s ""ps -p %s > /dev/null""' % (config.CUSTOM_SSH_CONFIG_PATH, self.hostname, self.pid)
            if call(cmd, shell=True) == 0:
                return True
            else:
                return RemoteCrashEvent(self.comp_name, self.hostname)
        # Return true because no information can be retrieved. The connection to the host has to be reestablished first.
        return True

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for remote component %s with pid %s on host %s"" % (self.comp_name, self.pid,
                                                                                 self.hostname)


class HostMonitorJob(object):
    """"""Class representing a host monitoring job.""""""
    def __init__(self, pid, hostname, host_status, host_lock):
        """"""Create host monitoring job.

        :param pid: Process id of the ssh connection
        :type pid: int
        :param hostname: Name of the host connected to
        :type hostname: str
        :param host_status: Status of the used hosts
        :type host_status: dict
        :param host_lock: Lock that has to be acquired in order to write to the host status dictionary.
        :type host_lock: Lock
        """"""
        self.pid = pid
        self.hostname = hostname
        self.host_status = host_status
        self.host_lock = host_lock

    def run_check(self):
        try:
            proc = Process(self.pid)
            if proc.is_running() and system(""exec >(ping %s -c 10 >/dev/null) </dev/null"" % self.hostname) is 0:
                return True
        except NoSuchProcess:
            pass

        self.host_lock.acquire()
        self.host_status[self.hostname] = None
        self.host_lock.release()

        return DisconnectEvent(self.hostname)

    def info(self):
        return ""Running ssh host check for %s with pid %s"" % (self.hostname, self.pid)


class CrashEvent(object):
    """"""Superclass to model a component crash.

    Provides the name of the crashed component.""""""

    def __init__(self, comp_name):
        """"""Initializes the crash event assigning the component name

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        self.comp_name = comp_name


class LocalCrashEvent(CrashEvent):
    """"""Crash event subclass for local component crashes.

    Provides the name of the crashed component and a short message.
    """"""

    def __init__(self, comp_name):
        """"""Creates a local crash event class with a component name and generates a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        super(LocalCrashEvent, self).__init__(comp_name)
        self.message = 'Component %s crashed on localhost' % comp_name


class RemoteCrashEvent(CrashEvent):
    """"""Crash event subclass for remote component crashes.

    Provides the name of the crashed component along with the host it ran on and a short message.
    """"""

    def __init__(self, comp_name, hostname):
        """"""Creates a remote crash event with a component name and a host generating a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        :param hostname: Name of the host the component was running on
        :type hostname: str
        """"""

        super(RemoteCrashEvent, self).__init__(comp_name)
        self.hostname = hostname
        self.message = 'Component %s crashed on remote host %s' % (comp_name, hostname)


class DisconnectEvent(object):
    """"""Class representing a disconnect event for remote hosts.""""""

    def __init__(self, hostname):
        """"""Creates a disconnect event with a hostname and generates a short message.""""""
        self.hostname = hostname
        self.message = 'Lost connection to remote host %s' % hostname


class MonitoringThread(Thread):
    """"""This class is monitoring thread that extends the threading.Thread class.""""""

    def __init__(self, queue):
        """"""Initializes the monitoring thread with its input queue.

        :param queue: Input queue the monitor retrieves its jobs from
        :type queue: Queue.Queue
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Initialized thread"")
        super(MonitoringThread, self).__init__()
        self.job_queue = queue
        self.subscribed_queues = []
        self.end = False

    def kill(self):
        """"""Shuts down the thread by signalling the run function to end.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Killing process monitoring thread"")
        self.end = True

    def add_subscriber(self, queue):
        """"""Adds a subscriber to the list of queues to send notifications to.

        :param queue: Subscribing queue that will get notifications by this thread
        :type queue: Queue.Queue
        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Added subscriber"")
        self.subscribed_queues.append(queue)

    def run(self):
        """"""Starts the monitoring thread.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Started run funtion"")
        while not self.end:

            comp_jobs = []
            jobs = []
            already_handleled = {}
            # Get all enqueued jobs for this iteration
            while not self.job_queue.empty():
                mon_job = self.job_queue.get()
                if isinstance(mon_job, HostMonitorJob):
                    jobs.append(mon_job)
                if isinstance(mon_job, ComponentMonitorJob) and mon_job.comp_name not in already_handleled:
                    comp_jobs.append(mon_job)
                    already_handleled[mon_job.comp_name] = True

            # Reorder job list to first check the hosts, then check the components because this makes sense
            jobs.extend(comp_jobs)
            for mon_job in jobs:
                logger.debug(mon_job.info())
                ret = mon_job.run_check()
                if ret is True:
                    logger.debug(""S'all good man"")
                    # If job is ok, put it back for the next iteration
                    self.job_queue.put(mon_job)
                else:
                    # If job is not ok, notify subscribers
                    logger.debug(""Check failed, notifying subscribers"")
                    for subscriber in self.subscribed_queues:
                        subscriber.put(ret)

            time.sleep(1)
/n/n/n",1
176,176,0c72e43a4863d07468a090cd4365c5505deb715b,"wiki/forms.py/n/n# -*- coding: utf-8 -*-
import re

from django import forms
from django.contrib.contenttypes.models import ContentType
from django.utils.translation import ugettext_lazy as _

from wiki.models import Article
from wiki.models import ChangeSet
from wiki.templatetags.wiki_extras import WIKI_WORD_RE

wikiword_pattern = re.compile('^' + WIKI_WORD_RE + '$')


class ArticleForm(forms.ModelForm):

    summary = forms.CharField(widget=forms.Textarea)

    comment = forms.CharField(required=False)
    user_ip = forms.CharField(widget=forms.HiddenInput)

    content_type = forms.ModelChoiceField(
        queryset=ContentType.objects.all(),
        required=False,
        widget=forms.HiddenInput)
    object_id = forms.IntegerField(required=False,
                                   widget=forms.HiddenInput)

    action = forms.CharField(widget=forms.HiddenInput)

    class Meta:
        model = Article
        exclude = ('creator', 'creator_ip',
                   'group', 'created_at', 'last_update')

    def clean_title(self):
        """"""Check for some errors regarding the title:

        1. Check for bad characters
        2. Check for already used titles

        """"""

        title = self.cleaned_data['title']
        if not wikiword_pattern.match(title):
            raise forms.ValidationError(
                _('The title can only consist of alphanumeric characters and the underscore'))

        # 'self.initial' contain the prefilled values of the form
        # We use title as default here, so for new articles 'pre_title'
        # contains the title of the new article.
        pre_title = self.initial.get('title', title)
        if pre_title != title:
            # Check if the new name once has been used
            try:
                cs = ChangeSet.objects.filter(old_title=title)
            except Changeset.DoesNotExist:
                # no old names found
                return title
            else:
                raise forms.ValidationError(
                    _('The title %(title)s is already in use, maybe an other article had once this name.'), params={'title': title},)
        # title not changed, no errors
        return title

    def clean(self):
        super(ArticleForm, self).clean()
        kw = {}

        if self.cleaned_data['action'] == 'create':
            try:
                kw['title'] = self.cleaned_data['title']
                kw['content_type'] = self.cleaned_data['content_type']
                kw['object_id'] = self.cleaned_data['object_id']
            except KeyError:
                pass  # some error in this fields

        return self.cleaned_data

    def cache_old_content(self):
        if self.instance.id is None:
            self.old_title = self.old_content = self.old_markup = ''
            self.is_new = True
        else:
            self.old_title = self.instance.title
            self.old_content = self.instance.content
            self.old_markup = self.instance.markup
            self.is_new = False

    def save(self, *args, **kwargs):
        # 0 - Extra data
        editor_ip = self.cleaned_data['user_ip']
        comment = self.cleaned_data['comment']

        # 2 - Save the Article
        article = super(ArticleForm, self).save(*args, **kwargs)

        # 3 - Set creator and group
        editor = getattr(self, 'editor', None)
        group = getattr(self, 'group', None)
        if self.is_new:
            article.creator_ip = editor_ip
            if editor is not None:
                article.creator = editor
                article.group = group
            article.save(*args, **kwargs)

        # 4 - Create new revision
        changeset = article.new_revision(
            self.old_content, self.old_title, self.old_markup,
            comment, editor_ip, editor)

        return article, changeset
/n/n/nwiki/views.py/n/n# -*- coding: utf-8 -*-

from datetime import datetime

from django.conf import settings
from django.core.cache import cache
from django.template import RequestContext
from django.core.urlresolvers import reverse
from django.http import (Http404, HttpResponseRedirect,
                         HttpResponseNotAllowed, HttpResponse, HttpResponseForbidden)
from django.shortcuts import get_object_or_404, render_to_response, redirect
from django.contrib.contenttypes.models import ContentType
from django.contrib import messages
from wiki.forms import ArticleForm
from wiki.models import Article, ChangeSet, dmp

from wiki.utils import get_ct
from django.contrib.auth.decorators import login_required

from wl_utils import get_real_ip
import re

# Settings
#  lock duration in minutes
try:
    WIKI_LOCK_DURATION = settings.WIKI_LOCK_DURATION
except AttributeError:
    WIKI_LOCK_DURATION = 15

try:
    from notification import models as notification
except ImportError:
    notification = None

# default querysets
ALL_ARTICLES = Article.objects.all()
ALL_CHANGES = ChangeSet.objects.all()


def get_articles_by_group(article_qs, group_slug=None,
                          group_slug_field=None, group_qs=None):
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs,
                                  **{group_slug_field: group_slug})
        article_qs = article_qs.filter(content_type=get_ct(group),
                                       object_id=group.id)
    return article_qs, group


def get_articles_for_object(object, article_qs=None):
    if article_qs is None:
        article_qs = ALL_ARTICLES
    return article_qs.filter(content_type=get_ct(object),
                             object_id=object.id)


def get_url(urlname, group=None, args=None, kw=None):
    if group is None:
        return reverse(urlname, args=args)
    else:
        app = group._meta.app_label
        urlconf = '.'.join([app, 'urls'])
        url = reverse(urlname, urlconf, kwargs=kw)
        return ''.join(['/', app, url])  # @@@ harcoded: /app/.../

# NOCOMM Franku: This Class is currently not used
# If we want this it has to be checked for the changes
# related to django 1.8.
# A javascript alert box is maybe a better solution


class ArticleEditLock(object):
    """"""A soft lock to edting an article.""""""

    def __init__(self, title, request, message_template=None):
        self.title = title
        self.user_ip = get_real_ip(request)
        self.created_at = datetime.now()

        if message_template is None:
            message_template = ('Possible edit conflict:'
                                ' another user started editing this article at %s')

        self.message_template = message_template

        cache.set(title, self, WIKI_LOCK_DURATION * 60)

    def create_message(self, request):
        """"""Send a message to the user if there is another user editing this
        article.""""""
        if not self.is_mine(request):
            user = request.user
            user.message_set.create(
                message=self.message_template % self.created_at)

    def is_mine(self, request):
        return self.user_ip == get_real_ip(request)


def has_read_perm(user, group, is_member, is_private):
    """""" Return True if the user has permission to *read*
    Articles, False otherwise.
    """"""
    if (group is None) or (is_member is None) or is_member(user, group):
        return True
    if (is_private is not None) and is_private(group):
        return False
    return True


def has_write_perm(user, group, is_member):
    """"""Return True if the user have permission to edit Articles, False
    otherwise.""""""
    if (group is None) or (is_member is None) or is_member(user, group):
        return True
    return False


def article_list(request,
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 ArticleClass=Article,
                 template_name='index.html',
                 template_dir='wiki',
                 extra_context=None,
                 is_member=None,
                 is_private=None,
                 *args, **kw):
    if request.method == 'GET':
        articles, group = get_articles_by_group(
            article_qs, group_slug,
            group_slug_field, group_qs)

        allow_read = has_read_perm(request.user, group, is_member, is_private)
        allow_write = has_write_perm(request.user, group, is_member)

        if not allow_read:
            return HttpResponseForbidden()

        articles = articles.order_by('title')

        template_params = {'articles': articles,
                           'allow_write': allow_write}

        if group_slug is not None:
            template_params['group'] = group
            new_article = ArticleClass(title='NewArticle',
                                       content_type=get_ct(group),
                                       object_id=group.id)
        else:
            new_article = ArticleClass(title='NewArticle')
        template_params['new_article'] = new_article
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


def view_article(request, title, revision=None,
                 ArticleClass=Article,  # to create an unsaved instance
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 template_name='view.html',
                 template_dir='wiki',
                 extra_context=None,
                 is_member=None,
                 is_private=None,
                 *args, **kw):

    if request.method == 'GET':
        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        is_observing = False
        redirected_from = None
        try:
            article = article_qs.get(**article_args)
            if notification is not None:
                is_observing = notification.is_observing(article, request.user)
        except ArticleClass.DoesNotExist:
            try:
                # try to find an article that once had this title
                article = ChangeSet.objects.filter(
                    old_title=title).order_by('-revision')[0].article
                redirected_from = title
                # if article is not None:
                #    return redirect(article, permanent=True)
            except IndexError:
                article = ArticleClass(**article_args)

        if revision is not None:
            changeset = get_object_or_404(
                article.changeset_set, revision=revision)
            article.content = changeset.get_content()

        template_params = {'article': article,
                           'revision': revision,
                           'redirected_from': redirected_from,
                           'allow_write': allow_write}

        if notification is not None:
            template_params.update({'is_observing': is_observing,
                                    'can_observe': True})

        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


@login_required
def edit_article(request, title,
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 ArticleClass=Article,  # to get the DoesNotExist exception
                 ArticleFormClass=ArticleForm,
                 template_name='edit.html',
                 template_dir='wiki',
                 extra_context=None,
                 check_membership=False,
                 is_member=None,
                 is_private=None,
                 *args, **kw):

    group = None
    article_args = {'title': title}
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        group_ct = get_ct(group)
        article_args.update({'content_type': group_ct,
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
        allow_write = has_write_perm(request.user, group, is_member)
    else:
        allow_read = allow_write = True

    if not allow_write:
        return HttpResponseForbidden()

    try:
        # Try to fetch an existing article
        article = article_qs.get(**article_args)
    except ArticleClass.DoesNotExist:
        # No article found, maybe we have a redirect
        try:
            cs = ChangeSet.objects.filter(old_title=title)[0]
            article = article_qs.get(title=cs.article)
        except IndexError:
            # No Article found and no redirect found
            article = None

    if request.method == 'POST':

        form = ArticleFormClass(request.POST, instance=article)
        
        form.cache_old_content()
        if form.is_valid():

            if request.user.is_authenticated():
                form.editor = request.user

            if ((article is None) and (group_slug is not None)):
                form.group = group

            new_article, changeset = form.save()

            return redirect(new_article)

    elif request.method == 'GET':
        user_ip = get_real_ip(request)

        # NOCOMM FrankU: Never worked IMHO
        # lock = cache.get(title, None)
        # if lock is None:
        #     lock = ArticleEditLock(title, request)
        # lock.create_message(request)

        initial = {'user_ip': user_ip}
        if group_slug is not None:
            initial.update({'content_type': group_ct.id,
                            'object_id': group.id})

        if article is None:
            initial.update({'title': title,
                            'action': 'create'})
            form = ArticleFormClass(initial=initial)
        else:
            initial['action'] = 'edit'
            form = ArticleFormClass(instance=article,
                                    initial=initial)
    if not article:
        template_params = {'form': form, 'new_article': True}
    else:
        template_params = {'form': form, 'new_article': False,
                           'content_type': ContentType.objects.get_for_model(Article).pk, 'object_id': article.pk,
                           'images': article.all_images(),
                           'article': article,
                           }

    if group_slug is not None:
        template_params['group'] = group
    if extra_context is not None:
        template_params.update(extra_context)

    return render_to_response('/'.join([template_dir, template_name]),
                              template_params,
                              context_instance=RequestContext(request))


def view_changeset(request, title, revision,
                   revision_from=None,
                   group_slug=None, group_slug_field=None, group_qs=None,
                   article_qs=ALL_ARTICLES,
                   changes_qs=ALL_CHANGES,
                   template_name='changeset.html',
                   template_dir='wiki',
                   extra_context=None,
                   is_member=None,
                   is_private=None,
                   *args, **kw):

    if request.method == 'GET':
        article_args = {'article__title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'article__content_type': get_ct(group),
                                 'article__object_id': group.id})
        changeset = get_object_or_404(
            changes_qs,
            revision=int(revision),
            **article_args)

        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        article = article_qs.get(**article_args)

        if revision_from is None:
            revision_from = int(revision) - 1

        from_value = None
        if int(revision) is not int(revision_from) + 1:
            from_value = revision_from

        template_params = {'article': article,
                           'article_title': article.title,
                           'changeset': changeset,
                           'differences': changeset.compare_to(revision_from),
                           'from': from_value,
                           'to': revision,
                           'allow_write': allow_write}

        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


def article_history(request, title,
                    group_slug=None, group_slug_field=None, group_qs=None,
                    article_qs=ALL_ARTICLES,
                    template_name='history.html',
                    template_dir='wiki',
                    extra_context=None,
                    is_member=None,
                    is_private=None,
                    *args, **kw):

    if request.method == 'GET':

        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        article = get_object_or_404(article_qs, **article_args)
        # changes = article.changeset_set.filter(
        #    reverted=False).order_by('-revision')
        changes = article.changeset_set.all().order_by('-revision')

        template_params = {'article': article,
                           'changes': changes,
                           'allow_write': allow_write}
        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))

    return HttpResponseNotAllowed(['GET'])


@login_required
def revert_to_revision(request, title,
                       group_slug=None, group_slug_field=None, group_qs=None,
                       article_qs=ALL_ARTICLES,
                       extra_context=None,
                       is_member=None,
                       is_private=None,
                       *args, **kw):

    if request.method == 'POST':

        revision = int(request.POST['revision'])

        article_args = {'title': title}

        group = None
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not (allow_read or allow_write):
            return HttpResponseForbidden()

        article = get_object_or_404(article_qs, **article_args)

        
        # Check whether there is another Article with the same name to which this article
        # want's to be reverted to. If so: prevent it and show a message.
        old_title = article.changeset_set.filter(
            revision=revision+1).get().old_title
        try:
            art = Article.objects.exclude(pk=article.pk).get(title=old_title)
        except Article.DoesNotExist:
            # No existing article found -> reverting possible
            if request.user.is_authenticated():
                article.revert_to(revision, get_real_ip(request), request.user)
            else:
                article.revert_to(revision, get_real_ip(request))
            return redirect(article)
        # An article with this name exists
        messages.error(
            request, 'Reverting not possible because an article with name \'%s\' already exists' % old_title)
        return redirect(article)

    return HttpResponseNotAllowed(['POST'])


def history(request,
            group_slug=None, group_slug_field=None, group_qs=None,
            article_qs=ALL_ARTICLES, changes_qs=ALL_CHANGES,
            template_name='recentchanges.html',
            template_dir='wiki',
            extra_context=None,
            *args, **kw):

    if request.method == 'GET':
        if group_slug is not None:
            group = get_object_or_404(group_qs,
                                      **{group_slug_field: group_slug})
            changes_qs = changes_qs.filter(article__content_type=get_ct(group),
                                           article__object_id=group.id)
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        template_params = {'changes': changes_qs.order_by('-modified'),
                           'allow_write': allow_write}
        if group_slug is not None:
            template_params['group'] = group_slug

        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


@login_required
def observe_article(request, title,
                    group_slug=None, group_slug_field=None, group_qs=None,
                    article_qs=ALL_ARTICLES,
                    template_name='recentchanges.html',
                    template_dir='wiki',
                    extra_context=None,
                    is_member=None,
                    is_private=None,
                    *args, **kw):
    article_args = {'title': title}
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        article_args.update({'content_type': get_ct(group),
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
    else:
        allow_read = True

    if not allow_read:
        return HttpResponseForbidden()

    article = get_object_or_404(article_qs, **article_args)

    if not notification.is_observing(article, request.user):
        notification.observe(article, request.user,
                             'wiki_observed_article_changed')

    return redirect(article)

    return HttpResponseNotAllowed(['POST'])


@login_required
def stop_observing_article(request, title,
                           group_slug=None, group_slug_field=None, group_qs=None,
                           article_qs=ALL_ARTICLES,
                           template_name='recentchanges.html',
                           template_dir='wiki',
                           extra_context=None,
                           is_member=None,
                           is_private=None,
                           *args, **kw):
    article_args = {'title': title}
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        article_args.update({'content_type': get_ct(group),
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
    else:
        allow_read = True

    if not allow_read:
        return HttpResponseForbidden()

    article = get_object_or_404(article_qs, **article_args)

    if notification.is_observing(article, request.user):
        notification.stop_observing(article, request.user)

    return redirect(article)


def article_preview(request):
    """"""This is a AJAX function that previews the body of the article as it is
    currently displayed.

    This function is actually pretty simple, it just runs the function
    through the view template and returns it to the caller

    """"""
    rv = do_wl_markdown(request.POST['body'], 'bleachit')
    return HttpResponse(rv, content_type='text/html')


def article_diff(request):
    """"""This is a AJAX function that diffs the body of the article as it is
    currently displayed with the current version of the article.""""""
    current_article = get_object_or_404(
        Article, pk=int(request.POST['article']))
    content = request.POST['body']

    diffs = dmp.diff_main(current_article.content, content)
    dmp.diff_cleanupSemantic(diffs)

    return HttpResponse(dmp.diff_prettyHtml(diffs), content_type='text/html')


def backlinks(request, title):
    """"""Simple text search for links in other wiki articles pointing to the
    current article.

    If we convert WikiWords to markdown wikilinks syntax, this view
    should be changed to use '[[title]]' for searching.

    """"""

    # Find old title(s) of this article
    this_article = Article.objects.get(title=title)
    changesets = this_article.changeset_set.all()
    old_titles = []
    for cs in changesets:
        if cs.old_title and cs.old_title != title and cs.old_title not in old_titles:
            old_titles.append(cs.old_title)

    # Differentiate between WikiWords and other
    m = re.match(r""(!?)(\b[A-Z][a-z]+[A-Z]\w+\b)"", title)
    if m:
        # title is a 'WikiWord' -> This catches also 'MingW' but we have no such title
        search_title = re.compile(r""%s"" % title)
    else:
        # Others must be written like links: '[Wiki Page](/wiki/Wiki Page)'
        search_title = re.compile(r""\/%s\)"" % title)
    
    # Search for current and previous titles
    found_old_links = []
    found_links = []
    articles_all = Article.objects.all().exclude(title=title)
    for article in articles_all:
        match = search_title.search(article.content)
        if match:
            found_links.append({'title': article.title})
        
        for old_title in old_titles:
            if old_title in article.content:
                found_old_links.append({'old_title': old_title, 'title': article.title })

    context = {'found_links': found_links,
               'found_old_links': found_old_links,
               'name': title}
    return render_to_response('wiki/backlinks.html',
                              context,
                              context_instance=RequestContext(request))
/n/n/n",0
177,177,0c72e43a4863d07468a090cd4365c5505deb715b,"/wiki/forms.py/n/n# -*- coding: utf-8 -*-
import re

from django import forms
from django.contrib.contenttypes.models import ContentType
from django.utils.translation import ugettext_lazy as _

from wiki.models import Article
from wiki.models import ChangeSet
from wiki.templatetags.wiki_extras import WIKI_WORD_RE

wikiword_pattern = re.compile('^' + WIKI_WORD_RE + '$')


class ArticleForm(forms.ModelForm):

    summary = forms.CharField(widget=forms.Textarea)

    comment = forms.CharField(required=False)
    user_ip = forms.CharField(widget=forms.HiddenInput)

    content_type = forms.ModelChoiceField(
        queryset=ContentType.objects.all(),
        required=False,
        widget=forms.HiddenInput)
    object_id = forms.IntegerField(required=False,
                                   widget=forms.HiddenInput)

    action = forms.CharField(widget=forms.HiddenInput)

    class Meta:
        model = Article
        exclude = ('creator', 'creator_ip',
                   'group', 'created_at', 'last_update')

    def clean_title(self):
        """"""Check for some errors regarding the title:
            1. Check for bad characters
            2. Check for reserved titles

            Checking for existing articles is done by Django on Database level
            """"""
        title = self.cleaned_data['title']
        if not wikiword_pattern.match(title):
            raise forms.ValidationError(_('The title contain bad characters.'))

        cs = ChangeSet.objects.filter(old_title=title).count()
        if cs > 0:
            raise forms.ValidationError(
                _('The title %(title)s is reserved for redirects or old links.'), params={'title': title},)
        # No errors
        return title

    def clean(self):
        super(ArticleForm, self).clean()
        kw = {}

        if self.cleaned_data['action'] == 'create':
            try:
                kw['title'] = self.cleaned_data['title']
                kw['content_type'] = self.cleaned_data['content_type']
                kw['object_id'] = self.cleaned_data['object_id']
            except KeyError:
                pass  # some error in this fields

        return self.cleaned_data

    def cache_old_content(self):
        if self.instance.id is None:
            self.old_title = self.old_content = self.old_markup = ''
            self.is_new = True
        else:
            self.old_title = self.instance.title
            self.old_content = self.instance.content
            self.old_markup = self.instance.markup
            self.is_new = False

    def save(self, *args, **kwargs):
        # 0 - Extra data
        editor_ip = self.cleaned_data['user_ip']
        comment = self.cleaned_data['comment']

        # 2 - Save the Article
        article = super(ArticleForm, self).save(*args, **kwargs)

        # 3 - Set creator and group
        editor = getattr(self, 'editor', None)
        group = getattr(self, 'group', None)
        if self.is_new:
            article.creator_ip = editor_ip
            if editor is not None:
                article.creator = editor
                article.group = group
            article.save(*args, **kwargs)

        # 4 - Create new revision
        changeset = article.new_revision(
            self.old_content, self.old_title, self.old_markup,
            comment, editor_ip, editor)

        return article, changeset
/n/n/n/wiki/views.py/n/n# -*- coding: utf-8 -*-

from datetime import datetime

from django.conf import settings
from django.core.cache import cache
from django.template import RequestContext
from django.core.urlresolvers import reverse
from django.http import (Http404, HttpResponseRedirect,
                         HttpResponseNotAllowed, HttpResponse, HttpResponseForbidden)
from django.shortcuts import get_object_or_404, render_to_response, redirect
from django.contrib.contenttypes.models import ContentType
from django.contrib import messages
from django.core.exceptions import ObjectDoesNotExist
from wiki.forms import ArticleForm
from wiki.models import Article, ChangeSet, dmp

from wiki.utils import get_ct
from django.contrib.auth.decorators import login_required

from wl_utils import get_real_ip
import re

# Settings
#  lock duration in minutes
try:
    WIKI_LOCK_DURATION = settings.WIKI_LOCK_DURATION
except AttributeError:
    WIKI_LOCK_DURATION = 15

try:
    from notification import models as notification
except ImportError:
    notification = None

# default querysets
ALL_ARTICLES = Article.objects.all()
ALL_CHANGES = ChangeSet.objects.all()


def get_articles_by_group(article_qs, group_slug=None,
                          group_slug_field=None, group_qs=None):
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs,
                                  **{group_slug_field: group_slug})
        article_qs = article_qs.filter(content_type=get_ct(group),
                                       object_id=group.id)
    return article_qs, group


def get_articles_for_object(object, article_qs=None):
    if article_qs is None:
        article_qs = ALL_ARTICLES
    return article_qs.filter(content_type=get_ct(object),
                             object_id=object.id)


def get_url(urlname, group=None, args=None, kw=None):
    if group is None:
        return reverse(urlname, args=args)
    else:
        app = group._meta.app_label
        urlconf = '.'.join([app, 'urls'])
        url = reverse(urlname, urlconf, kwargs=kw)
        return ''.join(['/', app, url])  # @@@ harcoded: /app/.../

# NOCOMM Franku: This Class is currently not used
# If we want this it has to be checked for the changes
# related to django 1.8.
# A javascript alert box is maybe a better solution


class ArticleEditLock(object):
    """"""A soft lock to edting an article.""""""

    def __init__(self, title, request, message_template=None):
        self.title = title
        self.user_ip = get_real_ip(request)
        self.created_at = datetime.now()

        if message_template is None:
            message_template = ('Possible edit conflict:'
                                ' another user started editing this article at %s')

        self.message_template = message_template

        cache.set(title, self, WIKI_LOCK_DURATION * 60)

    def create_message(self, request):
        """"""Send a message to the user if there is another user editing this
        article.""""""
        if not self.is_mine(request):
            user = request.user
            user.message_set.create(
                message=self.message_template % self.created_at)

    def is_mine(self, request):
        return self.user_ip == get_real_ip(request)


def has_read_perm(user, group, is_member, is_private):
    """""" Return True if the user has permission to *read*
    Articles, False otherwise.
    """"""
    if (group is None) or (is_member is None) or is_member(user, group):
        return True
    if (is_private is not None) and is_private(group):
        return False
    return True


def has_write_perm(user, group, is_member):
    """"""Return True if the user have permission to edit Articles, False
    otherwise.""""""
    if (group is None) or (is_member is None) or is_member(user, group):
        return True
    return False


def article_list(request,
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 ArticleClass=Article,
                 template_name='index.html',
                 template_dir='wiki',
                 extra_context=None,
                 is_member=None,
                 is_private=None,
                 *args, **kw):
    if request.method == 'GET':
        articles, group = get_articles_by_group(
            article_qs, group_slug,
            group_slug_field, group_qs)

        allow_read = has_read_perm(request.user, group, is_member, is_private)
        allow_write = has_write_perm(request.user, group, is_member)

        if not allow_read:
            return HttpResponseForbidden()

        articles = articles.order_by('title')

        template_params = {'articles': articles,
                           'allow_write': allow_write}

        if group_slug is not None:
            template_params['group'] = group
            new_article = ArticleClass(title='NewArticle',
                                       content_type=get_ct(group),
                                       object_id=group.id)
        else:
            new_article = ArticleClass(title='NewArticle')
        template_params['new_article'] = new_article
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


def view_article(request, title, revision=None,
                 ArticleClass=Article,  # to create an unsaved instance
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 template_name='view.html',
                 template_dir='wiki',
                 extra_context=None,
                 is_member=None,
                 is_private=None,
                 *args, **kw):

    if request.method == 'GET':
        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        is_observing = False
        redirected_from = None
        try:
            article = article_qs.get(**article_args)
            if notification is not None:
                is_observing = notification.is_observing(article, request.user)
        except ArticleClass.DoesNotExist:
            try:
                # try to find an article that once had this title
                article = ChangeSet.objects.filter(
                    old_title=title).order_by('-revision')[0].article
                redirected_from = title
                # if article is not None:
                #    return redirect(article, permanent=True)
            except IndexError:
                article = ArticleClass(**article_args)

        if revision is not None:
            changeset = get_object_or_404(
                article.changeset_set, revision=revision)
            article.content = changeset.get_content()

        template_params = {'article': article,
                           'revision': revision,
                           'redirected_from': redirected_from,
                           'allow_write': allow_write}

        if notification is not None:
            template_params.update({'is_observing': is_observing,
                                    'can_observe': True})

        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


@login_required
def edit_article(request, title,
                 group_slug=None, group_slug_field=None, group_qs=None,
                 article_qs=ALL_ARTICLES,
                 ArticleClass=Article,  # to get the DoesNotExist exception
                 ArticleFormClass=ArticleForm,
                 template_name='edit.html',
                 template_dir='wiki',
                 extra_context=None,
                 check_membership=False,
                 is_member=None,
                 is_private=None,
                 *args, **kw):

    group = None
    article_args = {'title': title}
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        group_ct = get_ct(group)
        article_args.update({'content_type': group_ct,
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
        allow_write = has_write_perm(request.user, group, is_member)
    else:
        allow_read = allow_write = True

    if not allow_write:
        return HttpResponseForbidden()

    try:
        article = article_qs.get(**article_args)
    except ArticleClass.DoesNotExist:
        article = None

    if request.method == 'POST':

        form = ArticleFormClass(request.POST, instance=article)
        
        form.cache_old_content()
        if form.is_valid():

            if request.user.is_authenticated():
                form.editor = request.user

            if ((article is None) and (group_slug is not None)):
                form.group = group

            new_article, changeset = form.save()

            return redirect(new_article)

    elif request.method == 'GET':
        user_ip = get_real_ip(request)

        # NOCOMM FrankU: Never worked IMHO
        # lock = cache.get(title, None)
        # if lock is None:
        #     lock = ArticleEditLock(title, request)
        # lock.create_message(request)

        initial = {'user_ip': user_ip}
        if group_slug is not None:
            initial.update({'content_type': group_ct.id,
                            'object_id': group.id})

        if article is None:
            initial.update({'title': title,
                            'action': 'create'})
            form = ArticleFormClass(initial=initial)
        else:
            initial['action'] = 'edit'
            form = ArticleFormClass(instance=article,
                                    initial=initial)
    if not article:
        template_params = {'form': form, 'new_article': True}
    else:
        template_params = {'form': form, 'new_article': False,
                           'content_type': ContentType.objects.get_for_model(Article).pk, 'object_id': article.pk,
                           'images': article.all_images(),
                           'article': article,
                           }

    if group_slug is not None:
        template_params['group'] = group
    if extra_context is not None:
        template_params.update(extra_context)

    return render_to_response('/'.join([template_dir, template_name]),
                              template_params,
                              context_instance=RequestContext(request))


def view_changeset(request, title, revision,
                   revision_from=None,
                   group_slug=None, group_slug_field=None, group_qs=None,
                   article_qs=ALL_ARTICLES,
                   changes_qs=ALL_CHANGES,
                   template_name='changeset.html',
                   template_dir='wiki',
                   extra_context=None,
                   is_member=None,
                   is_private=None,
                   *args, **kw):

    if request.method == 'GET':
        article_args = {'article__title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'article__content_type': get_ct(group),
                                 'article__object_id': group.id})
        changeset = get_object_or_404(
            changes_qs,
            revision=int(revision),
            **article_args)

        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        article = article_qs.get(**article_args)

        if revision_from is None:
            revision_from = int(revision) - 1

        from_value = None
        if int(revision) is not int(revision_from) + 1:
            from_value = revision_from

        template_params = {'article': article,
                           'article_title': article.title,
                           'changeset': changeset,
                           'differences': changeset.compare_to(revision_from),
                           'from': from_value,
                           'to': revision,
                           'allow_write': allow_write}

        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


def article_history(request, title,
                    group_slug=None, group_slug_field=None, group_qs=None,
                    article_qs=ALL_ARTICLES,
                    template_name='history.html',
                    template_dir='wiki',
                    extra_context=None,
                    is_member=None,
                    is_private=None,
                    *args, **kw):

    if request.method == 'GET':

        article_args = {'title': title}
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        article = get_object_or_404(article_qs, **article_args)
        # changes = article.changeset_set.filter(
        #    reverted=False).order_by('-revision')
        changes = article.changeset_set.all().order_by('-revision')

        template_params = {'article': article,
                           'changes': changes,
                           'allow_write': allow_write}
        if group_slug is not None:
            template_params['group'] = group
        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))

    return HttpResponseNotAllowed(['GET'])


@login_required
def revert_to_revision(request, title,
                       group_slug=None, group_slug_field=None, group_qs=None,
                       article_qs=ALL_ARTICLES,
                       extra_context=None,
                       is_member=None,
                       is_private=None,
                       *args, **kw):

    if request.method == 'POST':

        revision = int(request.POST['revision'])

        article_args = {'title': title}

        group = None
        if group_slug is not None:
            group = get_object_or_404(
                group_qs, **{group_slug_field: group_slug})
            article_args.update({'content_type': get_ct(group),
                                 'object_id': group.id})
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not (allow_read or allow_write):
            return HttpResponseForbidden()

        article = get_object_or_404(article_qs, **article_args)

        
        # Check whether there is another Article with the same name to which this article
        # want's to be reverted to. If so: prevent it and show a message.
        old_title = article.changeset_set.filter(
            revision=revision+1).get().old_title
        try:
            art = Article.objects.exclude(pk=article.pk).get(title=old_title)
        except ObjectDoesNotExist:
            # No existing article found -> reverting possible
            if request.user.is_authenticated():
                article.revert_to(revision, get_real_ip(request), request.user)
            else:
                article.revert_to(revision, get_real_ip(request))
            return redirect(article)
        # An article with this name exists
        messages.error(
            request, 'Reverting not possible because an article with name \'%s\' already exists' % old_title)
        return redirect(article)

    return HttpResponseNotAllowed(['POST'])


def history(request,
            group_slug=None, group_slug_field=None, group_qs=None,
            article_qs=ALL_ARTICLES, changes_qs=ALL_CHANGES,
            template_name='recentchanges.html',
            template_dir='wiki',
            extra_context=None,
            *args, **kw):

    if request.method == 'GET':
        if group_slug is not None:
            group = get_object_or_404(group_qs,
                                      **{group_slug_field: group_slug})
            changes_qs = changes_qs.filter(article__content_type=get_ct(group),
                                           article__object_id=group.id)
            allow_read = has_read_perm(request.user, group, is_member,
                                       is_private)
            allow_write = has_write_perm(request.user, group, is_member)
        else:
            allow_read = allow_write = True

        if not allow_read:
            return HttpResponseForbidden()

        template_params = {'changes': changes_qs.order_by('-modified'),
                           'allow_write': allow_write}
        if group_slug is not None:
            template_params['group'] = group_slug

        if extra_context is not None:
            template_params.update(extra_context)

        return render_to_response('/'.join([template_dir, template_name]),
                                  template_params,
                                  context_instance=RequestContext(request))
    return HttpResponseNotAllowed(['GET'])


@login_required
def observe_article(request, title,
                    group_slug=None, group_slug_field=None, group_qs=None,
                    article_qs=ALL_ARTICLES,
                    template_name='recentchanges.html',
                    template_dir='wiki',
                    extra_context=None,
                    is_member=None,
                    is_private=None,
                    *args, **kw):
    article_args = {'title': title}
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        article_args.update({'content_type': get_ct(group),
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
    else:
        allow_read = True

    if not allow_read:
        return HttpResponseForbidden()

    article = get_object_or_404(article_qs, **article_args)

    if not notification.is_observing(article, request.user):
        notification.observe(article, request.user,
                             'wiki_observed_article_changed')

    return redirect(article)

    return HttpResponseNotAllowed(['POST'])


@login_required
def stop_observing_article(request, title,
                           group_slug=None, group_slug_field=None, group_qs=None,
                           article_qs=ALL_ARTICLES,
                           template_name='recentchanges.html',
                           template_dir='wiki',
                           extra_context=None,
                           is_member=None,
                           is_private=None,
                           *args, **kw):
    article_args = {'title': title}
    group = None
    if group_slug is not None:
        group = get_object_or_404(group_qs, **{group_slug_field: group_slug})
        article_args.update({'content_type': get_ct(group),
                             'object_id': group.id})
        allow_read = has_read_perm(request.user, group, is_member,
                                   is_private)
    else:
        allow_read = True

    if not allow_read:
        return HttpResponseForbidden()

    article = get_object_or_404(article_qs, **article_args)

    if notification.is_observing(article, request.user):
        notification.stop_observing(article, request.user)

    return redirect(article)


def article_preview(request):
    """"""This is a AJAX function that previews the body of the article as it is
    currently displayed.

    This function is actually pretty simple, it just runs the function
    through the view template and returns it to the caller

    """"""
    rv = do_wl_markdown(request.POST['body'], 'bleachit')
    return HttpResponse(rv, content_type='text/html')


def article_diff(request):
    """"""This is a AJAX function that diffs the body of the article as it is
    currently displayed with the current version of the article.""""""
    current_article = get_object_or_404(
        Article, pk=int(request.POST['article']))
    content = request.POST['body']

    diffs = dmp.diff_main(current_article.content, content)
    dmp.diff_cleanupSemantic(diffs)

    return HttpResponse(dmp.diff_prettyHtml(diffs), content_type='text/html')


def backlinks(request, title):
    """"""Simple text search for links in other wiki articles pointing to the
    current article.

    If we convert WikiWords to markdown wikilinks syntax, this view
    should be changed to use '[[title]]' for searching.

    """"""

    # Find old title(s) of this article
    this_article = Article.objects.get(title=title)
    changesets = this_article.changeset_set.all()
    old_titles = []
    for cs in changesets:
        if cs.old_title and cs.old_title != title and cs.old_title not in old_titles:
            old_titles.append(cs.old_title)

    # Differentiate between WikiWords and other
    m = re.match(r""(!?)(\b[A-Z][a-z]+[A-Z]\w+\b)"", title)
    if m:
        # title is a 'WikiWord' -> This catches also 'MingW' but we have no such title
        search_title = re.compile(r""%s"" % title)
    else:
        # Others must be written like links: '[Wiki Page](/wiki/Wiki Page)'
        search_title = re.compile(r""\/%s\)"" % title)
    
    # Search for current and previous titles
    found_old_links = []
    found_links = []
    articles_all = Article.objects.all().exclude(title=title)
    for article in articles_all:
        match = search_title.search(article.content)
        if match:
            found_links.append({'title': article.title})
        
        for old_title in old_titles:
            if old_title in article.content:
                found_old_links.append({'old_title': old_title, 'title': article.title })

    context = {'found_links': found_links,
               'found_old_links': found_old_links,
               'name': title}
    return render_to_response('wiki/backlinks.html',
                              context,
                              context_instance=RequestContext(request))
/n/n/n",1
102,102,cab3420d6d962f15e4f23bdb6a168bd0a583f7c2,"funding/urls.py/n/n##coding: utf-8
# Url handlers des projets
from django.conf.urls.defaults import *
from django.views.generic.list_detail import object_detail
from django.views.generic.simple import direct_to_template
from oi.projects.models import Project
from oi.helpers import SPEC_TYPES

urlpatterns = patterns('oi.funding.views',
    (r'^(?P<object_id>\d+)/$', object_detail, {'template_name': 'funding/project_detail.html', 'queryset': Project.objects.all(), 'extra_context': {'types': SPEC_TYPES}}),
    (r'^(?P<object_id>\d+)/edit$', object_detail, {'template_name': 'funding/edit_project.html', 'queryset': Project.objects.all(), 'extra_context': {'types': SPEC_TYPES}}),
    (r'^(?P<id>\d+)/task$', 'get_feature'),
    (r'^(?P<id>\d+)/editspec/(?P<specid>\d+)$', 'editspec'),
)
/n/n/nfunding/views.py/n/nfrom django.http import Http404
from django.views.generic.simple import direct_to_template
from django.views.generic.list_detail import object_detail
from oi.projects.models import Project, OINeedsPrjPerms, Spec
from oi.helpers import OI_READ, OI_WRITE, SPEC_TYPES


def get_feature(request, id):
    task = Project.objects.get(id=id)
    if not task.has_perm(request.user, OI_READ):
        raise Http404
    return direct_to_template(request, template=""funding/feature.html"", extra_context={'object': task.master, 'task': task})
    
@OINeedsPrjPerms(OI_WRITE)
def editspec(request, id, specid):
    """"""Edit template of a spec contains a spec details edit template""""""
    spec=None
    order = request.GET.get(""specorder"")
    if specid!='0':
        spec = Spec.objects.get(id=specid)
        if spec.project.id != int(id):
            return HttpResponse(_(""Wrong arguments""), status=531)
        order = spec.order
    extra_context = {'divid': request.GET[""divid""], 'spec':spec, 'types':SPEC_TYPES, 'specorder':order}
    return object_detail(request, queryset=Project.objects, object_id=id, template_object_name='project', template_name='funding/spec/editspec.html', extra_context=extra_context)
/n/n/n",0
103,103,cab3420d6d962f15e4f23bdb6a168bd0a583f7c2,"/funding/views.py/n/nfrom django.http import Http404
from django.views.generic.simple import direct_to_template
from oi.projects.models import Project, OINeedsPrjPerms
from oi.helpers import OI_READ


def get_feature(request, id):
    task = Project.objects.get(id=id)
    if not task.has_perm(request.user, OI_READ):
        raise Http404
    return direct_to_template(request, template=""funding/feature.html"", extra_context={'object': task.master, 'task': task})
/n/n/n",1
24,24,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
25,25,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
42,42,a433c4628ce8df859efef8cd3e1e411ccd91f690,"gateway_code/utils/cli/serial_redirection.py/n/n#! /usr/bin/env python
# -*- coding:utf-8 -*-

# This file is a part of IoT-LAB gateway_code
# Copyright (C) 2015 INRIA (Contact: admin@iot-lab.info)
# Contributor(s) : see AUTHORS file
#
# This software is governed by the CeCILL license under French law
# and abiding by the rules of distribution of free software.  You can  use,
# modify and/ or redistribute the software under the terms of the CeCILL
# license as circulated by CEA, CNRS and INRIA at the following URL
# http://www.cecill.info.
#
# As a counterpart to the access to the source code and  rights to copy,
# modify and redistribute granted by the license, users are provided only
# with a limited warranty  and the software's author,  the holder of the
# economic rights,  and the successive licensors  have only  limited
# liability.
#
# The fact that you are presently reading this means that you have had
# knowledge of the CeCILL license and that you accept its terms.

"""""" CLI client for serial_redirection """"""

import signal
import gateway_code.board_config as board_config
from . import log_to_stderr


def _get_node(board_cfg):
    if board_cfg.linux_on_class is not None:
        # Linux open node
        return board_cfg.linux_on_class()
    return board_cfg.board_class()


@log_to_stderr
def main():
    """""" serial_redirection cli main function """"""
    board_cfg = board_config.BoardConfig()
    node = _get_node(board_cfg)
    try:
        node.serial_redirection.start()
        print 'Press Ctrl+C to stop'
        signal.pause()
    except KeyboardInterrupt:
        pass
    finally:
        node.serial_redirection.stop()
        print 'Stopped'
/n/n/n",0
43,43,a433c4628ce8df859efef8cd3e1e411ccd91f690,"/gateway_code/utils/cli/serial_redirection.py/n/n#! /usr/bin/env python
# -*- coding:utf-8 -*-

# This file is a part of IoT-LAB gateway_code
# Copyright (C) 2015 INRIA (Contact: admin@iot-lab.info)
# Contributor(s) : see AUTHORS file
#
# This software is governed by the CeCILL license under French law
# and abiding by the rules of distribution of free software.  You can  use,
# modify and/ or redistribute the software under the terms of the CeCILL
# license as circulated by CEA, CNRS and INRIA at the following URL
# http://www.cecill.info.
#
# As a counterpart to the access to the source code and  rights to copy,
# modify and redistribute granted by the license, users are provided only
# with a limited warranty  and the software's author,  the holder of the
# economic rights,  and the successive licensors  have only  limited
# liability.
#
# The fact that you are presently reading this means that you have had
# knowledge of the CeCILL license and that you accept its terms.

"""""" CLI client for serial_redirection """"""

import signal
import gateway_code.board_config as board_config
from .. import serial_redirection
from . import log_to_stderr


def _get_node(board_cfg):
    if board_cfg.linux_on_class is not None:
        # Linux open node
        return board_cfg.linux_on_class()
    return board_cfg.board_class()


@log_to_stderr
def main():
    """""" serial_redirection cli main function """"""
    board_cfg = board_config.BoardConfig()
    node = _get_node(board_cfg)
    redirect = serial_redirection.SerialRedirection(node.TTY, node.BAUDRATE)
    try:
        redirect.start()
        print 'Press Ctrl+C to stop'
        signal.pause()
    except KeyboardInterrupt:
        pass
    finally:
        redirect.stop()
        print 'Stopped'
/n/n/n",1
138,138,2a9f6ef71b8e23fd267ee2be1be26dde8ab67037,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path)).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nfrom __future__ import unicode_literals

import mimetypes
import unittest
from os import path

from django.conf.urls.static import static
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, filename))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/non_existing_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super(StaticHelperTest, self).setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super(StaticHelperTest, self).tearDown()
        urls.urlpatterns = self._old_views_urlpatterns


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        Test that a floating point mtime does not disturb was_modified_since.
        (#18675)
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0
139,139,2a9f6ef71b8e23fd267ee2be1be26dde8ab67037,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path))
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1
158,158,0f5ca9ab33c6ec56d3c44dbb307dc52ab61c2a49,"ckanext/data_qld/commands.py/n/nimport ckan.model as model
import ckan.plugins.toolkit as toolkit
import sqlalchemy
from ckan.lib.cli import CkanCommand
from ckan.model.package import Package
from ckanapi import LocalCKAN
import ckan.logic as logic
ValidationError = logic.ValidationError

_and_ = sqlalchemy.and_


class MigrateExtras(CkanCommand):
    """"""Migrates legacy field values that were added as free extras to datasets to their schema counterparts.
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(MigrateExtras, self).__init__(name)

    def get_package_ids(self):
        session = model.Session
        package_ids = []

        packages = (
            session.query(
                Package
            )
        )

        for pkg in packages:
            package_ids.append(pkg.id)

        return package_ids

    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):
        # https://github.com/ckan/ckanext-scheming/issues/158
        destination = LocalCKAN()
        destination.action.package_patch(id=package_id,
                                         security_classification=security_classification,
                                         data_driven_application=data_driven_application,
                                         version=version,
                                         author_email=author_email,
                                         notes=notes,
                                         update_frequency=update_frequency,
                                         resources=resources)

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        context = {'session': model.Session}

        # Step 1: Get all the package IDs.
        package_ids = self.get_package_ids()

        for package_id in package_ids:
            # Set some defaults
            default_security_classification = ""PUBLIC""
            default_data_driven_application = ""NO""
            default_version = ""1.0""
            default_author_email = ""opendata@qld.gov.au""
            default_update_frequency = ""annually""
            default_size = '1'  # 1 Byte
            resources = []

            pkg = toolkit.get_action('package_show')(context, {
                'id': package_id
            })

            if pkg['resources']:
                size = default_size

                for resource in pkg['resources']:
                    if 'size' in resource:
                        size = resource['size'] if resource['size'] is not None and resource[
                            'size'] != '0 bytes' else default_size

                    if 'name' in resource:
                        name = resource['name']

                    if 'description' in resource:
                        description = resource['description'] or name

                    update_resource = {
                        ""id"": resource['id'],
                        ""size"": size,
                        ""name"": name,
                        ""description"": description,
                        ""url"": resource['url']
                    }
                    resources.append(update_resource)

            # Go through the packages and check for presence of 'Security classification'
            # and 'Used in data-driven application' extras
            security_classification = default_security_classification
            data_driven_application = default_data_driven_application
            version = default_version
            author_email = default_author_email
            update_frequency = default_update_frequency

            if pkg.get('extras', None):

                for extra in pkg['extras']:
                    if extra['key'] == 'Security classification':
                        security_classification = extra['value'] or default_security_classification
                    elif extra['key'] in ['Used in data-driven application']:
                        data_driven_application = extra['value'] or default_data_driven_application

            if 'version' in pkg:
                version = pkg['version'] or default_version

            if 'author_email' in pkg:
                author_email = pkg['author_email'] or default_author_email

            if 'notes' in pkg:
                notes = pkg['notes'] or pkg['title']

            if 'update_frequency' in pkg:
                update_frequency = pkg['update_frequency'] or default_update_frequency

            try:
                self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)
            except ValidationError as e:
                print ('Package Failed: ', package_id, '\n', e.error_dict, )
                print ('Package Payload: ', package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)

        return 'SUCCESS'


class DemotePublishers(CkanCommand):
    """"""Demotes any existing 'publisher-*' users from admin to editor in their respective organisations
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(DemotePublishers, self).__init__(name)
        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')

    def get_organizations(self):
        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})

    def patch_organisation_users(self, org_id, users):
        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        username_prefix = self.options.username_prefix

        updates = 0

        for org in self.get_organizations():
            print('- - - - - - - - - - - - - - - - - - - - - - - - -')
            updates_required = False
            users = org.get('users', [])
            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))
            if users:
                for user in org['users']:
                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':
                        print('- Setting capacity for user %s to ""editor"" in organisation %s' % (user['name'], org['name']))
                        user['capacity'] = 'editor'
                        updates_required = True
                        updates += 1
                if updates_required:
                    print('- Updating user capacities for organisation %s' % org['name'])
                    self.patch_organisation_users(org['id'], users)
                else:
                    print('- Nothing to update for organisation %s' % org['name'])

        print('- - - - - - - - - - - - - - - - - - - - - - - - -')

        return ""COMPLETED. Total updates %s\n"" % updates
/n/n/n",0
159,159,0f5ca9ab33c6ec56d3c44dbb307dc52ab61c2a49,"/ckanext/data_qld/commands.py/n/nimport ckan.model as model
import ckan.plugins.toolkit as toolkit
import sqlalchemy
from ckan.lib.cli import CkanCommand
from ckan.model.package import Package
from ckanapi import LocalCKAN

_and_ = sqlalchemy.and_


class MigrateExtras(CkanCommand):
    """"""Migrates legacy field values that were added as free extras to datasets to their schema counterparts.
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(MigrateExtras, self).__init__(name)

    def get_package_ids(self):
        session = model.Session
        package_ids = []

        packages = (
            session.query(
                Package
            )
        )

        for pkg in packages:
            package_ids.append(pkg.id)

        return package_ids

    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):
        # https://github.com/ckan/ckanext-scheming/issues/158
        destination = LocalCKAN()
        destination.action.package_patch(id=package_id,
                                         security_classification=security_classification,
                                         data_driven_application=data_driven_application,
                                         version=version,
                                         author_email=author_email,
                                         notes=notes,
                                         update_frequency=update_frequency,
                                         resources=resources)

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        context = {'session': model.Session}

        # Step 1: Get all the package IDs.
        package_ids = self.get_package_ids()

        for package_id in package_ids:
            # Set some defaults
            default_security_classification = ""PUBLIC""
            default_data_driven_application = ""NO""
            default_version = ""1.0""
            default_author_email = ""opendata@qld.gov.au""
            default_update_frequency = ""annually""
            default_size = '1'  # 1 Byte
            resources = []

            pkg = toolkit.get_action('package_show')(context, {
                'id': package_id
            })

            if pkg['resources']:
                size = default_size

                for resource in pkg['resources']:
                    if 'size' in resource:
                        size = resource['size'] if resource['size'] is not None and resource[
                            'size'] != '0 bytes' else default_size

                    if 'name' in resource:
                        name = resource['name']

                    if 'description' in resource:
                        description = resource['description'] or name

                    update_resource = {
                        ""id"": resource['id'],
                        ""size"": size,
                        ""name"": name,
                        ""description"": description
                    }
                    resources.append(update_resource)

            # Go through the packages and check for presence of 'Security classification'
            # and 'Used in data-driven application' extras
            security_classification = default_security_classification
            data_driven_application = default_data_driven_application
            version = default_version
            author_email = default_author_email
            update_frequency = default_update_frequency

            if pkg.get('extras', None):

                for extra in pkg['extras']:
                    if extra['key'] == 'Security classification':
                        security_classification = extra['value'] or default_security_classification
                    elif extra['key'] in ['Used in data-driven application']:
                        data_driven_application = extra['value'] or default_data_driven_application

            if 'version' in pkg:
                version = pkg['version'] or default_version

            if 'author_email' in pkg:
                author_email = pkg['author_email'] or default_author_email

            if 'notes' in pkg:
                notes = pkg['notes'] or pkg['title']

            if 'update_frequency' in pkg:
                update_frequency = pkg['update_frequency'] or default_update_frequency

            self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)

        return 'SUCCESS'


class DemotePublishers(CkanCommand):
    """"""Demotes any existing 'publisher-*' users from admin to editor in their respective organisations
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(DemotePublishers, self).__init__(name)
        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')

    def get_organizations(self):
        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})

    def patch_organisation_users(self, org_id, users):
        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        username_prefix = self.options.username_prefix

        updates = 0

        for org in self.get_organizations():
            print('- - - - - - - - - - - - - - - - - - - - - - - - -')
            updates_required = False
            users = org.get('users', [])
            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))
            if users:
                for user in org['users']:
                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':
                        print('- Setting capacity for user %s to ""editor"" in organisation %s' % (user['name'], org['name']))
                        user['capacity'] = 'editor'
                        updates_required = True
                        updates += 1
                if updates_required:
                    print('- Updating user capacities for organisation %s' % org['name'])
                    self.patch_organisation_users(org['id'], users)
                else:
                    print('- Nothing to update for organisation %s' % org['name'])

        print('- - - - - - - - - - - - - - - - - - - - - - - - -')

        return ""COMPLETED. Total updates %s\n"" % updates
/n/n/n",1
14,14,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
15,15,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
110,110,cb8af69ba6faa7a52b5e905d6c50648cb053fef5,"redirects/django_views.py/n/nfrom urllib.parse import urlencode, quote_plus

from django.http import HttpResponse, JsonResponse
from django.conf import settings

from experiences.factories import create_get_experience_interactor
from profiles.factories import create_get_profile_interactor

EMAIL_CONFIRMATION_PATH = '/people/me/email-confirmation'
LOGIN_PATH = '/people/me/login'
EXPERIENCE_PATH = '/e'
PROFILE_PATH = '/p'
EXPERIENCE_DEEPLINK_PATH = '/experiences'
PROFILE_DEEPLINK_PATH = '/profiles'


def email_confirmation_redirect(request):
    response = HttpResponse('', status=302)
    response['Location'] = '{}{}?{}'.format(settings.APP_DEEPLINK_DOMAIN,
                                            EMAIL_CONFIRMATION_PATH, request.GET.urlencode())
    return response


def login_redirect(request):
    response = HttpResponse('', status=302)
    response['Location'] = '{}{}?{}'.format(settings.APP_DEEPLINK_DOMAIN,
                                            LOGIN_PATH, request.GET.urlencode())
    return response


def experience_redirect(request, experience_share_id):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        real_link = '{}{}/{}'.format(settings.PUBLIC_DOMAIN, EXPERIENCE_PATH, experience_share_id)
        link = dynamic_link.format(real_link)

        get_experience_interactor = create_get_experience_interactor()
        experience = get_experience_interactor.set_params(experience_share_id=experience_share_id,
                                                          logged_person_id='-1') \
                                              .execute()
        desc = (experience.description[:77] + '...') if len(experience.description) > 77 else experience.description
        preview_content = {'st': experience.title, 'sd': desc, 'si': experience.picture.small_url}
        preview_encoded = urlencode(preview_content, quote_via=quote_plus)
        link = '{}&{}'.format(link, preview_encoded)
    else:
        link = '{}{}/{}'.format(settings.APP_DEEPLINK_DOMAIN, EXPERIENCE_DEEPLINK_PATH, experience_share_id)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def profile_redirect(request, username):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        real_link = '{}{}/{}'.format(settings.PUBLIC_DOMAIN, PROFILE_PATH, username)
        link = dynamic_link.format(real_link)

        get_profile_interactor = create_get_profile_interactor()
        profile = get_profile_interactor.set_params(username=username, logged_person_id='-1').execute()
        preview_content = {'st': '@{}'.format(profile.username), 'sd': profile.bio, 'si': profile.picture.small_url}
        preview_encoded = urlencode(preview_content, quote_via=quote_plus)
        link = '{}&{}'.format(link, preview_encoded)
    else:
        link = '{}{}/{}'.format(settings.APP_DEEPLINK_DOMAIN, PROFILE_DEEPLINK_PATH, username)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def open_redirect(request):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        link = dynamic_link.format('{}/open'.format(settings.PUBLIC_DOMAIN))
    else:
        link = '{}/open'.format(settings.APP_DEEPLINK_DOMAIN)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def aasa_redirect(request):
    return JsonResponse({""applinks"": {""apps"": [], ""details"": [{""appID"": settings.APPLE_APPID, ""paths"": [""*""]}]}})
/n/n/nredirects/tests/test_integration.py/n/nimport json

from django.conf import settings
from django.test import TestCase, Client
from django.urls import reverse

from experiences.models import ORMExperience
from people.models import ORMPerson
from profiles.models import ORMProfile


class RedirectConfirmEmailTestCase(TestCase):

    def test_when_called_redirect_view_redirects_to_apps_url(self):
        RedirectConfirmEmailTestCase.ScenarioMaker() \
                .when_call_get_email_confirmation() \
                .then_response_should_be_a_redirect_to_app_deeplink_with_params()

    class ScenarioMaker:

        def when_call_get_email_confirmation(self):
            client = Client()
            self.response = client.get('{}?{}'.format(reverse('email-confirmation-redirect'), 'token=ABXZ'))
            return self

        def then_response_should_be_a_redirect_to_app_deeplink_with_params(self):
            assert self.response.status_code == 302
            assert self.response['Location'] == '{}{}?token=ABXZ'.format(settings.APP_DEEPLINK_DOMAIN,
                                                                         '/people/me/email-confirmation')
            return self


class RedirectLoginEmailTestCase(TestCase):

    def test_when_called_redirect_view_redirects_to_apps_url(self):
        RedirectLoginEmailTestCase.ScenarioMaker() \
                .when_call_login_email_redirect() \
                .then_response_should_be_a_redirect_to_app_deeplink_with_params()

    class ScenarioMaker:

        def when_call_login_email_redirect(self):
            client = Client()
            self.response = client.get('{}?{}'.format(reverse('login-redirect'), 'token=ABXZ'))
            return self

        def then_response_should_be_a_redirect_to_app_deeplink_with_params(self):
            assert self.response.status_code == 302
            assert self.response['Location'] == '{}{}?token=ABXZ'.format(settings.APP_DEEPLINK_DOMAIN,
                                                                         '/people/me/login')
            return self


class RedirectExperienceTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectExperienceTestCase.ScenarioMaker() \
                .given_an_experience_on_db(title='a', description='d', share_id='AsdE43E4', pic='url') \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_experience_redirect('AsdE43E4') \
                .then_response_should_be_a_redirect_to(
                    'http://dynamic.link/link=http://pachatary.com/e/AsdE43E4&other=param'
                    '&st=a&sd=d&si=%2Fmedia%2Furl.small')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectExperienceTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_experience_redirect('AsdE43E4') \
                .then_response_should_be_a_redirect_to('pachatary://app/experiences/AsdE43E4')

    class ScenarioMaker:

        def given_an_experience_on_db(self, title, description, share_id, pic):
            orm_person = ORMPerson.objects.create()
            ORMProfile.objects.create(person=orm_person, username='u')
            experience = ORMExperience.objects.create(title=title, description=description,
                                                      share_id=share_id, author=orm_person)
            experience.picture = pic
            experience.save()
            return self

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_experience_redirect(self, share_id):
            client = Client()
            self.response = client.get(reverse('experience-redirect', args=[share_id]))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class RedirectProfileTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectProfileTestCase.ScenarioMaker() \
                .given_a_profile(username='a_b.c', bio='my info', pic='url') \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_profile_redirect('a_b.c') \
                .then_response_should_be_a_redirect_to(
                        'http://dynamic.link/link=http://pachatary.com/p/a_b.c&other=param'
                        '&st=%40a_b.c&sd=my+info&si=%2Fmedia%2Furl.small')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectProfileTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_profile_redirect('a_b.c') \
                .then_response_should_be_a_redirect_to('pachatary://app/profiles/a_b.c')

    class ScenarioMaker:

        def given_a_profile(self, username, bio, pic):
            orm_person = ORMPerson.objects.create()
            profile = ORMProfile.objects.create(username=username, bio=bio, person=orm_person)
            profile.picture = pic
            profile.save()
            return self

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_profile_redirect(self, username):
            client = Client()
            self.response = client.get(reverse('profile-redirect', args=[username]))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class RedirectOpenTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectOpenTestCase.ScenarioMaker() \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_open_redirect() \
                .then_response_should_be_a_redirect_to('http://dynamic.link/link=http://pachatary.com/open&other=param')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectOpenTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_open_redirect() \
                .then_response_should_be_a_redirect_to('pachatary://app/open')

    class ScenarioMaker:

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_open_redirect(self):
            client = Client()
            self.response = client.get(reverse('open-redirect'))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class AASATestCase(TestCase):

    def test_aasa_returns_json_with_appid(self):
        AASATestCase.ScenarioMaker() \
                .given_an_apple_appid('ASDF.com.myapp.ios') \
                .when_call_aasa() \
                .then_response_should_be_json(
                    '{""applinks"": {""apps"": [], ""details"": [{""appID"": ""ASDF.com.myapp.ios"", ""paths"": [""*""]}]}}')

    class ScenarioMaker:

        def given_an_apple_appid(self, appid):
            settings.APPLE_APPID = appid
            return self

        def when_call_aasa(self):
            client = Client()
            self.response = client.get(reverse('aasa'))
            return self

        def then_response_should_be_json(self, json_string):
            assert json.loads(self.response.content) == json.loads(json_string)
            return self
/n/n/nredirects/urls.py/n/nfrom django.conf.urls import url

from .django_views import email_confirmation_redirect, login_redirect, experience_redirect, profile_redirect, \
        open_redirect, aasa_redirect

urlpatterns = [
    url(r'^redirects/people/me/email-confirmation$',
        email_confirmation_redirect,
        name='email-confirmation-redirect'),

    url(r'^redirects/people/me/login$',
        login_redirect,
        name='login-redirect'),

    url(r'^e/(?P<experience_share_id>[a-zA-Z0-9]+)$',
        experience_redirect,
        name='experience-redirect'),

    url(r'^p/(?P<username>[a-zA-Z0-9._]+)$',
        profile_redirect,
        name='profile-redirect'),

    url(r'^apple-app-site-association$',
        aasa_redirect,
        name='aasa'),

    url(r'^open$',
        open_redirect,
        name='open-redirect'),
]
/n/n/n",0
111,111,cb8af69ba6faa7a52b5e905d6c50648cb053fef5,"/redirects/django_views.py/n/nfrom urllib.parse import urlencode, quote_plus

from django.http import HttpResponse, JsonResponse
from django.conf import settings

from experiences.factories import create_get_experience_interactor
from profiles.factories import create_get_profile_interactor

EMAIL_CONFIRMATION_PATH = '/people/me/email-confirmation'
LOGIN_PATH = '/people/me/login'
EXPERIENCE_PATH = '/e'
PROFILE_PATH = '/p'
EXPERIENCE_DEEPLINK_PATH = '/experiences'
PROFILE_DEEPLINK_PATH = '/profiles'


def email_confirmation_redirect(request):
    response = HttpResponse('', status=302)
    response['Location'] = '{}{}?{}'.format(settings.APP_DEEPLINK_DOMAIN,
                                            EMAIL_CONFIRMATION_PATH, request.GET.urlencode())
    return response


def login_redirect(request):
    response = HttpResponse('', status=302)
    response['Location'] = '{}{}?{}'.format(settings.APP_DEEPLINK_DOMAIN,
                                            LOGIN_PATH, request.GET.urlencode())
    return response


def experience_redirect(request, experience_share_id):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        real_link = '{}{}/{}'.format(settings.PUBLIC_DOMAIN, EXPERIENCE_PATH, experience_share_id)
        link = dynamic_link.format(real_link)

        get_experience_interactor = create_get_experience_interactor()
        experience = get_experience_interactor.set_params(experience_share_id=experience_share_id,
                                                          logged_person_id='-1') \
                                              .execute()
        desc = (experience.description[:77] + '...') if len(experience.description) > 77 else experience.description
        preview_content = {'st': experience.title, 'sd': desc, 'si': experience.picture.small_url}
        preview_encoded = urlencode(preview_content, quote_via=quote_plus)
        link = '{}&{}'.format(link, preview_encoded)
    else:
        link = '{}{}/{}'.format(settings.APP_DEEPLINK_DOMAIN, EXPERIENCE_DEEPLINK_PATH, experience_share_id)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def profile_redirect(request, username):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        real_link = '{}{}/{}'.format(settings.PUBLIC_DOMAIN, PROFILE_PATH, username)
        link = dynamic_link.format(real_link)

        get_profile_interactor = create_get_profile_interactor()
        profile = get_profile_interactor.set_params(username=username, logged_person_id='-1').execute()
        preview_content = {'st': '@{}'.format(profile.username), 'sd': profile.bio, 'si': profile.picture.small_url}
        preview_encoded = urlencode(preview_content, quote_via=quote_plus)
        link = '{}&{}'.format(link, preview_encoded)
    else:
        link = '{}{}/{}'.format(settings.APP_DEEPLINK_DOMAIN, PROFILE_DEEPLINK_PATH, username)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def root_redirect(request):
    dynamic_link = settings.DYNAMIC_LINK
    if len(dynamic_link) > 0:
        link = dynamic_link.format('{}/'.format(settings.PUBLIC_DOMAIN))
    else:
        link = '{}/'.format(settings.APP_DEEPLINK_DOMAIN)

    response = HttpResponse('', status=302)
    response['Location'] = link
    return response


def aasa_redirect(request):
    return JsonResponse({""applinks"": {""apps"": [], ""details"": [{""appID"": settings.APPLE_APPID, ""paths"": [""*""]}]}})
/n/n/n/redirects/tests/test_integration.py/n/nimport json

from django.conf import settings
from django.test import TestCase, Client
from django.urls import reverse

from experiences.models import ORMExperience
from people.models import ORMPerson
from profiles.models import ORMProfile


class RedirectConfirmEmailTestCase(TestCase):

    def test_when_called_redirect_view_redirects_to_apps_url(self):
        RedirectConfirmEmailTestCase.ScenarioMaker() \
                .when_call_get_email_confirmation() \
                .then_response_should_be_a_redirect_to_app_deeplink_with_params()

    class ScenarioMaker:

        def when_call_get_email_confirmation(self):
            client = Client()
            self.response = client.get('{}?{}'.format(reverse('email-confirmation-redirect'), 'token=ABXZ'))
            return self

        def then_response_should_be_a_redirect_to_app_deeplink_with_params(self):
            assert self.response.status_code == 302
            assert self.response['Location'] == '{}{}?token=ABXZ'.format(settings.APP_DEEPLINK_DOMAIN,
                                                                         '/people/me/email-confirmation')
            return self


class RedirectLoginEmailTestCase(TestCase):

    def test_when_called_redirect_view_redirects_to_apps_url(self):
        RedirectLoginEmailTestCase.ScenarioMaker() \
                .when_call_login_email_redirect() \
                .then_response_should_be_a_redirect_to_app_deeplink_with_params()

    class ScenarioMaker:

        def when_call_login_email_redirect(self):
            client = Client()
            self.response = client.get('{}?{}'.format(reverse('login-redirect'), 'token=ABXZ'))
            return self

        def then_response_should_be_a_redirect_to_app_deeplink_with_params(self):
            assert self.response.status_code == 302
            assert self.response['Location'] == '{}{}?token=ABXZ'.format(settings.APP_DEEPLINK_DOMAIN,
                                                                         '/people/me/login')
            return self


class RedirectExperienceTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectExperienceTestCase.ScenarioMaker() \
                .given_an_experience_on_db(title='a', description='d', share_id='AsdE43E4', pic='url') \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_experience_redirect('AsdE43E4') \
                .then_response_should_be_a_redirect_to(
                    'http://dynamic.link/link=http://pachatary.com/e/AsdE43E4&other=param'
                    '&st=a&sd=d&si=%2Fmedia%2Furl.small')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectExperienceTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_experience_redirect('AsdE43E4') \
                .then_response_should_be_a_redirect_to('pachatary://app/experiences/AsdE43E4')

    class ScenarioMaker:

        def given_an_experience_on_db(self, title, description, share_id, pic):
            orm_person = ORMPerson.objects.create()
            ORMProfile.objects.create(person=orm_person, username='u')
            experience = ORMExperience.objects.create(title=title, description=description,
                                                      share_id=share_id, author=orm_person)
            experience.picture = pic
            experience.save()
            return self

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_experience_redirect(self, share_id):
            client = Client()
            self.response = client.get(reverse('experience-redirect', args=[share_id]))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class RedirectProfileTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectProfileTestCase.ScenarioMaker() \
                .given_a_profile(username='a_b.c', bio='my info', pic='url') \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_profile_redirect('a_b.c') \
                .then_response_should_be_a_redirect_to(
                        'http://dynamic.link/link=http://pachatary.com/p/a_b.c&other=param'
                        '&st=%40a_b.c&sd=my+info&si=%2Fmedia%2Furl.small')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectProfileTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_profile_redirect('a_b.c') \
                .then_response_should_be_a_redirect_to('pachatary://app/profiles/a_b.c')

    class ScenarioMaker:

        def given_a_profile(self, username, bio, pic):
            orm_person = ORMPerson.objects.create()
            profile = ORMProfile.objects.create(username=username, bio=bio, person=orm_person)
            profile.picture = pic
            profile.save()
            return self

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_profile_redirect(self, username):
            client = Client()
            self.response = client.get(reverse('profile-redirect', args=[username]))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class RedirectRootTestCase(TestCase):

    def test_when_there_is_a_dynamic_link_wraps_public_domain_url(self):
        RedirectRootTestCase.ScenarioMaker() \
                .given_a_public_domain('http://pachatary.com') \
                .given_a_dynamic_link('http://dynamic.link/link={}&other=param') \
                .when_call_root_redirect() \
                .then_response_should_be_a_redirect_to('http://dynamic.link/link=http://pachatary.com/&other=param')

    def test_when_there_is_no_dynamic_link_returns_deep_link(self):
        RedirectRootTestCase.ScenarioMaker() \
                .given_a_deep_link_domain('pachatary://app') \
                .given_a_dynamic_link('') \
                .when_call_root_redirect() \
                .then_response_should_be_a_redirect_to('pachatary://app/')

    class ScenarioMaker:

        def given_a_public_domain(self, public_domain):
            settings.PUBLIC_DOMAIN = public_domain
            return self

        def given_a_dynamic_link(self, dynamic_link):
            settings.DYNAMIC_LINK = dynamic_link
            return self

        def given_a_deep_link_domain(self, deep_link_domain):
            settings.APP_DEEPLINK_DOMAIN = deep_link_domain
            return self

        def when_call_root_redirect(self):
            client = Client()
            self.response = client.get(reverse('root-redirect'))
            return self

        def then_response_should_be_a_redirect_to(self, url):
            assert self.response.status_code == 302
            assert self.response['Location'] == url
            return self


class AASATestCase(TestCase):

    def test_aasa_returns_json_with_appid(self):
        AASATestCase.ScenarioMaker() \
                .given_an_apple_appid('ASDF.com.myapp.ios') \
                .when_call_aasa() \
                .then_response_should_be_json(
                    '{""applinks"": {""apps"": [], ""details"": [{""appID"": ""ASDF.com.myapp.ios"", ""paths"": [""*""]}]}}')

    class ScenarioMaker:

        def given_an_apple_appid(self, appid):
            settings.APPLE_APPID = appid
            return self

        def when_call_aasa(self):
            client = Client()
            self.response = client.get(reverse('aasa'))
            return self

        def then_response_should_be_json(self, json_string):
            assert json.loads(self.response.content) == json.loads(json_string)
            return self
/n/n/n/redirects/urls.py/n/nfrom django.conf.urls import url

from .django_views import email_confirmation_redirect, login_redirect, experience_redirect, profile_redirect, \
        root_redirect, aasa_redirect

urlpatterns = [
    url(r'^redirects/people/me/email-confirmation$',
        email_confirmation_redirect,
        name='email-confirmation-redirect'),

    url(r'^redirects/people/me/login$',
        login_redirect,
        name='login-redirect'),

    url(r'^e/(?P<experience_share_id>[a-zA-Z0-9]+)$',
        experience_redirect,
        name='experience-redirect'),

    url(r'^p/(?P<username>[a-zA-Z0-9._]+)$',
        profile_redirect,
        name='profile-redirect'),

    url(r'^apple-app-site-association$',
        aasa_redirect,
        name='aasa'),

    url(r'^$',
        root_redirect,
        name='root-redirect'),
]
/n/n/n",1
170,170,acc1c96a5e1ae4c222a7ad20619cf9d417752dac,"posts/views.py/n/nfrom django.shortcuts import render, get_object_or_404

from .models import Post

# Create your views here.
def home(request):
    posts = Post.objects.order_by('-pub_date')
    return render(request, 'posts/home.html', {'posts':posts})

def post_details(request, post_id):
    post = get_object_or_404(Post, pk=post_id)
    return render(request, 'posts/posts_detail.html', {'post':post})
/n/n/n",0
171,171,acc1c96a5e1ae4c222a7ad20619cf9d417752dac,"/posts/views.py/n/nfrom django.shortcuts import render

from .models import Post

# Create your views here.
def home(request):
    posts = Post.objects.order_by('pub_date')
    return render(request, 'posts/home.html', {'posts':posts})

def post_details(request, post_id):
    return render(request, 'posts/posts_detail.html', {'post_id':post_id})
/n/n/n",1
92,92,adbba525936b5903192ef12b50c1281cbed12ba6,"src/runServer.py/n/nfrom flask import Flask, redirect, url_for, render_template, request, session, flash
from flask.ext.sqlalchemy import SQLAlchemy
from oauth import OAuthSignIn
from subprocess import check_output, STDOUT, CalledProcessError
from werkzeug import generate_password_hash, check_password_hash, secure_filename

from database.database_create import Base, User
from database.database_insert import insert_user, insert_social_user
from database.database_query import query_user,query_social_user, number_of_users

import base64
import json
import os
import shutil
import tempfile

import parser

DEBUG = False
app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret'
app.config['OAUTH_CREDENTIALS'] = {
    'facebook': {
        'id': '604820106335654',
        'secret': '5eb3f15f84c722df9cbc577206557cc8'
    },
    'twitter': {
        'id': 'cGFr2WV93py7an7FrGXXNDS6p',
        'secret': 'U9ufkrhicVHrj5CGojmQ7ZCxSwytoShSgM0t9WCq0HbqcfKwL8'
    }
}
app.secret_key = 'fe2917b485cc985c47071f3e38273348' # echo team paddy paddy | md5sum
app.config['UPLOAD_FOLDER'] = 'userFiles/'
app.config['ALLOWED_EXTENSIONS'] = set(['pml'])


def get_resource_as_string(name, charset='utf-8'):
    with app.open_resource(name) as f:
        return f.read().decode(charset)
app.jinja_env.globals['get_resource_as_string'] = get_resource_as_string

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']

@app.route(""/"", methods=[""POST""])
def my_form_post():
    with tempfile.NamedTemporaryFile(mode='w+t', suffix='.pml') as f:
        fname = f.name

        f.write(request.form[""program""])
        f.flush()

        try:
            return check_output([""./pmlcheck"", fname], stderr=STDOUT).decode().replace(fname+':', ""Line "")
        except CalledProcessError as e:
            return e.output.decode().replace(fname+':', ""Line ""), 400


@app.route(""/"")
def editor(filename = """"):
    editor_content = """";
    if session.get('tempFile') is not None:
        if session['tempFile'] != """":
            editor_content = open(session['tempFile']).read();

    if 'filename' in request.args or filename != """" or 'currentFile' in session:
        if not filename:
            if 'filename' in request.args:
                filename = request.args['filename']
            else:
                 filename = session['currentFile']
        if ('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
            session['currentFile'] = filename
            try:
                with open(filepath) as f:
                    editor_content = f.read()
            except FileNotFoundError:
                editor_content = """" #TODO: some kind of message here

    return render_template(""editor.html"", editor_content=editor_content)


@app.route('/openFile')
def openFile():
    if (not 'email' in session) and (not 'social' in session):
        if 'diagram' in request.args:
            return redirect('/login?return_url=openFile&diagram=true')
        return redirect('/login?return_url=openFile')

    files = []
    if 'email' in session:
        email = session['email']
    elif 'social' in session:
        email = session['social']
    userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
    try:
        files = os.listdir(userpath)
    except: # userpath doesn't exist yet; create it and assume empty
        os.makedirs(userpath, exist_ok=True)
    # print (""CURRENT file: "", session['currentFile'])
    return render_template('openFile.html', files=files)

@app.route('/upload', methods=['POST'])
def upload():
    if (not 'email' in session) and (not 'social' in session):
        return """", 401 # not authorised
    if 'email' in session:
        email = session['email']
    elif 'social' in session:
        email = session['social']
    file = request.files['file']
    filename = """"
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
        os.makedirs(userpath, exist_ok=True)
        file.save(os.path.join(userpath, filename))
        session['currentFile'] = filename
        if 'diagram' in request.referrer:
            return redirect('/diagram?filename=%s'%filename)
        return redirect('/?filename=%s'%filename)
    flash(""Invalid file"")
    return redirect('/openFile')


@app.route('/save')
def save():
    if (not 'email' in session) and (not 'social' in session):
        return redirect('/login?return_url=saveAs')
    if 'currentFile' in session:
        return saveFile(session['currentFile'])
    if 'diagram' in request.referrer:
        return saveAs(True)
    return saveAs()

@app.route('/saveAs')
def saveAs(diagram=False):
    if (not 'email' in session) and (not 'social' in session):
        if 'diagram' in request.args or diagram:
            return redirect('/login?return_url=saveAs&diagram=true')
        return redirect('/login?return_url=saveAs')
    else:
        return render_template('saveFile.html', diagram=diagram)

@app.route('/saveAs', methods=['POST'])
@app.route('/save', methods=['POST'])
def saveFile(fname=None):
    if (not 'email' in session) and (not 'social' in session):
        return """", 401 # not authorised

    name = fname if fname else request.form['filename']
    if name:
        if name[-4:] != "".pml"": # check for '.pml' extension
            name += "".pml""

        if allowed_file(name):
            session['currentFile'] = name
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            savepath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            os.makedirs(savepath, exist_ok=True) # make the users save dir if it doesn't already exist

            saveFilePath = os.path.join(savepath, name)
            tempFilePath = session.pop(""tempFile"", None)

            if tempFilePath:
                shutil.copy(tempFilePath, saveFilePath)
                if ""diagram"" in request.referrer or 'diagram' in request.args or 'diagram' in request.form:
                    return redirect('/diagram?filename=%s'%name)
                else:
                    return redirect('/?filename=%s'%name)

    flash(""Invalid File"")
    return redirect('/saveAs')

@app.route(""/diagram"")
def diagram():
    if 'filename' in request.args:
        filename = request.args['filename']
        if('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
            session['currentFile'] = filename
            try:
                with open(filepath) as f:
                    data = f.read()
                    parsed = parser.parse(data)
                    return render_template(""diagramEditor.html"", data=json.dumps(parsed))
            except parser.ParserException: pass
            except FileNotFoundError:
                editor_content = """"

    elif 'tempFile' in session or 'currentFile' in session:
        if 'tempFile' in session:
            filepath = session['tempFile']
        if 'currentFile' in session and ('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            filename = session['currentFile']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
        with open(filepath) as f:
            data = f.read()
            try:
                parsed = parser.parse(data) #TODO: proper error message
                return render_template(""diagramEditor.html"", data=json.dumps(parsed))
            except parser.ParserException: pass

    return render_template(""diagramEditor.html"")

@app.route(""/signup"")
def renderSignUp():
    if 'return_url' in request.args:
        session['return_url'] = request.args['return_url']

    return render_template(""register.html"")


@app.route(""/signup"", methods=[""POST""])
def signUpButton():
    email = request.form[""email""]
    user = query_user(email)
    if user == None:
        password = request.form[""password""]
        password_hash = generate_password_hash(password)
        insert_user(email, password_hash)
        session['email'] = email

        returnUrl = session.pop('return_url', None)
        if returnUrl:
            return redirect(returnUrl)
        else:
            return redirect('/')
    # email has been used
    flash('Email already in use')
    return redirect('/signup')



@app.route(""/login"")
def login():
    if 'return_url' in request.args:
        session['return_url'] = request.args['return_url']
    return render_template(""login.html"")


@app.route(""/login"", methods=[""POST""])
def loginButton():
    email = request.form[""email""]
    password = request.form[""password""]
    user = query_user(email)
    if user != None:
        if check_password_hash(user.password, password):
            session['email'] = email
            returnUrl = session.pop('return_url', None)
            if returnUrl:
                if 'diagram' in request.args:
                    return redirect(returnUrl + '?diagram=true')
                return redirect(returnUrl)
            else:
                return redirect('/')

    flash('Incorrect Email/Password')
    return redirect('/login')


@app.route(""/logout"")
def logout():
    session.clear()
    if 'return_url' in request.args:
        return redirect(request.args['return_url'])
    else:
        return redirect('/')

@app.route(""/tmp"", methods=[""POST""])
def tmp():
    with tempfile.NamedTemporaryFile(mode=""w+t"", delete=False) as f:
        content = base64.b64decode(request.form[""content""]).decode()
        f.write(content)
        session[""tempFile""] = f.name
        return """"

@app.route(""/resetCurrent"")
def resetCurrent():
    session.pop('currentFile', None)
    session.pop('tempFile', None)
    return """"
@app.route('/authorize/<provider>')
def oauth_authorize(provider):
    oauth = OAuthSignIn.get_provider(provider)
    return oauth.authorize()


@app.route('/callback/<provider>')
def oauth_callback(provider):
    oauth = OAuthSignIn.get_provider(provider)
    social, username, email = oauth.callback()
    if social is None:
        flash('Authentication failed.')
        return redirect(url_for('login'))
    user = query_social_user(social);
    session['social'] = social
    if user is None:
        insert_social_user(social)
    return redirect('/')

if __name__ == ""__main__"":
	app.run(host=""localhost"", port=8000, debug=DEBUG)
/n/n/n",0
93,93,adbba525936b5903192ef12b50c1281cbed12ba6,"/src/runServer.py/n/nfrom flask import Flask, redirect, url_for, render_template, request, session, flash
from flask.ext.sqlalchemy import SQLAlchemy
from oauth import OAuthSignIn
from subprocess import check_output, STDOUT, CalledProcessError
from werkzeug import generate_password_hash, check_password_hash, secure_filename

from database.database_create import Base, User
from database.database_insert import insert_user, insert_social_user
from database.database_query import query_user,query_social_user, number_of_users

import base64
import json
import os
import shutil
import tempfile

import parser

DEBUG = True
app = Flask(__name__)
app.config['SECRET_KEY'] = 'secret'
app.config['OAUTH_CREDENTIALS'] = {
    'facebook': {
        'id': '604820106335654',
        'secret': '5eb3f15f84c722df9cbc577206557cc8'
    },
    'twitter': {
        'id': 'cGFr2WV93py7an7FrGXXNDS6p',
        'secret': 'U9ufkrhicVHrj5CGojmQ7ZCxSwytoShSgM0t9WCq0HbqcfKwL8'
    }
}
app.secret_key = 'fe2917b485cc985c47071f3e38273348' # echo team paddy paddy | md5sum
app.config['UPLOAD_FOLDER'] = 'userFiles/'
app.config['ALLOWED_EXTENSIONS'] = set(['pml'])


def get_resource_as_string(name, charset='utf-8'):
    with app.open_resource(name) as f:
        return f.read().decode(charset)
app.jinja_env.globals['get_resource_as_string'] = get_resource_as_string

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']

@app.route(""/"", methods=[""POST""])
def my_form_post():
    with tempfile.NamedTemporaryFile(mode='w+t', suffix='.pml') as f:
        fname = f.name

        f.write(request.form[""program""])
        f.flush()

        try:
            return check_output([""./pmlcheck"", fname], stderr=STDOUT).decode().replace(fname+':', ""Line "")
        except CalledProcessError as e:
            return e.output.decode().replace(fname+':', ""Line ""), 400


@app.route(""/"")
def editor(filename = """"):
    editor_content = """";
    if session.get('tempFile') is not None:
        if session['tempFile'] != """":
            editor_content = open(session['tempFile']).read();

    if 'filename' in request.args or filename != """" or 'currentFile' in session:
        if not filename:
            if 'filename' in request.args:
                filename = request.args['filename']
            else:
                 filename = session['currentFile']
        if ('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
            session['currentFile'] = filename
            try:
                with open(filepath) as f:
                    editor_content = f.read()
            except FileNotFoundError:
                editor_content = """" #TODO: some kind of message here

    return render_template(""editor.html"", editor_content=editor_content)


@app.route('/openFile')
def openFile():
    if (not 'email' in session) and (not 'social' in session):
        if 'diagram' in request.args:
            return redirect('/login?return_url=openFile&diagram=true')
        return redirect('/login?return_url=openFile')

    files = []
    if 'email' in session:
        email = session['email']
    elif 'social' in session:
        email = session['social']
    userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
    try:
        files = os.listdir(userpath)
    except: # userpath doesn't exist yet; create it and assume empty
        os.makedirs(userpath, exist_ok=True)
    # print (""CURRENT file: "", session['currentFile'])
    return render_template('openFile.html', files=files)

@app.route('/upload', methods=['POST'])
def upload():
    if (not 'email' in session) and (not 'social' in session):
        return """", 401 # not authorised
    if 'email' in session:
        email = session['email']
    elif 'social' in session:
        email = session['social']
    file = request.files['file']
    filename = """"
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
        os.makedirs(userpath, exist_ok=True)
        file.save(os.path.join(userpath, filename))
        session['currentFile'] = filename
        if 'diagram' in request.referrer:
            return redirect('/diagram?filename=%s'%filename)
        return redirect('/?filename=%s'%filename)
    flash(""Invalid file"")
    return redirect('/openFile')


@app.route('/save')
def save():
    if (not 'email' in session) and (not 'social' in session):
        return redirect('/login?return_url=saveAs')
    if 'currentFile' in session:
        return saveFile(session['currentFile'])
    if 'diagram' in request.referrer:
        return saveAs(True)
    return saveAs()

@app.route('/saveAs')
def saveAs(diagram=False):
    if (not 'email' in session) and (not 'social' in session):
        if 'diagram' in request.args or diagram:
            return redirect('/login?return_url=saveAs&diagram=true')
        return redirect('/login?return_url=saveAs')
    else:
        return render_template('saveFile.html', diagram=diagram)

@app.route('/saveAs', methods=['POST'])
@app.route('/save', methods=['POST'])
def saveFile(fname=None):
    if (not 'email' in session) and (not 'social' in session):
        return """", 401 # not authorised

    name = fname if fname else request.form['filename']
    if name:
        if name[-4:] != "".pml"": # check for '.pml' extension
            name += "".pml""

        if allowed_file(name):
            session['currentFile'] = name
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            savepath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            os.makedirs(savepath, exist_ok=True) # make the users save dir if it doesn't already exist

            saveFilePath = os.path.join(savepath, name)
            tempFilePath = session.pop(""tempFile"", None)

            if tempFilePath:
                shutil.copy(tempFilePath, saveFilePath)
                if ""diagram"" in request.referrer or 'diagram' in request.args or 'diagram' in request.form:
                    return redirect('/diagram?filename=%s'%name)
                else:
                    return redirect('/?filename=%s'%name)

    flash(""Invalid File"")
    return redirect('/saveAs')

@app.route(""/diagram"")
def diagram():
    if 'filename' in request.args:
        filename = request.args['filename']
        if('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
            session['currentFile'] = filename
            try:
                with open(filepath) as f:
                    data = f.read()
                    parsed = parser.parse(data)
                    return render_template(""diagramEditor.html"", data=json.dumps(parsed))
            except parser.ParserException: pass
            except FileNotFoundError:
                editor_content = """"

    elif 'tempFile' in session or 'currentFile' in session:
        if 'tempFile' in session:
            filepath = session['tempFile']
        if 'currentFile' in session and ('email' in session) or ('social' in session):
            if 'email' in session:
                email = session['email']
            elif 'social' in session:
                email = session['social']
            filename = session['currentFile']
            userpath = os.path.join(app.config['UPLOAD_FOLDER'], email)
            filepath = os.path.join(userpath, filename)
        with open(filepath) as f:
            data = f.read()
            try:
                parsed = parser.parse(data) #TODO: proper error message
                return render_template(""diagramEditor.html"", data=json.dumps(parsed))
            except parser.ParserException: pass

    return render_template(""diagramEditor.html"")

@app.route(""/signup"")
def renderSignUp():
    if 'return_url' in request.args:
        session['return_url'] = request.args['return_url']

    return render_template(""register.html"")


@app.route(""/signup"", methods=[""POST""])
def signUpButton():
    email = request.form[""email""]
    user = query_user(email)
    if user == None:
        password = request.form[""password""]
        password_hash = generate_password_hash(password)
        insert_user(email, password_hash)
        session['email'] = email

        returnUrl = session.pop('return_url', None)
        if returnUrl:
            return redirect(returnUrl)
        else:
            return redirect('/')
    # email has been used
    flash('Email already in use')
    return redirect('/signup')



@app.route(""/login"")
def login():
    if 'return_url' in request.args:
        session['return_url'] = request.args['return_url']
    return render_template(""login.html"")


@app.route(""/login"", methods=[""POST""])
def loginButton():
    email = request.form[""email""]
    password = request.form[""password""]
    user = query_user(email)
    if user != None:
        if check_password_hash(user.password, password):
            session['email'] = email
            returnUrl = session.pop('return_url', None)
            if returnUrl:
                return redirect(returnUrl)
            else:
                return redirect('/')

    flash('Incorrect Email/Password')
    return redirect('/login')


@app.route(""/logout"")
def logout():
    session.clear()
    if 'return_url' in request.args:
        return redirect(request.args['return_url'])
    else:
        return redirect('/')

@app.route(""/tmp"", methods=[""POST""])
def tmp():
    with tempfile.NamedTemporaryFile(mode=""w+t"", delete=False) as f:
        content = base64.b64decode(request.form[""content""]).decode()
        f.write(content)
        session[""tempFile""] = f.name
        return """"

@app.route(""/resetCurrent"")
def resetCurrent():
    session.pop('currentFile', None)
    session.pop('tempFile', None)
    return """"
@app.route('/authorize/<provider>')
def oauth_authorize(provider):
    oauth = OAuthSignIn.get_provider(provider)
    return oauth.authorize()


@app.route('/callback/<provider>')
def oauth_callback(provider):
    oauth = OAuthSignIn.get_provider(provider)
    social, username, email = oauth.callback()
    if social is None:
        flash('Authentication failed.')
        return redirect(url_for('login'))
    user = query_social_user(social);
    session['social'] = social
    if user is None:
        insert_social_user(social)
    return redirect('/')

if __name__ == ""__main__"":
	app.run(host=""localhost"", port=8000, debug=DEBUG)
/n/n/n",1
52,52,7b5ff46c9c697c626b89b6246736c4117faef133,"main.py/n/n#Imports for all the packages
import os.path
import re
import motor.motor_tornado
import argon2
from pymongo import MongoClient
import random
import tornado.httpserver
import tornado.ioloop
import tornado.options
import tornado.web
import pymongo
from tornado.options import define, options

#Setting options for the server
define(""port"", default=8100, help=""run on the given port"", type=int)


class BaseHandler(tornado.web.RequestHandler):
	"""""" BaseHandler():
	Class that'll be used later when @tornado.web.authenticated is needed for POST requests.
	""""""
	def get_current_user(self):
		return self.get_secure_cookie(""user"")


class ErrorHandler(tornado.web.ErrorHandler):
	""""""
	Default handler gonna to be used in case of 404 error
	""""""
	def write_error(self, status_code, **kwargs):
		if status_code in [403, 404, 500, 503]:
			self.redirect(""/"")


class IndexHandler(tornado.web.RequestHandler):
	"""""" IndexHandler():
	Class that handles /
	""""""
	def get(self):
		self.render('index.html')


class SignUpHandler(tornado.web.RequestHandler):
	"""""" SignUpHandler():
	Class that handles /signup
	""""""
	def get(self):
		""""""	get():
		Renders the Sign Up page when the user arrives at /signup. 
		""""""
		self.render('signup.html',error='')
	
	def check_if_exists(self):
		"""""" check_if_exists():
		Uses the pymongo driver(so everything is synchronous) to check if the username exists in database
		then checks if the email address also exists in the database
		depending on conditions, returns None or the error message to be displayed.
		""""""
		error = None
		document_username = sync_db.users.find_one({'username':self.username})
		if (document_username!=None):
			error = ""Username exists already""
		document_email = sync_db.users.find_one({'email':self.email})
		if (document_email!=None):
			error = ""Email exists already""
		return error

	async def do_insert(self,hashed_password):
		"""""" do_insert():
		Forms a document of the username, the email, and the hashed password
		and using the Motor driver(asynchronously) inserts the document into database.
		""""""
		document = {'username': self.username,'email': self.email,'password': hashed_password}
		result = await async_db.users.insert_one(document)

	def hash_password(self):
		"""""" hash_password():
		Initializes an instance of argon2.PasswordHasher from argon2, hashes the password,
		verifies if the hashing happened properly, re-hashes if the verification failed,
		and then returns hashed password.
		""""""
		ph = argon2.PasswordHasher()
		hashed_password = ph.hash(self.password)
		try:
			ph.verify(hashed_password,self.password)
		except argon2.exceptions.VerifyMismatchError:
			hashed_password = ph.hash(self.password)
		return hashed_password

	async def post(self):
		"""""" post():
		Sets class variables, does rudimentary checks on username and email submitted using regex
		and renders signup.html with the error if the regex fails to match the submitted value.
		Then checks if the submitted username and email already exist in database by calling check_if_exists 
		if check_if_exists returns not None then renders signup.html with the error. 
		After confirming that no errors have occured, hashes the password and then inserts it into the
		MongoDB database by calling hash_password() and do_insert() respectively.
		Finally, sets the secure cookie and logs in the user.
		""""""
		self.username = self.get_argument(""username"").lower()
		self.email = self.get_argument(""email"").lower()
		self.password = self.get_argument(""psword"").lower()

		if (re.fullmatch('^(?=.{8,20}$)(?![_.])(?!.*[_.]{2})[a-zA-Z0-9._]+(?<![_.])$', self.username) == None): #Found at :https://stackoverflow.com/questions/12018245/regular-expression-to-validate-username
			self.render(""signup.html"",error=""Your username doesn't follow our username rules. Please fix it."")
			return
		elif (re.fullmatch(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)', self.email) == None): #Rudimentary Regex, will need to be updated to be simpler and email validation by sending an email will have to be done
			self.render(""signup.html"",error=""Your email doesn't look like a valid email"")
			return

		does_it_exist = self.check_if_exists()
		if(does_it_exist!=None):
			self.render(""signup.html"",error=does_it_exist)
			return

		hashed_password = self.hash_password()
		await self.do_insert(hashed_password)

		self.set_secure_cookie(""user"", self.username)
		self.redirect('/postlogin')
		return

class SignInHandler(tornado.web.RequestHandler):
	"""""" SignInHandler():
	Class that handles /signin
	""""""
	def get(self):
		"""""" get():
		Renders the Sign In page when the user arrives at /signin
		""""""
		self.render('signin.html',error='')

	def check_database(self):
		"""""" check_database():
		Creates an instance of argon2.PasswordHasher, finds if there is any document in the database with the 
		username submitted, verifies the password with the hashed password inside the database if the 
		document exists, returns None or the error message.
		""""""
		ph = argon2.PasswordHasher()
		error = None
		document_username = sync_db.users.find_one({'username':self.username})
		if(document_username == None):
			error = ""User doesn't exist. Please sign up first!""
		else:
			try:
				ph.verify(document_username['password'],self.password)
			except argon2.exceptions.VerifyMismatchError:
				error = ""Password is wrong, try again!""
		return error			

	def post(self):
		"""""" post():
		Sets the class variables and checks the database to verify if the credentials exist and
		are valid, renders the Sign In page with the error if they don't.
		Finally, sets the secure cookie and redirects to /postlogin.
		""""""
		self.username = self.get_argument(""username"").lower()
		self.password = self.get_argument(""psword"").lower()

		check_details = self.check_database()
		if(check_details!=None):
			self.render('signin.html',error=check_details)
			return

		self.set_secure_cookie(""user"", self.username)
		self.redirect('/postlogin')
		return

class PostLoginHandler(BaseHandler):
	"""""" PostLoginHandler():
	Class that handles /postlogin
	""""""
	@tornado.web.authenticated
	def get(self):
		"""""" get():
		Renders the postlogin page, uses the decorator to make sure the user is logged in first.
		""""""
		self.render('postlogin.html',error='')
		return

class CreatePollHandler(BaseHandler):
	"""""" CreatePollHandler():
	Class that handles /createpoll
	""""""
	@tornado.web.authenticated
	def get(self):
		"""""" get():
		Renders the createpoll page. uses the decorator to make sure the user is logged in first.
		""""""
		self.render('createpoll.html',error='')
		return

class ExistingPollsHandler(BaseHandler):
	"""""" ExistingPollsHandler():
	Class that handles /existingpolls
	""""""
	@tornado.web.authenticated
	def get(self):
		"""""" get():
		Renders the existingpolls page. uses the decorator to make sure the user is logged in first.
		""""""
		self.render('existingpolls.html',error='')
		return

class LogoutHandler(tornado.web.RequestHandler):
	"""""" LogoutHandler():
	Class that handles /logout
	""""""
	@tornado.web.authenticated
	def get(self):
		"""""" get():
		Cleans out the secure cookie, but only after checking that the user is logged in first
		so as to not throw any errors. Also redirects to home page.
		""""""
		self.clear_cookie(""user"")
		self.redirect(""/"")

# ---------------------MODULES BEGIN---------------------

class CDNIncludesModule(tornado.web.UIModule):
	"""""" CDNIncludesModule():
	Class that has the CDN includes statements which are included in every page,
	except it's easier when it's made into a module.
	""""""
	def render(self):
		"""""" render():
		Renders the module as a HTML string.
		""""""
		return self.render_string('modules/CDN_includes.html')

class NavbarModule(tornado.web.UIModule):
	"""""" NavbarModule():
	Class that has the Navbar code, put into a module for easier integration.
	""""""
	def render(self):
		"""""" render():
		Renders the navbar code as an HTML string.
		""""""
		return self.render_string('modules/navbar.html')

# ---------------------MODULES END---------------------


#---------------------MAIN BEGINS---------------------
if __name__ == '__main__':
	tornado.options.parse_command_line() 
	settings = {
		""cookie_secret"": ""j84i6ykTfmew9As25eYqAbs5KIhrUv/gmp801s9zRo="",
		""xsrf_cookies"":True, 
		""login_url"": ""/index"",
		""default_handler_class"": ErrorHandler, #Error Handler in case of 404s
		""default_handler_args"": dict(status_code=404) #Argument that needs to be passed if 404 page is hit
	}
	async_db = motor.motor_tornado.MotorClient().example #Asynchronous DB driver  
	sync_db = MongoClient().example 					 #Synchronous DB driver

	application = tornado.web.Application(
		handlers = [
			(r'/',IndexHandler),
			(r'/signup', SignUpHandler),
			(r'/signin', SignInHandler),
			(r'/postlogin',PostLoginHandler),
			(r'/createpoll',CreatePollHandler),
			(r'/existingpolls',ExistingPollsHandler),
			(r'/logout', LogoutHandler)
		],
		template_path = os.path.join(os.path.dirname(__file__),""templates""),
		static_path = os.path.join(os.path.dirname(__file__),""static""),
		ui_modules={'cdn_includes': CDNIncludesModule, 'navbar':NavbarModule},
		debug = True,
		async_db = async_db,
		sync_db = sync_db,

		**settings
	)
	http_server = tornado.httpserver.HTTPServer(application)
	http_server.listen(options.port)
	tornado.ioloop.IOLoop.instance().start()

#---------------------MAIN ENDS---------------------/n/n/n",0
53,53,7b5ff46c9c697c626b89b6246736c4117faef133,"/main.py/n/n#Imports for all the packages
import os.path
import re
import motor.motor_tornado
from argon2 import PasswordHasher
from pymongo import MongoClient
import random
import tornado.httpserver
import tornado.ioloop
import tornado.options
import tornado.web
import pymongo
from tornado.options import define, options

#Setting options for the server
define(""port"", default=8100, help=""run on the given port"", type=int)

"""""" BaseHandler():
Class that'll be used later when @tornado.web.authenticated is needed for POST requests.
""""""

class BaseHandler(tornado.web.RequestHandler):
	def get_current_user(self):
		return self.get_secure_cookie(""user"")

"""""" SignUpHandler():
Class that handles /signup
""""""

class SignUpHandler(tornado.web.RequestHandler):
	""""""	get():
	Renders the Sign Up page when the user arrives at /signup. 
	""""""
	def get(self):
		self.render('signup.html',error='')
	
	"""""" check_if_exists():
	Uses the pymongo driver(so everything is synchronous) to check if the username exists in database
	then checks if the email address also exists in the database
	depending on conditions, returns None or the error message to be displayed.
	""""""
	def check_if_exists(self):
		error = None
		document_username = sync_db.users.find_one({'username':self.username})
		if (document_username!=None):
			error = ""Username exists already""
		document_email = sync_db.users.find_one({'email':self.email})
		if (document_email!=None):
			error = ""Email exists already""
		return error

	"""""" do_insert():
	Forms a document of the username, the email, and the hashed password
	and using the Motor driver(asynchronously) inserts the document into database.
	""""""
	async def do_insert(self,hashed_password):
		document = {'username': self.username,'email': self.email,'password': hashed_password}
		result = await async_db.users.insert_one(document)

	"""""" hash_password():
	Initializes an instance of PasswordHasher from argon2, hashes the password,
	verifies if the hashing happened properly, re-hashes if the verification failed,
	and then returns hashed password.
	""""""
	def hash_password(self):
		ph = PasswordHasher()
		hashed_password = ph.hash(self.password)
		try:
			ph.verify(hashed_password,self.password)
		except VerifyMismatchError:
			hashed_password = ph.hash(self.password)
		return hashed_password

	"""""" post():
	Sets class variables, does rudimentary checks on username and email submitted using regex
	and renders signup.html with the error if the regex fails to match the submitted value.
	Then checks if the submitted username and email already exist in database by calling check_if_exists 
	if check_if_exists returns not None then renders signup.html with the error. 
	After confirming that no errors have occured, hashes the password and then inserts it into the
	MongoDB database by calling hash_password() and do_insert() respectively.
	Finally, sets the secure cookie and logs in the user.
	""""""
	async def post(self):
		self.username = self.get_argument(""username"").lower()
		self.email = self.get_argument(""email"").lower()
		self.password = self.get_argument(""psword"").lower()

		if (re.fullmatch('^(?=.{8,20}$)(?![_.])(?!.*[_.]{2})[a-zA-Z0-9._]+(?<![_.])$', self.username) == None): #Found at :https://stackoverflow.com/questions/12018245/regular-expression-to-validate-username
			self.render(""signup.html"",error=""Your username doesn't follow our username rules. Please fix it."")
			return
		elif (re.fullmatch(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)', self.email) == None): #Rudimentary Regex, will need to be updated to be simpler and email validation by sending an email will have to be done
			self.render(""signup.html"",error=""Your email doesn't look like a valid email"")
			return

		does_it_exist = self.check_if_exists()
		if(does_it_exist!=None):
			self.render(""signup.html"",error=does_it_exist)
			return

		hashed_password = self.hash_password()
		await self.do_insert(hashed_password)

		self.set_secure_cookie(""user"", self.username)
		self.redirect('/postlogin')
		return

"""""" SignInHandler():
Class that handles /signin
""""""

class SignInHandler(tornado.web.RequestHandler):
	"""""" get():
	Renders the Sign In page when the user arrives at /signin
	""""""
	def get(self):
		self.render('signin.html',error='')

	"""""" check_database():
	Creates an instance of PasswordHasher, finds if there is any document in the database with the 
	username submitted, verifies the password with the hashed password inside the database if the 
	document exists, returns None or the error message.
	""""""
	def check_database(self):
		ph = PasswordHasher()
		error = None
		document_username = sync_db.users.find_one({'username':self.username})
		if(document_username == None):
			error = ""User doesn't exist. Please sign up first!""
		elif(ph.verify(document_username['password'],self.password)==False):
			error = ""Password is wrong, try again!""
		return error			

	"""""" post():
	Sets the class variables and checks the database to verify if the credentials exist and
	are valid, renders the Sign In page with the error if they don't.
	Finally, sets the secure cookie and redirects to /postlogin.
	""""""
	def post(self):
		self.username = self.get_argument(""username"").lower()
		self.password = self.get_argument(""psword"").lower()

		check_details = self.check_database()
		if(check_details!=None):
			self.render('signin.html',error=check_details)
			return

		self.set_secure_cookie(""user"", self.username)
		self.redirect('/postlogin')
		return

"""""" IndexHandler():
Class that handles /
""""""

class IndexHandler(tornado.web.RequestHandler):
	def get(self):
		self.render('index.html')

"""""" PostLoginHandler():
Class that handles /postlogin
""""""

class PostLoginHandler(tornado.web.RequestHandler):
	"""""" get():
	Checks if a secure_cookie exists, if it doesn't then it redirects the user to /,
	else it renders /postlogin.
	""""""
	def get(self):
		cookie_status = self.get_secure_cookie(""user"")
		if(cookie_status==None):
			self.render('index.html')
			return
		else:
			self.render('postlogin.html')
			return

"""""" BootstrapModule():
Class that has the bootstrap includes statements which are included in every page,
except it's easier when it's made into a module.
""""""

class BootstrapModule(tornado.web.UIModule):
	def render(self):
		return self.render_string('modules/bootstrap_include.html')

if __name__ == '__main__':
	tornado.options.parse_command_line() 
	settings = {
		""cookie_secret"": ""j84i6ykTfmew9As25eYqAbs5KIhrUv/gmp801s9zRo="",
		""xsrf_cookies"":True, 
		""login_url"": ""/signin"",
	}
	async_db = motor.motor_tornado.MotorClient().example #Asynchronous DB driver  
	sync_db = MongoClient().example 					 #Synchronous DB driver

	application = tornado.web.Application(
		handlers = [
			(r'/',IndexHandler),
			(r'/signup', SignUpHandler),
			(r'/signin', SignInHandler),
			(r'/postlogin',PostLoginHandler)
		],
		template_path = os.path.join(os.path.dirname(__file__),""templates""),
		static_path = os.path.join(os.path.dirname(__file__),""static""),
		ui_modules={'bootstrap': BootstrapModule},
		debug = True,
		async_db = async_db,
		sync_db = sync_db,

		**settings
	)
	http_server = tornado.httpserver.HTTPServer(application)
	http_server.listen(options.port)
	tornado.ioloop.IOLoop.instance().start()
/n/n/n",1
94,94,a719ecb0238990f6dd9c4c9f3f8d3f5efa32122b,"ci_build.py/n/nfrom optparse import OptionParser
from dependencies import read_dependencies_from_filename
import os
import platform
import threading
import sys
import subprocess
import shutil
import time
import ctypes
import datetime

VERSION = 5

DEFAULT_STEPS = ""default""
ALL_STEPS = ""all""
ILLEGAL_STEP_NAMES = [DEFAULT_STEPS, ALL_STEPS]

class BaseUserLock(object):
    def __init__(self, filename):
        self.filename = filename
        self.locktime = None
    def __enter__(self):
        dirname = os.path.split(os.path.abspath(self.filename))[0]
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        while True:
            if self.tryacquire(self.filename):
                break
            print ""Lockfile ""+self.filename+"" not available.""
            print ""Wait 30s...""
            time.sleep(30.0)
        self.locktime = datetime.datetime.now()
        print ""Lock acquired at ""+str(self.locktime)
    def __exit__(self, etype, einstance, etraceback):
        self.release()
        unlocktime = datetime.datetime.now()
        print ""Lock released at ""+str(unlocktime)
        print ""Lock was held for ""+str(unlocktime - self.locktime)

class WindowsUserLock(BaseUserLock):
    def __init__(self, name):
        BaseUserLock.__init__(self, os.environ[""APPDATA""]+""\\openhome-build\\""+name+"".lock"")
    def tryacquire(self, filename):
        self.handle = ctypes.windll.kernel32.CreateFileA(filename,7,0,0,2,0x04000100,0)
        return self.handle != -1
    def release(self):
        ctypes.windll.kernel32.CloseHandle(self.handle)

class PosixUserLock(BaseUserLock):
    def __init__(self, name):
        BaseUserLock.__init__(self, os.environ[""HOME""]+""/.openhome-build/""+name+"".lock"")
    def tryacquire(self, filename):
        import fcntl
        self.f = file(filename, ""w"")
        try:
            fcntl.lockf(self.f, fcntl.LOCK_EX)
            return True
        except IOError:
            self.f.close()
            return False
    def release(self):
        self.f.close()

def userlock(name):
    '''
    Acquire a lock scoped to the local user. Only one build at a time can run
    with the given name per user per machine. While waiting for the lock, prints
    a notice to stdout every 30s.
    '''
    if platform.system() == 'Windows':
        return WindowsUserLock(name)
    return PosixUserLock(name)

def get_vsvars_environment():
    """"""
    Returns a dictionary containing the environment variables set up by vsvars32.bat

    win32-specific
    """"""
    vs100comntools = os.environ['VS100COMNTOOLS']
    if vs100comntools is None:
        raise Exception(""VS100COMNTOOLS is not set in environment."")
    vsvars32 = os.path.join(vs100comntools, 'vsvars32.bat')
    python = sys.executable
    process = subprocess.Popen('(""%s"">nul)&&""%s"" -c ""import os; print repr(os.environ)""' % (vsvars32, python), stdout=subprocess.PIPE, shell=True)
    stdout, _ = process.communicate()
    exitcode = process.wait()
    if exitcode != 0:
        raise Exception(""Got error code %s from subprocess!"" % exitcode)
    return eval(stdout.strip())

def default_platform():
    if platform.system() == 'Windows':
        return 'Windows-x86'
    if platform.system() == 'Linux' and platform.architecture()[0] == '32bit':
        return 'Linux-x86'
    if platform.system() == 'Linux' and platform.architecture()[0] == '64bit':
        return 'Linux-x64'
    return None

def delete_directory(path, logfile=None):
    if logfile is None:
        logfile = open(os.devnull, ""w"")
    path = os.path.abspath(path)
    logfile.write('Deleting ""'+path+'""... ')
    shutil.rmtree(path, ignore_errors=True)
    if os.path.isdir(path):
        logfile.write('\nFailed.\n')
        raise Exception('Failed to delete ""%s""' % path)
    logfile.write('\nDone.\n')

class BuildStep(object):
    def __init__(self, name, action):
        if name in ILLEGAL_STEP_NAMES:
            fail(""'{0}' is not allowed as a build step name."".format(name))
        self.name = name
        self.condition_sets = []
        self.is_optional = False
        self.is_enabled_by_default = True
        self.action = action
    def add_conditions(self, condition_set):
        self.condition_sets.append(condition_set)
    def set_default(self, enabled_by_default):
        self.is_enabled_by_default = enabled_by_default
    def set_optional(self, optional):
        self.is_optional = optional
    def test_conditions(self, env):
        if len(self.condition_sets) == 0:
            return True
        for conditions in self.condition_sets:
            if all(key in env and env[key]==value for (key, value) in conditions.items()):
                return True
        return False
    def run(self, context):
        return self.action(context)

class BuildContext(object):
    pass

def flatten_string_list(arglist):
    """"""
    Assemble a list of string, such as for a subprocess call.
    Input should be a string or a list containing only
    strings or similar lists.
    Output will be a list containing only strings.
    """"""
    if isinstance(arglist, (str, unicode)):
        return [arglist]
    return sum([flatten_string_list(x) for x in arglist], [])

def flatten_comma_list(arglist):
    return sum([s.split("","") for s in arglist], [])

def process_kwargs(func_name, kwarg_dict, defaults_dict):
    result = dict(defaults_dict)
    for key, value in kwarg_dict.items():
        if key in result:
            result[key] = value
        else:
            raise TypeError(""{0}() got an unexpected keyword argument '{1}'"".format(func_name, key))
    return result

class Builder(object):
    def __init__(self):
        self._steps = []
        self._optionParser = OptionParser()
        self.add_bool_option(""-v"", ""--verbose"")
        self._enabled_options = set()
        self._disabled_options = set()
        self._disable_all_options = False
        self._enable_all_options = False
        #self._context = BuildContext()
    def build_condition(self, name=None, **conditions):
        """"""Decorator applied to functions in the build_behaviour file.""""""
        def decorator_func(f):
            if not hasattr(f, ""buildstep""):
                f.buildstep = BuildStep(name or f.__name__, f)
                self._steps.append(f.buildstep)
            f.buildstep.add_conditions(conditions)
            return f
        return decorator_func
    def build_step(self, name=None, optional=False, default=True):
        def decorator_func(f):
            if not hasattr(f, ""buildstep""):
                f.buildstep = BuildStep(f.__name__, f)
                self._steps.append(f.buildstep)
            if name is not None:
                f.buildstep.name = name
            f.buildstep.set_optional(optional)
            f.buildstep.set_default(default)
            return f
        return decorator_func
    def get_optional_steps(self):
        return (step.name for step in self._steps if self.is_optional)
    def specify_optional_steps(self, *steps):
        '''
        Specify which optional steps to include in the build.
        ""default"" includes all default steps.
        ""all"" includes all steps.
        ""foo"" or ""+foo"" includes step foo.
        ""-foo"" excludes step foo, even if ""default"" or ""all"" is present.
        '''
        steps = flatten_string_list(steps)
        steps = flatten_comma_list(steps)
        self._enable_all_options = ALL_STEPS in steps
        #self._enable_default_options = DEFAULT_STEPS in steps
        self._disable_all_options = DEFAULT_STEPS not in steps
        self._disabled_options = set(s[1:] for s in steps if s.startswith(""-""))
        self._enabled_options = set(s[1:] for s in steps if s.startswith(""+""))
        self._enabled_options = self._enabled_options.union(
                s for s in steps if s[0] not in ""+-"")
    def modify_optional_steps(self, *steps):
        '''
        Add or remove optional steps in the build.
        ""+foo"" include step foo.
        ""-foo"" exclude step foo.
        '''
        for name in steps:
            if name.startswith(""+""):
                name = name[1:]
                self._disabled_options.discard(name)
                self._enabled_options.add(name)
            elif name.startswith(""-""):
                name = name[1:]
                self._enabled_options.discard(name)
                self._disabled_options.add(name)
            else:
                raise TypeError(""Each step must be a string beginning with '+' or '-'."")

    def select_optional_steps(self, *args, **kwargs):
        '''
        Deprecated. Use specify_optional_steps or modify_optional_steps instead.
        '''
        kwargs = process_kwargs(
            ""select_optional_steps"",
            kwargs,
            {""disable_others"":False})
        if kwargs[""disable_others""]:
            self._enabled_options.clear()
            self._disable_all_options = True
        args = flatten_string_list(args)
        args = flatten_comma_list(args)
        self.modify_optional_steps(*args)

    def run(self, argv=None):
        self._context = BuildContext()
        options, args = self._optionParser.parse_args(argv)
        self._context.options = options
        self._context.args = args
        self._context.env = dict(os.environ)
        for step in self._steps:
            if step.test_conditions(self._context.env):
                enabled = True
                if step.is_optional:
                    enabled = step.is_enabled_by_default
                    if self._disable_all_options:
                        enabled = False
                    if step.name in self._enabled_options:
                        enabled = True
                    if step.name in self._disabled_options:
                        enabled = False
                if enabled:
                    print step.name
                    step.run(self._context)
    def add_bool_option(self, *args, **kwargs):
        kwargs=dict(kwargs)
        kwargs[""default""] = False
        kwargs[""action""] = ""store_true""
        self.add_option(*args, **kwargs)
    def add_option(self, *args, **kwargs):
        self._optionParser.add_option(*args, **kwargs)

    def _check_call(self, *args, **kwargs):
        if self._context.options.verbose:
            argstring = ["", "".join([repr(arg) for arg in args])]
            kwargstring = ["", "".join([""%s=%r"" % (k,v) for (k,v) in kwargs.items()])]
            print ""subprocess.check_call(%s)"" % ("", "".join(argstring+kwargstring))
        subprocess.check_call(*args, **kwargs)

    def python(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call([sys.executable] + args, env=self._context.env, **kwargs)
    def shell(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call(args, env=self._context.env, shell=True, **kwargs)
    def rsync(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call([""rsync""] + args, **kwargs)
    def _dependency_collection(self):
        return read_dependencies_from_filename(os.path.join('projectdata', 'dependencies.txt'), logfile=sys.stdout)
    def fetch_dependencies(self, *dependencies, **kwargs):
        kwargs = process_kwargs(
            ""fetch_dependencies"",
            kwargs,
            {""platform"":None})
        dependencies = flatten_string_list(dependencies)
        platform = kwargs['platform'] or self._context.env[""PLATFORM""]
        dependency_collection = self._dependency_collection()
        delete_directory(os.path.join('dependencies', platform), logfile=sys.stdout)
        if len(dependencies) > 0:
            if not dependency_collection.fetch(dependencies, self._context.env):
                raise AbortRunException()
    def get_dependency_args(self, *dependencies):
        dependencies = flatten_string_list(dependencies)
        dependency_collection = self._dependency_collection()
        return dependency_collection.get_args(dependencies, self._context.env)

class SshSession(object):
    def __init__(self, host, username):
        import paramiko
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(host, username=username, look_for_keys='True')
    def call(self, *args, **kwargs):
        stdin, stdout, stderr = self.ssh.exec_command(*args, **kwargs)
        def pump_output_thread(source, destination):
            for line in source:
                destination.write(line)
                destination.flush()
        stdout_thread = threading.Thread(target=pump_output_thread, args=(stdout, sys.stdout))
        stderr_thread = threading.Thread(target=pump_output_thread, args=(stderr, sys.stderr))
        stdout_thread.start()
        stderr_thread.start()
        stdout_thread.join()
        stderr_thread.join()
        return stdout.channel.recv_exit_status()
    def __call__(self, *args):
        return self.call(*args)
    def __enter__(self):
        return self
    def __exit__(self, ex_type, ex_value, ex_traceback):
        self.ssh.close()

class AbortRunException(Exception):
    def __init__(self, message=""Aborted due to error."", exitcode=1):
        Exception.__init__(self, message)
        self.message = message
        self.exitcode = exitcode

def fail(*args, **kwargs):
    '''
    fail(message, exitcode=1)
    Abort the build with an error message.
    '''
    raise AbortRunException(*args, **kwargs)

def require_version(required_version):
    '''Fail if the version of ohDevTools is too old.'''
    if VERSION<required_version:
        fail(""This build requires a newer version of ohDevTools. You have version {0}, but need version {1}."".format(VERSION, required_version),32)

def windows_program_exists(program):
    return subprocess.call([""where"", ""/q"", program], shell=False)==0

def other_program_exists(program):
    nul = open(os.devnull, ""w"")
    return subprocess.call([""/bin/sh"", ""-c"", ""command -v ""+program], shell=False, stdout=nul, stderr=nul)==0

program_exists = windows_program_exists if platform.platform().startswith(""Windows"") else other_program_exists

def scp(*args):
    program = None
    for p in [""scp"", ""pscp""]:
        if program_exists(p):
            program = p
            break
    if program is None:
        raise ""Cannot find scp (or pscp) in the path.""
    subprocess.check_call([program] + list(args))

def run(buildname=""build"", argv=None):
    builder = Builder()
    behaviour_globals = {
            'fetch_dependencies':builder.fetch_dependencies,
            'get_dependency_args':builder.get_dependency_args,
            'add_option':builder.add_option,
            'add_bool_option':builder.add_bool_option,
            'python':builder.python,
            'shell':builder.shell,
            'rsync':builder.rsync,
            'build_step':builder.build_step,
            'build_condition':builder.build_condition,
            'default_platform':default_platform,
            'get_vsvars_environment':get_vsvars_environment,
            'SshSession':SshSession,
            'select_optional_steps':builder.select_optional_steps,
            'modify_optional_steps':builder.modify_optional_steps,
            'specify_optional_steps':builder.specify_optional_steps,
            'userlock':userlock,
            'fail':fail,
            'scp':scp,
            'require_version':require_version
        }
    try:
        execfile(os.path.join('projectdata', buildname+'_behaviour.py'), behaviour_globals)
        builder.run(argv)
    except AbortRunException as e:
        print e.message
        sys.exit(e.exitcode)
/n/n/n",0
95,95,a719ecb0238990f6dd9c4c9f3f8d3f5efa32122b,"/ci_build.py/n/nfrom optparse import OptionParser
from dependencies import read_dependencies_from_filename
import os
import platform
import threading
import sys
import subprocess
import shutil
import time
import ctypes
import datetime

VERSION = 5

DEFAULT_STEPS = ""default""
ALL_STEPS = ""all""
ILLEGAL_STEP_NAMES = [DEFAULT_STEPS, ALL_STEPS]

class BaseUserLock(object):
    def __init__(self, filename):
        self.filename = filename
        self.locktime = None
    def __enter__(self):
        dirname = os.path.split(os.path.abspath(self.filename))[0]
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        while True:
            if self.tryacquire(self.filename):
                break
            print ""Lockfile ""+self.filename+"" not available.""
            print ""Wait 30s...""
            time.sleep(30.0)
        self.locktime = datetime.datetime.now()
        print ""Lock acquired at ""+str(self.locktime)
    def __exit__(self, etype, einstance, etraceback):
        self.release()
        unlocktime = datetime.datetime.now()
        print ""Lock released at ""+str(unlocktime)
        print ""Lock was held for ""+str(unlocktime - self.locktime)

class WindowsUserLock(BaseUserLock):
    def __init__(self, name):
        BaseUserLock.__init__(self, os.environ[""APPDATA""]+""\\openhome-build\\""+name+"".lock"")
    def tryacquire(self, filename):
        self.handle = ctypes.windll.kernel32.CreateFileA(filename,7,0,0,2,0x04000100,0)
        return self.handle != -1
    def release(self):
        ctypes.windll.kernel32.CloseHandle(self.handle)

class PosixUserLock(BaseUserLock):
    def __init__(self, name):
        BaseUserLock.__init__(self, os.environ[""HOME""]+""/.openhome-build/""+name+"".lock"")
    def tryacquire(self, filename):
        import fcntl
        self.f = file(filename, ""w"")
        try:
            fcntl.lockf(self.f, fcntl.LOCK_EX)
            return True
        except IOError:
            self.f.close()
            return False
    def release(self):
        self.f.close()

def userlock(name):
    '''
    Acquire a lock scoped to the local user. Only one build at a time can run
    with the given name per user per machine. While waiting for the lock, prints
    a notice to stdout every 30s.
    '''
    if platform.system() == 'Windows':
        return WindowsUserLock(name)
    return PosixUserLock(name)

def get_vsvars_environment():
    """"""
    Returns a dictionary containing the environment variables set up by vsvars32.bat

    win32-specific
    """"""
    vs100comntools = os.environ['VS100COMNTOOLS']
    if vs100comntools is None:
        raise Exception(""VS100COMNTOOLS is not set in environment."")
    vsvars32 = os.path.join(vs100comntools, 'vsvars32.bat')
    python = sys.executable
    process = subprocess.Popen('(""%s"">nul)&&""%s"" -c ""import os; print repr(os.environ)""' % (vsvars32, python), stdout=subprocess.PIPE, shell=True)
    stdout, _ = process.communicate()
    exitcode = process.wait()
    if exitcode != 0:
        raise Exception(""Got error code %s from subprocess!"" % exitcode)
    return eval(stdout.strip())

def default_platform():
    if platform.system() == 'Windows':
        return 'Windows-x86'
    if platform.system() == 'Linux' and platform.architecture()[0] == '32bit':
        return 'Linux-x86'
    if platform.system() == 'Linux' and platform.architecture()[0] == '64bit':
        return 'Linux-x64'
    return None

def delete_directory(path, logfile=None):
    if logfile is None:
        logfile = open(os.devnull, ""w"")
    path = os.path.abspath(path)
    logfile.write('Deleting ""'+path+'""... ')
    shutil.rmtree(path, ignore_errors=True)
    if os.path.isdir(path):
        logfile.write('\nFailed.\n')
        raise Exception('Failed to delete ""%s""' % path)
    logfile.write('\nDone.\n')

class BuildStep(object):
    def __init__(self, name, action):
        if name in ILLEGAL_STEP_NAMES:
            fail(""'{0}' is not allowed as a build step name."".format(name))
        self.name = name
        self.condition_sets = []
        self.is_optional = False
        self.is_enabled_by_default = True
        self.action = action
    def add_conditions(self, condition_set):
        self.condition_sets.append(condition_set)
    def set_default(self, enabled_by_default):
        self.is_enabled_by_default = enabled_by_default
    def set_optional(self, optional):
        self.is_optional = optional
    def test_conditions(self, env):
        if len(self.condition_sets) == 0:
            return True
        for conditions in self.condition_sets:
            if all(key in env and env[key]==value for (key, value) in conditions.items()):
                return True
        return False
    def run(self, context):
        return self.action(context)

class BuildContext(object):
    pass

def flatten_string_list(arglist):
    """"""
    Assemble a list of string, such as for a subprocess call.
    Input should be a string or a list containing only
    strings or similar lists.
    Output will be a list containing only strings.
    """"""
    if isinstance(arglist, (str, unicode)):
        return [arglist]
    return sum([flatten_string_list(x) for x in arglist], [])

def flatten_comma_list(arglist):
    return sum([s.split("","") for s in arglist], [])

def process_kwargs(func_name, kwarg_dict, defaults_dict):
    result = dict(defaults_dict)
    for key, value in kwarg_dict.items():
        if key in result:
            result[key] = value
        else:
            raise TypeError(""{0}() got an unexpected keyword argument '{1}'"".format(func_name, key))
    return result

class Builder(object):
    def __init__(self):
        self._steps = []
        self._optionParser = OptionParser()
        self.add_bool_option(""-v"", ""--verbose"")
        self._enabled_options = set()
        self._disabled_options = set()
        self._disable_all_options = False
        self._enable_all_options = False
        #self._context = BuildContext()
    def build_condition(self, name=None, **conditions):
        """"""Decorator applied to functions in the build_behaviour file.""""""
        def decorator_func(f):
            if not hasattr(f, ""buildstep""):
                f.buildstep = BuildStep(name or f.__name__, f)
                self._steps.append(f.buildstep)
            f.buildstep.add_conditions(conditions)
            return f
        return decorator_func
    def build_step(self, name=None, optional=False, default=True):
        def decorator_func(f):
            if not hasattr(f, ""buildstep""):
                f.buildstep = BuildStep(f.__name__, f)
                self._steps.append(f.buildstep)
            if name is not None:
                f.buildstep.name = name
            f.buildstep.set_optional(optional)
            f.buildstep.set_default(default)
            return f
        return decorator_func
    def get_optional_steps(self):
        return (step.name for step in self._steps if self.is_optional)
    def specify_optional_steps(self, *steps):
        '''
        Specify which optional steps to include in the build.
        ""default"" includes all default steps.
        ""all"" includes all steps.
        ""foo"" or ""+foo"" includes step foo.
        ""-foo"" excludes step foo, even if ""default"" or ""all"" is present.
        '''
        steps = flatten_string_list(steps)
        steps = flatten_comma_list(steps)
        self._enable_all_options = ALL_STEPS in steps
        #self._enable_default_options = DEFAULT_STEPS in steps
        self._disable_all_options = DEFAULT_STEPS not in steps
        self._disabled_options = set(s[1:] for s in steps if s.startswith(""-""))
        self._enabled_options = set(s[1:] for s in steps if s.startswith(""+""))
        self._enabled_options = self._enabled_options.union(
                s for s in steps if s[0] not in ""+-"")
    def modify_optional_steps(self, *steps):
        '''
        Add or remove optional steps in the build.
        ""+foo"" include step foo.
        ""-foo"" exclude step foo.
        '''
        for name in steps:
            if name.startswith(""+""):
                name = name[1:]
                self._disabled_options.discard(name)
                self._enabled_options.add(name)
            elif name.startswith(""-""):
                name = name[1:]
                self._enabled_options.discard(name)
                self._disabled_options.add(name)
            else:
                raise TypeError(""Each step must be a string beginning with '+' or '-'."")

    def select_optional_steps(self, *args, **kwargs):
        '''
        Deprecated. Use specify_optional_steps or modify_optional_steps instead.
        '''
        kwargs = process_kwargs(
            ""select_optional_steps"",
            kwargs,
            {""disable_others"":False})
        if kwargs[""disable_others""]:
            self._enabled_options.clear()
            self._disable_all_options = True
        args = flatten_string_list(args)
        args = flatten_comma_list(args)
        self.modify_optional_steps(*args)

    def run(self, argv=None):
        self._context = BuildContext()
        options, args = self._optionParser.parse_args(argv)
        self._context.options = options
        self._context.args = args
        self._context.env = dict(os.environ)
        for step in self._steps:
            if step.test_conditions(self._context.env):
                enabled = True
                if step.is_optional:
                    enabled = step.is_enabled_by_default
                    if self._disable_all_options:
                        enabled = False
                    if step.name in self._enabled_options:
                        enabled = True
                    if step.name in self._disabled_options:
                        enabled = False
                if enabled:
                    print step.name
                    step.run(self._context)
    def add_bool_option(self, *args, **kwargs):
        kwargs=dict(kwargs)
        kwargs[""default""] = False
        kwargs[""action""] = ""store_true""
        self.add_option(*args, **kwargs)
    def add_option(self, *args, **kwargs):
        self._optionParser.add_option(*args, **kwargs)

    def _check_call(self, *args, **kwargs):
        if self._context.options.verbose:
            argstring = ["", "".join([repr(arg) for arg in args])]
            kwargstring = ["", "".join([""%s=%r"" % (k,v) for (k,v) in kwargs.items()])]
            print ""subprocess.check_call(%s)"" % ("", "".join(argstring+kwargstring))
        subprocess.check_call(*args, **kwargs)

    def python(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call([sys.executable] + args, env=self._context.env, **kwargs)
    def shell(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call(args, env=self._context.env, shell=True, **kwargs)
    def rsync(self, *args, **kwargs):
        args = flatten_string_list(args)
        self._check_call([""rsync""] + args, **kwargs)
    def _dependency_collection(self):
        return read_dependencies_from_filename(os.path.join('projectdata', 'dependencies.txt'), logfile=sys.stdout)
    def fetch_dependencies(self, *dependencies, **kwargs):
        kwargs = process_kwargs(
            ""fetch_dependencies"",
            kwargs,
            {""platform"":None})
        dependencies = flatten_string_list(dependencies)
        platform = kwargs['platform'] or self._context.env[""PLATFORM""]
        dependency_collection = self._dependency_collection()
        delete_directory(os.path.join('dependencies', platform), logfile=sys.stdout)
        if len(dependencies) > 0:
            if not dependency_collection.fetch(dependencies, self._context.env):
                raise AbortRunException()
    def get_dependency_args(self, *dependencies):
        dependencies = flatten_string_list(dependencies)
        dependency_collection = self._dependency_collection()
        return dependency_collection.get_args(dependencies, self._context.env)

class SshSession(object):
    def __init__(self, host, username):
        import paramiko
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(host, username=username, look_for_keys='True')
    def call(self, *args, **kwargs):
        stdin, stdout, stderr = self.ssh.exec_command(*args, **kwargs)
        def pump_output_thread(source, destination):
            for line in source:
                destination.write(line)
                destination.flush()
        stdout_thread = threading.Thread(target=pump_output_thread, args=(stdout, sys.stdout))
        stderr_thread = threading.Thread(target=pump_output_thread, args=(stderr, sys.stderr))
        stdout_thread.start()
        stderr_thread.start()
        stdout_thread.join()
        stderr_thread.join()
        return stdout.channel.recv_exit_status()
    def __call__(self, *args):
        return self.call(*args)
    def __enter__(self):
        return self
    def __exit__(self, ex_type, ex_value, ex_traceback):
        self.ssh.close()

class AbortRunException(Exception):
    def __init__(self, message=""Aborted due to error."", exitcode=1):
        Exception.__init__(self, message)
        self.message = message
        self.exitcode = exitcode

def fail(*args, **kwargs):
    '''
    fail(message, exitcode=1)
    Abort the build with an error message.
    '''
    raise AbortRunException(*args, **kwargs)

def require_version(required_version):
    '''Fail if the version of ohDevTools is too old.'''
    if VERSION<required_version:
        fail(""This build requires a newer version of ohDevTools. You have version {0}, but need version {1}."".format(VERSION, required_version),32)

def windows_program_exists(program):
    return subprocess.call([""where"", ""/q"", program], shell=False)==0

def other_program_exists(program):
    return subprocess.call([""/bin/sh"", ""-c"", ""command -v ""+program], shell=False, stdout=open(os.devnull), stderr=open(os.devnull))==0

program_exists = windows_program_exists if platform.platform().startswith(""Windows"") else other_program_exists

def scp(*args):
    program = None
    for p in [""scp"", ""pscp""]:
        if program_exists(p):
            program = p
            break
    if program is None:
        raise ""Cannot find scp (or pscp) in the path.""
    subprocess.check_call([program] + list(args))

def run(buildname=""build"", argv=None):
    builder = Builder()
    behaviour_globals = {
            'fetch_dependencies':builder.fetch_dependencies,
            'get_dependency_args':builder.get_dependency_args,
            'add_option':builder.add_option,
            'add_bool_option':builder.add_bool_option,
            'python':builder.python,
            'shell':builder.shell,
            'rsync':builder.rsync,
            'build_step':builder.build_step,
            'build_condition':builder.build_condition,
            'default_platform':default_platform,
            'get_vsvars_environment':get_vsvars_environment,
            'SshSession':SshSession,
            'select_optional_steps':builder.select_optional_steps,
            'modify_optional_steps':builder.modify_optional_steps,
            'specify_optional_steps':builder.specify_optional_steps,
            'userlock':userlock,
            'fail':fail,
            'scp':scp,
            'require_version':require_version
        }
    try:
        execfile(os.path.join('projectdata', buildname+'_behaviour.py'), behaviour_globals)
        builder.run(argv)
    except AbortRunException as e:
        print e.message
        sys.exit(e.exitcode)
/n/n/n",1
140,140,8c333a6000efee9a3198d44e47868708b6347582,"engineauth/middleware.py/n/nfrom __future__ import absolute_import
from engineauth import models
from engineauth import utils
from engineauth.config import load_config
import re
from webob import Response
from webob import Request

class EngineAuthResponse(Response):

    def _save_session(self):
        session = self.request.session
        # Compare the hash that we set in load_session to the current one.
        # We only save the session and cookie if this value has changed.
        if self.request.session_hash == session.hash():
            return session
        session.put()
        # If we have a user_id we want to updated the
        # session to use the user_id as the key.
        if session.user_id is not None:
            session_id = session.key.id()
            if session_id != session.user_id:
                session = models.Session.upgrade_to_user_session(
                    session_id, session.user_id)
        self.set_cookie('_eauth', session.serialize())
        return self

    def _save_user(self):
        pass


class EngineAuthRequest(Request):

    ResponseClass = EngineAuthResponse

    def _load_session(self):
        value = self.cookies.get('_eauth')
        session = None
        if value:
            session = models.Session.get_by_value(value)
        if session is not None:
            # Create a hash for later comparison,
            # to determine if a put() is required
            session_hash = session.hash()
        else:
            session = models.Session.create()
            # set this to False to ensure a cookie
            # is saved later in the response.
            session_hash = '0'
        self.session = session
        self.session_hash = session_hash
        return self

    def _get_user_class(self):
        try:
            return utils.import_class(self._config['user_model'])
        except Exception:
            return models.User

    def _load_user(self):
        if self.session is not None and self.session.user_id:
            self.user = self._get_user_class().get_by_id(int(self.session.user_id))
            if self.user is None:
                # TODO: If the user_id from the session returns no user,
                # then remove it.
                pass
        else:
            self.user = None
        return self

    def _load_user_by_profile(self, profile):
        # if the user is logged in update that user with the profile details
        if self.user:
            self.user.add_profile(profile)
        # else get or create a user based on the profile
        else:
            self.user = self._get_user_class().get_or_create_by_profile(profile)
        # Add user to session
        self.session.user_id = self.user.get_id()
    load_user_by_profile = _load_user_by_profile

    def _add_message(self, message, level=None, key='_messages'):
        if not self.session.data.get(key):
            self.session.data[key] = []
        return self.session.data[key].append({
            'message': message, 'level': level})
    add_message = _add_message

    def _get_messages(self, key='_messages'):
        try:
            return self.session.data.pop(key)
        except KeyError:
            pass
    get_messages = _get_messages

    def _get_redirect_uri(self):
        try:
            return self.session.data.pop('_redirect_uri').encode('utf-8')
        except KeyError:
            return self._config['success_uri']
    get_redirect_uri = _get_redirect_uri

    def _set_globals(self, environ):
#        environ['ea.config'] = req.config
        environ['ea.session'] = self.session
        environ['ea.user'] = self.user


class AuthMiddleware(object):
    def __init__(self, app, config=None):
        self.app = app
        self._config = load_config(config)
        self._url_parse_re = re.compile(r'%s/([^\s/]+)/*(\S*)' %
                                        (self._config['base_uri']))

    def __call__(self, environ, start_response):
        # If the request is to the admin, return
        if environ['PATH_INFO'].startswith('/_ah/'):
            return self.app(environ, start_response)
        # load session
        req = EngineAuthRequest(environ)
        req._config = self._config
        req._load_session()
        req._load_user()
        resp = None
        # If the requesting url is for engineauth load the strategy
        if environ['PATH_INFO'].startswith(self._config['base_uri']):
            # extract provider and additional params from the url
            provider, provider_params = self._url_parse_re.match(
                req.path_info).group(1, 2)
            if provider:
                req.provider = provider
                req.provider_params = provider_params
                # load the desired strategy class
                strategy_class = self._load_strategy(provider)
                resp = req.get_response(strategy_class(self.app, self._config))
                if resp.request is None:
                    # TODO: determine why this is necessary.
                    resp.request = req
        if resp is None:
            resp = req.get_response(self.app)
        # Save session, return response
        resp._save_session()
        return resp(environ, start_response)

    def _load_strategy(self, provider):
        try:
            strategy_location = self._config[
                                'provider.{0}'.format(provider)]['class_path']
            return utils.import_class(strategy_location)
        except Exception, e:
            raise(Exception, ""You must provide a location for the {0} ""\
                             ""strategy. Add a 'location' key to the ""\
                             ""'provider.{0}' config dict"".format(provider))

/n/n/ntests/test_middleware.py/n/nfrom engineauth.middleware import AuthMiddleware
from engineauth.middleware import EngineAuthRequest
from engineauth import models
import test_base
import webapp2

__author__ = 'kyle.finley@gmail.com (Kyle Finley)'


app = AuthMiddleware(webapp2.WSGIApplication())

class TestAuthMiddleware(test_base.BaseTestCase):
    def setUp(self):
        super(TestAuthMiddleware, self).setUp()

    #    def test_load_config(self):
    #        req = EngineAuthRequest.blank('/auth/google')
    #        resp = req.get_response(app)
    #        self.assertEqual(resp, '/auth')

    def test_load_strategy(self):
        from engineauth.strategies.google import GoogleStrategy

        strategy_class = app._load_strategy('google')
        self.assertEqual(strategy_class, GoogleStrategy)
        self.assertRaises(Exception, app._load_strategy, 'enron')
        from engineauth.strategies.appengine_openid import\
            AppEngineOpenIDStrategy
        strategy_class = app._load_strategy('appengine_openid')
        self.assertEqual(strategy_class, AppEngineOpenIDStrategy)

    def test_load_session_no_session(self):
        req = EngineAuthRequest.blank('/auth/google')
        # No Session
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 0)
        sess = req._load_session()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)

    def test_laod_session_session_id_no_user_id(self):
        # Cookie session_id but no user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        req._load_session()
        self.assertTrue(req.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

    def test_laod_session_session_id_and_user_id(self):
        # Cookie session_id and user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        req._load_session()
        self.assertTrue(req.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)


    def test_laod_session_cookie_and_no_session(self):
        # Cookie and not session
        s = models.Session.create()
        old_sid = s.session_id
        s_serialized = s.serialize()
        s.key.delete()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 0)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s_serialized
        req._load_session()
        # Assert that a new session was created
        self.assertTrue(req.session.session_id != old_sid)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

    def test_save_session(self):
        # Cookie session_id but no user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)

        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        resp = req.get_response(app)
        resp.request = req
        resp._save_session()

        self.assertTrue(resp.request.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

        # Add a user_id to session
        resp.request.session.user_id = '1'
        resp._save_session()
        # a new session should be created with the user_id as it's id
#        self.assertEqual(resp.request.session.key.id(), '1')
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        s1 = models.Session.query().get()
        self.assertEqual(s1.key.id(), '1')

    def test__load_user(self):
        user = models.User.create_user('test:12345')
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req.session.user_id = user.get_id()
        req._load_user()
        self.assertEqual(user, req.user)

    def test__load_user_by_profile(self):
        # No existing User no logged in User
        auth_id = 'test:12345'
        user_info = {
            'auth_id': auth_id,
            'info': {},
        }
        # create profile
        p = models.UserProfile.get_or_create(auth_id, user_info)
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req._load_user()

        # User Count before
        user_count = models.User.query().count()
        self.assertEqual(user_count, 0)

        req.load_user_by_profile(p)

        # User Count after
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

        user = models.User.query().get()
        self.assertTrue(p.key.id() in user.auth_ids)

        # Yes existing User no logged in User
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req._load_user()

        req.load_user_by_profile(p)

        # Test to no new User was created
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

        # Yes existing User yes logged in User new Profile
        auth_id = 'test:abc'
        user_info = {
            'auth_id': auth_id,
            'info': {},
            }
        # create profile
        p1 = models.UserProfile.get_or_create(auth_id, user_info)
        req.load_user_by_profile(p1)

        # Test to no new User was created
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

    def test_add_message(self):
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()

        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        req.add_message('TEST MESSAGE')
        msgs = req.get_messages()
        self.assertEquals(msgs, [{'level': None, 'message':'TEST MESSAGE' }])

        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        # add message with level error
        req.add_message('TEST1', 'error')
        # add another message with level error
        req.add_message('TEST2', 'success')

        msgs = req.get_messages()
        self.assertEquals(msgs, [
                {'level': 'error', 'message':'TEST1' },
                {'level': 'success', 'message':'TEST2' },
        ])
        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        # Test with different key.
        # add message with level error
        req.add_message('TEST1', 'error')
        # add another message with level error
        req.add_message('TEST2', 'success', '_mykey')

        msgs = req.get_messages()
        self.assertEquals(msgs, [
                {'level': 'error', 'message':'TEST1' },
        ])
        msgs_key = req.get_messages('_mykey')
        self.assertEquals(msgs_key, [
                {'level': 'success', 'message':'TEST2' },
        ])
        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)
        msgs_key = req.get_messages()
        self.assertEquals(msgs_key, None)
/n/n/n",0
141,141,8c333a6000efee9a3198d44e47868708b6347582,"/engineauth/middleware.py/n/nfrom __future__ import absolute_import
from engineauth import models
from engineauth import utils
from engineauth.config import load_config
import re
from webob import Response
from webob import Request

class EngineAuthResponse(Response):

    def _save_session(self):
        session = self.request.session
        # Compare the hash that we set in load_session to the current one.
        # We only save the session and cookie if this value has changed.
        if self.request.session_hash == session.hash():
            return session
        session.put()
        # If we have a user_id we want to updated the
        # session to use the user_id as the key.
        if session.user_id is not None:
            session_id = session.key.id()
            if session_id != session.user_id:
                session = models.Session.upgrade_to_user_session(
                    session_id, session.user_id)
        self.set_cookie('_eauth', session.serialize())
        return self

    def _save_user(self):
        pass


class EngineAuthRequest(Request):

    ResponseClass = EngineAuthResponse

    def _load_session(self):
        value = self.cookies.get('_eauth')
        session = None
        if value:
            session = models.Session.get_by_value(value)
        if session is not None:
            # Create a hash for later comparison,
            # to determine if a put() is required
            session_hash = session.hash()
        else:
            session = models.Session.create()
            # set this to False to ensure a cookie
            # is saved later in the response.
            session_hash = '0'
        self.session = session
        self.session_hash = session_hash
        return self

    def _get_user_class(self):
        try:
            return utils.import_class(self._config['user_model'])
        except Exception:
            return models.User

    def _load_user(self):
        if self.session is not None and self.session.user_id:
            self.user = self._get_user_class().get_by_id(int(self.session.user_id))
            if self.user is None:
                # TODO: If the user_id from the session returns no user,
                # then remove it.
                pass
        else:
            self.user = None
        return self

    def _load_user_by_profile(self, profile):
        # if the user is logged in update that user with the profile details
        if self.user:
            self.user.add_profile(profile)
        # else get or create a user based on the profile
        else:
            self.user = self._get_user_class().get_or_create_by_profile(profile)
        # Add user to session
        self.session.user_id = self.user.get_id()
    load_user_by_profile = _load_user_by_profile

    def _add_message(self, message, level=None, key='_messages'):
        if not self.session.data.get(key):
            self.session.data[key] = []
        return self.session.data[key].append({
            'message': message, 'level': level})
    add_message = _add_message

    def _get_messages(self, key='_messages'):
        try:
            return self.session.data.pop(key)
        except KeyError:
            pass
    get_messages = _get_messages

    def _set_redirect_uri(self):
        next_uri = self.GET.get('next')
        if next_uri is not None:
            self.session.data['_redirect_uri'] = next_uri
    set_redirect_uri = _set_redirect_uri

    def _get_redirect_uri(self):
        try:
            return self.session.data.pop('_redirect_uri').encode('utf-8')
        except KeyError:
            return self._config['success_uri']
    get_redirect_uri = _get_redirect_uri

    def _set_globals(self, environ):
#        environ['ea.config'] = req.config
        environ['ea.session'] = self.session
        environ['ea.user'] = self.user


class AuthMiddleware(object):
    def __init__(self, app, config=None):
        self.app = app
        self._config = load_config(config)
        self._url_parse_re = re.compile(r'%s/([^\s/]+)/*(\S*)' %
                                        (self._config['base_uri']))

    def __call__(self, environ, start_response):
        # If the request is to the admin, return
        if environ['PATH_INFO'].startswith('/_ah/'):
            return self.app(environ, start_response)
        # load session
        req = EngineAuthRequest(environ)
        req._config = self._config
        req._load_session()
        req._load_user()
        req._set_redirect_uri()
        resp = None
        # If the requesting url is for engineauth load the strategy
        if environ['PATH_INFO'].startswith(self._config['base_uri']):
            # extract provider and additional params from the url
            provider, provider_params = self._url_parse_re.match(
                req.path_info).group(1, 2)
            if provider:
                req.provider = provider
                req.provider_params = provider_params
                # load the desired strategy class
                strategy_class = self._load_strategy(provider)
                resp = req.get_response(strategy_class(self.app, self._config))
                if resp.request is None:
                    # TODO: determine why this is necessary.
                    resp.request = req
        if resp is None:
            resp = req.get_response(self.app)
        # Save session, return response
        resp._save_session()
        return resp(environ, start_response)

    def _load_strategy(self, provider):
        try:
            strategy_location = self._config[
                                'provider.{0}'.format(provider)]['class_path']
            return utils.import_class(strategy_location)
        except Exception, e:
            raise(Exception, ""You must provide a location for the {0} ""\
                             ""strategy. Add a 'location' key to the ""\
                             ""'provider.{0}' config dict"".format(provider))

/n/n/n/tests/test_middleware.py/n/nfrom engineauth.middleware import AuthMiddleware
from engineauth.middleware import EngineAuthRequest
from engineauth import models
import test_base
import webapp2

__author__ = 'kyle.finley@gmail.com (Kyle Finley)'


app = AuthMiddleware(webapp2.WSGIApplication())

class TestAuthMiddleware(test_base.BaseTestCase):
    def setUp(self):
        super(TestAuthMiddleware, self).setUp()

    #    def test_load_config(self):
    #        req = EngineAuthRequest.blank('/auth/google')
    #        resp = req.get_response(app)
    #        self.assertEqual(resp, '/auth')

    def test_load_strategy(self):
        from engineauth.strategies.google import GoogleStrategy

        strategy_class = app._load_strategy('google')
        self.assertEqual(strategy_class, GoogleStrategy)
        self.assertRaises(Exception, app._load_strategy, 'enron')
        from engineauth.strategies.appengine_openid import\
            AppEngineOpenIDStrategy
        strategy_class = app._load_strategy('appengine_openid')
        self.assertEqual(strategy_class, AppEngineOpenIDStrategy)

    def test_load_session_no_session(self):
        req = EngineAuthRequest.blank('/auth/google')
        # No Session
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 0)
        sess = req._load_session()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)

    def test_laod_session_session_id_no_user_id(self):
        # Cookie session_id but no user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        req._load_session()
        self.assertTrue(req.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

    def test_laod_session_session_id_and_user_id(self):
        # Cookie session_id and user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        req._load_session()
        self.assertTrue(req.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)


    def test_laod_session_cookie_and_no_session(self):
        # Cookie and not session
        s = models.Session.create()
        old_sid = s.session_id
        s_serialized = s.serialize()
        s.key.delete()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 0)
        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s_serialized
        req._load_session()
        # Assert that a new session was created
        self.assertTrue(req.session.session_id != old_sid)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

    def test_save_session(self):
        # Cookie session_id but no user_id
        s = models.Session.create()
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)

        req = EngineAuthRequest.blank('/auth/google')
        req.cookies['_eauth'] = s.serialize()
        resp = req.get_response(app)
        resp.request = req
        resp._save_session()

        self.assertTrue(resp.request.session.session_id == s.session_id)
        # Assert No new session was created
        s_count2 = models.Session.query().count()
        self.assertTrue(s_count2 == 1)

        # Add a user_id to session
        resp.request.session.user_id = '1'
        resp._save_session()
        # a new session should be created with the user_id as it's id
#        self.assertEqual(resp.request.session.key.id(), '1')
        s_count = models.Session.query().count()
        self.assertTrue(s_count == 1)
        s1 = models.Session.query().get()
        self.assertEqual(s1.key.id(), '1')

    def test__load_user(self):
        user = models.User.create_user('test:12345')
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req.session.user_id = user.get_id()
        req._load_user()
        self.assertEqual(user, req.user)

    def test__load_user_by_profile(self):
        # No existing User no logged in User
        auth_id = 'test:12345'
        user_info = {
            'auth_id': auth_id,
            'info': {},
        }
        # create profile
        p = models.UserProfile.get_or_create(auth_id, user_info)
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req._load_user()

        # User Count before
        user_count = models.User.query().count()
        self.assertEqual(user_count, 0)

        req.load_user_by_profile(p)

        # User Count after
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

        user = models.User.query().get()
        self.assertTrue(p.key.id() in user.auth_ids)

        # Yes existing User no logged in User
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req._load_user()

        req.load_user_by_profile(p)

        # Test to no new User was created
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

        # Yes existing User yes logged in User new Profile
        auth_id = 'test:abc'
        user_info = {
            'auth_id': auth_id,
            'info': {},
            }
        # create profile
        p1 = models.UserProfile.get_or_create(auth_id, user_info)
        req.load_user_by_profile(p1)

        # Test to no new User was created
        user_count = models.User.query().count()
        self.assertEqual(user_count, 1)

    def test_add_message(self):
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()

        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        req.add_message('TEST MESSAGE')
        msgs = req.get_messages()
        self.assertEquals(msgs, [{'level': None, 'message':'TEST MESSAGE' }])

        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        # add message with level error
        req.add_message('TEST1', 'error')
        # add another message with level error
        req.add_message('TEST2', 'success')

        msgs = req.get_messages()
        self.assertEquals(msgs, [
                {'level': 'error', 'message':'TEST1' },
                {'level': 'success', 'message':'TEST2' },
        ])
        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)

        # Test with different key.
        # add message with level error
        req.add_message('TEST1', 'error')
        # add another message with level error
        req.add_message('TEST2', 'success', '_mykey')

        msgs = req.get_messages()
        self.assertEquals(msgs, [
                {'level': 'error', 'message':'TEST1' },
        ])
        msgs_key = req.get_messages('_mykey')
        self.assertEquals(msgs_key, [
                {'level': 'success', 'message':'TEST2' },
        ])
        # Get again should be none.
        msgs = req.get_messages()
        self.assertEquals(msgs, None)
        msgs_key = req.get_messages()
        self.assertEquals(msgs_key, None)

    def test_set_redirect_uri(self):
        # test without next uri
        req = EngineAuthRequest.blank('/auth/google')
        req._load_session()
        req.set_redirect_uri()
        req._config = {'success_uri': '/callback'}
        redirect_uri = req.get_redirect_uri()
        self.assertEqual(redirect_uri, '/callback')

        # test with out next uri
        req = EngineAuthRequest.blank('/auth/google?next=/newcallback')
        req._load_session()
        req.set_redirect_uri()
        req._config = {'success_uri': '/callback'}
        redirect_uri = req.get_redirect_uri()
        self.assertEqual(redirect_uri, '/newcallback')

        req = EngineAuthRequest.blank('/auth/google?next=/newcallback&a=121&123=a')
        req._load_session()
        req.set_redirect_uri()
        req._config = {'success_uri': '/callback'}
        redirect_uri = req.get_redirect_uri()
        self.assertEqual(redirect_uri, '/newcallback')/n/n/n",1
84,84,42f7ab049975b89f2195461391aead5e82fb2461,"dashboard/tests/functional/test_chemical_curation.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import fixtures_standard
from dashboard.models import RawChem


class ChemicalCurationTests(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_chemical_curation_page(self):
        """"""
        Ensure there is a chemical curation page
        :return: a 200 status code
        """"""
        response = self.client.get('/chemical_curation/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download Uncurated Chemicals')

        # Pick one curated and one non-curated RawChem record, and
        # confirm that the downloaded file excludes and includes them,
        # respectively.
        rc = RawChem.objects.filter(dsstox_id__isnull=True).first()
        response = self.client.get('/dl_raw_chems/')
        rc_row = f'%s,%s,%s,%s\r\n' % (rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content, 'The non-curated row should appear')

        rc = RawChem.objects.filter(dsstox_id__isnull=False).first()
        rc_row = f'%s,%s,%s,%s\r\n' % (rc.id, rc.raw_cas, rc.raw_chem_name, rc.sid if rc.sid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertNotIn(rc_row, response.content, 'The curated row should not appear')
/n/n/ndashboard/tests/functional/test_get_data.py/n/nfrom django.urls import resolve
from django.test import TestCase, override_settings
from django.test.client import Client

from django.contrib.auth import authenticate
from django.contrib.auth.models import User
from dashboard.models import PUC, Product, ProductToPUC, ProductDocument, DSSToxLookup
from dashboard.views.get_data import *
from django.test import TestCase
from django.test.client import Client

from dashboard.views.get_data import *
from dashboard.tests.loader import fixtures_standard


# from dashboard import views
# from django.urls import resolve
# from django.contrib.auth import authenticate
# from django.contrib.auth.models import User

@override_settings(ALLOWED_HOSTS=['testserver'])
class TestGetData(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dtxsid_pucs_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(0, ethylparaben_stats['pucs_n'])

        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        # get one of the products that was just linked to a data document with DTXSID9022528 in its extracted chemicals
        pid = dd.products.first().pk
        puc = PUC.objects.get(id=20)
        # add a puc to one of the products containing ethylparaben

        ppuc = ProductToPUC.objects.create(product=Product.objects.get(pk=pid),
                                           puc=puc,
                                           puc_assigned_usr=User.objects.get(username='Karyn'))
        ppuc.refresh_from_db()
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(1, ethylparaben_stats['pucs_n'])

    def test_dtxsid_dds_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            2, ethylparaben_stats['dds_n'], 'There should be 2 datadocuments associated with ethylaraben')
        # change the number of related data documents by deleting one
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))

        dd = dds[0]
        dd.delete()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            1, ethylparaben_stats['dds_n'], 'There should now be 1 datadocument associated with ethylaraben')

    def test_dtxsid_dds_wf_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['dds_wf_n'], 'There should be 1 extracted chemical \
        with weight fraction data associated with ethylparaben')
        # add weight fraction data to a different extractedchemical
        ec = ExtractedChemical.objects.get(rawchem_ptr_id=73)
        ec.raw_min_comp = 0.1
        ec.save()
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(2, ethylparaben_stats['dds_wf_n'], 'There should be 2 extracted chemicals \
        with weight fraction data associated with ethylparaben')

    def test_dtxsid_products_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)

        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(0, ethylparaben_stats['products_n'], 'There should be 0 products \
        associated with ethylparaben')
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['products_n'], 'There should now be 1 product \
        associated with ethylparaben')

    def test_habits_and_practices_cards(self):
        data = {'puc': ['2']}
        response = self.client.post('/get_data/', data=data)
        for hnp in [b'ball bearings',
                    b'motorcycle',
                    b'vitamin a&amp;d',
                    b'dish soap']:
            self.assertIn(hnp, response.content)

    def test_download_pucs_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download PUCs')
/n/n/ndashboard/tests/integration/test_browser_edits.py/n/nfrom lxml import html

from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.staticfiles.testing import StaticLiveServerTestCase

from dashboard.models import *
from selenium import webdriver
from django.conf import settings
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

            # Data Document Detail Page
            doc_detail_link = f'/datadocument/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_detail_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            doc_type_select.select_by_visible_text(""MSDS"")
            self.assertIn(doc_detail_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note
        5. Approve
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781,  # Chemical Presence List
                       354783,  # HHE Report
                       ]:
            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()

            # Modify the first raw_chem_name field's value
            #
            raw_chem = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_chem_name""]')
            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')
            # print(rawchem_id)

            raw_chem_name_field.send_keys(' edited')
            # save changes
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            # print(et.data_document.data_group.group_type)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm validation failure
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            # The QA notes field should be invalid
            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_qa_notes""]')
            self.assertIn('is-invalid', qa_notes_field.get_attribute('class'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')

    def test_datadoc_add_extracted(self):
        '''
        Test that when a datadocument has no ExtractedText,
        the user can add one in the browser
        1.
        '''

        for doc_id in [155324   # CO record with no ExtractedText
                       ]:
            # QA Page
            dd_url = self.live_server_url + f'/datadocument/{doc_id}/'
            self.browser.get(dd_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]').click()

            # Verify that the modal window appears by finding the Cancel button
            # The modal window does not immediately appear, so the browser
            # should wait for the button to be clickable
            wait = WebDriverWait(self.browser, 10)
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            self.assertEqual(""Cancel"", cancel_button.text,
                             'The Cancel button should say Cancel')
            cancel_button.click()
            # Verify that no ExtractedText record was created
            self.assertEqual(0, ExtractedText.objects.filter(
                data_document_id=doc_id).count(),
                ""the count of ExtractedText records related to the \
                data document should be zero"")

            # Wait for the modal div to disappear
            edit_modal = wait.until(
                ec.invisibility_of_element(
                    (By.XPATH, '//*[@id=""extextModal""]')
                )
            )
            # Click the Add button again to reopen the editor
            add_button = self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]')
            add_button.click()
            # Once again, check that the controls on the modal form are clickable
            # before trying to interact with them
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            prod_name_box = self.browser.find_element_by_id(
                'id_prod_name')
            # Add a prod_name value to the box
            prod_name_box.send_keys('Fake Product')
            save_button = self.browser.find_element_by_id(
                'extracted-text-modal-save')
            save_button.click()
            # Confirm the presence of the new ExtractedText record
            et = ExtractedText.objects.get(data_document_id=doc_id)
            self.assertEqual('Fake Product', et.prod_name,
                             ""The prod_name of the new object should match what was entered"")

/n/n/ndashboard/urls.py/n/nfrom django.urls import include, path
from django.conf import settings
from django.conf.urls.static import static

import dashboard.views.qa
from . import views

urlpatterns = [
    path('', views.index,                   name='index'),
    path('datasources/', views.data_source_list,
                                            name='data_source_list'),
    path('datasource/<int:pk>', views.data_source_detail,
                                            name='data_source_detail'),
    path('datasource/new/', views.data_source_create,
                                            name='data_source_new'),
    path('datasource/edit/<int:pk>/', views.data_source_update,
                                            name='data_source_edit'),
    path('datasource/delete/<int:pk>/', views.data_source_delete,
                                            name='data_source_delete'),
    path('datagroups/', views.data_group_list,
                                            name='data_group_list'),
    path('datagroup/<int:pk>/', views.data_group_detail,
                                            name='data_group_detail'),
    path('datagroup/docs_csv/<int:pk>/', views.dg_dd_csv_view,
                                            name='dg_dd_csv_view'),
    path('datagroup/pdfs_zipped/<int:pk>/', views.dg_pdfs_zip_view,
                                            name='dg_pdfs_zip_view'),
    path('datagroup/raw_extracted_records/<int:pk>/', views.dg_raw_extracted_records,
                                            name='dg_raw_extracted_records'),
    path('datasource/<int:pk>/datagroup_new/', views.data_group_create,
                                            name='data_group_new'),
    path('datagroup/<int:pk>/registered_records.csv', views.data_group_registered_records_csv,
                                            name=""registered_records.csv""),
    path('datagroup/edit/<int:pk>/', views.data_group_update,
                                            name='data_group_edit'),
    path('datagroup/delete/<int:pk>/', views.data_group_delete,
                                            name='data_group_delete'),
    path('datadocument/delete/<int:pk>/', views.data_document_delete,
                                            name='data_document_delete'),
    path('datadocument/note/<int:pk>/', views.data_document_note,
                                            name='data_document_note'),
    path('product_curation/', views.product_curation_index,
                                            name='product_curation'),
    path('chemical_curation/', views.chemical_curation_index,
         name='chemical_curation'),
    path('category_assignment/<int:pk>/', views.category_assignment,
                                            name='category_assignment'),
    path('link_product_list/<int:pk>/', views.link_product_list,
                                            name='link_product_list'),
    path('link_product_form/<int:pk>/', views.link_product_form,
                                            name='link_product_form'),
    path('qa/extractionscript/', views.qa_extractionscript_index,
                                            name='qa_extractionscript_index'),
    path('qa/extractionscript/<int:pk>/', dashboard.views.qa.qa_extraction_script,
                                            name='qa_extraction_script'),
    path('qa/extractedtext/<int:pk>/', dashboard.views.qa.extracted_text_qa,
                                            name='extracted_text_qa'),
    path('extractionscript/<int:pk>/', views.extraction_script_detail,
                                            name='extraction_script_detail'),
    path('qa/chemicalpresence/', views.qa_chemicalpresence_index,
                                            name='qa_chemicalpresence_index'),
    path('qa/chemicalpresencegroup/<int:pk>/', views.qa_chemicalpresence_group,
                                            name='qa_chemical_presence_group'),
    path('bulk_product_puc/', views.bulk_assign_puc_to_product,
                                            name='bulk_product_puc'),
    path('bulk_product_tag/', views.bulk_assign_tag_to_products,
                                            name='bulk_product_tag'),
    path('product_puc/<int:pk>/', views.assign_puc_to_product,
                                            name='product_puc'),
    path('product_puc_delete/<int:pk>/', views.detach_puc_from_product,
                                            name='product_puc_delete'),
    path('puc-autocomplete/', views.puc_autocomplete.PUCAutocomplete.as_view(),
                                            name='puc-autocomplete'),
    path('product/<int:pk>/', views.product_detail,
                                            name='product_detail'),
    path('product/edit/<int:pk>/', views.product_update,
                                            name='product_edit'),
    path('product/delete/<int:pk>/', views.product_delete,
                                            name='product_delete'),
    path('products/', views.product_list,  name='product_list'),
    path('datadocument/<int:pk>/', views.data_document_detail,
                                            name='data_document'),
    path('save_type/<int:pk>/', views.save_doc_form,
                                            name='save_doc_form'),
    path('save_ext/<int:pk>/', views.save_ext_form,
                                            name='save_ext_form'),
    path('search/', include('haystack.urls')),
    path('find/', views.search.FacetedSearchView.as_view(),
                                            name='haystack_search'),
    path('p_json/', views.product_ajax,     name='p_ajax_url'),
    path('pucs/', views.puc_list,           name='puc_list'),
    path('dl_pucs/', views.download_PUCs,   name='download_PUCs'),
    path('dl_raw_chems/', views.download_raw_chems,  
                                            name='download_raw_chems'),
    path('dsstox_lookup/<int:pk>/', views.dsstox_lookup_detail,
                                            name='dsstox_lookup'),
    path('habitsandpractices/<int:pk>/', views.habitsandpractices,
                                            name='habitsandpractices'),
    path('link_habitandpractice_to_puc/<int:pk>/', views.link_habitsandpractices,
                                            name='link_habitsandpractices'),
    path('get_data/', views.get_data,      name='get_data'),
    path('dl_chem_summary/', views.download_chem_stats,
                                            name='download_chem_stats'),
    path('upload/dtxsid_csv/', views.upload_dtxsid_csv,
                                            name='upload_dtxsid_csv'),
    path('get_data/get_dsstox_csv_template/', views.get_data_dsstox_csv_template,
                                            name='get_data_dsstox_csv_template'),
    path('datagroup/diagnostics/<int:pk>/',   views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('datagroup/diagnostics/',          views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('extractedtext/edit/<int:pk>/',   views.extracted_text_edit,
                                            name='extracted_text_edit'),
    path('extractedchild/edit/<int:pk>/',   views.extracted_child_edit,
                                            name='extracted_child_edit'),
    path('datadocument/edit/<int:pk>/',   views.data_document_edit,
                                            name='data_document_edit'),
]

if settings.DEBUG is True:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
/n/n/ndashboard/views/__init__.py/n/nfrom .dashboard import *
from .data_group import *
from .data_source import *
from .data_document import *
from .product_curation import *
from .qa import *
from .extraction_script import *
from .puc_autocomplete import *
from .ajax import *
from .search_forms import *
from .search import *
from .dsstox_lookup import *
from .habits_n_practices import *
from .get_data import *
from .puc import *
from .diagnostics import *
from .chemical_curation import *
/n/n/ndashboard/views/chemical_curation.py/n/nfrom django.contrib.auth.decorators import login_required
from django.shortcuts import render, reverse
from django.http import HttpResponseRedirect, HttpResponse
from django.contrib import messages
from dashboard.models import *
import datetime
import csv


@login_required()
def chemical_curation_index(request, template_name='chemical_curation/chemical_curation_index.html'):
    uncurated_chemical_count = RawChem.objects.filter(dsstox_id=None).count()
    records_processed = 0

    data = {'uncurated_chemical_count': uncurated_chemical_count, 'records_processed': records_processed}
    # if not GET, then proceed
    if ""POST"" == request.method:
        try:
            # if not a csv file
            csv_file = request.FILES[""csv_file""]
            if not csv_file.name.endswith('.csv'):
                data.update({'error_message': 'File is not CSV type'})
                return render(request, template_name, data)
            # if file is too large, return
            if csv_file.multiple_chunks():
                error = ""Uploaded file is too big (%.2f MB)."" % (csv_file.size / (1000 * 1000))
                data.update({'error_message': error})
                return render(request, template_name, data)

            file_data = csv_file.read().decode(""utf-8"")
            lines = file_data.split(""\n"")

            # loop over the lines and save them in db. If error , store as string and then display
            records_processed = len(lines) - 1
            for line in lines:
                fields = line.split("","")
                data_dict = {}
                data_dict[""raw_chemical_id""] = fields[0].strip()
                data_dict[""rid""] = fields[1].strip()
                data_dict[""sid""] = fields[2].strip()
                data_dict[""true_chemical_name""] = fields[3].strip()
                data_dict[""true_cas""] = fields[4].strip()
                # If it is the header row check to see that the columns are in order.
                if data_dict[""raw_chemical_id""] == ""external_id"":
                    if (data_dict[""raw_chemical_id""] != ""external_id"") or (data_dict['sid'] != 'sid') or \
                            (data_dict['true_chemical_name'] != ""true_chemical_name"") or \
                            (data_dict['true_cas'] != 'true_cas'):
                        data.update({""error_message"": ""Check to ensure your column headers are in order.""})
                        return render(request, template_name, data)
                else:
                    try:
                        # Check to see of SID exists,
                        if DSSToxLookup.objects.filter(sid=data_dict['sid']).exists():
                            # if is does exist Overwrite the existing True CAS and Chemname
                            DSSToxLookup.objects.filter(sid=data_dict['sid']). \
                                update(true_cas=data_dict[""true_cas""],
                                       true_chemname=data_dict[""true_chemical_name""])
                        # if not create it in DSSToxLookup
                        else:
                            chem = DSSToxLookup.objects.create(true_cas=data_dict[""true_cas""],
                                                               true_chemname=data_dict[""true_chemical_name""],
                                                               sid=data_dict[""sid""])
                            chem.save()
                        # ensure link back to DSSToxLookup
                        sid_id = DSSToxLookup.objects.filter(sid=data_dict['sid'])
                        RawChem.objects.filter(id=data_dict[""raw_chemical_id""]).update(rid=data_dict['rid'],
                                                                                       dsstox_id=sid_id[0].id)
                    except Exception as e:
                        print(e)
                        pass

        except Exception as e:
            # This is the catchall - MySQL database has went away, etc....
            error = ""Something is seriously wrong with this csv - %s"" % repr(e)
            data.update({""error_messasage"": error})
            return render(request, template_name, data)
            messages.error(request, ""Unable to upload file. "" + repr(e))
    if records_processed > 0:
        data.update({""records_processed"": records_processed})
    return render(request, template_name, data)


@login_required()
def download_raw_chems(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""uncurated_chemicals_%s.csv""' % \
                                      (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['dashboard_rawchem_id', 'raw_cas', 'raw_chem_name', 'rid'])
    for rawchem in RawChem.objects.filter(dsstox_id=None):
        writer.writerow([rawchem.id, rawchem.raw_cas, rawchem.raw_chem_name, rawchem.rid if rawchem.rid else ''])
    return response
/n/n/ndashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django import forms
from django.db import connection
from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, Subquery, OuterRef, F, Sum
from django.forms.models import model_to_dict

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp
/n/n/ndashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import timezone, safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.forms import ModelForm
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, 
                            BulkProductPUCForm, BulkProductTagForm, 
                            BulkPUCForm, ProductForm)
from taggit.forms import TagField
from taggit_labels.widgets import LabelWidget
from django.core.paginator import Paginator
from django.db.models import Max


class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]


class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))

    class Meta:
        model = Product
        fields = ['tags']

    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self


@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs
    return render(request, template_name, {'data_sources': data_sources})


@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})


@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})


@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})


@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)


@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})


@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter(Q(title__icontains=q) | Q(brand_name__icontains=q)).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})


@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })


@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')


@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/n",0
85,85,42f7ab049975b89f2195461391aead5e82fb2461,"/dashboard/tests/functional/test_get_data.py/n/nfrom django.urls import resolve
from django.test import TestCase, override_settings
from django.test.client import Client

from django.contrib.auth import authenticate
from django.contrib.auth.models import User
from dashboard.models import PUC, Product, ProductToPUC, ProductDocument, DSSToxLookup
from dashboard.views.get_data import *
from django.test import TestCase
from django.test.client import Client

from dashboard.views.get_data import *
from dashboard.tests.loader import fixtures_standard


# from dashboard import views
# from django.urls import resolve
# from django.contrib.auth import authenticate
# from django.contrib.auth.models import User

@override_settings(ALLOWED_HOSTS=['testserver'])
class TestGetData(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dtxsid_pucs_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(0, ethylparaben_stats['pucs_n'])

        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        # get one of the products that was just linked to a data document with DTXSID9022528 in its extracted chemicals
        pid = dd.products.first().pk
        puc = PUC.objects.get(id=20)
        # add a puc to one of the products containing ethylparaben

        ppuc = ProductToPUC.objects.create(product=Product.objects.get(pk=pid),
                                           puc=puc,
                                           puc_assigned_usr=User.objects.get(username='Karyn'))
        ppuc.refresh_from_db()
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(1, ethylparaben_stats['pucs_n'])

    def test_dtxsid_dds_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            2, ethylparaben_stats['dds_n'], 'There should be 2 datadocuments associated with ethylaraben')
        # change the number of related data documents by deleting one
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))

        dd = dds[0]
        dd.delete()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            1, ethylparaben_stats['dds_n'], 'There should now be 1 datadocument associated with ethylaraben')

    def test_dtxsid_dds_wf_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['dds_wf_n'], 'There should be 1 extracted chemical \
        with weight fraction data associated with ethylparaben')
        # add weight fraction data to a different extractedchemical
        ec = ExtractedChemical.objects.get(rawchem_ptr_id=73)
        ec.raw_min_comp = 0.1
        ec.save()
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(2, ethylparaben_stats['dds_wf_n'], 'There should be 2 extracted chemicals \
        with weight fraction data associated with ethylparaben')

    def test_dtxsid_products_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)

        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(0, ethylparaben_stats['products_n'], 'There should be 0 products \
        associated with ethylparaben')
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['products_n'], 'There should now be 1 product \
        associated with ethylparaben')

    def test_habits_and_practices_cards(self):
        data = {'puc': ['2']}
        response = self.client.post('/get_data/', data=data)
        for hnp in [b'ball bearings',
                    b'motorcycle',
                    b'vitamin a&amp;d',
                    b'dish soap']:
            self.assertIn(hnp, response.content)

    def test_download_pucs_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download PUCs')

    def test_download_raw_chem_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download Uncurated Chemicals')
        # Pick one curated and one non-curated RawChem record, and
        # confirm that the downloaded file excludes and includes them,
        # respectively.

        rc = RawChem.objects.filter(dsstox_id__isnull=True).first()
        response = self.client.get('/dl_raw_chems/')
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The non-curated row should appear')
        # The downloaded file should include the data group id of each uncurated chemical
        rc_row = f'%s,%s,%s,%s,%s\r\n' % (rc.extracted_text.data_document.data_group.id,
                                          rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The data group id should be in the output')

        rc = RawChem.objects.filter(dsstox_id__isnull=False).first()
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.sid if rc.sid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertNotIn(rc_row, response.content,
                         'The curated row should not appear')


/n/n/n/dashboard/tests/integration/test_browser_edits.py/n/nfrom lxml import html

from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.staticfiles.testing import StaticLiveServerTestCase

from dashboard.models import *
from selenium import webdriver
from django.conf import settings
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

            # Data Document Detail Page
            doc_detail_link = f'/datadocument/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_detail_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            doc_type_select.select_by_visible_text(""MSDS"")
            self.assertIn(doc_detail_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note
        5. Approve 
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781,  # Chemical Presence List
                       354783,  # HHE Report
                       ]:
            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()

            # Modify the first raw_chem_name field's value
            #
            raw_chem = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_chem_name""]')
            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')
            # print(rawchem_id)

            raw_chem_name_field.send_keys(' edited')
            # save changes
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            # print(et.data_document.data_group.group_type)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm validation failure
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            # The QA notes field should be invalid
            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_qa_notes""]')
            self.assertIn('is-invalid', qa_notes_field.get_attribute('class'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')

    def test_datadoc_add_extracted(self):
        '''
        Test that when a datadocument has no ExtractedText,
        the user can add one in the browser
        1. 
        '''

        for doc_id in [155324   # CO record with no ExtractedText
                       ]:
            # QA Page
            dd_url = self.live_server_url + f'/datadocument/{doc_id}/'
            self.browser.get(dd_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]').click()

            # Verify that the modal window appears by finding the Cancel button
            # The modal window does not immediately appear, so the browser
            # should wait for the button to be clickable
            wait = WebDriverWait(self.browser, 10)
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            self.assertEqual(""Cancel"", cancel_button.text,
                             'The Cancel button should say Cancel')
            cancel_button.click()
            # Verify that no ExtractedText record was created
            self.assertEqual(0, ExtractedText.objects.filter(
                data_document_id=doc_id).count(),
                ""the count of ExtractedText records related to the \
                data document should be zero"")

            # Wait for the modal div to disappear
            edit_modal = wait.until(
                ec.invisibility_of_element(
                    (By.XPATH, '//*[@id=""extextModal""]')
                )
            )
            # Click the Add button again to reopen the editor
            add_button = self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]')
            add_button.click()
            # Once again, check that the controls on the modal form are clickable
            # before trying to interact with them
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            prod_name_box = self.browser.find_element_by_id(
                'id_prod_name')
            # Add a prod_name value to the box
            prod_name_box.send_keys('Fake Product')
            save_button = self.browser.find_element_by_id(
                'extracted-text-modal-save')
            save_button.click()
            # Confirm the presence of the new ExtractedText record
            et = ExtractedText.objects.get(data_document_id=doc_id)
            self.assertEqual('Fake Product', et.prod_name,
                             ""The prod_name of the new object should match what was entered"")
            
/n/n/n/dashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django import forms
from django.db import connection
from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, Subquery, OuterRef, F, Sum
from django.forms.models import model_to_dict

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp

def download_raw_chems(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""uncurated_chemicals_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['data_group_id', 'dashboard_rawchem_id', 'raw_cas', 'raw_chem_name', 'rid'])
    for rawchem in RawChem.objects.filter(dsstox_id=None):
        writer.writerow([rawchem.extracted_text.data_document.data_group.id, rawchem.id, rawchem.raw_cas, rawchem.raw_chem_name, rawchem.rid if rawchem.rid else '' ])

    return response
/n/n/n/dashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import timezone, safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.forms import ModelForm
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, 
                            BulkProductPUCForm, BulkProductTagForm, 
                            BulkPUCForm, ProductForm)

from taggit.forms import TagField
from taggit_labels.widgets import LabelWidget
from django.core.paginator import Paginator
from django.db.models import Max

class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]

class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))
    class Meta:
        model = Product
        fields = ['tags']
    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self


@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs

    return render(request, template_name, {'data_sources': data_sources})

@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})

@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})

@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})

@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)

@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})

@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter( Q(title__icontains=q) | Q(brand_name__icontains=q) ).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})

@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })

@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')

@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/n",1
66,66,68ecc69ce1d4fb5bf76bf7997ba30d0523ee1118,"gitmate/settings.py/n/n""""""
Django settings for the GitMate project.

Generated by 'django-admin startproject' using Django 1.9.7.

For more information on this file, see
https://docs.djangoproject.com/en/1.9/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.9/ref/settings/
""""""

from ast import literal_eval
import os

import djcelery

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/1.9/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = os.environ.get('DJANGO_SECRET_KEY',
                            ('s#x)wcdigpbgi=7nxrbqbd&$yri@2k9bs%v@'
                             '*szo#&)c=qp+3-'))

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = literal_eval(os.environ.get('DJANGO_DEBUG', 'False'))
if DEBUG and not literal_eval(os.environ.get('FORCE_CELERY',
                                             'False')):  # pragma: nocover
    # let celery invoke all tasks locally
    CELERY_ALWAYS_EAGER = True
    # make celery raise exceptions when something fails
    CELERY_EAGER_PROPAGATES_EXCEPTIONS = True

HOOK_DOMAIN = os.environ.get('HOOK_DOMAIN', 'localhost:8000')

# django>=1.11 requires tests to use allowed hosts
ALLOWED_HOSTS = ['testing.com', 'localhost', '127.0.0.1', 'localhost:4200',
                 HOOK_DOMAIN]
ALLOWED_HOSTS += os.environ.get('DJANGO_ALLOWED_HOSTS', '').split()
CORS_ORIGIN_WHITELIST = ALLOWED_HOSTS
CORS_ALLOW_CREDENTIALS = True

GITMATE_PLUGINS = [
    'code_analysis',
    'welcome_commenter',
    'auto_label_pending_or_wip',
    'pr_size_labeller',
    'issue_labeller',
    'bug_spotter',
    'ack',
]

# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'social_django',
    'gitmate_config',
    'djcelery',
    'rest_framework',
    'rest_framework_docs',
    'corsheaders',
    'db_mutex',
    'coala_online',
] + ['gitmate_'+plugin for plugin in GITMATE_PLUGINS]

REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': (
        'rest_framework.authentication.BasicAuthentication',
        'rest_framework.authentication.SessionAuthentication'
    )
}

SOCIAL_AUTH_URL_NAMESPACE = 'auth'

# python-social-auth settings
SOCIAL_AUTH_LOGIN_REDIRECT_URL = os.environ.get(
    'SOCIAL_AUTH_REDIRECT', 'http://localhost:4200') + '/repositories'
SOCIAL_AUTH_LOGIN_URL = '/login'

SOCIAL_AUTH_PIPELINE = (
    # Get the information we can about the user and return it in a simple
    # format to create the user instance later. On some cases the details are
    # already part of the auth response from the provider, but sometimes this
    # could hit a provider API.
    'social.pipeline.social_auth.social_details',

    # Get the social uid from whichever service we're authing thru. The uid is
    # the unique identifier of the given user in the provider.
    'social.pipeline.social_auth.social_uid',

    # Verifies that the current auth process is valid within the current
    # project, this is were emails and domains whitelists are applied (if
    # defined).
    'social.pipeline.social_auth.auth_allowed',

    # Checks if the current social-account is already associated in the site.
    'social.pipeline.social_auth.social_user',

    # Make up a username for this person, appends a random string at the end if
    # there's any collision.
    'social.pipeline.user.get_username',

    # Send a validation email to the user to verify its email address.
    # Disabled by default.
    # 'social.pipeline.mail.mail_validation',

    # Associates the current social details with another user account with
    # a similar email address. Disabled by default.
    'social.pipeline.social_auth.associate_by_email',

    # Create a user account if we haven't found one yet.
    'social.pipeline.user.create_user',

    # Create the record that associated the social account with this user.
    'social.pipeline.social_auth.associate_user',

    # Populate the extra_data field in the social record with the values
    # specified by settings (and the default ones like access_token, etc).
    'social.pipeline.social_auth.load_extra_data',

    # Update the user record with any changed info from the auth service.
    'social.pipeline.user.user_details',
)

# Put gitmate's corresponding OAuth details here.
WEBHOOK_SECRET = os.environ.get('WEBHOOK_SECRET')
SOCIAL_AUTH_GITHUB_KEY = os.environ.get('SOCIAL_AUTH_GITHUB_KEY')
SOCIAL_AUTH_GITHUB_SECRET = os.environ.get('SOCIAL_AUTH_GITHUB_SECRET')
SOCIAL_AUTH_GITHUB_SCOPE = [
    'admin:repo_hook',
    'repo',
]

SOCIAL_AUTH_GITLAB_KEY = os.environ.get(
    'SOCIAL_AUTH_GITLAB_KEY')
SOCIAL_AUTH_GITLAB_SECRET = os.environ.get('SOCIAL_AUTH_GITLAB_SECRET')
# This needs to be specified as is including full domain name and protocol.
# Be extra careful and use the same URL used while registering the application
# on GitLab. ex. example.com/auth/complete/gitlab/
SOCIAL_AUTH_GITLAB_REDIRECT_URL = os.environ.get(
    'SOCIAL_AUTH_GITLAB_REDIRECT_URL')
SOCIAL_AUTH_GITLAB_SCOPE = ['api']

SOCIAL_AUTH_BITBUCKET_KEY = os.environ.get('SOCIAL_AUTH_BITBUCKET_KEY')
SOCIAL_AUTH_BITBUCKET_SECRET = os.environ.get('SOCIAL_AUTH_BITBUCKET_SECRET')

AUTHENTICATION_BACKENDS = (
    'social_core.backends.github.GithubOAuth2',
    'social_core.backends.gitlab.GitLabOAuth2',
    'social_core.backends.bitbucket.BitbucketOAuth',
    'django.contrib.auth.backends.ModelBackend'
)

MIDDLEWARE_CLASSES = [
    'corsheaders.middleware.CorsMiddleware',
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'gitmate.disable_csrf.DisableCSRF',
]

ROOT_URLCONF = 'gitmate.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'gitmate.wsgi.application'


# Database
# https://docs.djangoproject.com/en/1.9/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': os.environ.get('DB_NAME', 'postgres'),
        'USER': os.environ.get('DB_USER', 'postgres'),
        'PASSWORD': os.environ.get('DB_PASSWORD', ''),
        'HOST': os.environ.get('DB_ADDRESS', ''),
        'PORT': os.environ.get('DB_PORT', '')
    }
}


# Password validation
# https://docs.djangoproject.com/en/1.9/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/1.9/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/1.9/howto/static-files/
STATIC_ROOT = os.environ.get('DJANGO_STATIC_ROOT',
                             os.path.join(BASE_DIR, 'static'))
STATIC_URL = '/static/'
STATICFILES_DIRS = ()


# CELERY CONFIG
djcelery.setup_loader()

# RABBITMQ server base URL
BROKER_URL = os.environ.get('CELERY_BROKER_URL',
                            'amqp://admin:password@rabbit/')
/n/n/n",0
67,67,68ecc69ce1d4fb5bf76bf7997ba30d0523ee1118,"/gitmate/settings.py/n/n""""""
Django settings for the GitMate project.

Generated by 'django-admin startproject' using Django 1.9.7.

For more information on this file, see
https://docs.djangoproject.com/en/1.9/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.9/ref/settings/
""""""

from ast import literal_eval
import os

import djcelery

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/1.9/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = os.environ.get('DJANGO_SECRET_KEY',
                            ('s#x)wcdigpbgi=7nxrbqbd&$yri@2k9bs%v@'
                             '*szo#&)c=qp+3-'))

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = literal_eval(os.environ.get('DJANGO_DEBUG', 'False'))
if DEBUG and not literal_eval(os.environ.get('FORCE_CELERY',
                                             'False')):  # pragma: nocover
    # let celery invoke all tasks locally
    CELERY_ALWAYS_EAGER = True
    # make celery raise exceptions when something fails
    CELERY_EAGER_PROPAGATES_EXCEPTIONS = True

HOOK_DOMAIN = os.environ.get('HOOK_DOMAIN', 'localhost:8000')

# django>=1.11 requires tests to use allowed hosts
ALLOWED_HOSTS = ['testing.com', 'localhost', '127.0.0.1', 'localhost:4200',
                 HOOK_DOMAIN]
ALLOWED_HOSTS += os.environ.get('DJANGO_ALLOWED_HOSTS', '').split()
CORS_ORIGIN_WHITELIST = ALLOWED_HOSTS
CORS_ALLOW_CREDENTIALS = True

GITMATE_PLUGINS = [
    'code_analysis',
    'welcome_commenter',
    'auto_label_pending_or_wip',
    'pr_size_labeller',
    'issue_labeller',
    'bug_spotter',
    'ack',
]

# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'social_django',
    'gitmate_config',
    'djcelery',
    'rest_framework',
    'rest_framework_docs',
    'corsheaders',
    'db_mutex',
    'coala_online',
] + ['gitmate_'+plugin for plugin in GITMATE_PLUGINS]

REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': (
        'rest_framework.authentication.BasicAuthentication',
        'rest_framework.authentication.SessionAuthentication'
    )
}

SOCIAL_AUTH_URL_NAMESPACE = 'auth'

# python-social-auth settings
SOCIAL_AUTH_LOGIN_REDIRECT_URL = os.environ.get('SOCIAL_AUTH_REDIRECT',
                                                'http://localhost:4200')
SOCIAL_AUTH_LOGIN_URL = '/login'

SOCIAL_AUTH_PIPELINE = (
    # Get the information we can about the user and return it in a simple
    # format to create the user instance later. On some cases the details are
    # already part of the auth response from the provider, but sometimes this
    # could hit a provider API.
    'social.pipeline.social_auth.social_details',

    # Get the social uid from whichever service we're authing thru. The uid is
    # the unique identifier of the given user in the provider.
    'social.pipeline.social_auth.social_uid',

    # Verifies that the current auth process is valid within the current
    # project, this is were emails and domains whitelists are applied (if
    # defined).
    'social.pipeline.social_auth.auth_allowed',

    # Checks if the current social-account is already associated in the site.
    'social.pipeline.social_auth.social_user',

    # Make up a username for this person, appends a random string at the end if
    # there's any collision.
    'social.pipeline.user.get_username',

    # Send a validation email to the user to verify its email address.
    # Disabled by default.
    # 'social.pipeline.mail.mail_validation',

    # Associates the current social details with another user account with
    # a similar email address. Disabled by default.
    'social.pipeline.social_auth.associate_by_email',

    # Create a user account if we haven't found one yet.
    'social.pipeline.user.create_user',

    # Create the record that associated the social account with this user.
    'social.pipeline.social_auth.associate_user',

    # Populate the extra_data field in the social record with the values
    # specified by settings (and the default ones like access_token, etc).
    'social.pipeline.social_auth.load_extra_data',

    # Update the user record with any changed info from the auth service.
    'social.pipeline.user.user_details',
)

# Put gitmate's corresponding OAuth details here.
WEBHOOK_SECRET = os.environ.get('WEBHOOK_SECRET')
SOCIAL_AUTH_GITHUB_KEY = os.environ.get('SOCIAL_AUTH_GITHUB_KEY')
SOCIAL_AUTH_GITHUB_SECRET = os.environ.get('SOCIAL_AUTH_GITHUB_SECRET')
SOCIAL_AUTH_GITHUB_SCOPE = [
    'admin:repo_hook',
    'repo',
]

SOCIAL_AUTH_GITLAB_KEY = os.environ.get(
    'SOCIAL_AUTH_GITLAB_KEY')
SOCIAL_AUTH_GITLAB_SECRET = os.environ.get('SOCIAL_AUTH_GITLAB_SECRET')
# This needs to be specified as is including full domain name and protocol.
# Be extra careful and use the same URL used while registering the application
# on GitLab. ex. example.com/auth/complete/gitlab/
SOCIAL_AUTH_GITLAB_REDIRECT_URL = os.environ.get(
    'SOCIAL_AUTH_GITLAB_REDIRECT_URL')
SOCIAL_AUTH_GITLAB_SCOPE = ['api']

SOCIAL_AUTH_BITBUCKET_KEY = os.environ.get('SOCIAL_AUTH_BITBUCKET_KEY')
SOCIAL_AUTH_BITBUCKET_SECRET = os.environ.get('SOCIAL_AUTH_BITBUCKET_SECRET')

AUTHENTICATION_BACKENDS = (
    'social_core.backends.github.GithubOAuth2',
    'social_core.backends.gitlab.GitLabOAuth2',
    'social_core.backends.bitbucket.BitbucketOAuth',
    'django.contrib.auth.backends.ModelBackend'
)

MIDDLEWARE_CLASSES = [
    'corsheaders.middleware.CorsMiddleware',
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'gitmate.disable_csrf.DisableCSRF',
]

ROOT_URLCONF = 'gitmate.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'gitmate.wsgi.application'


# Database
# https://docs.djangoproject.com/en/1.9/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2',
        'NAME': os.environ.get('DB_NAME', 'postgres'),
        'USER': os.environ.get('DB_USER', 'postgres'),
        'PASSWORD': os.environ.get('DB_PASSWORD', ''),
        'HOST': os.environ.get('DB_ADDRESS', ''),
        'PORT': os.environ.get('DB_PORT', '')
    }
}


# Password validation
# https://docs.djangoproject.com/en/1.9/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.'
                'NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/1.9/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/1.9/howto/static-files/
STATIC_ROOT = os.environ.get('DJANGO_STATIC_ROOT',
                             os.path.join(BASE_DIR, 'static'))
STATIC_URL = '/static/'
STATICFILES_DIRS = ()


# CELERY CONFIG
djcelery.setup_loader()

# RABBITMQ server base URL
BROKER_URL = os.environ.get('CELERY_BROKER_URL',
                            'amqp://admin:password@rabbit/')
/n/n/n",1
80,80,a771b17caf63a406bb0cb4d142d70eb5425e0428,"dashboard/admin.py/n/nfrom django.contrib import admin
from dashboard.models import *
from django.db.models import Count

from django import forms
from taggit_labels.widgets import LabelWidget
from dashboard.signals import *

class PUCAdminForm(forms.ModelForm):
    class Meta:
        model = PUC
        fields = ['gen_cat', 'prod_fam', 'prod_type', 'description','tags','kind']
        readonly_fields = ('num_products',)
        widgets = {
            'tags': LabelWidget(model=PUCTag),
        }

class PUCAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'tag_list','num_products')
    list_filter = ('kind',)
    form = PUCAdminForm
    def get_changeform_initial_data(self, request):
        get_data = super(PUCAdmin, self).get_changeform_initial_data(request)
        get_data['last_edited_by'] = request.user.pk
        return get_data
    def get_queryset(self, request):
        return super(PUCAdmin, self).get_queryset(request).prefetch_related('tags').annotate(num_products=Count('products'))
    def num_products(self, obj):
        return obj.num_products
    num_products.short_description = 'Product Count'
    num_products.admin_order_field = 'num_products'
    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())

class HHDocAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'hhe_report_number')

class ScriptForm(forms.ModelForm):
    class Meta(object):
        model = Script
        fields = '__all__'

    def __init__(self, *args, **kwargs):
        super(ScriptForm, self).__init__(*args, **kwargs)
        if self.instance and self.instance.pk and not self.instance.script_type == 'EX':
            # Since the pk is set this is not a new instance
            self.fields['confidence'].widget = forms.HiddenInput()

class ScriptAdmin(admin.ModelAdmin):
    list_filter = ('script_type',)
    list_display = ('__str__','confidence_level')
    form = ScriptForm
    def confidence_level(self, obj):
        if obj.script_type == 'EX':
            return obj.confidence
        else:
            return ''

class PUCToTagAdmin(admin.ModelAdmin):
    list_display = ('content_object', 'tag', 'assumed')
    list_filter = ('tag',)
    def tag(self, obj):
        return obj.tag    
    def assumed(self, obj):
        return obj.assumed 

# Register your models here.
admin.site.register(DataSource)
admin.site.register(GroupType)
admin.site.register(DataGroup)
admin.site.register(DocumentType)
admin.site.register(DataDocument)
admin.site.register(Script, ScriptAdmin)
admin.site.register(Product)
admin.site.register(ProductToPUC)
admin.site.register(ProductDocument)
admin.site.register(SourceCategory)
admin.site.register(PUC, PUCAdmin)
admin.site.register(ExtractedText)
admin.site.register(ExtractedChemical)
admin.site.register(ExtractedFunctionalUse)
admin.site.register(ExtractedHabitsAndPractices)
admin.site.register(DSSToxLookup)
admin.site.register(QAGroup)
admin.site.register(UnitType)
admin.site.register(WeightFractionType)
admin.site.register(PUCTag) #,ProductTagAdmin
admin.site.register(Taxonomy)
admin.site.register(TaxonomySource)
admin.site.register(TaxonomyToPUC)
admin.site.register(ExtractedHHDoc, HHDocAdmin)
admin.site.register(ExtractedHHRec)
admin.site.register(PUCToTag, PUCToTagAdmin)
/n/n/ndashboard/forms.py/n/nfrom dal import autocomplete
from bootstrap_datepicker_plus import DatePickerInput

from django import forms
from django.forms import BaseInlineFormSet

from django.utils.translation import ugettext_lazy as _

from dashboard.models import *
from django.db.models import F
from dashboard.utils import get_extracted_models


class DataGroupForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = DataGroup
        fields = ['name', 'description', 'url', 'group_type', 'downloaded_by',
                  'downloaded_at', 'download_script', 'data_source', 'csv']
        widgets = {'downloaded_at': DatePickerInput()}
        labels = {'csv': _('Register Records CSV File'),
                  'url': _('URL'), }

    def __init__(self, *args, **kwargs):
        qs = Script.objects.filter(script_type='DL')
        self.user = kwargs.pop('user', None)
        super(DataGroupForm, self).__init__(*args, **kwargs)
        self.fields['csv'].widget.attrs.update({'accept': '.csv'})
        self.fields['download_script'].queryset = qs


class ExtractionScriptForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='EX'),
        label=""Extraction Script"")
    weight_fraction_type = forms.ModelChoiceField(
        queryset=WeightFractionType.objects.all(),
        label=""Weight Fraction Type"",
        initial=""1"")
    extract_file = forms.FileField(label=""Extracted Text CSV File"")

    def __init__(self, *args, **kwargs):
        self.dg_type = kwargs.pop('dg_type', 0)
        self.user = kwargs.pop('user', None)
        super(ExtractionScriptForm, self).__init__(*args, **kwargs)
        self.fields['weight_fraction_type'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['extract_file'].widget.attrs.update({'accept': '.csv'})
        if self.dg_type in ['FU', 'CP']:
            del self.fields['weight_fraction_type']
        self.collapsed = True


class CleanCompDataForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='DC'),
        label=""Data Cleaning Script"",
        required=True)
    clean_comp_data_file = forms.FileField(label=""Clean Composition Data CSV File"",
                                           required=True)

    def __init__(self, *args, **kwargs):
        super(CleanCompDataForm, self).__init__(*args, **kwargs)
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['clean_comp_data_file'].widget.attrs.update(
            {'accept': '.csv'})
        self.collapsed = True


class DataSourceForm(forms.ModelForm):
    required_css_class = 'required'

    class Meta:
        model = DataSource
        fields = ['title', 'url', 'estimated_records', 'state', 'priority',
                  'description']


class PriorityForm(forms.ModelForm):
    class Meta:
        model = DataSource
        fields = ['priority']

    def __init__(self, *args, **kwargs):
        super(PriorityForm, self).__init__(*args, **kwargs)
        self.fields['priority'].label = ''
        self.fields['priority'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


class QANotesForm(forms.ModelForm):
    class Meta:
        model = QANotes
        fields = ['qa_notes']
        widgets = {
            'qa_notes': forms.Textarea,
        }
        labels = {
            'qa_notes': _('QA Notes (required if approving edited records)'),
        }


class ExtractedTextQAForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'data_document', 'qa_checked']


class ProductLinkForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    document_type = forms.ModelChoiceField(
        queryset=DocumentType.objects.all(),
        label=""Data Document Type"",
        required=True)
    return_url = forms.CharField()

    class Meta:
        model = Product
        fields = ['title', 'manufacturer',
                  'brand_name', 'upc', 'size', 'color']

    def __init__(self, *args, **kwargs):
        super(ProductLinkForm, self).__init__(*args, **kwargs)
        self.fields['return_url'].widget = forms.HiddenInput()


class ProductForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = Product
        fields = ['title', 'manufacturer', 'brand_name', 'size', 'color',
                  'model_number', 'short_description', 'long_description']


class ProductViewForm(ProductForm):
    class Meta(ProductForm.Meta):
        exclude = ('title', 'long_description',)

    def __init__(self, *args, **kwargs):
        super(ProductForm, self).__init__(*args, **kwargs)
        for f in self.fields:
            self.fields[f].disabled = True


class BasePUCForm(forms.ModelForm):
    puc = forms.ModelChoiceField(
        queryset=PUC.objects.all(),
        label='Category',
        widget=autocomplete.ModelSelect2(
            url='puc-autocomplete',
            attrs={'data-minimum-input-length': 3, })
    )


class ProductPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']


class HabitsPUCForm(BasePUCForm):
    class Meta:
        model = ExtractedHabitsAndPracticesToPUC
        fields = ['puc']


class BulkProductPUCForm(forms.ModelForm):
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput(),
                             required=True)

    class Meta:
        model = ProductToPUC
        fields = ['puc', 'id_pks']


class BulkPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']

    def __init__(self, *args, **kwargs):
        super(BulkPUCForm, self).__init__(*args, **kwargs)
        lbl = 'Select PUC for Attribute to Assign to Selected Products'
        self.fields['puc'].label = lbl
        self.fields['puc'].widget.attrs['onchange'] = 'form.submit();'


class BulkProductTagForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    tag = forms.ModelChoiceField(queryset=PUCTag.objects.none(),
                                 label='Attribute')
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput())

    class Meta:
        model = ProductToPUC
        fields = ['tag', 'id_pks']

    def __init__(self, *args, **kwargs):
        super(BulkProductTagForm, self).__init__(*args, **kwargs)
        lbl = 'Select Attribute to Assign to Selected Products'
        self.fields['tag'].label = lbl


class ExtractedTextForm(forms.ModelForm):
    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'doc_date', 'rev_num']

        widgets = {
            'data_document': forms.HiddenInput(),
            'extraction_script': forms.HiddenInput(),
        }


class ExtractedCPCatForm(ExtractedTextForm):

    class Meta:
        model = ExtractedCPCat
        fields = ['doc_date', 'cat_code',
                  'description_cpcat', 'cpcat_sourcetype']


class ExtractedCPCatEditForm(ExtractedCPCatForm):

    class Meta(ExtractedCPCatForm.Meta):
        fields = ExtractedCPCatForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num', 'cpcat_code']


class ExtractedHHDocForm(ExtractedTextForm):

    class Meta:
        model = ExtractedHHDoc
        fields = ['hhe_report_number', 'study_location', 'naics_code', 'sampling_date', 'population_gender',
                  'population_age', 'population_other', 'occupation', 'facility']


class ExtractedHHDocEditForm(ExtractedHHDocForm):

    class Meta(ExtractedHHDocForm.Meta):
        fields = ExtractedHHDocForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num']


class DocumentTypeForm(forms.ModelForm):
    class Meta:
        model = DataDocument
        fields = ['document_type']

    def __init__(self, *args, **kwargs):
        super(DocumentTypeForm, self).__init__(*args, **kwargs)
        self.fields['document_type'].label = ''
        self.fields['document_type'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


def include_extract_form(dg):
    '''Returns the ExtractionScriptForm based on conditions of DataGroup
    type as well as whether all records are matched, but not extracted
    '''
    if not dg.type in ['FU', 'CO', 'CP']:
        return False
    if dg.all_matched() and not dg.all_extracted():
        return ExtractionScriptForm(dg_type=dg.type)
    else:
        return False


class ExtractedChemicalFormSet(BaseInlineFormSet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


class ExtractedChemicalForm(forms.ModelForm):
    def __init__(self, *args, **kwargs):
        super(ExtractedChemicalForm, self).__init__(*args, **kwargs)
        # the non-field properties need to be explicitly added
        if hasattr(self.instance, 'dsstox') and self.instance.dsstox is not None:
            self.fields['true_cas'] = forms.CharField(max_length=200)
            self.fields['true_cas'].initial = self.instance.dsstox.true_cas
            self.fields['true_cas'].disabled = True
            self.fields['true_chemname'] = forms.CharField(max_length=400)
            self.fields['true_chemname'].initial = self.instance.dsstox.true_chemname
            self.fields['true_chemname'].disabled = True
            self.fields['SID'] = forms.CharField(max_length=50)
            self.fields['SID'].initial = self.instance.dsstox.sid
            self.fields['SID'].disabled = True

    class Meta:
        model = ExtractedChemical
        fields = '__all__'


def include_clean_comp_data_form(dg):
    '''Returns the CleanCompDataForm based on conditions of DataGroup
    type = Composition and at least 1 document extracted
    '''
    if not dg.type in ['CO']:
        return False
    if dg.extracted_docs() > 0:
        return CleanCompDataForm()
    else:
        return False


def create_detail_formset(document, extra=1, can_delete=False, exclude=[]):
    '''Returns the pair of formsets that will be needed based on group_type.
    .                       ('CO'),('CP'),('FU'),('HP'),('HH')
    Parameters
        ----------
        document : DataDocument
            The parent DataDocument
        extra : integer
            How many empty forms should be created for new records
        can_delete : boolean
            whether a delete checkbox is included
        exclude : list
            which fields to leave out of the form
    .

    '''
    group_type = document.data_group.type
    parent, child = get_extracted_models(group_type)
    extracted = hasattr(document, 'extractedtext')

    def make_formset(parent_model, model,
                     formset=BaseInlineFormSet,
                     form=forms.ModelForm,
                     exclude=exclude):
        formset_fields = model.detail_fields()
        if exclude:
            formset_fields = [in_field for in_field in formset_fields if not in_field in exclude]
        return forms.inlineformset_factory(parent_model=parent_model,
                                           model=model,
                                           fields=formset_fields,
                                           formset=formset,  # this specifies a custom formset
                                           form=form,
                                           extra=extra,
                                           can_delete=can_delete)

    def one():  # for chemicals or unknown
        ChemicalFormSet = make_formset(
            parent_model=parent,
            model=child,
            formset=ExtractedChemicalFormSet,
            form=ExtractedChemicalForm
        )
        return (ExtractedTextForm, ChemicalFormSet)

    def two():  # for functional_use
        FunctionalUseFormSet = make_formset(parent, child)
        return (ExtractedTextForm, FunctionalUseFormSet)

    def three():  # for habits_and_practices
        HnPFormSet = make_formset(parent, child)
        return (ExtractedTextForm, HnPFormSet)

    def four():  # for extracted_list_presence
        ListPresenceFormSet = make_formset(parent, child)
        ParentForm = ExtractedCPCatForm if extracted else ExtractedCPCatEditForm


        return (ParentForm, ListPresenceFormSet)

    def five():  # for extracted_hh_rec
        HHFormSet = make_formset(parent, child)
        ParentForm = ExtractedHHDocForm if extracted else ExtractedHHDocEditForm
        return (ParentForm, HHFormSet)
    dg_types = {
        'CO': one,
        'UN': one,
        'FU': two,
        'HP': three,
        'CP': four,
        'HH': five,
    }
    func = dg_types.get(group_type, lambda: None)
    return func()
/n/n/ndashboard/migrations/0096_puc_kind.py/n/n# Generated by Django 2.1.7 on 2019-03-14 06:12
import taggit.managers
from datetime import datetime

from django.db import migrations, models


def populate_category_field(apps, schema_editor):
    '''
    populate the category field with the appropriate choice based upon the date.
    '''
    parting_day = datetime(2019,1,1)
    PUC = apps.get_model('dashboard', 'PUC')
    for puc in PUC.objects.all():
        if puc.created_at < parting_day:
            puc.kind = 'FO'
        else:
            puc.kind = 'OC'    
        puc.save()

class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0095_dd_url_and_raw_category_bigger'),
    ]

    operations = [
        migrations.AddField(
            model_name='puc',
            name='kind',
            field=models.CharField(blank=True, choices=[('UN', 'unknown'), ('FO', 'formulations'), ('AR', 'articles'), ('OC', 'occupational')], default='UN', max_length=2),
        ),
        migrations.AlterField(
            model_name='puc',
            name='tags',
            field=taggit.managers.TaggableManager(blank=True, help_text='A set of PUC Attributes applicable to this PUC', through='dashboard.PUCToTag', to='dashboard.PUCTag', verbose_name='Tags'),
        ),
        migrations.RunPython(populate_category_field,
                                reverse_code=migrations.RunPython.noop),
    ]
/n/n/ndashboard/migrations/0097_script_confidence.py/n/n# Generated by Django 2.1.7 on 2019-03-15 14:50

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0096_puc_kind'),
    ]

    operations = [
        migrations.AddField(
            model_name='script',
            name='confidence',
            field=models.PositiveSmallIntegerField(blank=True, default=1, validators=[django.core.validators.MaxValueValidator(100), django.core.validators.MinValueValidator(1)], verbose_name='Confidence'),
        ),
    ]
/n/n/ndashboard/migrations/0098_add_cpcat_qa_flag.py/n/n# Generated by Django 2.1.2 on 2019-03-20 08:56

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0097_script_confidence'),
    ]

    operations = [
        migrations.AddField(
            model_name='extractedlistpresence',
            name='qa_flag',
            field=models.BooleanField(default=False),
        ),
    ]
/n/n/ndashboard/migrations/0099_qagroup_script.py/n/n# Generated by Django 2.1.2 on 2019-03-22 10:14

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0098_add_cpcat_qa_flag'),
    ]

    operations = [
        migrations.AlterField(
            model_name='qagroup',
            name='extraction_script',
            field=models.ForeignKey(limit_choices_to={'script_type': 'EX'}, on_delete=django.db.models.deletion.CASCADE, related_name='qa_group', to='dashboard.Script'),
        ),
    ]
/n/n/ndashboard/migrations/0100_extractedchemical_names.py/n/n# Generated by Django 2.1.2 on 2019-03-27 01:58

import dashboard.models.extracted_chemical
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0099_qagroup_script'),
    ]

    operations = [
        migrations.AlterField(
            model_name='extractedchemical',
            name='ingredient_rank',
            field=models.PositiveIntegerField(blank=True, null=True, validators=[dashboard.models.extracted_chemical.validate_ingredient_rank], verbose_name='Ingredient rank'),
        ),
        migrations.AlterField(
            model_name='extractedchemical',
            name='raw_central_comp',
            field=models.CharField(blank=True, max_length=100, null=True, verbose_name='Raw central composition'),
        ),
    ]
/n/n/ndashboard/migrations/0101_verbose_dsstox_names.py/n/n# Generated by Django 2.1.2 on 2019-03-28 00:05

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0100_extractedchemical_names'),
    ]

    operations = [
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='sid',
            field=models.CharField(max_length=50, unique=True, verbose_name='DTXSID'),
        ),
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='true_cas',
            field=models.CharField(blank=True, max_length=50, null=True, verbose_name='True CAS'),
        ),
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='true_chemname',
            field=models.CharField(blank=True, max_length=500, null=True, verbose_name='True chemical name'),
        ),
    ]
/n/n/ndashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo
from .extracted_habits_and_practices_to_puc import (
                                            ExtractedHabitsAndPracticesToPUC)
from .extracted_habits_and_practices import ExtractedHabitsAndPractices


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.content_object)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/ndashboard/models/data_document.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.utils import timezone
from .document_type import DocumentType
from django.core.exceptions import ValidationError


class DataDocument(CommonInfo):

    filename = models.CharField(max_length=255)
    title = models.CharField(max_length=255)
    url = models.CharField(null=True, blank=True, max_length=275)
    raw_category = models.CharField(null=True, blank=True, max_length=100)
    data_group = models.ForeignKey('DataGroup', on_delete=models.CASCADE)
    products = models.ManyToManyField('Product', through='ProductDocument')
    matched = models.BooleanField(default=False)
    extracted = models.BooleanField(default=False)
    document_type = models.ForeignKey(DocumentType, on_delete=models.PROTECT,
                                                        null=True, blank=True)
    organization = models.CharField(max_length=255, blank=True)
    note = models.TextField(blank=True, null=True)

    class Meta:
        ordering = ['-id']

    def __str__(self):
        return str(self.title)

    @property
    def is_extracted(self):
        return hasattr(self,'extractedtext')

    def get_absolute_url(self):
        return reverse('data_document', kwargs={'pk': self.pk})

    def get_abstract_filename(self):
        ext = self.filename.split('.')[-1] #maybe not all are PDF??
        return f'document_{self.pk}.{ext}'

    def pdf_url(self):
        dg = self.data_group
        fn = self.get_abstract_filename()
        return f'/media/{dg.fs_id}/pdf/{fn}'

    def clean(self):
        # the document_type must be one of the children types
        # of the datadocument's parent datagroup
        this_type = self.data_group.group_type
        doc_types = DocumentType.objects.filter(group_type=this_type)
        if not self.document_type in doc_types:
            raise ValidationError(('The document type must be allowed by '
                                                    'the parent data group.'))
/n/n/ndashboard/models/data_group.py/n/nimport os
import shutil
import uuid
from factotum import settings
from pathlib import Path, PurePath

from django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.db.models.signals import pre_save
from django.dispatch import receiver
from model_utils import FieldTracker
from django.core.exceptions import ValidationError

from .group_type import GroupType
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_list_presence import ExtractedListPresence

# could be used for dynamically creating filename on instantiation
# in the 'upload_to' param on th FileField
def update_filename(instance, filename):
    name_fill_space = instance.name.replace(' ', '_')
    # potential space errors in name
    name = '{0}/{0}_{1}'.format(name_fill_space, filename)
    return name


def csv_upload_path(instance, filename):
    # potential space errors in name
    name = '{0}/{1}'.format(instance.fs_id, filename)
    return name

extract_models = {
    'CO': (ExtractedText, ExtractedChemical),
    'FU': (ExtractedText, ExtractedFunctionalUse),
    'CP': (ExtractedCPCat, ExtractedListPresence)
}



class DataGroup(CommonInfo):

    name = models.CharField(max_length=50)
    description = models.TextField(null=True, blank=True)
    downloaded_by = models.ForeignKey('auth.User',
                                    on_delete=models.SET_DEFAULT, default = 1)
    downloaded_at = models.DateTimeField()
    download_script = models.ForeignKey('Script',
                                    on_delete=models.SET_NULL, default=None,
                                    null=True, blank=True)
    data_source = models.ForeignKey('DataSource', on_delete=models.CASCADE)
    fs_id = models.UUIDField(default=uuid.uuid4, editable=False)
    csv = models.FileField(upload_to=csv_upload_path, null=True)
    zip_file = models.CharField(max_length=100)
    group_type = models.ForeignKey(GroupType, on_delete=models.SET_DEFAULT,
                                            default=1, null=True, blank=True)
    url = models.CharField(max_length=150, blank=True)

    tracker = FieldTracker()

    @property
    def type(self):
        return str(self.group_type.code)

    @property
    def is_composition(self):
        return self.type == 'CO'

    @property
    def is_habits_and_practices(self):
        return self.type == 'HP'

    @property
    def is_functional_use(self):
        return self.type == 'FU'

    @property
    def is_chemical_presence(self):
        return self.type == 'CP'

    @property
    def is_hh(self):
        return self.type == 'HH'


    def get_extract_models(self):
        '''returns a tuple with parent/child extract models'''
        return extract_models.get(self.type)

    def save(self, *args, **kwargs):
        super(DataGroup, self).save(*args, **kwargs)

    def matched_docs(self):
        return self.datadocument_set.filter(matched=True).count()

    def all_matched(self):
        return all(self.datadocument_set.values_list('matched', flat=True))

    def all_extracted(self):
        return all(self.datadocument_set.values_list('extracted', flat=True))

    def registered_docs(self):
        return self.datadocument_set.count()

    def extracted_docs(self):
        return self.datadocument_set.filter(extracted=True).count()

    def __str__(self):
        return self.name

    def get_absolute_url(self):
        return reverse('data_group_edit', kwargs={'pk': self.pk})

    def get_name_as_slug(self):
        return self.name.replace(' ', '_')

    def get_dg_folder(self):
        uuid_dir = f'{settings.MEDIA_ROOT}{str(self.fs_id)}'
        name_dir = f'{settings.MEDIA_ROOT}{self.get_name_as_slug()}'

        #this needs to handle missing csv files
        if bool(self.csv.name):
            # parse the media folder from the penultimate piece of csv file path
            p = PurePath(self.csv.path)
            csv_folder=p.parts[-2]
            csv_fullfolderpath   = f'{settings.MEDIA_ROOT}{csv_folder}'

        if os.path.isdir(uuid_dir):
            return uuid_dir # UUID-based folder
        elif bool(self.csv.name) and os.path.isdir(csv_fullfolderpath):
            return csv_fullfolderpath # csv path-based folder
        else:
            return 'no_folder_found'

    @property
    def dg_folder(self):
        '''This is a ""falsy"" property. If the folder cannot be found,
        dg.dg_folder evaluates to boolean False '''
        if self.get_dg_folder() != 'no_folder_found':
            return self.get_dg_folder()
        else:
            return False


    @property
    def csv_url(self):
        '''This is a ""falsy"" property. If the csv file cannot be found,
        dg.csv_url evaluates to boolean False '''
        try:
            self.csv.size
            csv_url = self.csv.url
        except ValueError:
            csv_url = False
        except:
            csv_url = False
        return csv_url


    @property
    def zip_url(self):
        '''This is a ""falsy"" property. If the zip file cannot be found,
        dg.zip_url evaluates to boolean False '''
        if self.get_zip_url()!='no_path_found':
            return(self.get_zip_url)
        else:
            return False
        

    def get_zip_url(self):
        # the path if the data group's folder was built from a UUID:
        uuid_path = f'{self.get_dg_folder()}/{str(self.fs_id)}.zip'
        # path if the data group's folder was built from old name-based method
        zip_file_path = f'{self.get_dg_folder()}/{self.get_name_as_slug()}.zip'
        if os.path.isfile(uuid_path):   # it is a newly-added data group
            zip_url = uuid_path
        elif os.path.isfile(zip_file_path): # it is a pre-UUID data group
            zip_url = zip_file_path
        else:
            zip_url = 'no_path_found'
        return zip_url


    def get_extracted_template_fieldnames(self):
        extract_fields = ['data_document_id','data_document_filename',
                            'prod_name', 'doc_date','rev_num', 'raw_category',
                            'raw_cas', 'raw_chem_name', 'report_funcuse']
        if self.type == 'FU':
            return extract_fields
        if self.type == 'CO':
            return extract_fields + ['raw_min_comp','raw_max_comp', 'unit_type',
                                        'ingredient_rank', 'raw_central_comp']
        if self.type == 'CP':
            for name in ['prod_name','rev_num','report_funcuse']:
                extract_fields.remove(name)
            return extract_fields + ['cat_code','description_cpcat',
                                    'cpcat_code','cpcat_sourcetype']

    def get_clean_comp_data_fieldnames(self):
        return ['id','lower_wf_analysis','central_wf_analysis', 'upper_wf_analysis']

    def clean_fields(self, exclude=None):
        super().clean_fields(exclude=exclude)
        if self.tracker.has_changed('group_type_id') and self.extracted_docs():
            msg = ""The Group Type may not be changed once extracted documents have been associated with the group.""
            raise ValidationError({'group_type': msg})


@receiver(models.signals.post_delete, sender=DataGroup)
def auto_delete_file_on_delete(sender, instance, **kwargs):
    """"""
    Deletes datagroup directory from filesystem
    when datagroup instance is deleted.
    """"""
    dg_folder = instance.get_dg_folder()
    if os.path.isdir(dg_folder):
        #print('deleting folder %s for data group %s'%(dg_folder, instance.pk))
        shutil.rmtree(dg_folder)
/n/n/ndashboard/models/dsstox_lookup.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse


class DSSToxLookup(CommonInfo):

    sid = models.CharField('DTXSID', max_length=50, null=False, blank=False, unique=True)
    true_cas = models.CharField('True CAS', max_length=50, null=True, blank=True)
    true_chemname = models.CharField('True chemical name', max_length=500, null=True, blank=True)

    def __str__(self):
        return self.true_chemname

    def get_absolute_url(self):
        return reverse('dsstox_lookup', kwargs={'pk': self.pk})



/n/n/ndashboard/models/extracted_chemical.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .extracted_text import ExtractedText
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .raw_chem import RawChem


def validate_ingredient_rank(value):
    if value < 1 or value > 999:
        raise ValidationError(
            (f'Quantity {value} is not allowed'), params={'value': value},)


class ExtractedChemical(CommonInfo, RawChem):

    raw_cas_old = models.CharField(
        ""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                         null=True, blank=True)
    raw_min_comp = models.CharField(""Raw minimum composition"", max_length=100,
                                    null=True, blank=True)
    raw_max_comp = models.CharField(""Raw maximum composition"", max_length=100,
                                    null=True, blank=True)
    unit_type = models.ForeignKey(UnitType, on_delete=models.PROTECT)
    report_funcuse = models.CharField(""Reported functional use"", max_length=100,
                                      null=True, blank=True)
    weight_fraction_type = models.ForeignKey(WeightFractionType,
                                             on_delete=models.PROTECT, null=True, default='1')
    ingredient_rank = models.PositiveIntegerField(""Ingredient rank"", null=True, blank=True,
                                                  validators=[validate_ingredient_rank])
    raw_central_comp = models.CharField(""Raw central composition"", max_length=100, null=True, blank=True)

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @classmethod
    def detail_fields(cls):
        return ['extracted_text', 'raw_chem_name', 'raw_cas', 'raw_min_comp', 'raw_central_comp',
                'raw_max_comp', 'unit_type', 'weight_fraction_type', 'report_funcuse',
                'ingredient_rank', 'rawchem_ptr']

    def get_datadocument_url(self):
        return self.extracted_text.data_document.get_absolute_url()

    @property
    def data_document(self):
        return self.extracted_text.data_document

    def indexing(self):
        obj = ExtractedChemicalIndex(
            meta={'id': self.id},
            chem_name=self.raw_chem_name,
            raw_cas=self.raw_cas,
            raw_chem_name=self.raw_chem_name,
            facet_model_name='Extracted Chemical',
        )
        obj.save()
        return obj.to_dict(include_meta=True)

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def true_cas(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_cas
        else:
            return None

    @property
    def true_chemname(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_chemname
        else:
            return None

    @property
    def sid(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.sid
        else:
            return None
/n/n/ndashboard/models/extracted_cpcat.py/n/nfrom django.db import models
from .extracted_text import ExtractedText


class ExtractedCPCat(ExtractedText):
    cat_code = models.CharField(""Cat Code"", max_length=100,
                                        null=True, blank=True)
    description_cpcat = models.CharField(""Description CPCat"", max_length=200,
                                        null=True, blank=True)
    cpcat_code = models.CharField(""CPCat Code"", max_length=50,
                                        null=True, blank=True)
    cpcat_sourcetype = models.CharField(""CPCat SourceType"", max_length=50,
                                        null=True, blank=True)

    def __str__(self):
        return str(self.prod_name)

    @property
    def qa_begun(self):
        return self.rawchem.select_subclasses().filter(extractedlistpresence__qa_flag=True).count() > 0/n/n/ndashboard/models/extracted_list_presence.py/n/nfrom django.db import models

from dashboard.models import CommonInfo
from .raw_chem import RawChem

class ExtractedListPresence(CommonInfo, RawChem):

    raw_cas_old = models.CharField(""Raw CAS"", max_length=100,
                                        null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                        null=True, blank=True)
    qa_flag = models.BooleanField(default=False)

    @classmethod
    def detail_fields(cls):
        return ['raw_cas','raw_chem_name']

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    def get_datadocument_url(self):
        return self.extracted_cpcat.data_document.get_absolute_url()

    def get_extractedtext(self):
        return self.extracted_cpcat.extractedtext_ptr
    
    @property
    def data_document(self):
        return self.extracted_text.data_document
/n/n/ndashboard/models/extracted_text.py/n/nfrom itertools import chain
from datetime import datetime
from model_utils.managers import InheritanceManager

from django.db import models
from django.core.exceptions import ValidationError
from django import forms
from django.urls import reverse


from .common_info import CommonInfo

    # this could potentially be used for 1:1 matching when uploading
    # coming in django v2.2!!
	# class Meta:
	# 	constraints = [
	# 		models.UniqueConstraint(fields=['prod_name','data_document'],
	# 								name='unique_assignment'),
	# 	]

class ExtractedText(CommonInfo):
    data_document = models.OneToOneField('DataDocument',on_delete=models.CASCADE,
                                                            primary_key=True)
    prod_name = models.CharField(max_length=500, null=True, blank=True)
    doc_date = models.CharField(max_length=25, null=True, blank=True)
    rev_num = models.CharField(max_length=50, null=True, blank=True)
    extraction_script = models.ForeignKey('Script', on_delete=models.CASCADE,
                                        limit_choices_to={'script_type': 'EX'})
    qa_checked = models.BooleanField(default=False, verbose_name=""QA approved"")
    qa_edited = models.BooleanField(default=False, verbose_name=""QA edited"")
    qa_approved_date = models.DateTimeField(null=True, blank=True,
                                                verbose_name=""QA approval date"")
    qa_approved_by = models.ForeignKey('auth.User', on_delete=models.SET_NULL,
                                                verbose_name = ""QA approved by"",
                                                null=True, blank=True,)
    qa_group = models.ForeignKey('QAGroup', verbose_name=""QA group"",
                                                     on_delete=models.SET_NULL,
                                                     null=True, blank=True)

    objects = InheritanceManager()


    def __str__(self):
        return str(self.data_document)

    def next_extracted_text_in_qa_group(self):
        nextid = 0
        # If the document is part of a Script-based QA Group, the 
        # next document is drawn from that group. If it is a CPCat
        # or HHE record, there is no next document
        extextnext = get_next_or_prev(ExtractedText.objects.filter(
            qa_group=self.qa_group, qa_checked=False), self, 'next')
        if extextnext:
            # Replace our item with the next one
            nextid = extextnext.pk
        if extextnext == self:
            nextid = 0
        return nextid
    
    def get_qa_index_path(self):
        """"""
        The type of data group to which the extracted text object belongs
        determines which QA index it will use.
        """"""
        group_type_code = self.data_document.data_group.group_type.code

        if group_type_code in ['CP','HH']:
            # TODO: change HH to its own path
            return reverse('qa_chemicalpresence_index')
        else:
            return reverse('qa_extractionscript_index')


    def fetch_extracted_records(self):
        return self.rawchem.all()

    def pull_out_cp(self):
        if hasattr(self, 'extractedcpcat'):
            return self.extractedcpcat
        else:
            return self

    def pull_out_hh(self):
        if hasattr(self, 'extractedhhdoc'):
            return self.extractedhhdoc
        else:
            return self

    def one_to_one_check(self, odict):
        '''
        Used in the upload of extracted text in the data_group_detail view, this
        returns a boolean to assure that there is a 1:1 relationship w/
        the Extracted{parent}, i.e. (Text/CPCat), and the DataDocument
        '''
        if hasattr(self, 'cat_code'):
            return self.cat_code != odict['cat_code']
        else:
            return self.prod_name != odict['prod_name']




def get_next_or_prev(models, item, direction):
    '''
    Returns the next or previous item of
    a query-set for 'item'.

    'models' is a query-set containing all
    items of which 'item' is a part of.

    direction is 'next' or 'prev'

    '''
    getit = False
    if direction == 'prev':
        models = models.reverse()
    for m in models:
        if getit:
            return m
        if item == m:
            getit = True
    if getit:
        # This would happen when the last
        # item made getit True
        return models[0]
    return False
/n/n/ndashboard/models/qa_group.py/n/nfrom django.db import models
from .common_info import CommonInfo
from .extracted_text import ExtractedText
from .script import Script


class QAGroup(CommonInfo):
    extraction_script = models.ForeignKey(Script,
                                    on_delete=models.CASCADE,
                                    related_name='qa_group',
                                    blank=False, null=False,
                                    limit_choices_to={'script_type': 'EX'}, )
    qa_complete = models.BooleanField(default=False)

    def __str__(self):
        return str(self.extraction_script) + '_' + str(self.pk)

    def get_approved_doc_count(self):
        return ExtractedText.objects.filter(qa_group=self,
                                            qa_checked=True).count()
/n/n/ndashboard/models/qa_notes.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from django.utils.translation import ugettext_lazy as _

from dashboard.models import ExtractedText


class QANotes(CommonInfo):
    extracted_text = models.OneToOneField(ExtractedText, on_delete=models.CASCADE)
    qa_notes = models.TextField(null=True, blank=True)

    def __str__(self):
        return 'Notes for {}'.format(self.extracted_text)

    def clean(self):
        if self.extracted_text.qa_edited and not self.qa_notes:
            raise ValidationError(
                    _('Before approving, please add a note explaining your edits to the extracted data'))
/n/n/ndashboard/models/script.py/n/nimport math
from random import shuffle

from django.db import models
from django.urls import reverse
from django.core.validators import (URLValidator, MaxValueValidator, 
                                                    MinValueValidator)

from .common_info import CommonInfo
from .data_document import DataDocument


class Script(CommonInfo):

    TYPE_CHOICES = (('DL', 'download'),
                    ('EX', 'extraction'),
                    ('PC', 'product categorization'),
                    ('DC', 'data cleaning'))

    # Specify the share of a script's ExtractedText objects that must be
    # approved in order for the script's QA sat
    QA_COMPLETE_PERCENTAGE = 0.2


    title = models.CharField(max_length=50)
    url = models.CharField(max_length  = 100,
                            null       = True,
                            blank      = True,
                            validators = [URLValidator()])
    qa_begun = models.BooleanField(default=False)
    script_type = models.CharField( max_length = 2,
                                    choices    = TYPE_CHOICES,
                                    blank      = False,
                                    default    = 'EX')
    confidence = models.PositiveSmallIntegerField('Confidence', blank=True,
                                                validators=[
                                                        MaxValueValidator(100),
                                                        MinValueValidator(1)],
                                                                default=1)

    def __str__(self):
        return str(self.title)

    def get_absolute_url(self):
        return reverse('extraction_script_edit', kwargs={'pk': self.pk})

    def get_datadocument_count(self):
        return DataDocument.objects.filter(
                extractedtext__extraction_script=self.pk).count()

    def get_qa_complete_extractedtext_count(self):
        return DataDocument.objects.filter(extractedtext__qa_checked=True,
                            extractedtext__extraction_script=self.pk).count()

    def get_pct_checked(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return ""{0:.0f}%"".format(pct)

    def get_pct_checked_numeric(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return pct

    def qa_button_text(self):
        if self.get_qa_status():
            return ""QA Complete"" 
        elif self.qa_begun:
            return ""Continue QA""
        else:
            return ""Begin QA""

    def get_qa_status(self):
        """"""
        Compare the derived percent checked against the threshold constant
        Return true when the percent checked is above the threshold
        """"""
        return self.get_pct_checked_numeric() >= self.QA_COMPLETE_PERCENTAGE * 100

    def create_qa_group(self, force_doc_id=None):
        """"""
        Creates a QA Group for the specified Script object;
        Use all the related ExtractedText records or, if there are more than 100,
        select 20% of them. 
        """"""
        from .qa_group import QAGroup
        from .extracted_text import ExtractedText
        es = self
        # Handle cases where a QA group already exists for the script
        if QAGroup.objects.filter(extraction_script = es).count() == 1:
            # This is a valid state
            return QAGroup.objects.get(extraction_script = es)
        elif QAGroup.objects.filter(extraction_script = es).count() > 1:
            # this is a failure mode induced by the system's allowing
            # duplicate QA Groups to be created for a single script
            return QAGroup.objects.filter(extraction_script = es).first()

        
        # Create a new QA Group for the ExtractionScript es
        qa_group = QAGroup.objects.create(extraction_script=es)
        # Collect all the ExtractedText object keys that are related
        # to the Script being QA'd and have not yet been checked
        doc_text_ids = list(ExtractedText.objects.filter(extraction_script=es,
                                                    qa_checked=False
                                                    ).values_list('pk',
                                                                flat=True))
        # If there are fewer than 100 related records, they make up the entire QA Group
        if len(doc_text_ids) < 100 and len(doc_text_ids) > 0:
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids)
        # Otherwise sample 20 percent
        elif len(doc_text_ids) >= 100 :
            # Otherwise sample 20% of them
            random_20 = math.ceil(len(doc_text_ids)/5)
            shuffle(doc_text_ids)  # this is used to make random selection of texts
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids[:random_20])
        else:
            # If there are no related ExtractedText records, something has gone wrong
            # Don't make a new QA Group with zero ExtractedTexts
            # print('The Script has no related ExtractedText records')
            texts = None

        # Set the qa_group attribute of each ExtractedText record to the new QA Group    
        if texts is not None:
            for text in texts:
                text.qa_group = qa_group
                text.save()

        # If the force_doc_id argument was populated, make sure it gets assigned 
        # to the new QA Group
        if force_doc_id is not None and ExtractedText.objects.filter(pk=force_doc_id).exists():
            text = ExtractedText.objects.get(pk=force_doc_id)
            text.qa_group = qa_group
            text.save()
        
        return qa_group

        
/n/n/ndashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        p = self.objects.puc
        puc_line = (p.gen_cat + ',' + p.prod_fam + ',' + p.prod_type + ',' + p.description +
                    ',' + str(p.get_level()) + ',' + str(p.product_count))
        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('gen_cat,prod_fam,prod_type,description,'
                                        'PUC_type,num_prods'))
        # check the PUC from loader
        self.assertEqual(csv_lines[1], puc_line)


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/ndashboard/tests/functional/test_datadocument_create.py/n/nfrom django.contrib.auth.models import User
from dashboard.models import DataGroup, DataDocument, GroupType, DocumentType
from django.test import RequestFactory, TestCase, Client
from dashboard.tests.loader import load_model_objects
from django.core.exceptions import ValidationError
from django.urls import resolve

import os
import io
from dashboard import views
from django.core.files.uploadedfile import (InMemoryUploadedFile,
                                            TemporaryUploadedFile)
from dashboard.tests.loader import fixtures_standard


class DDTestModel(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dd_model_with_wrong_document_type(self):
        # Choose a Composition group
        dgcomp = DataGroup.objects.filter(
            group_type__title='Composition').first()
        # Choose a document type from the wrong parent group type
        dt_fu = DocumentType.objects.filter(
            group_type__title='Functional use').first()
        dd = DataDocument.objects.create(
            filename=""some.pdf"", title=""My Document"", document_type=dt_fu, data_group=dgcomp)
        with self.assertRaises(ValidationError):
            dd.save()
            dd.full_clean()
        dt_comp = DocumentType.objects.filter(
            group_type__title='Composition').first()
        dd = DataDocument.objects.create(
            filename=""some.pdf"", title=""My Document"", document_type=dt_comp, data_group=dgcomp)
        dd.save()
        self.assertEqual(dt_comp.title, dd.document_type.title)


class DDTestUpload(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def testGoodGroupTypeInCSV(self):
        csv_string_good = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,MS,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,MS,, \n"")

        data = io.StringIO(csv_string_good)
        csv_len = len(csv_string_good)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['Composition Type Group'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        # does the datagroup in the ORM contain the new data docs?
        newdg_pk = resolve(resp.url).kwargs['pk']
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 2,
                         'There should be two new data documents')

    def testBadGroupTypeInCSV(self):
        csv_string_bad = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,9,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,4,, \n"")


        data = io.StringIO(csv_string_bad)
        csv_len = len(csv_string_bad)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['Composition Type Group'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        # the upload form should be invalid
        self.assertIn('CSV has bad data in row/s:'.encode(), resp.content)

    def test_upload_csv_as_datadoc(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                           ""Cal_Pesticide_Residues_1987.csv,Example Datadocument from CSV,SG,, \n""
                           )

        data = io.StringIO(csv_string)
        csv_len = len(csv_string)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['California Pesticides'],
                     'description': ['test data group'],
                     'group_type': ['6'],  # CPCat
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        newdg_pk = resolve(resp.url).kwargs['pk']

        # does the datagroup in the ORM contain the new data docs?
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 1,
                         'There should be one new data document')

        # Does the data document page include a link to the csv?  
        resp = self.client.get(f'/datadocument/%s/' % newdds[0].id)            
        # Does the response include a link to the data document's file?
        self.assertContains(resp, f'pdf/document_%s.csv' % newdds[0].id)
        
        
    def test_upload_html_as_datadoc(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                           ""alberto_balsam_conditioner_antioxidant_blueberry.html,Example Datadocument from HTML,ID,, \n""
                           )

        data = io.StringIO(csv_string)
        csv_len = len(csv_string)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['California Pesticides'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        newdg_pk = resolve(resp.url).kwargs['pk']

        # does the datagroup in the ORM contain the new data docs?
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 1,
                         'There should be one new data document')

        # Does the data document page include a link to the csv?  
        resp = self.client.get(f'/datadocument/%s/' % newdds[0].id)            
        # Does the response include a link to the data document's file?
        self.assertContains(resp, f'pdf/document_%s.html' % newdds[0].id)
/n/n/ndashboard/tests/functional/test_datadocument_detail.py/n/nfrom lxml import html

from django.test import Client
from django.urls import reverse
from django.test import TestCase, override_settings
from django.core.exceptions import ObjectDoesNotExist

from dashboard.forms import *
from factotum.settings import EXTRA
from dashboard.tests.loader import *


@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_absent_extracted_text(self):
        # Check every data document and confirm that its detail page loads,
        # with or without a detail formset
        for dd in DataDocument.objects.all():
            ddid = dd.id
            resp = self.client.get('/datadocument/%s/' % ddid)
            self.assertEqual(resp.status_code, 200, 'The page must return a 200 status code')
            try:
                extracted_text = ExtractedText.objects.get(data_document=dd)
            except ExtractedText.DoesNotExist:
                #print(dd.id)
                self.assertContains(resp, 'No Extracted Text exists for this Data Document')
            else:
                self.assertContains(resp, '<h4>Extracted Text')

    def test_script_links(self):
        doc = DataDocument.objects.first()
        #response = self.client.get(f'/datadocument/{doc.pk}/')
        response = self.client.get(f'/datadocument/179486/')
        self.assertIn('Download Script',response.content.decode('utf-8'))
        self.assertIn('Extraction Script',response.content.decode('utf-8'))

    def test_product_card_location(self):
        response = self.client.get('/datadocument/179486/')
        html = response.content.decode('utf-8')
        e_idx = html.index('<h4>Extracted Text')
        p_idx = html.index('<h4 class=""d-inline"">Products')
        self.assertTrue(p_idx > e_idx, ('Product card should come after ' 
                                        'Extracted Text card'))

    def test_product_create_link(self):
        response = self.client.get('/datadocument/167497/')
        self.assertContains(response, '/link_product_form/167497/')
        data = {'title'        : ['New Product'],
                'upc'          : ['stub_1860'],
                'document_type': [1],
                'return_url'   : ['/datadocument/167497/']}
        response = self.client.post('/link_product_form/167497/', data=data)
        self.assertRedirects(response,'/datadocument/167497/')
        response = self.client.get(response.url)
        self.assertContains(response, 'New Product')

    def test_product_title_duplication(self):
        response = self.client.get('/datadocument/245401/')
        self.assertContains(response, '/link_product_form/245401/')
        # Add a new Product
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9100'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9100')
        self.assertContains(response, f'product/%s' % new_product.id )

        # Add another new Product with the same title
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9101'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9101')
        self.assertContains(response, f'product/%s' % new_product.id )

    def test_add_extracted(self):
        '''Check that the user has the ability to create an extracted record
        when the document doesn't yet have an extracted record for data 
        group types 'CP' and 'HH'
        '''
        doc = DataDocument.objects.get(pk=354784)
        self.assertFalse(doc.extracted, (""This document is matched ""
                                                    ""but not extracted""))
        data = {'hhe_report_number': ['47']}
        response = self.client.post('/extractedtext/edit/354784/', data=data,
                                                            follow=True)
        doc = DataDocument.objects.get(pk=354784)
        self.assertTrue(doc.extracted, ""This document is not extracted "")
        page = html.fromstring(response.content)
        hhe_no = page.xpath('//dd[contains(@class, ""hh-report-no"")]')[0].text
        self.assertIn('47', hhe_no)


class TestDynamicDetailFormsets(TestCase):
    fixtures = fixtures_standard

    def setUp(self):

        self.client.login(username='Karyn', password='specialP@55word')

    def test_fetch_extracted_records(self):
        ''' Confirm that each detail child object returned by the fetch_extracted_records
        function has the correct parent '''
        for et in ExtractedText.objects.all():
            #print('Fetching extracted child records from %s: %s ' % (et.pk , et))
            for ex_child in et.fetch_extracted_records():
                child_model = ex_child.__class__ # the fetch_extracted_records function returns different classes
                #print('    %s: %s' % (ex_child.__class__.__name__ , ex_child ))
                self.assertEqual(et.pk , child_model.objects.get(pk=ex_child.pk).extracted_text.pk,
                    'The ExtractedChemical object with the returned child pk should have the correct extracted_text parent')

    def test_extractedsubclasses(self):
        ''' Confirm that the inheritance manager is returning appropriate
            subclass objects and ExtractedText base class objects 
         '''
        for doc in DataDocument.objects.all():
            try:
                extsub = ExtractedText.objects.get_subclass(data_document=doc)
                # A document with the CP data group type should be linked to 
                # ExtractedCPCat objects
                if doc.data_group.group_type.code=='CP':
                    #print(f'%s %s %s' % (doc.id, extsub, type(extsub)))
                    self.assertEqual(type(extsub) , ExtractedCPCat)
                elif doc.data_group.group_type.code=='HH':
                    self.assertEqual(type(extsub) , ExtractedHHDoc)
                else:
                    self.assertEqual(type(extsub) , ExtractedText)
            except ObjectDoesNotExist:
                pass
                #print('No extracted text for data document %s' % doc.id)


    def test_every_extractedtext(self):
        ''''Loop through all the ExtractedText objects and confirm that the new
        create_detail_formset method returns forms based on the correct models
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd, EXTRA)
            extracted_text_form = ParentForm(instance=et)
            child_formset = ChildForm(instance=et)
            # Compare the model of the child formset's QuerySet to the model
            # of the ExtractedText object's child objects
            dd_child_model  = get_extracted_models(dd.data_group.group_type.code)[1]
            childform_model = child_formset.__dict__.get('queryset').__dict__.get('model')
            self.assertEqual(dd_child_model, childform_model)

    def test_curated_chemical(self):
        ''''Confirm that if an ExtractedChemical record has been matched to DSSToxLookup, the 
            DSSToxLookup fields are displayed in the card
            This checks every data document.
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd)
            child_formset = ChildForm(instance=et)
            #print('Data doc %s , Group Type: %s ' % (dd.id, dd.data_group.type ))
            for form in child_formset.forms:
                if dd.data_group.type in ['CO','UN']:
                    ec = form.instance
                    if ec.dsstox is not None:
                        self.assertTrue( 'true_cas' in form.fields )
                        self.assertTrue( 'SID' in form.fields )
                    else:
                        self.assertFalse( 'true_cas' in form.fields )
                        self.assertFalse( 'SID' in form.fields )
                else:
                    self.assertFalse( 'true_cas' in form.fields )
            
    def test_num_forms(self):
        ''''Assure that the number of child forms is appropriate for the group
        type.
        '''
        group_models = {
                        'CO': ExtractedChemical,
                        'FU': ExtractedFunctionalUse,
                        'HP': ExtractedHabitsAndPractices,
                        'CP': ExtractedListPresence,
                        'HH': ExtractedHHRec
        }
        for code, model in group_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):

                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(
                                    reverse('data_document',kwargs={'pk': doc.pk})
                )
                num_forms = response.context['detail_formset'].total_form_count()
                children = model.objects.filter(
                                    extracted_text=doc.extractedtext
                ).count()
                if code in ['CO','FU','HP']:
                    error = (f'{model.__module__} should have the same number'
                                                        ' of forms as instances')
                    self.assertEqual(num_forms, children, error)
                if code in ['CP','HH']:
                    error = (f'{model.__module__} should have one more forms'
                                                                ' than instances')
                    self.assertEqual(num_forms, children + 1, error)/n/n/ndashboard/tests/functional/test_datagroup_create_upload.py/n/nimport os
import io
import shutil

from django.test import RequestFactory, TestCase, Client
from django.contrib.auth.models import User
from django.utils.datastructures import MultiValueDict
from django.core.files.uploadedfile import (InMemoryUploadedFile,
                                            TemporaryUploadedFile)

from factotum import settings
from dashboard import views
from dashboard.models import *
from dashboard.tests.loader import fixtures_standard


class RegisterRecordsTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')
        media_root = settings.MEDIA_ROOT
        shutil.rmtree(media_root)


    def tearDown(self):
        # clean up the file system by deleting the data group object
        if DataGroup.objects.filter(name='Walmart MSDS Test Group').exists():
            DataGroup.objects.get(name='Walmart MSDS Test Group').delete()

    def test_datagroup_create(self):
        long_fn = 'a filename that is too long ' * 10
        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,UN,, \n""
                f""{long_fn},Body Cream,1,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        form_data= {'name': ['Walmart MSDS Test Group'],
                    'description': ['test data group'],
                    'group_type': ['1'],
                    'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                    'downloaded_at': ['08/02/2018'],
                    'download_script': ['1'],
                    'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new/', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)
        dg_exists = DataGroup.objects.filter(
                                        name='Walmart MSDS Test Group').exists()
        self.assertContains(resp,'Filename too long')
        self.assertFalse(dg_exists,)

        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,UN,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,UN,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)


        self.assertEqual(resp.status_code,302,
                        ""Should be redirecting"")

        dg = DataGroup.objects.get(name='Walmart MSDS Test Group')


        self.assertEqual(f'/datagroup/{dg.pk}/', resp.url,
                        ""Should be redirecting to the proper URL"")

        # test whether the file system folder was created
        self.assertIn(str(dg.fs_id), os.listdir(settings.MEDIA_ROOT),
                        ""The data group's UUID should be a folder in MEDIA_ROOT"")

        # In the Data Group Detail Page
        resp = self.client.get(f'/datagroup/{dg.pk}/')

        # test whether the data documents were created
        docs = DataDocument.objects.filter(data_group=dg)
        self.assertEqual(len(docs), 2, ""there should be two associated documents"")

        # test whether the ""Download Registered Records"" link is like this example
        # <a href=""/datagroup/a9c7f5a7-5ad4-4f75-b877-a3747f0cc081/registered_records.csv"" class=""btn btn-secondary"">
        # <span class=""oi oi-spreadsheet""></span>&nbsp;Download Registered Records CSV</a>
        csv_href = f'/datagroup/{dg.pk}/registered_records.csv'
        self.assertIn(csv_href, str(resp._container),
                        ""The data group detail page must contain the right download link"")

        # grab a filename from a data document and see if it's in the csv
        doc_fn = docs.first().filename
        # test whether the registered records csv download link works
        resp_rr_csv = self.client.get(csv_href) # this object should be of type StreamingHttpResponse
        docfound = 'not found'
        for csv_row in resp_rr_csv.streaming_content:
            if doc_fn in str(csv_row):
                docfound = 'found'
        self.assertEqual(docfound, 'found', ""the document file name should appear in the registered records csv"")

        # Test whether the data document csv download works
        # URL on data group detail page: datagroup/docs_csv/{pk}/
        dd_csv_href = f'/datagroup/docs_csv/{dg.pk}/'  # this is an interpreted django URL
        resp_dd_csv = self.client.get(dd_csv_href)
        for csv_row in resp_dd_csv.streaming_content:
            if doc_fn in str(csv_row):
                docfound = 'found'
        self.assertEqual(docfound, 'found', ""the document file name should appear in the data documents csv"")


        # test whether the ""Download All PDF Documents"" link works
        dg_zip_href = f'/datagroup/pdfs_zipped/{dg.pk}/' # this is the django-interpreted URL
        self.assertIn(dg_zip_href, str(resp._container),
                        ""The data group detail page must contain the right zip download link"")
        resp_zip = self.client.get(dg_zip_href)

        # test uploading one pdf that matches a registered record
        f = TemporaryUploadedFile(name='0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                                content_type='application/pdf',
                                size=47,
                                charset=None)
        request = self.factory.post(path='/datagroup/%s' % dg.pk, data={'upload':'Submit'})
        request.FILES['multifiles'] = f
        request.user = User.objects.get(username='Karyn')
        resp = views.data_group_detail(request=request, pk=dg.pk)
        doc = DataDocument.objects.get(title='NUTRA NAIL')
        fn = doc.get_abstract_filename()
        folder_name = str(dg.fs_id)
        stored_file = f'{folder_name}/pdf/{fn}'
        pdf_path = f'{settings.MEDIA_ROOT}{stored_file}'
        self.assertTrue(os.path.exists( pdf_path ),
                            ""the stored file should be in MEDIA_ROOT/dg.fs_id"")
        f.close()

    def test_datagroup_create_dupe_filename(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,1,, \n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,Body Cream,1,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        form_data= {'name': ['Walmart MSDS Test Group'],
                    'description': ['test data group'],
                    'group_type': ['1'],
                    'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                    'downloaded_at': ['08/02/2018'],
                    'download_script': ['1'],
                    'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new/', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)

        self.assertContains(resp, 'Duplicate filename found')
/n/n/ndashboard/tests/functional/test_datagroup_detail.py/n/nfrom lxml import html
from importlib import import_module

from django.test import Client
from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard.views.data_group import ExtractionScriptForm, DataGroupForm
from django.core.files.uploadedfile import SimpleUploadedFile
from django.contrib.auth.models import User
from django.test import Client
from importlib import import_module
from django.db.models import Max

from dashboard.forms import *

from dashboard.models import *

class DataGroupDetailTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_detail_form_load(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(self.objects.doc.matched,
                    ('Document should start w/ matched False'))
        self.assertFalse(self.objects.doc.extracted,
                    ('Document should start w/ extracted False'))
        self.assertFalse(response.context['datagroup'].all_matched(),
                    ('UploadForm should be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(), (
                    'UploadForm should not be included in the page!'))
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(),
                    ('UploadForm should not be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_detail_template_fieldnames(self):
        pk = self.objects.dg.pk
        self.assertEqual(str(self.objects.dg.group_type),'Composition',
        'Type of DataGroup needs to be ""composition"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name',
                'report_funcuse','raw_min_comp','raw_max_comp', 'unit_type',
                'ingredient_rank', 'raw_central_comp'],
                ""Fieldnames passed are incorrect!"")
        self.objects.gt.title = 'Functional use'
        self.objects.gt.code = 'FU'
        self.objects.gt.save()
        self.assertEqual(str(self.objects.dg.group_type),'Functional use',
            'Type of DataGroup needs to be ""FU"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name','report_funcuse'],
                ""Fieldnames passed are incorrect!"")

    def test_unidentifed_group_type(self):
        pk = self.objects.dg.pk
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.gt.code = 'UN'
        self.objects.gt.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_bulk_create_products_form(self):
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        doc.matched = True
        self.objects.doc.matched = True
        doc.save()
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        self.assertIn('Bulk Create', response.content.decode(),
                            ""Bulk create button should be present."")
        p = Product.objects.create(upc='stub_47',data_source=self.objects.ds)
        ProductDocument.objects.create(document=doc, product=p)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
        'Product linked to all DataDocuments, no bulk_create needed.')
        self.objects.dg.group_type = GroupType.objects.create(
                                                title='Habits and practices')
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertNotIn('Bulk Create', response.content.decode(),
                            (""Bulk button shouldn't be present w/ ""
                            ""Habits and practices group_type.""))

    def test_bulk_create_post(self):
        '''test the POST to create Products and link if needed'''
        # create a new DataDocument with no Product
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        new_stub_id = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        response = self.client.post(f'/datagroup/{self.objects.dg.pk}/',
                                                                {'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Products linked to all DataDocuments, no bulk_create needed.')
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, 'unknown',
                                        'Title should be unknown in bulk_create')
        
        self.assertEqual(product.upc, f'stub_%s' % new_stub_id,
                                    'UPC should be created for second Product')

    def test_upload_note(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('Please limit upload to <600 documents at one time', response,
                      'Note to limit upload to <600 should be on the page')

    def test_extracted_count(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('0 extracted', response,
                      'Data Group should contain a count of 0 total extracted documents')
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('1 extracted', response,
                      'Data Group should contain a count of 1 total extracted documents')

    def test_delete_doc_button(self):
        url = f'/datagroup/{DataGroup.objects.first().id}/'
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-trash""></span>'
        self.assertIn(span, response,
                      'Trash button should be present if not matched.')
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-circle-check"" style=""color:green;""></span>'
        self.assertIn(span, response,
                      'Check should be present if matched.')

    def test_detail_table_headers(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertIn('<th>Product</th>', response,
                      'Data Group should have Product column.')
        fu = GroupType.objects.create(title='Functional use')
        self.objects.dg.group_type = fu
        self.objects.dg.save()
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertNotIn('<th>Product</th>', response,
                      'Data Group should have Product column.')

    def test_detail_datasource_link(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertContains(response,'<a href=""/datasource/',
                    msg_prefix='Should be able to get back to DataSource from here.')

    def test_edit_redirect(self):
        dgpk = self.objects.dg.pk
        dspk = str(self.objects.ds.pk)
        gtpk = str(self.objects.gt.pk)
        data = {'name': ['Changed Name'],
                'group_type': [gtpk],
                'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                'downloaded_at': ['08/20/2017'],
                'data_source': [dspk]}
        response = self.client.post(f'/datagroup/edit/{dgpk}/', data=data)
        self.assertEqual(response.status_code, 302,
                                         ""User is redirected to detail page."")
        self.assertEqual(response.url, f'/datagroup/{dgpk}/',
                                         ""Should go to detail page."")

class DataGroupDetailTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_download_raw_comp_data(self):
        # Ability to download, by data group, a csv file of raw extracted chemical composition data.
        # Download button would appear on data group detail page,
        # Download button would appear if any data documents have extracted text.
        # Only applies for data group type Composition. (group_type = 2)
        # Unidentified is excluded as of issue #502
        dg_co = DataGroup.objects.filter(group_type__code = 'CO').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_co.id)
        self.assertIn(b'Download Raw', resp.content)

        dg_un = DataGroup.objects.filter(group_type__code = 'UN').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_un.id)
        self.assertNotIn(b'Download Raw', resp.content)

        # Test download on all data groups with ExtractedChemicals, whether
        # they are CO or UN
        dg_ids = DataDocument.objects.filter(
            id__in=ExtractedChemical.objects.all().values('extracted_text_id')
            ).order_by().values_list('data_group_id',flat=True).distinct()

        for dg_id in dg_ids:
            #resp = self.client.get(f'/datagroup/%s/' % dg_id)
            resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_id)
            self.assertEqual(resp.status_code, 200)

        # File downloaded must include [specified fields]
        resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_ids[0])
        field_list = 'ExtractedChemical_id,raw_cas,raw_chem_name,raw_min_comp,raw_central_comp,raw_max_comp,unit_type'
        content = list(i.decode('utf-8') for i in resp.streaming_content)
        self.assertIn(field_list, content[1])
/n/n/ndashboard/tests/functional/test_datagroup_filesystem.py/n/nfrom django.urls import resolve
from django.test import RequestFactory, TestCase, Client
from django.http import HttpRequest
from dashboard import views
from dashboard.models import *
from factotum import settings
from django.core.files.uploadedfile import InMemoryUploadedFile, SimpleUploadedFile
import tempfile, csv, os, io, errno
from django.contrib.auth.models import User
from dashboard.tests.loader import fixtures_standard


def build_datagroup_folder(dirname, ):
    fullpath = f'{settings.MEDIA_ROOT}/{dirname}'
    try:
        os.makedirs(fullpath)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    pdfpath = f'{settings.MEDIA_ROOT}/{dirname}/pdf'
    try:
        os.makedirs(pdfpath)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,1,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,1,, \n"")

    with open(f'{fullpath}/British_Petroleum_Air_1_British_Petroleum_Air_1_register_recor_EHDBt5f.csv', 'w') as rr_csv:
        rr_csv.write(csv_string)
        rr_csv.close()


class DataGroupFileDownloadTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.c = Client()
        self.factory = RequestFactory()
        self.c.login(username='Karyn', password='specialP@55word')

    def tearDown(self):
        # clean up the file system by deleting the data group object
        dg = DataGroup.objects.get(pk=6)
        dg.delete()

    def test_old_path(self):
        '''
        Before we switched to UUIDs for the data group media folders,
        the paths were based on the original name of the data group
        '''
        testpath = 'British_Petroleum_(Air)_1'
        build_datagroup_folder(testpath)
        # the get_dg_folder() method should be able to find the newly-created directory
        dg = DataGroup.objects.get(pk=6)
        self.assertEqual(testpath, dg.get_dg_folder().rsplit('/')[-1],
        'The get_dg_folder() method should have returned the newly created directory')

    def test_get_dg_folder(self):
        '''
        The dev environment does not contain folders for most of the datagroups,
        so this just tests whether the methods can return their messages without
        errors.
        '''
        for dg in DataGroup.objects.all():
            with self.assertRaises(Exception):
                try:
                    folderpath = dg.get_dg_folder()
                    zippath = dg.get_zip_url()
                except:
                    pass
                else:
                    raise Exception

/n/n/ndashboard/tests/functional/test_datasource_index.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DataSourceTestWithFixtures(TestCase):
    fixtures = fixtures_standard
    
    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_extracted_counts(self):
        response = self.client.get('/datasources/').content.decode('utf8')
        self.assertIn('Extracted', response,
                      'The Extracted document count should be in the page after ticket 758')
        response_html = html.fromstring(response)
        ext_table_count = int(response_html.xpath(""//*[@id='sources']/tbody/tr[contains(., 'Airgas')]/td[4]"")[0].text)
        ext_orm_count = ExtractedText.objects.filter(data_document__data_group__data_source__title='Airgas').count()
        self.assertEqual(ext_table_count, ext_orm_count,
                         'The number of extracted records shown for Airgas should match what the ORM returns')

/n/n/ndashboard/tests/functional/test_extracted_qa.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import QAGroup, ExtractedText



class ExtractedQaTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_group_creation(self):
        # test the assignment of a qa_group to extracted text objects
        pk = self.objects.extext.pk
        self.assertIsNone(self.objects.extext.qa_group)
        self.assertEqual(len(QAGroup.objects.all()),0)
        pk = self.objects.extext.extraction_script.pk
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        self.assertEqual(response.status_code,200)
        qa_group = QAGroup.objects.get(
                        extraction_script=self.objects.extext.extraction_script)
        ext = ExtractedText.objects.get(qa_group=qa_group)
        self.assertIsNotNone(ext.qa_group)
        response = self.client.get(f'/qa/extractedtext/{ext.pk}/')

    def test_qa_approval_redirect(self):
        # first need to create a QAGroup w/ this get request.
        self.client.get(f'/qa/extractionscript/{self.objects.exscript.pk}/')
        pk = self.objects.extext.pk
        response = self.client.post(f'/qa/extractedtext/{pk}/',{'approve':[47]})
        self.assertEqual(response.url, '/qa/extractionscript/',(""User should be redirected to ""
                                ""QA homepage after last extext is approved.""))
/n/n/ndashboard/tests/functional/test_get_data.py/n/nfrom django.urls import resolve
from django.test import TestCase, override_settings
from django.test.client import Client

from django.contrib.auth import authenticate
from django.contrib.auth.models import User
from dashboard.models import PUC, Product, ProductToPUC, ProductDocument, DSSToxLookup
from dashboard.views.get_data import *
from django.test import TestCase
from django.test.client import Client

from dashboard.views.get_data import *
from dashboard.tests.loader import fixtures_standard


# from dashboard import views
# from django.urls import resolve
# from django.contrib.auth import authenticate
# from django.contrib.auth.models import User

@override_settings(ALLOWED_HOSTS=['testserver'])
class TestGetData(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dtxsid_pucs_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(0, ethylparaben_stats['pucs_n'])

        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        # get one of the products that was just linked to a data document with DTXSID9022528 in its extracted chemicals
        pid = dd.products.first().pk
        puc = PUC.objects.get(id=20)
        # add a puc to one of the products containing ethylparaben

        ppuc = ProductToPUC.objects.create(product=Product.objects.get(pk=pid),
                                           puc=puc,
                                           puc_assigned_usr=User.objects.get(username='Karyn'))
        ppuc.refresh_from_db()
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(1, ethylparaben_stats['pucs_n'])

    def test_dtxsid_dds_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            2, ethylparaben_stats['dds_n'], 'There should be 2 datadocuments associated with ethylaraben')
        # change the number of related data documents by deleting one
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))

        dd = dds[0]
        dd.delete()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            1, ethylparaben_stats['dds_n'], 'There should now be 1 datadocument associated with ethylaraben')

    def test_dtxsid_dds_wf_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['dds_wf_n'], 'There should be 1 extracted chemical \
        with weight fraction data associated with ethylparaben')
        # add weight fraction data to a different extractedchemical
        ec = ExtractedChemical.objects.get(rawchem_ptr_id=73)
        ec.raw_min_comp = 0.1
        ec.save()
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(2, ethylparaben_stats['dds_wf_n'], 'There should be 2 extracted chemicals \
        with weight fraction data associated with ethylparaben')

    def test_dtxsid_products_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)

        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(0, ethylparaben_stats['products_n'], 'There should be 0 products \
        associated with ethylparaben')
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['products_n'], 'There should now be 1 product \
        associated with ethylparaben')

    def test_habits_and_practices_cards(self):
        data = {'puc': ['2']}
        response = self.client.post('/get_data/', data=data)
        for hnp in [b'ball bearings',
                    b'motorcycle',
                    b'vitamin a&amp;d',
                    b'dish soap']:
            self.assertIn(hnp, response.content)

    def test_download_pucs_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download PUCs')

    def test_download_raw_chem_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download Uncurated Chemicals')
        # Pick one curated and one non-curated RawChem record, and
        # confirm that the downloaded file excludes and includes them,
        # respectively.

        rc = RawChem.objects.filter(dsstox_id__isnull=True).first()
        response = self.client.get('/dl_raw_chems/')
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The non-curated row should appear')
        # The downloaded file should include the data group id of each uncurated chemical
        rc_row = f'%s,%s,%s,%s,%s\r\n' % (rc.extracted_text.data_document.data_group.id,
                                          rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The data group id should be in the output')

        rc = RawChem.objects.filter(dsstox_id__isnull=False).first()
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.sid if rc.sid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertNotIn(rc_row, response.content,
                         'The curated row should not appear')


/n/n/ndashboard/tests/functional/test_nav_bar.py/n/nfrom django.urls import resolve
from django.test import TestCase
from django.test.client import Client
from django.http import HttpRequest
from dashboard.tests.loader import load_model_objects
from dashboard import views
from lxml import html


class NavBarTest(TestCase):
    '''this group of tests checks to see that the URL resolves to the
    appropriate view function.
    '''
    def setUp(self):
        self.objects = load_model_objects()
        self.client = Client()

    def test_home_page_returns_correct_html(self):
        # we need Karyn in the DB in order to log her in.
        # load_model_objects returns a `dot_notation` dict which we can
        # use all of the model objects from, seen in the print stmnt below.
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/')
        html = response.content.decode('utf8').rstrip()
        self.assertTrue(html.startswith('<!DOCTYPE html>'))
        self.assertIn('<title>factotum</title>', html)
        self.assertTrue(html.endswith('</html>'))

    def test_index_link(self):
        found = resolve('/')
        self.assertEqual(found.func, views.index)

    def test_data_sources_link(self):
        found = resolve('/datasources/')
        self.assertEqual(found.func, views.data_source_list)

    def test_product_curation_link(self):
        found = resolve('/product_curation/')
        self.assertEqual(found.func, views.product_curation_index)

    def test_qa_link(self):
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_get_data_without_auth(self):
        # the Get Data menu item should be available to a user who isn't logged in
        response = self.client.get('/')
        self.assertContains(response, 'Get Data')
        response = self.client.get('/get_data/')
        self.assertContains(response, 'Summary metrics by chemical')

    def test_data_curation(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('Data Curation',
                      response_html.xpath('string(//*[@id=""navbarDataCurationDropdownMenuLink""]/text())'),
                      'The Data Curation dropdown should appear in the navbar.')

/n/n/ndashboard/tests/functional/test_product_linkage.py/n/nfrom django.test import TestCase, override_settings
from dashboard.tests.loader import *
from dashboard.views.product_curation import ProductLinkForm
from lxml import html


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestProductLinkage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_datatype_update(self):
        self.assertTrue(ProductLinkForm().fields['document_type'],
                            'ProductLinkForm must include a document_type select input')
        dd = DataDocument.objects.get(pk=155324)
        dd.document_type_id = 1
        dd.save()
        self.assertEqual(dd.document_type_id, 1,
                         'DataDocument 155324 must have a document_type_id of 1 for test to function')
        response = self.client.post(f'/link_product_form/155324/',
                                    {'title': 'x',
                                     'manufacturer': '',
                                     'brand_name': '',
                                     'upc': 'none',
                                     'size': '',
                                     'color': '',
                                     'document_type': 2,
                                     'return_url': 'required'})
        dd.refresh_from_db()
        self.assertEqual(dd.document_type_id, 2,
                         'DataDocument 155324 should have a final document_type_id of 2')

    def test_datatype_options(self):
        # retrieve a sample datadocument
        dd = DataDocument.objects.get(pk=129298)

        # configure its datagroup to be of group type ""composition""
        dg = DataGroup.objects.get(pk=dd.data_group_id)
        dg.group_type_id = 2
        dg.save()

        response = self.client.get(f'/link_product_form/{str(dd.pk)}/').content.decode('utf8')
        response_html = html.fromstring(response)

        self.assertTrue(response_html.xpath('string(//*[@id=""id_document_type""]/option[@value=""5""])'),
                      'Document_type_id 5 should be an option when the datagroup type is composition.')

        self.assertFalse(response_html.xpath('string(//*[@id=""id_document_type""]/option[@value=""6""])'),
                      'Document_type_id 6 should NOT be an option when the datagroup type is composition.')

    def test_bulk_create_products(self):
        # DataGroup 19 is a Composition dg with unlinked products
        dg = DataGroup.objects.get(pk=19)
        response = self.client.get(f'/datagroup/19/')
        self.assertEqual(response.context['bulk'], 75,
                'Not all DataDocuments linked to Product, bulk_create needed')
        response = self.client.post(f'/datagroup/19/',{'bulk':75})
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        # pick documents and check the attributes of their now-related products
        # 1: check a case where the ExtractedText record had a prod_name to offer
        ets = ExtractedText.objects.filter(data_document__data_group=dg)
        et = ets.filter(prod_name__isnull=False).first()
        doc = DataDocument.objects.get(pk=et.data_document_id)
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, et.prod_name,
                                        'Title should be taken from ExtractedText.prod_name in bulk_create')
        # 2: check a case where ExtractedText.prod_name is None
        et = ets.filter(prod_name__isnull=True).first()
        doc = DataDocument.objects.get(pk=et.data_document_id)
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, '%s stub' % doc.title ,
                                        ('Title should be taken from the DataDocument.title in bulk_create,'
                                        'with ""stub"" appended') )
        # 3: check a case where the ExtractedText doesn't exist
        # Data Group 6 has 2 docs that are linked to Product and ExtractedText records
        dg = DataGroup.objects.get(pk=6)
        docs = DataDocument.objects.filter(data_group=dg)
        doc_list = docs.values_list('id')
        # delete the ExtractedText links and recreate them via the bulk request
        ExtractedText.objects.filter(data_document_id__in=doc_list).delete()
        # delete the ProductDocument links and recreate them via the bulk request
        ProductDocument.objects.filter(document_id__in=doc_list).delete()
        response = self.client.get(f'/datagroup/6/')
        self.assertEqual(response.context['bulk'], 2,
                'Not all DataDocuments linked to Product, bulk_create needed')
        response = self.client.post(f'/datagroup/6/',{'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        # check the titles of the newly-created products
        # they should be based on the document title
        doc = docs.first()
        prod = Product.objects.filter(documents__in=[doc]).first()
        self.assertEqual('%s stub' % doc.title, prod.title,
                'Product and DataDocument titles should be the same.')
        

/n/n/ndashboard/tests/functional/test_qa.py/n/nimport time
from django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import DataDocument, Script, ExtractedText
from lxml import html


class QATest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_scoreboard(self):
        response = self.client.get(
            '/qa/extractionscript/').content.decode('utf8')
        response_html = html.fromstring(response)

        row_count = len(response_html.xpath(
            '//table[@id=""extraction_script_table""]/tbody/tr'))
        scriptcount = Script.objects.filter(script_type='EX').count()
        self.assertEqual(scriptcount, row_count, ('The seed data contains 1 '
                                                  'Script object with the script_type'
                                                  'EX, which should appear in this table'))

        script_url = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[1]/a/@href')[0]
        self.assertEqual(script_url, 'http://www.epa.gov/',
                         'The URL on the page should be the external link to the script.')

        displayed_doc_count = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[2]')[0].text
        model_doc_count = DataDocument.objects.filter(
            extractedtext__extraction_script=self.objects.exscript.pk).count()

        self.assertEqual(displayed_doc_count, str(model_doc_count),
                         ('The displayed number of datadocuments should match '
                          'the number whose related extracted text objects used '
                          ' the extraction script'))

        displayed_pct_checked = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[3]')[0].text
        model_pct_checked = self.objects.exscript.get_pct_checked()
        self.assertEqual(displayed_pct_checked, model_pct_checked,
                         ('The displayed percentage should match what is derived from the model'))

        es = self.objects.exscript
        self.assertEqual(es.get_qa_complete_extractedtext_count(), 0,
                         ('The ExtractionScript object should return 0 qa_checked ExtractedText objects'))

        # Set qa_checked property to True for one of the ExtractedText objects
        self.assertEqual(self.objects.extext.qa_checked, False)
        self.objects.extext.qa_checked = True
        self.objects.extext.save()
        self.assertEqual(es.get_qa_complete_extractedtext_count(), 1,
                         ('The ExtractionScript object should now return 1 qa_checked ExtractedText object'))

        # A button for each row that will take you to the script's QA page
        script_qa_link = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[contains(.,""Test Extraction Script"")]/td[4]/a/@href')[0]
        self.assertIn(
            f'/qa/extractionscript/{str(self.objects.exscript.pk)}/', script_qa_link)

        # Before clicking the link, the script's qa_done property should be false
        self.assertEqual(es.qa_begun, False,
                         'The qa_begun property of the Script should be False')

        # The link should open a page where the h1 text matches the title of the Script
        response = self.client.get(script_qa_link).content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn(es.title, response_html.xpath('/html/body/div/h1/text()')[0],
                      'The <h1> text should equal the .title of the Script')

        # Opening the ExtractionScript's QA page should set its qa_begun property to True
        es.refresh_from_db()
        self.assertEqual(es.qa_begun, True,
                         'The qa_begun property of the ExtractionScript should now be True')

        # Go back to the QA index page to confirm that the QA is complete
        response = self.client.get(
            '/qa/extractionscript/').content.decode('utf8')
        response_html = html.fromstring(response)
        script_qa_status = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[contains(.,""Test Extraction Script"")]/td[4]/text()')[0]
        str_qa_complete = 'QA Complete'
        self.assertIn(str_qa_complete, script_qa_status,
                      'The QA Status field should now say ""QA Complete"" instead of ""Begin QA""')
/n/n/ndashboard/tests/functional/test_qa_seed_data.py/n/nfrom django.test import Client
from dashboard.tests.loader import *
from django.test import TestCase, override_settings, RequestFactory
from dashboard.models import DataDocument, Script, ExtractedText, ExtractedChemical, QAGroup
from django.db.models import Count


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestQaPage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_begin(self):
        """"""
        Check that starting the QA process flips the variable on the Script
        """"""
        self.assertFalse(Script.objects.get(pk=5).qa_begun,
                         'The Script should have qa_begun of False at the beginning')
        response = self.client.get('/qa/extractionscript/5/')
        self.assertTrue(Script.objects.get(pk=5).qa_begun,
                        'qa_begun should now be true')

    def test_new_qa_group_urls(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 15 has one ExtractedText object
        pk = 15
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertIn(f'/qa/extractedtext/{et.pk}/'.encode(), response.content)
        # After opening the URL, the following should be true:
        # One new QA group should be created
        group_count = QAGroup.objects.filter(extraction_script_id=pk).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(Script.objects.get(pk=15).qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        group_pk = QAGroup.objects.get(extraction_script_id=pk).pk
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertTrue(et.qa_group_id == group_pk)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/15/\'> Continue QA"".encode(), response.content)

    def test_qa_script_without_ext_text(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 9 has no ExtractedText objects
        pk = 9
        # a user will see no link on the QA index page, but it's still
        # possible to enter the URL
        response = self.client.get(f'/qa/extractionscript/{pk}/', follow=True)
        self.assertEqual(response.status_code, 200)

    def test_data_document_qa(self):
        # Open the QA page for a Composition ExtractedText record that has no QA group
        # and is in a Script with < 100 documents
        scr = Script.objects.annotate(num_ets=Count('extractedtext')).filter(
            num_ets__lt=100).filter(script_type='EX').first()
        pk = ExtractedText.objects.filter(qa_group=None).filter(extraction_script=scr
                                                                ).filter(
            data_document__data_group__group_type__code='CO').first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')

        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        group_count = QAGroup.objects.filter(extraction_script=scr).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(scr.qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        new_group = QAGroup.objects.get(extraction_script=scr)
        et = ExtractedText.objects.get(pk=pk)
        self.assertTrue(et.qa_group == new_group)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/{scr.pk}/\'> Continue QA"".encode(), response.content)

        # Open the QA page for an ExtractedText record that has no QA group and
        # is related to a script with over 100 documents
        scr = Script.objects.annotate(num_ets=Count(
            'extractedtext')).filter(num_ets__gt=100).first()
        pk = ExtractedText.objects.filter(extraction_script=scr).first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        new_group = QAGroup.objects.get(extraction_script=scr)

        # There should be a lot of ExtractedText records assigned to the QA Group
        initial_qa_count = ExtractedText.objects.filter(
            qa_group=new_group).count()
        self.assertTrue(initial_qa_count > 100)

        # Select a document that shares a Script with the
        # QA Group created above BUT DOES NOT BELONG TO THE QA GROUP
        pk = ExtractedText.objects.filter(
            extraction_script_id=scr.id).filter(qa_group=None).first().pk
        # Open its QA page via the /datdocument/qa path
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        # Make sure that the number of documents in the QA Group has increased
        self.assertGreater(ExtractedText.objects.filter(
            qa_group=new_group).count(), initial_qa_count)

    def test_habitsandpractices(self):
        # Begin from the QA index page
        response = self.client.get(f'/habitsandpractices/54/')
        self.assertContains(response, '<b>Add New Habit and Practice</b>')

    def test_dd_link(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractedtext/5', follow=True)
        self.assertIn(b'/datadocument/5', response.content)

    def test_approval(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Follow the first approval link
        response = self.client.get('/qa/extractedtext/7', follow=True)
        # print(response.context['extracted_text'])

    def test_hidden_fields(self):
        '''ExtractionScript 15 includes a functional use data group with pk = 5.
        Its QA page should hide the composition fields '''
        # Create the QA group by opening the Script's page
        response = self.client.get('/qa/extractionscript/15/', follow=True)
        # Open the DataGroup's first QA approval link
        response = self.client.get('/qa/extractedtext/5/', follow=True)
        # A raw_cas field should be in the page
        self.assertIn(
            b'<input type=""text"" name=""rawchem-1-raw_cas""', response.content)
        # There should not be any unit_type field in the functional use QA display
        self.assertNotIn(
            b'<input type=""text"" name=""rawchem-1-unit_type""', response.content)
        # The values shown should match the functional use record, not the chemical record
        self.assertIn(b'Functional Use Chem1', response.content)

        # Go back to a different ExtractionScript
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Open the QA page for a non-FunctionalUse document
        response = self.client.get('/qa/extractedtext/7/', follow=True)
        # This page should include a unit_type input form
        self.assertIn(b'rawchem-1-unit_type', response.content)

    def test_cpcat_qa(self):
        # Begin from the Chemical Presence QA index page
        response = self.client.get(f'/qa/chemicalpresence/')
        self.assertIn(
            f""/qa/chemicalpresencegroup/49/\'> View Chemical Presence Lists"".encode(), response.content)

        response = self.client.get(
            f'/qa/chemicalpresencegroup/49', follow=True)
        # The table should include the ""Begin QA"" link
        self.assertIn(
            f'/qa/extractedtext/254781/""> Begin QA'.encode(), response.content)

        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 0)
        response = self.client.get(f'/qa/extractedtext/254781/', follow=True)
        # Navigating to the extractedtext QA page should cause
        # the sampled child records to be flagged with qa_flag=True
        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 30)

        # The QA page should only show the flagged records
        elp_flagged = elps.filter(qa_flag=True).first()
        self.assertIn(elp_flagged.raw_cas.encode(), response.content)

        elp_not_flagged = elps.filter(qa_flag=False).first()
        self.assertNotIn(elp_not_flagged.raw_cas.encode(), response.content)

    def test_every_extractedtext_qa(self):
        # Attempt to open a QA page for every ExtractedText record
        for et in ExtractedText.objects.all():
            response = self.client.get(f'/qa/extractedtext/%s' % et.data_document_id, follow=True)
            if response.status_code != 200:
                print(et.data_document_id)
            self.assertEqual(response.status_code, 200)/n/n/ndashboard/tests/integration/test_browser_edits.py/n/nfrom lxml import html

from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.staticfiles.testing import StaticLiveServerTestCase

from dashboard.models import *
from selenium import webdriver
from django.conf import settings
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

            # Data Document Detail Page
            doc_detail_link = f'/datadocument/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_detail_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            doc_type_select.select_by_visible_text(""MSDS"")
            self.assertIn(doc_detail_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note
        5. Approve 
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781, # Chemical Presence List
                       354783, # HHE Report 
                       ]: 
            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()

            # Modify the first raw_chem_name field's value
            #  
            raw_chem = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_chem_name""]')
            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')
            # print(rawchem_id)

            raw_chem_name_field.send_keys(' edited')
            # save changes
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            # print(et.data_document.data_group.group_type)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm validation failure
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            # The QA notes field should be invalid
            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_qa_notes""]')
            self.assertIn('is-invalid', qa_notes_field.get_attribute('class'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')



/n/n/ndashboard/tests/integration/test_user_experience.py/n/nfrom lxml import html
from django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import *
import os
import csv
import time
import unittest
import collections
import json
import re
from selenium import webdriver
from selenium.webdriver.support.select import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from django.conf import settings
from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from dashboard.models import *


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestIntegration(StaticLiveServerTestCase):

    def setUp(self):
        self.objects = load_model_objects()
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_hem(self):
        for i in range(27):
            ds = DataSource.objects.create(title=f'Test_DS_{i}')
        list_url = self.live_server_url + '/datasources/'
        self.browser.get(list_url)
        row_count = len(self.browser.find_elements_by_xpath(""//table[@id='sources']/tbody/tr""))
        self.assertEqual(row_count, 25, 'Should be 25 datasources in the table')
        # go to edit page from datasource list
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")
        detail_url = self.live_server_url + f'/datasource/{ds.pk}'
        self.browser.get(detail_url)
        # go to edit page from datasource detail
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), detail_url,
                         ""User should go back to detail view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

        num_pucs = len(PUC.objects.filter(kind='FO'))
        self.browser.get(self.live_server_url)
        import time
        time.sleep(3)  # or however long you think it'll take you to scroll down to bubble chart
        bubbles = self.browser.find_elements_by_class_name('bubble')
        self.assertEqual(num_pucs, len(bubbles), ('There should be a circle'
                                                  'drawn for every PUC'))

    def test_datagroup(self):
        list_url = self.live_server_url + '/datagroups/'
        self.browser.get(list_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")

        dg = DataGroup.objects.first()
        ds_detail_url = f'{self.live_server_url}/datasource/{dg.data_source.pk}'
        self.browser.get(ds_detail_url)
        self.browser.find_elements_by_xpath('//*[@title=""edit""]')[1].click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), ds_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        dg_detail_url = f'{self.live_server_url}/datagroup/{dg.pk}/'
        self.browser.get(dg_detail_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), dg_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        edit_url = f'{self.live_server_url}/datagroup/edit/{dg.pk}/'
        self.browser.get(edit_url)
        self.browser.find_element_by_name('cancel').click()
        self.assertIn('/datagroups/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

    def test_product(self):
        p = self.objects.p
        puc = self.objects.puc
        tag = self.objects.pt
        PUCToTag.objects.create(content_object=puc, tag=tag)
        ProductToPUC.objects.create(product=p, puc=puc)
        url = self.live_server_url + f'/product/{p.pk}/'
        self.browser.get(url)
        submit = self.browser.find_element_by_id('tag_submit')
        self.assertFalse(submit.is_enabled(), ""Button should be disabled"")
        tag = self.browser.find_element_by_class_name('taggit-tag')
        tag.click()
        self.assertTrue(submit.is_enabled(), ""Button should be enabled"")

    def test_field_exclusion(self):
        doc = self.objects.doc
        # The element should not appear on the QA page
        qa_url = self.live_server_url + f'/qa/extractedtext/{doc.pk}/'
        self.browser.get(qa_url)
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_cas""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_chemname""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-SID""]')
        # make sure the test can pick up one that should be there
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-raw_cas""]')
        except NoSuchElementException:
            self.fail(""Absence of raw_cas element raised exception"")

        # The element should appear on the datadocument page
        dd_url = self.live_server_url + f'/datadocument/{doc.pk}/'
        self.browser.get(dd_url)
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        except NoSuchElementException:
            self.fail(""Absence of weight_fraction_type element raised exception"")

/n/n/ndashboard/tests/loader.py/n/nfrom django.utils import timezone
from django.contrib.auth.models import User

from dashboard.models import *

fixtures_standard = [ '00_superuser',
                      '01_lookups',
                      '02_datasource',
                      '03_datagroup',
                      '04_PUC', 
                      '05_product',
                      '06_datadocument',
                      '07_rawchem_etc',
                       '08_script',
                    '09_productdocument',  
                    '10_habits_and_practices',
                     '11_habits_and_practices_to_puc',
                      '12_product_to_puc',
                        '13_puc_tag'
                        ]

class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

def load_model_objects():
    user = User.objects.create_user(username='Karyn',
                                        password='specialP@55word')
    superuser = User.objects.create_superuser(username='SuperKaryn',
                                              password='specialP@55word',
                                              email='me@epa.gov')
    ds = DataSource.objects.create(title='Data Source for Test',
                                        estimated_records=2, state='AT',
                                        priority='HI')
    script = Script.objects.create(title='Test Download Script',
                                        url='http://www.epa.gov/',
                                        qa_begun=False, script_type='DL')
    exscript = Script.objects.create(title='Test Extraction Script',
                                   url='http://www.epa.gov/',
                                   qa_begun=False, script_type='EX')
    gt = GroupType.objects.create(title='Composition', code='CO')
    dg = DataGroup.objects.create(name='Data Group for Test',
                                        description='Testing...',
                                        data_source = ds,
                                        download_script=script,
                                        downloaded_by=user,
                                        downloaded_at=timezone.now(),
                                        group_type=gt,
                                        csv='register_records_matching.csv',
                                        url='https://www.epa.gov')
    dt = DocumentType.objects.create(title='MSDS',
                                    code='MS', group_type=gt)

    doc = DataDocument.objects.create(title='test document',
                                            data_group=dg,
                                            document_type=dt,
                                            filename='example.pdf')
    p = Product.objects.create(data_source=ds,
                                upc='Test UPC for ProductToPUC')

    puc = PUC.objects.create(gen_cat='Test General Category',
                              prod_fam='Test Product Family',
                              prod_type='Test Product Type',
                             description='Test Product Description',
                             last_edited_by = user,
                             kind='FO')

    extext = ExtractedText.objects.create(
                                    prod_name='Test Extracted Text Record',
                                    data_document=doc,
                                    extraction_script=exscript
                                    )
    ut = UnitType.objects.create(title='percent composition')
    wft = WeightFractionType.objects.create(title= 'reported', description= 'reported')
    ec = ExtractedChemical.objects.create(extracted_text=extext,
                                        unit_type=ut,
                                        weight_fraction_type = wft,
                                        raw_chem_name= 'Test Chem Name',
                                        raw_cas='test_cas'
                                        )
    rc = ec.rawchem_ptr
    ing = Ingredient.objects.create(lower_wf_analysis = 0.123456789012345,
                                    central_wf_analysis = 0.2,
                                    upper_wf_analysis = 1,
                                    script = script,
                                    rawchem_ptr = rc)
    
    pt = PUCTag.objects.create(name=""Test PUC Attribute"")
    pd = ProductDocument.objects.create(product=p, document=doc)
    ehp = ExtractedHabitsAndPractices.objects.create(extracted_text=extext,
                                                     product_surveyed='Test Product Surveyed',
                                                     prevalence='Continuous')


    return dotdict({'user':user,
                    'superuser':superuser,
                    'ds':ds,
                    'script':script,
                    'exscript':exscript,
                    'dg':dg,
                    'doc':doc,
                    'p':p,
                    'puc':puc,
                    'extext':extext,
                    'ut':ut,
                    'wft':wft,
                    'rc':rc,
                    'ec':ec,
                    'pt':pt,
                    'pd':pd,
                    'ing':ing,
                    'dt':dt,
                    'gt':gt,
                    'ehp':ehp
                    })
/n/n/ndashboard/tests/unit/test_models.py/n/nimport csv
from django.utils import timezone
from django.test import TestCase
from django.db.models import Count
from dashboard.models import *
from dashboard.tests.loader import *
from django.db.models import Q


def create_data_documents(data_group, source_type, pdfs):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdfs,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


def create_data_documents_with_txt(data_group, source_type, pdf_txt):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching_with_txt.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdf_txt,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


class ModelsTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')
        self.pdfs = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf']
        self.pdf_txt = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0bf5755e-3a08-4024-9d2f-0ea155a9bd17.txt']

    def test_object_creation(self):
        self.assertTrue(isinstance(self.objects.ds, DataSource))
        self.assertTrue(isinstance(self.objects.script, Script))
        self.assertTrue(isinstance(self.objects.extext, ExtractedText))
        self.assertTrue(isinstance(self.objects.ec, ExtractedChemical))
        self.assertTrue(isinstance(self.objects.ing, Ingredient))
        self.assertTrue(isinstance(self.objects.p, Product))
        self.assertTrue(isinstance(self.objects.pd, ProductDocument))
        self.assertTrue(isinstance(self.objects.pt, PUCTag))

    def test_datagroup(self):
        self.assertTrue(isinstance(self.objects.dg, DataGroup))

        self.assertEqual(str(self.objects.dg), self.objects.dg.name)
        self.assertEqual('https://www.epa.gov', self.objects.dg.url)

    def test_object_properties(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)
        self.assertTrue(hasattr(PUCToTag,'assumed'))
        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents(self.objects.dg,self.objects.st, self.pdfs)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdfs), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded pdf
        fn = docs[0].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))
        # DownloadScript
        self.assertEqual(str(self.objects.script), 'Test Download Script')
        # ExtractedText
        self.assertEqual(str(self.objects.extext),
                                    'test document')
        # RawChem
        self.assertEqual(str(self.objects.rc), 'Test Chem Name')
        # ExtractedChemical
        self.assertEqual(str(self.objects.ec), 'Test Chem Name')

    def test_product_attribute(self):
        self.assertEqual(ProductToTag.objects.count(), 0)
        p2t = ProductToTag.objects.create(content_object=self.objects.p,
                                            tag=self.objects.pt)
        self.assertEqual(ProductToTag.objects.count(), 1)

    def test_data_group(self):
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        self.assertFalse(self.objects.dg.all_matched())
        self.assertFalse(self.objects.dg.all_extracted())
        doc.matched = True
        doc.save()
        self.assertFalse(self.objects.dg.all_matched())
        self.objects.doc.matched = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_matched())
        doc.extracted = True
        doc.save()
        self.assertFalse(self.objects.dg.all_extracted())
        self.objects.doc.extracted = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_extracted())

    def test_extracted_habits_and_practices(self):
        puc2 = PUC.objects.create(gen_cat='Test General Category',
                                 prod_fam='Test Product Family',
                                 prod_type='Test Product Type',
                                 description='Test Product Description',
                                 last_edited_by = self.objects.user)
        self.assertEqual(ExtractedHabitsAndPractices.objects.count(), 1)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 0)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=self.objects.puc)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=puc2)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 2)

    def test_data_document_organization(self):
        self.assertEqual(self.objects.doc.organization, '')
        self.objects.doc.organization = 'Test Organization'
        self.objects.doc.save()
        self.assertEqual(DataDocument.objects.filter(organization='Test Organization').count(), 1)

    def test_data_document_filename(self):
        pk = self.objects.doc.pk
        self.assertEqual(self.objects.doc.get_abstract_filename(),
                        f'document_{pk}.pdf',
                        'This is used in the FileSystem naming convention.')

    def test_dg_with_txt(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)

        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents_with_txt(self.objects.dg,self.objects.st, self.pdf_txt)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdf_txt), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded text file
        fn = docs[1].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))

    def test_script_fields(self):
        fields = ['title','url','qa_begun','script_type','confidence']
        for fld in fields:
            self.assertIn(fld, Script.__dict__, (f'{fld} '
                                                'should be in Script model.'))


class PUCModelTest(TestCase):

    fixtures = fixtures_standard

    def test_puc_fields(self):
        fields = ['kind','gen_cat','prod_fam','prod_type','description',
                'last_edited_by','products','extracted_habits_and_practices',
                'tags']
        for fld in fields:
            self.assertIn(fld, PUC.__dict__, f'{fld} should be in PUC model.')

    def test_get_the_kids(self):
        '''Level 1 and 2 PUCs should accumulate lower level PUCs.
        '''
        puc = PUC.objects.get(pk=20) # PUC w/ only gen_cat value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=6) # PUC w/ gen_cat and prod_fam value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=126) # PUC w/ ALL values
        self.assertEqual(len(puc.get_the_kids()),1, ('PUC should only have '
                                                        'itself associated'))

    def test_puc_category_defaults(self):
        '''Assert that the prod_fam and prod_type are nulled w/ an
        empty string and not NULL.
        '''
        k = User.objects.get(username='Karyn')
        puc = PUC.objects.create(last_edited_by=k)
        self.assertTrue(puc.prod_fam == '')
        self.assertTrue(puc.prod_type == '')

    def test_product_counts(self):
        '''Make sure the product_count property
        returns the same thing as the num_products annotation'''
        pucs = PUC.objects.all().annotate(num_products=Count('products'))
        # pucs 1-3 have products associated with them
        self.assertEqual(pucs.get(pk=1).num_products , PUC.objects.get(pk=1).product_count)


class DataGroupFilesTest(TestCase):

    fixtures = fixtures_standard

    def test_filefield_properties(self):
        dg5 = DataGroup.objects.get(pk=5) # this datagroup has no csv value
        dg6 = DataGroup.objects.get(pk=6) # this one has a csv value, but no file
        dg50 = DataGroup.objects.get(pk=50) # this one has a /media/ folder

        # All of the falsy properties should return False rather than errors
        self.assertFalse(dg5.dg_folder)
        self.assertFalse(dg5.zip_url)

        self.assertFalse(dg6.dg_folder)
        self.assertFalse(dg6.zip_url)

        # 50 is the only datagroup that has a linked file in the /media folder
        self.assertTrue(dg50.dg_folder == dg50.get_dg_folder())
        self.assertFalse(dg50.zip_url)


class DataDocumentTest(TestCase):

    fixtures = fixtures_standard

    def test_datadocument_note(self):

        datadocument = DataDocument(filename=""MyFile.pdf"",
                                    title=""My Title"",
                                    data_group=DataGroup.objects.first(),
                                    note=""Some long note."")
        datadocument.save()
        self.assertTrue(datadocument.note, ""Some long note."")






/n/n/ndashboard/urls.py/n/nfrom django.urls import include, path
from django.conf import settings
from django.conf.urls.static import static

import dashboard.views.qa
from . import views

urlpatterns = [
    path('', views.index,                   name='index'),
    path('datasources/', views.data_source_list,
                                            name='data_source_list'),
    path('datasource/<int:pk>', views.data_source_detail,
                                            name='data_source_detail'),
    path('datasource/new/', views.data_source_create,
                                            name='data_source_new'),
    path('datasource/edit/<int:pk>/', views.data_source_update,
                                            name='data_source_edit'),
    path('datasource/delete/<int:pk>/', views.data_source_delete,
                                            name='data_source_delete'),
    path('datagroups/', views.data_group_list,
                                            name='data_group_list'),
    path('datagroup/<int:pk>/', views.data_group_detail,
                                            name='data_group_detail'),
    path('datagroup/docs_csv/<int:pk>/', views.dg_dd_csv_view,
                                            name='dg_dd_csv_view'),
    path('datagroup/pdfs_zipped/<int:pk>/', views.dg_pdfs_zip_view,
                                            name='dg_pdfs_zip_view'),
    path('datagroup/raw_extracted_records/<int:pk>/', views.dg_raw_extracted_records,
                                            name='dg_raw_extracted_records'),
    path('datasource/<int:pk>/datagroup_new/', views.data_group_create,
                                            name='data_group_new'),
    path('datagroup/<int:pk>/registered_records.csv', views.data_group_registered_records_csv,
                                            name=""registered_records.csv""),
    path('datagroup/edit/<int:pk>/', views.data_group_update,
                                            name='data_group_edit'),
    path('datagroup/delete/<int:pk>/', views.data_group_delete,
                                            name='data_group_delete'),
    path('datadocument/delete/<int:pk>/', views.data_document_delete,
                                            name='data_document_delete'),
    path('datadocument/note/<int:pk>/', views.data_document_note,
                                            name='data_document_note'),
    path('product_curation/', views.product_curation_index,
                                            name='product_curation'),
    path('category_assignment/<int:pk>/', views.category_assignment,
                                            name='category_assignment'),
    path('link_product_list/<int:pk>/', views.link_product_list,
                                            name='link_product_list'),
    path('link_product_form/<int:pk>/', views.link_product_form,
                                            name='link_product_form'),
    path('qa/extractionscript/', views.qa_extractionscript_index,
                                            name='qa_extractionscript_index'),
    path('qa/extractionscript/<int:pk>/', dashboard.views.qa.qa_extraction_script,
                                            name='qa_extraction_script'),
    path('qa/extractedtext/<int:pk>/', dashboard.views.qa.extracted_text_qa,
                                            name='extracted_text_qa'),
    path('extractionscript/<int:pk>/', views.extraction_script_detail,
                                            name='extraction_script_detail'),
    path('qa/chemicalpresence/', views.qa_chemicalpresence_index,
                                            name='qa_chemicalpresence_index'),
    path('qa/chemicalpresencegroup/<int:pk>/', views.qa_chemicalpresence_group,
                                            name='qa_chemical_presence_group'),
    path('bulk_product_puc/', views.bulk_assign_puc_to_product,
                                            name='bulk_product_puc'),
    path('bulk_product_tag/', views.bulk_assign_tag_to_products,
                                            name='bulk_product_tag'),
    path('product_puc/<int:pk>/', views.assign_puc_to_product,
                                            name='product_puc'),
    path('product_puc_delete/<int:pk>/', views.detach_puc_from_product,
                                            name='product_puc_delete'),
    path('puc-autocomplete/', views.puc_autocomplete.PUCAutocomplete.as_view(),
                                            name='puc-autocomplete'),
    path('product/<int:pk>/', views.product_detail,
                                            name='product_detail'),
    path('product/edit/<int:pk>/', views.product_update,
                                            name='product_edit'),
    path('product/delete/<int:pk>/', views.product_delete,
                                            name='product_delete'),
    path('products/', views.product_list,  name='product_list'),
    path('datadocument/<int:pk>/', views.data_document_detail,
                                            name='data_document'),
    path('save_type/<int:pk>/', views.save_doc_form,
                                            name='save_doc_form'),
    path('save_ext/<int:pk>/', views.save_ext_form,
                                            name='save_ext_form'),
    path('search/', include('haystack.urls')),
    path('find/', views.search.FacetedSearchView.as_view(),
                                            name='haystack_search'),
    path('p_json/', views.product_ajax,     name='p_ajax_url'),
    path('pucs/', views.puc_list,           name='puc_list'),
    path('dl_pucs/', views.download_PUCs,   name='download_PUCs'),
    path('dl_raw_chems/', views.download_raw_chems,  
                                            name='download_raw_chems'),
    path('dsstox_lookup/<int:pk>/', views.dsstox_lookup_detail,
                                            name='dsstox_lookup'),
    path('habitsandpractices/<int:pk>/', views.habitsandpractices,
                                            name='habitsandpractices'),
    path('link_habitandpractice_to_puc/<int:pk>/', views.link_habitsandpractices,
                                            name='link_habitsandpractices'),
    path('get_data/', views.get_data,      name='get_data'),
    path('dl_chem_summary/', views.download_chem_stats,
                                            name='download_chem_stats'),
    path('upload/dtxsid_csv/', views.upload_dtxsid_csv,
                                            name='upload_dtxsid_csv'),
    path('get_data/get_dsstox_csv_template/', views.get_data_dsstox_csv_template,
                                            name='get_data_dsstox_csv_template'),
    path('datagroup/diagnostics/<int:pk>/',   views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('datagroup/diagnostics/',          views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('extractedtext/edit/<int:pk>/',   views.extracted_text_edit,
                                            name='extracted_text_edit'),
    path('extractedchild/edit/<int:pk>/',   views.extracted_child_edit,
                                            name='extracted_child_edit'),
    path('datadocument/edit/<int:pk>/',   views.data_document_edit,
                                            name='data_document_edit'),
]

if settings.DEBUG is True:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
/n/n/ndashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['gen_cat','prod_fam','prod_type','description','PUC_type','num_prods']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                puc.description, 
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/ndashboard/views/data_document.py/n/nfrom django import forms
from django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from djqscsv import render_to_csv_response

from dashboard.forms import *
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if code in ['CP', 'HH'] else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=edit)
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if doc.is_extracted:

        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk) 
        extracted_text_form = ParentForm(instance=extracted_text)
        child_formset = ChildFormSet(instance=extracted_text)

        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True

        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

        colors = ['#d6d6a6', '#dfcaa9', '#d8e5bf'] * 47
        color = (hex for hex in colors)
        for form in child_formset.forms:
            form.color = next(color)
    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})


@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)


@login_required
def data_document_edit(request, pk):

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        return redirect(referer, pk=doc.pk)
    else:
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/ndashboard/views/data_source.py/n/nfrom datetime import datetime

from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from dashboard.forms import DataSourceForm, PriorityForm
from dashboard.models import DataSource, DataGroup, DataDocument
from .data_group import DataGroupForm
from django.db.models import Count, Q



@login_required()
def data_source_list(request, template_name='data_source/datasource_list.html'):
    datasources = DataSource.objects.all()
    ds_list, frm_list = [], []
    for ds in datasources:
        frm_list.append(PriorityForm(request.POST or None, instance=ds))
    registered = Count('datagroup__datadocument') 
    uploaded   = Count('datagroup__datadocument', filter=Q(datagroup__datadocument__matched=True))
    extracted  = Count('datagroup__datadocument__extractedtext')
    ds_list    = DataSource.objects.annotate(registered=registered).annotate(uploaded=uploaded, extracted=extracted)
    out = zip(ds_list, frm_list)
    if request.method == 'POST':
        datasource = DataSource.objects.get(pk=request.POST['ds_pk'])
        form = PriorityForm(request.POST or None, instance=datasource)
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
            return redirect('data_source_list')
    return render(request, template_name, {'object_list': out})


@login_required()
def data_source_detail(request, pk,
                        template_name='data_source/datasource_detail.html'):
    datasource = get_object_or_404(DataSource, pk=pk, )
    docs = DataDocument.objects.filter(data_group__in=DataGroup.objects.filter(data_source=datasource))
    datasource.registered = (len(docs)/float(datasource.estimated_records))*100
    datasource.uploaded = (len(docs.filter(matched=True))/float(
                                            datasource.estimated_records))*100

    form = PriorityForm(request.POST or None, instance=datasource)
    if request.method == 'POST':
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
    datagroup_list = DataGroup.objects.filter(data_source=pk)
    context =     {'object':             datasource,
                'datagroup_list':    datagroup_list,
                'form':             form}
    return render(request, template_name, context)


@login_required()
def data_source_create(request, template_name=('data_source/'
                                                'datasource_form.html')):
    form = DataSourceForm(request.POST or None)
    if form.is_valid():
        form.save()
        return redirect('data_source_list')
    return render(request, template_name, {'form': form})


@login_required()
def data_source_update(request, pk, template_name=('data_source/'
                                                    'datasource_form.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    form = DataSourceForm(request.POST or None, instance=datasource)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_source_detail', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})

@login_required()
def data_source_delete(request, pk,
                        template_name=('data_source/'
                                        'datasource_confirm_delete.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    if request.method == 'POST':
        datasource.delete()
        return redirect('data_source_list')
    return render(request, template_name, {'object': datasource})
/n/n/ndashboard/views/extraction_script.py/n/nfrom django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required

from dashboard.models import *


@login_required()
def extraction_script_list(request, template_name='qa/extraction_script_list.html'):
    """"""
    List view of extraction scripts
    """"""
    # TODO: the user is supposed to be able to click the filter button at the top of the table
    # and toggle between seeing all scripts and seeing only the ones with incomplete QA
    extractionscripts = Script.objects.filter(script_type='EX')
    data = {}
    data['object_list'] = extractionscripts
    return render(request, template_name, data)


@login_required()
def extraction_script_detail(request, pk,
                             template_name='extraction_script/extraction_script_detail.html'):
    extractionscript = get_object_or_404(Script, pk=pk)
    data = {}
    data['object_list'] = extractionscript
    return render(request, template_name, data)


/n/n/ndashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django import forms
from django.db import connection
from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, Subquery, OuterRef, F, Sum
from django.forms.models import model_to_dict

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp

def download_raw_chems(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""uncurated_chemicals_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['data_group_id', 'dashboard_rawchem_id', 'raw_cas', 'raw_chem_name', 'rid'])
    for rawchem in RawChem.objects.filter(dsstox_id=None):
        writer.writerow([rawchem.extracted_text.data_document.data_group.id, rawchem.id, rawchem.raw_cas, rawchem.raw_chem_name, rawchem.rid if rawchem.rid else '' ])

    return response
/n/n/ndashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import timezone, safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.forms import ModelForm
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, 
                            BulkProductPUCForm, BulkProductTagForm, 
                            BulkPUCForm, ProductForm)

from taggit.forms import TagField
from taggit_labels.widgets import LabelWidget
from django.core.paginator import Paginator
from django.db.models import Max

class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]

class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))
    class Meta:
        model = Product
        fields = ['tags']
    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self


@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs

    return render(request, template_name, {'data_sources': data_sources})

@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})

@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})

@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})

@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)

@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})

@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter( Q(title__icontains=q) | Q(brand_name__icontains=q) ).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})

@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })

@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')

@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/ndashboard/views/qa.py/n/nfrom django.shortcuts import render, redirect, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.db.models import Count, Q
from django.http import HttpResponse, HttpResponseRedirect
from django.core.exceptions import ValidationError, MultipleObjectsReturned, ObjectDoesNotExist
from django.urls import reverse
from django.utils import timezone

from dashboard.forms import create_detail_formset, QANotesForm, DocumentTypeForm
from dashboard.models import Script, DataGroup, DataDocument,\
    ExtractedCPCat, ExtractedText, ExtractedListPresence,\
    QAGroup, QANotes, DocumentType
from factotum.settings import EXTRA
from django import forms


@login_required()
def qa_extractionscript_index(request, template_name='qa/extraction_script_index.html'):
    datadocument_count = Count('extractedtext__extraction_script')
    qa_complete_extractedtext_count = Count('extractedtext', filter=Q(extractedtext__qa_checked=True))
    extraction_scripts = Script.objects.\
        annotate(datadocument_count=datadocument_count).\
        annotate(qa_complete_extractedtext_count=qa_complete_extractedtext_count).\
        filter(script_type='EX')

    return render(request, template_name, {'extraction_scripts': extraction_scripts})

@login_required()
def qa_chemicalpresence_index(request, template_name='qa/chemical_presence_index.html'):
    datagroups = DataGroup.objects.filter(group_type__code='CP').\
        annotate(datadocument_count=Count('datadocument'))

    return render(request, template_name, {'datagroups': datagroups})

@login_required()
def qa_chemicalpresence_group(request, pk, template_name='qa/chemical_presence.html'):
    datagroup = DataGroup.objects.get(pk=pk)
    if datagroup.group_type.code != 'CP':
        raise ValidationError('This DataGroup is not of a ChemicalPresence type')
    extractedcpcats = ExtractedCPCat.objects.filter(data_document__data_group=datagroup)
    return render(request, template_name, {'datagroup':datagroup, 'extractedcpcats':extractedcpcats})

def prep_cp_for_qa(extractedcpcat):
    '''
    Given an ExtractedCPCat object, select a sample of its ExtractedListPresence children
    for QA review.
    '''
    from random import shuffle
    QA_RECORDS_PER_DOCUMENT = 30

    if extractedcpcat.rawchem.count() > 0:
        list_presence_count = extractedcpcat.rawchem.count()
    else:
        return
    elps = extractedcpcat.rawchem.select_subclasses()
    non_qa_list_presence_ids = list(elps.filter(extractedlistpresence__qa_flag=False).values_list('pk',flat=True))

    # total number of qa-flagged listpresence objects
    list_presence_qa_count = elps.filter(extractedlistpresence__qa_flag=True).count()

    # if less than 30 records (or all records in set) flagged for QA, make up the difference
    if list_presence_qa_count < QA_RECORDS_PER_DOCUMENT and list_presence_qa_count < list_presence_count:
        random_x = QA_RECORDS_PER_DOCUMENT - list_presence_qa_count
        shuffle(non_qa_list_presence_ids)
        list_presence = ExtractedListPresence.objects.filter(pk__in=non_qa_list_presence_ids[:random_x])
        for lp in list_presence:
            lp.qa_flag = True
            lp.save()
    return

 


@login_required()
def qa_extraction_script(request, pk,
                         template_name='qa/extraction_script.html'):
    """"""
    The user reviews the extracted text and checks whether it was properly converted to data
    """"""
    es = get_object_or_404(Script, pk=pk)
    # If the Script has no related ExtractedText objects, redirect back to the QA index
    if ExtractedText.objects.filter(extraction_script = es).count() == 0 :
        return redirect('/qa/extractionscript/')
    # Check whether QA has begun for the script
    if es.qa_group.count() > 0:
        # if the QA process has begun, there will already be one QA Group
        # associated with the Script.
        try:
            # get the QA Group
            qa_group = QAGroup.objects.get(extraction_script=es,
                                           qa_complete=False)
        except MultipleObjectsReturned:
            qa_group = QAGroup.objects.filter(extraction_script=es,
                                              qa_complete=False).first()
        except ObjectDoesNotExist:
            print('No QA Group was found matching Extraction Script %s' % es.pk)

        texts = ExtractedText.objects.filter(qa_group=qa_group,
                                             qa_checked=False)
        return render(request, template_name, {'extractionscript': es,
                                               'extractedtexts': texts,
                                               'qagroup': qa_group})
    else:
        qa_group = es.create_qa_group()
        es.qa_begun = True
        es.save()
    # Collect all the ExtractedText objects in the QA Group
    texts = ExtractedText.objects.filter(qa_group=qa_group)

    return render(request, template_name, {'extractionscript': es,
                                           'extractedtexts': texts,
                                           'qagroup': qa_group})


def hide_dsstox_fields(formset):
    # Hide the curated DSSToxLookup fields in the formset if they appear
    for form in formset:
        for dssfield in ['true_cas','true_chemname','SID']:
            if dssfield in form.fields:
                form.fields[dssfield].widget = forms.HiddenInput()


@login_required()
def extracted_text_qa(request, pk,
                      template_name='qa/extracted_text_qa.html', nextid=0):
    """"""
    Detailed view of an ExtractedText object, where the user can approve the
    record, edit its ExtractedChemical objects, skip to the next ExtractedText
    in the QA group, or exit to the index page.
    This view processes objects of different models with different QA workflows. 
    The qa_focus variable is used to indicate whether an ExtractedText object is
    part of a QA Group, as with Composition records, or if the DataDocument/ExtractedText
    is its own QA Group, as with ExtractedCPCat and ExtractedHHDoc records.  
    """"""
    extext = get_object_or_404(
	        ExtractedText.objects.select_subclasses(), pk=pk)
    
    doc = DataDocument.objects.get(pk=pk)
    exscript = extext.extraction_script
    group_type_code = extext.data_document.data_group.group_type.code

    if group_type_code in ['CP','HH']:
        qa_focus = 'doc'
        #
        # Document-focused QA process
        #
        # If the object is an ExtractedCPCat record, there will be no Script
        # associated with it and no QA Group
        prep_cp_for_qa(extext)

        stats = ''
        qa_home_page = f'qa/chemicalpresencegroup/%s/' % extext.data_document.data_group.id
    else:
        qa_focus = 'script'
        #
        # Extraction Script-focused QA process
        #
        # when not coming from extraction_script page, the document's script might not have 
        # a QA Group yet. 
        if not extext.qa_group:
            # create the qa group with the optional ExtractedText pk argument
            # so that the ExtractedText gets added to the QA group even if the
            # group uses a random sample
            qa_group = exscript.create_qa_group(pk)
            exscript.qa_begun = True
            exscript.save()
            extext.qa_group = qa_group
            extext.save()
        # get the next unapproved Extracted Text object
        # Its ID will populate the URL for the ""Skip"" button
        if extext.qa_checked:  # if ExtractedText object's QA process done, use 0
            nextid = 0
        else:
            nextid = extext.next_extracted_text_in_qa_group()
        # derive number of approved records and remaining unapproved in QA Group
        a = extext.qa_group.get_approved_doc_count()
        r = ExtractedText.objects.filter(qa_group=extext.qa_group).count() - a
        stats = '%s document(s) approved, %s documents remaining' % (a, r)

    referer = 'data_document' if 'datadocument' in request.path else 'qa_extraction_script'

    # Create the formset factory for the extracted records
    # The model used for the formset depends on whether the
    # extracted text object matches a data document()
    # The QA view should exclude the weight_fraction_type field.
    ParentForm, ChildForm = create_detail_formset(
        doc, EXTRA, can_delete=True, 
        exclude=['weight_fraction_type', 'true_cas', 'true_chemname', 'sid'])
    # extext = extext.pull_out_cp()
    ext_form = ParentForm(instance=extext)
    detail_formset = ChildForm(instance=extext)
    
    # If the document is CPCat or HHE type, the display should only show the
    # child records where qa_flag = True
    if qa_focus == 'doc' and hasattr(detail_formset.get_queryset().model, 'qa_flag'):
        qs = detail_formset.get_queryset().filter(qa_flag=True)
        detail_formset._queryset = qs
    
    # This code is being repeated in the GET and POST blocks
    # 
    # Hide all the DSSToxLookup fields 
    hide_dsstox_fields(detail_formset)

    # Add CSS selector classes to each form
    for form in detail_formset:
        for field in form.fields:
            form.fields[field].widget.attrs.update(
                {'class': f'detail-control form-control %s' % doc.data_group.type}
            )

    note, created = QANotes.objects.get_or_create(extracted_text=extext)
    notesform = QANotesForm(instance=note)

    # Allow the user to edit the data document type
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    # the form class overrides the label, so over-override it
    document_type_form.fields['document_type'].label = 'Data document type:'

    context = {
        'extracted_text': extext,
        'doc': doc,
        'script': exscript,
        'stats': stats,
        'nextid': nextid,
        'detail_formset': detail_formset,
        'notesform': notesform,
        'ext_form': ext_form,
        'referer': referer,
        'document_type_form': document_type_form
    }

    if request.method == 'POST' and 'save' in request.POST:
        # The save action only applies to the child records and QA properties,
        # no need to save the ExtractedText form
        ParentForm, ChildForm = create_detail_formset(
            doc, EXTRA, can_delete=True, exclude=['weight_fraction_type'])
        # extext = extext.pull_out_cp()
        detail_formset = ChildForm(request.POST, instance=extext)

        notesform = QANotesForm(request.POST, instance=note)
        notesform.save()
        if detail_formset.has_changed():
            if detail_formset.is_valid() :
                detail_formset.save()
                extext.qa_edited = True
                extext.save()
                # rebuild the formset after saving it
                detail_formset = ChildForm(instance=extext)
            else:
                pass
                # print(detail_formset.errors)
                # TODO: iterate through this dict of errors and map each error to
                # the corresponding form in the template for rendering

            context['detail_formset'] = detail_formset
            context['ext_form'] = ext_form
            # calls the clean method? y?
            context.update({'notesform': notesform})

        # This code is being repeated in the GET and POST blocks
        # 
        # Hide all the DSSToxLookup fields 
        hide_dsstox_fields(detail_formset)

        # Add CSS selector classes to each form
        for form in detail_formset:
            for field in form.fields:
                form.fields[field].widget.attrs.update(
                    {'class': f'detail-control form-control %s' % doc.data_group.type}
                )

    elif request.method == 'POST' and 'approve' in request.POST:  # APPROVAL
        notesform = QANotesForm(request.POST, instance=note)
        context['notesform'] = notesform
        nextpk = extext.next_extracted_text_in_qa_group()
        if notesform.is_valid():
            extext.qa_approved_date = timezone.now()
            extext.qa_approved_by = request.user
            extext.qa_checked = True
            extext.save()
            notesform.save()
            # After approval, the user proceeds to either the next document
            # in the QA Group, to the extractionscript QA index, or to the
            # index page that matches the document's data group type 
            # 

            if referer == 'data_document':
                # The user got to the QA page from a data document detail page,
                # so return there
                return HttpResponseRedirect(
                    reverse(referer, kwargs={'pk': pk}))
            elif not nextpk == 0:
                return HttpResponseRedirect(
                    reverse('extracted_text_qa', args=[(nextpk)]))
            elif nextpk == 0:
                # return to the top of the most local QA stack.
                # that may be the list of ExtractionScripts or 
                # the list of Chemical Presence Data Groups
                return HttpResponseRedirect(
                        extext.get_qa_index_path()
                            )
        else:
            # The notesform is not valid
            pass
    return render(request, template_name, context)
/n/n/n",0
81,81,a771b17caf63a406bb0cb4d142d70eb5425e0428,"/dashboard/admin.py/n/nfrom django.contrib import admin
from dashboard.models import *
from django.db.models import Count

from django import forms
from taggit_labels.widgets import LabelWidget
from dashboard.signals import *

class PUCAdminForm(forms.ModelForm):
    class Meta:
        model = PUC
        fields = ['gen_cat', 'prod_fam', 'prod_type', 'description','tags',]
        readonly_fields = ('num_products',)
        widgets = {
            'tags': LabelWidget(model=PUCTag),
        }

class PUCAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'tag_list','num_products')
    form = PUCAdminForm
    def get_changeform_initial_data(self, request):
        get_data = super(PUCAdmin, self).get_changeform_initial_data(request)
        get_data['last_edited_by'] = request.user.pk
        return get_data
    def get_queryset(self, request):
        return super(PUCAdmin, self).get_queryset(request).prefetch_related('tags').annotate(num_products=Count('products'))
    def num_products(self, obj):
        return obj.num_products
    num_products.short_description = 'Product Count'
    num_products.admin_order_field = 'num_products'
    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())

class HHDocAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'hhe_report_number')

class PUCToTagAdmin(admin.ModelAdmin):
    list_display = ('content_object', 'tag', 'assumed')
    list_filter = ('tag',)
    def tag(self, obj):
        return obj.tag    
    def assumed(self, obj):
        return obj.assumed 

# Register your models here.
admin.site.register(DataSource)
admin.site.register(GroupType)
admin.site.register(DataGroup)
admin.site.register(DocumentType)
admin.site.register(DataDocument)
admin.site.register(Script)
admin.site.register(Product)
admin.site.register(ProductToPUC)
admin.site.register(ProductDocument)
admin.site.register(SourceCategory)
admin.site.register(PUC, PUCAdmin)
admin.site.register(ExtractedText)
admin.site.register(ExtractedChemical)
admin.site.register(ExtractedFunctionalUse)
admin.site.register(ExtractedHabitsAndPractices)
admin.site.register(DSSToxLookup)
admin.site.register(QAGroup)
admin.site.register(UnitType)
admin.site.register(WeightFractionType)
admin.site.register(PUCTag) #,ProductTagAdmin
admin.site.register(Taxonomy)
admin.site.register(TaxonomySource)
admin.site.register(TaxonomyToPUC)
admin.site.register(ExtractedHHDoc, HHDocAdmin)
admin.site.register(ExtractedHHRec)
admin.site.register(PUCToTag, PUCToTagAdmin)
/n/n/n",1
128,128,8f82110e49c7f357eb9cc77658d6c65f898dde09,"TexDBook/src/python/core/db_loader.py/n/nfrom peewee import Database, SqliteDatabase


def can_import_apsw():
    # type: () -> bool
    # return False  # for now
    try:
        import apsw
        return True
    except ImportError:
        return False


TexDBookDatabase = None  # type: Database

if can_import_apsw():
    from playhouse.apsw_ext import APSWDatabase
    
    TexDBookDatabase = APSWDatabase
else:
    TexDBookDatabase = SqliteDatabase
/n/n/n",0
129,129,8f82110e49c7f357eb9cc77658d6c65f898dde09,"/TexDBook/src/python/core/db_loader.py/n/nfrom peewee import Database, SqliteDatabase


def can_import_apsw():
    # type: () -> bool
    return False  # for now
    try:
        import apsw
        return True
    except ImportError:
        return False


TexDBookDatabase = None  # type: Database

if can_import_apsw():
    from playhouse.apsw_ext import APSWDatabase
    
    TexDBookDatabase = APSWDatabase
else:
    TexDBookDatabase = SqliteDatabase
/n/n/n",1
26,26,be720513bcc09df7b0787bc6bde033023933061f,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
27,27,be720513bcc09df7b0787bc6bde033023933061f,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
172,172,5dacbd856f34a1f35186284a9468adf708a62e4b,"src/Shortener_App/views.py/n/nfrom django.shortcuts import render, get_object_or_404, reverse, redirect, HttpResponse
from django.urls import reverse_lazy
from django.views import View
from django.views.generic import CreateView, ListView, UpdateView, DeleteView
from django.contrib.auth.mixins import LoginRequiredMixin
from .forms import ShortUrlForm, JustURLForm, CategoryModelForm, ManyURLSForm, JustULRUpdateForm, \
    CategoryUpdateModelForm, CounterCountingForm
from .models import JustURL, Category, ClickTracking
from .utils import create_short_url, token_generator, generate_csv, get_client_ip, check_input_url
from Shortener_Project.settings import SHORTCODE
import re


class HomeView(View):
    def get(self, request, *args, **kwargs):
        form = ShortUrlForm()
        return render(request, 'home.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = ShortUrlForm(request.POST or None)
        if form.is_valid():
            url = form.cleaned_data['input_url']
            category = form.cleaned_data['category']
            created = JustURL.objects.create(input_url=url, category=category)
            short_url = create_short_url(created)
            created.short_url = f'{request.get_host()}/{short_url}'
            created.save()
            if request.user.is_superuser:
                return redirect(reverse('url-detail-view', kwargs={'pk': created.pk}))
            return redirect(reverse('success-url-view', kwargs={'pk': created.pk}))
        return render(request, 'home.html', {'form': form})


class SuccessUrlView(View):
    def get(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm()
        return render(request, 'success-url-view.html', {'object': object,
                                                         'form': form})

    def post(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm(request.POST or None)
        if form.is_valid():
            ip = get_client_ip(request)
            client_agent = request.META['HTTP_USER_AGENT']
            clicktracker = ClickTracking.objects.create(
                client_ip=ip,
                user_agent=client_agent,
            )
            clicktracker.url.add(object)
            clicktracker.save()
            token = object.short_url[-SHORTCODE:]
            return link_redirect(request, token)
        return redirect('home-view')


class URLDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        form = CounterCountingForm()
        object = JustURL.objects.get(pk=pk)
        return render(request, 'url-detail-view.html', {'object': object,
                                                        'form': form})

    def post(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm(request.POST or None)
        if form.is_valid():
            ip = get_client_ip(request)
            client_agent = request.META['HTTP_USER_AGENT']
            clicktracker = ClickTracking.objects.create(
                client_ip=ip,
                user_agent=client_agent,
            )
            clicktracker.url.add(object)
            clicktracker.save()
            token = object.short_url[-SHORTCODE:]
            return link_redirect(request, token)
        return render(request, 'url-detail-view.html', {'object': object,
                                                        'form': form})


class URLUpdateView(LoginRequiredMixin, UpdateView):
    queryset = JustURL.objects.all()
    form_class = JustULRUpdateForm
    template_name = 'url-update-view.html'


class URLDeleteView(LoginRequiredMixin, DeleteView):
    model = JustURL
    template_name = 'url-delete-view.html'
    success_url = reverse_lazy('home-view')


class CustomShortURLCreateView(View):
    def get(self, request, *args, **kwargs):
        form = JustURLForm()
        return render(request, 'custom-short-url.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = JustURLForm(request.POST or None)
        if form.is_valid():
            url = form.cleaned_data['input_url']
            short_url = form.cleaned_data['short_url']
            category = form.cleaned_data['category']
            if JustURL.objects.filter(short_url__contains=short_url).exists():
                message = 'Token is already in use'
                return render(request, 'custom-short-url.html', {'form': JustURLForm,
                                                                 'message': message})
            created = JustURL.objects.create(input_url=url, short_url=f'{request.get_host()}/{short_url}',
                                             category=category)
            created.save()
            if request.user.is_superuser:
                return redirect(reverse('url-detail-view', kwargs={'pk': created.pk}))
            return redirect(reverse('success-url-view', kwargs={'pk': created.pk}))
        return render(request, 'home.html', {'form': form})


class ShortManyURLSView(View):
    def get(self, request, *args, **kwargs):
        form = ManyURLSForm()
        return render(request, 'short-many-urls.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = ManyURLSForm(request.POST or None)
        if form.is_valid():
            urls = form.cleaned_data['input_url']
            urls_list = re.findall(r""[\w.']+"", urls)
            data_list = []
            for url in urls_list:
                result = check_input_url(url)
                instance = JustURL.objects.create(input_url=result,
                                                  short_url=f'{request.get_host()}/{token_generator()}')
                instance.save()
                data = [instance.input_url, instance.short_url]
                data_list.append(data)

            response = HttpResponse(content_type='text/csv')
            response['Content-Disposition'] = 'attachment; filename=""many_urls.csv""'
            return generate_csv(data_list, response)
        return redirect('home-view')


class CategoryCreateView(LoginRequiredMixin, CreateView):
    template_name = 'category-create-view.html'
    form_class = CategoryModelForm


class CategoryListView(LoginRequiredMixin, ListView):
    queryset = Category.objects.all().order_by('name')
    template_name = 'category-list-view.html'
    paginate_by = 15

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        quantity = 0
        urls_without_category = JustURL.objects.filter(category=None).count()
        print(urls_without_category)
        queryset = Category.objects.all()
        for cat in queryset:
            quantity += cat.justurl_set.all().count()
        context['number_of_links'] = quantity
        context['urls_without_category'] = urls_without_category
        return context


class CategoryDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        object = Category.objects.get(pk=pk)
        visits = sum(link.count for link in object.justurl_set.all())
        return render(request, 'category-detail-view.html', {'object': object,
                                                             'visits': visits})


class CategoryUpdateView(LoginRequiredMixin, UpdateView):
    queryset = Category.objects.all()
    form_class = CategoryUpdateModelForm
    template_name = 'category-update-view.html'
    success_url = reverse_lazy('category-list-view')


class CategoryDeleteView(LoginRequiredMixin, DeleteView):
    model = Category
    template_name = 'category-delete-view.html'
    success_url = reverse_lazy('category-list-view')


class ClickTrackingDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        object = get_object_or_404(JustURL, pk=pk)
        reports = object.clicktracking_set.all().order_by('timestamp')
        return render(request, 'clicktracking-detail-view.html', {'object': object,
                                                                  'reports': reports})


def link_redirect(request, token):
    instance = JustURL.objects.get(short_url__icontains=token)
    instance.count += 1
    instance.save()
    return redirect(instance.input_url)
/n/n/nsrc/Shortener_Project/urls.py/n/nfrom django.contrib import admin
from django.urls import re_path
from Shortener_App.views import (
    HomeView,
    SuccessUrlView,
    CustomShortURLCreateView,
    ShortManyURLSView,
    URLDetailView,
    URLUpdateView,
    URLDeleteView,
    CategoryCreateView,
    CategoryListView,
    CategoryDetailView,
    CategoryUpdateView,
    CategoryDeleteView,
    ClickTrackingDetailView,
    link_redirect
)

urlpatterns = [
    re_path(r'admin/', admin.site.urls),
    #  urls
    re_path(r'^$', HomeView.as_view(), name='home-view'),
    re_path(r'^success/(?P<pk>(\d)+)/$', SuccessUrlView.as_view(), name='success-url-view'),
    re_path(r'^add-custom/$', CustomShortURLCreateView.as_view(), name='add-custom-url'),
    re_path(r'^add-many/$', ShortManyURLSView.as_view(), name='add-many-urls'),
    re_path(r'^detail/(?P<pk>(\d)+)/$', URLDetailView.as_view(), name='url-detail-view'),
    re_path(r'^update/(?P<pk>(\d)+)/$', URLUpdateView.as_view(), name='url-update-view'),
    re_path(r'^delete/(?P<pk>(\d)+)/$', URLDeleteView.as_view(), name='url-delete-view'),
    #  categories
    re_path(r'^category/add/$', CategoryCreateView.as_view(), name='category-create-view'),
    re_path(r'^categories/$', CategoryListView.as_view(), name='category-list-view'),
    re_path(r'^detail/category/(?P<pk>(\d)+)/$', CategoryDetailView.as_view(), name='category-detail-view'),
    re_path(r'^update/category/(?P<pk>(\d)+)/$', CategoryUpdateView.as_view(), name='category-update-view'),
    re_path(r'^delete/category/(?P<pk>(\d)+)/$', CategoryDeleteView.as_view(), name='category-delete-view'),
    #  click tracking
    re_path(r'^(?P<pk>(\d)+)/reports/$', ClickTrackingDetailView.as_view(), name='clicktracking-detail-view'),
    #  redirecting
    re_path(r'^(?P<token>(\w)+)/$', link_redirect, name='url-redirect-view'),
]
/n/n/n",0
173,173,5dacbd856f34a1f35186284a9468adf708a62e4b,"/src/Shortener_App/views.py/n/nfrom django.shortcuts import render, get_object_or_404, reverse, redirect, HttpResponse
from django.urls import reverse_lazy
from django.views import View
from django.views.generic import CreateView, ListView, UpdateView, DeleteView
from django.contrib.auth.mixins import LoginRequiredMixin
from .forms import ShortUrlForm, JustURLForm, CategoryModelForm, ManyURLSForm, JustULRUpdateForm, \
    CategoryUpdateModelForm, CounterCountingForm
from .models import JustURL, Category, ClickTracking
from .utils import create_short_url, token_generator, generate_csv, get_client_ip, check_input_url
import re


class HomeView(View):
    def get(self, request, *args, **kwargs):
        form = ShortUrlForm()
        return render(request, 'home.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = ShortUrlForm(request.POST or None)
        if form.is_valid():
            url = form.cleaned_data['input_url']
            category = form.cleaned_data['category']
            created = JustURL.objects.create(input_url=url, category=category)
            short_url = create_short_url(created)
            created.short_url = f'{request.get_host()}/{short_url}'
            created.save()
            if request.user.is_superuser:
                return redirect(reverse('url-detail-view', kwargs={'pk': created.pk}))
            return redirect(reverse('success-url-view', kwargs={'pk': created.pk}))
        return render(request, 'home.html', {'form': form})


class SuccessUrlView(View):
    def get(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm()
        return render(request, 'success-url-view.html', {'object': object,
                                                         'form': form})

    def post(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm(request.POST or None)
        if form.is_valid():
            object.count += 1
            ip = get_client_ip(request)
            client_agent = request.META['HTTP_USER_AGENT']
            clicktracker = ClickTracking.objects.create(
                client_ip=ip,
                user_agent=client_agent,
            )
            clicktracker.url.add(object)
            clicktracker.save()
            object.save()
            return link_redirect(request, pk)
        return redirect('home-view')


class URLDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        form = CounterCountingForm()
        object = JustURL.objects.get(pk=pk)
        return render(request, 'url-detail-view.html', {'object': object,
                                                        'form': form})

    def post(self, request, pk, *args, **kwargs):
        object = JustURL.objects.get(pk=pk)
        form = CounterCountingForm(request.POST or None)
        if form.is_valid():
            object.count += 1
            ip = get_client_ip(request)
            client_agent = request.META['HTTP_USER_AGENT']
            clicktracker = ClickTracking.objects.create(
                client_ip=ip,
                user_agent=client_agent,
            )
            clicktracker.url.add(object)
            clicktracker.save()
            object.save()
            return link_redirect(request, pk)
        return render(request, 'url-detail-view.html', {'object': object,
                                                        'form': form})


class URLUpdateView(LoginRequiredMixin, UpdateView):
    queryset = JustURL.objects.all()
    form_class = JustULRUpdateForm
    template_name = 'url-update-view.html'


class URLDeleteView(LoginRequiredMixin, DeleteView):
    model = JustURL
    template_name = 'url-delete-view.html'
    success_url = reverse_lazy('home-view')


class CustomShortURLCreateView(View):
    def get(self, request, *args, **kwargs):
        form = JustURLForm()
        return render(request, 'custom-short-url.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = JustURLForm(request.POST or None)
        if form.is_valid():
            url = form.cleaned_data['input_url']
            short_url = form.cleaned_data['short_url']
            category = form.cleaned_data['category']
            if JustURL.objects.filter(short_url__contains=short_url).exists():
                message = 'Token is already in use'
                return render(request, 'custom-short-url.html', {'form': JustURLForm,
                                                                 'message': message})
            created = JustURL.objects.create(input_url=url, short_url=f'{request.get_host()}/{short_url}',
                                             category=category)
            created.save()
            if request.user.is_superuser:
                return redirect(reverse('url-detail-view', kwargs={'pk': created.pk}))
            return redirect(reverse('success-url-view', kwargs={'pk': created.pk}))
        return render(request, 'home.html', {'form': form})


class ShortManyURLSView(View):
    def get(self, request, *args, **kwargs):
        form = ManyURLSForm()
        return render(request, 'short-many-urls.html', {'form': form})

    def post(self, request, *args, **kwargs):
        form = ManyURLSForm(request.POST or None)
        if form.is_valid():
            urls = form.cleaned_data['input_url']
            urls_list = re.findall(r""[\w.']+"", urls)
            data_list = []
            for url in urls_list:
                result = check_input_url(url)
                instance = JustURL.objects.create(input_url=result,
                                                  short_url=f'{request.get_host()}/{token_generator()}')
                instance.save()
                data = [instance.input_url, instance.short_url]
                data_list.append(data)

            response = HttpResponse(content_type='text/csv')
            response['Content-Disposition'] = 'attachment; filename=""many_urls.csv""'
            return generate_csv(data_list, response)
        return redirect('home-view')


class CategoryCreateView(LoginRequiredMixin, CreateView):
    template_name = 'category-create-view.html'
    form_class = CategoryModelForm


class CategoryListView(LoginRequiredMixin, ListView):
    queryset = Category.objects.all().order_by('name')
    template_name = 'category-list-view.html'
    paginate_by = 15

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        quantity = 0
        urls_without_category = JustURL.objects.filter(category=None).count()
        print(urls_without_category)
        queryset = Category.objects.all()
        for cat in queryset:
            quantity += cat.justurl_set.all().count()
        context['number_of_links'] = quantity
        context['urls_without_category'] = urls_without_category
        return context


class CategoryDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        object = Category.objects.get(pk=pk)
        visits = sum(link.count for link in object.justurl_set.all())
        return render(request, 'category-detail-view.html', {'object': object,
                                                             'visits': visits})


class CategoryUpdateView(LoginRequiredMixin, UpdateView):
    queryset = Category.objects.all()
    form_class = CategoryUpdateModelForm
    template_name = 'category-update-view.html'
    success_url = reverse_lazy('category-list-view')


class CategoryDeleteView(LoginRequiredMixin, DeleteView):
    model = Category
    template_name = 'category-delete-view.html'
    success_url = reverse_lazy('category-list-view')


class ClickTrackingDetailView(LoginRequiredMixin, View):
    def get(self, request, pk, *args, **kwargs):
        object = get_object_or_404(JustURL, pk=pk)
        reports = object.clicktracking_set.all().order_by('timestamp')
        return render(request, 'clicktracking-detail-view.html', {'object': object,
                                                                  'reports': reports})


def link_redirect(request, pk):
    instance = get_object_or_404(JustURL, pk=pk)
    return redirect(instance.input_url)
/n/n/n/src/Shortener_Project/urls.py/n/nfrom django.contrib import admin
from django.urls import re_path
from Shortener_App.views import (
    HomeView,
    SuccessUrlView,
    CustomShortURLCreateView,
    ShortManyURLSView,
    URLDetailView,
    URLUpdateView,
    URLDeleteView,
    CategoryCreateView,
    CategoryListView,
    CategoryDetailView,
    CategoryUpdateView,
    CategoryDeleteView,
    ClickTrackingDetailView,
    link_redirect
)

urlpatterns = [
    re_path(r'admin/', admin.site.urls),
    #  urls
    re_path(r'^$', HomeView.as_view(), name='home-view'),
    re_path(r'^success/(?P<pk>(\d)+)/$', SuccessUrlView.as_view(), name='success-url-view'),
    re_path(r'^add-custom/$', CustomShortURLCreateView.as_view(), name='add-custom-url'),
    re_path(r'^add-many/$', ShortManyURLSView.as_view(), name='add-many-urls'),
    re_path(r'^detail/(?P<pk>(\d)+)/$', URLDetailView.as_view(), name='url-detail-view'),
    re_path(r'^update/(?P<pk>(\d)+)/$', URLUpdateView.as_view(), name='url-update-view'),
    re_path(r'^delete/(?P<pk>(\d)+)/$', URLDeleteView.as_view(), name='url-delete-view'),
    #  categories
    re_path(r'^category/add/$', CategoryCreateView.as_view(), name='category-create-view'),
    re_path(r'^categories/$', CategoryListView.as_view(), name='category-list-view'),
    re_path(r'^detail/category/(?P<pk>(\d)+)/$', CategoryDetailView.as_view(), name='category-detail-view'),
    re_path(r'^update/category/(?P<pk>(\d)+)/$', CategoryUpdateView.as_view(), name='category-update-view'),
    re_path(r'^delete/category/(?P<pk>(\d)+)/$', CategoryDeleteView.as_view(), name='category-delete-view'),
    #  click tracking
    re_path(r'^(?P<pk>(\d)+)/reports/$', ClickTrackingDetailView.as_view(), name='clicktracking-detail-view'),
    #  redirecting
    re_path(r'^(?P<pk>(\d)+)/$', link_redirect, name='url-redirect-view'),
]
/n/n/n",1
126,126,50563e07a14d5f43f339e6b241ff2b9c8692ee1c,"keepassgtk/main_window.py/n/nfrom gi.repository import Gio, Gdk, Gtk
from keepassgtk.logging_manager import LoggingManager
from keepassgtk.database_manager import DatabaseManager
from keepassgtk.create_database import CreateDatabase
from keepassgtk.container_page import ContainerPage
from keepassgtk.unlock_database import UnlockDatabase
import keepassgtk.config_manager
import os
from os.path import exists
import ntpath
import gi
gi.require_version('Gtk', '3.0')
gi.require_version('Gdk', '3.0')


class MainWindow(Gtk.ApplicationWindow):
    application = NotImplemented
    database_manager = NotImplemented
    container = NotImplemented
    override_dialog = NotImplemented
    quit_dialog = NotImplemented
    filechooser_creation_dialog = NotImplemented
    headerbar = NotImplemented
    first_start_grid = NotImplemented
    logging_manager = LoggingManager(True)
    opened_databases = []
    databases_to_save = []

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        keepassgtk.config_manager.configure()
        self.assemble_window()

    def assemble_window(self):
        self.set_default_size(800, 500)
        
        self.create_headerbar()
        self.first_start_screen()

        self.connect(""delete-event"", self.on_application_quit)

        self.custom_css()

    #
    # Headerbar
    #

    def create_headerbar(self):
        builder = Gtk.Builder()
        builder.add_from_resource(""/run/terminal/KeepassGtk/main_window.ui"")

        self.headerbar = builder.get_object(""headerbar"")

        file_open_button = builder.get_object(""open_button"")
        file_open_button.connect(""clicked"", self.open_filechooser, None)

        file_new_button = builder.get_object(""new_button"")
        file_new_button.connect(""clicked"", self.create_filechooser, None)

        self.set_titlebar(self.headerbar)

    def set_headerbar(self):
        self.set_titlebar(self.headerbar)

    def get_headerbar(self):
        return self.headerbar

    #
    # Styles
    #

    def custom_css(self):
        screen = Gdk.Screen.get_default()

        css_provider = Gtk.CssProvider()
        css_provider_resource = Gio.File.new_for_uri(
            ""resource:///run/terminal/KeepassGtk/keepassgtk.css"")
        css_provider.load_from_file(css_provider_resource)

        context = Gtk.StyleContext()
        context.add_provider_for_screen(
            screen, css_provider, Gtk.STYLE_PROVIDER_PRIORITY_USER)

    #
    # First Start Screen
    #

    def first_start_screen(self):
        if keepassgtk.config_manager.has_group(""history"") and keepassgtk.config_manager.get_string(""history"", ""last-opened-db"") != """" and exists(keepassgtk.config_manager.get_string(""history"", ""last-opened-db"")):
            self.logging_manager.log_debug(
                ""Found last opened database entry ("" +
                keepassgtk.config_manager.get_string(
                    ""history"", ""last-opened-db"") + "")"")

            tab_title = ntpath.basename(keepassgtk.config_manager.get_string(
                ""history"", ""last-opened-db""))
            self.start_database_opening_routine(
                tab_title,
                keepassgtk.config_manager.get_string(
                    ""history"", ""last-opened-db""))
        else:
            self.logging_manager.log_debug(
                ""No / Not valid last opened database entry found."")
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/main_window.ui"")

            self.first_start_grid = builder.get_object(""first_start_grid"")
            self.add(self.first_start_grid)

    #
    # Container Methods (Gtk Notebook holds tabs)
    #

    def create_container(self):
        if self.first_start_grid != NotImplemented:
            self.first_start_grid.destroy()

        self.container = Gtk.Notebook()

        self.container.set_border_width(0)
        self.container.set_scrollable(True)
        self.container.set_show_border(False)
        self.container.connect(""switch-page"", self.on_tab_switch)

        self.add(self.container)
        self.show_all()

    def destroy_container(self):
        self.container.destroy()

    #
    # Open Database Methods
    #

    def open_filechooser(self, widget, none):
        filechooser_opening_dialog = Gtk.FileChooserDialog(
            ""Choose Keepass Database"", self, Gtk.FileChooserAction.OPEN,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL,
             Gtk.STOCK_OPEN, Gtk.ResponseType.OK))

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keepass 2 Database"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filechooser_opening_dialog.add_filter(filter_text)

        response = filechooser_opening_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(
                ""File selected: "" + filechooser_opening_dialog.get_filename())
            filechooser_opening_dialog.close()

            database_already_opened = False

            for db in self.opened_databases:
                if db.database_manager.database_path == filechooser_opening_dialog.get_filename():
                    database_already_opened = True
                    page_num = self.container.page_num(db.parent_widget)
                    self.container.set_current_page(page_num)
                    db.show_database_action_revealer(""Database already opened"")

            if database_already_opened is False:
                tab_title = self.create_tab_title_from_filepath(
                    filechooser_opening_dialog.get_filename())
                self.start_database_opening_routine(
                    tab_title, filechooser_opening_dialog.get_filename())
        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection canceled"")
            filechooser_opening_dialog.close()

    def start_database_opening_routine(self, tab_title, filepath):
        builder = Gtk.Builder()
        builder.add_from_resource(
            ""/run/terminal/KeepassGtk/create_database.ui"")
        headerbar = builder.get_object(""headerbar"")

        UnlockDatabase(self, self.create_tab(tab_title, headerbar), filepath)

    #
    # Create Database Methods
    #

    def create_filechooser(self, widget, none):
        self.filechooser_creation_dialog = Gtk.FileChooserDialog(
            ""Create new Database"", self, Gtk.FileChooserAction.SAVE,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL,
             Gtk.STOCK_SAVE, Gtk.ResponseType.OK))
        self.filechooser_creation_dialog.set_current_name(""Database.kdbx"")
        self.filechooser_creation_dialog.set_modal(True)

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keepass 2 Database"")
        filter_text.add_mime_type(""application/x-keepass2"")
        self.filechooser_creation_dialog.add_filter(filter_text)

        response = self.filechooser_creation_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.does_file_exist()
        elif response == Gtk.ResponseType.CANCEL:
            self.filechooser_creation_dialog.close()

    def does_file_exist(self):
        if os.path.exists(self.filechooser_creation_dialog.get_filename()):
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/override_dialog.ui"")
            self.override_dialog = builder.get_object(""override_dialog"")

            self.override_dialog.set_destroy_with_parent(True)
            self.override_dialog.set_modal(True)
            self.override_dialog.set_transient_for(self.filechooser_creation_dialog)

            cancel_button = builder.get_object(""cancel_button"")
            override_button = builder.get_object(""override_button"")

            cancel_button.connect(""clicked"", self.on_cancel_button_clicked)
            override_button.connect(""clicked"", self.on_override_button_clicked)

            self.override_dialog.present()
        else:
            self.copy_database_file()

            tab_title = self.create_tab_title_from_filepath(self.filechooser_creation_dialog.get_current_name())
            self.start_database_creation_routine(tab_title)

    def copy_database_file(self):
        stock_database = Gio.File.new_for_uri(
            ""resource:///run/terminal/KeepassGtk/database.kdbx"")
        new_database = Gio.File.new_for_path(
            self.filechooser_creation_dialog.get_filename())

        stock_database.copy(new_database, Gio.FileCopyFlags.OVERWRITE)
        self.filechooser_creation_dialog.close()

    def start_database_creation_routine(self, tab_title):
        self.database_manager = DatabaseManager(
            self.filechooser_creation_dialog.get_filename(),
            ""liufhre86ewoiwejmrcu8owe"")
        builder = Gtk.Builder()
        builder.add_from_resource(
            ""/run/terminal/KeepassGtk/create_database.ui"")
        headerbar = builder.get_object(""headerbar"")
        CreateDatabase(
            self, self.create_tab(tab_title, headerbar),
            self.database_manager)

    #
    # Tab Manager
    #

    def create_tab(self, title, headerbar):
        if self.container == NotImplemented:
            self.create_container()

        page_instance = ContainerPage(headerbar)

        tab_hbox = Gtk.HBox(False, 0)
        tab_label = Gtk.Label(title)
        tab_hbox.pack_start(tab_label, False, False, False)

        icon = Gio.ThemedIcon(name=""window-close-symbolic"")
        close_image = Gtk.Image.new_from_gicon(icon, Gtk.IconSize.BUTTON)
        close_button = Gtk.Button()
        close_button.set_relief(Gtk.ReliefStyle.NONE)
        close_button.set_focus_on_click(False)
        close_button.connect(""clicked"", self.on_tab_close_button_clicked, page_instance)
        close_button.add(close_image)

        tab_hbox.pack_start(close_button, False, False, False)
        tab_hbox.show_all()

        self.container.append_page(page_instance, tab_hbox)
        self.container.set_current_page(self.container.page_num(page_instance))
        self.update_tab_bar_visibility()

        return page_instance

    def update_tab_bar_visibility(self):
        if self.container.get_n_pages() > 1:
            self.container.set_show_tabs(True)
        else:
            self.container.set_show_tabs(False)

    def create_tab_title_from_filepath(self, filepath):
        return ntpath.basename(filepath)

    def close_tab(self, child_widget):
        page_num = self.container.page_num(child_widget)
        self.container.remove_page(page_num)
        self.update_tab_bar_visibility()

    #
    # Events
    #

    def on_tab_close_button_clicked(self, sender, widget):
        page_num = self.container.page_num(widget)

        for db in self.opened_databases:
            if db.window.container.page_num(db.parent_widget) == page_num:
                self.opened_databases.remove(db)

        self.container.remove_page(page_num)
        self.update_tab_bar_visibility()

    def on_cancel_button_clicked(self, widget):
        self.override_dialog.destroy()
        self.filechooser_creation_dialog.destroy()

    def on_override_button_clicked(self, widget):
        self.copy_database_file()

        tab_title = self.create_tab_title_from_filepath(
            self.filechooser_creation_dialog.get_current_name())
        self.start_database_creation_routine(tab_title)

        self.override_dialog.destroy()

    def on_tab_switch(self, notebook, tab, pagenum):
        headerbar = tab.get_headerbar()
        self.set_titlebar(headerbar)

    def on_save_check_button_toggled(self, check_button, db):
        if check_button.get_active():
            self.databases_to_save.append(db)
        else:
            self.databases_to_save.remove(db)

    def on_back_button_clicked(self, button):
        self.databases_to_save.clear()
        self.quit_dialog.destroy()

    def on_quit_button_clicked(self, button):
        for db in self.databases_to_save:
            db.database_manager.save_database()

        self.quit_dialog.destroy()
        self.application.quit()

    #
    # Application Quit Dialog
    #

    def on_application_quit(self, window, event):
        unsaved_databases_list = []
        for db in self.opened_databases:
            if db.database_manager.changes is True:
                unsaved_databases_list.append(db)

        if unsaved_databases_list.__len__() > 0:
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/quit_dialog.ui"")
            self.quit_dialog = builder.get_object(""quit_dialog"")

            self.quit_dialog.set_destroy_with_parent(True)
            self.quit_dialog.set_modal(True)
            self.quit_dialog.set_transient_for(self)

            back_button = builder.get_object(""back_button"")
            quit_button = builder.get_object(""quit_button"")

            back_button.connect(""clicked"", self.on_back_button_clicked)
            quit_button.connect(""clicked"", self.on_quit_button_clicked)

            unsaved_databases_list_box = builder.get_object(""unsaved_databases_list_box"")
                
            for db in unsaved_databases_list:
                unsaved_database_row = Gtk.ListBoxRow()
                check_button = Gtk.CheckButton()
                check_button.set_label(db.database_manager.database_path)
                check_button.connect(""toggled"", self.on_save_check_button_toggled, db)
                unsaved_database_row.add(check_button)
                unsaved_database_row.show_all()
                unsaved_databases_list_box.add(unsaved_database_row)

            self.quit_dialog.present()

            return(True)         
/n/n/nkeepassgtk/unlock_database.py/n/nfrom gi.repository import Gio, Gtk
from keepassgtk.database_manager import DatabaseManager
from keepassgtk.unlocked_database import UnlockedDatabase
import keepassgtk.config_manager
from keepassgtk.logging_manager import LoggingManager
import gi
gi.require_version('Gtk', '3.0')
import ntpath
import threading


class UnlockDatabase:
    builder = NotImplemented
    parent_widget = NotImplemented
    window = NotImplemented
    database_filepath = NotImplemented
    database_manager = NotImplemented
    unlock_database_stack_box = NotImplemented
    keyfile = NotImplemented
    composite_keyfile_path = NotImplemented
    logging_manager = LoggingManager(True)
    overlay = NotImplemented

    def __init__(self, window, widget, filepath):
        self.window = window
        self.parent_widget = widget
        self.database_filepath = filepath
        self.unlock_database()

    def unlock_database(self):
        self.builder = Gtk.Builder()
        self.builder.add_from_resource(""/run/terminal/KeepassGtk/unlock_database.ui"")

        self.set_headerbar()

        self.assemble_stack()
        self.connect_events()

    #
    # Headerbar
    #

    def set_headerbar(self):
        headerbar = self.builder.get_object(""headerbar"")
        headerbar.set_subtitle(self.database_filepath)
        self.window.set_titlebar(headerbar)
        self.parent_widget.set_headerbar(headerbar)
        back_button = self.builder.get_object(""back_button"")
        back_button.connect(""clicked"", self.on_headerbar_back_button_clicked)

    #
    # Stack
    #

    def assemble_stack(self):
        self.overlay = Gtk.Overlay()

        unlock_failed_overlay = self.builder.get_object(""unlock_failed_overlay"")
        self.overlay.add_overlay(unlock_failed_overlay)

        stack = Gtk.Stack()
        stack.set_transition_type(Gtk.StackTransitionType.CROSSFADE)

        self.unlock_database_stack_box = self.builder.get_object(""unlock_database_stack_box"")
        unlock_database_stack_switcher = self.builder.get_object(""unlock_database_stack_switcher"")
        unlock_database_stack_switcher.set_stack(stack)

        password_unlock_stack_page = self.builder.get_object(""password_unlock_stack_page"")
        keyfile_unlock_stack_page = self.builder.get_object(""keyfile_unlock_stack_page"")
        composite_unlock_stack_page = self.builder.get_object(""composite_unlock_stack_page"")

        stack.add_titled(password_unlock_stack_page, ""password_unlock"", ""Password"")
        stack.child_set_property(password_unlock_stack_page, ""icon-name"", ""input-dialpad-symbolic"")

        stack.add_titled(keyfile_unlock_stack_page, ""keyfile_unlock"", ""Keyfile"")
        stack.child_set_property(keyfile_unlock_stack_page, ""icon-name"", ""mail-attachment-symbolic"")

        stack.add_titled(composite_unlock_stack_page, ""composite_unlock"", ""Composite"")
        stack.child_set_property(composite_unlock_stack_page, ""icon-name"", ""insert-link-symbolic"")

        self.overlay.add(stack)
        self.unlock_database_stack_box.add(self.overlay)
        self.unlock_database_stack_box.show_all()

        self.parent_widget.add(self.unlock_database_stack_box)

    def connect_events(self):
        password_unlock_button = self.builder.get_object(""password_unlock_button"")
        password_unlock_button.connect(""clicked"", self.on_password_unlock_button_clicked)

        keyfile_unlock_button = self.builder.get_object(""keyfile_unlock_button"")
        keyfile_unlock_button.connect(""clicked"", self.on_keyfile_unlock_button_clicked)

        composite_unlock_button = self.builder.get_object(""composite_unlock_button"")
        composite_unlock_button.connect(""clicked"", self.on_composite_unlock_button_clicked)

        keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
        keyfile_unlock_select_button.connect(""clicked"", self.on_keyfile_unlock_select_button_clicked)

        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")
        composite_unlock_select_button.connect(""clicked"", self.on_composite_unlock_select_button_clicked)

        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")
        password_unlock_entry.connect(""activate"", self.on_password_unlock_button_clicked)
        password_unlock_entry.connect(""icon-press"", self.on_password_unlock_entry_secondary_clicked)

    #
    # Events
    #

    def on_password_unlock_entry_secondary_clicked(self, widget, position, eventbutton):
        if widget.get_visibility():
            widget.set_invisible_char("""")
            widget.set_visibility(False)
        else:
            widget.set_visibility(True)

    def on_headerbar_back_button_clicked(self, widget):
        self.window.set_headerbar()
        self.window.close_tab(self.parent_widget)

    def on_password_unlock_button_clicked(self, widget):
        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")

        database_already_opened = False

        for db in self.window.opened_databases:
            if db.database_manager.database_path == self.database_filepath:
                database_already_opened = True
                page_num = self.window.container.page_num(db.parent_widget)
                self.window.container.set_current_page(page_num)

                current_page_num = self.window.container.page_num(self.parent_widget)
                self.window.container.remove_page(current_page_num)

                db.show_database_action_revealer(""Database already opened"")

        if password_unlock_entry.get_text() != """" and database_already_opened is False:
            try:
                self.database_manager = DatabaseManager(self.database_filepath, password_unlock_entry.get_text())
                self.open_database_page()
                self.logging_manager.log_debug(""Opening of database was successfull"")
            except(OSError):
                self.show_unlock_failed_revealer()

                password_unlock_entry.grab_focus()
                password_unlock_entry.get_style_context().add_class(""error"")
                self.clear_input_fields()
                self.logging_manager.log_debug(""Could not open database, wrong password"")

    def on_keyfile_unlock_select_button_clicked(self, widget):
        keyfile_chooser_dialog = Gtk.FileChooserDialog(""Choose a keyfile"", self.window, Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keyfile"")
        filter_text.add_mime_type(""application/octet-stream"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filter_text.add_mime_type(""text/plain"")
        filter_text.add_mime_type(""application/x-iwork-keynote-sffkey"")
        keyfile_chooser_dialog.add_filter(filter_text)

        response = keyfile_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(""File selected: "" + keyfile_chooser_dialog.get_filename())
            keyfile_chooser_dialog.close()

            keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
            keyfile_unlock_select_button.get_style_context().remove_class(Gtk.STYLE_CLASS_DESTRUCTIVE_ACTION)
            keyfile_unlock_select_button.get_style_context().add_class(Gtk.STYLE_CLASS_SUGGESTED_ACTION)
            keyfile_unlock_select_button.set_label(ntpath.basename(keyfile_chooser_dialog.get_filename()))

        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection canceled"")
            keyfile_chooser_dialog.close()

    def on_keyfile_unlock_button_clicked(self, widget):
        keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
        keyfile_path = keyfile_unlock_select_button.get_label()

        try:
            self.database_manager = DatabaseManager(self.database_filepath, password=None, keyfile=keyfile_path)
            self.open_database_page()
            self.logging_manager.log_debug(""Database successfully opened with keyfile"")
        except(OSError, IndexError):
            self.show_unlock_failed_revealer()

            keyfile_unlock_select_button.get_style_context().add_class(Gtk.STYLE_CLASS_DESTRUCTIVE_ACTION)
            keyfile_unlock_select_button.set_label(""Try again"")

            self.logging_manager.log_debug(""Invalid keyfile chosen"")
            self.logging_manager.log_debug(""Keyfile path: "" + keyfile_path)

    def on_composite_unlock_select_button_clicked(self, widget):
        filechooser_opening_dialog = Gtk.FileChooserDialog(
            ""Choose Keyfile"", self.window, Gtk.FileChooserAction.OPEN,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN,
             Gtk.ResponseType.OK))
        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keyfile"")
        filter_text.add_mime_type(""application/octet-stream"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filter_text.add_mime_type(""text/plain"")
        filter_text.add_mime_type(""application/x-iwork-keynote-sffkey"")
        filechooser_opening_dialog.add_filter(filter_text)

        response = filechooser_opening_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(""File selected: "" + filechooser_opening_dialog.get_filename())
            filechooser_opening_dialog.close()
            file_path = filechooser_opening_dialog.get_filename()
            composite_unlock_select_button.set_label(ntpath.basename(file_path))
            self.composite_keyfile_path = file_path
        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection cancelled"")
            filechooser_opening_dialog.close()

    def on_composite_unlock_button_clicked(self, widget):
        composite_unlock_entry = self.builder.get_object(""composite_unlock_entry"")
        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")

        if composite_unlock_entry.get_text() is not """":
            try:
                self.database_manager = DatabaseManager(self.database_filepath, composite_unlock_entry.get_text(), self.composite_keyfile_path)
                self.open_database_page()
                self.logging_manager.log_debug(""Opening of database was successfull"")
            except(OSError):
                self.show_unlock_failed_revealer()

                composite_unlock_entry.grab_focus()
                composite_unlock_entry.get_style_context().add_class(""error"")
                composite_unlock_select_button.get_style_context().remove_class(""suggested-action"")
                composite_unlock_select_button.get_style_context().add_class(""destructive-action"")
                self.clear_input_fields()

                self.logging_manager.log_debug(""Could not open database, wrong password"")
        else:
            composite_unlock_entry.get_style_context().add_class(""error"")

    #
    # Open Database
    #
    def open_database_page(self):
        self.clear_input_fields()
        keepassgtk.config_manager.create_config_entry_string(""history"", ""last-opened-db"", str(self.database_filepath))
        keepassgtk.config_manager.save_config()

        self.unlock_database_stack_box.destroy()
        UnlockedDatabase(self.window, self.parent_widget, self.database_manager)

    #
    # Helper Functions
    #

    def clear_input_fields(self):
        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")
        composite_unlock_entry = self.builder.get_object(""composite_unlock_entry"")
        password_unlock_entry.set_text("""")
        composite_unlock_entry.set_text("""")

    def show_unlock_failed_revealer(self):
        unlock_failed_box = self.builder.get_object(""unlock_failed_box"")
        context = unlock_failed_box.get_style_context()
        context.add_class('NotifyRevealer')

        unlock_failed_revealer = self.builder.get_object(""unlock_failed_revealer"")
        unlock_failed_revealer.set_reveal_child(not unlock_failed_revealer.get_reveal_child())
        revealer_timer = threading.Timer(3.0, self.hide_unlock_failed_revealer)
        revealer_timer.start()

    def hide_unlock_failed_revealer(self):
        unlock_failed_revealer = self.builder.get_object(""unlock_failed_revealer"")
        unlock_failed_revealer.set_reveal_child(not unlock_failed_revealer.get_reveal_child())
/n/n/n",0
127,127,50563e07a14d5f43f339e6b241ff2b9c8692ee1c,"/keepassgtk/main_window.py/n/nfrom gi.repository import Gio, Gdk, Gtk
from keepassgtk.logging_manager import LoggingManager
from keepassgtk.database_manager import DatabaseManager
from keepassgtk.create_database import CreateDatabase
from keepassgtk.container_page import ContainerPage
from keepassgtk.unlock_database import UnlockDatabase
import keepassgtk.config_manager
import os
from os.path import exists
import ntpath
import gi
gi.require_version('Gtk', '3.0')
gi.require_version('Gdk', '3.0')


class MainWindow(Gtk.ApplicationWindow):
    application = NotImplemented
    database_manager = NotImplemented
    container = NotImplemented
    override_dialog = NotImplemented
    quit_dialog = NotImplemented
    filechooser_creation_dialog = NotImplemented
    headerbar = NotImplemented
    first_start_grid = NotImplemented
    logging_manager = LoggingManager(True)
    opened_databases = []
    databases_to_save = []

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        keepassgtk.config_manager.configure()
        self.assemble_window()

    def assemble_window(self):
        self.set_default_size(800, 500)
        
        self.create_headerbar()
        self.first_start_screen()

        self.connect(""delete-event"", self.on_application_quit)

        self.custom_css()

    #
    # Headerbar
    #

    def create_headerbar(self):
        builder = Gtk.Builder()
        builder.add_from_resource(""/run/terminal/KeepassGtk/main_window.ui"")

        self.headerbar = builder.get_object(""headerbar"")

        file_open_button = builder.get_object(""open_button"")
        file_open_button.connect(""clicked"", self.open_filechooser, None)

        file_new_button = builder.get_object(""new_button"")
        file_new_button.connect(""clicked"", self.create_filechooser, None)

        self.set_titlebar(self.headerbar)

    def set_headerbar(self):
        self.set_titlebar(self.headerbar)

    def get_headerbar(self):
        return self.headerbar

    #
    # Styles
    #

    def custom_css(self):
        screen = Gdk.Screen.get_default()

        css_provider = Gtk.CssProvider()
        css_provider_resource = Gio.File.new_for_uri(
            ""resource:///run/terminal/KeepassGtk/keepassgtk.css"")
        css_provider.load_from_file(css_provider_resource)

        context = Gtk.StyleContext()
        context.add_provider_for_screen(
            screen, css_provider, Gtk.STYLE_PROVIDER_PRIORITY_USER)

    #
    # First Start Screen
    #

    def first_start_screen(self):
        if keepassgtk.config_manager.has_group(""history"") and keepassgtk.config_manager.get_string(""history"", ""last-opened-db"") != """" and exists(keepassgtk.config_manager.get_string(""history"", ""last-opened-db"")):
            self.logging_manager.log_debug(
                ""Found last opened database entry ("" +
                keepassgtk.config_manager.get_string(
                    ""history"", ""last-opened-db"") + "")"")

            tab_title = ntpath.basename(keepassgtk.config_manager.get_string(
                ""history"", ""last-opened-db""))
            self.start_database_opening_routine(
                tab_title,
                keepassgtk.config_manager.get_string(
                    ""history"", ""last-opened-db""))
        else:
            self.logging_manager.log_debug(
                ""No / Not valid last opened database entry found."")
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/main_window.ui"")

            self.first_start_grid = builder.get_object(""first_start_grid"")
            self.add(self.first_start_grid)

    #
    # Container Methods (Gtk Notebook holds tabs)
    #

    def create_container(self):
        if self.first_start_grid != NotImplemented:
            self.first_start_grid.destroy()

        self.container = Gtk.Notebook()

        self.container.set_border_width(0)
        self.container.set_scrollable(True)
        self.container.set_show_border(False)
        self.container.connect(""switch-page"", self.on_tab_switch)

        self.add(self.container)
        self.show_all()

    def destroy_container(self):
        self.container.destroy()

    #
    # Open Database Methods
    #

    def open_filechooser(self, widget, none):
        filechooser_opening_dialog = Gtk.FileChooserDialog(
            ""Choose Keepass Database"", self, Gtk.FileChooserAction.OPEN,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL,
             Gtk.STOCK_OPEN, Gtk.ResponseType.OK))

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keepass 2 Database"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filechooser_opening_dialog.add_filter(filter_text)

        response = filechooser_opening_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(
                ""File selected: "" + filechooser_opening_dialog.get_filename())
            filechooser_opening_dialog.close()

            tab_title = self.create_tab_title_from_filepath(
                filechooser_opening_dialog.get_filename())
            self.start_database_opening_routine(
                tab_title, filechooser_opening_dialog.get_filename())
        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection canceled"")
            filechooser_opening_dialog.close()

    def start_database_opening_routine(self, tab_title, filepath):
        builder = Gtk.Builder()
        builder.add_from_resource(
            ""/run/terminal/KeepassGtk/create_database.ui"")
        headerbar = builder.get_object(""headerbar"")

        UnlockDatabase(self, self.create_tab(tab_title, headerbar), filepath)

    #
    # Create Database Methods
    #

    def create_filechooser(self, widget, none):
        self.filechooser_creation_dialog = Gtk.FileChooserDialog(
            ""Create new Database"", self, Gtk.FileChooserAction.SAVE,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL,
             Gtk.STOCK_SAVE, Gtk.ResponseType.OK))
        self.filechooser_creation_dialog.set_current_name(""Database.kdbx"")
        self.filechooser_creation_dialog.set_modal(True)

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keepass 2 Database"")
        filter_text.add_mime_type(""application/x-keepass2"")
        self.filechooser_creation_dialog.add_filter(filter_text)

        response = self.filechooser_creation_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.does_file_exist()
        elif response == Gtk.ResponseType.CANCEL:
            self.filechooser_creation_dialog.close()

    def does_file_exist(self):
        if os.path.exists(self.filechooser_creation_dialog.get_filename()):
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/override_dialog.ui"")
            self.override_dialog = builder.get_object(""override_dialog"")

            self.override_dialog.set_destroy_with_parent(True)
            self.override_dialog.set_modal(True)
            self.override_dialog.set_transient_for(self.filechooser_creation_dialog)

            cancel_button = builder.get_object(""cancel_button"")
            override_button = builder.get_object(""override_button"")

            cancel_button.connect(""clicked"", self.on_cancel_button_clicked)
            override_button.connect(""clicked"", self.on_override_button_clicked)

            self.override_dialog.present()
        else:
            self.copy_database_file()

            tab_title = self.create_tab_title_from_filepath(self.filechooser_creation_dialog.get_current_name())
            self.start_database_creation_routine(tab_title)

    def copy_database_file(self):
        stock_database = Gio.File.new_for_uri(
            ""resource:///run/terminal/KeepassGtk/database.kdbx"")
        new_database = Gio.File.new_for_path(
            self.filechooser_creation_dialog.get_filename())

        stock_database.copy(new_database, Gio.FileCopyFlags.OVERWRITE)
        self.filechooser_creation_dialog.close()

    def start_database_creation_routine(self, tab_title):
        self.database_manager = DatabaseManager(
            self.filechooser_creation_dialog.get_filename(),
            ""liufhre86ewoiwejmrcu8owe"")
        builder = Gtk.Builder()
        builder.add_from_resource(
            ""/run/terminal/KeepassGtk/create_database.ui"")
        headerbar = builder.get_object(""headerbar"")
        CreateDatabase(
            self, self.create_tab(tab_title, headerbar),
            self.database_manager)

    #
    # Tab Manager
    #

    def create_tab(self, title, headerbar):
        if self.container == NotImplemented:
            self.create_container()

        page_instance = ContainerPage(headerbar)

        tab_hbox = Gtk.HBox(False, 0)
        tab_label = Gtk.Label(title)
        tab_hbox.pack_start(tab_label, False, False, False)

        icon = Gio.ThemedIcon(name=""window-close-symbolic"")
        close_image = Gtk.Image.new_from_gicon(icon, Gtk.IconSize.BUTTON)
        close_button = Gtk.Button()
        close_button.set_relief(Gtk.ReliefStyle.NONE)
        close_button.set_focus_on_click(False)
        close_button.connect(""clicked"", self.on_tab_close_button_clicked, page_instance)
        close_button.add(close_image)

        tab_hbox.pack_start(close_button, False, False, False)
        tab_hbox.show_all()

        self.container.append_page(page_instance, tab_hbox)
        self.container.set_current_page(self.container.page_num(page_instance))
        self.update_tab_bar_visibility()

        return page_instance

    def update_tab_bar_visibility(self):
        if self.container.get_n_pages() > 1:
            self.container.set_show_tabs(True)
        else:
            self.container.set_show_tabs(False)

    def create_tab_title_from_filepath(self, filepath):
        return ntpath.basename(filepath)

    def close_tab(self, child_widget):
        page_num = self.container.page_num(child_widget)
        self.container.remove_page(page_num)
        self.update_tab_bar_visibility()

    #
    # Events
    #

    def on_tab_close_button_clicked(self, sender, widget):
        page_num = self.container.page_num(widget)

        for db in self.opened_databases:
            if db.window.container.page_num(db.parent_widget) == page_num:
                self.opened_databases.remove(db)

        self.container.remove_page(page_num)
        self.update_tab_bar_visibility()

    def on_cancel_button_clicked(self, widget):
        self.override_dialog.destroy()
        self.filechooser_creation_dialog.destroy()

    def on_override_button_clicked(self, widget):
        self.copy_database_file()

        tab_title = self.create_tab_title_from_filepath(
            self.filechooser_creation_dialog.get_current_name())
        self.start_database_creation_routine(tab_title)

        self.override_dialog.destroy()

    def on_tab_switch(self, notebook, tab, pagenum):
        headerbar = tab.get_headerbar()
        self.set_titlebar(headerbar)

    def on_save_check_button_toggled(self, check_button, db):
        if check_button.get_active():
            self.databases_to_save.append(db)
        else:
            self.databases_to_save.remove(db)

    def on_back_button_clicked(self, button):
        self.databases_to_save.clear()
        self.quit_dialog.destroy()

    def on_quit_button_clicked(self, button):
        for db in self.databases_to_save:
            db.database_manager.save_database()

        self.quit_dialog.destroy()
        self.application.quit()

    #
    # Application Quit Dialog
    #

    def on_application_quit(self, window, event):
        unsaved_databases_list = []
        for db in self.opened_databases:
            if db.database_manager.changes is True:
                unsaved_databases_list.append(db)

        if unsaved_databases_list.__len__() > 0:
            builder = Gtk.Builder()
            builder.add_from_resource(
                ""/run/terminal/KeepassGtk/quit_dialog.ui"")
            self.quit_dialog = builder.get_object(""quit_dialog"")

            self.quit_dialog.set_destroy_with_parent(True)
            self.quit_dialog.set_modal(True)
            self.quit_dialog.set_transient_for(self)

            back_button = builder.get_object(""back_button"")
            quit_button = builder.get_object(""quit_button"")

            back_button.connect(""clicked"", self.on_back_button_clicked)
            quit_button.connect(""clicked"", self.on_quit_button_clicked)

            unsaved_databases_list_box = builder.get_object(""unsaved_databases_list_box"")
                
            for db in unsaved_databases_list:
                unsaved_database_row = Gtk.ListBoxRow()
                check_button = Gtk.CheckButton()
                check_button.set_label(db.database_manager.database_path)
                check_button.connect(""toggled"", self.on_save_check_button_toggled, db)
                unsaved_database_row.add(check_button)
                unsaved_database_row.show_all()
                unsaved_databases_list_box.add(unsaved_database_row)

            self.quit_dialog.present()

            return(True)         
/n/n/n/keepassgtk/unlock_database.py/n/nfrom gi.repository import Gio, Gtk
from keepassgtk.database_manager import DatabaseManager
from keepassgtk.unlocked_database import UnlockedDatabase
import keepassgtk.config_manager
from keepassgtk.logging_manager import LoggingManager
import gi
gi.require_version('Gtk', '3.0')
import ntpath
import threading


class UnlockDatabase:
    builder = NotImplemented
    parent_widget = NotImplemented
    window = NotImplemented
    database_filepath = NotImplemented
    database_manager = NotImplemented
    unlock_database_stack_box = NotImplemented
    keyfile = NotImplemented
    composite_keyfile_path = NotImplemented
    logging_manager = LoggingManager(True)
    overlay = NotImplemented

    def __init__(self, window, widget, filepath):
        self.window = window
        self.parent_widget = widget
        self.database_filepath = filepath
        self.unlock_database()

    def unlock_database(self):
        self.builder = Gtk.Builder()
        self.builder.add_from_resource(""/run/terminal/KeepassGtk/unlock_database.ui"")

        self.set_headerbar()

        self.assemble_stack()
        self.connect_events()

    #
    # Headerbar
    #

    def set_headerbar(self):
        headerbar = self.builder.get_object(""headerbar"")
        headerbar.set_subtitle(self.database_filepath)
        self.window.set_titlebar(headerbar)
        self.parent_widget.set_headerbar(headerbar)
        back_button = self.builder.get_object(""back_button"")
        back_button.connect(""clicked"", self.on_headerbar_back_button_clicked)

    #
    # Stack
    #

    def assemble_stack(self):
        self.overlay = Gtk.Overlay()

        unlock_failed_overlay = self.builder.get_object(""unlock_failed_overlay"")
        self.overlay.add_overlay(unlock_failed_overlay)

        stack = Gtk.Stack()
        stack.set_transition_type(Gtk.StackTransitionType.CROSSFADE)

        self.unlock_database_stack_box = self.builder.get_object(""unlock_database_stack_box"")
        unlock_database_stack_switcher = self.builder.get_object(""unlock_database_stack_switcher"")
        unlock_database_stack_switcher.set_stack(stack)

        password_unlock_stack_page = self.builder.get_object(""password_unlock_stack_page"")
        keyfile_unlock_stack_page = self.builder.get_object(""keyfile_unlock_stack_page"")
        composite_unlock_stack_page = self.builder.get_object(""composite_unlock_stack_page"")

        stack.add_titled(password_unlock_stack_page, ""password_unlock"", ""Password"")
        stack.child_set_property(password_unlock_stack_page, ""icon-name"", ""input-dialpad-symbolic"")

        stack.add_titled(keyfile_unlock_stack_page, ""keyfile_unlock"", ""Keyfile"")
        stack.child_set_property(keyfile_unlock_stack_page, ""icon-name"", ""mail-attachment-symbolic"")

        stack.add_titled(composite_unlock_stack_page, ""composite_unlock"", ""Composite"")
        stack.child_set_property(composite_unlock_stack_page, ""icon-name"", ""insert-link-symbolic"")

        self.overlay.add(stack)
        self.unlock_database_stack_box.add(self.overlay)
        self.unlock_database_stack_box.show_all()

        self.parent_widget.add(self.unlock_database_stack_box)

    def connect_events(self):
        password_unlock_button = self.builder.get_object(""password_unlock_button"")
        password_unlock_button.connect(""clicked"", self.on_password_unlock_button_clicked)

        keyfile_unlock_button = self.builder.get_object(""keyfile_unlock_button"")
        keyfile_unlock_button.connect(""clicked"", self.on_keyfile_unlock_button_clicked)

        composite_unlock_button = self.builder.get_object(""composite_unlock_button"")
        composite_unlock_button.connect(""clicked"", self.on_composite_unlock_button_clicked)

        keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
        keyfile_unlock_select_button.connect(""clicked"", self.on_keyfile_unlock_select_button_clicked)

        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")
        composite_unlock_select_button.connect(""clicked"", self.on_composite_unlock_select_button_clicked)

        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")
        password_unlock_entry.connect(""activate"", self.on_password_unlock_button_clicked)
        password_unlock_entry.connect(""icon-press"", self.on_password_unlock_entry_secondary_clicked)

    #
    # Events
    #

    def on_password_unlock_entry_secondary_clicked(self, widget, position, eventbutton):
        if widget.get_visibility():
            widget.set_invisible_char("""")
            widget.set_visibility(False)
        else:
            widget.set_visibility(True)

    def on_headerbar_back_button_clicked(self, widget):
        self.window.set_headerbar()
        self.window.close_tab(self.parent_widget)

    def on_password_unlock_button_clicked(self, widget):
        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")

        if password_unlock_entry.get_text() != """":
            try:
                self.database_manager = DatabaseManager(self.database_filepath, password_unlock_entry.get_text())
                self.open_database_page()
                self.logging_manager.log_debug(""Opening of database was successfull"")
            except(OSError):
                self.show_unlock_failed_revealer()

                password_unlock_entry.grab_focus()
                password_unlock_entry.get_style_context().add_class(""error"")
                self.clear_input_fields()
                self.logging_manager.log_debug(""Could not open database, wrong password"")

    def on_keyfile_unlock_select_button_clicked(self, widget):
        keyfile_chooser_dialog = Gtk.FileChooserDialog(""Choose a keyfile"", self.window, Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keyfile"")
        filter_text.add_mime_type(""application/octet-stream"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filter_text.add_mime_type(""text/plain"")
        filter_text.add_mime_type(""application/x-iwork-keynote-sffkey"")
        keyfile_chooser_dialog.add_filter(filter_text)

        response = keyfile_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(""File selected: "" + keyfile_chooser_dialog.get_filename())
            keyfile_chooser_dialog.close()

            keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
            keyfile_unlock_select_button.get_style_context().remove_class(Gtk.STYLE_CLASS_DESTRUCTIVE_ACTION)
            keyfile_unlock_select_button.get_style_context().add_class(Gtk.STYLE_CLASS_SUGGESTED_ACTION)
            keyfile_unlock_select_button.set_label(ntpath.basename(keyfile_chooser_dialog.get_filename()))

        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection canceled"")
            keyfile_chooser_dialog.close()

    def on_keyfile_unlock_button_clicked(self, widget):
        keyfile_unlock_select_button = self.builder.get_object(""keyfile_unlock_select_button"")
        keyfile_path = keyfile_unlock_select_button.get_label()

        try:
            self.database_manager = DatabaseManager(self.database_filepath, password=None, keyfile=keyfile_path)
            self.open_database_page()
            self.logging_manager.log_debug(""Database successfully opened with keyfile"")
        except(OSError, IndexError):
            self.show_unlock_failed_revealer()

            keyfile_unlock_select_button.get_style_context().add_class(Gtk.STYLE_CLASS_DESTRUCTIVE_ACTION)
            keyfile_unlock_select_button.set_label(""Try again"")

            self.logging_manager.log_debug(""Invalid keyfile chosen"")
            self.logging_manager.log_debug(""Keyfile path: "" + keyfile_path)

    def on_composite_unlock_select_button_clicked(self, widget):
        filechooser_opening_dialog = Gtk.FileChooserDialog(
            ""Choose Keyfile"", self.window, Gtk.FileChooserAction.OPEN,
            (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN,
             Gtk.ResponseType.OK))
        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")

        filter_text = Gtk.FileFilter()
        filter_text.set_name(""Keyfile"")
        filter_text.add_mime_type(""application/octet-stream"")
        filter_text.add_mime_type(""application/x-keepass2"")
        filter_text.add_mime_type(""text/plain"")
        filter_text.add_mime_type(""application/x-iwork-keynote-sffkey"")
        filechooser_opening_dialog.add_filter(filter_text)

        response = filechooser_opening_dialog.run()
        if response == Gtk.ResponseType.OK:
            self.logging_manager.log_debug(""File selected: "" + filechooser_opening_dialog.get_filename())
            filechooser_opening_dialog.close()
            file_path = filechooser_opening_dialog.get_filename()
            composite_unlock_select_button.set_label(ntpath.basename(file_path))
            self.composite_keyfile_path = file_path
        elif response == Gtk.ResponseType.CANCEL:
            self.logging_manager.log_debug(""File selection cancelled"")
            filechooser_opening_dialog.close()

    def on_composite_unlock_button_clicked(self, widget):
        composite_unlock_entry = self.builder.get_object(""composite_unlock_entry"")
        composite_unlock_select_button = self.builder.get_object(""composite_unlock_select_button"")

        if composite_unlock_entry.get_text() is not """":
            try:
                self.database_manager = DatabaseManager(self.database_filepath, composite_unlock_entry.get_text(), self.composite_keyfile_path)
                self.open_database_page()
                self.logging_manager.log_debug(""Opening of database was successfull"")
            except(OSError):
                self.show_unlock_failed_revealer()

                composite_unlock_entry.grab_focus()
                composite_unlock_entry.get_style_context().add_class(""error"")
                composite_unlock_select_button.get_style_context().remove_class(""suggested-action"")
                composite_unlock_select_button.get_style_context().add_class(""destructive-action"")
                self.clear_input_fields()

                self.logging_manager.log_debug(""Could not open database, wrong password"")
        else:
            composite_unlock_entry.get_style_context().add_class(""error"")

    #
    # Open Database
    #
    def open_database_page(self):
        self.clear_input_fields()
        keepassgtk.config_manager.create_config_entry_string(""history"", ""last-opened-db"", str(self.database_filepath))
        keepassgtk.config_manager.save_config()

        self.unlock_database_stack_box.destroy()
        UnlockedDatabase(self.window, self.parent_widget, self.database_manager)

    #
    # Helper Functions
    #

    def clear_input_fields(self):
        password_unlock_entry = self.builder.get_object(""password_unlock_entry"")
        composite_unlock_entry = self.builder.get_object(""composite_unlock_entry"")
        password_unlock_entry.set_text("""")
        composite_unlock_entry.set_text("""")

    def show_unlock_failed_revealer(self):
        unlock_failed_box = self.builder.get_object(""unlock_failed_box"")
        context = unlock_failed_box.get_style_context()
        context.add_class('NotifyRevealer')

        unlock_failed_revealer = self.builder.get_object(""unlock_failed_revealer"")
        unlock_failed_revealer.set_reveal_child(not unlock_failed_revealer.get_reveal_child())
        revealer_timer = threading.Timer(3.0, self.hide_unlock_failed_revealer)
        revealer_timer.start()

    def hide_unlock_failed_revealer(self):
        unlock_failed_revealer = self.builder.get_object(""unlock_failed_revealer"")
        unlock_failed_revealer.set_reveal_child(not unlock_failed_revealer.get_reveal_child())
/n/n/n",1
168,168,3d9c0c5f366095cfda37cad40f5184d63f4c9698,"__manifest__.py/n/n# -*- coding: utf-8 -*-
# Copyright 2018 Fundament IT
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).
{
    'name': 'FIT BCNL Events',
    'category': 'Website',
    'version': '10.0.1.0.0',
    'author': 'Fundament IT',
    'website': 'https://fundament.it/',
    'licence': 'AGPL-3',
    'depends': ['website_event'],
    'summary':"""""""""""",
    'description': """"""
Extension for the default Events module; used for BCNL, .
    """""",
    'data': [
        'data/fit_actions.xml',
        'data/fit_event_views.xml',
        'data/fit_product_views.xml',
        'data/fit_res_partner_view.xml',
        'report/fit_report_subscription_view.xml',
        'security/user_groups.xml',
        'views/fit_event_views.xml',
        'views/fit_subscription_templates.xml',
        'views/fit_website_event_sale_templates.xml',
        'views/fit_website_event_templates.xml',
        'views/fit_website_templates.xml',
    ],
    'installable': True,
}/n/n/ncontrollers/fit_event_controller.py/n/n# -*- coding: utf-8 -*-
# Copyright 2018 Fundament IT
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).

import logging

from odoo import fields, http, _
from odoo.exceptions import ValidationError, UserError

_logger = logging.getLogger(__name__)


class WebsiteEventController(http.Controller):

    @http.route(['/fit_subscribe_controller/subscribe'], type='http', auth=""public"", website=True)
    def event_register(self, event_id, event_is_participating, **post):
        event_id = int(event_id)
        event_is_participating = event_is_participating
        event = http.request.env['event.event'].sudo().browse(event_id)
        subscription_update_counter = 0
        partner = http.request.env.user.partner_id
        partner_id = int(partner.id)
        if event_is_participating:
            for registration in event.registration_ids:
                for partner in registration.partner_id:
                    if partner.id == partner_id:
                        _logger.info('Found existing registration, set state to cancelled.')
                        #registration.unlink()
                        registration.state = 'cancel'
                        subscription_update_counter += 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
        else:
            #_logger.info('Search existing registration')
            existing_registration = http.request.env['event.registration'].sudo().search([('partner_id', '=', partner_id),
                                                                                          ('event_id', '=', event.id)])
            try:
                if existing_registration:
                    if event.seats_available > 0 and event.seats_availability == u'limited':
                        _logger.info('Found existing registration, set state to open (confirmed)')
                        existing_registration.state = 'open'
                        subscription_update_counter -= 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
                    else:
                        _logger.info('Found existing registration, no seats available')
                else:
                    if event.seats_available > 0 and event.seats_availability == u'limited':
                        _logger.info('No registration found, create new one')
                        http.request.env['event.registration'].sudo().create(
                            {
                                'partner_id': partner_id,
                                'event_id': event_id,
                                'name': partner.name if partner.name else '',
                                'phone': partner.mobile if partner.mobile else '',
                                'email': partner.email if partner.email else '',
                            }
                        )
                        subscription_update_counter -= 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
                    else:
                        _logger.info('No seats available')
            except ValidationError as e:
                _logger.error('Unable to register: '+str(e))

        referer = str(http.request.httprequest.headers.environ['HTTP_REFERER'])
        redirect = '/event' + str('/'+referer.split('/event')[-1])
        return http.request.redirect(redirect)

    @http.route(['/mijn-account/trainingen'], type='http', auth=""public"", website=True)
    def redirect_event(self, **post):
        _logger.info('OLD my account trainingen reference, redirect to /events')
        redirect = '/event'
        return http.request.redirect(redirect)

    def _update_counter_subscription(self, event, partner, subscription_update_counter):
        event_cat = str(event.event_type_id.name).lower()
        ai_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'ai_montly'),
                                                                        ('subscription_partner', '=', partner.id)])

        cf_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'cf_montly'),
                                                                        ('subscription_partner', '=', partner.id)])

        bc_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bc_montly'),
                                                                        ('subscription_partner', '=', partner.id)])
        bc_tickets = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bc_tickets'),
                                                                        ('subscription_partner', '=', partner.id)])
        bz_tickets = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bz_tickets'),
                                                                        ('subscription_partner', '=', partner.id)])

        # If user has active all-in subscription then skip furter processing
        if ai_monthly.subscription_is_active:
            return

        if event_cat == 'bokszaktraining':
            if bz_tickets:
                bz_tickets.subscription_counter += subscription_update_counter

        if event_cat == 'bootcamp':
            if bc_monthly and bc_monthly.subscription_is_active:
                return
            if cf_monthly and cf_monthly.subscription_is_active:
                return
            if bc_tickets:
                bc_tickets.subscription_counter += subscription_update_counter
/n/n/nmodels/fit_event.py/n/nimport logging
from datetime import datetime

from dateutil.relativedelta import relativedelta

from odoo import fields, models, api

_logger = logging.getLogger(__name__)


class FitEvent(models.Model):
    _name = 'event.event'
    _inherit = ['event.event']

    fit_is_participating = fields.Boolean(""Is Participating"", compute=""_fit_compute_is_participating"")
    website_published = fields.Boolean(default=True)
    fit_day_of_week = fields.Char(string='Dag', default='')
    fit_repetition_enabled = fields.Boolean(string='Herhalen?', default=False)
    fit_repetition = fields.Selection([('daily', 'Dagelijks'),
                                       ('weekly', 'Wekelijks'),
                                       ('monthly', 'Maandelijks')], string=""Schema herhaling"")

    def _fit_compute_is_participating(self):
        # we don't allow public user to see participating label
        if self.env.user != self.env.ref('base.public_user'):
            email = self.env.user.partner_id.email
            for event in self:
                domain = ['&', '|', ('email', '=', email), ('partner_id', '=', self.env.user.partner_id.id), ('event_id', '=', event.id),
                          ('state', '=', 'open')]
                count = self.env['event.registration'].search_count(domain)
                if count > 0:
                    event.fit_is_participating = True
                else:
                    event.fit_is_participating = False

    @api.onchange('date_begin')
    def update_day_of_week(self):
        start_date = self.date_begin_located
        if start_date:
            self.fit_day_of_week = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime('%a')

    def get_attendee_list(self):
        attendee_list = str('')
        counter = 1
        reg_ids = self.sudo().registration_ids
        reg_ids = sorted(reg_ids, key=lambda x: x.date_open, reverse=False)
        for registration in reg_ids:
            if registration.state == 'open':
                if counter == 1:
                    attendee_list += registration.partner_id.sudo().name
                else:
                    attendee_list += ', ' + registration.partner_id.sudo().name
                counter += 1
        return attendee_list

    def start_automatic_event_creation(self):
        repeating_event_ids = self.env['event.event'].search([('fit_repetition_enabled', '=', True)])
        for repeating_event in repeating_event_ids:
            _logger.info('Found repeating event: ' + repeating_event.name)
            if repeating_event.fit_repetition == 'daily':
                self._handle_daily_event_repetition(repeating_event)
            if repeating_event.fit_repetition == 'weekly':
                self._handle_weekly_event_repetition(repeating_event)
            if repeating_event.fit_repetition == 'monthly':
                self._handle_montly_event_repetition(repeating_event)

    def _handle_daily_event_repetition(self, old_repeating_event):
        _logger.info('Handling daily repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:%S')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:%S') + relativedelta(days=+1)
            new_end_date = end_date + relativedelta(days=+1)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _handle_weekly_event_repetition(self, old_repeating_event):
        _logger.info('Handling weekly repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:%S')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:%S') + relativedelta(days=+7)
            new_end_date = end_date + relativedelta(days=+7)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _handle_monthly_event_repetition(self, old_repeating_event):
        _logger.info('Handling monthly repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:%S')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:%S') + relativedelta(months=+1)
            new_end_date = end_date + relativedelta(months=+1)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _event_does_not_exist(self, old_repeating_event, new_end_date):
        _logger.info('Checking new event existence: ' + old_repeating_event.name + ', date: ' + str(new_end_date))
        old_event_cat = old_repeating_event.event_type_id.id
        existing_event = self.env['event.event'].search([('event_type_id', '=', old_event_cat), ('date_end', '=', str(new_end_date))])
        if existing_event:
            return False
        else:
            return True

    def _create_new_event(self, old_repeating_event, new_start_date, new_end_date):
        _logger.info('Start creation new repeating event')
        new_repeating_event = old_repeating_event.copy(default={'website_published': True})
        new_repeating_event.date_end = new_end_date
        new_repeating_event.date_begin = new_start_date

        # 'date_begin': str(new_start_date), 'date_end_': str(new_end_date),
        old_repeating_event.fit_repetition_enabled = False
        old_repeating_event.fit_repetition = ''
/n/n/nmodels/fit_event_subscription.py/n/nimport logging
from datetime import datetime

from dateutil.relativedelta import relativedelta

from odoo import fields, models, api

_logger = logging.getLogger(__name__)


class FitEventSubscription(models.Model):
    _name = 'fit.subscription'
    _description = 'Inschrijving'

    subscription_partner = fields.Many2one(comodel_name='res.partner', string='Inschrijving-Partner')
    subscription_is_active = fields.Boolean(compute='_is_active')
    subscription_start = fields.Date('Start inschrijving')
    subscription_end = fields.Date('Einde inschrijving')
    subscription_counter = fields.Integer('Strippenkaart')
    subscription_category = fields.Char()
    subscription_type = fields.Selection([('ai_montly', 'Maandelijks (All-in)'),
                                          ('cf_montly', 'Maandelijks (Crosstraining)'),
                                          ('bc_montly', 'Maandelijks (Bootcamp)'),
                                          ('bc_tickets', 'Strippenkaart (Bootcamp)'),
                                          ('bz_tickets', 'Strippenkaart (Bokszak)')])

    @api.one
    def _is_active(self):
        if self.subscription_type:
            _type = self.get_subscription_type(self.subscription_type)
            if _type == 'subscription':
                end_date = datetime.strptime(self.subscription_end, '%Y-%m-%d').date()
                start_date = datetime.strptime(self.subscription_start, '%Y-%m-%d').date()
                present = datetime.now().date()
                if present < start_date or present > end_date:
                    self.subscription_is_active = False
                else:
                    self.subscription_is_active = True
            if _type == 'tickets':
                if self.subscription_counter > 0:
                    self.subscription_is_active = True
                else:
                    self.subscription_is_active = False

    @api.onchange('subscription_start')
    def on_change_start(self):
        if self.subscription_type:
            dt = datetime.strptime(self.subscription_start, '%Y-%m-%d') + relativedelta(months=+1)
            self.subscription_end = dt

    @api.onchange('subscription_type')
    def on_change_type(self):
        if self.subscription_type:
            if self.subscription_type == 'ai_montly':
                self.subscription_category = 'allin'
            if self.subscription_type == 'cf_montly':
                self.subscription_category = 'crosstraining'
            if self.subscription_type == 'bc_montly' or self.subscription_type == 'bc_tickets':
                self.subscription_category = 'bootcamp'
            if self.subscription_type == 'bz_tickets':
                self.subscription_category = 'bokszak'

    def get_subscription_type_length(self, product, invoice_line, product_counter):
        given_type = product.fit_subscription_type
        if self.get_subscription_type(given_type) == 'subscription':
            return int(invoice_line.quantity) * int(product_counter)
        else:
            if self.get_subscription_type(given_type) == 'tickets':
                return int(invoice_line.quantity) * int(product_counter) * 10
        return 0

    def get_subscription_type(self, given_type):
        type_type = {
            'ai_montly': 'subscription',
            'cf_montly': 'subscription',
            'bc_montly': 'subscription',
            'bc_tickets': 'tickets',
            'bz_tickets': 'tickets',
        }
        return type_type.get(given_type, '')

    def _can_subscribe(self, event_type):
        event_cat = str(event_type.name).lower()
        if event_cat == 'bokszaktraining':
            event_cat = 'bokszak'
        if event_cat == 'open':
            event_cat = 'crosstraining'
        type = self.get_subscription_type(self.subscription_type)
        if not self.subscription_category:
            self.on_change_type()
        subscription_cat = str(self.subscription_category).lower()
        if subscription_cat == 'crosstraining' and type == 'subscription' and event_cat == 'bootcamp':
            event_cat = 'crosstraining'
        if subscription_cat == event_cat or subscription_cat == 'allin':
            if type == 'subscription':
                present = datetime.now().date()
                start = datetime.strptime(self.subscription_start, '%Y-%m-%d').date()
                end = datetime.strptime(self.subscription_end, '%Y-%m-%d').date()

                if start <= present and end >= present:
                    _logger.info('Montly subscription, can subscribe event_cat: %s, subscription_cat: %s, start %s, end %s, present %s', event_cat,
                                 subscription_cat, start, end, present)
                    return True

            if type == 'tickets':
                if self.subscription_counter > 0:
                    _logger.info('Ticket subscription, can subscribe event_cat: %s, subscription_cat: %s', event_cat, subscription_cat)
                    return True

    def update(self, product, payment_type, invoice_line, product_counter):
        _logger.info('Updating subscription: ' + str(payment_type))

        if product.fit_subscription_type == self.subscription_type:
            subscription_extension = self.get_subscription_type_length(product, invoice_line, product_counter)

            if self.get_subscription_type(self.subscription_type) == 'subscription':

                if payment_type == 'inbound':
                    present = datetime.now().date()
                    current_end_date = datetime.strptime(self.subscription_end, '%Y-%m-%d').date()

                    if present > current_end_date:
                        new_end_date = present + relativedelta(months=+subscription_extension)
                        old_end_date = present
                    else:
                        new_end_date = current_end_date + relativedelta(months=+subscription_extension)
                        old_end_date = current_end_date

                    if new_end_date.day < old_end_date.day:
                        new_end_date = new_end_date + relativedelta(months=+1)
                        new_end_date = new_end_date.replace(day=1)

                    self.subscription_end = new_end_date
                    return True
                else:
                    if payment_type == 'outbound':
                        new_end_date = datetime.strptime(self.subscription_end, '%Y-%m-%d') + relativedelta(months=-subscription_extension)
                        present = datetime.now().date()

                        if present > new_end_date.date():
                            self.subscription_end = present + relativedelta(days=-1)
                        else:
                            self.subscription_end = new_end_date
                        return True
            else:
                if self.get_subscription_type(self.subscription_type) == 'tickets':

                    if payment_type == 'inbound':
                        self.subscription_counter += subscription_extension
                        return True
                    else:
                        if payment_type == 'outbound':
                            new_subscription_counter = self.subscription_counter - subscription_extension

                            if new_subscription_counter < 0:
                                new_subscription_counter = 0
                            self.subscription_counter = new_subscription_counter
                            return True
        return False
/n/n/nmodels/fit_partner.py/n/nimport logging
from datetime import datetime

from dateutil.relativedelta import relativedelta

from odoo import fields, models

_logger = logging.getLogger(__name__)


class Partner(models.Model):
    _name = 'res.partner'
    _inherit = ['res.partner']

    fit_subscriptions = fields.One2many(comodel_name='fit.subscription', inverse_name='subscription_partner', string='Inschrijving',
                                        store='True')
    fit_subscription_count = fields.Integer('# Inschrijvingen', compute='_compute_total_scubscriptions')

    def _compute_total_scubscriptions(self):
        counter = 0
        for subscription in self.fit_subscriptions:
            counter += 1
        self.fit_subscription_count = counter

    def can_subscribe(self, event):
        can_subscribe = False
        event_type = str(event.event_type_id.name).lower()
        event_start = datetime.strptime(event.date_begin, '%Y-%m-%d %H:%M:%S')
        #if event_type == 'open' and len(self.fit_subscriptions) > 0:
        #    _logger.info('Can subscribe for open event id: %s, name: %s', event.event_type_id.name, event.name)
        #    return True
        if event_start < datetime.now():
            return False
        if (event_start + relativedelta(hours=-24)) > datetime.now() and event_type == 'crosstraining':
            return False
        for subscription in self.fit_subscriptions:
            if subscription._can_subscribe(event.event_type_id):
                _logger.info('Can subscribe for event id: %s, name: %s', event.event_type_id.name, event.name)
                can_subscribe = True
        return can_subscribe

    def can_unsubscribe(self, event):
        event_start = datetime.strptime(event.date_begin_located, '%Y-%m-%d %H:%M:%S')
        if event_start < datetime.now():
            return False
        return True
/n/n/n",0
169,169,3d9c0c5f366095cfda37cad40f5184d63f4c9698,"/__manifest__.py/n/n# -*- coding: utf-8 -*-
# Copyright 2018 Fundament IT
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).
{
    'name': 'FIT BCNL Events',
    'category': 'Website',
    'version': '10.0.0.0.1',
    'author': 'Fundament IT',
    'website': 'https://fundament.it/',
    'licence': 'AGPL-3',
    'depends': ['website_event'],
    'summary':"""""""""""",
    'description': """"""
Extension for the default Events module; used for BCNL, .
    """""",
    'data': [
        'data/fit_actions.xml',
        'data/fit_event_views.xml',
        'data/fit_product_views.xml',
        'data/fit_res_partner_view.xml',
        'report/fit_report_subscription_view.xml',
        'security/user_groups.xml',
        'views/fit_event_views.xml',
        'views/fit_subscription_templates.xml',
        'views/fit_website_event_sale_templates.xml',
        'views/fit_website_event_templates.xml',
        'views/fit_website_templates.xml',
    ],
    'installable': True,
}/n/n/n/controllers/fit_event_controller.py/n/n# -*- coding: utf-8 -*-
# Copyright 2018 Fundament IT
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).

import logging

from odoo import fields, http, _
from odoo.exceptions import ValidationError, UserError

_logger = logging.getLogger(__name__)


class WebsiteEventController(http.Controller):

    @http.route(['/fit_subscribe_controller/subscribe'], type='http', auth=""public"", website=True)
    def event_register(self, event_id, event_is_participating, **post):
        event_id = int(event_id)
        event_is_participating = event_is_participating
        event = http.request.env['event.event'].sudo().browse(event_id)
        subscription_update_counter = 0
        partner = http.request.env.user.partner_id
        partner_id = int(partner.id)
        if event_is_participating:
            for registration in event.registration_ids:
                for partner in registration.partner_id:
                    if partner.id == partner_id:
                        _logger.info('Found existing registration, set state to cancelled.')
                        #registration.unlink()
                        registration.state = 'cancel'
                        subscription_update_counter += 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
        else:
            #_logger.info('Search existing registration')
            existing_registration = http.request.env['event.registration'].sudo().search([('partner_id', '=', partner_id),
                                                                                          ('event_id', '=', event.id)])
            try:
                if existing_registration:
                    if event.seats_available > 0 and event.seats_availability == u'limited':
                        _logger.info('Found existing registration, set state to open (confirmed)')
                        existing_registration.state = 'open'
                        subscription_update_counter -= 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
                    else:
                        _logger.info('Found existing registration, no seats available')
                else:
                    if event.seats_available > 0 and event.seats_availability == u'limited':
                        _logger.info('No registration found, create new one')
                        http.request.env['event.registration'].sudo().create(
                            {
                                'partner_id': partner_id,
                                'event_id': event_id,
                                'name': partner.name if partner.name else '',
                                'phone': partner.mobile if partner.mobile else '',
                                'email': partner.email if partner.email else '',
                            }
                        )
                        subscription_update_counter -= 1
                        self._update_counter_subscription(event, partner, subscription_update_counter)
                    else:
                        _logger.info('No seats available')
            except ValidationError as e:
                _logger.error('Unable to register: '+str(e))

        referer = str(http.request.httprequest.headers.environ['HTTP_REFERER'])
        redirect = str('/'+referer.split('/')[-1])
        return http.request.redirect(redirect)

    def _update_counter_subscription(self, event, partner, subscription_update_counter):
        event_cat = str(event.event_type_id.name).lower()
        ai_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'ai_montly'),
                                                                        ('subscription_partner', '=', partner.id)])

        cf_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'cf_montly'),
                                                                        ('subscription_partner', '=', partner.id)])

        bc_monthly = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bc_montly'),
                                                                        ('subscription_partner', '=', partner.id)])
        bc_tickets = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bc_tickets'),
                                                                        ('subscription_partner', '=', partner.id)])
        bz_tickets = http.request.env['fit.subscription'].sudo().search([('subscription_type', '=', 'bz_tickets'),
                                                                        ('subscription_partner', '=', partner.id)])

        # If user has active all-in subscription then skip furter processing
        if ai_monthly.subscription_is_active:
            return;

        if event_cat == 'bokszaktraining':
            if bz_tickets:
                bz_tickets.subscription_counter += subscription_update_counter

        if event_cat == 'bootcamp':
            if bc_monthly and bc_monthly.subscription_is_active:
                return
            if cf_monthly and cf_monthly.subscription_is_active:
                return
            if bc_tickets:
                bc_tickets.subscription_counter += subscription_update_counter
/n/n/n/models/fit_event.py/n/nimport logging
from datetime import datetime

from dateutil.relativedelta import relativedelta

from odoo import fields, models, api

_logger = logging.getLogger(__name__)


class FitEvent(models.Model):
    _name = 'event.event'
    _inherit = ['event.event']

    fit_is_participating = fields.Boolean(""Is Participating"", compute=""_fit_compute_is_participating"")
    website_published = fields.Boolean(default=True)
    fit_day_of_week = fields.Char(string='Dag', default='')
    fit_repetition_enabled = fields.Boolean(string='Herhalen?', default=False)
    fit_repetition = fields.Selection([('daily', 'Dagelijks'),
                                       ('weekly', 'Wekelijks'),
                                       ('monthly', 'Maandelijks')], string=""Schema herhaling"")

    def _fit_compute_is_participating(self):
        # we don't allow public user to see participating label
        if self.env.user != self.env.ref('base.public_user'):
            email = self.env.user.partner_id.email
            for event in self:
                domain = ['&', '|', ('email', '=', email), ('partner_id', '=', self.env.user.partner_id.id), ('event_id', '=', event.id),
                          ('state', '=', 'open')]
                count = self.env['event.registration'].search_count(domain)
                if count > 0:
                    event.fit_is_participating = True
                else:
                    event.fit_is_participating = False

    @api.onchange('date_begin')
    def update_day_of_week(self):
        start_date = self.date_begin_located
        if start_date:
            self.fit_day_of_week = datetime.strptime(start_date, '%Y-%m-%d %H:%M:%S').strftime('%a')

    def get_attendee_list(self):
        attendee_list = str('')
        counter = 1
        reg_ids = self.sudo().registration_ids
        reg_ids = sorted(reg_ids, key=lambda x: x.date_open, reverse=False)
        for registration in reg_ids:
            if registration.state == 'open':
                if counter == 1:
                    attendee_list += registration.partner_id.sudo().name
                else:
                    attendee_list += ', ' + registration.partner_id.sudo().name
                counter += 1
        return attendee_list

    def start_automatic_event_creation(self):
        repeating_event_ids = self.env['event.event'].search([('fit_repetition_enabled', '=', True)])
        for repeating_event in repeating_event_ids:
            _logger.info('Found repeating event: ' + repeating_event.name)
            if repeating_event.fit_repetition == 'daily':
                self._handle_daily_event_repetition(repeating_event)
            if repeating_event.fit_repetition == 'weekly':
                self._handle_weekly_event_repetition(repeating_event)
            if repeating_event.fit_repetition == 'monthly':
                self._handle_montly_event_repetition(repeating_event)

    def _handle_daily_event_repetition(self, old_repeating_event):
        _logger.info('Handling daily repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:00')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:00') + relativedelta(days=+1)
            new_end_date = end_date + relativedelta(days=+1)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _handle_weekly_event_repetition(self, old_repeating_event):
        _logger.info('Handling weekly repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:00')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:00') + relativedelta(days=+7)
            new_end_date = end_date + relativedelta(days=+7)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _handle_monthly_event_repetition(self, old_repeating_event):
        _logger.info('Handling monthly repeating event')
        end_date = datetime.strptime(old_repeating_event.date_end, '%Y-%m-%d %H:%M:00')
        present = datetime.now()
        if present >= end_date:
            new_start_date = datetime.strptime(old_repeating_event.date_begin, '%Y-%m-%d %H:%M:00') + relativedelta(months=+1)
            new_end_date = end_date + relativedelta(months=+1)
            if self._event_does_not_exist(old_repeating_event, new_end_date):
                self._create_new_event(old_repeating_event, new_start_date, new_end_date)

    def _event_does_not_exist(self, old_repeating_event, new_end_date):
        _logger.info('Checking new event existence: ' + old_repeating_event.name + ', date: ' + str(new_end_date))
        old_event_cat = old_repeating_event.event_type_id.id
        existing_event = self.env['event.event'].search([('event_type_id', '=', old_event_cat), ('date_end', '=', str(new_end_date))])
        if existing_event:
            return False
        else:
            return True

    def _create_new_event(self, old_repeating_event, new_start_date, new_end_date):
        _logger.info('Start creation new repeating event')
        new_repeating_event = old_repeating_event.copy(default={'website_published': True})
        new_repeating_event.date_end = new_end_date
        new_repeating_event.date_begin = new_start_date

        # 'date_begin': str(new_start_date), 'date_end_': str(new_end_date),
        old_repeating_event.fit_repetition_enabled = False
        old_repeating_event.fit_repetition = ''
/n/n/n/models/fit_partner.py/n/nimport logging
from datetime import datetime

from dateutil.relativedelta import relativedelta

from odoo import fields, models

_logger = logging.getLogger(__name__)


class Partner(models.Model):
    _name = 'res.partner'
    _inherit = ['res.partner']

    fit_subscriptions = fields.One2many(comodel_name='fit.subscription', inverse_name='subscription_partner', string='Inschrijving',
                                        store='True')
    fit_subscription_count = fields.Integer('# Inschrijvingen', compute='_compute_total_scubscriptions')

    def _compute_total_scubscriptions(self):
        counter = 0
        for subscription in self.fit_subscriptions:
            counter += 1
        self.fit_subscription_count = counter

    def can_subscribe(self, event):
        can_subscribe = False
        event_type = str(event.event_type_id.name).lower()
        event_start = datetime.strptime(event.date_begin, '%Y-%m-%d %H:%M:%S')
        if event_type == 'open' and len(self.fit_subscriptions) > 0:
            _logger.info('Can subscribe for open event id: %s, name: %s', event.event_type_id.name, event.name)
            return True
        if event_start < datetime.now():
            return False
        if (event_start + relativedelta(hours=-24)) > datetime.now() and event_type == 'crosstraining':
            return False
        for subscription in self.fit_subscriptions:
            if subscription._can_subscribe(event.event_type_id):
                _logger.info('Can subscribe for event id: %s, name: %s', event.event_type_id.name, event.name)
                can_subscribe = True
        return can_subscribe

    def can_unsubscribe(self, event):
        event_start = datetime.strptime(event.date_begin_located, '%Y-%m-%d %H:%M:%S')
        if event_start < datetime.now():
            return False
        return True
/n/n/n",1
104,104,833fe4ff19f8669fe7419bfb182ea26ce9c3af6d,"cauldron/runner/python_file.py/n/nimport os
import sys
import threading
import traceback
import types
import codecs
from importlib.abc import InspectLoader

from cauldron import environ
from cauldron import templating
from cauldron.cli import threads
from cauldron.runner import redirection
from cauldron.session import projects


class UserAbortError(Exception):
    pass


def set_executing(on: bool):
    """"""

    :param on:
    :return:
    """"""

    my_thread = threading.current_thread()

    if isinstance(my_thread, threads.CauldronThread):
        my_thread.is_executing = on


def get_file_contents(source_path: str) -> str:
    """""" """"""

    try:
        with codecs.open(source_path, encoding='utf-8') as f:
            return f.read()
    except Exception:
        pass

    with open(source_path, 'r') as f:
        return f.read()

def run(
        project: 'projects.Project',
        step: 'projects.ProjectStep',
) -> dict:
    """"""

    :param project:
    :param step:
    :return:
    """"""

    module_name = step.definition.name.rsplit('.', 1)[0]
    module = types.ModuleType(module_name)

    source_code = get_file_contents(step.source_path)

    try:
        code = InspectLoader.source_to_code(source_code, step.source_path)
    except SyntaxError as error:
        return render_syntax_error(project, source_code, error)

    setattr(module, '__file__', step.source_path)
    setattr(
        module,
        '__package__',
        '.'.join(
            [project.id.replace('.', '-')] +
            step.filename.rsplit('.', 1)[0].split(os.sep)
        )
    )

    def exec_test():
        step.test_locals = dict()
        step.test_locals.update(module.__dict__)
        exec(code, step.test_locals)

    try:
        set_executing(True)
        threads.abort_thread()

        if environ.modes.has(environ.modes.TESTING):
            exec_test()
        else:
            exec(code, module.__dict__)
        out = None
    except threads.ThreadAbortError:
        out = {'success': False}
    except UserAbortError:
        out = None
    except Exception as error:
        out = render_error(project, error)

    set_executing(False)

    return {'success': True} if out is None else out


def render_syntax_error(
        project: 'projects.Project',
        code: str,
        error: SyntaxError
) -> dict:
    """"""

    :param project:
    :param code:
    :param error:
    :return:
    """"""

    stack = [dict(
        filename=error.filename,
        location=None,
        line_number=error.lineno,
        line=error.text.rstrip()
    )]

    render_data = dict(
        type=error.__class__.__name__,
        message='{}'.format(error),
        stack=stack
    )

    return dict(
        success=False,
        error=error,
        message=templating.render_template(
            'user-code-error.txt',
            **render_data
        ),
        html_message=templating.render_template(
            'user-code-error.html',
            **render_data
        )
    )


def get_stack_frames():
    """"""

    :return:
    """"""

    cauldron_path = environ.paths.package()
    resources_path = environ.paths.resources()
    frames = list(traceback.extract_tb(sys.exc_info()[-1])).copy()

    def is_cauldron_code(test_filename: str) -> bool:
        if not test_filename or not test_filename.startswith(cauldron_path):
            return False

        if test_filename.startswith(resources_path):
            return False

        return True

    while len(frames) > 1 and is_cauldron_code(frames[0].filename):
        frames.pop(0)

    return frames


def format_stack_frame(stack_frame, project: 'projects.Project'):
    """"""

    :param stack_frame:
    :param project:
    :return:
    """"""

    filename = stack_frame.filename
    if filename.startswith(project.source_directory):
        filename = filename[len(project.source_directory) + 1:]

    location = stack_frame.name
    if location == '<module>':
        location = None

    return dict(
        filename=filename,
        location=location,
        line_number=stack_frame.lineno,
        line=stack_frame.line
    )


def render_error(
        project: 'projects.Project',
        error: Exception
) -> dict:
    """"""

    :param project:
    :param error:
    :return:
    """"""

    render_data = dict(
        type=error.__class__.__name__,
        message='{}'.format(error),
        stack=[format_stack_frame(f, project) for f in get_stack_frames()]
    )

    return dict(
        success=False,
        error=error,
        message=templating.render_template(
            'user-code-error.txt',
            **render_data
        ),
        html_message=templating.render_template(
            'user-code-error.html',
            **render_data
        )
    )
/n/n/ncauldron/session/buffering.py/n/nimport io
import time

from cauldron.cli.threads import abort_thread


class RedirectBuffer(io.TextIOWrapper):
    """"""
    A class for intercepting and independently storing buffer writes for use
    within Cauldron step display.
    """"""

    def __init__(self, redirection_source):
        self.active = False
        self.bytes_buffer = io.BytesIO()
        self.redirection_source = redirection_source
        self.last_write_time = 0

        super(RedirectBuffer, self).__init__(
            buffer=self.bytes_buffer,
            encoding=redirection_source.encoding,
            write_through=True
        )

    @property
    def source_encoding(self):
        if self.redirection_source.encoding:
            return self.redirection_source.encoding
        return 'utf-8'

    def read_all(self, encoding: str = None) -> str:
        """"""
        Reads the current state of the buffer and returns a string those
        contents

        :return:
            A string for the current state of the print buffer contents
        """"""

        try:
            buffered_bytes = self.bytes_buffer.getvalue()
            if buffered_bytes is None:
                return ''

            return buffered_bytes.decode(encoding or self.source_encoding)
        except Exception as err:
            return 'Redirect Buffer Error: {}'.format(err)

    def flush_all(self, encoding: str = None) -> str:
        """"""

        :return:
        """"""

        self.bytes_buffer.seek(0)
        contents = self.bytes_buffer.read()
        self.bytes_buffer.truncate(0)
        self.bytes_buffer.seek(0)

        if contents is None:
            return ''

        return contents.decode(encoding or self.source_encoding)

    def write_both(self, *args, **kwargs):
        abort_thread()

        if self.active:
            # Only write to this buffer if redirection is active. This prevents
            # race conditions from mixing buffers when attaching or removing
            # the write buffer from its sys output.
            self.last_write_time = time.time()
            super(RedirectBuffer, self).write(*args, **kwargs)

        return self.redirection_source.write(*args, **kwargs)

    def __getattribute__(self, item):
        """"""

        :param item:
        :return:
        """"""

        abort_thread()

        if item == 'write':
            # Writing should be done to both the source buffer and the redirect
            # buffer so that they have identical copies of the same information
            # for their separate uses
            return self.write_both
        elif item == 'close':
            # The source buffer should not be closed. The redirect buffer is
            # what should be closed by calls to instances of this class
            return super(RedirectBuffer, self).__getattribute__(item)

        # Access the source buffer using a super call to prevent recursion
        source = super(RedirectBuffer, self) \
            .__getattribute__('redirection_source')

        if hasattr(source, item):
            # Preference should be given to the source buffer for all other
            # operations given that this class is a wrapper around the source
            # buffer with the only added functionality being the intercepting
            # and duplication of write operations
            return getattr(source, item)

        # If the source buffer doesn't have a particular attribute it should
        # an attribute specific to this class
        return super(RedirectBuffer, self).__getattribute__(item)
/n/n/ncauldron/session/report.py/n/nimport os
import time

from cauldron.render import texts as render_texts
from cauldron.session.buffering import RedirectBuffer
from cauldron.session.caching import SharedCache


class Report(object):
    """"""
    The display management class for each step in a project. These class
    instances are exposed to Cauldron users, which provide the functionality
    for adding various element types to the display.
    """"""

    def __init__(self, step=None):
        self.step = step
        self.body = []
        self.css = []
        self.data = SharedCache()
        self.files = SharedCache()
        self.title = self.definition.get('title')
        self.subtitle = self.definition.get('subtitle')
        self.summary = self.definition.get('summary')
        self.library_includes = []
        self.stdout_interceptor = None  # type: RedirectBuffer
        self.stderr_interceptor = None  # type: RedirectBuffer

        self._last_update_time = 0

    @property
    def last_update_time(self) -> float:
        """""" The last time at which the report was modified """"""
        stdout = self.stdout_interceptor
        stderr = self.stderr_interceptor

        return max([
            self._last_update_time,
            stdout.last_write_time if stdout else 0,
            stderr.last_write_time if stderr else 0,
        ])

    @property
    def project(self):
        return self.step.project if self.step else None

    @property
    def results_cache_path(self) -> str:
        """"""
        Location where step report is cached between sessions to
        prevent loss of display data between runs

        :return:
        """"""

        if not self.project:
            return ''
        return os.path.join(
            self.project.results_path,
            '.cache',
            'steps',
            '{}.json'.format(self.id)
        )

    @property
    def id(self):
        return self.step.definition.name if self.step else None

    @property
    def definition(self) -> dict:
        return self.step.definition if self.step else None

    def clear(self) -> 'Report':
        """"""
        Clear all user-data stored in this instance and reset it to its
        originally loaded state

        :return:
            The instance that was called for method chaining
        """"""
        self.body = []
        self.data = SharedCache()
        self.files = SharedCache()
        self._last_update_time = time.time()
        return self

    def append_body(self, dom: str):
        """"""

        :param dom:
        :return:
        """"""

        self.flush_stdout()
        self.body.append(dom)
        self._last_update_time = time.time()

    def read_stdout(self):
        """"""
        Reads the current state of the print buffer (if it exists) and returns
        a body-ready dom object of those contents without adding them to the
        actual report body. This is useful for creating intermediate body
        values for display while the method is still executing.

        :return:
            A dom string for the current state of the print buffer contents
        """"""

        try:
            contents = self.stdout_interceptor.read_all(encoding='utf-8')
        except Exception as err:
            contents = ''

        return render_texts.preformatted_text(contents)

    def flush_stdout(self):
        """"""
        Empties
        """"""

        try:
            contents = self.stdout_interceptor.flush_all(encoding='utf-8')
        except Exception:
            return

        if len(contents) > 0:
            self.body.append(render_texts.preformatted_text(contents))
            self._last_update_time = time.time()

        return contents

    def read_stderr(self):
        """"""
        Returns the current state of the stderr redirect buffer This is useful
        for creating intermediate display values while the step is still
        executing.

        :return:
            A string of the current state of the stderr redirect buffer contents
        """"""

        try:
            return self.stderr_interceptor.read_all(encoding='utf-8')
        except Exception:
            return ''

    def flush_stderr(self) -> str:
        """"""
        Empties
        """"""

        try:
            return self.stderr_interceptor.flush_all(encoding='utf-8')
        except Exception:
            return ''
/n/n/n",0
105,105,833fe4ff19f8669fe7419bfb182ea26ce9c3af6d,"/cauldron/runner/python_file.py/n/nimport os
import sys
import threading
import traceback
import types
from importlib.abc import InspectLoader

from cauldron import environ
from cauldron import templating
from cauldron.cli import threads
from cauldron.runner import redirection
from cauldron.session import projects


class UserAbortError(Exception):
    pass


def set_executing(on: bool):
    """"""

    :param on:
    :return:
    """"""

    my_thread = threading.current_thread()

    if isinstance(my_thread, threads.CauldronThread):
        my_thread.is_executing = on


def run(
        project: 'projects.Project',
        step: 'projects.ProjectStep',
) -> dict:
    """"""

    :param project:
    :param step:
    :return:
    """"""

    module_name = step.definition.name.rsplit('.', 1)[0]
    module = types.ModuleType(module_name)

    with open(step.source_path, 'r') as f:
        source_code = f.read()

    try:
        code = InspectLoader.source_to_code(source_code, step.source_path)
    except SyntaxError as error:
        return render_syntax_error(project, source_code, error)

    setattr(module, '__file__', step.source_path)
    setattr(
        module,
        '__package__',
        '.'.join(
            [project.id.replace('.', '-')] +
            step.filename.rsplit('.', 1)[0].split(os.sep)
        )
    )

    def exec_test():
        step.test_locals = dict()
        step.test_locals.update(module.__dict__)
        exec(code, step.test_locals)

    try:
        set_executing(True)
        threads.abort_thread()

        if environ.modes.has(environ.modes.TESTING):
            exec_test()
        else:
            exec(code, module.__dict__)
        out = None
    except threads.ThreadAbortError:
        out = {'success': False}
    except UserAbortError:
        out = None
    except Exception as error:
        out = render_error(project, error)

    set_executing(False)

    return {'success': True} if out is None else out


def render_syntax_error(
        project: 'projects.Project',
        code: str,
        error: SyntaxError
) -> dict:
    """"""

    :param project:
    :param code:
    :param error:
    :return:
    """"""

    stack = [dict(
        filename=error.filename,
        location=None,
        line_number=error.lineno,
        line=error.text.rstrip()
    )]

    render_data = dict(
        type=error.__class__.__name__,
        message='{}'.format(error),
        stack=stack
    )

    return dict(
        success=False,
        error=error,
        message=templating.render_template(
            'user-code-error.txt',
            **render_data
        ),
        html_message=templating.render_template(
            'user-code-error.html',
            **render_data
        )
    )


def get_stack_frames():
    """"""

    :return:
    """"""

    cauldron_path = environ.paths.package()
    resources_path = environ.paths.resources()
    frames = list(traceback.extract_tb(sys.exc_info()[-1])).copy()

    def is_cauldron_code(test_filename: str) -> bool:
        if not test_filename or not test_filename.startswith(cauldron_path):
            return False

        if test_filename.startswith(resources_path):
            return False

        return True

    while len(frames) > 1 and is_cauldron_code(frames[0].filename):
        frames.pop(0)

    return frames


def format_stack_frame(stack_frame, project: 'projects.Project'):
    """"""

    :param stack_frame:
    :param project:
    :return:
    """"""

    filename = stack_frame.filename
    if filename.startswith(project.source_directory):
        filename = filename[len(project.source_directory) + 1:]

    location = stack_frame.name
    if location == '<module>':
        location = None

    return dict(
        filename=filename,
        location=location,
        line_number=stack_frame.lineno,
        line=stack_frame.line
    )


def render_error(
        project: 'projects.Project',
        error: Exception
) -> dict:
    """"""

    :param project:
    :param error:
    :return:
    """"""

    render_data = dict(
        type=error.__class__.__name__,
        message='{}'.format(error),
        stack=[format_stack_frame(f, project) for f in get_stack_frames()]
    )

    return dict(
        success=False,
        error=error,
        message=templating.render_template(
            'user-code-error.txt',
            **render_data
        ),
        html_message=templating.render_template(
            'user-code-error.html',
            **render_data
        )
    )
/n/n/n/cauldron/session/buffering.py/n/nimport io
import time

from cauldron.cli.threads import abort_thread


class RedirectBuffer(io.TextIOWrapper):
    """"""
    A class for intercepting and independently storing buffer writes for use
    within Cauldron step display.
    """"""

    def __init__(self, redirection_source):
        self.active = False
        self.bytes_buffer = io.BytesIO()
        self.redirection_source = redirection_source
        self.last_write_time = 0

        super(RedirectBuffer, self).__init__(
            buffer=self.bytes_buffer,
            encoding=redirection_source.encoding,
            write_through=True
        )

    @property
    def source_encoding(self):
        if self.redirection_source.encoding:
            return self.redirection_source.encoding
        return 'utf8'

    def read_all(self) -> str:
        """"""
        Reads the current state of the buffer and returns a string those
        contents

        :return:
            A string for the current state of the print buffer contents
        """"""

        try:
            buffered_bytes = self.bytes_buffer.getvalue()
            if buffered_bytes is None:
                return ''

            return buffered_bytes.decode(self.source_encoding)
        except Exception as err:
            return 'Redirect Buffer Error: {}'.format(err)

    def flush_all(self) -> str:
        """"""

        :return:
        """"""

        self.bytes_buffer.seek(0)
        contents = self.bytes_buffer.read()
        self.bytes_buffer.truncate(0)
        self.bytes_buffer.seek(0)

        if contents is None:
            return ''

        return contents.decode(self.source_encoding)

    def write_both(self, *args, **kwargs):
        abort_thread()

        if self.active:
            # Only write to this buffer if redirection is active. This prevents
            # race conditions from mixing buffers when attaching or removing
            # the write buffer from its sys output.
            self.last_write_time = time.time()
            super(RedirectBuffer, self).write(*args, **kwargs)

        return self.redirection_source.write(*args, **kwargs)

    def __getattribute__(self, item):
        """"""

        :param item:
        :return:
        """"""

        abort_thread()

        if item == 'write':
            # Writing should be done to both the source buffer and the redirect
            # buffer so that they have identical copies of the same information
            # for their separate uses
            return self.write_both
        elif item == 'close':
            # The source buffer should not be closed. The redirect buffer is
            # what should be closed by calls to instances of this class
            return super(RedirectBuffer, self).__getattribute__(item)

        # Access the source buffer using a super call to prevent recursion
        source = super(RedirectBuffer, self) \
            .__getattribute__('redirection_source')

        if hasattr(source, item):
            # Preference should be given to the source buffer for all other
            # operations given that this class is a wrapper around the source
            # buffer with the only added functionality being the intercepting
            # and duplication of write operations
            return getattr(source, item)

        # If the source buffer doesn't have a particular attribute it should
        # an attribute specific to this class
        return super(RedirectBuffer, self).__getattribute__(item)
/n/n/n/cauldron/session/report.py/n/nimport os
import time

from cauldron.render import texts as render_texts
from cauldron.session.buffering import RedirectBuffer
from cauldron.session.caching import SharedCache


class Report(object):
    """"""
    The display management class for each step in a project. These class
    instances are exposed to Cauldron users, which provide the functionality
    for adding various element types to the display.
    """"""

    def __init__(self, step=None):
        self.step = step
        self.body = []
        self.css = []
        self.data = SharedCache()
        self.files = SharedCache()
        self.title = self.definition.get('title')
        self.subtitle = self.definition.get('subtitle')
        self.summary = self.definition.get('summary')
        self.library_includes = []
        self.stdout_interceptor = None  # type: RedirectBuffer
        self.stderr_interceptor = None  # type: RedirectBuffer

        self._last_update_time = 0

    @property
    def last_update_time(self) -> float:
        """""" The last time at which the report was modified """"""
        stdout = self.stdout_interceptor
        stderr = self.stderr_interceptor

        return max([
            self._last_update_time,
            stdout.last_write_time if stdout else 0,
            stderr.last_write_time if stderr else 0,
        ])

    @property
    def project(self):
        return self.step.project if self.step else None

    @property
    def results_cache_path(self) -> str:
        """"""
        Location where step report is cached between sessions to
        prevent loss of display data between runs

        :return:
        """"""

        if not self.project:
            return ''
        return os.path.join(
            self.project.results_path,
            '.cache',
            'steps',
            '{}.json'.format(self.id)
        )

    @property
    def id(self):
        return self.step.definition.name if self.step else None

    @property
    def definition(self) -> dict:
        return self.step.definition if self.step else None

    def clear(self) -> 'Report':
        """"""
        Clear all user-data stored in this instance and reset it to its
        originally loaded state

        :return:
            The instance that was called for method chaining
        """"""
        self.body = []
        self.data = SharedCache()
        self.files = SharedCache()
        self._last_update_time = time.time()
        return self

    def append_body(self, dom: str):
        """"""

        :param dom:
        :return:
        """"""

        self.flush_stdout()
        self.body.append(dom)
        self._last_update_time = time.time()

    def read_stdout(self):
        """"""
        Reads the current state of the print buffer (if it exists) and returns
        a body-ready dom object of those contents without adding them to the
        actual report body. This is useful for creating intermediate body
        values for display while the method is still executing.

        :return:
            A dom string for the current state of the print buffer contents
        """"""

        try:
            contents = self.stdout_interceptor.read_all()
        except Exception as err:
            contents = ''

        return render_texts.preformatted_text(contents)

    def flush_stdout(self):
        """"""
        Empties
        """"""

        try:
            contents = self.stdout_interceptor.flush_all()
        except Exception:
            return

        if len(contents) > 0:
            self.body.append(render_texts.preformatted_text(contents))
            self._last_update_time = time.time()

        return contents

    def read_stderr(self):
        """"""
        Returns the current state of the stderr redirect buffer This is useful
        for creating intermediate display values while the step is still
        executing.

        :return:
            A string of the current state of the stderr redirect buffer contents
        """"""

        try:
            return self.stderr_interceptor.read_all()
        except Exception:
            return ''

    def flush_stderr(self) -> str:
        """"""
        Empties
        """"""

        try:
            return self.stderr_interceptor.flush_all()
        except Exception:
            return ''
/n/n/n",1
98,98,888f6d388874b3da8faeafd97b82c251f7d4ea34,"server.py/n/n###########################################################################
# IMPORTS - GEN ###########################################################

from flask import (Flask,
                   # Flask allows app object
                   session, flash,
                   # session allows use of session storage for login
                   render_template, redirect,
                   # render_template allows html render functionality
                   request,
                   # request allows use of forms in html templates
                   jsonify)
from flask_debugtoolbar import DebugToolbarExtension
import jinja2
import random
from model import (db, connect_to_db,
                   User, Team, UserTeam, Board, Project, Phase)
import query as q
import helper as h

# import pdb; pdb.set_trace()


###########################################################################
# FLASK APP SETUP #########################################################

app = Flask(__name__)  # makes app object
app.secret_key = ""It's great to stay up late""
    # allows session use 'under the hood'

app.jinja_env.undefined = jinja2.StrictUndefined
    # Normally, if you refer to an undefined variable in a Jinja template,
        # Jinja silently ignores this. ""This makes debugging difficult, so
        # we'll set an attribute of the Jinja environment that says to make
        # this an error.""""
app.jinja_env.auto_reload = True
    # Fixes error that will sometimes happen where (in some versions of Flask)
        # you have to re-start your server with every change on your template.


###########################################################################
# SESSION STORAGE #########################################################

# Keys:
    # ""is_logged_in"" (bool)
    # ""user_id"" (int)
    # ""team_id"" (int)
    # ""new_user"" (bool)
    # ""displayname"" (str)
    # ""current_board"" (int)


###########################################################################
# INDEX ###################################################################

@app.route(""/"")
def index():
    """"""Return index (homepage).""""""

    logged_in = session.get(""is_logged_in"")

    # How do I check for logged in status before rendering?
    # What do I want to show for people who are logged in?
    return render_template(""home.html"")


###########################################################################
# REGISTRATION ############################################################

@app.route(""/register"", methods=[""POST""])
def make_new_user():
    """"""Validate new user form entry, register user if valid.""""""

    email = request.form.get('email')
    pw = request.form.get('pw')
    displayname = request.form.get('displayname')

    user_record = User.query.filter(User.email == email).first()
    # queries user table for first record for email; returns None if no record
    if user_record is None:

        new_user = q.make_user(email, pw, displayname)
        q.add_to_db(new_user)

        user = q.get_user_by_email(email)
        h.update_session_for_good_login(user.u_id, user.displayname)

        session[""new_user""] = True  # Pending: Tutorial
        flash(""Account created!"")
        return redirect(""/dashboard"")

    else:
        flash(""That email address has already been registered"")
        return redirect(""/"")


###########################################################################
# LOG IN ##################################################################

@app.route(""/login"", methods=[""GET""])
def display_login():
    """"""Load login form.""""""

    return render_template(""login.html"")


@app.route(""/login"", methods=[""POST""])
def log_in_returning_user():
    """"""Validate login entry.""""""

    # update login count to calculate attempts and remaining
    num_attempts = h.get_login_attempts()
    remaining = h.calc_attempts_remaining(num_attempts)

    # getting data from user input in login.html form
    email = request.form.get('email')
    pw = request.form.get('pw')

    user_record = q.get_user_by_email(email)

    if user_record is None:
        flash(""No account found with that email. Would you like to register?"")
        return redirect(""/login"")

    else:  # the email is valid

        # validate password, handle accordingly
        if user_record.password != pw:
            template = h.handle_bad_attempts(remaining)
            return render_template(template)

        # is valid password, handle accordingly
        else:
            h.update_session_for_good_login(user_record.u_id,
                                            user_record.displayname)
            flash(""Welcome back to SamePage"")
            return redirect(""/dashboard"")


 # LOGIN: PASSWORD HANDLING ##############################################

@app.route(""/login/password-recovery"")
def password_recovery():
    """"""Displays form to send email to user for password recovery""""""

    return ""OOOOOOOPS""


###########################################################################
# DASHBOARD ###############################################################

@app.route(""/dashboard"")
def dashboard():
    """"""Renders dashboard view, grabbing existing teams for display""""""

    session[""team_id""] = None
        # Creates a session key for team_id, which is needed in the new board
        # route, and therefore must be reset.

    if session.get(""new_user""):
        flash(""""""Welcome to SamePage. Hover over different areas on our pages
            for tutorial tips. You can turn the tutorial off and on from your
            Dashboard."""""")

    if session.get(""is_logged_in"") is True:
        # Fossil from validation version; does not hurt to keep
        teams_list = []
        invites_list = []
        user_id = session.get(""user_id"")
        user_object = q.get_user_object(user_id)

        ut_objects = user_object.userteams  # makes a list of objects
        for userteam in ut_objects:
            if userteam.is_member:
                team_dict = {""team_id"": userteam.team_id,
                             ""name"": userteam.team.name,
                             ""desc"": userteam.team.desc}
                teams_list.append(team_dict)
            elif userteam.is_member is None:
            # null value means invite decision pending
                invite_dict = {""team_id"": userteam.team_id,
                               ""name"": userteam.team.name,
                               ""desc"": userteam.team.desc}
                invites_list.append(invite_dict)

        return render_template('dashboard.html',
                               teams_list=teams_list,
                               invites_list=invites_list,
                               displayname=user_object.displayname)

    else:
        return redirect(""/"")
            # Prevents view if not logged in
            # Fossil from validation version; does not hurt to keep


@app.route(""/new-team"", methods=[""POST""])
def create_team():
    """"""Create Team model and UserTeam model, updating database each time.""""""

    name = request.form.get(""name"", ""Untitled"")
    desc = request.form.get(""description"", None)

    user_id = session.get(""user_id"")

    new_team = q.make_team(name, desc)
    q.add_to_db(new_team)

    # We now have the team id, so we can make the UserTeam relationship
    new_userteam = q.make_userteam(user_id, new_team.t_id)
    q.add_to_db(new_userteam)

    # flash(""Team created! MAKE POPUP TO ASK To GO STRAIGHT TO THE TEAM PAGE"")
    return jsonify({""teamId"": new_team.t_id})


@app.route(""/team-invitation"", methods=[""POST""])
def update_team_membership():
    """"""Update UserTeam membership field's value to true;
    update Dashboard with a redirect.""""""

    # This route is currently in contrast to the style of making a new team,
        # which is an ajax request.
    user_id = session[""user_id""]
    team_id = request.form.get(""team"")
    user_choice = request.form.get(""is_joining"")
        # in dashboard.html hidden value; True or False

    # Cleanse data from html as soon as possible
    if user_choice == ""True"":
        user_choice = True  # this line doesn't work
    else:
        user_choice = False

    q.update_userteam_relationship(user_id, team_id, user_choice)

    flash(""Your team invites have been updated!"")

    return redirect(""/dashboard"")


@app.route(""/ignored-teams"", methods=[""GET""])
def display_ignored_teams():
    """"""PENDING PENDING PENDING""""""
    return ""Pending my good lady""


###########################################################################
# TEAM MAIN AND BOARDS ####################################################

@app.route(""/view-team"")
def view_team():
    """"""Renders view of team page, with board""""""

    team_id = session.get(""team_id"")

    team_object = Team.query.filter_by(t_id=team_id).first()  # REFACTOR THIS

    return render_template(""team-main.html"", team=team_object)


@app.route(""/view-team"", methods=[""POST""])
def view_team_and_update_session():
    """"""Renders view of team page, with board""""""

    team_id = request.form.get(""team"")

    session[""team_id""] = team_id

    team_object = Team.query.filter_by(t_id=team_id).first()  # REFACTOR THIS

    return render_template(""team-main.html"", team=team_object)


@app.route(""/new-board"", methods=[""POST""])
def make_new_board():
    """"""Make a new board and update page without refresh; ajax.""""""

    ##### VALIDATION HERE PLEASE ######
    user_id = session.get(""user_id"")

    name = request.form.get(""new-board-name"", ""Untitled"")  # board's name input
        # Is this a good way to handle not requiring the team or board name
            # in the form, but in the data fields?
    desc = request.form.get(""new-board-desc"", None)  # board's desc input
    team_id = request.form.get(""team-id"")

    session[""team_id""] = team_id
    # Not sure if I need this; it should already be there, but this keeps it
        # current

    new_board = q.make_board(name, desc, team_id)
    q.add_to_db(new_board)
    session[""current_board""] = new_board.b_id

    flash(""New board successfully created."")
    return redirect(""/view-team"")


@app.route(""/current-board"", methods=[""POST""])
def update_most_recently_clicked_board():
    """""" """"""

    # Below is v1.0. Next version involves updating the db model to track this
    # information, so the board is displayed on login.

    board_id = request.form.get(""boardId"")
    session[""current_board""] = board_id
    print ""Session updated with board {}."".format(board_id)

    return ""HTTP-status-code: 200""


@app.route(""/claim-project"", methods=[""POST""])
def assign_user_to_project():
    """"""Update database with user_id for the project.""""""

    user_id = session.get(""user_id"")
    project_id = request.form.get(""projectId"")

    q.update_user_claiming_project(user_id, project_id)
        # Also updates project to ""item""

    return ""HTTP-status-code: 200""


@app.route(""/add-to-board"", methods=[""POST""])
def add_new_project_to_board():
    """"""Update database with new project and display on correct board
    on team main.""""""

    # make the board that the project was added to show by default. important
    title = request.form.get(""new-project-title"", ""Untitled"")
    # The title text box is required, but this is in case I change that soon.
    notes = request.form.get(""new-project-notes"", None)
    phase_code = request.form.get(""project-phase"")
    board_id = request.form.get(""board-id"")

    new_project = q.make_project(title, notes, phase_code, board_id)
    q.add_to_db(new_project)

    # need to use datetime, rather than session key, pick which board to have
        # open...how do we update datetime, and how do
    flash(""New a new {} has been added to your board!"".format(phase_code))
    return redirect(""/view-team"")


@app.route(""/view-details/<int:project_id>"", methods=['GET'])
def open_project_details(project_id):
    """""" """"""

    project_object = Project.query.filter_by(p_id=project_id).first()
    user_id = session.get(""user_id"")
    results = {""userId"": user_id,
               ""pOwnerId"": project_object.user_id,
               ""pTitle"": project_object.title,
               ""pNotes"": project_object.notes,
               ""pPhase"": project_object.phase_code,
               ""pUpvotes"": project_object.upvotes,
               ""pUpdated"": project_object.updated
               }
    if project_object.user_id:
        results[""pOwnerName""] = project_object.user.displayname
    print results.keys
    return jsonify(results)


@app.route(""/save-update/<int:project_id>"", methods=['POST'])
def save_updated_project_details(project_id):
    """""" """"""
    project_object = Project.query.filter_by(p_id=project_id).first()

    # One checkbox with name completed, so using .get is fine
    checked_lst = request.form.get(""completion"")
    updated_notes = request.form.get(""notes"")

    project_object.notes = updated_notes

    congratulatory_messages = [""High five!"", ""Nice work!"", ""You rock."",
                               ""Nice.""]

    if checked_lst == ""is-checked"":  # making explicit

        project_object.phase_code = ""done""
        flash(""Action item is completed. {}"".format
              (random.choice(congratulatory_messages)))
        # A little corny...but that is on brand. So why not.
    else:
        flash(""Changes saved."")
    db.session.commit()

    return redirect(""/view-team"")


@app.route(""/invite-teammates/<int:team_id>"", methods=['POST'])
def invite_new_teammates(team_id):
    """""" """"""

    team_object = Team.query.filter_by(t_id=team_id).first()

    emails_lst = request.form.getlist(""email"")
    messages_list = request.form.getlist(""email-message"")
    sender = session.get(""displayname"")

    default_message = """"""{sender} has invited you to join the team
    {team_name} on SamePage. Accept to help complete projects for
    {team_name}."""""".format(sender=sender, team_name=team_object.name)

    flash_message = ""Emails sent to\n""
    for i in xrange(len(emails_lst)):
        if not messages_list[i]:
            message = default_message
        else:
            message = messages_list[i]
        flash_message = flash_message + emails_lst[i] + ""\n""

        h.send_team_invite(emails_lst[i],
                           sender,
                           message,
                           team_object.name)

    flash(flash_message)
    return redirect(""/view-team"")


###########################################################################
# ACTION BOARD ############################################################

@app.route(""/actions-board"")
def display_user_actions_board():
    """"""Retrieve user and project data from db,
    render projects on action page. """"""

    if session.get(""is_logged_in"") is True:
        # Fossil from validation version; does not hurt to keep
        user_id = session.get(""user_id"")
        projects_objects = q.get_projects_by_user(user_id)

    return render_template(""actions-board.html"", projects=projects_objects)


###########################################################################
# LOG OUT #################################################################

@app.route(""/logout"", methods=[""POST""])
def logout_user():
    """""" """"""

    session.clear()
    # flash(""You have been logged out."")

    return redirect(""/"")


@app.route(""/logout"", methods=[""GET""])
def logout_user_when_site_crashes():
    """""" """"""

    return redirect(""/"")


###########################################################################
# EDIT HEADERS ############################################################

@app.after_request
def add_header(r):
    """"""Flask utility to force a cache reload by adding settings in headers""""""

    r.headers[""Cache-Control""] = ""no-cache, no-store, must-revalidate""
    # To fix the issue of making a new team on the dashboard, and using the
    #browser's nav button to go back
    return r

###########################################################################
# DIRECT FILE CALL ########################################################

if __name__ == ""__main__"":

    app.debug = True
    # prevents server side caching while in debug mode?
    app.jinja_env.auto_reload = app.debug

    connect_to_db(app)  # model file houses all ORM
    DebugToolbarExtension(app)  # Use the DebugToolbar
    app.run(host='0.0.0.0')  # DO NOT FORGET TO CHANGE THIS FOR RELEASE
/n/n/n",0
99,99,888f6d388874b3da8faeafd97b82c251f7d4ea34,"/server.py/n/n###########################################################################
# IMPORTS - GEN ###########################################################

from flask import (Flask,
                   # Flask allows app object
                   session, flash,
                   # session allows use of session storage for login
                   render_template, redirect,
                   # render_template allows html render functionality
                   request,
                   # request allows use of forms in html templates
                   jsonify)
from flask_debugtoolbar import DebugToolbarExtension
import jinja2
import random
from model import (db, connect_to_db,
                   User, Team, UserTeam, Board, Project, Phase)
import query as q
import helper as h

# import pdb; pdb.set_trace()


###########################################################################
# FLASK APP SETUP #########################################################

app = Flask(__name__)  # makes app object
app.secret_key = ""It's great to stay up late""
    # allows session use 'under the hood'

app.jinja_env.undefined = jinja2.StrictUndefined
    # Normally, if you refer to an undefined variable in a Jinja template,
        # Jinja silently ignores this. ""This makes debugging difficult, so
        # we'll set an attribute of the Jinja environment that says to make
        # this an error.""""
app.jinja_env.auto_reload = True
    # Fixes error that will sometimes happen where (in some versions of Flask)
        # you have to re-start your server with every change on your template.


###########################################################################
# SESSION STORAGE #########################################################

# Keys:
    # ""is_logged_in"" (bool)
    # ""user_id"" (int)
    # ""team_id"" (int)
    # ""new_user"" (bool)
    # ""displayname"" (str)
    # ""current_board"" (int)


###########################################################################
# INDEX ###################################################################

@app.route(""/"")
def index():
    """"""Return index (homepage).""""""

    logged_in = session.get(""is_logged_in"")

    # How do I check for logged in status before rendering?
    # What do I want to show for people who are logged in?
    return render_template(""home.html"")


###########################################################################
# REGISTRATION ############################################################

@app.route(""/register"", methods=[""POST""])
def make_new_user():
    """"""Validate new user form entry, register user if valid.""""""

    email = request.form.get('email')
    pw = request.form.get('pw')
    displayname = request.form.get('displayname')

    user_record = User.query.filter(User.email == email).first()
    # queries user table for first record for email; returns None if no record
    if user_record is None:

        new_user = q.make_user(email, pw, displayname)
        q.add_to_db(new_user)

        user = q.get_user_by_email(email)
        h.update_session_for_good_login(user.u_id, user.displayname)

        session[""new_user""] = True  # Pending: Tutorial
        flash(""Account created!"")
        return redirect(""/dashboard"")

    else:
        flash(""That email address has already been registered"")
        return redirect(""/"")


###########################################################################
# LOG IN ##################################################################

@app.route(""/login"", methods=[""GET""])
def display_login():
    """"""Load login form.""""""

    return render_template(""login.html"")


@app.route(""/login"", methods=[""POST""])
def log_in_returning_user():
    """"""Validate login entry.""""""

    # update login count to calculate attempts and remaining
    num_attempts = h.get_login_attempts()
    remaining = h.calc_attempts_remaining(num_attempts)

    # getting data from user input in login.html form
    email = request.form.get('email')
    pw = request.form.get('pw')

    user_record = q.get_user_by_email(email)

    if user_record is None:
        flash(""No account found with that email. Would you like to register?"")
        return redirect(""/login"")

    else:  # the email is valid

        # validate password, handle accordingly
        if user_record.password != pw:
            template = h.handle_bad_attempts(remaining)
            return render_template(template)

        # is valid password, handle accordingly
        else:
            h.update_session_for_good_login(user_record.u_id,
                                            user_record.displayname)
            flash(""Welcome back to SamePage"")
            return redirect(""/dashboard"")


 # LOGIN: PASSWORD HANDLING ##############################################

@app.route(""/login/password-recovery"")
def password_recovery():
    """"""Displays form to send email to user for password recovery""""""

    return ""OOOOOOOPS""


###########################################################################
# DASHBOARD ###############################################################

@app.route(""/dashboard"")
def dashboard():
    """"""Renders dashboard view, grabbing existing teams for display""""""

    session[""team_id""] = None
        # Creates a session key for team_id, which is needed in the new board
        # route, and therefore must be reset.

    if session.get(""new_user""):
        flash(""""""Welcome to SamePage. Hover over different areas on our pages
            for tutorial tips. You can turn the tutorial off and on from your
            Dashboard."""""")

    if session.get(""is_logged_in"") is True:
        # Fossil from validation version; does not hurt to keep
        teams_list = []
        invites_list = []
        user_id = session.get(""user_id"")
        user_object = q.get_user_object(user_id)

        ut_objects = user_object.userteams  # makes a list of objects
        for userteam in ut_objects:
            if userteam.is_member:
                team_dict = {""team_id"": userteam.team_id,
                             ""name"": userteam.team.name,
                             ""desc"": userteam.team.desc}
                teams_list.append(team_dict)
            elif userteam.is_member is None:
            # null value means invite decision pending
                invite_dict = {""team_id"": userteam.team_id,
                               ""name"": userteam.team.name,
                               ""desc"": userteam.team.desc}
                invites_list.append(invite_dict)

        return render_template('dashboard.html', 
                               teams_list=teams_list,
                               invites_list=invites_list, 
                               displayname=user_object.displayname)

    else:
        return redirect(""/"")
            # Prevents view if not logged in
            # Fossil from validation version; does not hurt to keep


@app.route(""/new-team"", methods=[""POST""])
def create_team():
    """"""Create Team model and UserTeam model, updating database each time.""""""

    name = request.form.get(""name"", ""Untitled"")
    desc = request.form.get(""description"", None)

    user_id = session.get(""user_id"")

    new_team = q.make_team(name, desc)
    q.add_to_db(new_team)

    # We now have the team id, so we can make the UserTeam relationship
    new_userteam = q.make_userteam(user_id, new_team.t_id)
    q.add_to_db(new_userteam)

    # flash(""Team created! MAKE POPUP TO ASK To GO STRAIGHT TO THE TEAM PAGE"")
    return jsonify({""teamId"": new_team.t_id})


@app.route(""/team-invitation"", methods=[""POST""])
def update_team_membership():
    """"""Update UserTeam membership field's value to true;
    update Dashboard with a redirect.""""""

    # This route is currently in contrast to the style of making a new team,
        # which is an ajax request.
    user_id = session[""user_id""]
    team_id = request.form.get(""team"")
    user_choice = request.form.get(""is_joining"")
        # in dashboard.html hidden value; True or False

    # Cleanse data from html as soon as possible
    if user_choice == ""True"":
        user_choice = True  # this line doesn't work
    else:
        user_choice = False

    q.update_userteam_relationship(user_id, team_id, user_choice)

    flash(""Your team invites have been updated!"")

    return redirect(""/dashboard"")


@app.route(""/ignored-teams"", methods=[""GET""])
def display_ignored_teams():
    """"""PENDING PENDING PENDING""""""
    return ""Pending my good lady""


###########################################################################
# TEAM MAIN AND BOARDS ####################################################

@app.route(""/view-team"")
def view_team():
    """"""Renders view of team page, with board""""""

    team_id = session.get(""team_id"")

    team_object = Team.query.filter_by(t_id=team_id).first()  # REFACTOR THIS

    return render_template(""team-main.html"", team=team_object)


@app.route(""/view-team"", methods=[""POST""])
def view_team_and_update_session():
    """"""Renders view of team page, with board""""""

    team_id = request.form.get(""team"")

    session[""team_id""] = team_id

    team_object = Team.query.filter_by(t_id=team_id).first()  # REFACTOR THIS

    return render_template(""team-main.html"", team=team_object)


@app.route(""/new-board"", methods=[""POST""])
def make_new_board():
    """"""Make a new board and update page without refresh; ajax.""""""

    ##### VALIDATION HERE PLEASE ######
    user_id = session.get(""user_id"")

    name = request.form.get(""new-board-name"", ""Untitled"")  # board's name input
        # Is this a good way to handle not requiring the team or board name
            # in the form, but in the data fields?
    desc = request.form.get(""new-board-desc"", None)  # board's desc input
    team_id = request.form.get(""team-id"")

    session[""team_id""] = team_id
    # Not sure if I need this; it should already be there, but this keeps it
        # current

    new_board = q.make_board(name, desc, team_id)
    q.add_to_db(new_board)

    flash(""Board created! MAKE THAT BOARD SHOW AS DEFAULT!!!!"")
    return redirect(""/view-team"")


@app.route(""/current-board"", methods=[""POST""])
def update_most_recently_clicked_board():
    """""" """"""

    # Below is v1.0. Next version involves updating the db model to track this
    # information, so the board is displayed on login.

    board_id = request.form.get(""boardId"")
    session[""current_board""] = board_id
    print ""Session updated with board {}."".format(board_id)

    return ""HTTP-status-code: 200""


@app.route(""/claim-project"", methods=[""POST""])
def assign_user_to_project():
    """"""Update database with user_id for the project.""""""

    user_id = session.get(""user_id"")
    project_id = request.form.get(""projectId"")

    q.update_user_claiming_project(user_id, project_id)
        # Also updates project to ""item""

    return ""HTTP-status-code: 200""


@app.route(""/add-to-board"", methods=[""POST""])
def add_new_project_to_board():
    """"""Update database with new project and display on correct board
    on team main.""""""

    # make the board that the project was added to show by default. important
    title = request.form.get(""new-project-title"", ""Untitled"")
    # The title text box is required, but this is in case I change that soon.
    notes = request.form.get(""new-project-notes"", None)
    phase_code = request.form.get(""project-phase"")
    board_id = request.form.get(""board-id"")

    new_project = q.make_project(title, notes, phase_code, board_id)
    q.add_to_db(new_project)

    # need to use datetime, rather than session key, pick which board to have
        # open...how do we update datetime, and how do
    flash(""New a new {} has been added to your board!"".format(phase_code))
    return redirect(""/view-team"")


@app.route(""/view-details/<int:project_id>"", methods=['GET'])
def open_project_details(project_id):
    """""" """"""

    project_object = Project.query.filter_by(p_id=project_id).first()
    user_id = session.get(""user_id"")
    results = {""userId"": user_id,
               ""pOwnerId"": project_object.user_id,
               ""pTitle"": project_object.title,
               ""pNotes"": project_object.notes,
               ""pPhase"": project_object.phase_code,
               ""pUpvotes"": project_object.upvotes,
               ""pUpdated"": project_object.updated
               }
    if project_object.user_id:
        results[""pOwnerName""] = project_object.user.displayname
    print results.keys
    return jsonify(results)


@app.route(""/save-update/<int:project_id>"", methods=['POST'])
def save_updated_project_details(project_id):
    """""" """"""
    project_object = Project.query.filter_by(p_id=project_id).first()

    # One checkbox with name completed, so using .get is fine
    checked_lst = request.form.get(""completion"")
    updated_notes = request.form.get(""notes"")

    project_object.notes = updated_notes

    congratulatory_messages = [""High five!"", ""Nice work!"", ""You rock."",
                               ""Nice.""]

    if checked_lst == ""is-checked"":  # making explicit

        project_object.phase_code = ""done""
        flash(""Action item is completed. {}"".format
              (random.choice(congratulatory_messages)))
        # A little corny...but that is on brand. So why not.
    else:
        flash(""Changes saved."")
    db.session.commit()

    return redirect(""/view-team"")


@app.route(""/invite-teammates/<int:team_id>"", methods=['POST'])
def invite_new_teammates(team_id):
    """""" """"""

    team_object = Team.query.filter_by(t_id=team_id).first()

    emails_lst = request.form.getlist(""email"")
    messages_list = request.form.getlist(""email-message"")
    sender = session.get(""displayname"")

    default_message = """"""{sender} has invited you to join the team
    {team_name} on SamePage. Accept to help complete projects for
    {team_name}."""""".format(sender=sender, team_name=team_object.name)

    flash_message = ""Emails sent to\n""
    for i in xrange(len(emails_lst)):
        if not messages_list[i]:
            message = default_message
        else:
            message = messages_list[i]
        flash_message = flash_message + emails_lst[i] + ""\n""

        h.send_team_invite(emails_lst[i],
                           sender,
                           message,
                           team_object.name)

    flash(flash_message)
    return redirect(""/view-team"")


###########################################################################
# ACTION BOARD ############################################################

@app.route(""/actions-board"")
def display_user_actions_board():
    """"""Retrieve user and project data from db,
    render projects on action page. """"""

    if session.get(""is_logged_in"") is True:
        # Fossil from validation version; does not hurt to keep
        user_id = session.get(""user_id"")
        projects_objects = q.get_projects_by_user(user_id)

    return render_template(""actions-board.html"", projects=projects_objects)


###########################################################################
# LOG OUT #################################################################

@app.route(""/logout"", methods=[""POST""])
def logout_user():
    """""" """"""

    session.clear()
    # flash(""You have been logged out."")

    return redirect(""/"")


@app.route(""/logout"", methods=[""GET""])
def logout_user_when_site_crashes():
    """""" """"""

    return redirect(""/"")


###########################################################################
# EDIT HEADERS ############################################################

@app.after_request
def add_header(r):
    """"""Flask utility to force a cache reload by adding settings in headers""""""

    r.headers[""Cache-Control""] = ""no-cache, no-store, must-revalidate""
    # To fix the issue of making a new team on the dashboard, and using the
    #browser's nav button to go back
    return r

###########################################################################
# DIRECT FILE CALL ########################################################

if __name__ == ""__main__"":

    app.debug = True
    # prevents server side caching while in debug mode?
    app.jinja_env.auto_reload = app.debug

    connect_to_db(app)  # model file houses all ORM
    DebugToolbarExtension(app)  # Use the DebugToolbar
    app.run(host='0.0.0.0')  # DO NOT FORGET TO CHANGE THIS FOR RELEASE
/n/n/n",1
146,146,4c50beb72795d4518a344c265dca009c926d12bb,"screendoor/forms.py/n/nimport magic
import mimetypes
from django import forms
from django.contrib.auth import get_user_model, authenticate
from django.contrib.auth.forms import UserCreationForm

from .models import ScreenDoorUser, Position, Applicant
from .uservisibletext import ErrorMessages, CreatePositionFormText, \
    CreateAccountFormText, StandardFormText, LoginFormText, \
    ImportApplicationsText


# For uploading completed applications to a position
class ImportApplicationsForm(forms.ModelForm):
    title = ImportApplicationsText.title
    description = ImportApplicationsText.description
    upload = ImportApplicationsText.upload
    browse = ImportApplicationsText.browse
    choose_files = ImportApplicationsText.choose_files

    class Meta:
        model = Applicant
        fields = ('pdf', )


# For creating a new position
class CreatePositionForm(forms.ModelForm):
    text = CreatePositionFormText.upload_new_position
    description = CreatePositionFormText.please_select_either_filetype
    pdf_name = CreatePositionFormText.pdf
    url_name = CreatePositionFormText.url
    pdf_text = CreatePositionFormText.browse_for_pdf
    url_text = CreatePositionFormText.link_to_job_description
    upload_text = CreatePositionFormText.choose_a_file
    browse_text = CreatePositionFormText.browse
    submit_text = CreatePositionFormText.submit

    class Meta:
        model = Position
        fields = ('pdf', 'url_ref')
        widgets = {'url_ref': forms.TextInput(attrs={'disabled': 'disabled'})}

    def clean(self):
        pdf = self.cleaned_data.get('pdf')
        url = self.cleaned_data.get('url_ref')
        # Check for an empty form
        if not pdf and not url:
            msg = forms.ValidationError(
                ErrorMessages.empty_create_position_form)
            self.add_error('pdf', msg)
            return
        # Check for an overfilled form
        elif pdf and url:
            msg = forms.ValidationError(
                ErrorMessages.overfilled_create_position_form)
            self.add_error('pdf', msg)
            return

        # Verify if the pdf upload has an correct mimetype (i.e. a pdf file)
        if pdf:
            file_type = mimetypes.MimeTypes().types_map_inv[1][
                magic.from_buffer(self.cleaned_data['pdf'].read(), mime=True)
            ][0]
            if not (file_type == '.pdf'):
                msg = forms.ValidationError(
                    ErrorMessages.incorrect_mime_type)
                self.add_error('pdf', msg)

        # Verify if the url matches the job.gc.ca domain
        if url:
            # Note: Below code is temporary, until url uploading is supported.
            msg = forms.ValidationError(
                ErrorMessages.url_upload_not_supported_yet)
            self.add_error('url_ref', msg)

            # Note: Desired code below.
            # if not ""https://emploisfp-psjobs.cfp-psc.gc.ca"" in url:
            #     msg = forms.ValidationError(
            #         ErrorMessages.invalid_url_domain)
            #     self.add_error('url_ref', msg)

        return self.cleaned_data


class ScreenDoorUserCreationForm(UserCreationForm):
    text = CreateAccountFormText.create_account
    email_text = CreateAccountFormText.email_address
    password_text = CreateAccountFormText.choose_password
    password_confirm_text = CreateAccountFormText.confirm_password
    email = forms.EmailField(
        label=StandardFormText.username_or_email_label, max_length=100)
    login_button_text = CreateAccountFormText.have_an_account_sign_in

    class Meta(UserCreationForm):
        model = ScreenDoorUser
        fields = ('email',)

    # Clean and validate fields. Password validation is handled by Django UserCreationForm
    def clean(self):
        email = self.cleaned_data.get('email')
        # Validate e-mail domain (canada.ca only)
        email_domain = email.split('@')[1].lower()
        if email_domain != ""canada.ca"":
            message = forms.ValidationError(
                format(ErrorMessages.invalid_email_domain % email_domain))
            self.add_error('email', message)
        # Validate if e-mail is unique in system
        elif get_user_model().objects.filter(username=email.lower()).exists():
            message = forms.ValidationError(
                format(ErrorMessages.user_already_exists % email))
            self.add_error('email', message)

        return self.cleaned_data


class LoginForm(forms.Form):
    login_text = LoginFormText.login
    create_account_text = StandardFormText.create_account
    email = forms.EmailField(
        label=StandardFormText.username_or_email_label, max_length=100)
    password = forms.CharField(
        label=LoginFormText.password, min_length=8, max_length=42, widget=forms.PasswordInput)

    def clean(self):
        # Entered e-mail is compared as lower to ensure login is not case-sensitive
        email = self.cleaned_data.get('email').lower()
        password = self.cleaned_data.get('password')
        user = authenticate(username=email, password=password)

        # Does user exist in system?
        if user is None:
            message = forms.ValidationError(ErrorMessages.invalid_un_or_pw)
            self.add_error('email', message)
        # Has user confirmed e-mail address
        elif user.email_confirmed is False:
            message = forms.ValidationError(ErrorMessages.unconfirmed_email)
            self.add_error('email', message)

        return self.cleaned_data

    def get_user(self):
        # Entered e-mail is compared as lower to ensure login is not case-sensitive
        email = self.cleaned_data.get('email').lower()
        password = self.cleaned_data.get('password')
        return authenticate(username=email, password=password)
/n/n/nscreendoor/uservisibletext.py/n/nfrom django.utils.translation import gettext as _


class StandardFormText():
    # Translators: StandardFormText
    username_or_email_label = _('Username/Email Address')
    # Translators: StandardFormText
    create_account = _(""Create Account"")


class ErrorMessages():
    # Translators: When a user tries to upload a blank position form.
    empty_create_position_form = _(
        'Please enter either a pdf file or a url link.')
    # Translators: When a user tries to upload a position form with too much data.
    overfilled_create_position_form = _(
        'Please enter *either* a pdf file or a url link, but not both.')
    # Translators: When a user tries to make a duplicate account. %s is the email (i.e. joesmith@canada.ca).
    user_already_exists = _('Username %s already exists.')
    # Translators: When a user tries to sign on with something that isn't a government email. %s is the email domain (i.e. email.ca).
    invalid_email_domain = _(
        'Invalid e-mail address domain: %s. Canada.ca email required.')
    # Translators: When a user tries to login with an unauthenticated account.
    unconfirmed_email = _('Email address for user not confirmed.')
    # Translators: When a user submits an invalid username and/or password.
    invalid_un_or_pw = _('Invalid username or password.')
    # Translators: When a user tries to upload a faulty pdf/
    incorrect_mime_type = _('Please enter a valid PDF file.')
    # Translators: When a user tries to upload a url other than the jobs.gc.ca posters.
    invalid_url_domain = _('Please enter a jobs.gc.ca url.')
    # Translators: When a user tries to upload a pdf we cant parse.
    incorrect_pdf_file = _(
        'Unable to parse pdf. Please enter a pdf from jobs.gc.ca.')
    # Translators: When a user tries to upload a url.
    url_upload_not_supported_yet = _(
        'URL uploading not currently supported. Please upload a pdf file.')


class InterfaceText():
    # Translators: Sidebar
    view_positions = _('View Positions')
    # Translators: Sidebar
    new_position = _('New Position')
    # Translators: Sidebar
    welcome_user = _(""Welcome, %s"")
    # Translators: Sidebar
    logout = _(""Logout"")


class ImportApplicationsText():
    # Translators: text for importing job applications for a position
    title = _('Upload Completed Applications')
    # Translators: text for importing job applications for a position
    description = _(
        'Click ""Browse"" and select one or more completed applications for position %s, then click ""Upload."" Note that PDFs are the only file format supported.')
    # Translators: text for importing job applications for a position
    upload = _('Upload')
    # Translators: text for importing job applications for a position
    browse = _('Browse')
    # Translators: text for importing job applications for a position
    choose_files = _('Choose one or more files')


class PositionText():
    # Translators: Confirming Position Information
    we_think_this_is_correct = _(
        'We think this is the position. Can you take a look and make sure it is correct?')
    # Translators: Confirming Position Information
    classification = _('Classification')
    # Translators: Confirming Position Inforation
    reference_number = _(""Reference number"")
    # Translators: Confirming Position Information
    selection_process_number = _(""Selection process number"")
    # Translators: Confirming Position Information
    date_closed = _(""Date closed"")
    # Translators: Confirming Position Information
    number_of_positions = _(""No. positions"")
    # Translators: Confirming Position Information
    salary_range = _(""Salary range"")
    # Translators: Confirming Position Information
    open_to = _(""Open to"")
    # Translators: Confirming Position Information
    position_information = _(""Position information"")
    # Translators: Confirming Position Information
    education_and_experience_criteria = _(""Education and Experience Criteria"")
    # Translators: Confirming Position Information
    education = _(""Education"")
    # Translators: Confirming Position Information
    experience = _(""Experience"")
    # Translators: Confirming Position Information
    assets = _(""Assets"")
    # Translators: Confirming Position Information
    save = _(""Save"")
    # Translators: Confirming Position Information
    edit = _(""Edit"")
    # Translators: Confirming Position Information
    back_to_positions = _(""Back to Positions"")
    # Translators: Confirming Position Informatino
    cannot_display_position = _(""Error: Position cannot be displayed"")


class CreatePositionFormText():
    # Translators: CreatePositionForm
    upload_new_position = _(""Upload New Position"")
    # Translators: CreatePositionForm
    please_select_either_filetype = _(
        ""Please select either PDF or link to the jobs.gc.ca posting"")
    # Translators: CreatePositionForm
    pdf = _(""PDF"")
    # Translators: CreatePositionForm
    url = _(""URL"")
    # Translators: CreatePositionForm
    browse_for_pdf = _(""Drag or browse for a PDF file"")
    # Translators: CreatePositionForm
    link_to_job_description = _(""Link to job description"")
    # Translators: CreatePositionForm
    choose_a_file = _(""Choose a file"")
    # Translators: CreatePositionForm
    browse = _(""Browse"")
    # Translators: CreatePositionForm
    submit = _(""Submit"")


class CreateAccountFormText():
    # Translators: CreatePositionForm
    create_account = _(""Create Account"")
    # Translators: CreatePositionForm
    email_address = _(""Email address"")
    # Translators: CreatePositionForm
    choose_password = _(""Choose a password"")
    # Translators: CreatePositionForm
    confirm_password = _(""Re-enter your password"")
    # Translators: CreatePositionForm
    have_an_account_sign_in = _(""Have an account? Sign in"")
    # Translators: CreatePositionForm
    account_created = _(
        ""Account %s created. Please check your e-mail for an activation link."")


class LoginFormText():
    # Translators: LoginForm
    login = _(""Login"")
    # Translators: LoginForm
    password = _(""Password"")
    # Translators: LoginForm
    account_confirmed = _(""Account %s confirmed. Please sign in below."")
    # Translators: LoginForm
    validation_error = _(
        ""Validation token error: invalid link or account is already confirmed."")


class PositionsViewText():
    # Translators: Position View
    sort_by = _(""Sort by"")
    # Translators: Position View
    position = _(""Position"")
    # Translators: Position View
    score = _(""Avg Score"")
    # Translators: Position View
    no_applicants = _(""Applicants"")
    # Translators: Position View
    date_closed = _(""Date closed"")
    # Translators: Position View
    date_uploaded = _(""Date uploaded"")
    # Translators: Position View
    view = _(""View"")
    # Translators: Position View
    download = _(""Download"")
    # Translators: Position View
    delete = _(""Delete"")
    # Translators: Position View
    upload_applications = _(""Upload applications"")
    # Translators: Position View
    no_positions = _(
        'You have not uploaded any positons. Click %s to get started.')
    # Translators: Position View
    confirm_delete_header = _(""Please Confirm Deletion"")
    # Translators: Position View
    confirm_delete = _(""Are you sure you want to delete this position?"")
    # Translators: Position View
    cancel = _(""Cancel"")
    # Translators: Position View
    expand_all = _(""Expand all"")
    # Translators: Position View
    collapse_all = _(""Collapse all"")

# translate command (in web sh): python manage.py makemessages -l pl
# note: Translators: is specific syntax that makes the comment appear in the translation file
# dont omit it
/n/n/nscreendoor/views.py/n/nfrom string import digits
from django.core.mail import send_mail
from django.shortcuts import render, redirect
from django.contrib.auth import get_user_model
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText
from .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm, ImportApplicationsText
from .models import EmailAuthenticateToken, Position
from screendoor.parseposter import parse_upload
from screendoor.redactor import parse_applications


# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for
# the requested page, or raising an exception such as Http404.
# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL


# Index currently redirects to the positions view if logged in
@login_required(login_url='login/', redirect_field_name=None)
def index(request):
    return redirect('positions')
    # Returns main page
    return render(request, 'index.html',
                  {'user': request.user, 'baseVisibleText': InterfaceText})


# Renders account registration form
def register_form(request):
    register_form = ScreenDoorUserCreationForm()
    if request.method == 'POST':
        # create a form instance and populate it with data from the request:
        register_form = ScreenDoorUserCreationForm(request.POST)
        # check whether form data is valid
        if register_form.is_valid():
            # Create user
            user = create_account(request)
            # Send confirmation e-mail
            send_user_email(request, user)
            # Redirects to...
            return render(request, 'registration/register.html',
                          {'register_form': register_form,
                           'account_created': format(CreateAccountFormText.account_created % user)})
    # Returns form page
    return render(request, 'registration/register.html',
                  {'register_form': register_form})


# Creates and returns user object from request data
def create_account(request):
    # Creates account and saves email, password, username to database
    user = get_user_model().objects.create_user(
        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())
    # Extrapolate first and last name from e-mail account (experimental)
    user.first_name = request.POST['email'].split('.')[0].title()
    user.last_name = request.POST['email'].split(
        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})
    # Set user as inactive until e-mail confirmation
    user.email_confirmed = False
    # Save updated user info to database
    user.save()
    return user


# Sends account confirmation e-mail to user
# Currently sends mock e-mail via console
def send_user_email(request, user):
    url = generate_confirmation_url(request, user)
    send_mail(
        'ScreenDoor: Please confirm e-mail address',
        'Please visit the following URL to confirm your account: ' + url,
        'screendoor@screendoor.ca',
        # Address: should be user.email
        [user.email],
        fail_silently=False,
    )


# Creates and returns a working account confirmation URL
def generate_confirmation_url(request, user):
    token = EmailAuthenticateToken()
    token.user = user
    token.create_key()
    token.save()
    # TODO: generate first part of URL programmatically not as hardcoded string
    return ""http://localhost:8000/confirm?key="" + str(token.key)


# Clears any GET data, i.e. account confirmation token string from URL
def clear_get_data(request):
    # Clears any GET data
    request.GET._mutable = True
    request.GET['key'] = None
    request.GET._mutable = False


# Returns true if user authentication token is valid and userhas been validated and saved
def authenticate_user(account_key):
    # If authentication key is valid, activate user and delete authentication token
    if EmailAuthenticateToken.objects.filter(key=account_key).exists():
        token = EmailAuthenticateToken.objects.get(key=account_key)
        user = token.user
        user.email_confirmed = True
        user.save()
        token.delete()
        return True
    return False


# Displays form for user login and calls validation methods
def login_form(request):
    # If user is not logged in, display login form
    if not request.user.is_authenticated:
        form = LoginForm()
        # Has the user hit login button
        if request.method == 'POST':
            clear_get_data(request)
            # Instantiate form object
            form = LoginForm(request.POST)
            # Validates form and persists username data
            if form.is_valid():
                user = form.get_user()
                # Logs in and redirects user
                login(request, user)
                return redirect('home')
        if request.GET.get('key') is not None:
            # Check if authentication key is valid
            if (authenticate_user(request.GET.get('key'))):
                # Display account confirmation message
                return render(request, 'registration/login.html',
                              {'login_form': form,
                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})
            # Display validation error message
            return render(request, 'registration/login.html',
                          {'login_form': form, 'validation_error': LoginFormText.validation_error})
        # Display login page
        return render(request, 'registration/login.html',
                      {'login_form': form})
    # If the user is already logged in, redirect to home
    return redirect('home')


# Logs out user
@login_required(login_url='/login/', redirect_field_name=None)
def logout_view(request):
    logout(request)
    return redirect('login')


# Run parse upload script and return dictionary
def parse_position_return_dictionary(create_position_form):
    # don't commit partial positions with only pdf/url into db
    return parse_upload(create_position_form.save(commit=False))


# Adds position to user data
def save_position_to_user(request):
    request.user.positions.add(Position.objects.get(
        id=request.session['position_id']))


# Displays form allowing users to upload job posting PDF files and URLs
@login_required(login_url='/login/', redirect_field_name=None)
def import_position(request):
    if request.method == 'POST':
        create_position_form = CreatePositionForm(
            request.POST, request.FILES)
        # Is the form data valid
        if create_position_form.is_valid():
            dictionary = parse_position_return_dictionary(create_position_form)
            errors = dictionary.get('errors')
            if errors:
                create_position_form.add_error('pdf', errors)
            # Is the parsed data valid (any errors added)
            if create_position_form.is_valid():
                position = dictionary.get('position')
                # Persist position ID in session for saving and editing
                request.session['position_id'] = position.id
                # Successful render of a position
                return render(request, 'createposition/importposition.html',
                              {'position': position, 'form': create_position_form,
                               'baseVisibleText': InterfaceText,
                               'userVisibleText': PositionText})
            # Display errors
            return render(request, 'createposition/importposition.html',
                          {'form': create_position_form,
                           'baseVisibleText': InterfaceText,
                           'userVisibleText': PositionText})
        # User pressed save button on uploaded and parsed position
        if request.POST.get(""save-position""):
            save_position_to_user(request)
            return redirect('home')
    # Default view for GET request
    create_position_form = CreatePositionForm()
    return render(request, 'createposition/importposition.html', {
        'form': CreatePositionForm, 'baseVisibleText': InterfaceText
    })


# Gets user's persisted positions sort method, or returns default
def get_positions_sort_method(request):
    try:
        return request.session['position_sort']
    except KeyError:
        return '-created'


# Changes positions sort method
def change_positions_sort_method(request, sort_by):
    if request.POST.get(""sort-created""):
        return '-created'
    elif request.POST.get(""sort-closed""):
        return '-date_closed'
    elif request.POST.get(""sort-position""):
        return 'position_title'
    return sort_by


# Data and visible text to render with positions list view
def positions_list_data(request, sort_by):
    return {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'applicationsForm': ImportApplicationsForm, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']
    }
  
  
# View of all positions associated with a user account
@login_required(login_url='/login/', redirect_field_name=None)
def positions(request):
    # Order of positions display
    sort_by = get_positions_sort_method(request)
    if request.method == 'POST':
        sort_by = change_positions_sort_method(request, sort_by)
        # User wants to view position detail
        if request.POST.get(""position""):
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
        # User wants to delete position
        elif request.POST.get(""delete""):
            Position.objects.get(
                id=request.POST.get(""id"")).delete()
        # User wants to upload applications for a position
        elif request.POST.get(""upload-applications""):
            upload_applications(request)
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
    # Persists positions sorting
    request.session['position_sort'] = sort_by
    # Displays list of positions
    return render(request, 'positions.html', positions_list_data(request, sort_by))


# Data and visible text to render with positions
def position_detail_data(request, position):
    return {'baseVisibleText': InterfaceText, 'applicationsForm': ImportApplicationsForm, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position}


# Position detail view
@login_required(login_url='/login/', redirect_field_name=None)
def position(request, position):
    return render(request, 'position.html', position_detail_data(request, position))


def upload_applications(request):
    position = Position.objects.get(
        id=request.POST.get(""id""))
    # form = ImportApplicationsForm(request.POST, request.FILES)
    # applications = import_applications(request)
    # position.applications.add(applications)
    # position.save()


def import_applications(request):
    if request.method == 'POST':
        form = ImportApplicationsForm(request.POST, request.FILES)
        if form.is_valid():
            breakpoint()
            parse_applications()
            # Call application parser logic here##

            return render(request, 'importapplications/applications.html', {
                'form': form})

    form = ImportApplicationsForm()
    return render(request, 'importapplications/applications.html', {
        'form': form})
/n/n/n",0
147,147,4c50beb72795d4518a344c265dca009c926d12bb,"/screendoor/forms.py/n/nfrom django import forms
from django.contrib.auth import get_user_model, authenticate
from django.contrib.auth.forms import UserCreationForm
from django.utils.translation import gettext as _
import magic, mimetypes

from .models import ScreenDoorUser, Position, Applicant
from .uservisibletext import ErrorMessages, CreatePositionFormText, \
    CreateAccountFormText, StandardFormText, LoginFormText

# For creating a new position
class ImportApplicationsForm(forms.ModelForm):
    class Meta:
        model = Applicant
        fields = ('pdf', )

# For creating a new position
class CreatePositionForm(forms.ModelForm):
    text = CreatePositionFormText.upload_new_position
    description = CreatePositionFormText.please_select_either_filetype
    pdf_name = CreatePositionFormText.pdf
    url_name = CreatePositionFormText.url
    pdf_text = CreatePositionFormText.browse_for_pdf
    url_text = CreatePositionFormText.link_to_job_description
    upload_text = CreatePositionFormText.choose_a_file
    browse_text = CreatePositionFormText.browse
    submit_text = CreatePositionFormText.submit

    class Meta:
        model = Position
        fields = ('pdf', 'url_ref')
        widgets = {'url_ref': forms.TextInput(attrs={'disabled': 'disabled'})}


    def clean(self):
        pdf = self.cleaned_data.get('pdf')
        url = self.cleaned_data.get('url_ref')
        # Check for an empty form
        if not pdf and not url:
            msg = forms.ValidationError(ErrorMessages.empty_create_position_form)
            self.add_error('pdf', msg)
            return
        # Check for an overfilled form
        elif pdf and url:
            msg = forms.ValidationError(
                ErrorMessages.overfilled_create_position_form)
            self.add_error('pdf', msg)
            return

        # Verify if the pdf upload has an correct mimetype (i.e. a pdf file)
        if pdf:
            file_type = mimetypes.MimeTypes().types_map_inv[1][
                magic.from_buffer(self.cleaned_data['pdf'].read(), mime=True)
            ][0]
            if not (file_type == '.pdf'):
                msg = forms.ValidationError(
                    ErrorMessages.incorrect_mime_type)
                self.add_error('pdf', msg)

        # Verify if the url matches the job.gc.ca domain
        if url:
            ## Note: Below code is temporary, until url uploading is supported.
            msg = forms.ValidationError(
                     ErrorMessages.url_upload_not_supported_yet)
            self.add_error('url_ref', msg)


            ## Note: Desired code below.
            # if not ""https://emploisfp-psjobs.cfp-psc.gc.ca"" in url:
            #     msg = forms.ValidationError(
            #         ErrorMessages.invalid_url_domain)
            #     self.add_error('url_ref', msg)

        return self.cleaned_data


class ScreenDoorUserCreationForm(UserCreationForm):
    text = CreateAccountFormText.create_account
    email_text = CreateAccountFormText.email_address
    password_text = CreateAccountFormText.choose_password
    password_confirm_text = CreateAccountFormText.confirm_password
    email = forms.EmailField(
        label=StandardFormText.username_or_email_label, max_length=100)
    login_button_text = CreateAccountFormText.have_an_account_sign_in

    class Meta(UserCreationForm):
        model = ScreenDoorUser
        fields = ('email',)

    # Clean and validate fields. Password validation is handled by Django UserCreationForm
    def clean(self):
        email = self.cleaned_data.get('email')
        # Validate e-mail domain (canada.ca only)
        email_domain = email.split('@')[1].lower()
        if email_domain != ""canada.ca"":
            message = forms.ValidationError(
                format(ErrorMessages.invalid_email_domain % email_domain))
            self.add_error('email', message)
        # Validate if e-mail is unique in system
        elif get_user_model().objects.filter(username=email.lower()).exists():
            message = forms.ValidationError(
                format(ErrorMessages.user_already_exists % email))
            self.add_error('email', message)

        return self.cleaned_data


class LoginForm(forms.Form):
    login_text = LoginFormText.login
    create_account_text = StandardFormText.create_account
    email = forms.EmailField(
        label=StandardFormText.username_or_email_label, max_length=100)
    password = forms.CharField(
        label=LoginFormText.password, min_length=8, max_length=42, widget=forms.PasswordInput)

    def clean(self):
        # Entered e-mail is compared as lower to ensure login is not case-sensitive
        email = self.cleaned_data.get('email').lower()
        password = self.cleaned_data.get('password')
        user = authenticate(username=email, password=password)

        # Does user exist in system?
        if user is None:
            message = forms.ValidationError(ErrorMessages.invalid_un_or_pw)
            self.add_error('email', message)
        # Has user confirmed e-mail address
        elif user.email_confirmed is False:
            message = forms.ValidationError(ErrorMessages.unconfirmed_email)
            self.add_error('email', message)

        return self.cleaned_data

    def get_user(self):
        # Entered e-mail is compared as lower to ensure login is not case-sensitive
        email = self.cleaned_data.get('email').lower()
        password = self.cleaned_data.get('password')
        return authenticate(username=email, password=password)
/n/n/n",1
136,136,001ff508081a893d0cf81df1214dbd234606c360,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path)).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nfrom __future__ import unicode_literals

import mimetypes
import unittest
from os import path

from django.conf.urls.static import static
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, filename))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/non_existing_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super(StaticHelperTest, self).setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super(StaticHelperTest, self).tearDown()
        urls.urlpatterns = self._old_views_urlpatterns


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0
137,137,001ff508081a893d0cf81df1214dbd234606c360,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path))
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1
68,68,c5f718ab9523ed3c45530d31f4ff1257904a34b3,"src/mmw/apps/home/urls.py/n/n# -*- coding: utf-8 -*-
from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from django.conf.urls import patterns, url
from apps.home.views import (
    home_page,
    project,
    project_clone,
    project_via_hydroshare,
    projects,
)


urlpatterns = patterns(
    '',
    url(r'^$', home_page, name='home_page'),
    url(r'^draw/?$', home_page, name='home_page'),
    url(r'^account/?$', home_page, name='account'),
    url(r'^projects/$', projects, name='projects'),
    url(r'^project/$', project, name='project'),
    url(r'^project/new/', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/$', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/clone/?$',
        project_clone, name='project_clone'),
    url(r'^project/(?P<proj_id>[0-9]+)/scenario/(?P<scenario_id>[0-9]+)/$',
        project, name='project'),
    url(r'^project/compare/$', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/compare/$', project, name='project'),
    url(r'^project/via/hydroshare/(?P<resource>\w+)/?$',
        project_via_hydroshare, name='project_via_hydroshare'),
    url(r'^analyze$', home_page, name='analyze'),
    url(r'^search$', home_page, name='search'),
    url(r'^error', home_page, name='error'),
    url(r'^sign-up', home_page, name='sign_up'),
)
/n/n/nsrc/mmw/apps/home/views.py/n/n# -*- coding: utf-8 -*-
from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

import json
from urlparse import urljoin
from copy import deepcopy

from django.http import Http404
from django.shortcuts import render_to_response, get_object_or_404, redirect
from django.template import RequestContext
from django.template.context_processors import csrf
from django.conf import settings
from django.contrib.auth.models import User

from rest_framework.authtoken.models import Token

from apps.export.models import HydroShareResource
from apps.modeling.models import Project, Scenario
from apps.user.models import UserProfile
from apps.user.countries import COUNTRY_CHOICES


def home_page(request):
    return render_to_response('home/home.html', get_context(request))


def projects(request):
    if request.user.is_authenticated():
        return render_to_response('home/home.html', get_context(request))
    else:
        return redirect('/')


def project(request, proj_id=None, scenario_id=None):
    """"""
    If proj_id was specified, check that the user owns
    the project or if the project is public.
    If not, return a 404. Otherwise, just load the index
    template and the let the front-end handle the route
    and request the project through the API.

    If proj_id is not specified, then throw a 404 error.
    """"""

    if proj_id:
        project = get_object_or_404(Project, id=proj_id)

        if project.user != request.user and project.is_private:
            raise Http404

        context = get_context(request)
        context.update({'project': True})

        return render_to_response('home/home.html', context)
    else:
        return redirect('/projects/')


def project_clone(request, proj_id=None):
    """"""
    If proj_id was specified, check that the user owns
    the project or if the project is public.
    If not, return a 404. Otherwise, create a new
    project and scenarios from the old one, assign to
    current user, and redirect to it.
    """"""

    if not proj_id or not request.user.is_authenticated():
        raise Http404

    project = get_object_or_404(Project, id=proj_id)

    if project.user != request.user and project.is_private:
        raise Http404

    project.pk = None
    project.user = request.user
    project.save()

    for scenario in Scenario.objects    \
            .filter(project_id=proj_id) \
            .order_by('created_at'):
        scenario.pk = None
        scenario.project = project
        scenario.save()

    return redirect('/project/{0}'.format(project.id))


def project_via_hydroshare(request, resource):
    """"""Redirect to project given a HydroShare resource, if found.""""""

    hsresource = get_object_or_404(HydroShareResource, resource=resource)

    return redirect('/project/{}/'.format(hsresource.project_id))


def get_layer_url(layer):
    """""" For layers that are served off our tile server,
    the URL depends on the environment. Therefore, we
    get it dynamically from the settings file and populate
    the layer config with the endpoint.
    """"""
    tiler_prefix = '//'
    tiler_host = settings.TILER_HOST
    tiler_postfix = '/{z}/{x}/{y}'
    tiler_base = '%s%s' % (tiler_prefix, tiler_host)

    return urljoin(tiler_base, layer['code'] + tiler_postfix)


def get_model_packages():
    for model_package in settings.MODEL_PACKAGES:
        if model_package['name'] in settings.DISABLED_MODEL_PACKAGES:
            model_package['disabled'] = True
    return settings.MODEL_PACKAGES


def create_layer_config_with_urls(layer_type):
    layers = deepcopy(settings.LAYER_GROUPS[layer_type])
    [set_url(layer) for layer in layers
        if 'url' not in layer and 'table_name' in layer]
    return layers


def set_url(layer):
    layer.update({'url': get_layer_url(layer)})


def get_api_token():
    try:
        client_app_user = User.objects.get(
            username=settings.CLIENT_APP_USERNAME)
        token = Token.objects.get(user=client_app_user)
        return token.key
    except User.DoesNotExist, Token.DoesNotExist:
        return None


def get_client_settings(request):
    # BiG-CZ mode applies when either request host contains predefined host, or
    # ?bigcz query parameter is present. This covers staging sites, etc.
    bigcz = settings.BIGCZ_HOST in request.get_host() or 'bigcz' in request.GET
    favicon = 'favicon-bigcz' if bigcz else 'favicon'
    title = 'BiG CZ Data Portal' if bigcz else 'Model My Watershed'
    max_area = settings.BIGCZ_MAX_AREA if bigcz else settings.MMW_MAX_AREA
    EMBED_FLAG = settings.ITSI['embed_flag']
    client_settings = {
        'client_settings': json.dumps({
            EMBED_FLAG: request.session.get(EMBED_FLAG, False),
            'base_layers': create_layer_config_with_urls('basemap'),
            'boundary_layers': create_layer_config_with_urls('boundary'),
            'coverage_layers': create_layer_config_with_urls('coverage'),
            'stream_layers': create_layer_config_with_urls('stream'),
            'nhd_perimeter': settings.NHD_REGION2_PERIMETER,
            'conus_perimeter': settings.CONUS_PERIMETER,
            'draw_tools': settings.DRAW_TOOLS,
            'map_controls': settings.MAP_CONTROLS,
            'vizer_urls': settings.VIZER_URLS,
            'vizer_ignore': settings.VIZER_IGNORE,
            'vizer_names': settings.VIZER_NAMES,
            'model_packages': get_model_packages(),
            'max_area': max_area,
            'mapshed_max_area': settings.GWLFE_CONFIG['MaxAoIArea'],
            'data_catalog_enabled': bigcz,
            'data_catalog_page_size': settings.BIGCZ_CLIENT_PAGE_SIZE,
            'itsi_enabled': not bigcz,
            'title': title,
            'api_token': get_api_token(),
            'choices': {
                'UserProfile': {
                    'user_type': UserProfile.USER_TYPE_CHOICES,
                    'country': COUNTRY_CHOICES,
                }
            },
            'enabled_features': settings.ENABLED_FEATURES,
        }),
        'google_maps_api_key': settings.GOOGLE_MAPS_API_KEY,
        'title': title,
        'favicon': favicon + '.png',
        'favicon2x': favicon + '@2x.png',
    }

    return client_settings


def get_context(request):
    context = RequestContext(request)
    context.update(csrf(request))
    context.update(get_client_settings(request))

    return context
/n/n/n",0
69,69,c5f718ab9523ed3c45530d31f4ff1257904a34b3,"/src/mmw/apps/home/urls.py/n/n# -*- coding: utf-8 -*-
from __future__ import print_function
from __future__ import unicode_literals
from __future__ import division

from django.conf.urls import patterns, url
from apps.home.views import home_page, projects, project, project_clone


urlpatterns = patterns(
    '',
    url(r'^$', home_page, name='home_page'),
    url(r'^draw/?$', home_page, name='home_page'),
    url(r'^account/?$', home_page, name='account'),
    url(r'^projects/$', projects, name='projects'),
    url(r'^project/$', project, name='project'),
    url(r'^project/new/', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/$', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/clone/?$',
        project_clone, name='project_clone'),
    url(r'^project/(?P<proj_id>[0-9]+)/scenario/(?P<scenario_id>[0-9]+)/$',
        project, name='project'),
    url(r'^project/compare/$', project, name='project'),
    url(r'^project/(?P<proj_id>[0-9]+)/compare/$', project, name='project'),
    url(r'^analyze$', home_page, name='analyze'),
    url(r'^search$', home_page, name='search'),
    url(r'^error', home_page, name='error'),
    url(r'^sign-up', home_page, name='sign_up'),
)
/n/n/n",1
96,96,4eced1d18440b57bdfecebd9a30d8932e732e58c,"benchmarkSpider/benchmarkSpider/generator.py/n/nfrom utility import *
import json

def createGetScript(endpoint, params):
    script = 'curl '+start_url+endpoint+'?'
    keys = params.keys()
    values = params.values()
    pair = [keys[i]+'='+values[i] for i in range(len(keys))]
    evil_param = '&'.join(pair)
    script+=evil_param
    return script

def createPostScript(endpoint, params):
    keys = params.keys()
    values = params.values()
    pair = [keys[i]+'='+values[i] for i in range(len(keys))]
    evil_param = '&'.join(pair)
    script = 'curl -d ' + '""'+ evil_param  +'"" '+'-X POST '+start_url+endpoint
    return script

def genDT(endpoint,params,method):
    scope = {
        'class':DT,
        'results':{
            start_url: [
                {
                    'endpoint': endpoint,
                    'params': params,
                    'method': method
                }
            ]
        }
    }    

    script = ''
    if method == 'GET':
        script = createGetScript(endpoint, params)
        
    return scope, script

def genSI(endpoint, params, method):
    scope = {
        'class':SI,
        'results':{
            start_url: [
                {
                    'endpoint': endpoint,
                    'params': params,
                    'method': method
                }
            ]
        }
    }

    if method == 'POST':
        script = createPostScript(endpoint,params)
    
    return scope, script

def genSCI():
    pass

def genSSCI():
    pass

def genCSRF():
    pass

def genOR(endpoint, params, method):
    scope = {
        'class':OR,
        'results':{
            start_url: [
                {
                    'endpoint': endpoint,
                    'params': params,
                    'method': method
                }
            ]
        }
    }    

    script = ''
    if method == 'GET':
        script = createGetScript(endpoint, params)
        
    return scope, script  

render = {
    DT: genDT,
    SI: genSI,
    CSRF: genCSRF,
    OR: genOR,
    SSCI: genSSCI,
    SCI: genSCI
}

class generator(object):
    def __init__(self,category):
        self.scope = {}
        self.category = category
        self.cate_str = '_'.join(category.split(' '))
        self.count = 0
        
    def updateScope(self,scope):
        if(self.count):
            self.scope['results'][start_url]+=scope['results'][start_url]
        else:
            self.scope=scope
        self.count += 1
        
    def saveScript(self,script):
        script_name = 'result/'+self.cate_str+'_attack'+str(self.count)+'.sh'
        with open(script_name, 'w') as f:
            f.write(script)

    def saveScope(self):
        file_name = 'result/'+self.cate_str+'_scope.json'
        with open(file_name,'w+') as f:
            json.dump(self.scope,f)

/n/n/nbenchmarkSpider/benchmarkSpider/open_redirect.py/n/nimport requests
from generator import render, generator
from utility import *

banner(OR)
links = read_links()
redirect_url = 'https://www.google.com'

method = 'GET'
OR_generator = generator(OR)

for link in links:
    url = get_url(link)
    params = get_params(url)
    if params:
	for param in params:
	    temp_params = params.copy()
            temp_params[param] = redirect_url
    	    fullURL = generate_url_with_params(url, temp_params)
            req = requests.get(fullURL)
            if req.content.find('google') != -1 and req.history:
	        endpoint = url.path
                scope, script = render[OR](endpoint,temp_params,method)
                OR_generator.updateScope(scope)
                OR_generator.saveScript(script)                        
                success_message(fullURL)
OR_generator.saveScope()

print '\n'
/n/n/n",0
97,97,4eced1d18440b57bdfecebd9a30d8932e732e58c,"/benchmarkSpider/benchmarkSpider/generator.py/n/nfrom utility import *
import json

def createGetScript(endpoint, params):
    script = 'curl '+start_url+endpoint+'?'
    keys = params.keys()
    values = params.values()
    pair = [keys[i]+'='+values[i] for i in range(len(keys))]
    evil_param = '&'.join(pair)
    script+=evil_param
    return script

def createPostScript(endpoint, params):
    keys = params.keys()
    values = params.values()
    pair = [keys[i]+'='+values[i] for i in range(len(keys))]
    evil_param = '&'.join(pair)
    script = 'curl -d ' + '""'+ evil_param  +'"" '+'-X POST '+start_url+endpoint
    return script

def genDT(endpoint,params,method):
    scope = {
        'class':DT,
        'results':{
            start_url: [
                {
                    'endpoint': endpoint,
                    'params': params,
                    'method': method
                }
            ]
        }
    }    

    script = ''
    if method == 'GET':
        script = createGetScript(endpoint, params)
        
    return scope, script

def genSI(endpoint, params, method):
    scope = {
        'class':SI,
        'results':{
            start_url: [
                {
                    'endpoint': endpoint,
                    'params': params,
                    'method': method
                }
            ]
        }
    }

    if method == 'POST':
        script = createPostScript(endpoint,params)
    
    return scope, script

def genSCI():
    pass

def genSSCI():
    pass

def genCSRF():
    pass

def genOR():
    pass

render = {
    DT: genDT,
    SI: genSI,
    CSRF: genCSRF,
    OR: genOR,
    SSCI: genSSCI,
    SCI: genSCI
}

class generator(object):
    def __init__(self,category):
        self.scope = {}
        self.category = category
        self.cate_str = '_'.join(category.split(' '))
        self.count = 0
        
    def updateScope(self,scope):
        if(self.count):
            self.scope['results'][start_url]+=scope['results'][start_url]
        else:
            self.scope=scope
        self.count += 1
        
    def saveScript(self,script):
        script_name = 'result/'+self.cate_str+'_attack'+str(self.count)+'.sh'
        with open(script_name, 'w') as f:
            f.write(script)

    def saveScope(self):
        file_name = 'result/'+self.cate_str+'_scope.json'
        with open(file_name,'w+') as f:
            json.dump(self.scope,f)

/n/n/n/benchmarkSpider/benchmarkSpider/open_redirect.py/n/nimport requests
from utility import *

banner(OR)
links = read_links()

for link in links:
    url = get_url(link)
    params = get_params(url)
    params['redirect'] = 'https://www.google.com'
    fullURL = generate_url_with_params(url, params)
    req = requests.get(fullURL)
    if req.content.find('google') != -1:
        success_message(fullURL)

print '\n'
/n/n/n",1
56,56,7c610e95d8fccc797e153781ecde013da604b242,"modoboa/core/tests/test_authentication.py/n/n""""""Tests for core application.""""""

from __future__ import unicode_literals

import smtplib

from unittest import skipIf
from mock import patch

from django.core import mail
from django.core.urlresolvers import reverse
from django.test import override_settings

from modoboa.lib.tests import ModoTestCase
from modoboa.lib.tests import NO_SMTP

from .. import factories
from .. import models


class AuthenticationTestCase(ModoTestCase):
    """"""Validate authentication scenarios.""""""

    @classmethod
    def setUpTestData(cls):
        """"""Create test data.""""""
        super(AuthenticationTestCase, cls).setUpTestData()
        cls.account = factories.UserFactory(
            username=""user@test.com"", groups=('SimpleUsers',)
        )

    def test_authentication(self):
        """"""Validate simple case.""""""
        self.client.logout()
        data = {""username"": ""user@test.com"", ""password"": ""toto""}
        response = self.client.post(reverse(""core:login""), data)
        self.assertEqual(response.status_code, 302)
        self.assertTrue(response.url.endswith(reverse(""core:user_index"")))

        response = self.client.post(reverse(""core:logout""), {})
        self.assertEqual(response.status_code, 302)

        data = {""username"": ""admin"", ""password"": ""password""}
        response = self.client.post(reverse(""core:login""), data)
        self.assertEqual(response.status_code, 302)
        self.assertTrue(response.url.endswith(reverse(""core:dashboard"")))

    def test_open_redirect(self):
        """"""Check that open redirect is not allowed.""""""
        self.client.logout()
        data = {""username"": ""admin"", ""password"": ""password""}

        # 1. Check valid redirection
        url = ""{}?next=/admin/"".format(reverse(""core:login""))
        response = self.client.post(url, data)
        self.assertEqual(response.status_code, 302)
        self.assertTrue(response.url.endswith(reverse(""admin:index"")))
        self.client.logout()

        # 2. Check bad redirection
        url = ""{}?next=http://www.evil.com"".format(reverse(""core:login""))
        response = self.client.post(url, data)
        self.assertEqual(response.status_code, 302)
        self.assertTrue(response.url.endswith(reverse(""core:dashboard"")))


class PasswordResetTestCase(ModoTestCase):
    """"""Test password reset service.""""""

    @classmethod
    def setUpTestData(cls):
        """"""Create test data.""""""
        super(PasswordResetTestCase, cls).setUpTestData()
        cls.account_ok = factories.UserFactory(
            username=""user@test.com"", secondary_email=""test@ext.com"",
            groups=('SimpleUsers',)
        )
        cls.account_ko = factories.UserFactory(
            username=""user2@test.com"", groups=('SimpleUsers',)
        )

    def test_reset_password(self):
        """"""Validate simple case.""""""
        self.client.logout()
        url = reverse(""password_reset"")
        data = {""email"": self.account_ok.email}
        response = self.client.post(url, data, follow=True)
        self.assertContains(
            response,
            ""We've emailed you instructions for setting your password"")
        self.assertEqual(len(mail.outbox), 1)

    def test_reset_password_no_secondary_email(self):
        """"""Check email is not sent.""""""
        self.client.logout()
        url = reverse(""password_reset"")
        data = {""email"": self.account_ko.email}
        response = self.client.post(url, data, follow=True)
        self.assertContains(
            response,
            ""We've emailed you instructions for setting your password"")
        self.assertEqual(len(mail.outbox), 0)


@skipIf(NO_SMTP, 'No SMTP server available')
@override_settings(AUTHENTICATION_BACKENDS=(
    ""modoboa.lib.authbackends.SMTPBackend"",
    ""django.contrib.auth.backends.ModelBackend""
))
class SMTPAuthenticationTestCase(ModoTestCase):
    """"""Validate SMTP authentication scenarios.""""""

    def _test_smtp_authentication(self, mock_smtp):
        """"""Common code to check authentication""""""
        self.client.logout()
        username = ""user@unknown.test""
        password = ""toto""
        data = {""username"": username, ""password"": password}
        response = self.client.post(reverse(""core:login""), data)
        self.assertEqual(response.status_code, 302)
        self.assertTrue(response.url.endswith(reverse(""core:user_index"")))
        mock_smtp.return_value.login.assert_called_once_with(
            username, password)
        self.assertTrue(
            models.User.objects.filter(username=username).exists())

    @patch(""smtplib.SMTP"")
    def test_smtp_authentication(self, mock_smtp):
        """"""Check simple SMTP authentication.""""""
        self._test_smtp_authentication(mock_smtp)

    @patch(""smtplib.SMTP_SSL"")
    @override_settings(AUTH_SMTP_SECURED_MODE=""ssl"")
    def test_smtp_authentication_over_ssl(self, mock_smtp):
        """"""Check SMTP authentication over SSL.""""""
        self._test_smtp_authentication(mock_smtp)

    @patch(""smtplib.SMTP"")
    @override_settings(AUTH_SMTP_SECURED_MODE=""starttls"")
    def test_smtp_authentication_over_starttls(self, mock_smtp):
        """"""Check SMTP authentication over STARTTLS.""""""
        self._test_smtp_authentication(mock_smtp)

    @patch(""smtplib.SMTP"")
    def test_smtp_authentication_failure(self, mock_smtp):
        """"""Check SMTP authentication failure.""""""
        instance = mock_smtp.return_value
        instance.login.side_effect = smtplib.SMTPAuthenticationError(
            450, ""User not found"")
        self.client.logout()
        username = ""user@unknown.test""
        password = ""toto""
        data = {""username"": username, ""password"": password}
        response = self.client.post(reverse(""core:login""), data)
        self.assertEqual(response.status_code, 401)
        mock_smtp.return_value.login.assert_called_once_with(
            username, password)
        self.assertFalse(
            models.User.objects.filter(username=username).exists())
/n/n/nmodoboa/core/views/base.py/n/n""""""Base core views.""""""

from __future__ import unicode_literals

from django.core.urlresolvers import reverse
from django.utils.http import is_safe_url
from django.views import generic

from django.contrib.auth import mixins as auth_mixins

from ..extensions import exts_pool


def find_nextlocation(request, user):
    """"""Find next location for given user after login.""""""
    if not user.last_login:
        # Redirect to profile on first login
        return reverse(""core:user_index"")
    nextlocation = request.POST.get(""next"", request.GET.get(""next""))
    condition = (
        nextlocation is not None and nextlocation != """" and
        is_safe_url(nextlocation, host=request.get_host())
    )
    if condition:
        return nextlocation
    if request.user.role == ""SimpleUsers"":
        topredir = request.localconfig.parameters.get_value(
            ""default_top_redirection"")
        if topredir != ""user"":
            infos = exts_pool.get_extension_infos(topredir)
            nextlocation = infos[""topredirection_url""]
        else:
            nextlocation = reverse(""core:user_index"")
    else:
        nextlocation = reverse(""core:dashboard"")
    return nextlocation


class RootDispatchView(auth_mixins.LoginRequiredMixin, generic.RedirectView):
    """"""Handle root dispatching based on role.""""""

    def get_redirect_url(self):
        """"""Find proper next hop.""""""
        return find_nextlocation(self.request, self.request.user)
/n/n/n",0
57,57,7c610e95d8fccc797e153781ecde013da604b242,"/modoboa/core/views/base.py/n/n""""""Base core views.""""""

from __future__ import unicode_literals

from django.core.urlresolvers import reverse
from django.views import generic

from django.contrib.auth import mixins as auth_mixins

from ..extensions import exts_pool


def find_nextlocation(request, user):
    """"""Find next location for given user after login.""""""
    if not user.last_login:
        # Redirect to profile on first login
        return reverse(""core:user_index"")
    nextlocation = request.POST.get(""next"", None)
    if nextlocation is None or nextlocation == ""None"":
        if request.user.role == ""SimpleUsers"":
            topredir = request.localconfig.parameters.get_value(
                ""default_top_redirection"")
            if topredir != ""user"":
                infos = exts_pool.get_extension_infos(topredir)
                nextlocation = infos[""topredirection_url""]
            else:
                nextlocation = reverse(""core:user_index"")
        else:
            nextlocation = reverse(""core:dashboard"")
    return nextlocation


class RootDispatchView(auth_mixins.LoginRequiredMixin, generic.RedirectView):
    """"""Handle root dispatching based on role.""""""

    def get_redirect_url(self):
        """"""Find proper next hop.""""""
        return find_nextlocation(self.request, self.request.user)
/n/n/n",1
62,62,f668ed16923a278b50c9dc10d543badf8d462c91,"benwaonline/__init__.py/n/nimport os
from flask import Flask, g, url_for, request, flash, redirect
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from flask_admin import Admin, helpers
from flask_security import Security
from flask_login import LoginManager
from flask_uploads import patch_request_class, configure_uploads
from werkzeug.utils import find_modules, import_string

from config import app_config

from benwaonline.back import back
from benwaonline.database import db
from benwaonline.oauth import oauth
from benwaonline.admin import setup_adminviews
from benwaonline.models import user_datastore, User
from benwaonline.gallery import gallery, images
from benwaonline.user import user
from benwaonline.auth import auth

FILE_SIZE_LIMIT = 10 * 1024 * 1024

security = Security()
login_manager = LoginManager()

def create_app(config=None):
    app = Flask(__name__)
    app.config.from_object(app_config[config])

    # app.config.update(config or {})
    app.config.from_envvar('BENWAONLINE_SETTINGS', silent=True)
    app.config.from_object('secrets')
    app.config['SECURITY_LOGIN_USER_TEMPLATE'] = 'login.html'
    db.init_app(app)
    migrate = Migrate(app, db)
    oauth.init_app(app)

    login_manager.init_app(app)
    login_manager.login_view = ""auth.oauthorize""

    @login_manager.user_loader
    def load_user(user_id):
        return User.get(user_id)

    security_ctx = security.init_app(app, user_datastore)
    @security_ctx.context_processor
    def security_context_processor():
        return dict(
            admin_base_template=admin.base_template,
            admin_view=admin.index_view,
            h=helpers,
            get_url=url_for
        )

    admin = Admin(app, name='benwaonline', template_mode='bootstrap3')
    setup_adminviews(admin, db)

    register_blueprints(app)
    register_cli(app)
    register_teardowns(app)

    app.register_blueprint(gallery)
    app.register_blueprint(auth)
    app.register_blueprint(user)

    configure_uploads(app, (images,))
    patch_request_class(app, FILE_SIZE_LIMIT)

    return app

def register_blueprints(app):
    """"""
    Register all blueprint modules
    Reference: Armin Ronacher, ""Flask for Fun and for Profit"" PyBay 2016.
    """"""
    for name in find_modules('benwaonline.blueprints'):
        mod = import_string(name)
        if hasattr(mod, 'bp'):
            app.register_blueprint(mod.bp)

    return None

def register_cli(app):
    @app.cli.command('initdb')
    def initdb_command():
        """"""Creates the database tables.""""""
        init_db()
        print('Initialized the database.')

def init_db():
    import benwaonline.models
    db.create_all()

def register_teardowns(app):
    @app.teardown_appcontext
    def close_db(error):
        """"""Closes the database again at the end of the request.""""""
        if hasattr(g, 'sqlite_db'):
            g.sqlite_db.close()
/n/n/nbenwaonline/auth/views.py/n/nfrom urllib.parse import urlparse, urljoin
from flask import request, session, g, redirect, url_for, \
     render_template, flash, abort

from flask_login import login_user, logout_user, current_user
from flask_security import login_required

from benwaonline.back import back
from benwaonline.database import db
from benwaonline.oauth import twitter
from benwaonline.models import user_datastore, User, Role
from benwaonline.auth import auth
from benwaonline.auth.forms import RegistrationForm

@auth.before_request
def before_request():
    g.user = current_user

@auth.route('/logout')
@login_required
def logout():
    logout_user()
    return back.redirect()

# @auth.route('/login')
# def redirect_login():
#     return redirect(url_for('auth.oauthorize'))

@auth.route('/login/auth', methods=[""GET"", ""POST""])
def oauthorize():
    if g.user.is_authenticated:
        return back.redirect()

    if request.method == 'POST':
        callback_url = url_for('auth.oauthorize_callback', next=request.args.get('next'))
        return twitter.authorize(callback=callback_url)

    return render_template('login.html', url=back.url())

@auth.route('/oauthorize')
def oauthorize_callback():
    resp = twitter.authorized_response()
    if not resp:
        flash(u'You denied the request to sign in.')
        return back.redirect()

    user_id = resp['user_id']
    user = User.query.filter_by(user_id=user_id).first()

    if user:
        login_user(user)
        flash('You were signed in as %s' % user.username)
        return back.redirect()

    else:
        session['user_id'] = user_id
        session['token'] = resp['oauth_token']
        session['secret'] = resp['oauth_token_secret']
        return redirect(url_for('auth.signup'))

@auth.route('/signup', methods=['GET', 'POST'])
def signup():
    if g.user.is_authenticated:
        return back.redirect()

    form = RegistrationForm()
    if request.method == 'POST' and form.validate_on_submit():
        username = ''.join([form.adj.data, form.benwa.data, form.pos.data])
        name_exists = User.query.filter_by(username=username).all()
        if name_exists:
            flash('Username %s already in use' % username)
            return redirect(url_for('auth.signup'))

        user = user_datastore.create_user(user_id=session['user_id'], username=username)
        user_datastore.add_role_to_user(user, Role.query.filter_by(name='member').first())
        user.oauth_token = session.pop('token')
        user.oauth_secret = session.pop('secret')
        db.session.commit()

        login_user(user)

        flash('You were signed in as %s' % user.username)
        return back.redirect()

    return render_template('signup.html', form=form)/n/n/nbenwaonline/back.py/n/nimport os
import functools
from config import app_config
from flask import session, redirect, url_for, request

class back(object):
    """"""To be used in views.

    Use `anchor` decorator to mark a view as a possible point of return.

    `url()` is the last saved url.

    Use `redirect` to return to the last return point visited.
    """"""
    config_name = os.getenv('FLASK_CONFIG')
    cfg = app_config[config_name]
    cookie = cfg.REDIRECT_BACK_COOKIE if hasattr(cfg, 'REDIRECT_BACK_COOKIE') else 'back'
    default_view = cfg.REDIRECT_BACK_DEFAULT if hasattr(cfg, 'REDIRECT_BACK_DEFAULT') else 'index'

    @staticmethod
    def anchor(func, cookie=cookie):
        @functools.wraps(func)
        def result(*args, **kwargs):
            session[cookie] = request.url
            return func(*args, **kwargs)
        return result

    @staticmethod
    def url(default=default_view, cookie=cookie):
        print(url_for(default))
        return session.get(cookie, url_for(default))

    @staticmethod
    def redirect(default=default_view, cookie=cookie):
        return redirect(back.url(default, cookie))

back = back()
/n/n/nbenwaonline/blueprints/benwaonline.py/n/nfrom flask import Blueprint, render_template
from benwaonline.back import back

bp = Blueprint('benwaonline', __name__)

@bp.route('/')
@back.anchor
def under_construction():
    return render_template('index.html')
/n/n/nbenwaonline/gallery/views.py/n/nfrom os.path import join
from datetime import datetime
from flask import redirect, url_for, render_template, flash, g, current_app
from werkzeug.utils import secure_filename
from flask_security import login_required, current_user

from benwaonline.back import back
from benwaonline.database import db
from benwaonline.models import Post, Tag, Comment, Preview, Image

from benwaonline.gallery import gallery
from benwaonline.gallery.forms import CommentForm, PostForm

@gallery.before_request
def before_request():
    g.user = current_user

@gallery.route('/gallery/')
@gallery.route('/gallery/<string:tags>/')
@back.anchor
def show_posts(tags='all'):
    if tags == 'all':
        posts = Post.query.all()
    else:
        split = tags.split(' ')
        posts = []
        for s in split:
            results = Post.query.filter(Post.tags.any(name=s))
            posts.extend(results)

    tags = Tag.query.all()

    return render_template('gallery.html', posts=posts, tags=tags)

# @gallery.route('/gallery/benwa/')
# @back.anchor
# def show_post_redirect():
#     return redirect(url_for('gallery.show_posts'))

@gallery.route('/gallery/benwa/<int:post_id>')
def show_post(post_id):
    post = Post.query.paginate(post_id, 1, False)
    if post.items:
        return render_template('show.html', post=post, form=CommentForm())

    flash('That Benwa doesn\'t exist yet')

    return back.redirect()

# Will need to add Role/Permissions to this later
@gallery.route('/gallery/benwa/add', methods=['GET', 'POST'])
@login_required
@back.anchor
def add_post():
    form = PostForm()
    if form.validate_on_submit():
        f = form.image.data
        fname = secure_filename(f.filename)
        f.save(join(
            current_app.config['UPLOADED_BENWA_DIR'], fname
        ))

        fpath = '/'.join(['thumbs', fname])
        created = datetime.utcnow()
        preview = Preview(filepath=fpath, created=created)
        db.session.add(preview)

        fpath = '/'.join(['imgs', fname])
        image = Image(filepath=fpath, created=created, preview=preview)
        db.session.add(image)

        # 'benwa' is forever the first tag in the database
        tags = [Tag.query.get(1)]
        added_tags = [get_or_create_tag(db.session, tag)[0] for tag in form.tags.data if tag]
        tags.extend(added_tags)

        post = Post(title=fname, created=datetime.utcnow(), image=image, tags=tags)
        db.session.add(post)

        current_user.posts.append(post)
        db.session.commit()

        return redirect(url_for('gallery.show_post', post_id=post.id))

    flash('There was an issue with adding the benwa')
    return render_template('image_upload.html', form=form)

def get_or_create_tag(session, tagname):
    instance = Tag.query.filter_by(name=tagname).first()
    if instance:
        return instance, False
    else:
        created = datetime.utcnow()
        instance = Tag(name=tagname, created=created)
        session.add(instance)

    return instance, True

@gallery.route('/gallery/benwa/<int:post_id>/comment/add', methods=['POST'])
@login_required
def add_comment(post_id):
    form = CommentForm()
    if form.validate_on_submit():
        post = Post.query.get(post_id)
        comment = Comment(content=form.content.data,\
                created=datetime.utcnow(), user=current_user, post=post)
        db.session.add(comment)
        db.session.commit()

    return redirect(url_for('gallery.show_post', post_id=post_id))

@gallery.route('/gallery/benwa/<int:post_id>/comment/delete/<int:comment_id>', methods=['GET',  'POST'])
@login_required
def delete_comment(post_id, comment_id):
    comment = Comment.query.get_or_404(comment_id)

    if current_user.has_role('admin') or comment.owner(current_user):
        db.session.delete(comment)
        db.session.commit()
    else:
        flash('you can\'t delete this comment')

    return redirect(url_for('gallery.show_post', post_id=post_id))
/n/n/nconfig.py/n/nimport os

BASE = os.path.abspath(os.path.dirname(__file__))

class Config(object):
    BASE_DIR = BASE
    SQLALCHEMY_MIGRATE_REPO = os.path.join(BASE_DIR, 'db_repository')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    UPLOADED_IMAGES_DEST = '/static/'
    UPLOADED_BENWA_DIR = os.path.join(BASE, 'benwaonline', 'static', 'tempbenwas')
    REDIRECT_BACK_DEFAULT = 'gallery.show_posts'

class DevConfig(Config):
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(BASE, 'db', 'benwaonline.db')
    DEBUG = True
    SECRET_KEY = 'not-so-secret'

class TestConfig(Config):
    SQLALCHEMY_DATABASE_URI = 'sqlite://'
    TESTING = True
    TWITTER_CONSUMER_KEY = 'consume'
    TWITTER_CONSUMER_SECRET = 'secret'
    WTF_CSRF_ENABLED = False
    SECRET_KEY = 'not-so-secret'

class ProdConfig(Config):
    DEBUG = False

app_config = {
    'dev': DevConfig,
    'test': TestConfig,
    'prod': ProdConfig
}
/n/n/n",0
63,63,f668ed16923a278b50c9dc10d543badf8d462c91,"/benwaonline/__init__.py/n/nimport os
from flask import Flask, g, url_for
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from flask_admin import Admin, helpers
from flask_security import Security
from flask_login import LoginManager
from flask_uploads import patch_request_class, configure_uploads
from werkzeug.utils import find_modules, import_string

from config import app_config

from benwaonline.database import db
from benwaonline.oauth import oauth
from benwaonline.admin import setup_adminviews
from benwaonline.models import user_datastore, User
from benwaonline.gallery import gallery
from benwaonline.gallery.forms import images
from benwaonline.user import user
from benwaonline.auth import auth

FILE_SIZE_LIMIT = 10 * 1024 * 1024

security = Security()
login_manager = LoginManager()

def create_app(config=None):
    app = Flask(__name__)
    app.config.from_object(app_config[config])

    # app.config.update(config or {})
    app.config.from_envvar('BENWAONLINE_SETTINGS', silent=True)
    app.config.from_object('secrets')

    db.init_app(app)
    migrate = Migrate(app, db)
    oauth.init_app(app)

    login_manager.init_app(app)
    @login_manager.user_loader
    def load_user(user_id):
        return User.get(user_id)

    security_ctx = security.init_app(app, user_datastore)
    @security_ctx.context_processor
    def security_context_processor():
        return dict(
            admin_base_template=admin.base_template,
            admin_view=admin.index_view,
            h=helpers,
            get_url=url_for
        )

    admin = Admin(app, name='benwaonline', template_mode='bootstrap3')
    setup_adminviews(admin, db)

    register_blueprints(app)
    register_cli(app)
    register_teardowns(app)

    app.register_blueprint(gallery)
    app.register_blueprint(auth)
    app.register_blueprint(user)

    configure_uploads(app, (images,))
    patch_request_class(app, FILE_SIZE_LIMIT)

    return app

def register_blueprints(app):
    """"""
    Register all blueprint modules
    Reference: Armin Ronacher, ""Flask for Fun and for Profit"" PyBay 2016.
    """"""
    for name in find_modules('benwaonline.blueprints'):
        mod = import_string(name)
        if hasattr(mod, 'bp'):
            app.register_blueprint(mod.bp)

    return None

def register_cli(app):
    @app.cli.command('initdb')
    def initdb_command():
        """"""Creates the database tables.""""""
        init_db()
        print('Initialized the database.')

def init_db():
    import benwaonline.models
    db.create_all()

def register_teardowns(app):
    @app.teardown_appcontext
    def close_db(error):
        """"""Closes the database again at the end of the request.""""""
        if hasattr(g, 'sqlite_db'):
            g.sqlite_db.close()

/n/n/n/benwaonline/auth/views.py/n/nfrom flask import Blueprint, request, session, g, redirect, url_for, \
     render_template, flash

from flask_login import login_user, logout_user, current_user
from flask_security import login_required

from benwaonline.database import db
from benwaonline.oauth import twitter
from benwaonline.models import user_datastore, User
from benwaonline.auth import auth
from benwaonline.auth.forms import RegistrationForm

@auth.before_request
def before_request():
    g.user = current_user

@auth.route('/test')
def test():
    if current_user.is_authenticated:
        return str(current_user.username) + str(current_user.user_id)

    return ""not logged in "" + str(current_user.is_authenticated)

@auth.route('/logout')
@login_required
def logout():
    logout_user()
    return redirect(url_for('auth.test'))

@auth.route('/login/auth')
def oauthorize():
    if g.user.is_authenticated:
        return redirect(url_for('gallery.show_posts'))
    callback_url = url_for('auth.oauthorize_callback', next=request.args.get('next'))
    return twitter.authorize(callback=callback_url or request.referrer or None)

@auth.route('/oauthorize')
def oauthorize_callback():
    resp = twitter.authorized_response()
    if not resp:
        flash(u'You denied the request to sign in.')
        return redirect(url_for('gallery.show_posts'))

    user_id = resp['user_id']
    user = User.query.filter_by(user_id=user_id).first()

    if user:
        login_user(user)
        next = request.args.get('next')
        flash('You were signed in as %s' % user.username)
        return redirect(url_for('auth.test'))

    else:
        session['user_id'] = user_id
        session['token'] = resp['oauth_token']
        session['secret'] = resp['oauth_token_secret']
        return redirect(url_for('auth.signup'))

@auth.route('/signup', methods=['GET', 'POST'])
def signup():
    form = RegistrationForm()
    if request.method == 'POST' and form.validate_on_submit():
        username = ''.join([form.adj.data, form.benwa.data, form.pos.data])
        name_exists = User.query.filter(User.username == username).all()
        if name_exists:
            flash('Username %s already in use' % username)
            return redirect(url_for('auth.signup'))

        user = user_datastore.create_user(user_id=session['user_id'], username=username)
        user.oauth_token = session.pop('token')
        user.oauth_secret = session.pop('secret')
        db.session.commit()

        login_user(user)
        next = request.args.get('next')

        flash('You were signed in as %s' % user.username)
        return redirect(url_for('auth.test'))

    flash('There was an issue with sign up, please try again')
    return render_template('signup.html', form=form)/n/n/n/benwaonline/blueprints/benwaonline.py/n/nfrom flask import Blueprint, request, session, g, redirect, url_for, \
     render_template, flash, current_app

bp = Blueprint('benwaonline', __name__)

@bp.route('/')
def under_construction():
    # return redirect(url_for('gallery.show_posts'))
    return redirect(url_for('auth.test'))
/n/n/n/benwaonline/gallery/views.py/n/nfrom os.path import join
from datetime import datetime
from flask import request, redirect, url_for, render_template, flash, g, current_app
from werkzeug.utils import secure_filename
from flask_security import login_required, current_user

from benwaonline.database import db
from benwaonline.models import Post, Tag, Comment, Preview, Image
from benwaonline.gallery import gallery
from benwaonline.gallery.forms import CommentForm, PostForm

@gallery.before_request
def before_request():
    g.user = current_user

@gallery.route('/gallery/')
@gallery.route('/gallery/<string:tags>/')
def show_posts(tags='all'):
    if tags == 'all':
        posts = Post.query.all()
    else:
        split = tags.split(' ')
        posts = []
        for s in split:
            results = Post.query.filter(Post.tags.any(name=s))
            posts.extend(results)

    tags = Tag.query.all()

    return render_template('gallery.html', posts=posts, tags=tags)

@gallery.route('/gallery/benwa/')
def show_post_redirect():
    return redirect(url_for('gallery.show_posts'))

@gallery.route('/gallery/benwa/<int:post_id>')
def show_post(post_id):
    post = Post.query.paginate(post_id, 1, False)
    # Look at docs for get_or_404 or w.e
    if post.items:
        return render_template('show.html', post=post, form=CommentForm())

    flash('That Benwa doesn\'t exist yet')
    return redirect(url_for('gallery.show_posts'))

# Will need to add Role/Permissions to this later
@gallery.route('/gallery/benwa/add', methods=['GET', 'POST'])
@login_required
def add_post():
    form = PostForm()
    if form.validate_on_submit():
        f = form.image.data
        fname = secure_filename(f.filename)
        f.save(join(
            current_app.static_folder, current_app.config['STATIC_BENWA_DIR'], fname
        ))
        fpath = '/'.join(['thumbs', fname])
        created = datetime.utcnow()
        preview = Preview(filepath=fpath, created=created)
        db.session.add(preview)

        fpath = '/'.join(['imgs', fname])
        image = Image(filepath=fpath, created=created, preview=preview)
        db.session.add(image)

        # 'benwa' is the forever the first tag in the database
        tags = [Tag.query.get(1)]
        added_tags = [get_or_create_tag(db.session, tag)[0] for tag in form.tags.data if tag]
        tags.extend(added_tags)

        post = Post(title=fname, created=datetime.utcnow(), image=image, tags=tags)
        db.session.add(post)

        current_user.posts.append(post)
        db.session.commit()

        return redirect(url_for('gallery.show_post', post_id=post.id))

    flash('There was an issue with adding the benwa')
    return render_template('image_upload.html', form=form)

def get_or_create_tag(session, tagname):
    instance = Tag.query.filter_by(name=tagname).first()
    if instance:
        return instance, False
    else:
        created = datetime.utcnow()
        instance = Tag(name=tagname, created=created)
        session.add(instance)

    return instance, True

@gallery.route('/gallery/benwa/<int:post_id>/comment/add', methods=['POST'])
@login_required
def add_comment(post_id):
    form = CommentForm()
    if form.validate_on_submit():
        post = Post.query.get(post_id)
        comment = Comment(content=form.content.data,\
                created=datetime.utcnow(), user=current_user, post=post)
        db.session.add(comment)
        db.session.commit()

    return redirect(url_for('gallery.show_post', post_id=post_id))

@gallery.route('/gallery/benwa/<int:post_id>/comment/delete/<int:comment_id>', methods=['GET',  'POST'])
@login_required
# @roles_accepted('admin', 'member')
def delete_comment(post_id, comment_id):
    comment = Comment.query.get_or_404(comment_id)

    if current_user.has_role('admin') or comment.owner(current_user):
        db.session.delete(comment)
        db.session.commit()
    else:
        flash('you can\'t delete this comment')

    return redirect(url_for('gallery.show_post', post_id=post_id))
/n/n/n/config.py/n/nimport os

BASE = os.path.abspath(os.path.dirname(__file__))

class Config(object):
    BASE_DIR = BASE
    SQLALCHEMY_MIGRATE_REPO = os.path.join(BASE_DIR, 'db_repository')
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    UPLOADED_IMAGES_DEST = '/static/'
    UPLOADED_BENWA_DIR = os.path.join(BASE, 'static', 'tempbenwas')

class DevConfig(Config):
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(BASE, 'db', 'benwaonline.db')
    DEBUG = True
    SECRET_KEY = 'not-so-secret'

class TestConfig(Config):
    SQLALCHEMY_DATABASE_URI = 'sqlite://'
    TESTING = True
    TWITTER_CONSUMER_KEY = 'consume'
    TWITTER_CONSUMER_SECRET = 'secret'
    WTF_CSRF_ENABLED = False
    SECRET_KEY = 'not-so-secret'

class ProdConfig(Config):
    DEBUG = False

app_config = {
    'dev': DevConfig,
    'test': TestConfig,
    'prod': ProdConfig
}
/n/n/n",1
28,28,f2ef8b4ffa445be00f6602e446e60916f4ee4d30,"flask_oidc/__init__.py/n/nfrom functools import wraps
import os
import json
from base64 import b64encode
import time as time_module
from copy import copy
import logging

from six.moves.urllib.parse import urlencode
from flask import request, session, redirect, url_for, g
from oauth2client.client import flow_from_clientsecrets, OAuth2WebServerFlow,\
    AccessTokenRefreshError
import httplib2
from itsdangerous import JSONWebSignatureSerializer, BadSignature, \
    TimedJSONWebSignatureSerializer, SignatureExpired

__all__ = ['OpenIDConnect', 'MemoryCredentials']

logger = logging.getLogger(__name__)


class MemoryCredentials(dict):
    """"""
    Non-persistent local credentials store.
    Use this if you only have one app server, and don't mind making everyone
    log in again after a restart.
    """"""
    pass


class OpenIDConnect(object):
    """"""
    @see: https://developers.google.com/api-client-library/python/start/get_started
    @see: https://developers.google.com/api-client-library/python/samples/authorized_api_web_server_calendar.py
    """"""
    def __init__(self, app=None, credentials_store=None, http=None, time=None,
                 urandom=None):
        self.credentials_store = credentials_store\
            if credentials_store is not None\
            else MemoryCredentials()

        # stuff that we might want to override for tests
        self.http = http if http is not None else httplib2.Http()
        self.time = time if time is not None else time_module.time
        self.urandom = urandom if urandom is not None else os.urandom

        # get stuff from the app's config, which may override stuff set above
        if app is not None:
            self.init_app(app)

    def init_app(self, app):
        """"""
        Do setup that requires a Flask app.
        """"""
        self.app = app

        # Set some default configuration options
        app.config.setdefault('OIDC_SCOPES', ['openid', 'email'])
        app.config.setdefault('OIDC_GOOGLE_APPS_DOMAIN', None)
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_NAME', 'oidc_id_token')
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_TTL', 7 * 86400)  # 7 days
        # should ONLY be turned off for local debugging
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_SECURE', True)
        app.config.setdefault('OIDC_VALID_ISSUERS',
                              ['accounts.google.com',
                               'https://accounts.google.com'])
        app.config.setdefault('OIDC_CLOCK_SKEW', 60)  # 1 minute
        app.config.setdefault('OIDC_REQUIRE_VERIFIED_EMAIL', True)

        # register callback route and cookie-setting decorator
        app.route('/oidc_callback')(self.oidc_callback)
        app.before_request(self.before_request)
        app.after_request(self.after_request)

        # load client_secrets.json
        self.flow = flow_from_clientsecrets(
            app.config['OIDC_CLIENT_SECRETS'],
            scope=app.config['OIDC_SCOPES'])
        assert isinstance(self.flow, OAuth2WebServerFlow)

        # create signers using the Flask secret key
        self.destination_serializer = JSONWebSignatureSerializer(
            app.config['SECRET_KEY'])
        self.cookie_serializer = TimedJSONWebSignatureSerializer(
            app.config['SECRET_KEY'])

        try:
            self.credentials_store = app.config['OIDC_CREDENTIALS_STORE']
        except KeyError:
            pass

    def get_cookie_id_token(self):
        try:
            id_token_cookie = request.cookies[self.app.config['OIDC_ID_TOKEN_COOKIE_NAME']]
            return self.cookie_serializer.loads(id_token_cookie)
        except (KeyError, SignatureExpired):
            logger.debug(""Missing or invalid ID token cookie"", exc_info=True)
            return None

    def set_cookie_id_token(self, id_token):
        """"""
        Cooperates with @after_request to set a new ID token cookie.
        """"""
        g.oidc_id_token = id_token
        g.oidc_id_token_dirty = True

    def after_request(self, response):
        """"""
        Set a new ID token cookie if the ID token has changed.
        """"""
        if getattr(g, 'oidc_id_token_dirty', False):
            signed_id_token = self.cookie_serializer.dumps(g.oidc_id_token)
            response.set_cookie(
                self.app.config['OIDC_ID_TOKEN_COOKIE_NAME'], signed_id_token,
                secure=self.app.config['OIDC_ID_TOKEN_COOKIE_SECURE'],
                httponly=True,
                max_age=self.app.config['OIDC_ID_TOKEN_COOKIE_TTL'])
        return response

    def before_request(self):
        g.oidc_id_token = None
        self.authenticate_or_redirect()

    def authenticate_or_redirect(self):
        """"""
        Helper function suitable for @app.before_request and @check (below).
        Sets g.oidc_id_token to the ID token if the user has successfully
        authenticated, else returns a redirect object so they can go try
        to authenticate.
        :return: A redirect, or None if the user is authenticated.
        """"""
        # the auth callback and error pages don't need user to be authenticated
        if request.endpoint in frozenset(['oidc_callback', 'oidc_error']):
            return None

        # retrieve signed ID token cookie
        id_token = self.get_cookie_id_token()
        if id_token is None:
            return self.redirect_to_auth_server(request.url)

        # ID token expired
        # when Google is the IdP, this happens after one hour
        if self.time() >= id_token['exp']:
            # get credentials from store
            try:
                credentials = self.credentials_store[id_token['sub']]
            except KeyError:
                logger.debug(""Expired ID token, credentials missing"",
                             exc_info=True)
                return self.redirect_to_auth_server(request.url)

            # refresh and store credentials
            try:
                credentials.refresh(self.http)
                id_token = credentials.id_token
                self.credentials_store[id_token['sub']] = credentials
                self.set_cookie_id_token(id_token)
            except AccessTokenRefreshError:
                # Can't refresh. Wipe credentials and redirect user to IdP
                # for re-authentication.
                logger.debug(""Expired ID token, can't refresh credentials"",
                             exc_info=True)
                del self.credentials_store[id_token['sub']]
                return self.redirect_to_auth_server(request.url)

        # make ID token available to views
        g.oidc_id_token = id_token

        return None

    def require_login(self, view_func):
        """"""
        Use this to decorate view functions if only some of your app's views
        require authentication.
        """"""
        @wraps(view_func)
        def decorated(*args, **kwargs):
            if g.oidc_id_token is None:
                return self.redirect_to_auth_server(request.url)
            return view_func(*args, **kwargs)
        return decorated
    # Backwards compatibility
    check = require_login

    def flow_for_request(self):
        """"""
        Build a flow with the correct absolute callback URL for this request.
        :return:
        """"""
        flow = copy(self.flow)
        flow.redirect_uri = url_for('oidc_callback', _external=True)
        return flow

    def redirect_to_auth_server(self, destination):
        """"""
        Set a CSRF token in the session, and redirect to the IdP.
        :param destination: the page that the user was going to,
                            before we noticed they weren't logged in
        :return: a redirect response
        """"""
        destination = self.destination_serializer.dumps(destination)
        csrf_token = b64encode(self.urandom(24)).decode('utf-8')
        session['oidc_csrf_token'] = csrf_token
        state = {
            'csrf_token': csrf_token,
            'destination': destination,
        }
        extra_params = {
            'state': json.dumps(state),
        }
        flow = self.flow_for_request()
        auth_url = '{url}&{extra_params}'.format(
            url=flow.step1_get_authorize_url(),
            extra_params=urlencode(extra_params))
        # if the user has an ID token, it's invalid, or we wouldn't be here
        self.set_cookie_id_token(None)
        return redirect(auth_url)

    def is_id_token_valid(self, id_token):
        """"""
        Check if `id_token` is a current ID token for this application,
        was issued by the Apps domain we expected,
        and that the email address has been verified.

        @see: http://openid.net/specs/openid-connect-core-1_0.html#IDTokenValidation
        """"""
        if not id_token:
            return False

        # step 2: check issuer
        if id_token['iss'] not in self.app.config['OIDC_VALID_ISSUERS']:
            logger.error('id_token issued by non-trusted issuer: %s'
                         % id_token['iss'])
            return False

        if isinstance(id_token['aud'], list):
            # step 3 for audience list
            if self.flow.client_id not in id_token['aud']:
                logger.error('We are not a valid audience')
                return False
            # step 4
            if 'azp' not in id_token:
                logger.error('Multiple audiences and not authorized party')
                return False
        else:
            # step 3 for single audience
            if id_token['aud'] != self.flow.client_id:
                logger.error('We are not the audience')
                return False

        # step 5
        if 'azp' in id_token and id_token['azp'] != self.flow.client_id:
            logger.error('Authorized Party is not us')
            return False

        # step 6-8: TLS checked

        # step 9: check exp
        if int(self.time()) >= int(id_token['exp']):
            logger.error('Token has expired')
            return False

        # step 10: check iat
        if id_token['iat'] < (self.time() - self.app.config['OIDC_CLOCK_SKEW']):
            logger.error('Token issued in the past')
            return False

        # (not required if using HTTPS?) step 11: check nonce

        # step 12-13: not requested acr or auth_time, so not needed to test

        # additional steps specific to our usage
        if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:
            logger.error('Invalid google apps domain')
            return False

        if not id_token.get('email_verified', False) and \
                self.app.config['OIDC_REQUIRE_VERIFIED_EMAIL']:
            logger.error('Email not verified')
            return False

        return True

    WRONG_GOOGLE_APPS_DOMAIN = 'WRONG_GOOGLE_APPS_DOMAIN'

    def oidc_callback(self):
        """"""
        Exchange the auth code for actual credentials,
        then redirect to the originally requested page.
        """"""
        # retrieve session and callback variables
        try:
            session_csrf_token = session.pop('oidc_csrf_token')

            state = json.loads(request.args['state'])
            csrf_token = state['csrf_token']
            destination = state['destination']

            code = request.args['code']
        except (KeyError, ValueError):
            logger.debug(""Can't retrieve CSRF token, state, or code"",
                         exc_info=True)
            return self.oidc_error()

        # check callback CSRF token passed to IdP
        # against session CSRF token held by user
        if csrf_token != session_csrf_token:
            logger.debug(""CSRF token mismatch"")
            return self.oidc_error()

        # make a request to IdP to exchange the auth code for OAuth credentials
        flow = self.flow_for_request()
        credentials = flow.step2_exchange(code, http=self.http)
        id_token = credentials.id_token
        if not self.is_id_token_valid(id_token):
            logger.debug(""Invalid ID token"")
            if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:
                return self.oidc_error(
                    ""You must log in with an account from the {0} domain.""
                    .format(self.app.config['OIDC_GOOGLE_APPS_DOMAIN']),
                    self.WRONG_GOOGLE_APPS_DOMAIN)
            return self.oidc_error()

        # store credentials by subject
        # when Google is the IdP, the subject is their G+ account number
        self.credentials_store[id_token['sub']] = credentials

        # Check whether somebody messed with the destination
        destination = destination
        try:
            response = redirect(self.destination_serializer.loads(destination))
        except BadSignature:
            logger.error('Destination signature did not match. Rogue IdP?')
            response = redirect('/')

        # set a persistent signed cookie containing the ID token
        # and redirect to the final destination
        self.set_cookie_id_token(id_token)
        return response

    def oidc_error(self, message='Not Authorized', code=None):
        return (message, 401, {
            'Content-Type': 'text/plain',
        })
/n/n/n",0
29,29,f2ef8b4ffa445be00f6602e446e60916f4ee4d30,"/flask_oidc/__init__.py/n/nfrom functools import wraps
import os
import json
from base64 import b64encode
import time as time_module
from copy import copy
import logging

from six.moves.urllib.parse import urlencode
from flask import request, session, redirect, url_for, g
from oauth2client.client import flow_from_clientsecrets, OAuth2WebServerFlow,\
    AccessTokenRefreshError
import httplib2
from itsdangerous import TimedJSONWebSignatureSerializer, SignatureExpired

__all__ = ['OpenIDConnect', 'MemoryCredentials']

logger = logging.getLogger(__name__)


class MemoryCredentials(dict):
    """"""
    Non-persistent local credentials store.
    Use this if you only have one app server, and don't mind making everyone
    log in again after a restart.
    """"""
    pass


class OpenIDConnect(object):
    """"""
    @see: https://developers.google.com/api-client-library/python/start/get_started
    @see: https://developers.google.com/api-client-library/python/samples/authorized_api_web_server_calendar.py
    """"""
    def __init__(self, app=None, credentials_store=None, http=None, time=None,
                 urandom=None):
        self.credentials_store = credentials_store\
            if credentials_store is not None\
            else MemoryCredentials()

        # stuff that we might want to override for tests
        self.http = http if http is not None else httplib2.Http()
        self.time = time if time is not None else time_module.time
        self.urandom = urandom if urandom is not None else os.urandom

        # get stuff from the app's config, which may override stuff set above
        if app is not None:
            self.init_app(app)

    def init_app(self, app):
        """"""
        Do setup that requires a Flask app.
        """"""
        self.app = app

        # Set some default configuration options
        app.config.setdefault('OIDC_SCOPES', ['openid', 'email'])
        app.config.setdefault('OIDC_GOOGLE_APPS_DOMAIN', None)
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_NAME', 'oidc_id_token')
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_TTL', 7 * 86400)  # 7 days
        # should ONLY be turned off for local debugging
        app.config.setdefault('OIDC_ID_TOKEN_COOKIE_SECURE', True)
        app.config.setdefault('OIDC_VALID_ISSUERS',
                              ['accounts.google.com',
                               'https://accounts.google.com'])
        app.config.setdefault('OIDC_CLOCK_SKEW', 60)  # 1 minute
        app.config.setdefault('OIDC_REQUIRE_VERIFIED_EMAIL', True)

        # register callback route and cookie-setting decorator
        app.route('/oidc_callback')(self.oidc_callback)
        app.before_request(self.before_request)
        app.after_request(self.after_request)

        # load client_secrets.json
        self.flow = flow_from_clientsecrets(
            app.config['OIDC_CLIENT_SECRETS'],
            scope=app.config['OIDC_SCOPES'])
        assert isinstance(self.flow, OAuth2WebServerFlow)

        # create a cookie signer using the Flask secret key
        self.cookie_serializer = TimedJSONWebSignatureSerializer(
            app.config['SECRET_KEY'])

        try:
            self.credentials_store = app.config['OIDC_CREDENTIALS_STORE']
        except KeyError:
            pass

    def get_cookie_id_token(self):
        try:
            id_token_cookie = request.cookies[self.app.config['OIDC_ID_TOKEN_COOKIE_NAME']]
            return self.cookie_serializer.loads(id_token_cookie)
        except (KeyError, SignatureExpired):
            logger.debug(""Missing or invalid ID token cookie"", exc_info=True)
            return None

    def set_cookie_id_token(self, id_token):
        """"""
        Cooperates with @after_request to set a new ID token cookie.
        """"""
        g.oidc_id_token = id_token
        g.oidc_id_token_dirty = True

    def after_request(self, response):
        """"""
        Set a new ID token cookie if the ID token has changed.
        """"""
        if getattr(g, 'oidc_id_token_dirty', False):
            signed_id_token = self.cookie_serializer.dumps(g.oidc_id_token)
            response.set_cookie(
                self.app.config['OIDC_ID_TOKEN_COOKIE_NAME'], signed_id_token,
                secure=self.app.config['OIDC_ID_TOKEN_COOKIE_SECURE'],
                httponly=True,
                max_age=self.app.config['OIDC_ID_TOKEN_COOKIE_TTL'])
        return response

    def before_request(self):
        g.oidc_id_token = None
        self.authenticate_or_redirect()

    def authenticate_or_redirect(self):
        """"""
        Helper function suitable for @app.before_request and @check (below).
        Sets g.oidc_id_token to the ID token if the user has successfully
        authenticated, else returns a redirect object so they can go try
        to authenticate.
        :return: A redirect, or None if the user is authenticated.
        """"""
        # the auth callback and error pages don't need user to be authenticated
        if request.endpoint in frozenset(['oidc_callback', 'oidc_error']):
            return None

        # retrieve signed ID token cookie
        id_token = self.get_cookie_id_token()
        if id_token is None:
            return self.redirect_to_auth_server(request.url)

        # ID token expired
        # when Google is the IdP, this happens after one hour
        if self.time() >= id_token['exp']:
            # get credentials from store
            try:
                credentials = self.credentials_store[id_token['sub']]
            except KeyError:
                logger.debug(""Expired ID token, credentials missing"",
                             exc_info=True)
                return self.redirect_to_auth_server(request.url)

            # refresh and store credentials
            try:
                credentials.refresh(self.http)
                id_token = credentials.id_token
                self.credentials_store[id_token['sub']] = credentials
                self.set_cookie_id_token(id_token)
            except AccessTokenRefreshError:
                # Can't refresh. Wipe credentials and redirect user to IdP
                # for re-authentication.
                logger.debug(""Expired ID token, can't refresh credentials"",
                             exc_info=True)
                del self.credentials_store[id_token['sub']]
                return self.redirect_to_auth_server(request.url)

        # make ID token available to views
        g.oidc_id_token = id_token

        return None

    def require_login(self, view_func):
        """"""
        Use this to decorate view functions if only some of your app's views
        require authentication.
        """"""
        @wraps(view_func)
        def decorated(*args, **kwargs):
            if g.oidc_id_token is None:
                return self.redirect_to_auth_server(request.url)
            return view_func(*args, **kwargs)
        return decorated
    # Backwards compatibility
    check = require_login

    def flow_for_request(self):
        """"""
        Build a flow with the correct absolute callback URL for this request.
        :return:
        """"""
        flow = copy(self.flow)
        flow.redirect_uri = url_for('oidc_callback', _external=True)
        return flow

    def redirect_to_auth_server(self, destination):
        """"""
        Set a CSRF token in the session, and redirect to the IdP.
        :param destination: the page that the user was going to,
                            before we noticed they weren't logged in
        :return: a redirect response
        """"""
        csrf_token = b64encode(self.urandom(24)).decode('utf-8')
        session['oidc_csrf_token'] = csrf_token
        state = {
            'csrf_token': csrf_token,
            'destination': destination,
        }
        extra_params = {
            'state': json.dumps(state),
        }
        flow = self.flow_for_request()
        auth_url = '{url}&{extra_params}'.format(
            url=flow.step1_get_authorize_url(),
            extra_params=urlencode(extra_params))
        # if the user has an ID token, it's invalid, or we wouldn't be here
        self.set_cookie_id_token(None)
        return redirect(auth_url)

    def is_id_token_valid(self, id_token):
        """"""
        Check if `id_token` is a current ID token for this application,
        was issued by the Apps domain we expected,
        and that the email address has been verified.

        @see: http://openid.net/specs/openid-connect-core-1_0.html#IDTokenValidation
        """"""
        if not id_token:
            return False

        # step 2: check issuer
        if id_token['iss'] not in self.app.config['OIDC_VALID_ISSUERS']:
            logger.error('id_token issued by non-trusted issuer: %s'
                         % id_token['iss'])
            return False

        if isinstance(id_token['aud'], list):
            # step 3 for audience list
            if self.flow.client_id not in id_token['aud']:
                logger.error('We are not a valid audience')
                return False
            # step 4
            if 'azp' not in id_token:
                logger.error('Multiple audiences and not authorized party')
                return False
        else:
            # step 3 for single audience
            if id_token['aud'] != self.flow.client_id:
                logger.error('We are not the audience')
                return False

        # step 5
        if 'azp' in id_token and id_token['azp'] != self.flow.client_id:
            logger.error('Authorized Party is not us')
            return False

        # step 6-8: TLS checked

        # step 9: check exp
        if int(self.time()) >= int(id_token['exp']):
            logger.error('Token has expired')
            return False

        # step 10: check iat
        if id_token['iat'] < (self.time() - self.app.config['OIDC_CLOCK_SKEW']):
            logger.error('Token issued in the past')
            return False

        # (not required if using HTTPS?) step 11: check nonce

        # step 12-13: not requested acr or auth_time, so not needed to test

        # additional steps specific to our usage
        if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:
            logger.error('Invalid google apps domain')
            return False

        if not id_token.get('email_verified', False) and \
                self.app.config['OIDC_REQUIRE_VERIFIED_EMAIL']:
            logger.error('Email not verified')
            return False

        return True

    WRONG_GOOGLE_APPS_DOMAIN = 'WRONG_GOOGLE_APPS_DOMAIN'

    def oidc_callback(self):
        """"""
        Exchange the auth code for actual credentials,
        then redirect to the originally requested page.
        """"""
        # retrieve session and callback variables
        try:
            session_csrf_token = session.pop('oidc_csrf_token')

            state = json.loads(request.args['state'])
            csrf_token = state['csrf_token']
            destination = state['destination']

            code = request.args['code']
        except (KeyError, ValueError):
            logger.debug(""Can't retrieve CSRF token, state, or code"",
                         exc_info=True)
            return self.oidc_error()

        # check callback CSRF token passed to IdP
        # against session CSRF token held by user
        if csrf_token != session_csrf_token:
            logger.debug(""CSRF token mismatch"")
            return self.oidc_error()

        # make a request to IdP to exchange the auth code for OAuth credentials
        flow = self.flow_for_request()
        credentials = flow.step2_exchange(code, http=self.http)
        id_token = credentials.id_token
        if not self.is_id_token_valid(id_token):
            logger.debug(""Invalid ID token"")
            if id_token.get('hd') != self.app.config['OIDC_GOOGLE_APPS_DOMAIN']:
                return self.oidc_error(
                    ""You must log in with an account from the {0} domain.""
                    .format(self.app.config['OIDC_GOOGLE_APPS_DOMAIN']),
                    self.WRONG_GOOGLE_APPS_DOMAIN)
            return self.oidc_error()

        # store credentials by subject
        # when Google is the IdP, the subject is their G+ account number
        self.credentials_store[id_token['sub']] = credentials

        # set a persistent signed cookie containing the ID token
        # and redirect to the final destination
        # TODO: validate redirect destination
        response = redirect(destination)
        self.set_cookie_id_token(id_token)
        return response

    def oidc_error(self, message='Not Authorized', code=None):
        return (message, 401, {
            'Content-Type': 'text/plain',
        })
/n/n/n",1
82,82,f166c8d6511dd4d6b2e5138aceaa4fb5bb5c5379,"dashboard/admin.py/n/nfrom django.contrib import admin
from dashboard.models import *
from django.db.models import Count

from django import forms
from taggit_labels.widgets import LabelWidget
from dashboard.signals import *

class PUCAdminForm(forms.ModelForm):
    class Meta:
        model = PUC
        fields = ['gen_cat', 'prod_fam', 'prod_type', 'description','tags','kind']
        readonly_fields = ('num_products',)
        widgets = {
            'tags': LabelWidget(model=PUCTag),
        }

class PUCAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'tag_list','num_products')
    list_filter = ('kind',)
    form = PUCAdminForm
    def get_changeform_initial_data(self, request):
        get_data = super(PUCAdmin, self).get_changeform_initial_data(request)
        get_data['last_edited_by'] = request.user.pk
        return get_data
    def get_queryset(self, request):
        return super(PUCAdmin, self).get_queryset(request).prefetch_related('tags').annotate(num_products=Count('products'))
    def num_products(self, obj):
        return obj.num_products
    num_products.short_description = 'Product Count'
    num_products.admin_order_field = 'num_products'
    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())

class HHDocAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'hhe_report_number')

class ScriptForm(forms.ModelForm):
    class Meta(object):
        model = Script
        fields = '__all__'

    def __init__(self, *args, **kwargs):
        super(ScriptForm, self).__init__(*args, **kwargs)
        if self.instance and self.instance.pk and not self.instance.script_type == 'EX':
            # Since the pk is set this is not a new instance
            self.fields['confidence'].widget = forms.HiddenInput()

class ScriptAdmin(admin.ModelAdmin):
    list_filter = ('script_type',)
    list_display = ('__str__','confidence_level')
    form = ScriptForm
    def confidence_level(self, obj):
        if obj.script_type == 'EX':
            return obj.confidence
        else:
            return ''

class PUCToTagAdmin(admin.ModelAdmin):
    list_display = ('content_object', 'tag', 'assumed')
    list_filter = ('tag',)
    def tag(self, obj):
        return obj.tag    
    def assumed(self, obj):
        return obj.assumed 

# Register your models here.
admin.site.register(DataSource)
admin.site.register(GroupType)
admin.site.register(DataGroup)
admin.site.register(DocumentType)
admin.site.register(DataDocument)
admin.site.register(Script, ScriptAdmin)
admin.site.register(Product)
admin.site.register(ProductToPUC)
admin.site.register(ProductDocument)
admin.site.register(SourceCategory)
admin.site.register(PUC, PUCAdmin)
admin.site.register(ExtractedText)
admin.site.register(ExtractedChemical)
admin.site.register(ExtractedFunctionalUse)
admin.site.register(ExtractedHabitsAndPractices)
admin.site.register(DSSToxLookup)
admin.site.register(QAGroup)
admin.site.register(UnitType)
admin.site.register(WeightFractionType)
admin.site.register(PUCTag) #,ProductTagAdmin
admin.site.register(Taxonomy)
admin.site.register(TaxonomySource)
admin.site.register(TaxonomyToPUC)
admin.site.register(ExtractedHHDoc, HHDocAdmin)
admin.site.register(ExtractedHHRec)
admin.site.register(PUCToTag, PUCToTagAdmin)
/n/n/ndashboard/forms.py/n/nfrom dal import autocomplete
from bootstrap_datepicker_plus import DatePickerInput

from django import forms
from django.forms import BaseInlineFormSet

from django.utils.translation import ugettext_lazy as _

from dashboard.models import *
from django.db.models import F
from dashboard.utils import get_extracted_models


class DataGroupForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = DataGroup
        fields = ['name', 'description', 'url', 'group_type', 'downloaded_by',
                  'downloaded_at', 'download_script', 'data_source', 'csv']
        widgets = {'downloaded_at': DatePickerInput()}
        labels = {'csv': _('Register Records CSV File'),
                  'url': _('URL'), }

    def __init__(self, *args, **kwargs):
        qs = Script.objects.filter(script_type='DL')
        self.user = kwargs.pop('user', None)
        super(DataGroupForm, self).__init__(*args, **kwargs)
        self.fields['csv'].widget.attrs.update({'accept': '.csv'})
        self.fields['download_script'].queryset = qs


class ExtractionScriptForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='EX'),
        label=""Extraction Script"")
    weight_fraction_type = forms.ModelChoiceField(
        queryset=WeightFractionType.objects.all(),
        label=""Weight Fraction Type"",
        initial=""1"")
    extract_file = forms.FileField(label=""Extracted Text CSV File"")

    def __init__(self, *args, **kwargs):
        self.dg_type = kwargs.pop('dg_type', 0)
        self.user = kwargs.pop('user', None)
        super(ExtractionScriptForm, self).__init__(*args, **kwargs)
        self.fields['weight_fraction_type'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['extract_file'].widget.attrs.update({'accept': '.csv'})
        if self.dg_type in ['FU', 'CP']:
            del self.fields['weight_fraction_type']
        self.collapsed = True


class CleanCompDataForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='DC'),
        label=""Data Cleaning Script"",
        required=True)
    clean_comp_data_file = forms.FileField(label=""Clean Composition Data CSV File"",
                                           required=True)

    def __init__(self, *args, **kwargs):
        super(CleanCompDataForm, self).__init__(*args, **kwargs)
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['clean_comp_data_file'].widget.attrs.update(
            {'accept': '.csv'})
        self.collapsed = True


class DataSourceForm(forms.ModelForm):
    required_css_class = 'required'

    class Meta:
        model = DataSource
        fields = ['title', 'url', 'estimated_records', 'state', 'priority',
                  'description']


class PriorityForm(forms.ModelForm):
    class Meta:
        model = DataSource
        fields = ['priority']

    def __init__(self, *args, **kwargs):
        super(PriorityForm, self).__init__(*args, **kwargs)
        self.fields['priority'].label = ''
        self.fields['priority'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


class QANotesForm(forms.ModelForm):
    class Meta:
        model = QANotes
        fields = ['qa_notes']
        widgets = {
            'qa_notes': forms.Textarea,
        }
        labels = {
            'qa_notes': _('QA Notes (required if approving edited records)'),
        }


class ExtractedTextQAForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'data_document', 'qa_checked']


class ProductLinkForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    document_type = forms.ModelChoiceField(
        queryset=DocumentType.objects.all(),
        label=""Data Document Type"",
        required=True)
    return_url = forms.CharField()

    class Meta:
        model = Product
        fields = ['title', 'manufacturer',
                  'brand_name', 'upc', 'size', 'color']

    def __init__(self, *args, **kwargs):
        super(ProductLinkForm, self).__init__(*args, **kwargs)
        self.fields['return_url'].widget = forms.HiddenInput()


class ProductForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = Product
        fields = ['title', 'manufacturer', 'brand_name', 'size', 'color',
                  'model_number', 'short_description', 'long_description']


class ProductViewForm(ProductForm):
    class Meta(ProductForm.Meta):
        exclude = ('title', 'long_description',)

    def __init__(self, *args, **kwargs):
        super(ProductForm, self).__init__(*args, **kwargs)
        for f in self.fields:
            self.fields[f].disabled = True


class BasePUCForm(forms.ModelForm):
    puc = forms.ModelChoiceField(
        queryset=PUC.objects.all(),
        label='Category',
        widget=autocomplete.ModelSelect2(
            url='puc-autocomplete',
            attrs={'data-minimum-input-length': 3, })
    )


class ProductPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']


class HabitsPUCForm(BasePUCForm):
    class Meta:
        model = ExtractedHabitsAndPracticesToPUC
        fields = ['puc']


class BulkProductPUCForm(forms.ModelForm):
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput(),
                             required=True)

    class Meta:
        model = ProductToPUC
        fields = ['puc', 'id_pks']


class BulkPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']

    def __init__(self, *args, **kwargs):
        super(BulkPUCForm, self).__init__(*args, **kwargs)
        lbl = 'Select PUC for Attribute to Assign to Selected Products'
        self.fields['puc'].label = lbl
        self.fields['puc'].widget.attrs['onchange'] = 'form.submit();'


class BulkProductTagForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    tag = forms.ModelChoiceField(queryset=PUCTag.objects.none(),
                                 label='Attribute')
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput())

    class Meta:
        model = ProductToPUC
        fields = ['tag', 'id_pks']

    def __init__(self, *args, **kwargs):
        super(BulkProductTagForm, self).__init__(*args, **kwargs)
        lbl = 'Select Attribute to Assign to Selected Products'
        self.fields['tag'].label = lbl


class ExtractedTextForm(forms.ModelForm):
    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'doc_date', 'rev_num']

        widgets = {
            'data_document': forms.HiddenInput(),
            'extraction_script': forms.HiddenInput(),
        }


class ExtractedCPCatForm(ExtractedTextForm):

    class Meta:
        model = ExtractedCPCat
        fields = ['doc_date', 'cat_code',
                  'description_cpcat', 'cpcat_sourcetype']


class ExtractedCPCatEditForm(ExtractedCPCatForm):

    class Meta(ExtractedCPCatForm.Meta):
        fields = ExtractedCPCatForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num', 'cpcat_code']


class ExtractedHHDocForm(ExtractedTextForm):

    class Meta:
        model = ExtractedHHDoc
        fields = ['hhe_report_number', 'study_location', 'naics_code', 'sampling_date', 'population_gender',
                  'population_age', 'population_other', 'occupation', 'facility']


class ExtractedHHDocEditForm(ExtractedHHDocForm):

    class Meta(ExtractedHHDocForm.Meta):
        fields = ExtractedHHDocForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num']


class DocumentTypeForm(forms.ModelForm):
    class Meta:
        model = DataDocument
        fields = ['document_type']

    def __init__(self, *args, **kwargs):
        super(DocumentTypeForm, self).__init__(*args, **kwargs)
        self.fields['document_type'].label = ''
        self.fields['document_type'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


def include_extract_form(dg):
    '''Returns the ExtractionScriptForm based on conditions of DataGroup
    type as well as whether all records are matched, but not extracted
    '''
    if not dg.type in ['FU', 'CO', 'CP']:
        return False
    if dg.all_matched() and not dg.all_extracted():
        return ExtractionScriptForm(dg_type=dg.type)
    else:
        return False


class ExtractedChemicalFormSet(BaseInlineFormSet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


class ExtractedChemicalForm(forms.ModelForm):
    def __init__(self, *args, **kwargs):
        super(ExtractedChemicalForm, self).__init__(*args, **kwargs)
        # the non-field properties need to be explicitly added
        if hasattr(self.instance, 'dsstox') and self.instance.dsstox is not None:
            self.fields['true_cas'] = forms.CharField(max_length=200)
            self.fields['true_cas'].initial = self.instance.dsstox.true_cas
            self.fields['true_cas'].disabled = True
            self.fields['true_chemname'] = forms.CharField(max_length=400)
            self.fields['true_chemname'].initial = self.instance.dsstox.true_chemname
            self.fields['true_chemname'].disabled = True
            self.fields['SID'] = forms.CharField(max_length=50)
            self.fields['SID'].initial = self.instance.dsstox.sid
            self.fields['SID'].disabled = True

    class Meta:
        model = ExtractedChemical
        fields = '__all__'


def include_clean_comp_data_form(dg):
    '''Returns the CleanCompDataForm based on conditions of DataGroup
    type = Composition and at least 1 document extracted
    '''
    if not dg.type in ['CO']:
        return False
    if dg.extracted_docs() > 0:
        return CleanCompDataForm()
    else:
        return False


def create_detail_formset(document, extra=1, can_delete=False, exclude=[]):
    '''Returns the pair of formsets that will be needed based on group_type.
    .                       ('CO'),('CP'),('FU'),('HP'),('HH')
    Parameters
        ----------
        document : DataDocument
            The parent DataDocument
        extra : integer
            How many empty forms should be created for new records
        can_delete : boolean
            whether a delete checkbox is included
        exclude : list
            which fields to leave out of the form
    .

    '''
    group_type = document.data_group.type
    parent, child = get_extracted_models(group_type)
    extracted = hasattr(document, 'extractedtext')

    def make_formset(parent_model, model,
                     formset=BaseInlineFormSet,
                     form=forms.ModelForm,
                     exclude=exclude):
        formset_fields = model.detail_fields()
        if exclude:
            formset_fields = [in_field for in_field in formset_fields if not in_field in exclude]
        return forms.inlineformset_factory(parent_model=parent_model,
                                           model=model,
                                           fields=formset_fields,
                                           formset=formset,  # this specifies a custom formset
                                           form=form,
                                           extra=extra,
                                           can_delete=can_delete)

    def one():  # for chemicals or unknown
        ChemicalFormSet = make_formset(
            parent_model=parent,
            model=child,
            formset=ExtractedChemicalFormSet,
            form=ExtractedChemicalForm
        )
        return (ExtractedTextForm, ChemicalFormSet)

    def two():  # for functional_use
        FunctionalUseFormSet = make_formset(parent, child)
        return (ExtractedTextForm, FunctionalUseFormSet)

    def three():  # for habits_and_practices
        HnPFormSet = make_formset(parent, child)
        return (ExtractedTextForm, HnPFormSet)

    def four():  # for extracted_list_presence
        ListPresenceFormSet = make_formset(parent, child)
        ParentForm = ExtractedCPCatForm if extracted else ExtractedCPCatEditForm


        return (ParentForm, ListPresenceFormSet)

    def five():  # for extracted_hh_rec
        HHFormSet = make_formset(parent, child)
        ParentForm = ExtractedHHDocForm if extracted else ExtractedHHDocEditForm
        return (ParentForm, HHFormSet)
    dg_types = {
        'CO': one,
        'UN': one,
        'FU': two,
        'HP': three,
        'CP': four,
        'HH': five,
    }
    func = dg_types.get(group_type, lambda: None)
    return func()
/n/n/ndashboard/migrations/0096_puc_kind.py/n/n# Generated by Django 2.1.7 on 2019-03-14 06:12
import taggit.managers
from datetime import datetime

from django.db import migrations, models


def populate_category_field(apps, schema_editor):
    '''
    populate the category field with the appropriate choice based upon the date.
    '''
    parting_day = datetime(2019,1,1)
    PUC = apps.get_model('dashboard', 'PUC')
    for puc in PUC.objects.all():
        if puc.created_at < parting_day:
            puc.kind = 'FO'
        else:
            puc.kind = 'OC'    
        puc.save()

class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0095_dd_url_and_raw_category_bigger'),
    ]

    operations = [
        migrations.AddField(
            model_name='puc',
            name='kind',
            field=models.CharField(blank=True, choices=[('UN', 'unknown'), ('FO', 'formulations'), ('AR', 'articles'), ('OC', 'occupational')], default='UN', max_length=2),
        ),
        migrations.AlterField(
            model_name='puc',
            name='tags',
            field=taggit.managers.TaggableManager(blank=True, help_text='A set of PUC Attributes applicable to this PUC', through='dashboard.PUCToTag', to='dashboard.PUCTag', verbose_name='Tags'),
        ),
        migrations.RunPython(populate_category_field,
                                reverse_code=migrations.RunPython.noop),
    ]
/n/n/ndashboard/migrations/0097_script_confidence.py/n/n# Generated by Django 2.1.7 on 2019-03-15 14:50

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0096_puc_kind'),
    ]

    operations = [
        migrations.AddField(
            model_name='script',
            name='confidence',
            field=models.PositiveSmallIntegerField(blank=True, default=1, validators=[django.core.validators.MaxValueValidator(100), django.core.validators.MinValueValidator(1)], verbose_name='Confidence'),
        ),
    ]
/n/n/ndashboard/migrations/0098_add_cpcat_qa_flag.py/n/n# Generated by Django 2.1.2 on 2019-03-20 08:56

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0097_script_confidence'),
    ]

    operations = [
        migrations.AddField(
            model_name='extractedlistpresence',
            name='qa_flag',
            field=models.BooleanField(default=False),
        ),
    ]
/n/n/ndashboard/migrations/0099_qagroup_script.py/n/n# Generated by Django 2.1.2 on 2019-03-22 10:14

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0098_add_cpcat_qa_flag'),
    ]

    operations = [
        migrations.AlterField(
            model_name='qagroup',
            name='extraction_script',
            field=models.ForeignKey(limit_choices_to={'script_type': 'EX'}, on_delete=django.db.models.deletion.CASCADE, related_name='qa_group', to='dashboard.Script'),
        ),
    ]
/n/n/ndashboard/migrations/0100_extractedchemical_names.py/n/n# Generated by Django 2.1.2 on 2019-03-27 01:58

import dashboard.models.extracted_chemical
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0099_qagroup_script'),
    ]

    operations = [
        migrations.AlterField(
            model_name='extractedchemical',
            name='ingredient_rank',
            field=models.PositiveIntegerField(blank=True, null=True, validators=[dashboard.models.extracted_chemical.validate_ingredient_rank], verbose_name='Ingredient rank'),
        ),
        migrations.AlterField(
            model_name='extractedchemical',
            name='raw_central_comp',
            field=models.CharField(blank=True, max_length=100, null=True, verbose_name='Raw central composition'),
        ),
    ]
/n/n/ndashboard/migrations/0101_verbose_dsstox_names.py/n/n# Generated by Django 2.1.2 on 2019-03-28 00:05

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0100_extractedchemical_names'),
    ]

    operations = [
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='sid',
            field=models.CharField(max_length=50, unique=True, verbose_name='DTXSID'),
        ),
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='true_cas',
            field=models.CharField(blank=True, max_length=50, null=True, verbose_name='True CAS'),
        ),
        migrations.AlterField(
            model_name='dsstoxlookup',
            name='true_chemname',
            field=models.CharField(blank=True, max_length=500, null=True, verbose_name='True chemical name'),
        ),
    ]
/n/n/ndashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo
from .extracted_habits_and_practices_to_puc import (
                                            ExtractedHabitsAndPracticesToPUC)
from .extracted_habits_and_practices import ExtractedHabitsAndPractices


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.content_object)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/ndashboard/models/data_document.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.utils import timezone
from .document_type import DocumentType
from django.core.exceptions import ValidationError


class DataDocument(CommonInfo):

    filename = models.CharField(max_length=255)
    title = models.CharField(max_length=255)
    url = models.CharField(null=True, blank=True, max_length=275)
    raw_category = models.CharField(null=True, blank=True, max_length=100)
    data_group = models.ForeignKey('DataGroup', on_delete=models.CASCADE)
    products = models.ManyToManyField('Product', through='ProductDocument')
    matched = models.BooleanField(default=False)
    extracted = models.BooleanField(default=False)
    document_type = models.ForeignKey(DocumentType, on_delete=models.PROTECT,
                                                        null=True, blank=True)
    organization = models.CharField(max_length=255, blank=True)
    note = models.TextField(blank=True, null=True)

    class Meta:
        ordering = ['-id']

    def __str__(self):
        return str(self.title)

    @property
    def is_extracted(self):
        return hasattr(self,'extractedtext')

    def get_absolute_url(self):
        return reverse('data_document', kwargs={'pk': self.pk})

    def get_abstract_filename(self):
        ext = self.filename.split('.')[-1] #maybe not all are PDF??
        return f'document_{self.pk}.{ext}'

    def pdf_url(self):
        dg = self.data_group
        fn = self.get_abstract_filename()
        return f'/media/{dg.fs_id}/pdf/{fn}'

    def clean(self):
        # the document_type must be one of the children types
        # of the datadocument's parent datagroup
        this_type = self.data_group.group_type
        doc_types = DocumentType.objects.filter(group_type=this_type)
        if not self.document_type in doc_types:
            raise ValidationError(('The document type must be allowed by '
                                                    'the parent data group.'))
/n/n/ndashboard/models/data_group.py/n/nimport os
import shutil
import uuid
from factotum import settings
from pathlib import Path, PurePath

from django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.db.models.signals import pre_save
from django.dispatch import receiver
from model_utils import FieldTracker
from django.core.exceptions import ValidationError

from .group_type import GroupType
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_list_presence import ExtractedListPresence

# could be used for dynamically creating filename on instantiation
# in the 'upload_to' param on th FileField
def update_filename(instance, filename):
    name_fill_space = instance.name.replace(' ', '_')
    # potential space errors in name
    name = '{0}/{0}_{1}'.format(name_fill_space, filename)
    return name


def csv_upload_path(instance, filename):
    # potential space errors in name
    name = '{0}/{1}'.format(instance.fs_id, filename)
    return name

extract_models = {
    'CO': (ExtractedText, ExtractedChemical),
    'FU': (ExtractedText, ExtractedFunctionalUse),
    'CP': (ExtractedCPCat, ExtractedListPresence)
}



class DataGroup(CommonInfo):

    name = models.CharField(max_length=50)
    description = models.TextField(null=True, blank=True)
    downloaded_by = models.ForeignKey('auth.User',
                                    on_delete=models.SET_DEFAULT, default = 1)
    downloaded_at = models.DateTimeField()
    download_script = models.ForeignKey('Script',
                                    on_delete=models.SET_NULL, default=None,
                                    null=True, blank=True)
    data_source = models.ForeignKey('DataSource', on_delete=models.CASCADE)
    fs_id = models.UUIDField(default=uuid.uuid4, editable=False)
    csv = models.FileField(upload_to=csv_upload_path, null=True)
    zip_file = models.CharField(max_length=100)
    group_type = models.ForeignKey(GroupType, on_delete=models.SET_DEFAULT,
                                            default=1, null=True, blank=True)
    url = models.CharField(max_length=150, blank=True)

    tracker = FieldTracker()

    @property
    def type(self):
        return str(self.group_type.code)

    @property
    def is_composition(self):
        return self.type == 'CO'

    @property
    def is_habits_and_practices(self):
        return self.type == 'HP'

    @property
    def is_functional_use(self):
        return self.type == 'FU'

    @property
    def is_chemical_presence(self):
        return self.type == 'CP'

    @property
    def is_hh(self):
        return self.type == 'HH'


    def get_extract_models(self):
        '''returns a tuple with parent/child extract models'''
        return extract_models.get(self.type)

    def save(self, *args, **kwargs):
        super(DataGroup, self).save(*args, **kwargs)

    def matched_docs(self):
        return self.datadocument_set.filter(matched=True).count()

    def all_matched(self):
        return all(self.datadocument_set.values_list('matched', flat=True))

    def all_extracted(self):
        return all(self.datadocument_set.values_list('extracted', flat=True))

    def registered_docs(self):
        return self.datadocument_set.count()

    def extracted_docs(self):
        return self.datadocument_set.filter(extracted=True).count()

    def __str__(self):
        return self.name

    def get_absolute_url(self):
        return reverse('data_group_edit', kwargs={'pk': self.pk})

    def get_name_as_slug(self):
        return self.name.replace(' ', '_')

    def get_dg_folder(self):
        uuid_dir = f'{settings.MEDIA_ROOT}{str(self.fs_id)}'
        name_dir = f'{settings.MEDIA_ROOT}{self.get_name_as_slug()}'

        #this needs to handle missing csv files
        if bool(self.csv.name):
            # parse the media folder from the penultimate piece of csv file path
            p = PurePath(self.csv.path)
            csv_folder=p.parts[-2]
            csv_fullfolderpath   = f'{settings.MEDIA_ROOT}{csv_folder}'

        if os.path.isdir(uuid_dir):
            return uuid_dir # UUID-based folder
        elif bool(self.csv.name) and os.path.isdir(csv_fullfolderpath):
            return csv_fullfolderpath # csv path-based folder
        else:
            return 'no_folder_found'

    @property
    def dg_folder(self):
        '''This is a ""falsy"" property. If the folder cannot be found,
        dg.dg_folder evaluates to boolean False '''
        if self.get_dg_folder() != 'no_folder_found':
            return self.get_dg_folder()
        else:
            return False


    @property
    def csv_url(self):
        '''This is a ""falsy"" property. If the csv file cannot be found,
        dg.csv_url evaluates to boolean False '''
        try:
            self.csv.size
            csv_url = self.csv.url
        except ValueError:
            csv_url = False
        except:
            csv_url = False
        return csv_url


    @property
    def zip_url(self):
        '''This is a ""falsy"" property. If the zip file cannot be found,
        dg.zip_url evaluates to boolean False '''
        if self.get_zip_url()!='no_path_found':
            return(self.get_zip_url)
        else:
            return False
        

    def get_zip_url(self):
        # the path if the data group's folder was built from a UUID:
        uuid_path = f'{self.get_dg_folder()}/{str(self.fs_id)}.zip'
        # path if the data group's folder was built from old name-based method
        zip_file_path = f'{self.get_dg_folder()}/{self.get_name_as_slug()}.zip'
        if os.path.isfile(uuid_path):   # it is a newly-added data group
            zip_url = uuid_path
        elif os.path.isfile(zip_file_path): # it is a pre-UUID data group
            zip_url = zip_file_path
        else:
            zip_url = 'no_path_found'
        return zip_url


    def get_extracted_template_fieldnames(self):
        extract_fields = ['data_document_id','data_document_filename',
                            'prod_name', 'doc_date','rev_num', 'raw_category',
                            'raw_cas', 'raw_chem_name', 'report_funcuse']
        if self.type == 'FU':
            return extract_fields
        if self.type == 'CO':
            return extract_fields + ['raw_min_comp','raw_max_comp', 'unit_type',
                                        'ingredient_rank', 'raw_central_comp']
        if self.type == 'CP':
            for name in ['prod_name','rev_num','report_funcuse']:
                extract_fields.remove(name)
            return extract_fields + ['cat_code','description_cpcat',
                                    'cpcat_code','cpcat_sourcetype']

    def get_clean_comp_data_fieldnames(self):
        return ['id','lower_wf_analysis','central_wf_analysis', 'upper_wf_analysis']

    def clean_fields(self, exclude=None):
        super().clean_fields(exclude=exclude)
        if self.tracker.has_changed('group_type_id') and self.extracted_docs():
            msg = ""The Group Type may not be changed once extracted documents have been associated with the group.""
            raise ValidationError({'group_type': msg})


@receiver(models.signals.post_delete, sender=DataGroup)
def auto_delete_file_on_delete(sender, instance, **kwargs):
    """"""
    Deletes datagroup directory from filesystem
    when datagroup instance is deleted.
    """"""
    dg_folder = instance.get_dg_folder()
    if os.path.isdir(dg_folder):
        #print('deleting folder %s for data group %s'%(dg_folder, instance.pk))
        shutil.rmtree(dg_folder)
/n/n/ndashboard/models/dsstox_lookup.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse


class DSSToxLookup(CommonInfo):

    sid = models.CharField('DTXSID', max_length=50, null=False, blank=False, unique=True)
    true_cas = models.CharField('True CAS', max_length=50, null=True, blank=True)
    true_chemname = models.CharField('True chemical name', max_length=500, null=True, blank=True)

    def __str__(self):
        return self.true_chemname

    def get_absolute_url(self):
        return reverse('dsstox_lookup', kwargs={'pk': self.pk})



/n/n/ndashboard/models/extracted_chemical.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .extracted_text import ExtractedText
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .raw_chem import RawChem


def validate_ingredient_rank(value):
    if value < 1 or value > 999:
        raise ValidationError(
            (f'Quantity {value} is not allowed'), params={'value': value},)


class ExtractedChemical(CommonInfo, RawChem):

    raw_cas_old = models.CharField(
        ""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                         null=True, blank=True)
    raw_min_comp = models.CharField(""Raw minimum composition"", max_length=100,
                                    null=True, blank=True)
    raw_max_comp = models.CharField(""Raw maximum composition"", max_length=100,
                                    null=True, blank=True)
    unit_type = models.ForeignKey(UnitType, on_delete=models.PROTECT)
    report_funcuse = models.CharField(""Reported functional use"", max_length=100,
                                      null=True, blank=True)
    weight_fraction_type = models.ForeignKey(WeightFractionType,
                                             on_delete=models.PROTECT, null=True, default='1')
    ingredient_rank = models.PositiveIntegerField(""Ingredient rank"", null=True, blank=True,
                                                  validators=[validate_ingredient_rank])
    raw_central_comp = models.CharField(""Raw central composition"", max_length=100, null=True, blank=True)

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @classmethod
    def detail_fields(cls):
        return ['extracted_text', 'raw_chem_name', 'raw_cas', 'raw_min_comp', 'raw_central_comp',
                'raw_max_comp', 'unit_type', 'weight_fraction_type', 'report_funcuse',
                'ingredient_rank', 'rawchem_ptr']

    def get_datadocument_url(self):
        return self.extracted_text.data_document.get_absolute_url()

    @property
    def data_document(self):
        return self.extracted_text.data_document

    def indexing(self):
        obj = ExtractedChemicalIndex(
            meta={'id': self.id},
            chem_name=self.raw_chem_name,
            raw_cas=self.raw_cas,
            raw_chem_name=self.raw_chem_name,
            facet_model_name='Extracted Chemical',
        )
        obj.save()
        return obj.to_dict(include_meta=True)

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def true_cas(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_cas
        else:
            return None

    @property
    def true_chemname(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_chemname
        else:
            return None

    @property
    def sid(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.sid
        else:
            return None
/n/n/ndashboard/models/extracted_cpcat.py/n/nfrom django.db import models
from .extracted_text import ExtractedText


class ExtractedCPCat(ExtractedText):
    cat_code = models.CharField(""Cat Code"", max_length=100,
                                        null=True, blank=True)
    description_cpcat = models.CharField(""Description CPCat"", max_length=200,
                                        null=True, blank=True)
    cpcat_code = models.CharField(""CPCat Code"", max_length=50,
                                        null=True, blank=True)
    cpcat_sourcetype = models.CharField(""CPCat SourceType"", max_length=50,
                                        null=True, blank=True)

    def __str__(self):
        return str(self.prod_name)

    @property
    def qa_begun(self):
        return self.rawchem.select_subclasses().filter(extractedlistpresence__qa_flag=True).count() > 0/n/n/ndashboard/models/extracted_list_presence.py/n/nfrom django.db import models

from dashboard.models import CommonInfo
from .raw_chem import RawChem

class ExtractedListPresence(CommonInfo, RawChem):

    raw_cas_old = models.CharField(""Raw CAS"", max_length=100,
                                        null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                        null=True, blank=True)
    qa_flag = models.BooleanField(default=False)

    @classmethod
    def detail_fields(cls):
        return ['raw_cas','raw_chem_name']

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    def get_datadocument_url(self):
        return self.extracted_cpcat.data_document.get_absolute_url()

    def get_extractedtext(self):
        return self.extracted_cpcat.extractedtext_ptr
    
    @property
    def data_document(self):
        return self.extracted_text.data_document
/n/n/ndashboard/models/extracted_text.py/n/nfrom itertools import chain
from datetime import datetime
from model_utils.managers import InheritanceManager

from django.db import models
from django.core.exceptions import ValidationError
from django import forms
from django.urls import reverse


from .common_info import CommonInfo

    # this could potentially be used for 1:1 matching when uploading
    # coming in django v2.2!!
	# class Meta:
	# 	constraints = [
	# 		models.UniqueConstraint(fields=['prod_name','data_document'],
	# 								name='unique_assignment'),
	# 	]

class ExtractedText(CommonInfo):
    data_document = models.OneToOneField('DataDocument',on_delete=models.CASCADE,
                                                            primary_key=True)
    prod_name = models.CharField(max_length=500, null=True, blank=True)
    doc_date = models.CharField(max_length=25, null=True, blank=True)
    rev_num = models.CharField(max_length=50, null=True, blank=True)
    extraction_script = models.ForeignKey('Script', on_delete=models.CASCADE,
                                        limit_choices_to={'script_type': 'EX'})
    qa_checked = models.BooleanField(default=False, verbose_name=""QA approved"")
    qa_edited = models.BooleanField(default=False, verbose_name=""QA edited"")
    qa_approved_date = models.DateTimeField(null=True, blank=True,
                                                verbose_name=""QA approval date"")
    qa_approved_by = models.ForeignKey('auth.User', on_delete=models.SET_NULL,
                                                verbose_name = ""QA approved by"",
                                                null=True, blank=True,)
    qa_group = models.ForeignKey('QAGroup', verbose_name=""QA group"",
                                                     on_delete=models.SET_NULL,
                                                     null=True, blank=True)

    objects = InheritanceManager()


    def __str__(self):
        return str(self.data_document)

    def next_extracted_text_in_qa_group(self):
        nextid = 0
        # If the document is part of a Script-based QA Group, the 
        # next document is drawn from that group. If it is a CPCat
        # or HHE record, there is no next document
        extextnext = get_next_or_prev(ExtractedText.objects.filter(
            qa_group=self.qa_group, qa_checked=False), self, 'next')
        if extextnext:
            # Replace our item with the next one
            nextid = extextnext.pk
        if extextnext == self:
            nextid = 0
        return nextid
    
    def get_qa_index_path(self):
        """"""
        The type of data group to which the extracted text object belongs
        determines which QA index it will use.
        """"""
        group_type_code = self.data_document.data_group.group_type.code

        if group_type_code in ['CP','HH']:
            # TODO: change HH to its own path
            return reverse('qa_chemicalpresence_index')
        else:
            return reverse('qa_extractionscript_index')


    def fetch_extracted_records(self):
        return self.rawchem.all()

    def pull_out_cp(self):
        if hasattr(self, 'extractedcpcat'):
            return self.extractedcpcat
        else:
            return self

    def pull_out_hh(self):
        if hasattr(self, 'extractedhhdoc'):
            return self.extractedhhdoc
        else:
            return self

    def one_to_one_check(self, odict):
        '''
        Used in the upload of extracted text in the data_group_detail view, this
        returns a boolean to assure that there is a 1:1 relationship w/
        the Extracted{parent}, i.e. (Text/CPCat), and the DataDocument
        '''
        if hasattr(self, 'cat_code'):
            return self.cat_code != odict['cat_code']
        else:
            return self.prod_name != odict['prod_name']




def get_next_or_prev(models, item, direction):
    '''
    Returns the next or previous item of
    a query-set for 'item'.

    'models' is a query-set containing all
    items of which 'item' is a part of.

    direction is 'next' or 'prev'

    '''
    getit = False
    if direction == 'prev':
        models = models.reverse()
    for m in models:
        if getit:
            return m
        if item == m:
            getit = True
    if getit:
        # This would happen when the last
        # item made getit True
        return models[0]
    return False
/n/n/ndashboard/models/qa_group.py/n/nfrom django.db import models
from .common_info import CommonInfo
from .extracted_text import ExtractedText
from .script import Script


class QAGroup(CommonInfo):
    extraction_script = models.ForeignKey(Script,
                                    on_delete=models.CASCADE,
                                    related_name='qa_group',
                                    blank=False, null=False,
                                    limit_choices_to={'script_type': 'EX'}, )
    qa_complete = models.BooleanField(default=False)

    def __str__(self):
        return str(self.extraction_script) + '_' + str(self.pk)

    def get_approved_doc_count(self):
        return ExtractedText.objects.filter(qa_group=self,
                                            qa_checked=True).count()
/n/n/ndashboard/models/qa_notes.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from django.utils.translation import ugettext_lazy as _

from dashboard.models import ExtractedText


class QANotes(CommonInfo):
    extracted_text = models.OneToOneField(ExtractedText, on_delete=models.CASCADE)
    qa_notes = models.TextField(null=True, blank=True)

    def __str__(self):
        return 'Notes for {}'.format(self.extracted_text)

    def clean(self):
        if self.extracted_text.qa_edited and not self.qa_notes:
            raise ValidationError(
                    _('Before approving, please add a note explaining your edits to the extracted data'))
/n/n/ndashboard/models/script.py/n/nimport math
from random import shuffle

from django.db import models
from django.urls import reverse
from django.core.validators import (URLValidator, MaxValueValidator, 
                                                    MinValueValidator)

from .common_info import CommonInfo
from .data_document import DataDocument


class Script(CommonInfo):

    TYPE_CHOICES = (('DL', 'download'),
                    ('EX', 'extraction'),
                    ('PC', 'product categorization'),
                    ('DC', 'data cleaning'))

    # Specify the share of a script's ExtractedText objects that must be
    # approved in order for the script's QA sat
    QA_COMPLETE_PERCENTAGE = 0.2


    title = models.CharField(max_length=50)
    url = models.CharField(max_length  = 100,
                            null       = True,
                            blank      = True,
                            validators = [URLValidator()])
    qa_begun = models.BooleanField(default=False)
    script_type = models.CharField( max_length = 2,
                                    choices    = TYPE_CHOICES,
                                    blank      = False,
                                    default    = 'EX')
    confidence = models.PositiveSmallIntegerField('Confidence', blank=True,
                                                validators=[
                                                        MaxValueValidator(100),
                                                        MinValueValidator(1)],
                                                                default=1)

    def __str__(self):
        return str(self.title)

    def get_absolute_url(self):
        return reverse('extraction_script_edit', kwargs={'pk': self.pk})

    def get_datadocument_count(self):
        return DataDocument.objects.filter(
                extractedtext__extraction_script=self.pk).count()

    def get_qa_complete_extractedtext_count(self):
        return DataDocument.objects.filter(extractedtext__qa_checked=True,
                            extractedtext__extraction_script=self.pk).count()

    def get_pct_checked(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return ""{0:.0f}%"".format(pct)

    def get_pct_checked_numeric(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return pct

    def qa_button_text(self):
        if self.get_qa_status():
            return ""QA Complete"" 
        elif self.qa_begun:
            return ""Continue QA""
        else:
            return ""Begin QA""

    def get_qa_status(self):
        """"""
        Compare the derived percent checked against the threshold constant
        Return true when the percent checked is above the threshold
        """"""
        return self.get_pct_checked_numeric() >= self.QA_COMPLETE_PERCENTAGE * 100

    def create_qa_group(self, force_doc_id=None):
        """"""
        Creates a QA Group for the specified Script object;
        Use all the related ExtractedText records or, if there are more than 100,
        select 20% of them. 
        """"""
        from .qa_group import QAGroup
        from .extracted_text import ExtractedText
        es = self
        # Handle cases where a QA group already exists for the script
        if QAGroup.objects.filter(extraction_script = es).count() == 1:
            # This is a valid state
            return QAGroup.objects.get(extraction_script = es)
        elif QAGroup.objects.filter(extraction_script = es).count() > 1:
            # this is a failure mode induced by the system's allowing
            # duplicate QA Groups to be created for a single script
            return QAGroup.objects.filter(extraction_script = es).first()

        
        # Create a new QA Group for the ExtractionScript es
        qa_group = QAGroup.objects.create(extraction_script=es)
        # Collect all the ExtractedText object keys that are related
        # to the Script being QA'd and have not yet been checked
        doc_text_ids = list(ExtractedText.objects.filter(extraction_script=es,
                                                    qa_checked=False
                                                    ).values_list('pk',
                                                                flat=True))
        # If there are fewer than 100 related records, they make up the entire QA Group
        if len(doc_text_ids) < 100 and len(doc_text_ids) > 0:
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids)
        # Otherwise sample 20 percent
        elif len(doc_text_ids) >= 100 :
            # Otherwise sample 20% of them
            random_20 = math.ceil(len(doc_text_ids)/5)
            shuffle(doc_text_ids)  # this is used to make random selection of texts
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids[:random_20])
        else:
            # If there are no related ExtractedText records, something has gone wrong
            # Don't make a new QA Group with zero ExtractedTexts
            # print('The Script has no related ExtractedText records')
            texts = None

        # Set the qa_group attribute of each ExtractedText record to the new QA Group    
        if texts is not None:
            for text in texts:
                text.qa_group = qa_group
                text.save()

        # If the force_doc_id argument was populated, make sure it gets assigned 
        # to the new QA Group
        if force_doc_id is not None and ExtractedText.objects.filter(pk=force_doc_id).exists():
            text = ExtractedText.objects.get(pk=force_doc_id)
            text.qa_group = qa_group
            text.save()
        
        return qa_group

        
/n/n/ndashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        p = self.objects.puc
        puc_line = (p.gen_cat + ',' + p.prod_fam + ',' + p.prod_type + ',' + p.description +
                    ',' + str(p.get_level()) + ',' + str(p.product_count))
        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('gen_cat,prod_fam,prod_type,description,'
                                        'PUC_type,num_prods'))
        # check the PUC from loader
        self.assertEqual(csv_lines[1], puc_line)


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/ndashboard/tests/functional/test_datadocument_create.py/n/nfrom django.contrib.auth.models import User
from dashboard.models import DataGroup, DataDocument, GroupType, DocumentType
from django.test import RequestFactory, TestCase, Client
from dashboard.tests.loader import load_model_objects
from django.core.exceptions import ValidationError
from django.urls import resolve

import os
import io
from dashboard import views
from django.core.files.uploadedfile import (InMemoryUploadedFile,
                                            TemporaryUploadedFile)
from dashboard.tests.loader import fixtures_standard


class DDTestModel(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dd_model_with_wrong_document_type(self):
        # Choose a Composition group
        dgcomp = DataGroup.objects.filter(
            group_type__title='Composition').first()
        # Choose a document type from the wrong parent group type
        dt_fu = DocumentType.objects.filter(
            group_type__title='Functional use').first()
        dd = DataDocument.objects.create(
            filename=""some.pdf"", title=""My Document"", document_type=dt_fu, data_group=dgcomp)
        with self.assertRaises(ValidationError):
            dd.save()
            dd.full_clean()
        dt_comp = DocumentType.objects.filter(
            group_type__title='Composition').first()
        dd = DataDocument.objects.create(
            filename=""some.pdf"", title=""My Document"", document_type=dt_comp, data_group=dgcomp)
        dd.save()
        self.assertEqual(dt_comp.title, dd.document_type.title)


class DDTestUpload(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def testGoodGroupTypeInCSV(self):
        csv_string_good = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,MS,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,MS,, \n"")

        data = io.StringIO(csv_string_good)
        csv_len = len(csv_string_good)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['Composition Type Group'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        # does the datagroup in the ORM contain the new data docs?
        newdg_pk = resolve(resp.url).kwargs['pk']
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 2,
                         'There should be two new data documents')

    def testBadGroupTypeInCSV(self):
        csv_string_bad = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,9,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,4,, \n"")


        data = io.StringIO(csv_string_bad)
        csv_len = len(csv_string_bad)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['Composition Type Group'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        # the upload form should be invalid
        self.assertIn('CSV has bad data in row/s:'.encode(), resp.content)

    def test_upload_csv_as_datadoc(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                           ""Cal_Pesticide_Residues_1987.csv,Example Datadocument from CSV,SG,, \n""
                           )

        data = io.StringIO(csv_string)
        csv_len = len(csv_string)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['California Pesticides'],
                     'description': ['test data group'],
                     'group_type': ['6'],  # CPCat
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        newdg_pk = resolve(resp.url).kwargs['pk']

        # does the datagroup in the ORM contain the new data docs?
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 1,
                         'There should be one new data document')

        # Does the data document page include a link to the csv?  
        resp = self.client.get(f'/datadocument/%s/' % newdds[0].id)            
        # Does the response include a link to the data document's file?
        self.assertContains(resp, f'pdf/document_%s.csv' % newdds[0].id)
        
        
    def test_upload_html_as_datadoc(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                           ""alberto_balsam_conditioner_antioxidant_blueberry.html,Example Datadocument from HTML,ID,, \n""
                           )

        data = io.StringIO(csv_string)
        csv_len = len(csv_string)

        sample_csv = InMemoryUploadedFile(data,
                                          field_name='csv',
                                          name='register_records.csv',
                                          content_type='text/csv',
                                          size=csv_len,
                                          charset='utf-8')
        form_data = {'name': ['California Pesticides'],
                     'description': ['test data group'],
                     'group_type': ['2'],  # Composition
                     'downloaded_by': ['1'],
                     'downloaded_at': ['08/02/2018'],
                     'download_script': ['1'],
                     'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(pk=10, request=request)
        self.assertEqual(resp.status_code, 302,
                         ""Should be redirected to new datagroup detail page"")
        newdg_pk = resolve(resp.url).kwargs['pk']

        # does the datagroup in the ORM contain the new data docs?
        newdg = DataGroup.objects.get(pk=newdg_pk)
        newdds = DataDocument.objects.filter(data_group=newdg)
        self.assertEqual(newdds.count(), 1,
                         'There should be one new data document')

        # Does the data document page include a link to the csv?  
        resp = self.client.get(f'/datadocument/%s/' % newdds[0].id)            
        # Does the response include a link to the data document's file?
        self.assertContains(resp, f'pdf/document_%s.html' % newdds[0].id)
/n/n/ndashboard/tests/functional/test_datadocument_detail.py/n/nfrom lxml import html

from django.test import Client
from django.urls import reverse
from django.test import TestCase, override_settings
from django.core.exceptions import ObjectDoesNotExist

from dashboard.forms import *
from factotum.settings import EXTRA
from dashboard.tests.loader import *


@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_absent_extracted_text(self):
        # Check every data document and confirm that its detail page loads,
        # with or without a detail formset
        for dd in DataDocument.objects.all():
            ddid = dd.id
            resp = self.client.get('/datadocument/%s/' % ddid)
            self.assertEqual(resp.status_code, 200, 'The page must return a 200 status code')
            try:
                extracted_text = ExtractedText.objects.get(data_document=dd)
            except ExtractedText.DoesNotExist:
                #print(dd.id)
                self.assertContains(resp, 'No Extracted Text exists for this Data Document')
            else:
                self.assertContains(resp, '<h4>Extracted Text')

    def test_script_links(self):
        doc = DataDocument.objects.first()
        #response = self.client.get(f'/datadocument/{doc.pk}/')
        response = self.client.get(f'/datadocument/179486/')
        self.assertIn('Download Script',response.content.decode('utf-8'))
        self.assertIn('Extraction Script',response.content.decode('utf-8'))

    def test_product_card_location(self):
        response = self.client.get('/datadocument/179486/')
        html = response.content.decode('utf-8')
        e_idx = html.index('<h4>Extracted Text')
        p_idx = html.index('<h4 class=""d-inline"">Products')
        self.assertTrue(p_idx > e_idx, ('Product card should come after ' 
                                        'Extracted Text card'))

    def test_product_create_link(self):
        response = self.client.get('/datadocument/167497/')
        self.assertContains(response, '/link_product_form/167497/')
        data = {'title'        : ['New Product'],
                'upc'          : ['stub_1860'],
                'document_type': [1],
                'return_url'   : ['/datadocument/167497/']}
        response = self.client.post('/link_product_form/167497/', data=data)
        self.assertRedirects(response,'/datadocument/167497/')
        response = self.client.get(response.url)
        self.assertContains(response, 'New Product')

    def test_product_title_duplication(self):
        response = self.client.get('/datadocument/245401/')
        self.assertContains(response, '/link_product_form/245401/')
        # Add a new Product
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9100'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9100')
        self.assertContains(response, f'product/%s' % new_product.id )

        # Add another new Product with the same title
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9101'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9101')
        self.assertContains(response, f'product/%s' % new_product.id )

    def test_add_extracted(self):
        '''Check that the user has the ability to create an extracted record
        when the document doesn't yet have an extracted record for data 
        group types 'CP' and 'HH'
        '''
        doc = DataDocument.objects.get(pk=354784)
        self.assertFalse(doc.extracted, (""This document is matched ""
                                                    ""but not extracted""))
        data = {'hhe_report_number': ['47']}
        response = self.client.post('/extractedtext/edit/354784/', data=data,
                                                            follow=True)
        doc = DataDocument.objects.get(pk=354784)
        self.assertTrue(doc.extracted, ""This document is not extracted "")
        page = html.fromstring(response.content)
        hhe_no = page.xpath('//dd[contains(@class, ""hh-report-no"")]')[0].text
        self.assertIn('47', hhe_no)


class TestDynamicDetailFormsets(TestCase):
    fixtures = fixtures_standard

    def setUp(self):

        self.client.login(username='Karyn', password='specialP@55word')

    def test_fetch_extracted_records(self):
        ''' Confirm that each detail child object returned by the fetch_extracted_records
        function has the correct parent '''
        for et in ExtractedText.objects.all():
            #print('Fetching extracted child records from %s: %s ' % (et.pk , et))
            for ex_child in et.fetch_extracted_records():
                child_model = ex_child.__class__ # the fetch_extracted_records function returns different classes
                #print('    %s: %s' % (ex_child.__class__.__name__ , ex_child ))
                self.assertEqual(et.pk , child_model.objects.get(pk=ex_child.pk).extracted_text.pk,
                    'The ExtractedChemical object with the returned child pk should have the correct extracted_text parent')

    def test_extractedsubclasses(self):
        ''' Confirm that the inheritance manager is returning appropriate
            subclass objects and ExtractedText base class objects 
         '''
        for doc in DataDocument.objects.all():
            try:
                extsub = ExtractedText.objects.get_subclass(data_document=doc)
                # A document with the CP data group type should be linked to 
                # ExtractedCPCat objects
                if doc.data_group.group_type.code=='CP':
                    #print(f'%s %s %s' % (doc.id, extsub, type(extsub)))
                    self.assertEqual(type(extsub) , ExtractedCPCat)
                elif doc.data_group.group_type.code=='HH':
                    self.assertEqual(type(extsub) , ExtractedHHDoc)
                else:
                    self.assertEqual(type(extsub) , ExtractedText)
            except ObjectDoesNotExist:
                pass
                #print('No extracted text for data document %s' % doc.id)


    def test_every_extractedtext(self):
        ''''Loop through all the ExtractedText objects and confirm that the new
        create_detail_formset method returns forms based on the correct models
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd, EXTRA)
            extracted_text_form = ParentForm(instance=et)
            child_formset = ChildForm(instance=et)
            # Compare the model of the child formset's QuerySet to the model
            # of the ExtractedText object's child objects
            dd_child_model  = get_extracted_models(dd.data_group.group_type.code)[1]
            childform_model = child_formset.__dict__.get('queryset').__dict__.get('model')
            self.assertEqual(dd_child_model, childform_model)

    def test_curated_chemical(self):
        ''''Confirm that if an ExtractedChemical record has been matched to DSSToxLookup, the 
            DSSToxLookup fields are displayed in the card
            This checks every data document.
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd)
            child_formset = ChildForm(instance=et)
            #print('Data doc %s , Group Type: %s ' % (dd.id, dd.data_group.type ))
            for form in child_formset.forms:
                if dd.data_group.type in ['CO','UN']:
                    ec = form.instance
                    if ec.dsstox is not None:
                        self.assertTrue( 'true_cas' in form.fields )
                        self.assertTrue( 'SID' in form.fields )
                    else:
                        self.assertFalse( 'true_cas' in form.fields )
                        self.assertFalse( 'SID' in form.fields )
                else:
                    self.assertFalse( 'true_cas' in form.fields )
            
    def test_num_forms(self):
        ''''Assure that the number of child forms is appropriate for the group
        type.
        '''
        group_models = {
                        'CO': ExtractedChemical,
                        'FU': ExtractedFunctionalUse,
                        'HP': ExtractedHabitsAndPractices,
                        'CP': ExtractedListPresence,
                        'HH': ExtractedHHRec
        }
        for code, model in group_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):

                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(
                                    reverse('data_document',kwargs={'pk': doc.pk})
                )
                num_forms = response.context['detail_formset'].total_form_count()
                children = model.objects.filter(
                                    extracted_text=doc.extractedtext
                ).count()
                if code in ['CO','FU','HP']:
                    error = (f'{model.__module__} should have the same number'
                                                        ' of forms as instances')
                    self.assertEqual(num_forms, children, error)
                if code in ['CP','HH']:
                    error = (f'{model.__module__} should have one more forms'
                                                                ' than instances')
                    self.assertEqual(num_forms, children + 1, error)/n/n/ndashboard/tests/functional/test_datagroup_create_upload.py/n/nimport os
import io
import shutil

from django.test import RequestFactory, TestCase, Client
from django.contrib.auth.models import User
from django.utils.datastructures import MultiValueDict
from django.core.files.uploadedfile import (InMemoryUploadedFile,
                                            TemporaryUploadedFile)

from factotum import settings
from dashboard import views
from dashboard.models import *
from dashboard.tests.loader import fixtures_standard


class RegisterRecordsTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')
        media_root = settings.MEDIA_ROOT
        shutil.rmtree(media_root)


    def tearDown(self):
        # clean up the file system by deleting the data group object
        if DataGroup.objects.filter(name='Walmart MSDS Test Group').exists():
            DataGroup.objects.get(name='Walmart MSDS Test Group').delete()

    def test_datagroup_create(self):
        long_fn = 'a filename that is too long ' * 10
        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,UN,, \n""
                f""{long_fn},Body Cream,1,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        form_data= {'name': ['Walmart MSDS Test Group'],
                    'description': ['test data group'],
                    'group_type': ['1'],
                    'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                    'downloaded_at': ['08/02/2018'],
                    'download_script': ['1'],
                    'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new/', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)
        dg_exists = DataGroup.objects.filter(
                                        name='Walmart MSDS Test Group').exists()
        self.assertContains(resp,'Filename too long')
        self.assertFalse(dg_exists,)

        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,UN,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,UN,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        request = self.factory.post(path='/datagroup/new', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)


        self.assertEqual(resp.status_code,302,
                        ""Should be redirecting"")

        dg = DataGroup.objects.get(name='Walmart MSDS Test Group')


        self.assertEqual(f'/datagroup/{dg.pk}/', resp.url,
                        ""Should be redirecting to the proper URL"")

        # test whether the file system folder was created
        self.assertIn(str(dg.fs_id), os.listdir(settings.MEDIA_ROOT),
                        ""The data group's UUID should be a folder in MEDIA_ROOT"")

        # In the Data Group Detail Page
        resp = self.client.get(f'/datagroup/{dg.pk}/')

        # test whether the data documents were created
        docs = DataDocument.objects.filter(data_group=dg)
        self.assertEqual(len(docs), 2, ""there should be two associated documents"")

        # test whether the ""Download Registered Records"" link is like this example
        # <a href=""/datagroup/a9c7f5a7-5ad4-4f75-b877-a3747f0cc081/registered_records.csv"" class=""btn btn-secondary"">
        # <span class=""oi oi-spreadsheet""></span>&nbsp;Download Registered Records CSV</a>
        csv_href = f'/datagroup/{dg.pk}/registered_records.csv'
        self.assertIn(csv_href, str(resp._container),
                        ""The data group detail page must contain the right download link"")

        # grab a filename from a data document and see if it's in the csv
        doc_fn = docs.first().filename
        # test whether the registered records csv download link works
        resp_rr_csv = self.client.get(csv_href) # this object should be of type StreamingHttpResponse
        docfound = 'not found'
        for csv_row in resp_rr_csv.streaming_content:
            if doc_fn in str(csv_row):
                docfound = 'found'
        self.assertEqual(docfound, 'found', ""the document file name should appear in the registered records csv"")

        # Test whether the data document csv download works
        # URL on data group detail page: datagroup/docs_csv/{pk}/
        dd_csv_href = f'/datagroup/docs_csv/{dg.pk}/'  # this is an interpreted django URL
        resp_dd_csv = self.client.get(dd_csv_href)
        for csv_row in resp_dd_csv.streaming_content:
            if doc_fn in str(csv_row):
                docfound = 'found'
        self.assertEqual(docfound, 'found', ""the document file name should appear in the data documents csv"")


        # test whether the ""Download All PDF Documents"" link works
        dg_zip_href = f'/datagroup/pdfs_zipped/{dg.pk}/' # this is the django-interpreted URL
        self.assertIn(dg_zip_href, str(resp._container),
                        ""The data group detail page must contain the right zip download link"")
        resp_zip = self.client.get(dg_zip_href)

        # test uploading one pdf that matches a registered record
        f = TemporaryUploadedFile(name='0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                                content_type='application/pdf',
                                size=47,
                                charset=None)
        request = self.factory.post(path='/datagroup/%s' % dg.pk, data={'upload':'Submit'})
        request.FILES['multifiles'] = f
        request.user = User.objects.get(username='Karyn')
        resp = views.data_group_detail(request=request, pk=dg.pk)
        doc = DataDocument.objects.get(title='NUTRA NAIL')
        fn = doc.get_abstract_filename()
        folder_name = str(dg.fs_id)
        stored_file = f'{folder_name}/pdf/{fn}'
        pdf_path = f'{settings.MEDIA_ROOT}{stored_file}'
        self.assertTrue(os.path.exists( pdf_path ),
                            ""the stored file should be in MEDIA_ROOT/dg.fs_id"")
        f.close()

    def test_datagroup_create_dupe_filename(self):
        csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,1,, \n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,Body Cream,1,, \n"")
        data = io.StringIO(csv_string)
        sample_csv = InMemoryUploadedFile(data,
                                            field_name='csv',
                                            name='register_records.csv',
                                            content_type='text/csv',
                                            size=len(csv_string),
                                            charset='utf-8')
        form_data= {'name': ['Walmart MSDS Test Group'],
                    'description': ['test data group'],
                    'group_type': ['1'],
                    'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                    'downloaded_at': ['08/02/2018'],
                    'download_script': ['1'],
                    'data_source': ['10']}
        request = self.factory.post(path='/datagroup/new/', data=form_data)
        request.FILES['csv'] = sample_csv
        request.user = User.objects.get(username='Karyn')
        request.session={}
        request.session['datasource_title'] = 'Walmart'
        request.session['datasource_pk'] = 10
        resp = views.data_group_create(request=request, pk=10)

        self.assertContains(resp, 'Duplicate filename found')
/n/n/ndashboard/tests/functional/test_datagroup_detail.py/n/nfrom lxml import html
from importlib import import_module

from django.test import Client
from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard.views.data_group import ExtractionScriptForm, DataGroupForm
from django.core.files.uploadedfile import SimpleUploadedFile
from django.contrib.auth.models import User
from django.test import Client
from importlib import import_module
from django.db.models import Max

from dashboard.forms import *

from dashboard.models import *

class DataGroupDetailTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_detail_form_load(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(self.objects.doc.matched,
                    ('Document should start w/ matched False'))
        self.assertFalse(self.objects.doc.extracted,
                    ('Document should start w/ extracted False'))
        self.assertFalse(response.context['datagroup'].all_matched(),
                    ('UploadForm should be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(), (
                    'UploadForm should not be included in the page!'))
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(),
                    ('UploadForm should not be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_detail_template_fieldnames(self):
        pk = self.objects.dg.pk
        self.assertEqual(str(self.objects.dg.group_type),'Composition',
        'Type of DataGroup needs to be ""composition"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name',
                'report_funcuse','raw_min_comp','raw_max_comp', 'unit_type',
                'ingredient_rank', 'raw_central_comp'],
                ""Fieldnames passed are incorrect!"")
        self.objects.gt.title = 'Functional use'
        self.objects.gt.code = 'FU'
        self.objects.gt.save()
        self.assertEqual(str(self.objects.dg.group_type),'Functional use',
            'Type of DataGroup needs to be ""FU"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name','report_funcuse'],
                ""Fieldnames passed are incorrect!"")

    def test_unidentifed_group_type(self):
        pk = self.objects.dg.pk
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.gt.code = 'UN'
        self.objects.gt.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_bulk_create_products_form(self):
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        doc.matched = True
        self.objects.doc.matched = True
        doc.save()
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        self.assertIn('Bulk Create', response.content.decode(),
                            ""Bulk create button should be present."")
        p = Product.objects.create(upc='stub_47',data_source=self.objects.ds)
        ProductDocument.objects.create(document=doc, product=p)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
        'Product linked to all DataDocuments, no bulk_create needed.')
        self.objects.dg.group_type = GroupType.objects.create(
                                                title='Habits and practices')
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertNotIn('Bulk Create', response.content.decode(),
                            (""Bulk button shouldn't be present w/ ""
                            ""Habits and practices group_type.""))

    def test_bulk_create_post(self):
        '''test the POST to create Products and link if needed'''
        # create a new DataDocument with no Product
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        new_stub_id = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        response = self.client.post(f'/datagroup/{self.objects.dg.pk}/',
                                                                {'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Products linked to all DataDocuments, no bulk_create needed.')
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, 'unknown',
                                        'Title should be unknown in bulk_create')
        
        self.assertEqual(product.upc, f'stub_%s' % new_stub_id,
                                    'UPC should be created for second Product')

    def test_upload_note(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('Please limit upload to <600 documents at one time', response,
                      'Note to limit upload to <600 should be on the page')

    def test_extracted_count(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('0 extracted', response,
                      'Data Group should contain a count of 0 total extracted documents')
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('1 extracted', response,
                      'Data Group should contain a count of 1 total extracted documents')

    def test_delete_doc_button(self):
        url = f'/datagroup/{DataGroup.objects.first().id}/'
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-trash""></span>'
        self.assertIn(span, response,
                      'Trash button should be present if not matched.')
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-circle-check"" style=""color:green;""></span>'
        self.assertIn(span, response,
                      'Check should be present if matched.')

    def test_detail_table_headers(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertIn('<th>Product</th>', response,
                      'Data Group should have Product column.')
        fu = GroupType.objects.create(title='Functional use')
        self.objects.dg.group_type = fu
        self.objects.dg.save()
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertNotIn('<th>Product</th>', response,
                      'Data Group should have Product column.')

    def test_detail_datasource_link(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertContains(response,'<a href=""/datasource/',
                    msg_prefix='Should be able to get back to DataSource from here.')

    def test_edit_redirect(self):
        dgpk = self.objects.dg.pk
        dspk = str(self.objects.ds.pk)
        gtpk = str(self.objects.gt.pk)
        data = {'name': ['Changed Name'],
                'group_type': [gtpk],
                'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                'downloaded_at': ['08/20/2017'],
                'data_source': [dspk]}
        response = self.client.post(f'/datagroup/edit/{dgpk}/', data=data)
        self.assertEqual(response.status_code, 302,
                                         ""User is redirected to detail page."")
        self.assertEqual(response.url, f'/datagroup/{dgpk}/',
                                         ""Should go to detail page."")

class DataGroupDetailTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_download_raw_comp_data(self):
        # Ability to download, by data group, a csv file of raw extracted chemical composition data.
        # Download button would appear on data group detail page,
        # Download button would appear if any data documents have extracted text.
        # Only applies for data group type Composition. (group_type = 2)
        # Unidentified is excluded as of issue #502
        dg_co = DataGroup.objects.filter(group_type__code = 'CO').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_co.id)
        self.assertIn(b'Download Raw', resp.content)

        dg_un = DataGroup.objects.filter(group_type__code = 'UN').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_un.id)
        self.assertNotIn(b'Download Raw', resp.content)

        # Test download on all data groups with ExtractedChemicals, whether
        # they are CO or UN
        dg_ids = DataDocument.objects.filter(
            id__in=ExtractedChemical.objects.all().values('extracted_text_id')
            ).order_by().values_list('data_group_id',flat=True).distinct()

        for dg_id in dg_ids:
            #resp = self.client.get(f'/datagroup/%s/' % dg_id)
            resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_id)
            self.assertEqual(resp.status_code, 200)

        # File downloaded must include [specified fields]
        resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_ids[0])
        field_list = 'ExtractedChemical_id,raw_cas,raw_chem_name,raw_min_comp,raw_central_comp,raw_max_comp,unit_type'
        content = list(i.decode('utf-8') for i in resp.streaming_content)
        self.assertIn(field_list, content[1])
/n/n/ndashboard/tests/functional/test_datagroup_filesystem.py/n/nfrom django.urls import resolve
from django.test import RequestFactory, TestCase, Client
from django.http import HttpRequest
from dashboard import views
from dashboard.models import *
from factotum import settings
from django.core.files.uploadedfile import InMemoryUploadedFile, SimpleUploadedFile
import tempfile, csv, os, io, errno
from django.contrib.auth.models import User
from dashboard.tests.loader import fixtures_standard


def build_datagroup_folder(dirname, ):
    fullpath = f'{settings.MEDIA_ROOT}/{dirname}'
    try:
        os.makedirs(fullpath)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    pdfpath = f'{settings.MEDIA_ROOT}/{dirname}/pdf'
    try:
        os.makedirs(pdfpath)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    csv_string = (""filename,title,document_type,url,organization\n""
                ""0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf,NUTRA NAIL,1,, \n""
                ""0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf,Body Cream,1,, \n"")

    with open(f'{fullpath}/British_Petroleum_Air_1_British_Petroleum_Air_1_register_recor_EHDBt5f.csv', 'w') as rr_csv:
        rr_csv.write(csv_string)
        rr_csv.close()


class DataGroupFileDownloadTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.c = Client()
        self.factory = RequestFactory()
        self.c.login(username='Karyn', password='specialP@55word')

    def tearDown(self):
        # clean up the file system by deleting the data group object
        dg = DataGroup.objects.get(pk=6)
        dg.delete()

    def test_old_path(self):
        '''
        Before we switched to UUIDs for the data group media folders,
        the paths were based on the original name of the data group
        '''
        testpath = 'British_Petroleum_(Air)_1'
        build_datagroup_folder(testpath)
        # the get_dg_folder() method should be able to find the newly-created directory
        dg = DataGroup.objects.get(pk=6)
        self.assertEqual(testpath, dg.get_dg_folder().rsplit('/')[-1],
        'The get_dg_folder() method should have returned the newly created directory')

    def test_get_dg_folder(self):
        '''
        The dev environment does not contain folders for most of the datagroups,
        so this just tests whether the methods can return their messages without
        errors.
        '''
        for dg in DataGroup.objects.all():
            with self.assertRaises(Exception):
                try:
                    folderpath = dg.get_dg_folder()
                    zippath = dg.get_zip_url()
                except:
                    pass
                else:
                    raise Exception

/n/n/ndashboard/tests/functional/test_datasource_index.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DataSourceTestWithFixtures(TestCase):
    fixtures = fixtures_standard
    
    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_extracted_counts(self):
        response = self.client.get('/datasources/').content.decode('utf8')
        self.assertIn('Extracted', response,
                      'The Extracted document count should be in the page after ticket 758')
        response_html = html.fromstring(response)
        ext_table_count = int(response_html.xpath(""//*[@id='sources']/tbody/tr[contains(., 'Airgas')]/td[4]"")[0].text)
        ext_orm_count = ExtractedText.objects.filter(data_document__data_group__data_source__title='Airgas').count()
        self.assertEqual(ext_table_count, ext_orm_count,
                         'The number of extracted records shown for Airgas should match what the ORM returns')

/n/n/ndashboard/tests/functional/test_extracted_qa.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import QAGroup, ExtractedText



class ExtractedQaTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_group_creation(self):
        # test the assignment of a qa_group to extracted text objects
        pk = self.objects.extext.pk
        self.assertIsNone(self.objects.extext.qa_group)
        self.assertEqual(len(QAGroup.objects.all()),0)
        pk = self.objects.extext.extraction_script.pk
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        self.assertEqual(response.status_code,200)
        qa_group = QAGroup.objects.get(
                        extraction_script=self.objects.extext.extraction_script)
        ext = ExtractedText.objects.get(qa_group=qa_group)
        self.assertIsNotNone(ext.qa_group)
        response = self.client.get(f'/qa/extractedtext/{ext.pk}/')

    def test_qa_approval_redirect(self):
        # first need to create a QAGroup w/ this get request.
        self.client.get(f'/qa/extractionscript/{self.objects.exscript.pk}/')
        pk = self.objects.extext.pk
        response = self.client.post(f'/qa/extractedtext/{pk}/',{'approve':[47]})
        self.assertEqual(response.url, '/qa/extractionscript/',(""User should be redirected to ""
                                ""QA homepage after last extext is approved.""))
/n/n/ndashboard/tests/functional/test_get_data.py/n/nfrom django.urls import resolve
from django.test import TestCase, override_settings
from django.test.client import Client

from django.contrib.auth import authenticate
from django.contrib.auth.models import User
from dashboard.models import PUC, Product, ProductToPUC, ProductDocument, DSSToxLookup
from dashboard.views.get_data import *
from django.test import TestCase
from django.test.client import Client

from dashboard.views.get_data import *
from dashboard.tests.loader import fixtures_standard


# from dashboard import views
# from django.urls import resolve
# from django.contrib.auth import authenticate
# from django.contrib.auth.models import User

@override_settings(ALLOWED_HOSTS=['testserver'])
class TestGetData(TestCase):

    fixtures = fixtures_standard

    def setUp(self):
        self.client = Client()

    def test_dtxsid_pucs_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(0, ethylparaben_stats['pucs_n'])

        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        # get one of the products that was just linked to a data document with DTXSID9022528 in its extracted chemicals
        pid = dd.products.first().pk
        puc = PUC.objects.get(id=20)
        # add a puc to one of the products containing ethylparaben

        ppuc = ProductToPUC.objects.create(product=Product.objects.get(pk=pid),
                                           puc=puc,
                                           puc_assigned_usr=User.objects.get(username='Karyn'))
        ppuc.refresh_from_db()
        stats = stats_by_dtxsids(dtxs)
        # select out the stats for one DTXSID, ethylparaben
        ethylparaben_stats = stats.get(sid='DTXSID9022528')
        self.assertEqual(1, ethylparaben_stats['pucs_n'])

    def test_dtxsid_dds_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            2, ethylparaben_stats['dds_n'], 'There should be 2 datadocuments associated with ethylaraben')
        # change the number of related data documents by deleting one
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))

        dd = dds[0]
        dd.delete()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(
            1, ethylparaben_stats['dds_n'], 'There should now be 1 datadocument associated with ethylaraben')

    def test_dtxsid_dds_wf_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['dds_wf_n'], 'There should be 1 extracted chemical \
        with weight fraction data associated with ethylparaben')
        # add weight fraction data to a different extractedchemical
        ec = ExtractedChemical.objects.get(rawchem_ptr_id=73)
        ec.raw_min_comp = 0.1
        ec.save()
        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(2, ethylparaben_stats['dds_wf_n'], 'There should be 2 extracted chemicals \
        with weight fraction data associated with ethylparaben')

    def test_dtxsid_products_n(self):
        dtxs = [""DTXSID9022528"", ""DTXSID1020273"",
                ""DTXSID6026296"", ""DTXSID2021781""]
        # Functional test: the stats calculation
        stats = stats_by_dtxsids(dtxs)

        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e

        self.assertEqual(0, ethylparaben_stats['products_n'], 'There should be 0 products \
        associated with ethylparaben')
        self.client.login(username='Karyn', password='specialP@55word')
        # get the associated documents for linking to products
        dds = DataDocument.objects.filter(pk__in=ExtractedChemical.objects.filter(dsstox__sid='DTXSID9022528').
                                          values('extracted_text__data_document'))
        dd = dds[0]

        ds = dd.data_group.data_source
        p = Product.objects.create(data_source=ds, title='Test Product',
                                   upc='Test UPC for ProductToPUC')
        pd = ProductDocument.objects.create(document=dd, product=p)
        pd.save()
        dd.refresh_from_db()

        stats = stats_by_dtxsids(dtxs)
        for e in stats:
            if e['sid'] == 'DTXSID9022528':
                ethylparaben_stats = e
        self.assertEqual(1, ethylparaben_stats['products_n'], 'There should now be 1 product \
        associated with ethylparaben')

    def test_habits_and_practices_cards(self):
        data = {'puc': ['2']}
        response = self.client.post('/get_data/', data=data)
        for hnp in [b'ball bearings',
                    b'motorcycle',
                    b'vitamin a&amp;d',
                    b'dish soap']:
            self.assertIn(hnp, response.content)

    def test_download_pucs_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download PUCs')

    def test_download_raw_chem_button(self):
        response = self.client.get('/get_data/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Download Uncurated Chemicals')
        # Pick one curated and one non-curated RawChem record, and
        # confirm that the downloaded file excludes and includes them,
        # respectively.

        rc = RawChem.objects.filter(dsstox_id__isnull=True).first()
        response = self.client.get('/dl_raw_chems/')
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The non-curated row should appear')
        # The downloaded file should include the data group id of each uncurated chemical
        rc_row = f'%s,%s,%s,%s,%s\r\n' % (rc.extracted_text.data_document.data_group.id,
                                          rc.id, rc.raw_cas, rc.raw_chem_name, rc.rid if rc.rid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertIn(rc_row, response.content,
                      'The data group id should be in the output')

        rc = RawChem.objects.filter(dsstox_id__isnull=False).first()
        rc_row = f'%s,%s,%s,%s\r\n' % (
            rc.id, rc.raw_cas, rc.raw_chem_name, rc.sid if rc.sid else '')
        rc_row = bytes(rc_row, 'utf-8')
        self.assertNotIn(rc_row, response.content,
                         'The curated row should not appear')


/n/n/ndashboard/tests/functional/test_nav_bar.py/n/nfrom django.urls import resolve
from django.test import TestCase
from django.test.client import Client
from django.http import HttpRequest
from dashboard.tests.loader import load_model_objects
from dashboard import views
from lxml import html


class NavBarTest(TestCase):
    '''this group of tests checks to see that the URL resolves to the
    appropriate view function.
    '''
    def setUp(self):
        self.objects = load_model_objects()
        self.client = Client()

    def test_home_page_returns_correct_html(self):
        # we need Karyn in the DB in order to log her in.
        # load_model_objects returns a `dot_notation` dict which we can
        # use all of the model objects from, seen in the print stmnt below.
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/')
        html = response.content.decode('utf8').rstrip()
        self.assertTrue(html.startswith('<!DOCTYPE html>'))
        self.assertIn('<title>factotum</title>', html)
        self.assertTrue(html.endswith('</html>'))

    def test_index_link(self):
        found = resolve('/')
        self.assertEqual(found.func, views.index)

    def test_data_sources_link(self):
        found = resolve('/datasources/')
        self.assertEqual(found.func, views.data_source_list)

    def test_product_curation_link(self):
        found = resolve('/product_curation/')
        self.assertEqual(found.func, views.product_curation_index)

    def test_qa_link(self):
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_get_data_without_auth(self):
        # the Get Data menu item should be available to a user who isn't logged in
        response = self.client.get('/')
        self.assertContains(response, 'Get Data')
        response = self.client.get('/get_data/')
        self.assertContains(response, 'Summary metrics by chemical')

    def test_data_curation(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('Data Curation',
                      response_html.xpath('string(//*[@id=""navbarDataCurationDropdownMenuLink""]/text())'),
                      'The Data Curation dropdown should appear in the navbar.')

/n/n/ndashboard/tests/functional/test_product_linkage.py/n/nfrom django.test import TestCase, override_settings
from dashboard.tests.loader import *
from dashboard.views.product_curation import ProductLinkForm
from lxml import html


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestProductLinkage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_datatype_update(self):
        self.assertTrue(ProductLinkForm().fields['document_type'],
                            'ProductLinkForm must include a document_type select input')
        dd = DataDocument.objects.get(pk=155324)
        dd.document_type_id = 1
        dd.save()
        self.assertEqual(dd.document_type_id, 1,
                         'DataDocument 155324 must have a document_type_id of 1 for test to function')
        response = self.client.post(f'/link_product_form/155324/',
                                    {'title': 'x',
                                     'manufacturer': '',
                                     'brand_name': '',
                                     'upc': 'none',
                                     'size': '',
                                     'color': '',
                                     'document_type': 2,
                                     'return_url': 'required'})
        dd.refresh_from_db()
        self.assertEqual(dd.document_type_id, 2,
                         'DataDocument 155324 should have a final document_type_id of 2')

    def test_datatype_options(self):
        # retrieve a sample datadocument
        dd = DataDocument.objects.get(pk=129298)

        # configure its datagroup to be of group type ""composition""
        dg = DataGroup.objects.get(pk=dd.data_group_id)
        dg.group_type_id = 2
        dg.save()

        response = self.client.get(f'/link_product_form/{str(dd.pk)}/').content.decode('utf8')
        response_html = html.fromstring(response)

        self.assertTrue(response_html.xpath('string(//*[@id=""id_document_type""]/option[@value=""5""])'),
                      'Document_type_id 5 should be an option when the datagroup type is composition.')

        self.assertFalse(response_html.xpath('string(//*[@id=""id_document_type""]/option[@value=""6""])'),
                      'Document_type_id 6 should NOT be an option when the datagroup type is composition.')

    def test_bulk_create_products(self):
        # DataGroup 19 is a Composition dg with unlinked products
        dg = DataGroup.objects.get(pk=19)
        response = self.client.get(f'/datagroup/19/')
        self.assertEqual(response.context['bulk'], 75,
                'Not all DataDocuments linked to Product, bulk_create needed')
        response = self.client.post(f'/datagroup/19/',{'bulk':75})
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        # pick documents and check the attributes of their now-related products
        # 1: check a case where the ExtractedText record had a prod_name to offer
        ets = ExtractedText.objects.filter(data_document__data_group=dg)
        et = ets.filter(prod_name__isnull=False).first()
        doc = DataDocument.objects.get(pk=et.data_document_id)
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, et.prod_name,
                                        'Title should be taken from ExtractedText.prod_name in bulk_create')
        # 2: check a case where ExtractedText.prod_name is None
        et = ets.filter(prod_name__isnull=True).first()
        doc = DataDocument.objects.get(pk=et.data_document_id)
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, '%s stub' % doc.title ,
                                        ('Title should be taken from the DataDocument.title in bulk_create,'
                                        'with ""stub"" appended') )
        # 3: check a case where the ExtractedText doesn't exist
        # Data Group 6 has 2 docs that are linked to Product and ExtractedText records
        dg = DataGroup.objects.get(pk=6)
        docs = DataDocument.objects.filter(data_group=dg)
        doc_list = docs.values_list('id')
        # delete the ExtractedText links and recreate them via the bulk request
        ExtractedText.objects.filter(data_document_id__in=doc_list).delete()
        # delete the ProductDocument links and recreate them via the bulk request
        ProductDocument.objects.filter(document_id__in=doc_list).delete()
        response = self.client.get(f'/datagroup/6/')
        self.assertEqual(response.context['bulk'], 2,
                'Not all DataDocuments linked to Product, bulk_create needed')
        response = self.client.post(f'/datagroup/6/',{'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        # check the titles of the newly-created products
        # they should be based on the document title
        doc = docs.first()
        prod = Product.objects.filter(documents__in=[doc]).first()
        self.assertEqual('%s stub' % doc.title, prod.title,
                'Product and DataDocument titles should be the same.')
        

/n/n/ndashboard/tests/functional/test_qa.py/n/nimport time
from django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import DataDocument, Script, ExtractedText
from lxml import html


class QATest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_scoreboard(self):
        response = self.client.get(
            '/qa/extractionscript/').content.decode('utf8')
        response_html = html.fromstring(response)

        row_count = len(response_html.xpath(
            '//table[@id=""extraction_script_table""]/tbody/tr'))
        scriptcount = Script.objects.filter(script_type='EX').count()
        self.assertEqual(scriptcount, row_count, ('The seed data contains 1 '
                                                  'Script object with the script_type'
                                                  'EX, which should appear in this table'))

        script_url = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[1]/a/@href')[0]
        self.assertEqual(script_url, 'http://www.epa.gov/',
                         'The URL on the page should be the external link to the script.')

        displayed_doc_count = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[2]')[0].text
        model_doc_count = DataDocument.objects.filter(
            extractedtext__extraction_script=self.objects.exscript.pk).count()

        self.assertEqual(displayed_doc_count, str(model_doc_count),
                         ('The displayed number of datadocuments should match '
                          'the number whose related extracted text objects used '
                          ' the extraction script'))

        displayed_pct_checked = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[' + str(row_count) + ']/td[3]')[0].text
        model_pct_checked = self.objects.exscript.get_pct_checked()
        self.assertEqual(displayed_pct_checked, model_pct_checked,
                         ('The displayed percentage should match what is derived from the model'))

        es = self.objects.exscript
        self.assertEqual(es.get_qa_complete_extractedtext_count(), 0,
                         ('The ExtractionScript object should return 0 qa_checked ExtractedText objects'))

        # Set qa_checked property to True for one of the ExtractedText objects
        self.assertEqual(self.objects.extext.qa_checked, False)
        self.objects.extext.qa_checked = True
        self.objects.extext.save()
        self.assertEqual(es.get_qa_complete_extractedtext_count(), 1,
                         ('The ExtractionScript object should now return 1 qa_checked ExtractedText object'))

        # A button for each row that will take you to the script's QA page
        script_qa_link = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[contains(.,""Test Extraction Script"")]/td[4]/a/@href')[0]
        self.assertIn(
            f'/qa/extractionscript/{str(self.objects.exscript.pk)}/', script_qa_link)

        # Before clicking the link, the script's qa_done property should be false
        self.assertEqual(es.qa_begun, False,
                         'The qa_begun property of the Script should be False')

        # The link should open a page where the h1 text matches the title of the Script
        response = self.client.get(script_qa_link).content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn(es.title, response_html.xpath('/html/body/div/h1/text()')[0],
                      'The <h1> text should equal the .title of the Script')

        # Opening the ExtractionScript's QA page should set its qa_begun property to True
        es.refresh_from_db()
        self.assertEqual(es.qa_begun, True,
                         'The qa_begun property of the ExtractionScript should now be True')

        # Go back to the QA index page to confirm that the QA is complete
        response = self.client.get(
            '/qa/extractionscript/').content.decode('utf8')
        response_html = html.fromstring(response)
        script_qa_status = response_html.xpath(
            '//*[@id=""extraction_script_table""]/tbody/tr[contains(.,""Test Extraction Script"")]/td[4]/text()')[0]
        str_qa_complete = 'QA Complete'
        self.assertIn(str_qa_complete, script_qa_status,
                      'The QA Status field should now say ""QA Complete"" instead of ""Begin QA""')
/n/n/ndashboard/tests/functional/test_qa_seed_data.py/n/nfrom django.test import Client
from dashboard.tests.loader import *
from django.test import TestCase, override_settings, RequestFactory
from dashboard.models import DataDocument, Script, ExtractedText, ExtractedChemical, QAGroup
from django.db.models import Count


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestQaPage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_begin(self):
        """"""
        Check that starting the QA process flips the variable on the Script
        """"""
        self.assertFalse(Script.objects.get(pk=5).qa_begun,
                         'The Script should have qa_begun of False at the beginning')
        response = self.client.get('/qa/extractionscript/5/')
        self.assertTrue(Script.objects.get(pk=5).qa_begun,
                        'qa_begun should now be true')

    def test_new_qa_group_urls(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 15 has one ExtractedText object
        pk = 15
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertIn(f'/qa/extractedtext/{et.pk}/'.encode(), response.content)
        # After opening the URL, the following should be true:
        # One new QA group should be created
        group_count = QAGroup.objects.filter(extraction_script_id=pk).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(Script.objects.get(pk=15).qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        group_pk = QAGroup.objects.get(extraction_script_id=pk).pk
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertTrue(et.qa_group_id == group_pk)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/15/\'> Continue QA"".encode(), response.content)

    def test_qa_script_without_ext_text(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 9 has no ExtractedText objects
        pk = 9
        # a user will see no link on the QA index page, but it's still
        # possible to enter the URL
        response = self.client.get(f'/qa/extractionscript/{pk}/', follow=True)
        self.assertEqual(response.status_code, 200)

    def test_data_document_qa(self):
        # Open the QA page for a Composition ExtractedText record that has no QA group
        # and is in a Script with < 100 documents
        scr = Script.objects.annotate(num_ets=Count('extractedtext')).filter(
            num_ets__lt=100).filter(script_type='EX').first()
        pk = ExtractedText.objects.filter(qa_group=None).filter(extraction_script=scr
                                                                ).filter(
            data_document__data_group__group_type__code='CO').first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')

        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        group_count = QAGroup.objects.filter(extraction_script=scr).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(scr.qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        new_group = QAGroup.objects.get(extraction_script=scr)
        et = ExtractedText.objects.get(pk=pk)
        self.assertTrue(et.qa_group == new_group)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/{scr.pk}/\'> Continue QA"".encode(), response.content)

        # Open the QA page for an ExtractedText record that has no QA group and
        # is related to a script with over 100 documents
        scr = Script.objects.annotate(num_ets=Count(
            'extractedtext')).filter(num_ets__gt=100).first()
        pk = ExtractedText.objects.filter(extraction_script=scr).first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        new_group = QAGroup.objects.get(extraction_script=scr)

        # There should be a lot of ExtractedText records assigned to the QA Group
        initial_qa_count = ExtractedText.objects.filter(
            qa_group=new_group).count()
        self.assertTrue(initial_qa_count > 100)

        # Select a document that shares a Script with the
        # QA Group created above BUT DOES NOT BELONG TO THE QA GROUP
        pk = ExtractedText.objects.filter(
            extraction_script_id=scr.id).filter(qa_group=None).first().pk
        # Open its QA page via the /datdocument/qa path
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        # Make sure that the number of documents in the QA Group has increased
        self.assertGreater(ExtractedText.objects.filter(
            qa_group=new_group).count(), initial_qa_count)

    def test_habitsandpractices(self):
        # Begin from the QA index page
        response = self.client.get(f'/habitsandpractices/54/')
        self.assertContains(response, '<b>Add New Habit and Practice</b>')

    def test_dd_link(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractedtext/5', follow=True)
        self.assertIn(b'/datadocument/5', response.content)

    def test_approval(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Follow the first approval link
        response = self.client.get('/qa/extractedtext/7', follow=True)
        # print(response.context['extracted_text'])

    def test_hidden_fields(self):
        '''ExtractionScript 15 includes a functional use data group with pk = 5.
        Its QA page should hide the composition fields '''
        # Create the QA group by opening the Script's page
        response = self.client.get('/qa/extractionscript/15/', follow=True)
        # Open the DataGroup's first QA approval link
        response = self.client.get('/qa/extractedtext/5/', follow=True)
        # A raw_cas field should be in the page
        self.assertIn(
            b'<input type=""text"" name=""rawchem-1-raw_cas""', response.content)
        # There should not be any unit_type field in the functional use QA display
        self.assertNotIn(
            b'<input type=""text"" name=""rawchem-1-unit_type""', response.content)
        # The values shown should match the functional use record, not the chemical record
        self.assertIn(b'Functional Use Chem1', response.content)

        # Go back to a different ExtractionScript
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Open the QA page for a non-FunctionalUse document
        response = self.client.get('/qa/extractedtext/7/', follow=True)
        # This page should include a unit_type input form
        self.assertIn(b'rawchem-1-unit_type', response.content)

    def test_cpcat_qa(self):
        # Begin from the Chemical Presence QA index page
        response = self.client.get(f'/qa/chemicalpresence/')
        self.assertIn(
            f""/qa/chemicalpresencegroup/49/\'> View Chemical Presence Lists"".encode(), response.content)

        response = self.client.get(
            f'/qa/chemicalpresencegroup/49', follow=True)
        # The table should include the ""Begin QA"" link
        self.assertIn(
            f'/qa/extractedtext/254781/""> Begin QA'.encode(), response.content)

        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 0)
        response = self.client.get(f'/qa/extractedtext/254781/', follow=True)
        # Navigating to the extractedtext QA page should cause
        # the sampled child records to be flagged with qa_flag=True
        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 30)

        # The QA page should only show the flagged records
        elp_flagged = elps.filter(qa_flag=True).first()
        self.assertIn(elp_flagged.raw_cas.encode(), response.content)

        elp_not_flagged = elps.filter(qa_flag=False).first()
        self.assertNotIn(elp_not_flagged.raw_cas.encode(), response.content)

    def test_every_extractedtext_qa(self):
        # Attempt to open a QA page for every ExtractedText record
        for et in ExtractedText.objects.all():
            response = self.client.get(f'/qa/extractedtext/%s' % et.data_document_id, follow=True)
            if response.status_code != 200:
                print(et.data_document_id)
            self.assertEqual(response.status_code, 200)/n/n/ndashboard/tests/integration/test_browser_edits.py/n/nfrom lxml import html

from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.staticfiles.testing import StaticLiveServerTestCase

from dashboard.models import *
from selenium import webdriver
from django.conf import settings
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

            # Data Document Detail Page
            doc_detail_link = f'/datadocument/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_detail_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            doc_type_select.select_by_visible_text(""MSDS"")
            self.assertIn(doc_detail_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note
        5. Approve 
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781, # Chemical Presence List
                       354783, # HHE Report 
                       ]: 
            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()

            # Modify the first raw_chem_name field's value
            #  
            raw_chem = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_chem_name""]')
            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')
            # print(rawchem_id)

            raw_chem_name_field.send_keys(' edited')
            # save changes
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            # print(et.data_document.data_group.group_type)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm validation failure
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            # The QA notes field should be invalid
            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_qa_notes""]')
            self.assertIn('is-invalid', qa_notes_field.get_attribute('class'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')



/n/n/ndashboard/tests/integration/test_user_experience.py/n/nfrom lxml import html
from django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import *
import os
import csv
import time
import unittest
import collections
import json
import re
from selenium import webdriver
from selenium.webdriver.support.select import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from django.conf import settings
from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from dashboard.models import *


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestIntegration(StaticLiveServerTestCase):

    def setUp(self):
        self.objects = load_model_objects()
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_hem(self):
        for i in range(27):
            ds = DataSource.objects.create(title=f'Test_DS_{i}')
        list_url = self.live_server_url + '/datasources/'
        self.browser.get(list_url)
        row_count = len(self.browser.find_elements_by_xpath(""//table[@id='sources']/tbody/tr""))
        self.assertEqual(row_count, 25, 'Should be 25 datasources in the table')
        # go to edit page from datasource list
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")
        detail_url = self.live_server_url + f'/datasource/{ds.pk}'
        self.browser.get(detail_url)
        # go to edit page from datasource detail
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), detail_url,
                         ""User should go back to detail view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

        num_pucs = len(PUC.objects.filter(kind='FO'))
        self.browser.get(self.live_server_url)
        import time
        time.sleep(3)  # or however long you think it'll take you to scroll down to bubble chart
        bubbles = self.browser.find_elements_by_class_name('bubble')
        self.assertEqual(num_pucs, len(bubbles), ('There should be a circle'
                                                  'drawn for every PUC'))

    def test_datagroup(self):
        list_url = self.live_server_url + '/datagroups/'
        self.browser.get(list_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")

        dg = DataGroup.objects.first()
        ds_detail_url = f'{self.live_server_url}/datasource/{dg.data_source.pk}'
        self.browser.get(ds_detail_url)
        self.browser.find_elements_by_xpath('//*[@title=""edit""]')[1].click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), ds_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        dg_detail_url = f'{self.live_server_url}/datagroup/{dg.pk}/'
        self.browser.get(dg_detail_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), dg_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        edit_url = f'{self.live_server_url}/datagroup/edit/{dg.pk}/'
        self.browser.get(edit_url)
        self.browser.find_element_by_name('cancel').click()
        self.assertIn('/datagroups/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

    def test_product(self):
        p = self.objects.p
        puc = self.objects.puc
        tag = self.objects.pt
        PUCToTag.objects.create(content_object=puc, tag=tag)
        ProductToPUC.objects.create(product=p, puc=puc)
        url = self.live_server_url + f'/product/{p.pk}/'
        self.browser.get(url)
        submit = self.browser.find_element_by_id('tag_submit')
        self.assertFalse(submit.is_enabled(), ""Button should be disabled"")
        tag = self.browser.find_element_by_class_name('taggit-tag')
        tag.click()
        self.assertTrue(submit.is_enabled(), ""Button should be enabled"")

    def test_field_exclusion(self):
        doc = self.objects.doc
        # The element should not appear on the QA page
        qa_url = self.live_server_url + f'/qa/extractedtext/{doc.pk}/'
        self.browser.get(qa_url)
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_cas""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_chemname""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-SID""]')
        # make sure the test can pick up one that should be there
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-raw_cas""]')
        except NoSuchElementException:
            self.fail(""Absence of raw_cas element raised exception"")

        # The element should appear on the datadocument page
        dd_url = self.live_server_url + f'/datadocument/{doc.pk}/'
        self.browser.get(dd_url)
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        except NoSuchElementException:
            self.fail(""Absence of weight_fraction_type element raised exception"")

/n/n/ndashboard/tests/loader.py/n/nfrom django.utils import timezone
from django.contrib.auth.models import User

from dashboard.models import *

fixtures_standard = [ '00_superuser',
                      '01_lookups',
                      '02_datasource',
                      '03_datagroup',
                      '04_PUC', 
                      '05_product',
                      '06_datadocument',
                      '07_rawchem_etc',
                       '08_script',
                    '09_productdocument',  
                    '10_habits_and_practices',
                     '11_habits_and_practices_to_puc',
                      '12_product_to_puc',
                        '13_puc_tag'
                        ]

class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

def load_model_objects():
    user = User.objects.create_user(username='Karyn',
                                        password='specialP@55word')
    superuser = User.objects.create_superuser(username='SuperKaryn',
                                              password='specialP@55word',
                                              email='me@epa.gov')
    ds = DataSource.objects.create(title='Data Source for Test',
                                        estimated_records=2, state='AT',
                                        priority='HI')
    script = Script.objects.create(title='Test Download Script',
                                        url='http://www.epa.gov/',
                                        qa_begun=False, script_type='DL')
    exscript = Script.objects.create(title='Test Extraction Script',
                                   url='http://www.epa.gov/',
                                   qa_begun=False, script_type='EX')
    gt = GroupType.objects.create(title='Composition', code='CO')
    dg = DataGroup.objects.create(name='Data Group for Test',
                                        description='Testing...',
                                        data_source = ds,
                                        download_script=script,
                                        downloaded_by=user,
                                        downloaded_at=timezone.now(),
                                        group_type=gt,
                                        csv='register_records_matching.csv',
                                        url='https://www.epa.gov')
    dt = DocumentType.objects.create(title='MSDS',
                                    code='MS', group_type=gt)

    doc = DataDocument.objects.create(title='test document',
                                            data_group=dg,
                                            document_type=dt,
                                            filename='example.pdf')
    p = Product.objects.create(data_source=ds,
                                upc='Test UPC for ProductToPUC')

    puc = PUC.objects.create(gen_cat='Test General Category',
                              prod_fam='Test Product Family',
                              prod_type='Test Product Type',
                             description='Test Product Description',
                             last_edited_by = user,
                             kind='FO')

    extext = ExtractedText.objects.create(
                                    prod_name='Test Extracted Text Record',
                                    data_document=doc,
                                    extraction_script=exscript
                                    )
    ut = UnitType.objects.create(title='percent composition')
    wft = WeightFractionType.objects.create(title= 'reported', description= 'reported')
    ec = ExtractedChemical.objects.create(extracted_text=extext,
                                        unit_type=ut,
                                        weight_fraction_type = wft,
                                        raw_chem_name= 'Test Chem Name',
                                        raw_cas='test_cas'
                                        )
    rc = ec.rawchem_ptr
    ing = Ingredient.objects.create(lower_wf_analysis = 0.123456789012345,
                                    central_wf_analysis = 0.2,
                                    upper_wf_analysis = 1,
                                    script = script,
                                    rawchem_ptr = rc)
    
    pt = PUCTag.objects.create(name=""Test PUC Attribute"")
    pd = ProductDocument.objects.create(product=p, document=doc)
    ehp = ExtractedHabitsAndPractices.objects.create(extracted_text=extext,
                                                     product_surveyed='Test Product Surveyed',
                                                     prevalence='Continuous')


    return dotdict({'user':user,
                    'superuser':superuser,
                    'ds':ds,
                    'script':script,
                    'exscript':exscript,
                    'dg':dg,
                    'doc':doc,
                    'p':p,
                    'puc':puc,
                    'extext':extext,
                    'ut':ut,
                    'wft':wft,
                    'rc':rc,
                    'ec':ec,
                    'pt':pt,
                    'pd':pd,
                    'ing':ing,
                    'dt':dt,
                    'gt':gt,
                    'ehp':ehp
                    })
/n/n/ndashboard/tests/unit/test_models.py/n/nimport csv
from django.utils import timezone
from django.test import TestCase
from django.db.models import Count
from dashboard.models import *
from dashboard.tests.loader import *
from django.db.models import Q


def create_data_documents(data_group, source_type, pdfs):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdfs,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


def create_data_documents_with_txt(data_group, source_type, pdf_txt):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching_with_txt.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdf_txt,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


class ModelsTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')
        self.pdfs = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf']
        self.pdf_txt = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0bf5755e-3a08-4024-9d2f-0ea155a9bd17.txt']

    def test_object_creation(self):
        self.assertTrue(isinstance(self.objects.ds, DataSource))
        self.assertTrue(isinstance(self.objects.script, Script))
        self.assertTrue(isinstance(self.objects.extext, ExtractedText))
        self.assertTrue(isinstance(self.objects.ec, ExtractedChemical))
        self.assertTrue(isinstance(self.objects.ing, Ingredient))
        self.assertTrue(isinstance(self.objects.p, Product))
        self.assertTrue(isinstance(self.objects.pd, ProductDocument))
        self.assertTrue(isinstance(self.objects.pt, PUCTag))

    def test_datagroup(self):
        self.assertTrue(isinstance(self.objects.dg, DataGroup))

        self.assertEqual(str(self.objects.dg), self.objects.dg.name)
        self.assertEqual('https://www.epa.gov', self.objects.dg.url)

    def test_object_properties(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)
        self.assertTrue(hasattr(PUCToTag,'assumed'))
        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents(self.objects.dg,self.objects.st, self.pdfs)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdfs), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded pdf
        fn = docs[0].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))
        # DownloadScript
        self.assertEqual(str(self.objects.script), 'Test Download Script')
        # ExtractedText
        self.assertEqual(str(self.objects.extext),
                                    'test document')
        # RawChem
        self.assertEqual(str(self.objects.rc), 'Test Chem Name')
        # ExtractedChemical
        self.assertEqual(str(self.objects.ec), 'Test Chem Name')

    def test_product_attribute(self):
        self.assertEqual(ProductToTag.objects.count(), 0)
        p2t = ProductToTag.objects.create(content_object=self.objects.p,
                                            tag=self.objects.pt)
        self.assertEqual(ProductToTag.objects.count(), 1)

    def test_data_group(self):
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        self.assertFalse(self.objects.dg.all_matched())
        self.assertFalse(self.objects.dg.all_extracted())
        doc.matched = True
        doc.save()
        self.assertFalse(self.objects.dg.all_matched())
        self.objects.doc.matched = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_matched())
        doc.extracted = True
        doc.save()
        self.assertFalse(self.objects.dg.all_extracted())
        self.objects.doc.extracted = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_extracted())

    def test_extracted_habits_and_practices(self):
        puc2 = PUC.objects.create(gen_cat='Test General Category',
                                 prod_fam='Test Product Family',
                                 prod_type='Test Product Type',
                                 description='Test Product Description',
                                 last_edited_by = self.objects.user)
        self.assertEqual(ExtractedHabitsAndPractices.objects.count(), 1)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 0)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=self.objects.puc)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=puc2)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 2)

    def test_data_document_organization(self):
        self.assertEqual(self.objects.doc.organization, '')
        self.objects.doc.organization = 'Test Organization'
        self.objects.doc.save()
        self.assertEqual(DataDocument.objects.filter(organization='Test Organization').count(), 1)

    def test_data_document_filename(self):
        pk = self.objects.doc.pk
        self.assertEqual(self.objects.doc.get_abstract_filename(),
                        f'document_{pk}.pdf',
                        'This is used in the FileSystem naming convention.')

    def test_dg_with_txt(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)

        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents_with_txt(self.objects.dg,self.objects.st, self.pdf_txt)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdf_txt), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded text file
        fn = docs[1].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))

    def test_script_fields(self):
        fields = ['title','url','qa_begun','script_type','confidence']
        for fld in fields:
            self.assertIn(fld, Script.__dict__, (f'{fld} '
                                                'should be in Script model.'))


class PUCModelTest(TestCase):

    fixtures = fixtures_standard

    def test_puc_fields(self):
        fields = ['kind','gen_cat','prod_fam','prod_type','description',
                'last_edited_by','products','extracted_habits_and_practices',
                'tags']
        for fld in fields:
            self.assertIn(fld, PUC.__dict__, f'{fld} should be in PUC model.')

    def test_get_the_kids(self):
        '''Level 1 and 2 PUCs should accumulate lower level PUCs.
        '''
        puc = PUC.objects.get(pk=20) # PUC w/ only gen_cat value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=6) # PUC w/ gen_cat and prod_fam value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=126) # PUC w/ ALL values
        self.assertEqual(len(puc.get_the_kids()),1, ('PUC should only have '
                                                        'itself associated'))

    def test_puc_category_defaults(self):
        '''Assert that the prod_fam and prod_type are nulled w/ an
        empty string and not NULL.
        '''
        k = User.objects.get(username='Karyn')
        puc = PUC.objects.create(last_edited_by=k)
        self.assertTrue(puc.prod_fam == '')
        self.assertTrue(puc.prod_type == '')

    def test_product_counts(self):
        '''Make sure the product_count property
        returns the same thing as the num_products annotation'''
        pucs = PUC.objects.all().annotate(num_products=Count('products'))
        # pucs 1-3 have products associated with them
        self.assertEqual(pucs.get(pk=1).num_products , PUC.objects.get(pk=1).product_count)


class DataGroupFilesTest(TestCase):

    fixtures = fixtures_standard

    def test_filefield_properties(self):
        dg5 = DataGroup.objects.get(pk=5) # this datagroup has no csv value
        dg6 = DataGroup.objects.get(pk=6) # this one has a csv value, but no file
        dg50 = DataGroup.objects.get(pk=50) # this one has a /media/ folder

        # All of the falsy properties should return False rather than errors
        self.assertFalse(dg5.dg_folder)
        self.assertFalse(dg5.zip_url)

        self.assertFalse(dg6.dg_folder)
        self.assertFalse(dg6.zip_url)

        # 50 is the only datagroup that has a linked file in the /media folder
        self.assertTrue(dg50.dg_folder == dg50.get_dg_folder())
        self.assertFalse(dg50.zip_url)


class DataDocumentTest(TestCase):

    fixtures = fixtures_standard

    def test_datadocument_note(self):

        datadocument = DataDocument(filename=""MyFile.pdf"",
                                    title=""My Title"",
                                    data_group=DataGroup.objects.first(),
                                    note=""Some long note."")
        datadocument.save()
        self.assertTrue(datadocument.note, ""Some long note."")






/n/n/ndashboard/urls.py/n/nfrom django.urls import include, path
from django.conf import settings
from django.conf.urls.static import static

import dashboard.views.qa
from . import views

urlpatterns = [
    path('', views.index,                   name='index'),
    path('datasources/', views.data_source_list,
                                            name='data_source_list'),
    path('datasource/<int:pk>', views.data_source_detail,
                                            name='data_source_detail'),
    path('datasource/new/', views.data_source_create,
                                            name='data_source_new'),
    path('datasource/edit/<int:pk>/', views.data_source_update,
                                            name='data_source_edit'),
    path('datasource/delete/<int:pk>/', views.data_source_delete,
                                            name='data_source_delete'),
    path('datagroups/', views.data_group_list,
                                            name='data_group_list'),
    path('datagroup/<int:pk>/', views.data_group_detail,
                                            name='data_group_detail'),
    path('datagroup/docs_csv/<int:pk>/', views.dg_dd_csv_view,
                                            name='dg_dd_csv_view'),
    path('datagroup/pdfs_zipped/<int:pk>/', views.dg_pdfs_zip_view,
                                            name='dg_pdfs_zip_view'),
    path('datagroup/raw_extracted_records/<int:pk>/', views.dg_raw_extracted_records,
                                            name='dg_raw_extracted_records'),
    path('datasource/<int:pk>/datagroup_new/', views.data_group_create,
                                            name='data_group_new'),
    path('datagroup/<int:pk>/registered_records.csv', views.data_group_registered_records_csv,
                                            name=""registered_records.csv""),
    path('datagroup/edit/<int:pk>/', views.data_group_update,
                                            name='data_group_edit'),
    path('datagroup/delete/<int:pk>/', views.data_group_delete,
                                            name='data_group_delete'),
    path('datadocument/delete/<int:pk>/', views.data_document_delete,
                                            name='data_document_delete'),
    path('datadocument/note/<int:pk>/', views.data_document_note,
                                            name='data_document_note'),
    path('product_curation/', views.product_curation_index,
                                            name='product_curation'),
    path('category_assignment/<int:pk>/', views.category_assignment,
                                            name='category_assignment'),
    path('link_product_list/<int:pk>/', views.link_product_list,
                                            name='link_product_list'),
    path('link_product_form/<int:pk>/', views.link_product_form,
                                            name='link_product_form'),
    path('qa/extractionscript/', views.qa_extractionscript_index,
                                            name='qa_extractionscript_index'),
    path('qa/extractionscript/<int:pk>/', dashboard.views.qa.qa_extraction_script,
                                            name='qa_extraction_script'),
    path('qa/extractedtext/<int:pk>/', dashboard.views.qa.extracted_text_qa,
                                            name='extracted_text_qa'),
    path('extractionscript/<int:pk>/', views.extraction_script_detail,
                                            name='extraction_script_detail'),
    path('qa/chemicalpresence/', views.qa_chemicalpresence_index,
                                            name='qa_chemicalpresence_index'),
    path('qa/chemicalpresencegroup/<int:pk>/', views.qa_chemicalpresence_group,
                                            name='qa_chemical_presence_group'),
    path('bulk_product_puc/', views.bulk_assign_puc_to_product,
                                            name='bulk_product_puc'),
    path('bulk_product_tag/', views.bulk_assign_tag_to_products,
                                            name='bulk_product_tag'),
    path('product_puc/<int:pk>/', views.assign_puc_to_product,
                                            name='product_puc'),
    path('product_puc_delete/<int:pk>/', views.detach_puc_from_product,
                                            name='product_puc_delete'),
    path('puc-autocomplete/', views.puc_autocomplete.PUCAutocomplete.as_view(),
                                            name='puc-autocomplete'),
    path('product/<int:pk>/', views.product_detail,
                                            name='product_detail'),
    path('product/edit/<int:pk>/', views.product_update,
                                            name='product_edit'),
    path('product/delete/<int:pk>/', views.product_delete,
                                            name='product_delete'),
    path('products/', views.product_list,  name='product_list'),
    path('datadocument/<int:pk>/', views.data_document_detail,
                                            name='data_document'),
    path('save_type/<int:pk>/', views.save_doc_form,
                                            name='save_doc_form'),
    path('save_ext/<int:pk>/', views.save_ext_form,
                                            name='save_ext_form'),
    path('search/', include('haystack.urls')),
    path('find/', views.search.FacetedSearchView.as_view(),
                                            name='haystack_search'),
    path('p_json/', views.product_ajax,     name='p_ajax_url'),
    path('pucs/', views.puc_list,           name='puc_list'),
    path('dl_pucs/', views.download_PUCs,   name='download_PUCs'),
    path('dl_raw_chems/', views.download_raw_chems,  
                                            name='download_raw_chems'),
    path('dsstox_lookup/<int:pk>/', views.dsstox_lookup_detail,
                                            name='dsstox_lookup'),
    path('habitsandpractices/<int:pk>/', views.habitsandpractices,
                                            name='habitsandpractices'),
    path('link_habitandpractice_to_puc/<int:pk>/', views.link_habitsandpractices,
                                            name='link_habitsandpractices'),
    path('get_data/', views.get_data,      name='get_data'),
    path('dl_chem_summary/', views.download_chem_stats,
                                            name='download_chem_stats'),
    path('upload/dtxsid_csv/', views.upload_dtxsid_csv,
                                            name='upload_dtxsid_csv'),
    path('get_data/get_dsstox_csv_template/', views.get_data_dsstox_csv_template,
                                            name='get_data_dsstox_csv_template'),
    path('datagroup/diagnostics/<int:pk>/',   views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('datagroup/diagnostics/',          views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('extractedtext/edit/<int:pk>/',   views.extracted_text_edit,
                                            name='extracted_text_edit'),
    path('extractedchild/edit/<int:pk>/',   views.extracted_child_edit,
                                            name='extracted_child_edit'),
    path('datadocument/edit/<int:pk>/',   views.data_document_edit,
                                            name='data_document_edit'),
]

if settings.DEBUG is True:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
/n/n/ndashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['gen_cat','prod_fam','prod_type','description','PUC_type','num_prods']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                puc.description, 
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/ndashboard/views/data_document.py/n/nfrom django import forms
from django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from djqscsv import render_to_csv_response

from dashboard.forms import *
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if code in ['CP', 'HH'] else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=edit)
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if doc.is_extracted:

        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk) 
        extracted_text_form = ParentForm(instance=extracted_text)
        child_formset = ChildFormSet(instance=extracted_text)

        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True

        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

        colors = ['#d6d6a6', '#dfcaa9', '#d8e5bf'] * 47
        color = (hex for hex in colors)
        for form in child_formset.forms:
            form.color = next(color)
    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})


@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)


@login_required
def data_document_edit(request, pk):

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        return redirect(referer, pk=doc.pk)
    else:
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/ndashboard/views/data_source.py/n/nfrom datetime import datetime

from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from dashboard.forms import DataSourceForm, PriorityForm
from dashboard.models import DataSource, DataGroup, DataDocument
from .data_group import DataGroupForm
from django.db.models import Count, Q



@login_required()
def data_source_list(request, template_name='data_source/datasource_list.html'):
    datasources = DataSource.objects.all()
    ds_list, frm_list = [], []
    for ds in datasources:
        frm_list.append(PriorityForm(request.POST or None, instance=ds))
    registered = Count('datagroup__datadocument') 
    uploaded   = Count('datagroup__datadocument', filter=Q(datagroup__datadocument__matched=True))
    extracted  = Count('datagroup__datadocument__extractedtext')
    ds_list    = DataSource.objects.annotate(registered=registered).annotate(uploaded=uploaded, extracted=extracted)
    out = zip(ds_list, frm_list)
    if request.method == 'POST':
        datasource = DataSource.objects.get(pk=request.POST['ds_pk'])
        form = PriorityForm(request.POST or None, instance=datasource)
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
            return redirect('data_source_list')
    return render(request, template_name, {'object_list': out})


@login_required()
def data_source_detail(request, pk,
                        template_name='data_source/datasource_detail.html'):
    datasource = get_object_or_404(DataSource, pk=pk, )
    docs = DataDocument.objects.filter(data_group__in=DataGroup.objects.filter(data_source=datasource))
    datasource.registered = (len(docs)/float(datasource.estimated_records))*100
    datasource.uploaded = (len(docs.filter(matched=True))/float(
                                            datasource.estimated_records))*100

    form = PriorityForm(request.POST or None, instance=datasource)
    if request.method == 'POST':
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
    datagroup_list = DataGroup.objects.filter(data_source=pk)
    context =     {'object':             datasource,
                'datagroup_list':    datagroup_list,
                'form':             form}
    return render(request, template_name, context)


@login_required()
def data_source_create(request, template_name=('data_source/'
                                                'datasource_form.html')):
    form = DataSourceForm(request.POST or None)
    if form.is_valid():
        form.save()
        return redirect('data_source_list')
    return render(request, template_name, {'form': form})


@login_required()
def data_source_update(request, pk, template_name=('data_source/'
                                                    'datasource_form.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    form = DataSourceForm(request.POST or None, instance=datasource)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_source_detail', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})

@login_required()
def data_source_delete(request, pk,
                        template_name=('data_source/'
                                        'datasource_confirm_delete.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    if request.method == 'POST':
        datasource.delete()
        return redirect('data_source_list')
    return render(request, template_name, {'object': datasource})
/n/n/ndashboard/views/extraction_script.py/n/nfrom django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required

from dashboard.models import *


@login_required()
def extraction_script_list(request, template_name='qa/extraction_script_list.html'):
    """"""
    List view of extraction scripts
    """"""
    # TODO: the user is supposed to be able to click the filter button at the top of the table
    # and toggle between seeing all scripts and seeing only the ones with incomplete QA
    extractionscripts = Script.objects.filter(script_type='EX')
    data = {}
    data['object_list'] = extractionscripts
    return render(request, template_name, data)


@login_required()
def extraction_script_detail(request, pk,
                             template_name='extraction_script/extraction_script_detail.html'):
    extractionscript = get_object_or_404(Script, pk=pk)
    data = {}
    data['object_list'] = extractionscript
    return render(request, template_name, data)


/n/n/ndashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django import forms
from django.db import connection
from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, Subquery, OuterRef, F, Sum
from django.forms.models import model_to_dict

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp

def download_raw_chems(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""uncurated_chemicals_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['data_group_id', 'dashboard_rawchem_id', 'raw_cas', 'raw_chem_name', 'rid'])
    for rawchem in RawChem.objects.filter(dsstox_id=None):
        writer.writerow([rawchem.extracted_text.data_document.data_group.id, rawchem.id, rawchem.raw_cas, rawchem.raw_chem_name, rawchem.rid if rawchem.rid else '' ])

    return response
/n/n/ndashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import timezone, safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.forms import ModelForm
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, 
                            BulkProductPUCForm, BulkProductTagForm, 
                            BulkPUCForm, ProductForm)

from taggit.forms import TagField
from taggit_labels.widgets import LabelWidget
from django.core.paginator import Paginator
from django.db.models import Max

class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]

class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))
    class Meta:
        model = Product
        fields = ['tags']
    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self


@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs

    return render(request, template_name, {'data_sources': data_sources})

@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})

@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})

@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})

@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)

@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})

@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter( Q(title__icontains=q) | Q(brand_name__icontains=q) ).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})

@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })

@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})

@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')

@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/ndashboard/views/qa.py/n/nfrom django.shortcuts import render, redirect, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.db.models import Count, Q
from django.http import HttpResponse, HttpResponseRedirect
from django.core.exceptions import ValidationError, MultipleObjectsReturned, ObjectDoesNotExist
from django.urls import reverse
from django.utils import timezone

from dashboard.forms import create_detail_formset, QANotesForm, DocumentTypeForm
from dashboard.models import Script, DataGroup, DataDocument,\
    ExtractedCPCat, ExtractedText, ExtractedListPresence,\
    QAGroup, QANotes, DocumentType
from factotum.settings import EXTRA
from django import forms


@login_required()
def qa_extractionscript_index(request, template_name='qa/extraction_script_index.html'):
    datadocument_count = Count('extractedtext__extraction_script')
    qa_complete_extractedtext_count = Count('extractedtext', filter=Q(extractedtext__qa_checked=True))
    extraction_scripts = Script.objects.\
        annotate(datadocument_count=datadocument_count).\
        annotate(qa_complete_extractedtext_count=qa_complete_extractedtext_count).\
        filter(script_type='EX')

    return render(request, template_name, {'extraction_scripts': extraction_scripts})

@login_required()
def qa_chemicalpresence_index(request, template_name='qa/chemical_presence_index.html'):
    datagroups = DataGroup.objects.filter(group_type__code='CP').\
        annotate(datadocument_count=Count('datadocument'))

    return render(request, template_name, {'datagroups': datagroups})

@login_required()
def qa_chemicalpresence_group(request, pk, template_name='qa/chemical_presence.html'):
    datagroup = DataGroup.objects.get(pk=pk)
    if datagroup.group_type.code != 'CP':
        raise ValidationError('This DataGroup is not of a ChemicalPresence type')
    extractedcpcats = ExtractedCPCat.objects.filter(data_document__data_group=datagroup)
    return render(request, template_name, {'datagroup':datagroup, 'extractedcpcats':extractedcpcats})

def prep_cp_for_qa(extractedcpcat):
    '''
    Given an ExtractedCPCat object, select a sample of its ExtractedListPresence children
    for QA review.
    '''
    from random import shuffle
    QA_RECORDS_PER_DOCUMENT = 30

    if extractedcpcat.rawchem.count() > 0:
        list_presence_count = extractedcpcat.rawchem.count()
    else:
        return
    elps = extractedcpcat.rawchem.select_subclasses()
    non_qa_list_presence_ids = list(elps.filter(extractedlistpresence__qa_flag=False).values_list('pk',flat=True))

    # total number of qa-flagged listpresence objects
    list_presence_qa_count = elps.filter(extractedlistpresence__qa_flag=True).count()

    # if less than 30 records (or all records in set) flagged for QA, make up the difference
    if list_presence_qa_count < QA_RECORDS_PER_DOCUMENT and list_presence_qa_count < list_presence_count:
        random_x = QA_RECORDS_PER_DOCUMENT - list_presence_qa_count
        shuffle(non_qa_list_presence_ids)
        list_presence = ExtractedListPresence.objects.filter(pk__in=non_qa_list_presence_ids[:random_x])
        for lp in list_presence:
            lp.qa_flag = True
            lp.save()
    return

 


@login_required()
def qa_extraction_script(request, pk,
                         template_name='qa/extraction_script.html'):
    """"""
    The user reviews the extracted text and checks whether it was properly converted to data
    """"""
    es = get_object_or_404(Script, pk=pk)
    # If the Script has no related ExtractedText objects, redirect back to the QA index
    if ExtractedText.objects.filter(extraction_script = es).count() == 0 :
        return redirect('/qa/extractionscript/')
    # Check whether QA has begun for the script
    if es.qa_group.count() > 0:
        # if the QA process has begun, there will already be one QA Group
        # associated with the Script.
        try:
            # get the QA Group
            qa_group = QAGroup.objects.get(extraction_script=es,
                                           qa_complete=False)
        except MultipleObjectsReturned:
            qa_group = QAGroup.objects.filter(extraction_script=es,
                                              qa_complete=False).first()
        except ObjectDoesNotExist:
            print('No QA Group was found matching Extraction Script %s' % es.pk)

        texts = ExtractedText.objects.filter(qa_group=qa_group,
                                             qa_checked=False)
        return render(request, template_name, {'extractionscript': es,
                                               'extractedtexts': texts,
                                               'qagroup': qa_group})
    else:
        qa_group = es.create_qa_group()
        es.qa_begun = True
        es.save()
    # Collect all the ExtractedText objects in the QA Group
    texts = ExtractedText.objects.filter(qa_group=qa_group)

    return render(request, template_name, {'extractionscript': es,
                                           'extractedtexts': texts,
                                           'qagroup': qa_group})


def hide_dsstox_fields(formset):
    # Hide the curated DSSToxLookup fields in the formset if they appear
    for form in formset:
        for dssfield in ['true_cas','true_chemname','SID']:
            if dssfield in form.fields:
                form.fields[dssfield].widget = forms.HiddenInput()


@login_required()
def extracted_text_qa(request, pk,
                      template_name='qa/extracted_text_qa.html', nextid=0):
    """"""
    Detailed view of an ExtractedText object, where the user can approve the
    record, edit its ExtractedChemical objects, skip to the next ExtractedText
    in the QA group, or exit to the index page.
    This view processes objects of different models with different QA workflows. 
    The qa_focus variable is used to indicate whether an ExtractedText object is
    part of a QA Group, as with Composition records, or if the DataDocument/ExtractedText
    is its own QA Group, as with ExtractedCPCat and ExtractedHHDoc records.  
    """"""
    extext = get_object_or_404(
	        ExtractedText.objects.select_subclasses(), pk=pk)
    
    doc = DataDocument.objects.get(pk=pk)
    exscript = extext.extraction_script
    group_type_code = extext.data_document.data_group.group_type.code

    if group_type_code in ['CP','HH']:
        qa_focus = 'doc'
        #
        # Document-focused QA process
        #
        # If the object is an ExtractedCPCat record, there will be no Script
        # associated with it and no QA Group
        prep_cp_for_qa(extext)

        stats = ''
        qa_home_page = f'qa/chemicalpresencegroup/%s/' % extext.data_document.data_group.id
    else:
        qa_focus = 'script'
        #
        # Extraction Script-focused QA process
        #
        # when not coming from extraction_script page, the document's script might not have 
        # a QA Group yet. 
        if not extext.qa_group:
            # create the qa group with the optional ExtractedText pk argument
            # so that the ExtractedText gets added to the QA group even if the
            # group uses a random sample
            qa_group = exscript.create_qa_group(pk)
            exscript.qa_begun = True
            exscript.save()
            extext.qa_group = qa_group
            extext.save()
        # get the next unapproved Extracted Text object
        # Its ID will populate the URL for the ""Skip"" button
        if extext.qa_checked:  # if ExtractedText object's QA process done, use 0
            nextid = 0
        else:
            nextid = extext.next_extracted_text_in_qa_group()
        # derive number of approved records and remaining unapproved in QA Group
        a = extext.qa_group.get_approved_doc_count()
        r = ExtractedText.objects.filter(qa_group=extext.qa_group).count() - a
        stats = '%s document(s) approved, %s documents remaining' % (a, r)

    referer = 'data_document' if 'datadocument' in request.path else 'qa_extraction_script'

    # Create the formset factory for the extracted records
    # The model used for the formset depends on whether the
    # extracted text object matches a data document()
    # The QA view should exclude the weight_fraction_type field.
    ParentForm, ChildForm = create_detail_formset(
        doc, EXTRA, can_delete=True, 
        exclude=['weight_fraction_type', 'true_cas', 'true_chemname', 'sid'])
    # extext = extext.pull_out_cp()
    ext_form = ParentForm(instance=extext)
    detail_formset = ChildForm(instance=extext)
    
    # If the document is CPCat or HHE type, the display should only show the
    # child records where qa_flag = True
    if qa_focus == 'doc' and hasattr(detail_formset.get_queryset().model, 'qa_flag'):
        qs = detail_formset.get_queryset().filter(qa_flag=True)
        detail_formset._queryset = qs
    
    # This code is being repeated in the GET and POST blocks
    # 
    # Hide all the DSSToxLookup fields 
    hide_dsstox_fields(detail_formset)

    # Add CSS selector classes to each form
    for form in detail_formset:
        for field in form.fields:
            form.fields[field].widget.attrs.update(
                {'class': f'detail-control form-control %s' % doc.data_group.type}
            )

    note, created = QANotes.objects.get_or_create(extracted_text=extext)
    notesform = QANotesForm(instance=note)

    # Allow the user to edit the data document type
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    # the form class overrides the label, so over-override it
    document_type_form.fields['document_type'].label = 'Data document type:'

    context = {
        'extracted_text': extext,
        'doc': doc,
        'script': exscript,
        'stats': stats,
        'nextid': nextid,
        'detail_formset': detail_formset,
        'notesform': notesform,
        'ext_form': ext_form,
        'referer': referer,
        'document_type_form': document_type_form
    }

    if request.method == 'POST' and 'save' in request.POST:
        # The save action only applies to the child records and QA properties,
        # no need to save the ExtractedText form
        ParentForm, ChildForm = create_detail_formset(
            doc, EXTRA, can_delete=True, exclude=['weight_fraction_type'])
        # extext = extext.pull_out_cp()
        detail_formset = ChildForm(request.POST, instance=extext)

        notesform = QANotesForm(request.POST, instance=note)
        notesform.save()
        if detail_formset.has_changed():
            if detail_formset.is_valid() :
                detail_formset.save()
                extext.qa_edited = True
                extext.save()
                # rebuild the formset after saving it
                detail_formset = ChildForm(instance=extext)
            else:
                pass
                # print(detail_formset.errors)
                # TODO: iterate through this dict of errors and map each error to
                # the corresponding form in the template for rendering

            context['detail_formset'] = detail_formset
            context['ext_form'] = ext_form
            # calls the clean method? y?
            context.update({'notesform': notesform})

        # This code is being repeated in the GET and POST blocks
        # 
        # Hide all the DSSToxLookup fields 
        hide_dsstox_fields(detail_formset)

        # Add CSS selector classes to each form
        for form in detail_formset:
            for field in form.fields:
                form.fields[field].widget.attrs.update(
                    {'class': f'detail-control form-control %s' % doc.data_group.type}
                )

    elif request.method == 'POST' and 'approve' in request.POST:  # APPROVAL
        notesform = QANotesForm(request.POST, instance=note)
        context['notesform'] = notesform
        nextpk = extext.next_extracted_text_in_qa_group()
        if notesform.is_valid():
            extext.qa_approved_date = timezone.now()
            extext.qa_approved_by = request.user
            extext.qa_checked = True
            extext.save()
            notesform.save()
            # After approval, the user proceeds to either the next document
            # in the QA Group, to the extractionscript QA index, or to the
            # index page that matches the document's data group type 
            # 

            if referer == 'data_document':
                # The user got to the QA page from a data document detail page,
                # so return there
                return HttpResponseRedirect(
                    reverse(referer, kwargs={'pk': pk}))
            elif not nextpk == 0:
                return HttpResponseRedirect(
                    reverse('extracted_text_qa', args=[(nextpk)]))
            elif nextpk == 0:
                # return to the top of the most local QA stack.
                # that may be the list of ExtractionScripts or 
                # the list of Chemical Presence Data Groups
                return HttpResponseRedirect(
                        extext.get_qa_index_path()
                            )
        else:
            # The notesform is not valid
            pass
    return render(request, template_name, context)
/n/n/n",0
83,83,f166c8d6511dd4d6b2e5138aceaa4fb5bb5c5379,"/dashboard/admin.py/n/nfrom django.contrib import admin
from dashboard.models import *
from django.db.models import Count

from django import forms
from taggit_labels.widgets import LabelWidget
from dashboard.signals import *

class PUCAdminForm(forms.ModelForm):
    class Meta:
        model = PUC
        fields = ['gen_cat', 'prod_fam', 'prod_type', 'description','tags',]
        readonly_fields = ('num_products',)
        widgets = {
            'tags': LabelWidget(model=PUCTag),
        }

class PUCAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'tag_list','num_products')
    form = PUCAdminForm
    def get_changeform_initial_data(self, request):
        get_data = super(PUCAdmin, self).get_changeform_initial_data(request)
        get_data['last_edited_by'] = request.user.pk
        return get_data
    def get_queryset(self, request):
        return super(PUCAdmin, self).get_queryset(request).prefetch_related('tags').annotate(num_products=Count('products'))
    def num_products(self, obj):
        return obj.num_products
    num_products.short_description = 'Product Count'
    num_products.admin_order_field = 'num_products'
    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())

class HHDocAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'hhe_report_number')

class PUCToTagAdmin(admin.ModelAdmin):
    list_display = ('content_object', 'tag', 'assumed')
    list_filter = ('tag',)
    def tag(self, obj):
        return obj.tag    
    def assumed(self, obj):
        return obj.assumed 

# Register your models here.
admin.site.register(DataSource)
admin.site.register(GroupType)
admin.site.register(DataGroup)
admin.site.register(DocumentType)
admin.site.register(DataDocument)
admin.site.register(Script)
admin.site.register(Product)
admin.site.register(ProductToPUC)
admin.site.register(ProductDocument)
admin.site.register(SourceCategory)
admin.site.register(PUC, PUCAdmin)
admin.site.register(ExtractedText)
admin.site.register(ExtractedChemical)
admin.site.register(ExtractedFunctionalUse)
admin.site.register(ExtractedHabitsAndPractices)
admin.site.register(DSSToxLookup)
admin.site.register(QAGroup)
admin.site.register(UnitType)
admin.site.register(WeightFractionType)
admin.site.register(PUCTag) #,ProductTagAdmin
admin.site.register(Taxonomy)
admin.site.register(TaxonomySource)
admin.site.register(TaxonomyToPUC)
admin.site.register(ExtractedHHDoc, HHDocAdmin)
admin.site.register(ExtractedHHRec)
admin.site.register(PUCToTag, PUCToTagAdmin)
/n/n/n",1
174,174,671a06608c5210f205d93c0c235c94a8783892b9,"hyperion/lib/monitoring/threads.py/n/nfrom threading import Thread, Lock
import logging
import sys
import time
import hyperion.lib.util.config as config
from os import system
from subprocess import call
from psutil import Process, NoSuchProcess
is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as Queue
else:
    import queue as Queue
    

class ComponentMonitorJob(object):
    """"""Abstract class that represents a component monitoring job (local or remote).""""""

    def __init__(self, pid, comp_name):
        """"""Initializes component monitoring job.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""
        self.pid = pid
        self.comp_name = comp_name

    def run_check(self):
        """"""You need to override this function in monitoring subclasses. It is called in the main monitoring thread.

        :return: True on a successful check, otherwise a CrashEvent is generated
        :rtype: bool or CrashEvent
        """"""


class LocalComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a local component monitoring job.""""""

    def __init__(self, pid, comp_name):
        """"""Creates a monitoring job for a local component.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""

        super(LocalComponentMonitoringJob, self).__init__(pid, comp_name)

    def run_check(self):
        """"""Runs a check if the pid exists and has not finished yet.

        :return: True if the component is running, otherwise returns a generated ``LocalCrashEvent``
        :rtype bool or LocalCrashEvent
        """"""
        try:
            proc = Process(self.pid)
            if proc.is_running():
                return True
        except NoSuchProcess:
            pass
        return CrashEvent(self.comp_name)

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for local component %s with pid %s"" % (self.comp_name, self.pid)


class RemoteComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a remote component monitoring job.""""""

    def __init__(self, pid, comp_name, hostname, host_status):
        """"""Creates a remote component monitoring job.

        :param pid: Process id on the remote machine
        :type pid: int
        :param comp_name: Name of the monitored component
        :type comp_name: str
        :param hostname: Name of the host running the component
        :type hostname: str
        """"""

        super(RemoteComponentMonitoringJob, self).__init__(pid, comp_name)
        self.hostname = hostname
        self.host_status = host_status

    def run_check(self):
        """"""Runs a check if a remote process is still running.

        :return: True if the component is still running or the host is not reachable, otherwise a ``RemoteCrashEvent`` is generated.
        :rtype: bool or RemoteCrashEvent
        """"""

        if self.host_status.get(self.hostname):
            cmd = 'ssh -F %s %s ""ps -p %s > /dev/null""' % (config.CUSTOM_SSH_CONFIG_PATH, self.hostname, self.pid)
            if call(cmd, shell=True) == 0:
                return True
            else:
                return RemoteCrashEvent(self.comp_name, self.hostname)
        # Return true because no information can be retrieved. The connection to the host has to be reestablished first.
        return True

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for remote component %s with pid %s on host %s"" % (self.comp_name, self.pid,
                                                                                 self.hostname)


class HostMonitorJob(object):
    """"""Class representing a host monitoring job.""""""
    def __init__(self, pid, hostname, host_status, host_lock):
        """"""Create host monitoring job.

        :param pid: Process id of the ssh connection
        :type pid: int
        :param hostname: Name of the host connected to
        :type hostname: str
        :param host_status: Status of the used hosts
        :type host_status: dict
        :param host_lock: Lock that has to be acquired in order to write to the host status dictionary.
        :type host_lock: Lock
        """"""
        self.pid = pid
        self.hostname = hostname
        self.host_status = host_status
        self.host_lock = host_lock

    def run_check(self):
        try:
            proc = Process(self.pid)
            if proc.is_running() and system('(ping -w2 -c 1 %s) > /dev/null' % self.hostname) is 0:
                return True
        except NoSuchProcess:
            pass

        self.host_lock.acquire()
        self.host_status[self.hostname] = None
        self.host_lock.release()

        return DisconnectEvent(self.hostname)

    def info(self):
        return ""Running ssh host check for %s with pid %s"" % (self.hostname, self.pid)


class CrashEvent(object):
    """"""Superclass to model a component crash.

    Provides the name of the crashed component.""""""

    def __init__(self, comp_name):
        """"""Initializes the crash event assigning the component name

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        self.comp_name = comp_name


class LocalCrashEvent(CrashEvent):
    """"""Crash event subclass for local component crashes.

    Provides the name of the crashed component and a short message.
    """"""

    def __init__(self, comp_name):
        """"""Creates a local crash event class with a component name and generates a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        super(LocalCrashEvent, self).__init__(comp_name)
        self.message = 'Component %s crashed on localhost' % comp_name


class RemoteCrashEvent(CrashEvent):
    """"""Crash event subclass for remote component crashes.

    Provides the name of the crashed component along with the host it ran on and a short message.
    """"""

    def __init__(self, comp_name, hostname):
        """"""Creates a remote crash event with a component name and a host generating a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        :param hostname: Name of the host the component was running on
        :type hostname: str
        """"""

        super(RemoteCrashEvent, self).__init__(comp_name)
        self.hostname = hostname
        self.message = 'Component %s crashed on remote host %s' % (comp_name, hostname)


class DisconnectEvent(object):
    """"""Class representing a disconnect event for remote hosts.""""""

    def __init__(self, hostname):
        """"""Creates a disconnect event with a hostname and generates a short message.""""""
        self.hostname = hostname
        self.message = 'Lost connection to remote host %s' % hostname


class MonitoringThread(Thread):
    """"""This class is monitoring thread that extends the threading.Thread class.""""""

    def __init__(self, queue):
        """"""Initializes the monitoring thread with its input queue.

        :param queue: Input queue the monitor retrieves its jobs from
        :type queue: Queue.Queue
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Initialized thread"")
        super(MonitoringThread, self).__init__()
        self.job_queue = queue
        self.subscribed_queues = []
        self.end = False

    def kill(self):
        """"""Shuts down the thread by signalling the run function to end.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Killing process monitoring thread"")
        self.end = True

    def add_subscriber(self, queue):
        """"""Adds a subscriber to the list of queues to send notifications to.

        :param queue: Subscribing queue that will get notifications by this thread
        :type queue: Queue.Queue
        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Added subscriber"")
        self.subscribed_queues.append(queue)

    def run(self):
        """"""Starts the monitoring thread.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Started run funtion"")
        while not self.end:

            comp_jobs = []
            jobs = []
            already_handleled = {}
            # Get all enqueued jobs for this iteration
            while not self.job_queue.empty():
                mon_job = self.job_queue.get()
                if isinstance(mon_job, HostMonitorJob):
                    jobs.append(mon_job)
                if isinstance(mon_job, ComponentMonitorJob) and mon_job.comp_name not in already_handleled:
                    comp_jobs.append(mon_job)
                    already_handleled[mon_job.comp_name] = True

            # Reorder job list to first check the hosts, then check the components because this makes sense
            jobs.extend(comp_jobs)
            for mon_job in jobs:
                logger.debug(mon_job.info())
                ret = mon_job.run_check()
                if ret is True:
                    logger.debug(""S'all good man"")
                    # If job is ok, put it back for the next iteration
                    self.job_queue.put(mon_job)
                else:
                    # If job is not ok, notify subscribers
                    logger.debug(""Check failed, notifying subscribers"")
                    for subscriber in self.subscribed_queues:
                        subscriber.put(ret)

            time.sleep(1)
/n/n/nhyperion/manager.py/n/n#! /usr/bin/env python
from libtmux import Server, Window
from yaml import load, dump
import logging
import os
import sys
import socket
import uuid
import shutil
from psutil import Process, NoSuchProcess
from subprocess import call, Popen, PIPE
from threading import Lock
from enum import Enum
from signal import SIGTERM
from time import sleep, time
from lib.util.setupParser import Loader
from lib.util.depTree import Node, dep_resolve
from lib.monitoring.threads import LocalComponentMonitoringJob, RemoteComponentMonitoringJob, \
    HostMonitorJob, MonitoringThread
import lib.util.exception as exceptions
import lib.util.config as config

is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as queue
else:
    import queue as queue

logging.basicConfig(level=logging.WARNING, format=config.FORMAT, datefmt='%I:%M:%S')

BASE_DIR = os.path.dirname(__file__)
""""""Path to the directory this file is contained in""""""

SCRIPT_CLONE_PATH = (""%s/bin/start_named_clone_session.sh"" % BASE_DIR)
""""""File path of the 'clone session' script""""""


class CheckState(Enum):
    """"""Enum that provides information about the status of a run check""""""
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4
    UNREACHABLE = 5
    NOT_INSTALLED = 6


class StartState(Enum):
    """"""Enum that provides information about the start state of a component""""""
    STARTED = 0
    ALREADY_RUNNING = 1
    FAILED = 2


###################
# Logging
###################
def setup_log(window, filepath, comp_name):
    """"""Redirect stdout and stderr of window to file.

    Rotate logs and ensure the log directory for component `comp_name` exists, than,
    redirect the outputs of `window` to /dev/tty to undo the case that previous output was already redirected.
    After that redirect outputs to `file`.

    :param window: tmux reference to the window the component is being run in.
    :type window: Window
    :param filepath: filepath of the component log file
    :type filepath: str
    :param comp_name: name of the component being run
    :type comp_name: str
    :return: None
    """"""

    clear_log(filepath)
    ensure_dir(filepath)

    window.cmd(""send-keys"", ""exec > /dev/tty"", ""Enter"")

    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % filepath, ""Enter"")
    # Reroute stdout to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % filepath, ""Enter"")
    # Reroute stdin to log file <== causes remote host to logout, disabled for now
    # window.cmd(""send-keys"", ""exec 0> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\\t$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    """"""If found rename the log at file_path to a uuid.

    :param file_path: log file path
    :type file_path: str
    :return: None
    """"""

    if os.path.isfile(file_path):
        directory = os.path.dirname(file_path)
        os.rename(file_path, ""%s/%s.log"" % (directory, uuid.uuid4().hex))


def ensure_dir(file_path):
    """"""If not already existing, recursively create parent directory of file_path.

    :param file_path: log file path
    :type file_path: str
    :return: None
    """"""

    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)


class AbstractController(object):
    """"""Abstract controller class that defines basic controller variables and methods.""""""

    def __init__(self, configfile):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.config = None
        self.session = None
        self.server = None

    def load_config(self, filename=""default.yaml""):
        """"""Load configuration recursively from yaml file.

        :param filename: path to the configuration file.
        :type filename: str
        :return: None
        """"""
        try:
            with open(filename) as data_file:
                self.config = load(data_file, Loader)
        except IOError as e:
            self.logger.critical(""No such file '%s' found"" % filename)
            raise e

    ###################
    # Component Management
    ###################
    def run_component_check(self, comp):
        """"""Runs the component check defined in the component configuration and returns the exit state.

        :param comp: Component configuration
        :type comp: dict
        :return: Check exit state (fail = False / success = True).
        :rtype: bool
        """"""
        self.logger.debug(""Running specific component check for %s"" % comp['name'])
        if call(comp['cmd'][1]['check'], shell=True) == 0:
            self.logger.debug(""Check returned true"")
            return True
        else:
            self.logger.debug(""Check returned false"")
            return False

    def check_local_component(self, comp):
        """"""Check if a local component is running and return the corresponding CheckState.

        :param comp: Component configuration
        :type comp: dict
        :return: tuple of pid and component status. If the component is not running, the pid is 0.
        :rtype: (int, CheckState)
        """"""
        logger = self.logger

        logger.debug(""Running component check for %s"" % comp['name'])
        check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
        window = self.find_window(comp['name'])

        ret = None
        pid = 0

        if window:
            w_pid = self.get_window_pid(window)
            logger.debug(""Found window pid: %s"" % w_pid)

            # May return more child pids if logging is done via tee (which then was started twice in the window too)
            procs = []
            for entry in w_pid:
                procs.extend(Process(entry).children(recursive=True))

            pids = []
            for p in procs:
                if p.name() != 'tee':
                    pids.append(p.pid)
            logger.debug(""Window is running %s non-logger child processes: %s"" % (len(pids), pids))

            if len(pids) < 1:
                logger.debug(""Main process has finished. Running custom check if available"")
                if check_available and self.run_component_check(comp):
                    logger.debug(""Process terminated but check was successful"")
                    ret = CheckState.STOPPED_BUT_SUCCESSFUL
                else:
                    logger.debug(""Check failed or no check available: returning false"")
                    ret = CheckState.STOPPED
            elif check_available and self.run_component_check(comp):
                logger.debug(""Check succeeded"")
                pid = pids[0]
                ret = CheckState.RUNNING
            elif not check_available:
                logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
                pid = pids[0]
                ret = CheckState.RUNNING
            else:
                logger.debug(""Check failed: returning false"")
                ret = CheckState.STOPPED
        else:
            logger.debug(""%s window is not running. Running custom check"" % comp['name'])
            if check_available and self.run_component_check(comp):
                logger.debug(""Component was not started by Hyperion, but the check succeeded"")
                ret = CheckState.STARTED_BY_HAND
            else:
                logger.debug(""Window not running and no check command is available or it failed: returning false"")
                ret = CheckState.STOPPED

        return pid, ret

    def get_window_pid(self, window):
        """"""Returns pid of the tmux window process.

        :param window: tmux window
        :type window: Window
        :return: pid of the window as list
        :rtype: list of int
        """"""
        self.logger.debug(""Fetching pids of window %s"" % window.name)
        r = window.cmd('list-panes',
                       ""-F #{pane_pid}"")
        return [int(p) for p in r.stdout]

    def get_component_wait(self, comp):
        """"""Returns time to wait after component start (default of 5 seconds unless overwritten in configuration).

        :param comp: Component configuration
        :return: Component wait time
        :rtype: float
        """"""
        self.logger.debug(""Retrieving wait time of component %s"" % comp['name'])
        if 'wait' in comp:
            self.logger.debug(""Found %s seconds as wait time for %s"" % (float(comp['wait']), comp['name']))
            return float(comp['wait'])
        else:
            self.logger.debug(""No wait time for %s found, using default of %s seconds"" %
                              (comp['name'], config.DEFAULT_COMP_WAIT_TIME))
            return config.DEFAULT_COMP_WAIT_TIME

    def get_component_by_name(self, comp_name):
        """"""Return component configuration by providing only the name.

        :param comp_name: Component name
        :type comp_name: str
        :return: Component configuration
        :rtype: dict
        :raises exceptions.WindowNotFoundException: If component was not found
        """"""
        self.logger.debug(""Searching for %s in components"" % comp_name)
        for group in self.config['groups']:
            for comp in group['components']:
                if comp['name'] == comp_name:
                    self.logger.debug(""Component %s found"" % comp_name)
                    return comp
        raise exceptions.WindowNotFoundException('Component %s not found in current configuration' % comp_name)

    ###################
    # TMUX
    ###################
    def kill_session_by_name(self, name):
        """"""Kill tmux session by name.

        :param name: Name of the session to be killed
        :type name: str
        :return: None
        """"""
        self.logger.debug(""Killing session by name %s"" % name)
        session = self.server.find_where({
            ""session_name"": name
        })
        session.kill_session()

    def kill_window(self, window):
        """"""Kill tmux window by reference.

        :param window: Window to be killed
        :type window: Window
        :return: None
        """"""
        self.logger.debug(""Killing window by name %s"" % window.name)
        window.cmd(""send-keys"", """", ""C-c"")
        window.kill_window()

    def start_window(self, window, comp, log_file):
        """"""Execute cmd in window.

        Mainly used to run a component start command in its designated window

        :param window: Window the component will be started in
        :type window: Window
        :param comp: Component configuration
        :type comp: dict
        :param log_file: log file path
        :type log_file: str
        :return: None
        """"""

        cmd = comp['cmd'][0]['start']
        comp_name = comp['name']

        pid = self.get_window_pid(window)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))

        for proc in procs:
            self.logger.debug(""Killing leftover child process %s"" % proc.name())
            os.kill(proc.pid, SIGTERM)

        self.logger.debug(""Rotating log for %s"" % comp_name)
        setup_log(window, log_file, comp_name)
        self.logger.debug(""Running start command for %s"" % comp_name)
        window.cmd(""send-keys"", cmd, ""Enter"")

    def find_window(self, window_name):
        """"""Return window by name (None if not found).

        :param window_name: Window name
        :type window_name: str
        :return: Window with name `window_name`
        :rtype: Window or None
        """"""

        window = self.session.find_where({
            ""window_name"": window_name
        })
        return window

    def send_main_session_command(self, cmd):
        """"""Send command to the main window of the master session.

        `Session.cmd` sends the command to the currently active window of the session, and when issuing commands to the
        session, usually it is not intended to interact with component windows thus this functions fetches the main
        window and calls the `cmd` function on it.

        :param cmd: Command to execute
        :type cmd: str
        :return: None
        """"""
        self.logger.debug(""Sending command to master session main window: %s"" % cmd)
        window = self.find_window('Main')
        window.cmd(""send-keys"", cmd, ""Enter"")


class ControlCenter(AbstractController):
    """"""Controller class that is able to handle a master session.""""""

    def __init__(self, configfile=None, monitor_enabled=False):
        """"""Sets up the ControlCenter

        Initializes an empty node dict, an empty host_list dict, creates a queue for monitor jobs and a monitoring
        thread that is started right away and sets a handler for signals. After that the configuration file is loaded
        and a master session with a main window is created if not already existing.

        :param configfile: Path to the configuration to initialize
        :type configfile: str
        :param monitor_enabled: Whether the monitoring thread should be launched or not
        :type monitor_enabled: bool
        """"""

        super(ControlCenter, self).__init__(configfile)
        self.nodes = {}
        self.host_list = {}
        self.host_list_lock = Lock()
        self.monitor_queue = queue.Queue()
        self.mon_thread = MonitoringThread(self.monitor_queue)
        if monitor_enabled:
            self.mon_thread.start()

        if configfile:
            try:
                self.load_config(configfile)
            except IOError:
                self.cleanup()
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def init(self):
        """"""Initialize the controller.

        Sets up master ssh connections to all used hosts, copies components to them if they are reachable and computes a
        dependency tree for all used components.

        :return: None
        """"""
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            self.setup_ssh_config()

            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        if comp['host'] not in self.host_list:
                            if self.establish_master_connection(comp['host']):
                                self.logger.debug(""Master connection to %s established!"" % comp['host'])
                        if self.host_list.get(comp['host']) is not None:
                            self.copy_component_to_remote(comp, comp['host'])
                        else:
                            self.logger.debug(""Not copying because host %s is not reachable: %s"" %
                                              (comp['host'], self.host_list.get(comp['name'])))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        """"""Parses all components constructing a dependency tree.

        :param exit_on_fail: Whether the program should be exited on an encountered error
        :type exit_on_fail: bool
        :return: None
        """"""
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.add_edge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.add_edge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        self.logger.debug(""exit on fail: %s"" % exit_on_fail)
                        if exit_on_fail:
                            self.cleanup(status=1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except exceptions.CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                self.cleanup(status=1)

    def copy_component_to_remote(self, comp, host):
        """"""Copies `comp` to `TMP_SLAVE_DIR` on the remote host `host`.

        To do so `comp` gets temporarily saved as standalone configfile on the local machine (in `TMP_COMP_DIR`) and
        then scpd to `TMP_SLAVE_DIR` on `host` (after ensuring the containing directory exists via mkdir -p invocation
        over ssh in the main window of the master session).

        :param comp: Component to copy
        :type comp: dict
        :param host: Host to copy the component to
        :type host: str
        :return: None
        """"""

        comp_name = comp['name']

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (config.TMP_COMP_DIR, comp_name))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(comp, outfile, default_flow_style=False)

            self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp_name, host))
            cmd = (""ssh -F %s %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
                   (config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, tmp_comp_path, host,
                    config.TMP_SLAVE_DIR, comp_name))
            self.send_main_session_command(cmd)

    def setup_ssh_config(self):
        """"""Creates an ssh configuration that is saved to `CUSTOM_SSH_CONFIG_PATH`.

        The user config in `SSH_CONFIG_PATH` is copied to `CUSTOM_SSH_CONFIG_PATH` and then appends the lines enabling
        master connections for all hosts to it. This is done in order to use the master connection feature without
        tempering with the users standard configuration.

        :return: None
        """"""
        try:
            self.logger.debug(""Trying to copy ssh config from %s to %s"" % (config.SSH_CONFIG_PATH,
                                                                           config.CUSTOM_SSH_CONFIG_PATH))
            ensure_dir(config.CUSTOM_SSH_CONFIG_PATH)
            ensure_dir('%s/somefile' % config.SSH_CONTROLMASTERS_PATH)
            shutil.copy(config.SSH_CONFIG_PATH, config.CUSTOM_SSH_CONFIG_PATH)
        except IOError:
            self.logger.critical(""Could not copy ssh config! Make sure you have a config in your users .ssh folder!"")
            sys.exit(1)

        try:
            conf = open(config.CUSTOM_SSH_CONFIG_PATH, 'a')
            conf.write(""Host *\n    ControlMaster yes\n    ControlPath ~/.ssh/controlmasters/%C"")
        except IOError:
            self.logger.error(""Could not append to custom ssh config!"")

    def establish_master_connection(self, hostname):
        """"""Create a master ssh connection to host `hostname` in a dedicated window.

        The pid of the ssh session is put into the monitoring thread to have a means to check if the connection still
        exists. Also `host` is added to the list of known hosts with its current status.

        :param hostname: Host to establish a connection with
        :type hostname: str
        :return: Whether establishing the connection was successful or not
        :rtype: bool
        """"""

        self.logger.debug(""Establishing master connection to host %s"" % hostname)

        cmd = 'ssh -F %s %s -o BatchMode=yes -o ConnectTimeout=%s' % (config.CUSTOM_SSH_CONFIG_PATH,
                                                                      hostname, config.SSH_CONNECTION_TIMEOUT)

        is_up = True if os.system('ping -w2 -c 1 %s > /dev/null' % hostname) is 0 else False
        if not is_up:
            self.logger.error(""Host %s is not reachable!"" % hostname)

            self.host_list_lock.acquire()
            self.host_list[hostname] = None
            self.host_list_lock.release()
            return False

        window = self.find_window('ssh-%s' % hostname)
        if window:
            self.logger.debug(""Connecting to '%s' in old window"" % hostname)
            window.cmd(""send-keys"", """", ""C-c"")
        else:
            self.logger.debug(""Connecting to '%s' in new window"" % hostname)
            window = self.session.new_window('ssh-%s' % hostname)
        window.cmd(""send-keys"", cmd, ""Enter"")

        t_end = time() + config.SSH_CONNECTION_TIMEOUT

        pid = self.get_window_pid(window)
        pids = []

        while time() < t_end:
            procs = []
            for entry in pid:
                procs.extend(Process(entry).children(recursive=True))

            for p in procs:
                try:
                    if p.name() == 'ssh':
                        pids.append(p.pid)
                except NoSuchProcess:
                    pass
            if len(pids) > 0:
                break

        if len(pids) < 1:
            self.host_list_lock.acquire()
            self.host_list[hostname] = None
            self.host_list_lock.release()
            return False

        ssh_proc = Process(pids[0])
        # Add host to known list with process to poll from
        self.host_list_lock.acquire()
        self.host_list[hostname] = ssh_proc
        self.host_list_lock.release()

        self.logger.debug(""Testing if connection was successful"")
        if ssh_proc.is_running():
            self.logger.debug(""Adding ssh master to monitor queue"")
            self.monitor_queue.put(HostMonitorJob(pids[0], hostname, self.host_list, self.host_list_lock))
            self.logger.debug(""SSH process still running. Connection was successful"")
            return True
        else:
            self.logger.debug(""SSH process has finished. Connection was not successful. Check if an ssh connection ""
                              ""is allowed or if the certificate has to be renewed"")
            return False

    def reconnect_with_host(self, hostname):
        """"""Re-establish master connection to host `hostname`

        :param hostname: Host to connect to
        :type hostname: str
        :return: Whether establishing the connection was successful or not
        :rtype: bool
        """"""

        # Check if really necessary
        self.logger.debug(""Reconnecting with %s"" % hostname)
        proc = self.host_list.get(hostname)
        if proc is not None:
            self.logger.debug(""Killing off leftover process"")
            proc.kill()

        # Start new connection
        if self.establish_master_connection(hostname):
            # Sync components
            self.logger.debug(""Syncinc components to remote host"")
            for group in self.config['groups']:
                for comp in group['components']:
                    if comp['host'] == hostname:
                        self.copy_component_to_remote(comp, comp['host'])
            return True
        else:
            return False

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        """"""Stop component `comp`.

        Invokes the lower level stop function depending on whether the component is run locally or on a remote host.

        :param comp: Component to stop
        :type comp: dict
        :return: None
        """"""

        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp)
        else:
            window = self.find_window(comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.debug(""Shutting down window..."")
                self.kill_window(window)
                self.logger.debug(""... done!"")

    def stop_remote_component(self, comp):
        """"""Stops remote component `comp`.

        Via ssh Hyperion is executed on the remote host in slave mode with the --kill option.

        :param comp: Component to stop
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']
        # invoke Hyperion in slave kill mode on remote host
        if not self.host_list[host]:
            self.logger.error(""Host %s is unreachable. Can not stop component %s"" % (host, comp_name))
            return

        cmd = (""ssh -F %s %s 'hyperion --config %s/%s.yaml slave --kill'"" % (
            config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, comp_name))
        self.send_main_session_command(cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):
        """"""Invoke dependency based start of component `comp`.

        Traverses the path of dependencies and invokes a call to ``start_component_without_deps`` for all found
        dependencies before calling it for `comp`.

        :param comp: Component to start
        :type comp: dict
        :return: Information on the start process
        :rtype: StartState
        """"""

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    # Wait component time for startup
                    sleep(self.get_component_wait(comp))

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                                state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 10:
                            return StartState.FAILED
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
            return StartState.ALREADY_RUNNING
        else:
            self.start_component_without_deps(comp)
        return StartState.STARTED

    def start_component_without_deps(self, comp):
        """"""Chooses which lower level start function to use depending on whether the component is run on a remote host or not.

        :param comp: Component to start
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']

        if host != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp_name, host))
            self.start_remote_component(comp)
        else:
            log_file = (""%s/%s/latest.log"" % (config.TMP_LOG_PATH, comp_name))
            window = self.find_window(comp_name)

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp_name)
                self.start_window(window, comp, log_file)
            else:
                self.logger.debug(""creating window '%s'"" % comp_name)
                window = self.session.new_window(comp_name)
                self.start_window(window, comp, log_file)

    def start_remote_component(self, comp):
        """"""Start component 'comp' on remote host.

        The remote component is started by invoking Hyperion over ssh in slave mode.

        :param comp: Component to start
        :type comp: dict
        :return: None
        """"""

        comp_name = comp['name']
        host = comp['host']
        # invoke Hyperion in slave mode on each remote host

        if not self.host_list[host]:
            self.logger.error(""Hot %s is not reachable. Can not start component %s"" % (host, comp_name))
            return

        cmd = (""ssh -F %s %s 'hyperion --config %s/%s.yaml slave'"" % (
            config.CUSTOM_SSH_CONFIG_PATH, host, config.TMP_SLAVE_DIR, comp_name))
        self.send_main_session_command(cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        """"""Runs component check for `comp` and returns status.

        If `comp` is run locally the call is redirected to ``check_local_component``. If the `comp` is run on a remote
        host the function checks, if the host is reachable and on success issues an ssh command over the master
        connection which starts Hyperion in slave mode with the check option. The return value of the call is then
        interpreted for further processing.

        :param comp: Component to check
        :type comp: dict
        :return: State of the component
        :rtype: CheckState
        """"""
        if self.run_on_localhost(comp):
            ret = self.check_local_component(comp)

            pid = ret[0]
            if pid != 0:
                self.monitor_queue.put(LocalComponentMonitoringJob(pid, comp['name']))
            return ret[1]
        else:
            self.logger.debug(""Starting remote check"")
            if self.host_list.get(comp['host']) is not None:
                p = Popen(['ssh', '-F', config.CUSTOM_SSH_CONFIG_PATH, comp['host'], 'hyperion --config %s/%s.yaml slave -c' %
                           (config.TMP_SLAVE_DIR, comp['name'])], stdin=PIPE, stdout=PIPE, stderr=PIPE)

                while p.poll() is None:
                    sleep(.5)
                pid = int(p.stdout.readlines()[-1])

                if pid != 0:
                    self.logger.debug(""Got remote pid %s for component %s"" % (pid, comp['name']))
                    self.monitor_queue.put(RemoteComponentMonitoringJob(pid, comp['name'], comp['host'], self.host_list))
                rc = CheckState(p.returncode)
                try:
                    return rc
                except ValueError:
                    self.logger.error(""Hyperion is not installed on host %s!"" % comp['host'])
                    return CheckState.NOT_INSTALLED
            else:
                self.logger.error(""Host %s is unreachable. Can not run check for component %s!"" % (comp['host'],
                                                                                                   comp['name']))
                return CheckState.UNREACHABLE

    ###################
    # CLI Functions
    ###################
    def list_components(self):
        """"""List all components used by the current configuration.

        :return: List of components
        :rtype: list of str
        """"""

        return [self.nodes.get(node).comp_name for node in self.nodes]

    def start_by_cli(self, comp_name):
        """"""Interface function for starting component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to start
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')

        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return

        logger.info(""Starting component '%s' ..."" % comp_name)
        ret = self.start_component(comp)
        if ret is StartState.STARTED:
            logger.info(""Started component '%s'"" % comp_name)
            sleep(self.get_component_wait(comp))
            ret = self.check_component(comp)
            logger.info(""Check returned status: %s"" % ret.name)
        elif ret is StartState.FAILED:
            logger.info(""Starting '%s' failed!"" % comp_name)
        elif ret is StartState.ALREADY_RUNNING:
            logger.info(""Aborted '%s' start: Component is already running!"" % comp_name)

    def stop_by_cli(self, comp_name):
        """"""Interface function for stopping component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to stop
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')
        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return
        logger.info(""Stopping component '%s' ..."")
        self.stop_component(comp)
        sleep(2)
        ret = self.check_component(comp)
        logger.info(""Check returned status: %s"" % ret.name)

    def check_by_cli(self, comp_name):
        """"""Interface function for checking component by name `comp_name` from the cli.

        Logging information is provided on the INFO level.

        :param comp_name: Name of the component to check
        :type comp_name: str
        :return: None
        """"""

        logger = logging.getLogger('CLI-RESPONSE')
        logger.info(""Checking component %s ..."" % comp_name)
        try:
            comp = self.get_component_by_name(comp_name)
        except exceptions.WindowNotFoundException as e:
            logger.warning(e.message)
            return
        ret = self.check_component(comp)
        logger.info(""Check returned status: %s"" % ret.name)

    def start_clone_session_and_attach(self, comp_name):
        """"""Interface function for show term of component by name `comp_name` from the cli. !!(NYI)!!

        :param comp_name: Name of the component to show
        :type comp_name: str
        :return: None
        """"""

        self.logger.debug(""NYI"")

    def show_comp_log(self, comp_name):
        """"""Interface function for viewing the log of component by name `comp_name` from the cli. !!(NYI)!!

        :param comp_name: Name of the component whose log to show
        :type comp_name: str
        :return: None
        """"""

        self.logger.debug(""NYI"")

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        """"""Get a list of all components that `comp` depends on.

        :param comp: Component to get dependencies from
        :type comp: dict
        :return: List of components
        :rtype: list of Node
        """"""

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        """"""Check if 'hostname' resolves to localhost.

        :param hostname: Name of host to check
        :type hostname: str
        :return: Whether 'host' resolves to localhost or not
        :rtype: bool
        """"""

        try:
            hn_out = socket.gethostbyname('%s' % hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            self.logger.error(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)


    def run_on_localhost(self, comp):
        """"""Check if component 'comp' is run on localhost or not.

        :param comp: Component to check
        :type comp: dict
        :return: Whether component is run on localhost or not
        :rtype: bool
        """"""

        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        """"""Kill tmux session by name 'name' on host 'host'

        :param name: Name of the session to kill
        :type name: str
        :param host: Host that the session runs on
        :type host: str
        :return: None
        """"""

        cmd = ""ssh -F %s -t %s 'tmux kill-session -t %s'"" % (config.CUSTOM_SSH_CONFIG_PATH, host, name)
        self.send_main_session_command(cmd)

    def start_clone_session(self, comp):
        """"""Start a clone session of the master session and open the window of component `comp`.

        Because the libtmux library does not provide functions to achieve this, a bash script is run to automatize the
        process.

        :param comp: Component whose window is to be shown in the cloned session
        :type comp_name: str
        :returns None
        """"""

        comp_name = comp['name']
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, self.session_name, comp_name)
        self.send_main_session_command(cmd)

    def start_remote_clone_session(self, comp):
        """"""Start a clone session of the remote slave session and open the window of component `comp`.

        Same as ``start_clone_session`` only that the bash script is fed into a ssh command issued over the main window
        of the master session.

        :param comp: Component whose window is to be shown in the clone session
        :type comp: dict
        :return:
        """"""

        session_name = 'slave-session'
        comp_name = comp['name']
        hostname = comp['host']

        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh -F %s %s 'bash -s' < %s"" % (config.CUSTOM_SSH_CONFIG_PATH, hostname, remote_cmd)
        self.send_main_session_command(cmd)

    ###################
    # Safe shutdown
    ###################
    def signal_handler(self, signum, frame):
        """"""Handler that invokes cleanup on a received signal.

        :param signum: Signal signum
        :type int
        :param frame:
        :return: None
        """"""
        self.logger.debug(""received signal %s. Running cleanup"" % signum)
        self.cleanup()

    def cleanup(self, full=False, status=0):
        """"""Clean up for safe shutdown.

        Kills the monitoring thread and if full shutdown is requested also the ssh slave sessions and master connections
        and then shuts down the local tmux master session.

        :param full: Whether everything shall be shutdown or not
        :type full: bool
        :return: None
        """"""

        self.logger.info(""Shutting down safely..."")

        self.logger.debug(""Killing monitoring thread"")
        self.mon_thread.kill()

        if full:
            self.logger.debug(""Chose full shutdown. Killing remote and main sessions"")

            for host in self.host_list:
                window = self.find_window('ssh-%s' % host)

                if window:
                    self.logger.debug(""Killing remote slave session of host %s"" % host)
                    self.kill_remote_session_by_name(""slave-session"", host)
                    self.logger.debug(""Close ssh-master window of host %s"" % host)
                    self.kill_window(window)

            self.kill_session_by_name(self.session_name)
        self.logger.info(""... Done"")
        exit(status)


class SlaveLauncher(AbstractController):
    """"""Controller class that performs a single slave execution task.""""""

    def __init__(self, configfile, kill_mode=False, check_mode=False):
        """"""Initializes slave.

        :param configfile: Path to configuration file (component configuration)
        :type configfile: str
        :param kill_mode: Whether the slave was started in kill mode or not
        :type kill_mode: bool
        :param check_mode: Whether the slave was started in check mode or not
        :type check_mode: bool
        """"""

        super(SlaveLauncher, self).__init__(configfile)
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.debug('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.debug('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.debug(""No slave session found on server. Aborting"")
            # Print fake pid
            print(0)
            exit(CheckState.STOPPED.value)

        if configfile:
            try:
                self.load_config(configfile)
            except IOError:
                exit(1)
            self.window_name = self.config['name']
            self.log_file = (""%s/%s/latest.log"" % (config.TMP_LOG_PATH, self.window_name))
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def init(self):
        """"""Runs the mode specified by the slave execution call (start/stop or preparation for check)

        :return: None
        """"""
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = self.find_window(self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.debug(""Shutting down window..."")
                    self.kill_window(window)
                    self.logger.debug(""... done!"")
            elif not self.kill_mode:
                self.logger.debug(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                self.start_window(window, self.config, self.log_file)

            else:
                self.logger.debug(""There is no component running by the name '%s'. Exiting kill mode"" %
                                  self.window_name)

    def run_check(self):
        """"""Run check for the current component.

        :return: Status of the component
        :rtype: CheckState
        """"""
        if not self.config:
            self.logger.error(""Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error(""Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        ret = self.check_local_component(self.config)
        print(ret[0])
        exit(ret[1].value)
/n/n/n",0
175,175,671a06608c5210f205d93c0c235c94a8783892b9,"/hyperion/lib/monitoring/threads.py/n/nfrom threading import Thread, Lock
import logging
import sys
import time
import hyperion.lib.util.config as config
from os import system
from subprocess import call
from psutil import Process, NoSuchProcess
is_py2 = sys.version[0] == '2'
if is_py2:
    import Queue as Queue
else:
    import queue as Queue
    

class ComponentMonitorJob(object):
    """"""Abstract class that represents a component monitoring job (local or remote).""""""

    def __init__(self, pid, comp_name):
        """"""Initializes component monitoring job.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""
        self.pid = pid
        self.comp_name = comp_name

    def run_check(self):
        """"""You need to override this function in monitoring subclasses. It is called in the main monitoring thread.

        :return: True on a successful check, otherwise a CrashEvent is generated
        :rtype: bool or CrashEvent
        """"""


class LocalComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a local component monitoring job.""""""

    def __init__(self, pid, comp_name):
        """"""Creates a monitoring job for a local component.

        :param pid: Process id of the component
        :type pid: int
        :param comp_name: Name of the component
        :type comp_name: str
        """"""

        super(LocalComponentMonitoringJob, self).__init__(pid, comp_name)

    def run_check(self):
        """"""Runs a check if the pid exists and has not finished yet.

        :return: True if the component is running, otherwise returns a generated ``LocalCrashEvent``
        :rtype bool or LocalCrashEvent
        """"""
        try:
            proc = Process(self.pid)
            if proc.is_running():
                return True
        except NoSuchProcess:
            pass
        return CrashEvent(self.comp_name)

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for local component %s with pid %s"" % (self.comp_name, self.pid)


class RemoteComponentMonitoringJob(ComponentMonitorJob):
    """"""Class that represents a remote component monitoring job.""""""

    def __init__(self, pid, comp_name, hostname, host_status):
        """"""Creates a remote component monitoring job.

        :param pid: Process id on the remote machine
        :type pid: int
        :param comp_name: Name of the monitored component
        :type comp_name: str
        :param hostname: Name of the host running the component
        :type hostname: str
        """"""

        super(RemoteComponentMonitoringJob, self).__init__(pid, comp_name)
        self.hostname = hostname
        self.host_status = host_status

    def run_check(self):
        """"""Runs a check if a remote process is still running.

        :return: True if the component is still running or the host is not reachable, otherwise a ``RemoteCrashEvent`` is generated.
        :rtype: bool or RemoteCrashEvent
        """"""

        if self.host_status.get(self.hostname):
            cmd = 'ssh -F %s %s ""ps -p %s > /dev/null""' % (config.CUSTOM_SSH_CONFIG_PATH, self.hostname, self.pid)
            if call(cmd, shell=True) == 0:
                return True
            else:
                return RemoteCrashEvent(self.comp_name, self.hostname)
        # Return true because no information can be retrieved. The connection to the host has to be reestablished first.
        return True

    def info(self):
        """"""Generate a status information for the job describing what is being monitored.

        :return: Information about this job
        :rtype: str
        """"""

        return ""Running check for remote component %s with pid %s on host %s"" % (self.comp_name, self.pid,
                                                                                 self.hostname)


class HostMonitorJob(object):
    """"""Class representing a host monitoring job.""""""
    def __init__(self, pid, hostname, host_status, host_lock):
        """"""Create host monitoring job.

        :param pid: Process id of the ssh connection
        :type pid: int
        :param hostname: Name of the host connected to
        :type hostname: str
        :param host_status: Status of the used hosts
        :type host_status: dict
        :param host_lock: Lock that has to be acquired in order to write to the host status dictionary.
        :type host_lock: Lock
        """"""
        self.pid = pid
        self.hostname = hostname
        self.host_status = host_status
        self.host_lock = host_lock

    def run_check(self):
        try:
            proc = Process(self.pid)
            if proc.is_running() and system(""exec >(ping %s -c 10 >/dev/null) </dev/null"" % self.hostname) is 0:
                return True
        except NoSuchProcess:
            pass

        self.host_lock.acquire()
        self.host_status[self.hostname] = None
        self.host_lock.release()

        return DisconnectEvent(self.hostname)

    def info(self):
        return ""Running ssh host check for %s with pid %s"" % (self.hostname, self.pid)


class CrashEvent(object):
    """"""Superclass to model a component crash.

    Provides the name of the crashed component.""""""

    def __init__(self, comp_name):
        """"""Initializes the crash event assigning the component name

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        self.comp_name = comp_name


class LocalCrashEvent(CrashEvent):
    """"""Crash event subclass for local component crashes.

    Provides the name of the crashed component and a short message.
    """"""

    def __init__(self, comp_name):
        """"""Creates a local crash event class with a component name and generates a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        """"""

        super(LocalCrashEvent, self).__init__(comp_name)
        self.message = 'Component %s crashed on localhost' % comp_name


class RemoteCrashEvent(CrashEvent):
    """"""Crash event subclass for remote component crashes.

    Provides the name of the crashed component along with the host it ran on and a short message.
    """"""

    def __init__(self, comp_name, hostname):
        """"""Creates a remote crash event with a component name and a host generating a short message.

        :param comp_name: Name of the crashed component
        :type comp_name: str
        :param hostname: Name of the host the component was running on
        :type hostname: str
        """"""

        super(RemoteCrashEvent, self).__init__(comp_name)
        self.hostname = hostname
        self.message = 'Component %s crashed on remote host %s' % (comp_name, hostname)


class DisconnectEvent(object):
    """"""Class representing a disconnect event for remote hosts.""""""

    def __init__(self, hostname):
        """"""Creates a disconnect event with a hostname and generates a short message.""""""
        self.hostname = hostname
        self.message = 'Lost connection to remote host %s' % hostname


class MonitoringThread(Thread):
    """"""This class is monitoring thread that extends the threading.Thread class.""""""

    def __init__(self, queue):
        """"""Initializes the monitoring thread with its input queue.

        :param queue: Input queue the monitor retrieves its jobs from
        :type queue: Queue.Queue
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Initialized thread"")
        super(MonitoringThread, self).__init__()
        self.job_queue = queue
        self.subscribed_queues = []
        self.end = False

    def kill(self):
        """"""Shuts down the thread by signalling the run function to end.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Killing process monitoring thread"")
        self.end = True

    def add_subscriber(self, queue):
        """"""Adds a subscriber to the list of queues to send notifications to.

        :param queue: Subscribing queue that will get notifications by this thread
        :type queue: Queue.Queue
        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.debug(""Added subscriber"")
        self.subscribed_queues.append(queue)

    def run(self):
        """"""Starts the monitoring thread.

        :return: None
        """"""

        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)
        logger.debug(""Started run funtion"")
        while not self.end:

            comp_jobs = []
            jobs = []
            already_handleled = {}
            # Get all enqueued jobs for this iteration
            while not self.job_queue.empty():
                mon_job = self.job_queue.get()
                if isinstance(mon_job, HostMonitorJob):
                    jobs.append(mon_job)
                if isinstance(mon_job, ComponentMonitorJob) and mon_job.comp_name not in already_handleled:
                    comp_jobs.append(mon_job)
                    already_handleled[mon_job.comp_name] = True

            # Reorder job list to first check the hosts, then check the components because this makes sense
            jobs.extend(comp_jobs)
            for mon_job in jobs:
                logger.debug(mon_job.info())
                ret = mon_job.run_check()
                if ret is True:
                    logger.debug(""S'all good man"")
                    # If job is ok, put it back for the next iteration
                    self.job_queue.put(mon_job)
                else:
                    # If job is not ok, notify subscribers
                    logger.debug(""Check failed, notifying subscribers"")
                    for subscriber in self.subscribed_queues:
                        subscriber.put(ret)

            time.sleep(1)
/n/n/n",1
108,108,bd7dcfb73ae422f00ac14aa77c2f0ab46818b95d,"acedu/hwmllib.py/n/nimport codecs
import locale
import sys

# Wrap sys.stdout into a StreamWriter to allow writing unicode.
sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout) 




import acsetup
import acedu.paths
from acedu.paths import meta

from fsitem import File

from acminidom import getText, attributesOf, parse

import os

import pprint

from hwml import Problem, Assignment

from collections import defaultdict

import string

class HWML_processor(object):

	def __init__(self):
		self.prepared = False

	def prepare_for_processing(self,verbose=True):
	
		if self.prepared:
			return self.code_list

		self.d = acedu.paths.corresponding()

		# I need a pretty printer for debugging purposes
		self.p = pprint.PrettyPrinter(indent=2)

		self.homeworks = []

		for homework_file in self.d:
			try:
				if homework_file.basename() != "".DS_Store"":
					self.homeworks.append(parse(homework_file.path))
			except:
				print ""parsing error in ""+homework_file.path
				raise

		self.assignments = []

		for homework in self.homeworks:
			for assignment in homework.getElementsByTagName(""assignment""):
				self.assignments.append(assignment)

		if True:
			for assignment in self.assignments:
				attrs = attributesOf(assignment)
				if verbose: print attrs[""student""]
				if verbose: print attrs[""book_edition""]
				if verbose: print attrs[""chapter""]
				problems = assignment.getElementsByTagName(""problem"")
				for problem in problems:
					attrs = attributesOf(problem)
					if verbose: print attrs[""number""]
					if verbose: print getText(problem.childNodes)

		self.assignments = map(Assignment,self.assignments)

		self.book_editions = defaultdict(list)
		for assignment in self.assignments:
			self.book_editions[assignment.book_edition].append(assignment)
		#print self.book_editions

		for assignment in self.assignments:
			if(assignment.chapter != int(sys.argv[1])):
				print assignment.student
				assert(assignment.chapter == int(sys.argv[1]))
			num_probs = len(assignment.problems)
			if verbose: print ""num_probs was ""+str(num_probs)+"" for student ""+assignment.student

		self.problems = []

		for assignment in self.assignments:
			for problem in assignment.problems:
				prob_id = (assignment.book_edition,assignment.chapter,problem.number)
				prob_data = (assignment.student,problem.text)
				prob = (prob_id,prob_data)
				self.problems.append(prob)

		if verbose: print len(self.problems)
		
		self.problem_groups = defaultdict(list)

		self.code_list = []
		for problem in self.problems:
			prob_id, prob_data = problem
			assignment_student,problem_text = prob_data
			prob_code_data = (assignment_student,prob_id)
			prob_code = str(hash(prob_code_data))
			self.code_list.append((prob_code,prob_code_data))
			self.problem_groups[prob_id].append(prob_code+""\n""+(""=""*50)+""\n""+problem_text)

		if verbose: self.p.pprint(self.code_list)

		#print ""by groups""
		self.count_by_groups = 0
		for group in self.problem_groups.values():
			self.count_by_groups += len(group)
		if verbose: print self.count_by_groups
		
		self.prepared = True
		
		return self.code_list
		
	def process_hwml(self,verbose=True):
		self.prepare_for_processing(verbose)

		another_count_by_groups = 0

		result = {""value"":""""}

		def output(value):
			if verbose:
				print value
			result[""value""] = result[""value""] + str(value) + ""\n""

		for key,value in self.problem_groups.items():
			book_edition,chapter,problem_number = key
			output( ""In book edition ""+str(book_edition)+"" on problem ""+problem_number )

			for item in value:
				output( ""=""*50 )
				output( item )
				output( ""=""*50 )
				another_count_by_groups += 1
				output( ""problem count is now: ""+str(another_count_by_groups) )
				
		return result[""value""]
			
	def detect_cheating(self,verbose=True):
		self.prepare_for_processing(verbose)
		
		problem_text_students = defaultdict(list)
		
		for assignment in self.assignments:
			for problem in assignment.problems:
				p_text = string.join(string.split(problem.text))
				problem_text_students[p_text].append(assignment.student)

		results = []

		for t,s in problem_text_students.items():
			if len(s) > 1:
				if verbose: print (t,s)
				results.append((t,s))

		return results

def process_problem_scores(code_list,the_file = False):
	problems = []

	if the_file:
		pass
	else:
		the_file = meta().file_with_name(""problem_scores.csv"").open(""w"")

	class problem(object):
		def __init__(self,student,book_edition,chapter,number,code):
			self.student = student
			self.book_edition = book_edition
			self.chapter = chapter
			self.number = number
			self.code = code

	from collections import defaultdict

	problem_groups = defaultdict(list)

	for code_item in code_list:
		(prob_code,prob_code_data) = code_item
		(assignment_student,prob_id) = prob_code_data
		(book_edition,chapter,problem_number) = prob_id
		problem_groups[prob_id].append( problem( assignment_student, book_edition, chapter, problem_number, prob_code ) )
	
	another_count_by_groups = 0

	header = ""Answer ID Number, Book Edition, Student Name, Problem Number, Problem Order, Score, Notes""
	if the_file:
		the_file.write(header+""\n"")
	else:
		print header

	for key,value in problem_groups.items():
		book_edition,chapter,problem_number = key
		for item in value:
			another_count_by_groups += 1
			to_write = str(item.code)+"", ""+str(book_edition)+"", ""+str(item.student)+"", ""+str(item.number)+"", ""+str(another_count_by_groups)
			if the_file:
				the_file.write(to_write+""\n"")
			else:
				print to_write
		
		
def process_hwml():
	hp = HWML_processor()
	hp.process_hwml()

def detect_cheating():
	hp = HWML_processor()
	hp.detect_cheating()
/n/n/nacedu/paths.py/n/nimport acsetup
import fsitem
from decorator import Decorator

def courses():
	h = fsitem.home()
	return h[""Documents""][""Courses""]

def year(yr):
	class _Year(Decorator):
		def is_year(self):
			return True
		def semester(self,name):
			s = str(name)+"" ""+str(yr)
			s = _Semester(self.__getitem__(s))
			s.year = yr
			s.semester = name
			return s
	return _Year(courses()[str(yr)])

class _Course(Decorator):
	def is_course(self):
		return True
	def data(self):
		return self.__getitem__(""Data"")
	def code(self):
		return self.__getitem__(""Code"")
	def lesson_plans(self):
		return self.__getitem__(""Lesson Plans"")

class _Semester(Decorator):
	def is_semester(self):
		return True
	def data(self):
		return self.__getitem__(""Data"")
	def code(self):
		return self.__getitem__(""Code"")
	def course(self,name):
		c = _Course(self.__getitem__(name))
		c.semester = self.semester
		c.year = self.year
		c.course = name
		return c
	def courses(self):
		return [self.course(item.basename()) for item in self.folders() if 2 == len(item.basename().split())]
		

def position(where=None):
	if where is None:
		ancestors = list(fsitem.current().ancestors())
	else:
		ancestors = list(where.ancestors())
	c = courses()
	c_a = [item.path for item in c.ancestors()]
	l = [item for item in ancestors if item.path not in c_a and item.path != c.path]
	if len(l) > 0:
		p = l[-1]
		yr = int(p.basename())
		yr = year(yr)
	if len(l) > 1:
		p = l[-2]
		full_semester = p.basename()
		semester,space,ear = full_semester.partition("" "")
		sem = yr.semester(semester)
	if len(l) > 2:
		p = l[-3]
		course = sem.course(p.basename())
		return course
	if len(l) > 1:
		return sem
	if len(l) > 0:
		return yr

def corresponding(where=None):
	if where is None:
		where = fsitem.current()
	p = position()
	d = p.data()
	s = where - p.code()
	return d[s]

def meta(where=None):
	base = corresponding(where)
	parent = base.parent()
	meta_dir = parent.folder_with_name(base.basename()+"".meta"")
	return meta_dir

if __name__ == ""__main__"":
	print position().__dict__.values()[0].__dict__
/n/n/nfsitem.py/n/n""""""
	Use
		file:///Users/chenan/Other/python-2.7.2-docs-html/library/os.html#files-and-directories
	and
		file:///Users/chenan/Other/python-2.7.2-docs-html/library/os.path.html

	but make a more OO interface to dealing with files and folders.

	Envisioned class hierarchy would be something like:

	FSItem
		Folder
		File

	and there would be intuitive properties/methods
	for FSItem, Folder, and File
""""""

import os
import os.path
import fnmatch

def specialized(p):
	if os.path.isdir(p):
		return Folder(p)
	elif os.path.isfile(p):
		return File(p)
	elif os.path.islink(p):
		return Link(p)
	else:
		return FSItem(p)

def fs_object(p):
	p = os.path.abspath(p)
	return specialized(p)

class FSPath(object):
	def __init__(self,path):
		self.path = path
	def __sub__(self,other):
		if (self.path.startswith(other.path)):
			r = self.path[len(other.path):]
			assert((other.path+r) == self.path)
			return r[1:]
		else:
			raise IndexError, (other.path,self.path)
	
	# from os.path
	def abspath(self):
		return FSPath(os.path.abspath(self.path))
	def basename(self):
		return os.path.basename(self.path)
	def dirname(self):
		return FSPath(os.path.dirname(self.path))
	def exists(self):
		return FSPath(os.path.exists(self.path))
	def lexists(self):
		return FSPath(os.path.lexists(self.path))
	def expanduser(self):
		return FSPath(os.path.expanduser(self.path))
	def expandvars(self):
		return FSPath(os.path.expandvars(self.path))
	def getatime(self):
		return os.path.getatime(self.path)
	def getctime(self):
		return os.path.getctime(self.path)
	def getsize(self):
		return os.path.getsize(self.path)
	def isabs(self):
		return os.path.isabs(self.path)
	def isfile(self):
		return os.path.isfile(self.path)
	def isdir(self):
		return os.path.isdir(self.path)
	def islink(self):
		return os.path.islink(self.path)
	def ismount(self):
		return os.path.ismount(self.path)
	def normcase(self):
		return FSPath(os.path.normcase(self.path))
	def realpath(self):
		return FSPath(os.path.realpath(self.path))
	def relpath(self,start=None):
		if start is None:
			return FSPath(os.path.relpath(self.path))
		else:
			return FSPath(os.path.relpath(self.path,start))
	def samefile(self,other):
		if isinstance(other,FSPath):
			return os.path.samefile(self.path,other.path)
		else:
			return os.path.samefile(self.path,other)
	def split(self):
		head,tail = os.path.split(self.path)
		return (FSPath(head),FSPath(tail))
	def splitdrive(self):
		return os.path.splitdrive(self.path)
	def splitext(self):
		return os.path.splitext(self.path)
	def splitunc(self):
		return os.path.splitunc(self.path)
	
	# from os
	def access(self,mode):
		return os.access(self.path,mode)
	def chdir(self):
		return os.chdir(self.path)
		
	@staticmethod
	def getcwd(self):
		return FSPath(os.getcwd())
	
	def chflags(self,flags):
		return os.chflags(self.path,flags)
	def chroot(self):
		return os.chroot(self.path)
	def chmod(self,mode):
		return os.chmod(self.path,mode)
	def chown(self,uid=-1,gid=-1):
		return os.chown(self.path,uid,gid)
	def lchflags(self,flags):
		return os.lchflags(self.path,flags)
	def lchmod(self,mode):
		return os.lchmod(self.path,mode)
	def lchown(self,uid=-1,gid=-1):
		return os.lchown(self.path,uid,gid)
	def link(self,link_name):
		return os.link(self.path,link_name)
	def listdir(self):
		# need to wrap this better in a subclass
		return os.listdir(self.path)
	def lstat(self):
		return os.lstat(self.path)
	def mkfifo(self,mode=None):
		if mode is None:
			return os.mkfifo(self.path)
		else:
			return os.mkfifo(self.path,mode)
	# mknod, major, minor, makedev not implemented
	def mkdir(self,mode=None):
		if mode is None:
			return os.mkdir(self.path)
		else:
			return os.mkdir(self.path,mode)
	def makedirs(self,mode=None):
		if mode is None:
			return os.makedirs(self.path)
		else:
			return os.makedirs(self.path,mode)
	# pathconf, pathconf_names not implemented
	def readlink(self):
		return FSPath(os.readlink(self.path))
	def remove(self):
		return os.remove(self.path)
	def removedirs(self):
		return os.removedirs(self.path)
	def rename(self,other):
		if isinstance(other,FSPath):
			return os.rename(self.path,other.path)
		else:
			return os.rename(self.path,other)
	def renames(self,other):
		if isinstance(other,FSPath):
			return os.renames(self.path,other.path)
		else:
			return os.renames(self.path,other)
	def rmdir(self):
		return os.rmdir(self.path)
	def stat(self):
		return os.stat(self.path)
	def statvfs(self):
		return os.statvfs(self.path)
	def symlink(self,link_name):
		return os.symlink(self.path,link_name)
	def unlink(self):
		return os.unlink(self.path)
	def utime(self,times):
		return os.utime(self.path,times)
	def walk(self,topdown=True, onerror=None, followlinks=False):
		return os.walk(self.path,topdown,onerror,followlinks)


class FSPathList(list):
	def _raw(self):
		return map(lambda x: x.path,self)
	
	# from os.path
	def commonprefix(self):
		return FSPath(os.path.commonprefix(self._raw()))
	def join(self):
		return FSPath(*(self._raw()))

class FSItem(FSPath):
	def __init__(self,path):
		super(FSItem,self).__init__(path)
		assert(self.isabs())
		assert(self.lexists())
	def parent(self):
		p = self.dirname()
		return Folder(p.path)
	def ancestors(self):
		c = FSItem(self.path)
		p = c.parent()
		while c.path is not p.path:
			yield p
			c = p
			p = c.parent()
		
	def common_parent(self,other):
		l = FSPathList()
		l.append(self)
		l.append(other)
		r = l.commonprefix()
		if r.isdir():
			return Folder(r.path)
		else:
			return Folder(r.dirname().path)
	def walk(self,*args,**kwargs):
		r = super(FSItem,self).walk(*args,**kwargs)
		for root, dirs, files in r:
			folder_list = [Folder(os.path.join(root,dir)) for dir in dirs]
			file_list = [File(os.path.join(root,a_file)) for a_file in files]
			yield (Folder(root),folder_list,file_list)

class Folder(FSItem):
	def __init__(self,path):
		super(Folder,self).__init__(path)
		assert(self.isdir())
	def __iter__(self):
		r = self.items()
		return r
	def items(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			yield specialized(p)
	def __getitem__(self,name):
		for item in self.items():
			if item.basename() == name:
				return item
		if name == """":
			return self
		raise IndexError, (name,self.path)
	def folders(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			if os.path.isdir(p):
				yield Folder(p)
	def files(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			if os.path.isfile(p):
				yield File(p)
	
	def create(self,what,name):
		t = os.path.join(self.path,name)
		if what is Folder:
			os.mkdir(t)
			return Folder(t)
		elif what is File:
			f = open(t,""w"")
			f.close()
			return File(t)
		else:
			raise NotImplementedError
	
	def file_with_name(self,name):
		try:
			the_file = self[name]
		except IndexError:
			the_file = self.create(File,name)
		return the_file

	def folder_with_name(self,name):
		try:
			the_folder = self[name]
		except IndexError:
			the_folder = self.create(Folder,name)
		return the_folder

def safe_open(path_or_File,mode):
	if isinstance(path_or_File,File):
		return path_or_File.open(mode)
	else:
		return open(path_or_File,mode)

class Line(object):
	def __init__(self,file,number,text,column=0,original=None):
		self.file = file
		self.number = number
		self.text = text
		self.column = column
		if original is None:
			self.original = self
		else:
			self.original = original
	def clone(self,file=None,number=None,text=None,column=None,original=None,leading_whitespace=None):
		result = Line(self.file,self.number,self.text,self.column,self.original)
		if file is None: pass
		else: result.file = file
		if number is None: pass
		else: result.number = number
		if text is None: pass
		else: result.text = text
		if column is None: pass
		else: result.column = column
		if original is None: pass
		else: result.original = original
		
		if leading_whitespace is None: pass
		else
			try:
				result.leading_whitespace = self.leading_whitespace
			except:
				pass
			
		return result
		
	def strip_from(self,text):
		l,m,r = self.text.rpartition(text)
		result = self.clone()
		if len(m):
			result.text = l
		else:
			result.text = r
		return result
	def strip(self):
		new_text = self.text.lstrip()
		leading_whitespace = self.text[0:len(self.original.text) - len(new_text)]
		column = len(leading_whitespace)
		result = self.clone(
			text = new_text.rstrip(),
			column = column,
			leading_whitespace = leading_whitespace
		)
		return result
	def __len__(self):
		return len(self.text)
	def split(self):
		return self.text.split()

class File(FSItem):
	def __init__(self,path):
		super(File,self).__init__(path)
		assert(self.isfile())
	def read(self):
		f = open(self.path,""rU"")
		r = f.read()
		f.close()
		return r
	def readlines(self):
		f = open(self.path,""rU"")
		r = f.readlines()
		f.close()
		return r
	def read_Lines(self):	
		with open(self.path,""rU"") as f:
			count = 0
			for l in f:
				count += 1
				yield Line(self,count,l)
	def write(self,o):
		f = open(self.path,""wb"")
		r = f.write(o)
		f.close()
		return r
	def writelines(self,o):
		f = open(self.path,""wb"")
		r = f.writelines(o)
		f.close()
		return r
	def open(self,mode):
		return open(self.path,mode)
		

class Link(FSItem):
	def __init__(self,path):
		super(Link,self).__init__(path)
		assert(self.islink())

def current():
	r = Folder(os.getcwd())
	return r

def root():
	return Folder(""/"")

def home():
	return Folder(os.path.expanduser(""~""))

def FSItemList(FSPathList):
	def common_parent(self):
		r = None
		for item in self:
			if r is None:
				r = item
			else:
				r = r.common_parent(item)
		return r
	def filter(self,pattern):
		return fnmatch.filter(self,pattern)

if __name__ == ""__main__"":
	print ""current has ""
	for item in current():
		print item.path
	print ""root has ""
	for item in root():
		print item.path
	print ""home has ""
	for item in home():
		print item.path
/n/n/n",0
109,109,bd7dcfb73ae422f00ac14aa77c2f0ab46818b95d,"/acedu/hwmllib.py/n/nimport codecs
import locale
import sys

# Wrap sys.stdout into a StreamWriter to allow writing unicode.
sys.stdout = codecs.getwriter(locale.getpreferredencoding())(sys.stdout) 




import acsetup
import acedu.paths
from fsitem import File

from acminidom import getText, attributesOf, parse

import os

import pprint

from hwml import Problem, Assignment

from collections import defaultdict

import string

class HWML_processor(object):

	def prepare_for_processing(self):

		self.d = acedu.paths.corresponding()

		# I need a pretty printer for debugging purposes
		self.p = pprint.PrettyPrinter(indent=2)

		self.homeworks = []

		for homework_file in self.d:
			try:
				if homework_file.basename() != "".DS_Store"":
					self.homeworks.append(parse(homework_file.path))
			except:
				print ""parsing error in ""+homework_file.path
				raise

		self.assignments = []

		for homework in self.homeworks:
			for assignment in homework.getElementsByTagName(""assignment""):
				self.assignments.append(assignment)

		if True:
			for assignment in self.assignments:
				attrs = attributesOf(assignment)
				print attrs[""student""]
				print attrs[""book_edition""]
				print attrs[""chapter""]
				problems = assignment.getElementsByTagName(""problem"")
				for problem in problems:
					attrs = attributesOf(problem)
					print attrs[""number""]
					print getText(problem.childNodes)

		self.assignments = map(Assignment,self.assignments)

		self.book_editions = defaultdict(list)
		for assignment in self.assignments:
			self.book_editions[assignment.book_edition].append(assignment)
		#print self.book_editions

		for assignment in self.assignments:
			assert(assignment.chapter == int(sys.argv[1]))
			num_probs = len(assignment.problems)
			print ""num_probs was ""+str(num_probs)+"" for student ""+assignment.student

		self.problems = []

		for assignment in self.assignments:
			for problem in assignment.problems:
				prob_id = (assignment.book_edition,assignment.chapter,problem.number)
				prob_data = (assignment.student,problem.text)
				prob = (prob_id,prob_data)
				self.problems.append(prob)

		print len(self.problems)
		
		self.problem_groups = defaultdict(list)

		self.code_list = []
		for problem in self.problems:
			prob_id, prob_data = problem
			assignment_student,problem_text = prob_data
			prob_code_data = (assignment_student,prob_id)
			prob_code = str(hash(prob_code_data))
			self.code_list.append((prob_code,prob_code_data))
			self.problem_groups[prob_id].append(prob_code+""\n""+(""=""*50)+""\n""+problem_text)

		self.p.pprint(self.code_list)

		#print ""by groups""
		self.count_by_groups = 0
		for group in self.problem_groups.values():
			self.count_by_groups += len(group)
		print self.count_by_groups
		
	def process_hwml(self):
		self.prepare_for_processing()

		another_count_by_groups = 0

		for key,value in self.problem_groups.items():
			book_edition,chapter,problem_number = key
			print ""In book edition ""+str(book_edition)+"" on problem ""+problem_number

			for item in value:
				print ""=""*50
				print item
				print ""=""*50
				another_count_by_groups += 1
				print ""problem count is now: ""+str(another_count_by_groups)
			
	def detect_cheating(self):
		self.prepare_for_processing()
		
		problem_text_students = defaultdict(list)
		
		for assignment in self.assignments:
			for problem in assignment.problems:
				p_text = string.join(string.split(problem.text))
				problem_text_students[p_text].append(assignment.student)

		for t,s in problem_text_students.items():
			if len(s) > 1:
				print (t,s)

def process_problem_scores(code_list):
	problems = []

	class problem(object):
		def __init__(self,student,book_edition,chapter,number,code):
			self.student = student
			self.book_edition = book_edition
			self.chapter = chapter
			self.number = number
			self.code = code

	from collections import defaultdict

	problem_groups = defaultdict(list)

	for code_item in code_list:
		(prob_code,prob_code_data) = code_item
		(assignment_student,prob_id) = prob_code_data
		(book_edition,chapter,problem_number) = prob_id
		problem_groups[prob_id].append(problem(assignment_student,book_edition,chapter,problem_number,prob_code))
	
	another_count_by_groups = 0

	print ""Answer ID Number, Book Edition, Student Name, Problem Number, Problem Order, Score, Notes""

	for key,value in problem_groups.items():
		book_edition,chapter,problem_number = key
		for item in value:
			another_count_by_groups += 1
			print str(item.code)+"", ""+str(book_edition)+"", ""+str(item.student)+"", ""+str(item.number)+"", ""+str(another_count_by_groups)
		
		
def process_hwml():
	hp = HWML_processor()
	hp.process_hwml()

def detect_cheating():
	hp = HWML_processor()
	hp.detect_cheating()
/n/n/n/fsitem.py/n/n""""""
	Use
		file:///Users/chenan/Other/python-2.7.2-docs-html/library/os.html#files-and-directories
	and
		file:///Users/chenan/Other/python-2.7.2-docs-html/library/os.path.html

	but make a more OO interface to dealing with files and folders.

	Envisioned class hierarchy would be something like:

	FSItem
		Folder
		File

	and there would be intuitive properties/methods
	for FSItem, Folder, and File
""""""

import os
import os.path
import fnmatch

def specialized(p):
	if os.path.isdir(p):
		return Folder(p)
	elif os.path.isfile(p):
		return File(p)
	elif os.path.islink(p):
		return Link(p)
	else:
		return FSItem(p)

def fs_object(p):
	p = os.path.abspath(p)
	return specialized(p)

class FSPath(object):
	def __init__(self,path):
		self.path = path
	def __sub__(self,other):
		if (self.path.startswith(other.path)):
			r = self.path[len(other.path):]
			assert((other.path+r) == self.path)
			return r[1:]
		else:
			raise IndexError, (other.path,self.path)
	
	# from os.path
	def abspath(self):
		return FSPath(os.path.abspath(self.path))
	def basename(self):
		return os.path.basename(self.path)
	def dirname(self):
		return FSPath(os.path.dirname(self.path))
	def exists(self):
		return FSPath(os.path.exists(self.path))
	def lexists(self):
		return FSPath(os.path.lexists(self.path))
	def expanduser(self):
		return FSPath(os.path.expanduser(self.path))
	def expandvars(self):
		return FSPath(os.path.expandvars(self.path))
	def getatime(self):
		return os.path.getatime(self.path)
	def getctime(self):
		return os.path.getctime(self.path)
	def getsize(self):
		return os.path.getsize(self.path)
	def isabs(self):
		return os.path.isabs(self.path)
	def isfile(self):
		return os.path.isfile(self.path)
	def isdir(self):
		return os.path.isdir(self.path)
	def islink(self):
		return os.path.islink(self.path)
	def ismount(self):
		return os.path.ismount(self.path)
	def normcase(self):
		return FSPath(os.path.normcase(self.path))
	def realpath(self):
		return FSPath(os.path.realpath(self.path))
	def relpath(self,start=None):
		if start is None:
			return FSPath(os.path.relpath(self.path))
		else:
			return FSPath(os.path.relpath(self.path,start))
	def samefile(self,other):
		if isinstance(other,FSPath):
			return os.path.samefile(self.path,other.path)
		else:
			return os.path.samefile(self.path,other)
	def split(self):
		head,tail = os.path.split(self.path)
		return (FSPath(head),FSPath(tail))
	def splitdrive(self):
		return os.path.splitdrive(self.path)
	def splitext(self):
		return os.path.splitext(self.path)
	def splitunc(self):
		return os.path.splitunc(self.path)
	
	# from os
	def access(self,mode):
		return os.access(self.path,mode)
	def chdir(self):
		return os.chdir(self.path)
		
	@staticmethod
	def getcwd(self):
		return FSPath(os.getcwd())
	
	def chflags(self,flags):
		return os.chflags(self.path,flags)
	def chroot(self):
		return os.chroot(self.path)
	def chmod(self,mode):
		return os.chmod(self.path,mode)
	def chown(self,uid=-1,gid=-1):
		return os.chown(self.path,uid,gid)
	def lchflags(self,flags):
		return os.lchflags(self.path,flags)
	def lchmod(self,mode):
		return os.lchmod(self.path,mode)
	def lchown(self,uid=-1,gid=-1):
		return os.lchown(self.path,uid,gid)
	def link(self,link_name):
		return os.link(self.path,link_name)
	def listdir(self):
		# need to wrap this better in a subclass
		return os.listdir(self.path)
	def lstat(self):
		return os.lstat(self.path)
	def mkfifo(self,mode=None):
		if mode is None:
			return os.mkfifo(self.path)
		else:
			return os.mkfifo(self.path,mode)
	# mknod, major, minor, makedev not implemented
	def mkdir(self,mode=None):
		if mode is None:
			return os.mkdir(self.path)
		else:
			return os.mkdir(self.path,mode)
	def makedirs(self,mode=None):
		if mode is None:
			return os.makedirs(self.path)
		else:
			return os.makedirs(self.path,mode)
	# pathconf, pathconf_names not implemented
	def readlink(self):
		return FSPath(os.readlink(self.path))
	def remove(self):
		return os.remove(self.path)
	def removedirs(self):
		return os.removedirs(self.path)
	def rename(self,other):
		if isinstance(other,FSPath):
			return os.rename(self.path,other.path)
		else:
			return os.rename(self.path,other)
	def renames(self,other):
		if isinstance(other,FSPath):
			return os.renames(self.path,other.path)
		else:
			return os.renames(self.path,other)
	def rmdir(self):
		return os.rmdir(self.path)
	def stat(self):
		return os.stat(self.path)
	def statvfs(self):
		return os.statvfs(self.path)
	def symlink(self,link_name):
		return os.symlink(self.path,link_name)
	def unlink(self):
		return os.unlink(self.path)
	def utime(self,times):
		return os.utime(self.path,times)
	def walk(self,topdown=True, onerror=None, followlinks=False):
		return os.walk(self.path,topdown,onerror,followlinks)


class FSPathList(list):
	def _raw(self):
		return map(lambda x: x.path,self)
	
	# from os.path
	def commonprefix(self):
		return FSPath(os.path.commonprefix(self._raw()))
	def join(self):
		return FSPath(*(self._raw()))

class FSItem(FSPath):
	def __init__(self,path):
		super(FSItem,self).__init__(path)
		assert(self.isabs())
		assert(self.lexists())
	def parent(self):
		p = self.dirname()
		return Folder(p.path)
	def ancestors(self):
		c = FSItem(self.path)
		p = c.parent()
		while c.path is not p.path:
			yield p
			c = p
			p = c.parent()
		
	def common_parent(self,other):
		l = FSPathList()
		l.append(self)
		l.append(other)
		r = l.commonprefix()
		if r.isdir():
			return Folder(r.path)
		else:
			return Folder(r.dirname().path)
	def walk(self,*args,**kwargs):
		r = super(FSItem,self).walk(*args,**kwargs)
		for root, dirs, files in r:
			folder_list = [Folder(os.path.join(root,dir)) for dir in dirs]
			file_list = [File(os.path.join(root,a_file)) for a_file in files]
			yield (Folder(root),folder_list,file_list)

class Folder(FSItem):
	def __init__(self,path):
		super(Folder,self).__init__(path)
		assert(self.isdir())
	def __iter__(self):
		r = self.items()
		return r
	def items(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			yield specialized(p)
	def __getitem__(self,name):
		for item in self.items():
			if item.basename() == name:
				return item
		if name == """":
			return self
		raise IndexError, (name,self.path)
	def folders(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			if os.path.isdir(p):
				yield Folder(p)
	def files(self):
		for item in self.listdir():
			p = os.path.abspath(os.path.join(self.path,item))
			if os.path.isfile(p):
				yield File(p)
	
	def create(self,what,name):
		t = os.path.join(self.path,name)
		if what is Folder:
			os.mkdir(t)
			return Folder(t)
		elif what is File:
			f = open(t,""w"")
			f.close()
			return File(t)
		else:
			raise NotImplementedError
	
	def file_with_name(self,name):
		try:
			the_file = self[name]
		except IndexError:
			the_file = self.create(File,name)
		return the_file


class Line(object):
	def __init__(self,file,number,text):
		self.file = file
		self.number = number
		self.text = text

class File(FSItem):
	def __init__(self,path):
		super(File,self).__init__(path)
		assert(self.isfile())
	def read(self):
		f = open(self.path,""rU"")
		r = f.read()
		f.close()
		return r
	def readlines(self):
		f = open(self.path,""rU"")
		r = f.readlines()
		f.close()
		return r
	def read_Lines(self):	
		with open(self.path,""rU"") as f:
			count = 0
			for l in f:
				count += 1
				yield Line(self,count,l)
	def write(self,o):
		f = open(self.path,""wb"")
		r = f.write(o)
		f.close()
		return r
	def writelines(self,o):
		f = open(self.path,""wb"")
		r = f.writelines(o)
		f.close()
		return r
	def open(self,mode):
		return open(self.path,mode)
		

class Link(FSItem):
	def __init__(self,path):
		super(Link,self).__init__(path)
		assert(self.islink())

def current():
	r = Folder(os.getcwd())
	return r

def root():
	return Folder(""/"")

def home():
	return Folder(os.path.expanduser(""~""))

def FSItemList(FSPathList):
	def common_parent(self):
		r = None
		for item in self:
			if r is None:
				r = item
			else:
				r = r.common_parent(item)
		return r
	def filter(self,pattern):
		return fnmatch.filter(self,pattern)

if __name__ == ""__main__"":
	print ""current has ""
	for item in current():
		print item.path
	print ""root has ""
	for item in root():
		print item.path
	print ""home has ""
	for item in home():
		print item.path
/n/n/n",1
86,86,3af0180c9f5cc891e624183c3cd45dfd0572e6ab,"dashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo
from .extracted_habits_and_practices_to_puc import (
                                            ExtractedHabitsAndPracticesToPUC)
from .extracted_habits_and_practices import ExtractedHabitsAndPractices


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.tag)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/ndashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        puc = self.objects.puc
        # import pdb; pdb.set_trace()

        allowedTag = PUCTag.objects.create(name='aerosol')
        PUCToTag.objects.create(tag=allowedTag,content_object=puc,assumed=False)

        assumedTag = PUCTag.objects.create(name='foamspray')
        PUCToTag.objects.create(tag=assumedTag,content_object=puc,assumed=True)

        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('General category,Product family,Product type,'
            'Allowed attributes,Assumed attributes,Description,PUC type,PUC level,Product count'))
        # check the PUC from loader
        row1 = csv_lines[1].split(',')
        self.assertEqual(len(row1), 9)
        self.assertEqual(row1[0], 'Test General Category')
        self.assertEqual(row1[1], 'Test Product Family')
        self.assertEqual(row1[2], 'Test Product Type')
        self.assertEqual(row1[3], 'aerosol; foamspray')
        self.assertEqual(row1[4], 'foamspray')
        self.assertEqual(row1[5], 'Test Product Description')
        self.assertEqual(row1[6], 'FO')
        self.assertEqual(row1[7], '3')
        self.assertEqual(row1[8], '0')

class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/ndashboard/tests/functional/test_datadocument_detail.py/n/nfrom lxml import html

from django.test import Client
from django.urls import reverse
from django.test import TestCase, override_settings
from django.core.exceptions import ObjectDoesNotExist

from dashboard.forms import *
from factotum.settings import EXTRA
from dashboard.tests.loader import *


@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_absent_extracted_text(self):
        # Check every data document and confirm that its detail page loads,
        # with or without a detail formset
        for dd in DataDocument.objects.all():
            ddid = dd.id
            resp = self.client.get('/datadocument/%s/' % ddid)
            self.assertEqual(resp.status_code, 200, 'The page must return a 200 status code')
            try:
                extracted_text = ExtractedText.objects.get(data_document=dd)
            except ExtractedText.DoesNotExist:
                #print(dd.id)
                self.assertContains(resp, 'No Extracted Text exists for this Data Document')
            else:
                self.assertContains(resp, '<h4>Extracted Text')

    def test_script_links(self):
        doc = DataDocument.objects.first()
        #response = self.client.get(f'/datadocument/{doc.pk}/')
        response = self.client.get(f'/datadocument/179486/')
        self.assertIn('Download Script',response.content.decode('utf-8'))
        self.assertIn('Extraction Script',response.content.decode('utf-8'))

    def test_product_card_location(self):
        response = self.client.get('/datadocument/179486/')
        html = response.content.decode('utf-8')
        e_idx = html.index('<h4>Extracted Text')
        p_idx = html.index('<h4 class=""d-inline"">Products')
        self.assertTrue(p_idx > e_idx, ('Product card should come after ' 
                                        'Extracted Text card'))

    def test_product_create_link(self):
        response = self.client.get('/datadocument/167497/')
        self.assertContains(response, '/link_product_form/167497/')
        data = {'title'        : ['New Product'],
                'upc'          : ['stub_1860'],
                'document_type': [1],
                'return_url'   : ['/datadocument/167497/']}
        response = self.client.post('/link_product_form/167497/', data=data)
        self.assertRedirects(response,'/datadocument/167497/')
        response = self.client.get(response.url)
        self.assertContains(response, 'New Product')

    def test_product_title_duplication(self):
        response = self.client.get('/datadocument/245401/')
        self.assertContains(response, '/link_product_form/245401/')
        # Add a new Product
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9100'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9100')
        self.assertContains(response, f'product/%s' % new_product.id )

        # Add another new Product with the same title
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9101'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9101')
        self.assertContains(response, f'product/%s' % new_product.id )

    def test_add_extracted(self):
        '''Check that the user has the ability to create an extracted record
        when the document doesn't yet have an extracted record for data 
        group types 'CP' and 'HH'
        '''
        doc = DataDocument.objects.get(pk=354784)
        self.assertFalse(doc.extracted, (""This document is matched ""
                                                    ""but not extracted""))
        data = {'hhe_report_number': ['47']}
        response = self.client.post('/extractedtext/edit/354784/', data=data,
                                                            follow=True)
        doc = DataDocument.objects.get(pk=354784)
        self.assertTrue(doc.extracted, ""This document is not extracted "")
        page = html.fromstring(response.content)
        hhe_no = page.xpath('//dd[contains(@class, ""hh-report-no"")]')[0].text
        self.assertIn('47', hhe_no)


class TestDynamicDetailFormsets(TestCase):
    fixtures = fixtures_standard

    def setUp(self):

        self.client.login(username='Karyn', password='specialP@55word')

    def test_fetch_extracted_records(self):
        ''' Confirm that each detail child object returned by the fetch_extracted_records
        function has the correct parent '''
        for et in ExtractedText.objects.all():
            #print('Fetching extracted child records from %s: %s ' % (et.pk , et))
            for ex_child in et.fetch_extracted_records():
                child_model = ex_child.__class__ # the fetch_extracted_records function returns different classes
                #print('    %s: %s' % (ex_child.__class__.__name__ , ex_child ))
                self.assertEqual(et.pk , child_model.objects.get(pk=ex_child.pk).extracted_text.pk,
                    'The ExtractedChemical object with the returned child pk should have the correct extracted_text parent')

    def test_extractedsubclasses(self):
        ''' Confirm that the inheritance manager is returning appropriate
            subclass objects and ExtractedText base class objects 
         '''
        for doc in DataDocument.objects.all():
            try:
                extsub = ExtractedText.objects.get_subclass(data_document=doc)
                # A document with the CP data group type should be linked to 
                # ExtractedCPCat objects
                if doc.data_group.group_type.code=='CP':
                    #print(f'%s %s %s' % (doc.id, extsub, type(extsub)))
                    self.assertEqual(type(extsub) , ExtractedCPCat)
                elif doc.data_group.group_type.code=='HH':
                    self.assertEqual(type(extsub) , ExtractedHHDoc)
                else:
                    self.assertEqual(type(extsub) , ExtractedText)
            except ObjectDoesNotExist:
                pass
                #print('No extracted text for data document %s' % doc.id)


    def test_every_extractedtext(self):
        ''''Loop through all the ExtractedText objects and confirm that the new
        create_detail_formset method returns forms based on the correct models
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd, EXTRA)
            extracted_text_form = ParentForm(instance=et)
            child_formset = ChildForm(instance=et)
            # Compare the model of the child formset's QuerySet to the model
            # of the ExtractedText object's child objects
            dd_child_model  = get_extracted_models(dd.data_group.group_type.code)[1]
            childform_model = child_formset.__dict__.get('queryset').__dict__.get('model')
            self.assertEqual(dd_child_model, childform_model)

    def test_curated_chemical(self):
        ''''Confirm that if an ExtractedChemical record has been matched to DSSToxLookup, the 
            DSSToxLookup fields are displayed in the card
            This checks every data document.
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd)
            child_formset = ChildForm(instance=et)
            #print('Data doc %s , Group Type: %s ' % (dd.id, dd.data_group.type ))
            for form in child_formset.forms:
                if dd.data_group.type in ['CO','UN']:
                    ec = form.instance
                    if ec.dsstox is not None:
                        self.assertTrue( 'true_cas' in form.fields )
                        self.assertTrue( 'SID' in form.fields )
                    else:
                        self.assertFalse( 'true_cas' in form.fields )
                        self.assertFalse( 'SID' in form.fields )
                else:
                    self.assertFalse( 'true_cas' in form.fields )
            
    def test_num_forms(self):
        ''''Assure that the number of child forms is appropriate for the group
        type.
        '''
        group_models = {
                        'CO': ExtractedChemical,
                        'FU': ExtractedFunctionalUse,
                        'HP': ExtractedHabitsAndPractices,
                        'CP': ExtractedListPresence,
                        'HH': ExtractedHHRec
        }
        for code, model in group_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):

                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(
                                    reverse('data_document',kwargs={'pk': doc.pk})
                )
                num_forms = response.context['detail_formset'].total_form_count()
                children = model.objects.filter(
                                    extracted_text=doc.extractedtext
                ).count()

                if doc.detail_page_editable:
                    error = (f'{model.__module__} should have one more forms'
                                                                ' than instances')
                    self.assertEqual(num_forms, children + 1, error)
                else:
                    error = (f'{model.__module__} should have the same number'
                                                        ' of forms as instances')
                    self.assertEqual(num_forms, children, error)

/n/n/ndashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['General category','Product family','Product type','Allowed attributes','Assumed attributes','Description','PUC type','PUC level','Product count']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                # list(puc.get_allowed_tags()),
                '; '.join([str(allowedTag) for allowedTag in puc.puctotag_set.all()]),
                '; '.join([str(assumedTag) for assumedTag in puc.puctotag_set.filter(assumed=True)]),
                puc.description, 
                puc.kind,
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/n",0
87,87,3af0180c9f5cc891e624183c3cd45dfd0572e6ab,"/dashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo
from .extracted_habits_and_practices_to_puc import (
                                            ExtractedHabitsAndPracticesToPUC)
from .extracted_habits_and_practices import ExtractedHabitsAndPractices


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.content_object)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/n/dashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        p = self.objects.puc
        puc_line = (p.gen_cat + ',' + p.prod_fam + ',' + p.prod_type + ',' + p.description +
                    ',' + str(p.get_level()) + ',' + str(p.product_count))
        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('gen_cat,prod_fam,prod_type,description,'
                                        'PUC_type,num_prods'))
        # check the PUC from loader
        self.assertEqual(csv_lines[1], puc_line)


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/n/dashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['gen_cat','prod_fam','prod_type','description','PUC_type','num_prods']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                puc.description, 
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/n",1
90,90,e92a2741b4c9bc60f407ac33aee3eb2f7c226fee,"dashboard/views/data_document.py/n/nfrom django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404
from django.core.exceptions import ObjectDoesNotExist
from djqscsv import render_to_csv_response

from dashboard.forms import *
from dashboard.forms import ExtractedListPresenceTagForm
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if doc.detail_page_editable else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=bool(edit))
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if code == 'CP':
        # although keywords display as if at the datadocument level, they are
        # attached to each list_presence record. To display, we're getting the
        # tags associated with the first list_presence record, but on saving
        # (in save_list_presence_tag_form()) we loop over the whole set
        try:
            list_presence = doc.extractedtext.rawchem.select_subclasses('extractedlistpresence').first()
            list_presence_tag_form = ExtractedListPresenceTagForm(instance=list_presence)
            context.update({'list_presence_tag_form': list_presence_tag_form})
        except ObjectDoesNotExist:
            pass
    if doc.is_extracted:
        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk)
        child_formset = ChildFormSet(instance=extracted_text)
        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True
        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)

@login_required()
def save_list_presence_tag_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    extracted_text = get_object_or_404(ExtractedText, pk=pk)
    for extracted_list_presence in extracted_text.rawchem.select_subclasses('extractedlistpresence'):
        tag_form = ExtractedListPresenceTagForm(request.POST or None, instance=extracted_list_presence)
        if tag_form.is_valid():
            tag_form.save()
    return redirect(referer, pk=pk)

@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})

@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)

@login_required
def data_document_edit(request, pk, template_name=('data_document/'
                                                    'data_document_form.html')):
    datadocument = get_object_or_404(DataDocument, pk=pk)
    form = DataDocumentForm(request.POST or None, instance=datadocument)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_document', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    try:
        exttext = model.objects.get_subclass(data_document_id=pk)
    except ExtractedText.DoesNotExist:
        exttext = model(data_document_id=pk, extraction_script=script)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/n",0
91,91,e92a2741b4c9bc60f407ac33aee3eb2f7c226fee,"/dashboard/views/data_document.py/n/nfrom django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404
from django.core.exceptions import ObjectDoesNotExist
from djqscsv import render_to_csv_response

from dashboard.forms import *
from dashboard.forms import ExtractedListPresenceTagForm
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if doc.detail_page_editable else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=bool(edit))
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if code == 'CP':
        # although keywords display as if at the datadocument level, they are
        # attached to each list_presence record. To display, we're getting the
        # tags associated with the first list_presence record, but on saving
        # (in save_list_presence_tag_form()) we loop over the whole set
        try:
            list_presence = doc.extractedtext.rawchem.select_subclasses('extractedlistpresence').first()
            list_presence_tag_form = ExtractedListPresenceTagForm(instance=list_presence)
            context.update({'list_presence_tag_form': list_presence_tag_form})
        except ObjectDoesNotExist:
            pass
    if doc.is_extracted:
        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk)
        child_formset = ChildFormSet(instance=extracted_text)
        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True
        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)

@login_required()
def save_list_presence_tag_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    extracted_text = get_object_or_404(ExtractedText, pk=pk)
    for extracted_list_presence in extracted_text.rawchem.select_subclasses('extractedlistpresence'):
        tag_form = ExtractedListPresenceTagForm(request.POST or None, instance=extracted_list_presence)
        if tag_form.is_valid():
            tag_form.save()
    return redirect(referer, pk=pk)

@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})

@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)

@login_required
def data_document_edit(request, pk, template_name=('data_document/'
                                                    'data_document_form.html')):
    datadocument = get_object_or_404(DataDocument, pk=pk)
    form = DataDocumentForm(request.POST or None, instance=datadocument)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_document', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/n",1
58,58,ef527f84abf0cee7ac6ad832828ff92311440ee4,"python/ray/experimental/state.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import defaultdict
import json
import redis
import sys
import time

import ray
from ray.function_manager import FunctionDescriptor
import ray.gcs_utils
import ray.ray_constants as ray_constants
from ray.utils import (decode, binary_to_object_id, binary_to_hex,
                       hex_to_binary)


def parse_client_table(redis_client):
    """"""Read the client table.

    Args:
        redis_client: A client to the primary Redis shard.

    Returns:
        A list of information about the nodes in the cluster.
    """"""
    NIL_CLIENT_ID = ray.ObjectID.nil().binary()
    message = redis_client.execute_command(""RAY.TABLE_LOOKUP"",
                                           ray.gcs_utils.TablePrefix.CLIENT,
                                           """", NIL_CLIENT_ID)

    # Handle the case where no clients are returned. This should only
    # occur potentially immediately after the cluster is started.
    if message is None:
        return []

    node_info = {}
    gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(message, 0)

    ordered_client_ids = []

    # Since GCS entries are append-only, we override so that
    # only the latest entries are kept.
    for i in range(gcs_entry.EntriesLength()):
        client = (ray.gcs_utils.ClientTableData.GetRootAsClientTableData(
            gcs_entry.Entries(i), 0))

        resources = {
            decode(client.ResourcesTotalLabel(i)):
            client.ResourcesTotalCapacity(i)
            for i in range(client.ResourcesTotalLabelLength())
        }
        client_id = ray.utils.binary_to_hex(client.ClientId())

        # If this client is being removed, then it must
        # have previously been inserted, and
        # it cannot have previously been removed.
        if not client.IsInsertion():
            assert client_id in node_info, ""Client removed not found!""
            assert node_info[client_id][""IsInsertion""], (
                ""Unexpected duplicate removal of client."")
        else:
            ordered_client_ids.append(client_id)

        node_info[client_id] = {
            ""ClientID"": client_id,
            ""IsInsertion"": client.IsInsertion(),
            ""NodeManagerAddress"": decode(
                client.NodeManagerAddress(), allow_none=True),
            ""NodeManagerPort"": client.NodeManagerPort(),
            ""ObjectManagerPort"": client.ObjectManagerPort(),
            ""ObjectStoreSocketName"": decode(
                client.ObjectStoreSocketName(), allow_none=True),
            ""RayletSocketName"": decode(
                client.RayletSocketName(), allow_none=True),
            ""Resources"": resources
        }
    # NOTE: We return the list comprehension below instead of simply doing
    # 'list(node_info.values())' in order to have the nodes appear in the order
    # that they joined the cluster. Python dictionaries do not preserve
    # insertion order. We could use an OrderedDict, but then we'd have to be
    # sure to only insert a given node a single time (clients that die appear
    # twice in the GCS log).
    return [node_info[client_id] for client_id in ordered_client_ids]


class GlobalState(object):
    """"""A class used to interface with the Ray control state.

    # TODO(zongheng): In the future move this to use Ray's redis module in the
    # backend to cut down on # of request RPCs.

    Attributes:
        redis_client: The Redis client used to query the primary redis server.
        redis_clients: Redis clients for each of the Redis shards.
    """"""

    def __init__(self):
        """"""Create a GlobalState object.""""""
        # The redis server storing metadata, such as function table, client
        # table, log files, event logs, workers/actions info.
        self.redis_client = None
        # Clients for the redis shards, storing the object table & task table.
        self.redis_clients = None

    def _check_connected(self):
        """"""Check that the object has been initialized before it is used.

        Raises:
            Exception: An exception is raised if ray.init() has not been called
                yet.
        """"""
        if self.redis_client is None:
            raise Exception(""The ray.global_state API cannot be used before ""
                            ""ray.init has been called."")

        if self.redis_clients is None:
            raise Exception(""The ray.global_state API cannot be used before ""
                            ""ray.init has been called."")

    def _initialize_global_state(self,
                                 redis_ip_address,
                                 redis_port,
                                 redis_password=None,
                                 timeout=20):
        """"""Initialize the GlobalState object by connecting to Redis.

        It's possible that certain keys in Redis may not have been fully
        populated yet. In this case, we will retry this method until they have
        been populated or we exceed a timeout.

        Args:
            redis_ip_address: The IP address of the node that the Redis server
                lives on.
            redis_port: The port that the Redis server is listening on.
            redis_password: The password of the redis server.
        """"""
        self.redis_client = redis.StrictRedis(
            host=redis_ip_address, port=redis_port, password=redis_password)

        start_time = time.time()

        num_redis_shards = None
        ip_address_ports = []

        while time.time() - start_time < timeout:
            # Attempt to get the number of Redis shards.
            num_redis_shards = self.redis_client.get(""NumRedisShards"")
            if num_redis_shards is None:
                print(""Waiting longer for NumRedisShards to be populated."")
                time.sleep(1)
                continue
            num_redis_shards = int(num_redis_shards)
            if num_redis_shards < 1:
                raise Exception(""Expected at least one Redis shard, found ""
                                ""{}."".format(num_redis_shards))

            # Attempt to get all of the Redis shards.
            ip_address_ports = self.redis_client.lrange(
                ""RedisShards"", start=0, end=-1)
            if len(ip_address_ports) != num_redis_shards:
                print(""Waiting longer for RedisShards to be populated."")
                time.sleep(1)
                continue

            # If we got here then we successfully got all of the information.
            break

        # Check to see if we timed out.
        if time.time() - start_time >= timeout:
            raise Exception(""Timed out while attempting to initialize the ""
                            ""global state. num_redis_shards = {}, ""
                            ""ip_address_ports = {}"".format(
                                num_redis_shards, ip_address_ports))

        # Get the rest of the information.
        self.redis_clients = []
        for ip_address_port in ip_address_ports:
            shard_address, shard_port = ip_address_port.split(b"":"")
            self.redis_clients.append(
                redis.StrictRedis(
                    host=shard_address,
                    port=shard_port,
                    password=redis_password))

    def _execute_command(self, key, *args):
        """"""Execute a Redis command on the appropriate Redis shard based on key.

        Args:
            key: The object ID or the task ID that the query is about.
            args: The command to run.

        Returns:
            The value returned by the Redis command.
        """"""
        client = self.redis_clients[key.redis_shard_hash() % len(
            self.redis_clients)]
        return client.execute_command(*args)

    def _keys(self, pattern):
        """"""Execute the KEYS command on all Redis shards.

        Args:
            pattern: The KEYS pattern to query.

        Returns:
            The concatenated list of results from all shards.
        """"""
        result = []
        for client in self.redis_clients:
            result.extend(list(client.scan_iter(match=pattern)))
        return result

    def _object_table(self, object_id):
        """"""Fetch and parse the object table information for a single object ID.

        Args:
            object_id: An object ID to get information about.

        Returns:
            A dictionary with information about the object ID in question.
        """"""
        # Allow the argument to be either an ObjectID or a hex string.
        if not isinstance(object_id, ray.ObjectID):
            object_id = ray.ObjectID(hex_to_binary(object_id))

        # Return information about a single object ID.
        message = self._execute_command(object_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.OBJECT, """",
                                        object_id.binary())
        if message is None:
            return {}
        gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        assert gcs_entry.EntriesLength() > 0

        entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(
            gcs_entry.Entries(0), 0)

        object_info = {
            ""DataSize"": entry.ObjectSize(),
            ""Manager"": entry.Manager(),
            ""IsEviction"": [entry.IsEviction()],
        }

        for i in range(1, gcs_entry.EntriesLength()):
            entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(
                gcs_entry.Entries(i), 0)
            object_info[""IsEviction""].append(entry.IsEviction())

        return object_info

    def object_table(self, object_id=None):
        """"""Fetch and parse the object table info for one or more object IDs.

        Args:
            object_id: An object ID to fetch information about. If this is
                None, then the entire object table is fetched.

        Returns:
            Information from the object table.
        """"""
        self._check_connected()
        if object_id is not None:
            # Return information about a single object ID.
            return self._object_table(object_id)
        else:
            # Return the entire object table.
            object_keys = self._keys(ray.gcs_utils.TablePrefix_OBJECT_string +
                                     ""*"")
            object_ids_binary = {
                key[len(ray.gcs_utils.TablePrefix_OBJECT_string):]
                for key in object_keys
            }

            results = {}
            for object_id_binary in object_ids_binary:
                results[binary_to_object_id(object_id_binary)] = (
                    self._object_table(binary_to_object_id(object_id_binary)))
            return results

    def _task_table(self, task_id):
        """"""Fetch and parse the task table information for a single task ID.

        Args:
            task_id: A task ID to get information about.

        Returns:
            A dictionary with information about the task ID in question.
        """"""
        assert isinstance(task_id, ray.TaskID)
        message = self._execute_command(task_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.RAYLET_TASK,
                                        """", task_id.binary())
        if message is None:
            return {}
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        assert gcs_entries.EntriesLength() == 1

        task_table_message = ray.gcs_utils.Task.GetRootAsTask(
            gcs_entries.Entries(0), 0)

        execution_spec = task_table_message.TaskExecutionSpec()
        task_spec = task_table_message.TaskSpecification()
        task = ray._raylet.Task.from_string(task_spec)
        function_descriptor_list = task.function_descriptor_list()
        function_descriptor = FunctionDescriptor.from_bytes_list(
            function_descriptor_list)
        task_spec_info = {
            ""DriverID"": task.driver_id().hex(),
            ""TaskID"": task.task_id().hex(),
            ""ParentTaskID"": task.parent_task_id().hex(),
            ""ParentCounter"": task.parent_counter(),
            ""ActorID"": (task.actor_id().hex()),
            ""ActorCreationID"": task.actor_creation_id().hex(),
            ""ActorCreationDummyObjectID"": (
                task.actor_creation_dummy_object_id().hex()),
            ""ActorCounter"": task.actor_counter(),
            ""Args"": task.arguments(),
            ""ReturnObjectIDs"": task.returns(),
            ""RequiredResources"": task.required_resources(),
            ""FunctionID"": function_descriptor.function_id.hex(),
            ""FunctionHash"": binary_to_hex(function_descriptor.function_hash),
            ""ModuleName"": function_descriptor.module_name,
            ""ClassName"": function_descriptor.class_name,
            ""FunctionName"": function_descriptor.function_name,
        }

        return {
            ""ExecutionSpec"": {
                ""Dependencies"": [
                    execution_spec.Dependencies(i)
                    for i in range(execution_spec.DependenciesLength())
                ],
                ""LastTimestamp"": execution_spec.LastTimestamp(),
                ""NumForwards"": execution_spec.NumForwards()
            },
            ""TaskSpec"": task_spec_info
        }

    def task_table(self, task_id=None):
        """"""Fetch and parse the task table information for one or more task IDs.

        Args:
            task_id: A hex string of the task ID to fetch information about. If
                this is None, then the task object table is fetched.

        Returns:
            Information from the task table.
        """"""
        self._check_connected()
        if task_id is not None:
            task_id = ray.TaskID(hex_to_binary(task_id))
            return self._task_table(task_id)
        else:
            task_table_keys = self._keys(
                ray.gcs_utils.TablePrefix_RAYLET_TASK_string + ""*"")
            task_ids_binary = [
                key[len(ray.gcs_utils.TablePrefix_RAYLET_TASK_string):]
                for key in task_table_keys
            ]

            results = {}
            for task_id_binary in task_ids_binary:
                results[binary_to_hex(task_id_binary)] = self._task_table(
                    ray.TaskID(task_id_binary))
            return results

    def function_table(self, function_id=None):
        """"""Fetch and parse the function table.

        Returns:
            A dictionary that maps function IDs to information about the
                function.
        """"""
        self._check_connected()
        function_table_keys = self.redis_client.keys(
            ray.gcs_utils.FUNCTION_PREFIX + ""*"")
        results = {}
        for key in function_table_keys:
            info = self.redis_client.hgetall(key)
            function_info_parsed = {
                ""DriverID"": binary_to_hex(info[b""driver_id""]),
                ""Module"": decode(info[b""module""]),
                ""Name"": decode(info[b""name""])
            }
            results[binary_to_hex(info[b""function_id""])] = function_info_parsed
        return results

    def client_table(self):
        """"""Fetch and parse the Redis DB client table.

        Returns:
            Information about the Ray clients in the cluster.
        """"""
        self._check_connected()

        return parse_client_table(self.redis_client)

    def _profile_table(self, batch_id):
        """"""Get the profile events for a given batch of profile events.

        Args:
            batch_id: An identifier for a batch of profile events.

        Returns:
            A list of the profile events for the specified batch.
        """"""
        # TODO(rkn): This method should support limiting the number of log
        # events and should also support returning a window of events.
        message = self._execute_command(batch_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.PROFILE, """",
                                        batch_id.binary())

        if message is None:
            return []

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        profile_events = []
        for i in range(gcs_entries.EntriesLength()):
            profile_table_message = (
                ray.gcs_utils.ProfileTableData.GetRootAsProfileTableData(
                    gcs_entries.Entries(i), 0))

            component_type = decode(profile_table_message.ComponentType())
            component_id = binary_to_hex(profile_table_message.ComponentId())
            node_ip_address = decode(
                profile_table_message.NodeIpAddress(), allow_none=True)

            for j in range(profile_table_message.ProfileEventsLength()):
                profile_event_message = profile_table_message.ProfileEvents(j)

                profile_event = {
                    ""event_type"": decode(profile_event_message.EventType()),
                    ""component_id"": component_id,
                    ""node_ip_address"": node_ip_address,
                    ""component_type"": component_type,
                    ""start_time"": profile_event_message.StartTime(),
                    ""end_time"": profile_event_message.EndTime(),
                    ""extra_data"": json.loads(
                        decode(profile_event_message.ExtraData())),
                }

                profile_events.append(profile_event)

        return profile_events

    def profile_table(self):
        profile_table_keys = self._keys(
            ray.gcs_utils.TablePrefix_PROFILE_string + ""*"")
        batch_identifiers_binary = [
            key[len(ray.gcs_utils.TablePrefix_PROFILE_string):]
            for key in profile_table_keys
        ]

        result = defaultdict(list)
        for batch_id in batch_identifiers_binary:
            profile_data = self._profile_table(binary_to_object_id(batch_id))
            # Note that if keys are being evicted from Redis, then it is
            # possible that the batch will be evicted before we get it.
            if len(profile_data) > 0:
                component_id = profile_data[0][""component_id""]
                result[component_id].extend(profile_data)

        return dict(result)

    def _seconds_to_microseconds(self, time_in_seconds):
        """"""A helper function for converting seconds to microseconds.""""""
        time_in_microseconds = 10**6 * time_in_seconds
        return time_in_microseconds

    # Colors are specified at
    # https://github.com/catapult-project/catapult/blob/master/tracing/tracing/base/color_scheme.html.  # noqa: E501
    _default_color_mapping = defaultdict(
        lambda: ""generic_work"", {
            ""worker_idle"": ""cq_build_abandoned"",
            ""task"": ""rail_response"",
            ""task:deserialize_arguments"": ""rail_load"",
            ""task:execute"": ""rail_animation"",
            ""task:store_outputs"": ""rail_idle"",
            ""wait_for_function"": ""detailed_memory_dump"",
            ""ray.get"": ""good"",
            ""ray.put"": ""terrible"",
            ""ray.wait"": ""vsync_highlight_color"",
            ""submit_task"": ""background_memory_dump"",
            ""fetch_and_run_function"": ""detailed_memory_dump"",
            ""register_remote_function"": ""detailed_memory_dump"",
        })

    # These colors are for use in Chrome tracing.
    _chrome_tracing_colors = [
        ""thread_state_uninterruptible"",
        ""thread_state_iowait"",
        ""thread_state_running"",
        ""thread_state_runnable"",
        ""thread_state_sleeping"",
        ""thread_state_unknown"",
        ""background_memory_dump"",
        ""light_memory_dump"",
        ""detailed_memory_dump"",
        ""vsync_highlight_color"",
        ""generic_work"",
        ""good"",
        ""bad"",
        ""terrible"",
        # ""black"",
        # ""grey"",
        # ""white"",
        ""yellow"",
        ""olive"",
        ""rail_response"",
        ""rail_animation"",
        ""rail_idle"",
        ""rail_load"",
        ""startup"",
        ""heap_dump_stack_frame"",
        ""heap_dump_object_type"",
        ""heap_dump_child_node_arrow"",
        ""cq_build_running"",
        ""cq_build_passed"",
        ""cq_build_failed"",
        ""cq_build_abandoned"",
        ""cq_build_attempt_runnig"",
        ""cq_build_attempt_passed"",
        ""cq_build_attempt_failed"",
    ]

    def chrome_tracing_dump(self, filename=None):
        """"""Return a list of profiling events that can viewed as a timeline.

        To view this information as a timeline, simply dump it as a json file
        by passing in ""filename"" or using using json.dump, and then load go to
        chrome://tracing in the Chrome web browser and load the dumped file.
        Make sure to enable ""Flow events"" in the ""View Options"" menu.

        Args:
            filename: If a filename is provided, the timeline is dumped to that
                file.

        Returns:
            If filename is not provided, this returns a list of profiling
                events. Each profile event is a dictionary.
        """"""
        # TODO(rkn): Support including the task specification data in the
        # timeline.
        # TODO(rkn): This should support viewing just a window of time or a
        # limited number of events.

        profile_table = self.profile_table()
        all_events = []

        for component_id_hex, component_events in profile_table.items():
            # Only consider workers and drivers.
            component_type = component_events[0][""component_type""]
            if component_type not in [""worker"", ""driver""]:
                continue

            for event in component_events:
                new_event = {
                    # The category of the event.
                    ""cat"": event[""event_type""],
                    # The string displayed on the event.
                    ""name"": event[""event_type""],
                    # The identifier for the group of rows that the event
                    # appears in.
                    ""pid"": event[""node_ip_address""],
                    # The identifier for the row that the event appears in.
                    ""tid"": event[""component_type""] + "":"" +
                    event[""component_id""],
                    # The start time in microseconds.
                    ""ts"": self._seconds_to_microseconds(event[""start_time""]),
                    # The duration in microseconds.
                    ""dur"": self._seconds_to_microseconds(event[""end_time""] -
                                                         event[""start_time""]),
                    # What is this?
                    ""ph"": ""X"",
                    # This is the name of the color to display the box in.
                    ""cname"": self._default_color_mapping[event[""event_type""]],
                    # The extra user-defined data.
                    ""args"": event[""extra_data""],
                }

                # Modify the json with the additional user-defined extra data.
                # This can be used to add fields or override existing fields.
                if ""cname"" in event[""extra_data""]:
                    new_event[""cname""] = event[""extra_data""][""cname""]
                if ""name"" in event[""extra_data""]:
                    new_event[""name""] = event[""extra_data""][""name""]

                all_events.append(new_event)

        if filename is not None:
            with open(filename, ""w"") as outfile:
                json.dump(all_events, outfile)
        else:
            return all_events

    def chrome_tracing_object_transfer_dump(self, filename=None):
        """"""Return a list of transfer events that can viewed as a timeline.

        To view this information as a timeline, simply dump it as a json file
        by passing in ""filename"" or using using json.dump, and then load go to
        chrome://tracing in the Chrome web browser and load the dumped file.
        Make sure to enable ""Flow events"" in the ""View Options"" menu.

        Args:
            filename: If a filename is provided, the timeline is dumped to that
                file.

        Returns:
            If filename is not provided, this returns a list of profiling
                events. Each profile event is a dictionary.
        """"""
        client_id_to_address = {}
        for client_info in ray.global_state.client_table():
            client_id_to_address[client_info[""ClientID""]] = ""{}:{}"".format(
                client_info[""NodeManagerAddress""],
                client_info[""ObjectManagerPort""])

        all_events = []

        for key, items in self.profile_table().items():
            # Only consider object manager events.
            if items[0][""component_type""] != ""object_manager"":
                continue

            for event in items:
                if event[""event_type""] == ""transfer_send"":
                    object_id, remote_client_id, _, _ = event[""extra_data""]

                elif event[""event_type""] == ""transfer_receive"":
                    object_id, remote_client_id, _, _ = event[""extra_data""]

                elif event[""event_type""] == ""receive_pull_request"":
                    object_id, remote_client_id = event[""extra_data""]

                else:
                    assert False, ""This should be unreachable.""

                # Choose a color by reading the first couple of hex digits of
                # the object ID as an integer and turning that into a color.
                object_id_int = int(object_id[:2], 16)
                color = self._chrome_tracing_colors[object_id_int % len(
                    self._chrome_tracing_colors)]

                new_event = {
                    # The category of the event.
                    ""cat"": event[""event_type""],
                    # The string displayed on the event.
                    ""name"": event[""event_type""],
                    # The identifier for the group of rows that the event
                    # appears in.
                    ""pid"": client_id_to_address[key],
                    # The identifier for the row that the event appears in.
                    ""tid"": client_id_to_address[remote_client_id],
                    # The start time in microseconds.
                    ""ts"": self._seconds_to_microseconds(event[""start_time""]),
                    # The duration in microseconds.
                    ""dur"": self._seconds_to_microseconds(event[""end_time""] -
                                                         event[""start_time""]),
                    # What is this?
                    ""ph"": ""X"",
                    # This is the name of the color to display the box in.
                    ""cname"": color,
                    # The extra user-defined data.
                    ""args"": event[""extra_data""],
                }
                all_events.append(new_event)

                # Add another box with a color indicating whether it was a send
                # or a receive event.
                if event[""event_type""] == ""transfer_send"":
                    additional_event = new_event.copy()
                    additional_event[""cname""] = ""black""
                    all_events.append(additional_event)
                elif event[""event_type""] == ""transfer_receive"":
                    additional_event = new_event.copy()
                    additional_event[""cname""] = ""grey""
                    all_events.append(additional_event)
                else:
                    pass

        if filename is not None:
            with open(filename, ""w"") as outfile:
                json.dump(all_events, outfile)
        else:
            return all_events

    def workers(self):
        """"""Get a dictionary mapping worker ID to worker information.""""""
        worker_keys = self.redis_client.keys(""Worker*"")
        workers_data = {}

        for worker_key in worker_keys:
            worker_info = self.redis_client.hgetall(worker_key)
            worker_id = binary_to_hex(worker_key[len(""Workers:""):])

            workers_data[worker_id] = {
                ""node_ip_address"": decode(worker_info[b""node_ip_address""]),
                ""plasma_store_socket"": decode(
                    worker_info[b""plasma_store_socket""])
            }
            if b""stderr_file"" in worker_info:
                workers_data[worker_id][""stderr_file""] = decode(
                    worker_info[b""stderr_file""])
            if b""stdout_file"" in worker_info:
                workers_data[worker_id][""stdout_file""] = decode(
                    worker_info[b""stdout_file""])
        return workers_data

    def actors(self):
        actor_keys = self.redis_client.keys(""Actor:*"")
        actor_info = {}
        for key in actor_keys:
            info = self.redis_client.hgetall(key)
            actor_id = key[len(""Actor:""):]
            assert len(actor_id) == ray_constants.ID_SIZE
            actor_info[binary_to_hex(actor_id)] = {
                ""class_id"": binary_to_hex(info[b""class_id""]),
                ""driver_id"": binary_to_hex(info[b""driver_id""]),
                ""local_scheduler_id"": binary_to_hex(
                    info[b""local_scheduler_id""]),
                ""num_gpus"": int(info[b""num_gpus""]),
                ""removed"": decode(info[b""removed""]) == ""True""
            }
        return actor_info

    def _job_length(self):
        event_log_sets = self.redis_client.keys(""event_log*"")
        overall_smallest = sys.maxsize
        overall_largest = 0
        num_tasks = 0
        for event_log_set in event_log_sets:
            fwd_range = self.redis_client.zrange(
                event_log_set, start=0, end=0, withscores=True)
            overall_smallest = min(overall_smallest, fwd_range[0][1])

            rev_range = self.redis_client.zrevrange(
                event_log_set, start=0, end=0, withscores=True)
            overall_largest = max(overall_largest, rev_range[0][1])

            num_tasks += self.redis_client.zcount(
                event_log_set, min=0, max=time.time())
        if num_tasks == 0:
            return 0, 0, 0
        return overall_smallest, overall_largest, num_tasks

    def cluster_resources(self):
        """"""Get the current total cluster resources.

        Note that this information can grow stale as nodes are added to or
        removed from the cluster.

        Returns:
            A dictionary mapping resource name to the total quantity of that
                resource in the cluster.
        """"""
        resources = defaultdict(int)
        clients = self.client_table()
        for client in clients:
            # Only count resources from live clients.
            if client[""IsInsertion""]:
                for key, value in client[""Resources""].items():
                    resources[key] += value

        return dict(resources)

    def _live_client_ids(self):
        """"""Returns a set of client IDs corresponding to clients still alive.""""""
        return {
            client[""ClientID""]
            for client in self.client_table() if client[""IsInsertion""]
        }

    def available_resources(self):
        """"""Get the current available cluster resources.

        This is different from `cluster_resources` in that this will return
        idle (available) resources rather than total resources.

        Note that this information can grow stale as tasks start and finish.

        Returns:
            A dictionary mapping resource name to the total quantity of that
                resource in the cluster.
        """"""
        available_resources_by_id = {}

        subscribe_clients = [
            redis_client.pubsub(ignore_subscribe_messages=True)
            for redis_client in self.redis_clients
        ]
        for subscribe_client in subscribe_clients:
            subscribe_client.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL)

        client_ids = self._live_client_ids()

        while set(available_resources_by_id.keys()) != client_ids:
            for subscribe_client in subscribe_clients:
                # Parse client message
                raw_message = subscribe_client.get_message()
                if (raw_message is None or raw_message[""channel""] !=
                        ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL):
                    continue
                data = raw_message[""data""]
                gcs_entries = (
                    ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
                        data, 0))
                heartbeat_data = gcs_entries.Entries(0)
                message = (ray.gcs_utils.HeartbeatTableData.
                           GetRootAsHeartbeatTableData(heartbeat_data, 0))
                # Calculate available resources for this client
                num_resources = message.ResourcesAvailableLabelLength()
                dynamic_resources = {}
                for i in range(num_resources):
                    resource_id = decode(message.ResourcesAvailableLabel(i))
                    dynamic_resources[resource_id] = (
                        message.ResourcesAvailableCapacity(i))

                # Update available resources for this client
                client_id = ray.utils.binary_to_hex(message.ClientId())
                available_resources_by_id[client_id] = dynamic_resources

            # Update clients in cluster
            client_ids = self._live_client_ids()

            # Remove disconnected clients
            for client_id in available_resources_by_id.keys():
                if client_id not in client_ids:
                    del available_resources_by_id[client_id]

        # Calculate total available resources
        total_available_resources = defaultdict(int)
        for available_resources in available_resources_by_id.values():
            for resource_id, num_available in available_resources.items():
                total_available_resources[resource_id] += num_available

        # Close the pubsub clients to avoid leaking file descriptors.
        for subscribe_client in subscribe_clients:
            subscribe_client.close()

        return dict(total_available_resources)

    def _error_messages(self, job_id):
        """"""Get the error messages for a specific job.

        Args:
            job_id: The ID of the job to get the errors for.

        Returns:
            A list of the error messages for this job.
        """"""
        assert isinstance(job_id, ray.DriverID)
        message = self.redis_client.execute_command(
            ""RAY.TABLE_LOOKUP"", ray.gcs_utils.TablePrefix.ERROR_INFO, """",
            job_id.binary())

        # If there are no errors, return early.
        if message is None:
            return []

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)
        error_messages = []
        for i in range(gcs_entries.EntriesLength()):
            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(
                gcs_entries.Entries(i), 0)
            assert job_id.binary() == error_data.JobId()
            error_message = {
                ""type"": decode(error_data.Type()),
                ""message"": decode(error_data.ErrorMessage()),
                ""timestamp"": error_data.Timestamp(),
            }
            error_messages.append(error_message)
        return error_messages

    def error_messages(self, job_id=None):
        """"""Get the error messages for all jobs or a specific job.

        Args:
            job_id: The specific job to get the errors for. If this is None,
                then this method retrieves the errors for all jobs.

        Returns:
            A dictionary mapping job ID to a list of the error messages for
                that job.
        """"""
        if job_id is not None:
            assert isinstance(job_id, ray.DriverID)
            return self._error_messages(job_id)

        error_table_keys = self.redis_client.keys(
            ray.gcs_utils.TablePrefix_ERROR_INFO_string + ""*"")
        job_ids = [
            key[len(ray.gcs_utils.TablePrefix_ERROR_INFO_string):]
            for key in error_table_keys
        ]

        return {
            binary_to_hex(job_id): self._error_messages(ray.DriverID(job_id))
            for job_id in job_ids
        }
/n/n/npython/ray/gcs_utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import flatbuffers
import ray.core.generated.ErrorTableData

from ray.core.generated.ClientTableData import ClientTableData
from ray.core.generated.DriverTableData import DriverTableData
from ray.core.generated.ErrorTableData import ErrorTableData
from ray.core.generated.GcsTableEntry import GcsTableEntry
from ray.core.generated.HeartbeatBatchTableData import HeartbeatBatchTableData
from ray.core.generated.HeartbeatTableData import HeartbeatTableData
from ray.core.generated.Language import Language
from ray.core.generated.ObjectTableData import ObjectTableData
from ray.core.generated.ProfileTableData import ProfileTableData
from ray.core.generated.TablePrefix import TablePrefix
from ray.core.generated.TablePubsub import TablePubsub

from ray.core.generated.ray.protocol.Task import Task

__all__ = [
    ""GcsTableEntry"", ""ClientTableData"", ""ErrorTableData"", ""HeartbeatTableData"",
    ""HeartbeatBatchTableData"", ""DriverTableData"", ""ProfileTableData"",
    ""ObjectTableData"", ""Task"", ""TablePrefix"", ""TablePubsub"", ""Language"",
    ""construct_error_message""
]

FUNCTION_PREFIX = ""RemoteFunction:""
LOG_FILE_CHANNEL = ""RAY_LOG_CHANNEL""

# xray heartbeats
XRAY_HEARTBEAT_CHANNEL = str(TablePubsub.HEARTBEAT).encode(""ascii"")
XRAY_HEARTBEAT_BATCH_CHANNEL = str(TablePubsub.HEARTBEAT_BATCH).encode(""ascii"")

# xray driver updates
XRAY_DRIVER_CHANNEL = str(TablePubsub.DRIVER).encode(""ascii"")

# These prefixes must be kept up-to-date with the TablePrefix enum in gcs.fbs.
# TODO(rkn): We should use scoped enums, in which case we should be able to
# just access the flatbuffer generated values.
TablePrefix_RAYLET_TASK_string = ""RAYLET_TASK""
TablePrefix_OBJECT_string = ""OBJECT""
TablePrefix_ERROR_INFO_string = ""ERROR_INFO""
TablePrefix_PROFILE_string = ""PROFILE""


def construct_error_message(driver_id, error_type, message, timestamp):
    """"""Construct a serialized ErrorTableData object.

    Args:
        driver_id: The ID of the driver that the error should go to. If this is
            nil, then the error will go to all drivers.
        error_type: The type of the error.
        message: The error message.
        timestamp: The time of the error.

    Returns:
        The serialized object.
    """"""
    builder = flatbuffers.Builder(0)
    driver_offset = builder.CreateString(driver_id.binary())
    error_type_offset = builder.CreateString(error_type)
    message_offset = builder.CreateString(message)

    ray.core.generated.ErrorTableData.ErrorTableDataStart(builder)
    ray.core.generated.ErrorTableData.ErrorTableDataAddJobId(
        builder, driver_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddType(
        builder, error_type_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddErrorMessage(
        builder, message_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddTimestamp(
        builder, timestamp)
    error_data_offset = ray.core.generated.ErrorTableData.ErrorTableDataEnd(
        builder)
    builder.Finish(error_data_offset)

    return bytes(builder.Output())
/n/n/npython/ray/import_thread.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import traceback

import ray
from ray import ray_constants
from ray import cloudpickle as pickle
from ray import profiling
from ray import utils


class ImportThread(object):
    """"""A thread used to import exports from the driver or other workers.

    Note: The driver also has an import thread, which is used only to import
    custom class definitions from calls to register_custom_serializer that
    happen under the hood on workers.

    Attributes:
        worker: the worker object in this process.
        mode: worker mode
        redis_client: the redis client used to query exports.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    def __init__(self, worker, mode, threads_stopped):
        self.worker = worker
        self.mode = mode
        self.redis_client = worker.redis_client
        self.threads_stopped = threads_stopped

    def start(self):
        """"""Start the import thread.""""""
        self.t = threading.Thread(target=self._run, name=""ray_import_thread"")
        # Making the thread a daemon causes it to exit
        # when the main thread exits.
        self.t.daemon = True
        self.t.start()

    def join_import_thread(self):
        """"""Wait for the thread to exit.""""""
        self.t.join()

    def _run(self):
        import_pubsub_client = self.redis_client.pubsub()
        # Exports that are published after the call to
        # import_pubsub_client.subscribe and before the call to
        # import_pubsub_client.listen will still be processed in the loop.
        import_pubsub_client.subscribe(""__keyspace@0__:Exports"")
        # Keep track of the number of imports that we've imported.
        num_imported = 0

        try:
            # Get the exports that occurred before the call to subscribe.
            with self.worker.lock:
                export_keys = self.redis_client.lrange(""Exports"", 0, -1)
                for key in export_keys:
                    num_imported += 1
                    self._process_key(key)

            while True:
                # Exit if we received a signal that we should stop.
                if self.threads_stopped.is_set():
                    return

                msg = import_pubsub_client.get_message()
                if msg is None:
                    self.threads_stopped.wait(timeout=0.01)
                    continue

                with self.worker.lock:
                    if msg[""type""] == ""subscribe"":
                        continue
                    assert msg[""data""] == b""rpush""
                    num_imports = self.redis_client.llen(""Exports"")
                    assert num_imports >= num_imported
                    for i in range(num_imported, num_imports):
                        num_imported += 1
                        key = self.redis_client.lindex(""Exports"", i)
                        self._process_key(key)
        finally:
            # Close the pubsub client to avoid leaking file descriptors.
            import_pubsub_client.close()

    def _process_key(self, key):
        """"""Process the given export key from redis.""""""
        # Handle the driver case first.
        if self.mode != ray.WORKER_MODE:
            if key.startswith(b""FunctionsToRun""):
                with profiling.profile(
                        ""fetch_and_run_function"", worker=self.worker):
                    self.fetch_and_execute_function_to_run(key)
            # Return because FunctionsToRun are the only things that
            # the driver should import.
            return

        if key.startswith(b""RemoteFunction""):
            with profiling.profile(
                    ""register_remote_function"", worker=self.worker):
                (self.worker.function_actor_manager.
                 fetch_and_register_remote_function(key))
        elif key.startswith(b""FunctionsToRun""):
            with profiling.profile(
                    ""fetch_and_run_function"", worker=self.worker):
                self.fetch_and_execute_function_to_run(key)
        elif key.startswith(b""ActorClass""):
            # Keep track of the fact that this actor class has been
            # exported so that we know it is safe to turn this worker
            # into an actor of that class.
            self.worker.function_actor_manager.imported_actor_classes.add(key)
        # TODO(rkn): We may need to bring back the case of
        # fetching actor classes here.
        else:
            raise Exception(""This code should be unreachable."")

    def fetch_and_execute_function_to_run(self, key):
        """"""Run on arbitrary function on the worker.""""""
        (driver_id, serialized_function,
         run_on_other_drivers) = self.redis_client.hmget(
             key, [""driver_id"", ""function"", ""run_on_other_drivers""])

        if (utils.decode(run_on_other_drivers) == ""False""
                and self.worker.mode == ray.SCRIPT_MODE
                and driver_id != self.worker.task_driver_id.binary()):
            return

        try:
            # Deserialize the function.
            function = pickle.loads(serialized_function)
            # Run the function.
            function({""worker"": self.worker})
        except Exception:
            # If an exception was thrown when the function was run, we record
            # the traceback and notify the scheduler of the failure.
            traceback_str = traceback.format_exc()
            # Log the error message.
            utils.push_error_to_driver(
                self.worker,
                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,
                traceback_str,
                driver_id=ray.DriverID(driver_id))
/n/n/npython/ray/log_monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import errno
import logging
import os
import traceback

import colorama

import ray.ray_constants as ray_constants
import ray.utils

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)


class LogFileInfo(object):
    def __init__(self,
                 filename=None,
                 size_when_last_opened=None,
                 file_position=None,
                 file_handle=None):
        assert (filename is not None and size_when_last_opened is not None
                and file_position is not None)
        self.filename = filename
        self.size_when_last_opened = size_when_last_opened
        self.file_position = file_position
        self.file_handle = file_handle
        self.worker_pid = None


class LogMonitor(object):
    """"""A monitor process for monitoring Ray log files.

    This class mantains a list of open files and a list of closed log files. We
    can't simply leave all files open because we'll run out of file
    descriptors.

    The ""run"" method of this class will cycle between doing several things:
    1. First, it will check if any new files have appeared in the log
       directory. If so, they will be added to the list of closed files.
    2. Then, if we are unable to open any new files, we will close all of the
       files.
    3. Then, we will open as many closed files as we can that may have new
       lines (judged by an increase in file size since the last time the file
       was opened).
    4. Then we will loop through the open files and see if there are any new
       lines in the file. If so, we will publish them to Redis.

    Attributes:
        host (str): The hostname of this machine. Used to improve the log
            messages published to Redis.
        logs_dir (str): The directory that the log files are in.
        redis_client: A client used to communicate with the Redis server.
        log_filenames (set): This is the set of filenames of all files in
            open_file_infos and closed_file_infos.
        open_file_infos (list[LogFileInfo]): Info for all of the open files.
        closed_file_infos (list[LogFileInfo]): Info for all of the closed
            files.
        can_open_more_files (bool): True if we can still open more files and
            false otherwise.
    """"""

    def __init__(self, logs_dir, redis_address, redis_password=None):
        """"""Initialize the log monitor object.""""""
        self.host = os.uname()[1]
        self.logs_dir = logs_dir
        self.redis_client = ray.services.create_redis_client(
            redis_address, password=redis_password)
        self.log_filenames = set()
        self.open_file_infos = []
        self.closed_file_infos = []
        self.can_open_more_files = True

    def close_all_files(self):
        """"""Close all open files (so that we can open more).""""""
        while len(self.open_file_infos) > 0:
            file_info = self.open_file_infos.pop(0)
            file_info.file_handle.close()
            file_info.file_handle = None
            self.closed_file_infos.append(file_info)
        self.can_open_more_files = True

    def update_log_filenames(self):
        """"""Update the list of log files to monitor.""""""
        log_filenames = os.listdir(self.logs_dir)

        for log_filename in log_filenames:
            full_path = os.path.join(self.logs_dir, log_filename)
            if full_path not in self.log_filenames:
                self.log_filenames.add(full_path)
                self.closed_file_infos.append(
                    LogFileInfo(
                        filename=full_path,
                        size_when_last_opened=0,
                        file_position=0,
                        file_handle=None))
                logger.info(""Beginning to track file {}"".format(log_filename))

    def open_closed_files(self):
        """"""Open some closed files if they may have new lines.

        Opening more files may require us to close some of the already open
        files.
        """"""
        if not self.can_open_more_files:
            # If we can't open any more files. Close all of the files.
            self.close_all_files()

        files_with_no_updates = []
        while len(self.closed_file_infos) > 0:
            if (len(self.open_file_infos) >=
                    ray_constants.LOG_MONITOR_MAX_OPEN_FILES):
                self.can_open_more_files = False
                break

            file_info = self.closed_file_infos.pop(0)
            assert file_info.file_handle is None
            # Get the file size to see if it has gotten bigger since we last
            # opened it.
            try:
                file_size = os.path.getsize(file_info.filename)
            except (IOError, OSError) as e:
                # Catch ""file not found"" errors.
                if e.errno == errno.ENOENT:
                    logger.warning(""Warning: The file {} was not ""
                                   ""found."".format(file_info.filename))
                    self.log_filenames.remove(file_info.filename)
                    continue
                raise e

            # If some new lines have been added to this file, try to reopen the
            # file.
            if file_size > file_info.size_when_last_opened:
                try:
                    f = open(file_info.filename, ""r"")
                except (IOError, OSError) as e:
                    if e.errno == errno.ENOENT:
                        logger.warning(""Warning: The file {} was not ""
                                       ""found."".format(file_info.filename))
                        self.log_filenames.remove(file_info.filename)
                        continue
                    else:
                        raise e

                f.seek(file_info.file_position)
                file_info.filesize_when_last_opened = file_size
                file_info.file_handle = f
                self.open_file_infos.append(file_info)
            else:
                files_with_no_updates.append(file_info)

        # Add the files with no changes back to the list of closed files.
        self.closed_file_infos += files_with_no_updates

    def check_log_files_and_publish_updates(self):
        """"""Get any changes to the log files and push updates to Redis.""""""
        for file_info in self.open_file_infos:
            assert not file_info.file_handle.closed

            lines_to_publish = []
            max_num_lines_to_read = 100
            for _ in range(max_num_lines_to_read):
                next_line = file_info.file_handle.readline()
                if next_line == """":
                    break
                if next_line[-1] == ""\n"":
                    next_line = next_line[:-1]
                lines_to_publish.append(next_line)

            # Publish the lines if this is a worker process.
            filename = file_info.filename.split(""/"")[-1]
            is_worker = (filename.startswith(""worker"")
                         and (filename.endswith(""out"")
                              or filename.endswith(""err"")))
            output_type = ""stdout"" if filename.endswith(""out"") else ""stderr""

            if is_worker and file_info.file_position == 0:
                if (len(lines_to_publish) > 0 and
                        lines_to_publish[0].startswith(""Ray worker pid: "")):
                    file_info.worker_pid = int(
                        lines_to_publish[0].split("" "")[-1])
                    lines_to_publish = lines_to_publish[1:]

            # Record the current position in the file.
            file_info.file_position = file_info.file_handle.tell()

            if len(lines_to_publish) > 0 and is_worker:
                lines_to_publish.insert(
                    0, ""{}{}{} (pid={}, host={})"".format(
                        colorama.Fore.CYAN, ""worker ({})"".format(output_type),
                        colorama.Fore.RESET, file_info.worker_pid, self.host))

                self.redis_client.publish(ray.gcs_utils.LOG_FILE_CHANNEL,
                                          ""\n"".join(lines_to_publish))

    def run(self):
        """"""Run the log monitor.

        This will query Redis once every second to check if there are new log
        files to monitor. It will also store those log files in Redis.
        """"""
        while True:
            self.update_log_filenames()
            self.open_closed_files()
            self.check_log_files_and_publish_updates()


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""log monitor to connect ""
                     ""to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""The address to use for Redis."")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    parser.add_argument(
        ""--logs-dir"",
        required=True,
        type=str,
        help=""Specify the path of the temporary directory used by Ray ""
        ""processes."")
    args = parser.parse_args()
    ray.utils.setup_logger(args.logging_level, args.logging_format)

    log_monitor = LogMonitor(
        args.logs_dir, args.redis_address, redis_password=args.redis_password)

    try:
        log_monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = ray.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = (""The log monitor on node {} failed with the following ""
                   ""error:\n{}"".format(os.uname()[1], traceback_str))
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.LOG_MONITOR_DIED_ERROR, message)
        raise e
/n/n/npython/ray/monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import time
import traceback

import redis

import ray
from ray.autoscaler.autoscaler import LoadMetrics, StandardAutoscaler
import ray.cloudpickle as pickle
import ray.gcs_utils
import ray.utils
import ray.ray_constants as ray_constants
from ray.services import get_ip_address, get_port
from ray.utils import (binary_to_hex, binary_to_object_id, hex_to_binary,
                       setup_logger)

logger = logging.getLogger(__name__)


class Monitor(object):
    """"""A monitor for Ray processes.

    The monitor is in charge of cleaning up the tables in the global state
    after processes have died. The monitor is currently not responsible for
    detecting component failures.

    Attributes:
        redis: A connection to the Redis server.
        subscribe_client: A pubsub client for the Redis server. This is used to
            receive notifications about failed components.
    """"""

    def __init__(self, redis_address, autoscaling_config, redis_password=None):
        # Initialize the Redis clients.
        self.state = ray.experimental.state.GlobalState()
        redis_ip_address = get_ip_address(args.redis_address)
        redis_port = get_port(args.redis_address)
        self.state._initialize_global_state(
            redis_ip_address, redis_port, redis_password=redis_password)
        self.redis = ray.services.create_redis_client(
            redis_address, password=redis_password)
        # Setup subscriptions to the primary Redis server and the Redis shards.
        self.primary_subscribe_client = self.redis.pubsub(
            ignore_subscribe_messages=True)
        # Keep a mapping from local scheduler client ID to IP address to use
        # for updating the load metrics.
        self.local_scheduler_id_to_ip_map = {}
        self.load_metrics = LoadMetrics()
        if autoscaling_config:
            self.autoscaler = StandardAutoscaler(autoscaling_config,
                                                 self.load_metrics)
        else:
            self.autoscaler = None

        # Experimental feature: GCS flushing.
        self.issue_gcs_flushes = ""RAY_USE_NEW_GCS"" in os.environ
        self.gcs_flush_policy = None
        if self.issue_gcs_flushes:
            # Data is stored under the first data shard, so we issue flushes to
            # that redis server.
            addr_port = self.redis.lrange(""RedisShards"", 0, -1)
            if len(addr_port) > 1:
                logger.warning(
                    ""Monitor: ""
                    ""TODO: if launching > 1 redis shard, flushing needs to ""
                    ""touch shards in parallel."")
                self.issue_gcs_flushes = False
            else:
                addr_port = addr_port[0].split(b"":"")
                self.redis_shard = redis.StrictRedis(
                    host=addr_port[0],
                    port=addr_port[1],
                    password=redis_password)
                try:
                    self.redis_shard.execute_command(""HEAD.FLUSH 0"")
                except redis.exceptions.ResponseError as e:
                    logger.info(
                        ""Monitor: ""
                        ""Turning off flushing due to exception: {}"".format(
                            str(e)))
                    self.issue_gcs_flushes = False

    def __del__(self):
        """"""Destruct the monitor object.""""""
        # We close the pubsub client to avoid leaking file descriptors.
        self.primary_subscribe_client.close()

    def subscribe(self, channel):
        """"""Subscribe to the given channel on the primary Redis shard.

        Args:
            channel (str): The channel to subscribe to.

        Raises:
            Exception: An exception is raised if the subscription fails.
        """"""
        self.primary_subscribe_client.subscribe(channel)

    def xray_heartbeat_batch_handler(self, unused_channel, data):
        """"""Handle an xray heartbeat batch message from Redis.""""""

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        heartbeat_data = gcs_entries.Entries(0)

        message = (ray.gcs_utils.HeartbeatBatchTableData.
                   GetRootAsHeartbeatBatchTableData(heartbeat_data, 0))

        for j in range(message.BatchLength()):
            heartbeat_message = message.Batch(j)

            num_resources = heartbeat_message.ResourcesAvailableLabelLength()
            static_resources = {}
            dynamic_resources = {}
            for i in range(num_resources):
                dyn = heartbeat_message.ResourcesAvailableLabel(i)
                static = heartbeat_message.ResourcesTotalLabel(i)
                dynamic_resources[dyn] = (
                    heartbeat_message.ResourcesAvailableCapacity(i))
                static_resources[static] = (
                    heartbeat_message.ResourcesTotalCapacity(i))

            # Update the load metrics for this local scheduler.
            client_id = ray.utils.binary_to_hex(heartbeat_message.ClientId())
            ip = self.local_scheduler_id_to_ip_map.get(client_id)
            if ip:
                self.load_metrics.update(ip, static_resources,
                                         dynamic_resources)
            else:
                logger.warning(
                    ""Monitor: ""
                    ""could not find ip for client {}"".format(client_id))

    def _xray_clean_up_entries_for_driver(self, driver_id):
        """"""Remove this driver's object/task entries from redis.

        Removes control-state entries of all tasks and task return
        objects belonging to the driver.

        Args:
            driver_id: The driver id.
        """"""

        xray_task_table_prefix = (
            ray.gcs_utils.TablePrefix_RAYLET_TASK_string.encode(""ascii""))
        xray_object_table_prefix = (
            ray.gcs_utils.TablePrefix_OBJECT_string.encode(""ascii""))

        task_table_objects = self.state.task_table()
        driver_id_hex = binary_to_hex(driver_id)
        driver_task_id_bins = set()
        for task_id_hex, task_info in task_table_objects.items():
            task_table_object = task_info[""TaskSpec""]
            task_driver_id_hex = task_table_object[""DriverID""]
            if driver_id_hex != task_driver_id_hex:
                # Ignore tasks that aren't from this driver.
                continue
            driver_task_id_bins.add(hex_to_binary(task_id_hex))

        # Get objects associated with the driver.
        object_table_objects = self.state.object_table()
        driver_object_id_bins = set()
        for object_id, _ in object_table_objects.items():
            task_id_bin = ray._raylet.compute_task_id(object_id).binary()
            if task_id_bin in driver_task_id_bins:
                driver_object_id_bins.add(object_id.binary())

        def to_shard_index(id_bin):
            return binary_to_object_id(id_bin).redis_shard_hash() % len(
                self.state.redis_clients)

        # Form the redis keys to delete.
        sharded_keys = [[] for _ in range(len(self.state.redis_clients))]
        for task_id_bin in driver_task_id_bins:
            sharded_keys[to_shard_index(task_id_bin)].append(
                xray_task_table_prefix + task_id_bin)
        for object_id_bin in driver_object_id_bins:
            sharded_keys[to_shard_index(object_id_bin)].append(
                xray_object_table_prefix + object_id_bin)

        # Remove with best effort.
        for shard_index in range(len(sharded_keys)):
            keys = sharded_keys[shard_index]
            if len(keys) == 0:
                continue
            redis = self.state.redis_clients[shard_index]
            num_deleted = redis.delete(*keys)
            logger.info(""Monitor: ""
                        ""Removed {} dead redis entries of the ""
                        ""driver from redis shard {}."".format(
                            num_deleted, shard_index))
            if num_deleted != len(keys):
                logger.warning(""Monitor: ""
                               ""Failed to remove {} relevant redis ""
                               ""entries from redis shard {}."".format(
                                   len(keys) - num_deleted, shard_index))

    def xray_driver_removed_handler(self, unused_channel, data):
        """"""Handle a notification that a driver has been removed.

        Args:
            unused_channel: The message channel.
            data: The message data.
        """"""
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        driver_data = gcs_entries.Entries(0)
        message = ray.gcs_utils.DriverTableData.GetRootAsDriverTableData(
            driver_data, 0)
        driver_id = message.DriverId()
        logger.info(""Monitor: ""
                    ""XRay Driver {} has been removed."".format(
                        binary_to_hex(driver_id)))
        self._xray_clean_up_entries_for_driver(driver_id)

    def process_messages(self, max_messages=10000):
        """"""Process all messages ready in the subscription channels.

        This reads messages from the subscription channels and calls the
        appropriate handlers until there are no messages left.

        Args:
            max_messages: The maximum number of messages to process before
                returning.
        """"""
        subscribe_clients = [self.primary_subscribe_client]
        for subscribe_client in subscribe_clients:
            for _ in range(max_messages):
                message = subscribe_client.get_message()
                if message is None:
                    # Continue on to the next subscribe client.
                    break

                # Parse the message.
                channel = message[""channel""]
                data = message[""data""]

                # Determine the appropriate message handler.
                if channel == ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL:
                    # Similar functionality as local scheduler info channel
                    message_handler = self.xray_heartbeat_batch_handler
                elif channel == ray.gcs_utils.XRAY_DRIVER_CHANNEL:
                    # Handles driver death.
                    message_handler = self.xray_driver_removed_handler
                else:
                    raise Exception(""This code should be unreachable."")

                # Call the handler.
                message_handler(channel, data)

    def update_local_scheduler_map(self):
        local_schedulers = self.state.client_table()
        self.local_scheduler_id_to_ip_map = {}
        for local_scheduler_info in local_schedulers:
            client_id = local_scheduler_info.get(""DBClientID"") or \
                local_scheduler_info[""ClientID""]
            ip_address = (
                local_scheduler_info.get(""AuxAddress"")
                or local_scheduler_info[""NodeManagerAddress""]).split("":"")[0]
            self.local_scheduler_id_to_ip_map[client_id] = ip_address

    def _maybe_flush_gcs(self):
        """"""Experimental: issue a flush request to the GCS.

        The purpose of this feature is to control GCS memory usage.

        To activate this feature, Ray must be compiled with the flag
        RAY_USE_NEW_GCS set, and Ray must be started at run time with the flag
        as well.
        """"""
        if not self.issue_gcs_flushes:
            return
        if self.gcs_flush_policy is None:
            serialized = self.redis.get(""gcs_flushing_policy"")
            if serialized is None:
                # Client has not set any policy; by default flushing is off.
                return
            self.gcs_flush_policy = pickle.loads(serialized)

        if not self.gcs_flush_policy.should_flush(self.redis_shard):
            return

        max_entries_to_flush = self.gcs_flush_policy.num_entries_to_flush()
        num_flushed = self.redis_shard.execute_command(
            ""HEAD.FLUSH {}"".format(max_entries_to_flush))
        logger.info(""Monitor: num_flushed {}"".format(num_flushed))

        # This flushes event log and log files.
        ray.experimental.flush_redis_unsafe(self.redis)

        self.gcs_flush_policy.record_flush()

    def run(self):
        """"""Run the monitor.

        This function loops forever, checking for messages about dead database
        clients and cleaning up state accordingly.
        """"""
        # Initialize the subscription channel.
        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)
        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)

        # TODO(rkn): If there were any dead clients at startup, we should clean
        # up the associated state in the state tables.

        # Handle messages from the subscription channels.
        while True:
            # Update the mapping from local scheduler client ID to IP address.
            # This is only used to update the load metrics for the autoscaler.
            self.update_local_scheduler_map()

            # Process autoscaling actions
            if self.autoscaler:
                self.autoscaler.update()

            self._maybe_flush_gcs()

            # Process a round of messages.
            self.process_messages()

            # Wait for a heartbeat interval before processing the next round of
            # messages.
            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)

        # TODO(rkn): This infinite loop should be inside of a try/except block,
        # and if an exception is thrown we should push an error message to all
        # drivers.


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""monitor to connect to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""the address to use for Redis"")
    parser.add_argument(
        ""--autoscaling-config"",
        required=False,
        type=str,
        help=""the path to the autoscaling config file"")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    setup_logger(args.logging_level, args.logging_format)

    if args.autoscaling_config:
        autoscaling_config = os.path.expanduser(args.autoscaling_config)
    else:
        autoscaling_config = None

    monitor = Monitor(
        args.redis_address,
        autoscaling_config,
        redis_password=args.redis_password)

    try:
        monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = ray.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = ""The monitor failed with the following error:\n{}"".format(
            traceback_str)
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)
        raise e
/n/n/npython/ray/node.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import atexit
import json
import os
import logging
import signal
import threading
import time

import ray
import ray.ray_constants as ray_constants
from ray.tempfile_services import (
    get_logs_dir_path, get_object_store_socket_name, get_raylet_socket_name,
    new_log_monitor_log_file, new_monitor_log_file,
    new_raylet_monitor_log_file, new_plasma_store_log_file,
    new_raylet_log_file, new_webui_log_file, set_temp_root,
    try_to_create_directory)

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray configures it by default automatically
# using logging.basicConfig in its entry/init points.
logger = logging.getLogger(__name__)


class Node(object):
    """"""An encapsulation of the Ray processes on a single node.

    This class is responsible for starting Ray processes and killing them.

    Attributes:
        all_processes (dict): A mapping from process type (str) to a list of
            ProcessInfo objects. All lists have length one except for the Redis
            server list, which has multiple.
    """"""

    def __init__(self, ray_params, head=False, shutdown_at_exit=True):
        """"""Start a node.

        Args:
            ray_params (ray.params.RayParams): The parameters to use to
                configure the node.
            head (bool): True if this is the head node, which means it will
                start additional processes like the Redis servers, monitor
                processes, and web UI.
            shutdown_at_exit (bool): If true, a handler will be registered to
                shutdown the processes started here when the Python interpreter
                exits.
        """"""
        self.all_processes = {}

        ray_params.update_if_absent(
            node_ip_address=ray.services.get_node_ip_address(),
            include_log_monitor=True,
            resources={},
            include_webui=False,
            worker_path=os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                ""workers/default_worker.py""))

        if head:
            ray_params.update_if_absent(num_redis_shards=1, include_webui=True)
        else:
            redis_client = ray.services.create_redis_client(
                ray_params.redis_address, ray_params.redis_password)
            ray_params.include_java = (
                ray.services.include_java_from_redis(redis_client))

        self._ray_params = ray_params
        self._config = (json.loads(ray_params._internal_config)
                        if ray_params._internal_config else None)
        self._node_ip_address = ray_params.node_ip_address
        self._redis_address = ray_params.redis_address
        self._plasma_store_socket_name = None
        self._raylet_socket_name = None
        self._webui_url = None

        self.start_ray_processes()

        if shutdown_at_exit:
            atexit.register(lambda: self.kill_all_processes(
                check_alive=False, allow_graceful=True))

    @property
    def node_ip_address(self):
        """"""Get the cluster Redis address.""""""
        return self._node_ip_address

    @property
    def redis_address(self):
        """"""Get the cluster Redis address.""""""
        return self._redis_address

    @property
    def plasma_store_socket_name(self):
        """"""Get the node's plasma store socket name.""""""
        return self._plasma_store_socket_name

    @property
    def webui_url(self):
        """"""Get the cluster's web UI url.""""""
        return self._webui_url

    @property
    def raylet_socket_name(self):
        """"""Get the node's raylet socket name.""""""
        return self._raylet_socket_name

    def prepare_socket_file(self, socket_path):
        """"""Prepare the socket file for raylet and plasma.

        This method helps to prepare a socket file.
        1. Make the directory if the directory does not exist.
        2. If the socket file exists, raise exception.

        Args:
            socket_path (string): the socket file to prepare.
        """"""
        if not os.path.exists(socket_path):
            path = os.path.dirname(socket_path)
            if not os.path.isdir(path):
                try_to_create_directory(path)
        else:
            raise Exception(""Socket file {} exists!"".format(socket_path))

    def start_redis(self):
        """"""Start the Redis servers.""""""
        assert self._redis_address is None
        (self._redis_address, redis_shards,
         process_infos) = ray.services.start_redis(
             self._node_ip_address,
             port=self._ray_params.redis_port,
             redis_shard_ports=self._ray_params.redis_shard_ports,
             num_redis_shards=self._ray_params.num_redis_shards,
             redis_max_clients=self._ray_params.redis_max_clients,
             redirect_output=self._ray_params.redirect_output,
             redirect_worker_output=self._ray_params.redirect_worker_output,
             password=self._ray_params.redis_password,
             redis_max_memory=self._ray_params.redis_max_memory)
        assert (
            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (
            process_infos)

    def start_log_monitor(self):
        """"""Start the log monitor.""""""
        stdout_file, stderr_file = new_log_monitor_log_file()
        process_info = ray.services.start_log_monitor(
            self.redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_LOG_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] = [
            process_info
        ]

    def start_ui(self):
        """"""Start the web UI.""""""
        stdout_file, stderr_file = new_webui_log_file()
        self._webui_url, process_info = ray.services.start_ui(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        assert ray_constants.PROCESS_TYPE_WEB_UI not in self.all_processes
        if process_info is not None:
            self.all_processes[ray_constants.PROCESS_TYPE_WEB_UI] = [
                process_info
            ]

    def start_plasma_store(self):
        """"""Start the plasma store.""""""
        assert self._plasma_store_socket_name is None
        # If the user specified a socket name, use it.
        self._plasma_store_socket_name = (
            self._ray_params.plasma_store_socket_name
            or get_object_store_socket_name())
        self.prepare_socket_file(self._plasma_store_socket_name)
        stdout_file, stderr_file = (new_plasma_store_log_file(
            self._ray_params.redirect_output))
        process_info = ray.services.start_plasma_store(
            self._node_ip_address,
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            object_store_memory=self._ray_params.object_store_memory,
            plasma_directory=self._ray_params.plasma_directory,
            huge_pages=self._ray_params.huge_pages,
            plasma_store_socket_name=self._plasma_store_socket_name)
        assert (
            ray_constants.PROCESS_TYPE_PLASMA_STORE not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_PLASMA_STORE] = [
            process_info
        ]

    def start_raylet(self, use_valgrind=False, use_profiler=False):
        """"""Start the raylet.

        Args:
            use_valgrind (bool): True if we should start the process in
                valgrind.
            use_profiler (bool): True if we should start the process in the
                valgrind profiler.
        """"""
        assert self._raylet_socket_name is None
        # If the user specified a socket name, use it.
        self._raylet_socket_name = (self._ray_params.raylet_socket_name
                                    or get_raylet_socket_name())
        self.prepare_socket_file(self._raylet_socket_name)
        stdout_file, stderr_file = new_raylet_log_file(
            redirect_output=self._ray_params.redirect_output)
        process_info = ray.services.start_raylet(
            self._redis_address,
            self._node_ip_address,
            self._raylet_socket_name,
            self._plasma_store_socket_name,
            self._ray_params.worker_path,
            self._ray_params.num_cpus,
            self._ray_params.num_gpus,
            self._ray_params.resources,
            self._ray_params.object_manager_port,
            self._ray_params.node_manager_port,
            self._ray_params.redis_password,
            use_valgrind=use_valgrind,
            use_profiler=use_profiler,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            config=self._config,
            include_java=self._ray_params.include_java,
            java_worker_options=self._ray_params.java_worker_options,
        )
        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]

    def start_worker(self):
        """"""Start a worker process.""""""
        raise NotImplementedError

    def start_monitor(self):
        """"""Start the monitor.""""""
        stdout_file, stderr_file = new_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_monitor(
            self._redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            autoscaling_config=self._ray_params.autoscaling_config,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_MONITOR] = [process_info]

    def start_raylet_monitor(self):
        """"""Start the raylet monitor.""""""
        stdout_file, stderr_file = new_raylet_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_raylet_monitor(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password,
            config=self._config)
        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in
                self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [
            process_info
        ]

    def start_ray_processes(self):
        """"""Start all of the processes on the node.""""""
        set_temp_root(self._ray_params.temp_dir)
        logger.info(
            ""Process STDOUT and STDERR is being redirected to {}."".format(
                get_logs_dir_path()))

        # If this is the head node, start the relevant head node processes.
        if self._redis_address is None:
            self.start_redis()
            self.start_monitor()
            self.start_raylet_monitor()

        self.start_plasma_store()
        self.start_raylet()

        if self._ray_params.include_log_monitor:
            self.start_log_monitor()
        if self._ray_params.include_webui:
            self.start_ui()

    def _kill_process_type(self,
                           process_type,
                           allow_graceful=False,
                           check_alive=True,
                           wait=False):
        """"""Kill a process of a given type.

        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all
        of the Redis servers.

        If the process was started in valgrind, then we will raise an exception
        if the process has a non-zero exit code.

        Args:
            process_type: The type of the process to kill.
            allow_graceful (bool): Send a SIGTERM first and give the process
                time to exit gracefully. If that doesn't work, then use
                SIGKILL. We usually want to do this outside of tests.
            check_alive (bool): If true, then we expect the process to be alive
                and will raise an exception if the process is already dead.
            wait (bool): If true, then this method will not return until the
                process in question has exited.

        Raises:
            This process raises an exception in the following cases:
                1. The process had already died and check_alive is true.
                2. The process had been started in valgrind and had a non-zero
                   exit code.
        """"""
        process_infos = self.all_processes[process_type]
        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:
            assert len(process_infos) == 1
        for process_info in process_infos:
            process = process_info.process
            # Handle the case where the process has already exited.
            if process.poll() is not None:
                if check_alive:
                    raise Exception(""Attempting to kill a process of type ""
                                    ""'{}', but this process is already dead.""
                                    .format(process_type))
                else:
                    continue

            if process_info.use_valgrind:
                process.terminate()
                process.wait()
                if process.returncode != 0:
                    message = (""Valgrind detected some errors in process of ""
                               ""type {}. Error code {}."".format(
                                   process_type, process.returncode))
                    if process_info.stdout_file is not None:
                        with open(process_info.stdout_file, ""r"") as f:
                            message += ""\nPROCESS STDOUT:\n"" + f.read()
                    if process_info.stderr_file is not None:
                        with open(process_info.stderr_file, ""r"") as f:
                            message += ""\nPROCESS STDERR:\n"" + f.read()
                    raise Exception(message)
                continue

            if process_info.use_valgrind_profiler:
                # Give process signal to write profiler data.
                os.kill(process.pid, signal.SIGINT)
                # Wait for profiling data to be written.
                time.sleep(0.1)

            if allow_graceful:
                # Allow the process one second to exit gracefully.
                process.terminate()
                timer = threading.Timer(1, lambda process: process.kill(),
                                        [process])
                try:
                    timer.start()
                    process.wait()
                finally:
                    timer.cancel()

                if process.poll() is not None:
                    continue

            # If the process did not exit within one second, force kill it.
            process.kill()
            # The reason we usually don't call process.wait() here is that
            # there's some chance we'd end up waiting a really long time.
            if wait:
                process.wait()

        del self.all_processes[process_type]

    def kill_redis(self, check_alive=True):
        """"""Kill the Redis servers.

        Args:
            check_alive (bool): Raise an exception if any of the processes
                were already dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)

    def kill_plasma_store(self, check_alive=True):
        """"""Kill the plasma store.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)

    def kill_raylet(self, check_alive=True):
        """"""Kill the raylet.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET, check_alive=check_alive)

    def kill_log_monitor(self, check_alive=True):
        """"""Kill the log monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_LOG_MONITOR, check_alive=check_alive)

    def kill_monitor(self, check_alive=True):
        """"""Kill the monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_MONITOR, check_alive=check_alive)

    def kill_raylet_monitor(self, check_alive=True):
        """"""Kill the raylet monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET_MONITOR, check_alive=check_alive)

    def kill_all_processes(self, check_alive=True, allow_graceful=False):
        """"""Kill all of the processes.

        Note that This is slower than necessary because it calls kill, wait,
        kill, wait, ... instead of kill, kill, ..., wait, wait, ...

        Args:
            check_alive (bool): Raise an exception if any of the processes were
                already dead.
        """"""
        # Kill the raylet first. This is important for suppressing errors at
        # shutdown because we give the raylet a chance to exit gracefully and
        # clean up its child worker processes. If we were to kill the plasma
        # store (or Redis) first, that could cause the raylet to exit
        # ungracefully, leading to more verbose output from the workers.
        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:
            self._kill_process_type(
                ray_constants.PROCESS_TYPE_RAYLET,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

        # We call ""list"" to copy the keys because we are modifying the
        # dictionary while iterating over it.
        for process_type in list(self.all_processes.keys()):
            self._kill_process_type(
                process_type,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

    def live_processes(self):
        """"""Return a list of the live processes.

        Returns:
            A list of the live processes.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is None:
                    result.append((process_type, process_info.process))
        return result

    def dead_processes(self):
        """"""Return a list of the dead processes.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            A list of the dead processes ignoring the ones that have been
                explicitly killed.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is not None:
                    result.append((process_type, process_info.process))
        return result

    def any_processes_alive(self):
        """"""Return true if any processes are still alive.

        Returns:
            True if any process is still alive.
        """"""
        return any(self.live_processes())

    def remaining_processes_alive(self):
        """"""Return true if all remaining processes are still alive.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            True if any process that wasn't explicitly killed is still alive.
        """"""
        return not any(self.dead_processes())
/n/n/npython/ray/parameter.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging

import ray.ray_constants as ray_constants


class RayParams(object):
    """"""A class used to store the parameters used by Ray.

    Attributes:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        redis_port (int): The port that the primary Redis shard should listen
            to. If None, then a random port will be chosen.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_cpus (int): Number of CPUs to configure the raylet with.
        num_gpus (int): Number of GPUs to configure the raylet with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries. This only applies to the
            sharded redis tables (task and object tables).
        object_manager_port int: The port to use for the object manager.
        node_manager_port: The port to use for the node manager.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        worker_path (str): The path of the source code that will be run by the
            worker.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        include_log_monitor (bool): If True, then start a log monitor to
            monitor the log files for all processes on this node and push their
            contents to Redis.
        autoscaling_config: path to autoscaling config file.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.
    """"""

    def __init__(self,
                 redis_address=None,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_store_memory=None,
                 redis_max_memory=None,
                 redis_port=None,
                 redis_shard_ports=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 node_ip_address=None,
                 object_id_seed=None,
                 num_workers=None,
                 local_mode=False,
                 driver_mode=None,
                 redirect_worker_output=True,
                 redirect_output=True,
                 num_redis_shards=None,
                 redis_max_clients=None,
                 redis_password=None,
                 plasma_directory=None,
                 worker_path=None,
                 huge_pages=False,
                 include_webui=None,
                 logging_level=logging.INFO,
                 logging_format=ray_constants.LOGGER_FORMAT,
                 plasma_store_socket_name=None,
                 raylet_socket_name=None,
                 temp_dir=None,
                 include_log_monitor=None,
                 autoscaling_config=None,
                 include_java=False,
                 java_worker_options=None,
                 _internal_config=None):
        self.object_id_seed = object_id_seed
        self.redis_address = redis_address
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.object_store_memory = object_store_memory
        self.redis_max_memory = redis_max_memory
        self.redis_port = redis_port
        self.redis_shard_ports = redis_shard_ports
        self.object_manager_port = object_manager_port
        self.node_manager_port = node_manager_port
        self.node_ip_address = node_ip_address
        self.num_workers = num_workers
        self.local_mode = local_mode
        self.driver_mode = driver_mode
        self.redirect_worker_output = redirect_worker_output
        self.redirect_output = redirect_output
        self.num_redis_shards = num_redis_shards
        self.redis_max_clients = redis_max_clients
        self.redis_password = redis_password
        self.plasma_directory = plasma_directory
        self.worker_path = worker_path
        self.huge_pages = huge_pages
        self.include_webui = include_webui
        self.plasma_store_socket_name = plasma_store_socket_name
        self.raylet_socket_name = raylet_socket_name
        self.temp_dir = temp_dir
        self.include_log_monitor = include_log_monitor
        self.autoscaling_config = autoscaling_config
        self.include_java = include_java
        self.java_worker_options = java_worker_options
        self._internal_config = _internal_config
        self._check_usage()

    def update(self, **kwargs):
        """"""Update the settings according to the keyword arguments.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update: %s"" % arg)

        self._check_usage()

    def update_if_absent(self, **kwargs):
        """"""Update the settings when the target fields are None.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                if getattr(self, arg) is None:
                    setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update_if_absent: %s"" % arg)

        self._check_usage()

    def _check_usage(self):
        if self.resources is not None:
            assert ""CPU"" not in self.resources, (
                ""'CPU' should not be included in the resource dictionary. Use ""
                ""num_cpus instead."")
            assert ""GPU"" not in self.resources, (
                ""'GPU' should not be included in the resource dictionary. Use ""
                ""num_gpus instead."")

        if self.num_workers is not None:
            raise ValueError(
                ""The 'num_workers' argument is deprecated. Please use ""
                ""'num_cpus' instead."")

        if self.include_java is None and self.java_worker_options is not None:
            raise ValueError(""Should not specify `java-worker-options` ""
                             ""without providing `include-java`."")
/n/n/npython/ray/profiling.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import time
import threading
import traceback

import ray

LOG_POINT = 0
LOG_SPAN_START = 1
LOG_SPAN_END = 2


class _NullLogSpan(object):
    """"""A log span context manager that does nothing""""""

    def __enter__(self):
        pass

    def __exit__(self, type, value, tb):
        pass


NULL_LOG_SPAN = _NullLogSpan()


def profile(event_type, extra_data=None, worker=None):
    """"""Profile a span of time so that it appears in the timeline visualization.

    Note that this only works in the raylet code path.

    This function can be used as follows (both on the driver or within a task).

    .. code-block:: python

        with ray.profile(""custom event"", extra_data={'key': 'value'}):
            # Do some computation here.

    Optionally, a dictionary can be passed as the ""extra_data"" argument, and
    it can have keys ""name"" and ""cname"" if you want to override the default
    timeline display text and box color. Other values will appear at the bottom
    of the chrome tracing GUI when you click on the box corresponding to this
    profile span.

    Args:
        event_type: A string describing the type of the event.
        extra_data: This must be a dictionary mapping strings to strings. This
            data will be added to the json objects that are used to populate
            the timeline, so if you want to set a particular color, you can
            simply set the ""cname"" attribute to an appropriate color.
            Similarly, if you set the ""name"" attribute, then that will set the
            text displayed on the box in the timeline.

    Returns:
        An object that can profile a span of time via a ""with"" statement.
    """"""
    if worker is None:
        worker = ray.worker.global_worker
    return RayLogSpanRaylet(worker.profiler, event_type, extra_data=extra_data)


class Profiler(object):
    """"""A class that holds the profiling states.

    Attributes:
        worker: the worker to profile.
        events: the buffer of events.
        lock: the lock to protect access of events.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    def __init__(self, worker, threads_stopped):
        self.worker = worker
        self.events = []
        self.lock = threading.Lock()
        self.threads_stopped = threads_stopped

    def start_flush_thread(self):
        self.t = threading.Thread(
            target=self._periodically_flush_profile_events,
            name=""ray_push_profiling_information"")
        # Making the thread a daemon causes it to exit when the main thread
        # exits.
        self.t.daemon = True
        self.t.start()

    def join_flush_thread(self):
        """"""Wait for the flush thread to exit.""""""
        self.t.join()

    def _periodically_flush_profile_events(self):
        """"""Drivers run this as a thread to flush profile data in the
        background.""""""
        # Note(rkn): This is run on a background thread in the driver. It uses
        # the local scheduler client. This should be ok because it doesn't read
        # from the local scheduler client and we have the GIL here. However,
        # if either of those things changes, then we could run into issues.
        while True:
            # Sleep for 1 second. This will be interrupted if
            # self.threads_stopped is set.
            self.threads_stopped.wait(timeout=1)

            # Exit if we received a signal that we should stop.
            if self.threads_stopped.is_set():
                return

            self.flush_profile_data()

    def flush_profile_data(self):
        """"""Push the logged profiling data to the global control store.""""""
        with self.lock:
            events = self.events
            self.events = []

        if self.worker.mode == ray.WORKER_MODE:
            component_type = ""worker""
        else:
            component_type = ""driver""

        self.worker.raylet_client.push_profile_events(
            component_type, ray.UniqueID(self.worker.worker_id),
            self.worker.node_ip_address, events)

    def add_event(self, event):
        with self.lock:
            self.events.append(event)


class RayLogSpanRaylet(object):
    """"""An object used to enable logging a span of events with a with statement.

    Attributes:
        event_type (str): The type of the event being logged.
        extra_data: Additional information to log.
    """"""

    def __init__(self, profiler, event_type, extra_data=None):
        """"""Initialize a RayLogSpanRaylet object.""""""
        self.profiler = profiler
        self.event_type = event_type
        self.extra_data = extra_data if extra_data is not None else {}

    def set_attribute(self, key, value):
        """"""Add a key-value pair to the extra_data dict.

        This can be used to add attributes that are not available when
        ray.profile was called.

        Args:
            key: The attribute name.
            value: The attribute value.
        """"""
        if not isinstance(key, str) or not isinstance(value, str):
            raise ValueError(""The arguments 'key' and 'value' must both be ""
                             ""strings. Instead they are {} and {}."".format(
                                 key, value))
        self.extra_data[key] = value

    def __enter__(self):
        """"""Log the beginning of a span event.

        Returns:
            The object itself is returned so that if the block is opened using
                ""with ray.profile(...) as prof:"", we can call
                ""prof.set_attribute"" inside the block.
        """"""
        self.start_time = time.time()
        return self

    def __exit__(self, type, value, tb):
        """"""Log the end of a span event. Log any exception that occurred.""""""
        for key, value in self.extra_data.items():
            if not isinstance(key, str) or not isinstance(value, str):
                raise ValueError(""The extra_data argument must be a ""
                                 ""dictionary mapping strings to strings. ""
                                 ""Instead it is {}."".format(self.extra_data))

        if type is not None:
            extra_data = json.dumps({
                ""type"": str(type),
                ""value"": str(value),
                ""traceback"": str(traceback.format_exc()),
            })
        else:
            extra_data = json.dumps(self.extra_data)

        event = {
            ""event_type"": self.event_type,
            ""start_time"": self.start_time,
            ""end_time"": time.time(),
            ""extra_data"": extra_data,
        }

        self.profiler.add_event(event)
/n/n/npython/ray/ray_constants.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function
""""""Ray constants used in the Python code.""""""

import os


def env_integer(key, default):
    if key in os.environ:
        return int(os.environ[key])
    return default


ID_SIZE = 20

# The default maximum number of bytes to allocate to the object store unless
# overridden by the user.
DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES = 20 * 10**9
# The smallest cap on the memory used by the object store that we allow.
OBJECT_STORE_MINIMUM_MEMORY_BYTES = 10**7
# The default maximum number of bytes that the non-primary Redis shards are
# allowed to use unless overridden by the user.
DEFAULT_REDIS_MAX_MEMORY_BYTES = 10**10
# The smallest cap on the memory used by Redis that we allow.
REDIS_MINIMUM_MEMORY_BYTES = 10**7

# If a remote function or actor (or some other export) has serialized size
# greater than this quantity, print an warning.
PICKLE_OBJECT_WARNING_SIZE = 10**7

# The maximum resource quantity that is allowed. TODO(rkn): This could be
# relaxed, but the current implementation of the node manager will be slower
# for large resource quantities due to bookkeeping of specific resource IDs.
MAX_RESOURCE_QUANTITY = 512

# Different types of Ray errors that can be pushed to the driver.
# TODO(rkn): These should be defined in flatbuffers and must be synced with
# the existing C++ definitions.
WAIT_FOR_CLASS_PUSH_ERROR = ""wait_for_class""
PICKLING_LARGE_OBJECT_PUSH_ERROR = ""pickling_large_object""
WAIT_FOR_FUNCTION_PUSH_ERROR = ""wait_for_function""
TASK_PUSH_ERROR = ""task""
REGISTER_REMOTE_FUNCTION_PUSH_ERROR = ""register_remote_function""
FUNCTION_TO_RUN_PUSH_ERROR = ""function_to_run""
VERSION_MISMATCH_PUSH_ERROR = ""version_mismatch""
CHECKPOINT_PUSH_ERROR = ""checkpoint""
REGISTER_ACTOR_PUSH_ERROR = ""register_actor""
WORKER_CRASH_PUSH_ERROR = ""worker_crash""
WORKER_DIED_PUSH_ERROR = ""worker_died""
WORKER_POOL_LARGE_ERROR = ""worker_pool_large""
PUT_RECONSTRUCTION_PUSH_ERROR = ""put_reconstruction""
INFEASIBLE_TASK_ERROR = ""infeasible_task""
REMOVED_NODE_ERROR = ""node_removed""
MONITOR_DIED_ERROR = ""monitor_died""
LOG_MONITOR_DIED_ERROR = ""log_monitor_died""

# Abort autoscaling if more than this number of errors are encountered. This
# is a safety feature to prevent e.g. runaway node launches.
AUTOSCALER_MAX_NUM_FAILURES = env_integer(""AUTOSCALER_MAX_NUM_FAILURES"", 5)

# The maximum number of nodes to launch in a single request.
# Multiple requests may be made for this batch size, up to
# the limit of AUTOSCALER_MAX_CONCURRENT_LAUNCHES.
AUTOSCALER_MAX_LAUNCH_BATCH = env_integer(""AUTOSCALER_MAX_LAUNCH_BATCH"", 5)

# Max number of nodes to launch at a time.
AUTOSCALER_MAX_CONCURRENT_LAUNCHES = env_integer(
    ""AUTOSCALER_MAX_CONCURRENT_LAUNCHES"", 10)

# Interval at which to perform autoscaling updates.
AUTOSCALER_UPDATE_INTERVAL_S = env_integer(""AUTOSCALER_UPDATE_INTERVAL_S"", 5)

# The autoscaler will attempt to restart Ray on nodes it hasn't heard from
# in more than this interval.
AUTOSCALER_HEARTBEAT_TIMEOUT_S = env_integer(""AUTOSCALER_HEARTBEAT_TIMEOUT_S"",
                                             30)

# Max number of retries to AWS (default is 5, time increases exponentially)
BOTO_MAX_RETRIES = env_integer(""BOTO_MAX_RETRIES"", 12)

LOGGER_FORMAT = (
    ""%(asctime)s\t%(levelname)s %(filename)s:%(lineno)s -- %(message)s"")
LOGGER_FORMAT_HELP = ""The logging format. default='{}'"".format(LOGGER_FORMAT)
LOGGER_LEVEL = ""info""
LOGGER_LEVEL_CHOICES = ['debug', 'info', 'warning', 'error', 'critical']
LOGGER_LEVEL_HELP = (""The logging level threshold, choices=['debug', 'info',""
                     "" 'warning', 'error', 'critical'], default='info'"")

# A constant indicating that an actor doesn't need reconstructions.
NO_RECONSTRUCTION = 0
# A constant indicating that an actor should be reconstructed infinite times.
INFINITE_RECONSTRUCTION = 2**30

# Constants used to define the different process types.
PROCESS_TYPE_MONITOR = ""monitor""
PROCESS_TYPE_RAYLET_MONITOR = ""raylet_monitor""
PROCESS_TYPE_LOG_MONITOR = ""log_monitor""
PROCESS_TYPE_WORKER = ""worker""
PROCESS_TYPE_RAYLET = ""raylet""
PROCESS_TYPE_PLASMA_STORE = ""plasma_store""
PROCESS_TYPE_REDIS_SERVER = ""redis_server""
PROCESS_TYPE_WEB_UI = ""web_ui""

LOG_MONITOR_MAX_OPEN_FILES = 200
/n/n/npython/ray/services.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import collections
import json
import logging
import multiprocessing
import os
import random
import resource
import socket
import subprocess
import sys
import time
import redis

import pyarrow
# Ray modules
import ray
import ray.ray_constants as ray_constants

from ray.tempfile_services import (
    get_gdb_init_path,
    get_ipython_notebook_path,
    get_logs_dir_path,
    get_temp_root,
    new_redis_log_file,
)

# True if processes are run in the valgrind profiler.
RUN_RAYLET_PROFILER = False
RUN_PLASMA_STORE_PROFILER = False

# Location of the redis server and module.
RAY_HOME = os.path.join(os.path.dirname(__file__), ""../.."")
REDIS_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/thirdparty/redis/src/redis-server"")
REDIS_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/gcs/redis_module/libray_redis_module.so"")

# Location of the credis server and modules.
# credis will be enabled if the environment variable RAY_USE_NEW_GCS is set.
CREDIS_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/redis/src/redis-server"")
CREDIS_MASTER_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/build/src/libmaster.so"")
CREDIS_MEMBER_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/build/src/libmember.so"")

# Location of the plasma object store executable.
PLASMA_STORE_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/plasma/plasma_store_server"")

# Location of the raylet executables.
RAYLET_MONITOR_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/raylet/raylet_monitor"")
RAYLET_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)), ""core/src/ray/raylet/raylet"")

DEFAULT_JAVA_WORKER_OPTIONS = ""-classpath {}"".format(
    os.path.join(
        os.path.abspath(os.path.dirname(__file__)), ""../../../build/java/*""))

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)

ProcessInfo = collections.namedtuple(""ProcessInfo"", [
    ""process"", ""stdout_file"", ""stderr_file"", ""use_valgrind"", ""use_gdb"",
    ""use_valgrind_profiler"", ""use_perftools_profiler"", ""use_tmux""
])


def address(ip_address, port):
    return ip_address + "":"" + str(port)


def get_ip_address(address):
    assert type(address) == str, ""Address must be a string""
    ip_address = address.split("":"")[0]
    return ip_address


def get_port(address):
    try:
        port = int(address.split("":"")[1])
    except Exception:
        raise Exception(""Unable to parse port from address {}"".format(address))
    return port


def new_port():
    return random.randint(10000, 65535)


def include_java_from_redis(redis_client):
    """"""This is used for query include_java bool from redis.

    Args:
        redis_client (StrictRedis): The redis client to GCS.

    Returns:
        True if this cluster backend enables Java worker.
    """"""
    return redis_client.get(""INCLUDE_JAVA"") == b""1""


def remaining_processes_alive():
    """"""See if the remaining processes are alive or not.

    Note that this ignores processes that have been explicitly killed,
    e.g., via a command like node.kill_raylet().

    Returns:
        True if the remaining processes started by ray.init() are alive and
            False otherwise.

    Raises:
        Exception: An exception is raised if the processes were not started by
            ray.init().
    """"""
    if ray.worker._global_node is None:
        raise Exception(""This process is not in a position to determine ""
                        ""whether all processes are alive or not."")
    return ray.worker._global_node.remaining_processes_alive()


def address_to_ip(address):
    """"""Convert a hostname to a numerical IP addresses in an address.

    This should be a no-op if address already contains an actual numerical IP
    address.

    Args:
        address: This can be either a string containing a hostname (or an IP
            address) and a port or it can be just an IP address.

    Returns:
        The same address but with the hostname replaced by a numerical IP
            address.
    """"""
    address_parts = address.split("":"")
    ip_address = socket.gethostbyname(address_parts[0])
    # Make sure localhost isn't resolved to the loopback ip
    if ip_address == ""127.0.0.1"":
        ip_address = get_node_ip_address()
    return "":"".join([ip_address] + address_parts[1:])


def get_node_ip_address(address=""8.8.8.8:53""):
    """"""Determine the IP address of the local node.

    Args:
        address (str): The IP address and port of any known live service on the
            network you care about.

    Returns:
        The IP address of the current node.
    """"""
    ip_address, port = address.split("":"")
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        # This command will raise an exception if there is no internet
        # connection.
        s.connect((ip_address, int(port)))
        node_ip_address = s.getsockname()[0]
    except Exception as e:
        node_ip_address = ""127.0.0.1""
        # [Errno 101] Network is unreachable
        if e.errno == 101:
            try:
                # try get node ip address from host name
                host_name = socket.getfqdn(socket.gethostname())
                node_ip_address = socket.gethostbyname(host_name)
            except Exception:
                pass

    return node_ip_address


def create_redis_client(redis_address, password=None):
    """"""Create a Redis client.

    Args:
        The IP address, port, and password of the Redis server.

    Returns:
        A Redis client.
    """"""
    redis_ip_address, redis_port = redis_address.split("":"")
    # For this command to work, some other client (on the same machine
    # as Redis) must have run ""CONFIG SET protected-mode no"".
    return redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=password)


def start_ray_process(command,
                      process_type,
                      env_updates=None,
                      cwd=None,
                      use_valgrind=False,
                      use_gdb=False,
                      use_valgrind_profiler=False,
                      use_perftools_profiler=False,
                      use_tmux=False,
                      stdout_file=None,
                      stderr_file=None):
    """"""Start one of the Ray processes.

    TODO(rkn): We need to figure out how these commands interact. For example,
    it may only make sense to start a process in gdb if we also start it in
    tmux. Similarly, certain combinations probably don't make sense, like
    simultaneously running the process in valgrind and the profiler.

    Args:
        command (List[str]): The command to use to start the Ray process.
        process_type (str): The type of the process that is being started
            (e.g., ""raylet"").
        env_updates (dict): A dictionary of additional environment variables to
            run the command with (in addition to the caller's environment
            variables).
        cwd (str): The directory to run the process in.
        use_valgrind (bool): True if we should start the process in valgrind.
        use_gdb (bool): True if we should start the process in gdb.
        use_valgrind_profiler (bool): True if we should start the process in
            the valgrind profiler.
        use_perftools_profiler (bool): True if we should profile the process
            using perftools.
        use_tmux (bool): True if we should start the process in tmux.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        Information about the process that was started including a handle to
            the process that was started.
    """"""
    # Detect which flags are set through environment variables.
    valgrind_env_var = ""RAY_{}_VALGRIND"".format(process_type.upper())
    if os.environ.get(valgrind_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", valgrind_env_var)
        use_valgrind = True
    valgrind_profiler_env_var = ""RAY_{}_VALGRIND_PROFILER"".format(
        process_type.upper())
    if os.environ.get(valgrind_profiler_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."",
                    valgrind_profiler_env_var)
        use_valgrind_profiler = True
    perftools_profiler_env_var = ""RAY_{}_PERFTOOLS_PROFILER"".format(
        process_type.upper())
    if os.environ.get(perftools_profiler_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."",
                    perftools_profiler_env_var)
        use_perftools_profiler = True
    tmux_env_var = ""RAY_{}_TMUX"".format(process_type.upper())
    if os.environ.get(tmux_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", tmux_env_var)
        use_tmux = True
    gdb_env_var = ""RAY_{}_GDB"".format(process_type.upper())
    if os.environ.get(gdb_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", gdb_env_var)
        use_gdb = True

    if sum(
        [use_gdb, use_valgrind, use_valgrind_profiler, use_perftools_profiler
         ]) > 1:
        raise ValueError(
            ""At most one of the 'use_gdb', 'use_valgrind', ""
            ""'use_valgrind_profiler', and 'use_perftools_profiler' flags can ""
            ""be used at a time."")
    if env_updates is None:
        env_updates = {}
    if not isinstance(env_updates, dict):
        raise ValueError(""The 'env_updates' argument must be a dictionary."")

    modified_env = os.environ.copy()
    modified_env.update(env_updates)

    if use_gdb:
        if not use_tmux:
            raise ValueError(
                ""If 'use_gdb' is true, then 'use_tmux' must be true as well."")
        gdb_init_path = get_gdb_init_path(process_type)
        ray_process_path = command[0]
        ray_process_args = command[1:]
        run_args = "" "".join([""'{}'"".format(arg) for arg in ray_process_args])
        with open(gdb_init_path, ""w"") as gdb_init_file:
            gdb_init_file.write(""run {}"".format(run_args))
        command = [""gdb"", ray_process_path, ""-x"", gdb_init_path]

    if use_valgrind:
        command = [
            ""valgrind"", ""--track-origins=yes"", ""--leak-check=full"",
            ""--show-leak-kinds=all"", ""--leak-check-heuristics=stdstring"",
            ""--error-exitcode=1""
        ] + command

    if use_valgrind_profiler:
        command = [""valgrind"", ""--tool=callgrind""] + command

    if use_perftools_profiler:
        modified_env[""LD_PRELOAD""] = os.environ[""PERFTOOLS_PATH""]
        modified_env[""CPUPROFILE""] = os.environ[""PERFTOOLS_LOGFILE""]

    if use_tmux:
        # The command has to be created exactly as below to ensure that it
        # works on all versions of tmux. (Tested with tmux 1.8-5, travis'
        # version, and tmux 2.1)
        command = [""tmux"", ""new-session"", ""-d"", ""{}"".format("" "".join(command))]

    process = subprocess.Popen(
        command,
        env=modified_env,
        cwd=cwd,
        stdout=stdout_file,
        stderr=stderr_file)

    return ProcessInfo(
        process=process,
        stdout_file=stdout_file.name if stdout_file is not None else None,
        stderr_file=stderr_file.name if stderr_file is not None else None,
        use_valgrind=use_valgrind,
        use_gdb=use_gdb,
        use_valgrind_profiler=use_valgrind_profiler,
        use_perftools_profiler=use_perftools_profiler,
        use_tmux=use_tmux)


def wait_for_redis_to_start(redis_ip_address,
                            redis_port,
                            password=None,
                            num_retries=5):
    """"""Wait for a Redis server to be available.

    This is accomplished by creating a Redis client and sending a random
    command to the server until the command gets through.

    Args:
        redis_ip_address (str): The IP address of the redis server.
        redis_port (int): The port of the redis server.
        password (str): The password of the redis server.
        num_retries (int): The number of times to try connecting with redis.
            The client will sleep for one second between attempts.

    Raises:
        Exception: An exception is raised if we could not connect with Redis.
    """"""
    redis_client = redis.StrictRedis(
        host=redis_ip_address, port=redis_port, password=password)
    # Wait for the Redis server to start.
    counter = 0
    while counter < num_retries:
        try:
            # Run some random command and see if it worked.
            logger.info(
                ""Waiting for redis server at {}:{} to respond..."".format(
                    redis_ip_address, redis_port))
            redis_client.client_list()
        except redis.ConnectionError:
            # Wait a little bit.
            time.sleep(1)
            logger.info(""Failed to connect to the redis server, retrying."")
            counter += 1
        else:
            break
    if counter == num_retries:
        raise Exception(""Unable to connect to Redis. If the Redis instance is ""
                        ""on a different machine, check that your firewall is ""
                        ""configured properly."")


def _autodetect_num_gpus():
    """"""Attempt to detect the number of GPUs on this machine.

    TODO(rkn): This currently assumes Nvidia GPUs and Linux.

    Returns:
        The number of GPUs if any were detected, otherwise 0.
    """"""
    proc_gpus_path = ""/proc/driver/nvidia/gpus""
    if os.path.isdir(proc_gpus_path):
        return len(os.listdir(proc_gpus_path))
    return 0


def _compute_version_info():
    """"""Compute the versions of Python, pyarrow, and Ray.

    Returns:
        A tuple containing the version information.
    """"""
    ray_version = ray.__version__
    python_version = ""."".join(map(str, sys.version_info[:3]))
    pyarrow_version = pyarrow.__version__
    return ray_version, python_version, pyarrow_version


def _put_version_info_in_redis(redis_client):
    """"""Store version information in Redis.

    This will be used to detect if workers or drivers are started using
    different versions of Python, pyarrow, or Ray.

    Args:
        redis_client: A client for the primary Redis shard.
    """"""
    redis_client.set(""VERSION_INFO"", json.dumps(_compute_version_info()))


def check_version_info(redis_client):
    """"""Check if various version info of this process is correct.

    This will be used to detect if workers or drivers are started using
    different versions of Python, pyarrow, or Ray. If the version
    information is not present in Redis, then no check is done.

    Args:
        redis_client: A client for the primary Redis shard.

    Raises:
        Exception: An exception is raised if there is a version mismatch.
    """"""
    redis_reply = redis_client.get(""VERSION_INFO"")

    # Don't do the check if there is no version information in Redis. This
    # is to make it easier to do things like start the processes by hand.
    if redis_reply is None:
        return

    true_version_info = tuple(json.loads(ray.utils.decode(redis_reply)))
    version_info = _compute_version_info()
    if version_info != true_version_info:
        node_ip_address = ray.services.get_node_ip_address()
        error_message = (""Version mismatch: The cluster was started with:\n""
                         ""    Ray: "" + true_version_info[0] + ""\n""
                         ""    Python: "" + true_version_info[1] + ""\n""
                         ""    Pyarrow: "" + str(true_version_info[2]) + ""\n""
                         ""This process on node "" + node_ip_address +
                         "" was started with:"" + ""\n""
                         ""    Ray: "" + version_info[0] + ""\n""
                         ""    Python: "" + version_info[1] + ""\n""
                         ""    Pyarrow: "" + str(version_info[2]))
        if version_info[:2] != true_version_info[:2]:
            raise Exception(error_message)
        else:
            logger.warning(error_message)


def start_redis(node_ip_address,
                port=None,
                redis_shard_ports=None,
                num_redis_shards=1,
                redis_max_clients=None,
                redirect_output=False,
                redirect_worker_output=False,
                password=None,
                use_credis=None,
                redis_max_memory=None,
                include_java=False):
    """"""Start the Redis global state store.

    Args:
        node_ip_address: The IP address of the current node. This is only used
            for recording the log filenames in Redis.
        port (int): If provided, the primary Redis shard will be started on
            this port.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_redis_shards (int): If provided, the number of Redis shards to
            start, in addition to the primary one. The default value is one
            shard.
        redis_max_clients: If this is provided, Ray will attempt to configure
            Redis with this maxclients number.
        redirect_output (bool): True if output should be redirected to a file
            and false otherwise.
        redirect_worker_output (bool): True if worker output should be
            redirected to a file and false otherwise. Workers will have access
            to this value when they start up.
        password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        use_credis: If True, additionally load the chain-replicated libraries
            into the redis servers.  Defaults to None, which means its value is
            set by the presence of ""RAY_USE_NEW_GCS"" in os.environ.
        redis_max_memory: The max amount of memory (in bytes) to allow each
            redis shard to use. Once the limit is exceeded, redis will start
            LRU eviction of entries. This only applies to the sharded redis
            tables (task, object, and profile tables). By default, this is
            capped at 10GB but can be set higher.
        include_java (bool): If True, the raylet backend can also support
            Java worker.

    Returns:
        A tuple of the address for the primary Redis shard, a list of
            addresses for the remaining shards, and the processes that were
            started.
    """"""
    redis_stdout_file, redis_stderr_file = new_redis_log_file(redirect_output)

    if redis_shard_ports is None:
        redis_shard_ports = num_redis_shards * [None]
    elif len(redis_shard_ports) != num_redis_shards:
        raise Exception(""The number of Redis shard ports does not match the ""
                        ""number of Redis shards."")

    processes = []

    if use_credis is None:
        use_credis = (""RAY_USE_NEW_GCS"" in os.environ)
    if use_credis:
        if password is not None:
            # TODO(pschafhalter) remove this once credis supports
            # authenticating Redis ports
            raise Exception(""Setting the `redis_password` argument is not ""
                            ""supported in credis. To run Ray with ""
                            ""password-protected Redis ports, ensure that ""
                            ""the environment variable `RAY_USE_NEW_GCS=off`."")
        assert num_redis_shards == 1, (
            ""For now, RAY_USE_NEW_GCS supports 1 shard, and credis ""
            ""supports 1-node chain for that shard only."")

    if use_credis:
        redis_executable = CREDIS_EXECUTABLE
        # TODO(suquark): We need credis here because some symbols need to be
        # imported from credis dynamically through dlopen when Ray is built
        # with RAY_USE_NEW_GCS=on. We should remove them later for the primary
        # shard.
        # See src/ray/gcs/redis_module/ray_redis_module.cc
        redis_modules = [CREDIS_MASTER_MODULE, REDIS_MODULE]
    else:
        redis_executable = REDIS_EXECUTABLE
        redis_modules = [REDIS_MODULE]

    # Start the primary Redis shard.
    port, p = _start_redis_instance(
        redis_executable,
        modules=redis_modules,
        port=port,
        password=password,
        redis_max_clients=redis_max_clients,
        # Below we use None to indicate no limit on the memory of the
        # primary Redis shard.
        redis_max_memory=None,
        stdout_file=redis_stdout_file,
        stderr_file=redis_stderr_file)
    processes.append(p)
    redis_address = address(node_ip_address, port)

    # Register the number of Redis shards in the primary shard, so that clients
    # know how many redis shards to expect under RedisShards.
    primary_redis_client = redis.StrictRedis(
        host=node_ip_address, port=port, password=password)
    primary_redis_client.set(""NumRedisShards"", str(num_redis_shards))

    # Put the redirect_worker_output bool in the Redis shard so that workers
    # can access it and know whether or not to redirect their output.
    primary_redis_client.set(""RedirectOutput"", 1
                             if redirect_worker_output else 0)

    # put the include_java bool to primary redis-server, so that other nodes
    # can access it and know whether or not to enable cross-languages.
    primary_redis_client.set(""INCLUDE_JAVA"", 1 if include_java else 0)

    # Store version information in the primary Redis shard.
    _put_version_info_in_redis(primary_redis_client)

    # Cap the memory of the other redis shards if no limit is provided.
    redis_max_memory = (redis_max_memory if redis_max_memory is not None else
                        ray_constants.DEFAULT_REDIS_MAX_MEMORY_BYTES)
    if redis_max_memory < ray_constants.REDIS_MINIMUM_MEMORY_BYTES:
        raise ValueError(""Attempting to cap Redis memory usage at {} bytes, ""
                         ""but the minimum allowed is {} bytes."".format(
                             redis_max_memory,
                             ray_constants.REDIS_MINIMUM_MEMORY_BYTES))

    # Start other Redis shards. Each Redis shard logs to a separate file,
    # prefixed by ""redis-<shard number>"".
    redis_shards = []
    for i in range(num_redis_shards):
        redis_stdout_file, redis_stderr_file = new_redis_log_file(
            redirect_output, shard_number=i)

        if use_credis:
            redis_executable = CREDIS_EXECUTABLE
            # It is important to load the credis module BEFORE the ray module,
            # as the latter contains an extern declaration that the former
            # supplies.
            redis_modules = [CREDIS_MEMBER_MODULE, REDIS_MODULE]
        else:
            redis_executable = REDIS_EXECUTABLE
            redis_modules = [REDIS_MODULE]

        redis_shard_port, p = _start_redis_instance(
            redis_executable,
            modules=redis_modules,
            port=redis_shard_ports[i],
            password=password,
            redis_max_clients=redis_max_clients,
            redis_max_memory=redis_max_memory,
            stdout_file=redis_stdout_file,
            stderr_file=redis_stderr_file)
        processes.append(p)

        shard_address = address(node_ip_address, redis_shard_port)
        redis_shards.append(shard_address)
        # Store redis shard information in the primary redis shard.
        primary_redis_client.rpush(""RedisShards"", shard_address)

    if use_credis:
        # Configure the chain state. The way it is intended to work is
        # the following:
        #
        # PRIMARY_SHARD
        #
        # SHARD_1 (master replica) -> SHARD_1 (member replica)
        #                                        -> SHARD_1 (member replica)
        #
        # SHARD_2 (master replica) -> SHARD_2 (member replica)
        #                                        -> SHARD_2 (member replica)
        # ...
        #
        #
        # If we have credis members in future, their modules should be:
        # [CREDIS_MEMBER_MODULE, REDIS_MODULE], and they will be initialized by
        # execute_command(""MEMBER.CONNECT_TO_MASTER"", node_ip_address, port)
        #
        # Currently we have num_redis_shards == 1, so only one chain will be
        # created, and the chain only contains master.

        # TODO(suquark): Currently, this is not correct because we are
        # using the master replica as the primary shard. This should be
        # fixed later. I had tried to fix it but failed because of heartbeat
        # issues.
        primary_client = redis.StrictRedis(
            host=node_ip_address, port=port, password=password)
        shard_client = redis.StrictRedis(
            host=node_ip_address, port=redis_shard_port, password=password)
        primary_client.execute_command(""MASTER.ADD"", node_ip_address,
                                       redis_shard_port)
        shard_client.execute_command(""MEMBER.CONNECT_TO_MASTER"",
                                     node_ip_address, port)

    return redis_address, redis_shards, processes


def _start_redis_instance(executable,
                          modules,
                          port=None,
                          redis_max_clients=None,
                          num_retries=20,
                          stdout_file=None,
                          stderr_file=None,
                          password=None,
                          redis_max_memory=None):
    """"""Start a single Redis server.

    Notes:
        If ""port"" is not None, then we will only use this port and try
        only once. Otherwise, random ports will be used and the maximum
        retries count is ""num_retries"".

    Args:
        executable (str): Full path of the redis-server executable.
        modules (list of str): A list of pathnames, pointing to the redis
            module(s) that will be loaded in this redis server.
        port (int): If provided, start a Redis server with this port.
        redis_max_clients: If this is provided, Ray will attempt to configure
            Redis with this maxclients number.
        num_retries (int): The number of times to attempt to start Redis. If a
            port is provided, this defaults to 1.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries.

    Returns:
        A tuple of the port used by Redis and ProcessInfo for the process that
            was started. If a port is passed in, then the returned port value
            is the same.

    Raises:
        Exception: An exception is raised if Redis could not be started.
    """"""
    assert os.path.isfile(executable)
    for module in modules:
        assert os.path.isfile(module)
    counter = 0
    if port is not None:
        # If a port is specified, then try only once to connect.
        # This ensures that we will use the given port.
        num_retries = 1
    else:
        port = new_port()

    load_module_args = []
    for module in modules:
        load_module_args += [""--loadmodule"", module]

    while counter < num_retries:
        if counter > 0:
            logger.warning(""Redis failed to start, retrying now."")

        # Construct the command to start the Redis server.
        command = [executable]
        if password:
            command += [""--requirepass"", password]
        command += (
            [""--port"", str(port), ""--loglevel"", ""warning""] + load_module_args)
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_REDIS_SERVER,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        time.sleep(0.1)
        # Check if Redis successfully started (or at least if it the executable
        # did not exit within 0.1 seconds).
        if process_info.process.poll() is None:
            break
        port = new_port()
        counter += 1
    if counter == num_retries:
        raise Exception(""Couldn't start Redis. Check log files: {} {}"".format(
            stdout_file.name, stderr_file.name))

    # Create a Redis client just for configuring Redis.
    redis_client = redis.StrictRedis(
        host=""127.0.0.1"", port=port, password=password)
    # Wait for the Redis server to start.
    wait_for_redis_to_start(""127.0.0.1"", port, password=password)
    # Configure Redis to generate keyspace notifications. TODO(rkn): Change
    # this to only generate notifications for the export keys.
    redis_client.config_set(""notify-keyspace-events"", ""Kl"")

    # Configure Redis to not run in protected mode so that processes on other
    # hosts can connect to it. TODO(rkn): Do this in a more secure way.
    redis_client.config_set(""protected-mode"", ""no"")

    # Discard old task and object metadata.
    if redis_max_memory is not None:
        redis_client.config_set(""maxmemory"", str(redis_max_memory))
        redis_client.config_set(""maxmemory-policy"", ""allkeys-lru"")
        redis_client.config_set(""maxmemory-samples"", ""10"")
        logger.info(""Starting Redis shard with {} GB max memory."".format(
            round(redis_max_memory / 1e9, 2)))

    # If redis_max_clients is provided, attempt to raise the number of maximum
    # number of Redis clients.
    if redis_max_clients is not None:
        redis_client.config_set(""maxclients"", str(redis_max_clients))
    else:
        # If redis_max_clients is not provided, determine the current ulimit.
        # We will use this to attempt to raise the maximum number of Redis
        # clients.
        current_max_clients = int(
            redis_client.config_get(""maxclients"")[""maxclients""])
        # The below command should be the same as doing ulimit -n.
        ulimit_n = resource.getrlimit(resource.RLIMIT_NOFILE)[0]
        # The quantity redis_client_buffer appears to be the required buffer
        # between the maximum number of redis clients and ulimit -n. That is,
        # if ulimit -n returns 10000, then we can set maxclients to
        # 10000 - redis_client_buffer.
        redis_client_buffer = 32
        if current_max_clients < ulimit_n - redis_client_buffer:
            redis_client.config_set(""maxclients"",
                                    ulimit_n - redis_client_buffer)

    # Increase the hard and soft limits for the redis client pubsub buffer to
    # 128MB. This is a hack to make it less likely for pubsub messages to be
    # dropped and for pubsub connections to therefore be killed.
    cur_config = (redis_client.config_get(""client-output-buffer-limit"")[
        ""client-output-buffer-limit""])
    cur_config_list = cur_config.split()
    assert len(cur_config_list) == 12
    cur_config_list[8:] = [""pubsub"", ""134217728"", ""134217728"", ""60""]
    redis_client.config_set(""client-output-buffer-limit"",
                            "" "".join(cur_config_list))
    # Put a time stamp in Redis to indicate when it was started.
    redis_client.set(""redis_start_time"", time.time())
    return port, process_info


def start_log_monitor(redis_address,
                      stdout_file=None,
                      stderr_file=None,
                      redis_password=None):
    """"""Start a log monitor process.

    Args:
        redis_address (str): The address of the Redis instance.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    log_monitor_filepath = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""log_monitor.py"")
    command = [
        sys.executable, ""-u"", log_monitor_filepath,
        ""--redis-address={}"".format(redis_address), ""--logs-dir={}"".format(
            get_logs_dir_path())
    ]
    if redis_password:
        command += [""--redis-password"", redis_password]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_LOG_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_ui(redis_address, stdout_file=None, stderr_file=None):
    """"""Start a UI process.

    Args:
        redis_address: The address of the primary Redis shard.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        A tuple of the web UI url and ProcessInfo for the process that was
            started.
    """"""

    port = 8888
    while True:
        try:
            port_test_socket = socket.socket()
            port_test_socket.bind((""127.0.0.1"", port))
            port_test_socket.close()
            break
        except socket.error:
            port += 1

    notebook_name = get_ipython_notebook_path()
    new_notebook_directory = os.path.dirname(notebook_name)
    # We generate the token used for authentication ourselves to avoid
    # querying the jupyter server.
    token = ray.utils.decode(binascii.hexlify(os.urandom(24)))
    # The --ip=0.0.0.0 flag is intended to enable connecting to a notebook
    # running within a docker container (from the outside).
    command = [
        ""jupyter"", ""notebook"", ""--no-browser"", ""--port={}"".format(port),
        ""--ip=0.0.0.0"", ""--NotebookApp.iopub_data_rate_limit=10000000000"",
        ""--NotebookApp.open_browser=False"",
        ""--NotebookApp.token={}"".format(token)
    ]
    # If the user is root, add the --allow-root flag.
    if os.geteuid() == 0:
        command.append(""--allow-root"")

    try:
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_WEB_UI,
            env_updates={""REDIS_ADDRESS"": redis_address},
            cwd=new_notebook_directory,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
    except Exception:
        logger.warning(""Failed to start the UI, you may need to run ""
                       ""'pip install jupyter'."")
    else:
        webui_url = (""http://localhost:{}/notebooks/{}?token={}"".format(
            port, os.path.basename(notebook_name), token))
        print(""\n"" + ""="" * 70)
        print(""View the web UI at {}"".format(webui_url))
        print(""="" * 70 + ""\n"")
        return webui_url, process_info
    return None, None


def check_and_update_resources(num_cpus, num_gpus, resources):
    """"""Sanity check a resource dictionary and add sensible defaults.

    Args:
        num_cpus: The number of CPUs.
        num_gpus: The number of GPUs.
        resources: A dictionary mapping resource names to resource quantities.

    Returns:
        A new resource dictionary.
    """"""
    if resources is None:
        resources = {}
    resources = resources.copy()
    assert ""CPU"" not in resources
    assert ""GPU"" not in resources
    if num_cpus is not None:
        resources[""CPU""] = num_cpus
    if num_gpus is not None:
        resources[""GPU""] = num_gpus

    if ""CPU"" not in resources:
        # By default, use the number of hardware execution threads for the
        # number of cores.
        resources[""CPU""] = multiprocessing.cpu_count()

    # See if CUDA_VISIBLE_DEVICES has already been set.
    gpu_ids = ray.utils.get_cuda_visible_devices()

    # Check that the number of GPUs that the local scheduler wants doesn't
    # excede the amount allowed by CUDA_VISIBLE_DEVICES.
    if (""GPU"" in resources and gpu_ids is not None
            and resources[""GPU""] > len(gpu_ids)):
        raise Exception(""Attempting to start local scheduler with {} GPUs, ""
                        ""but CUDA_VISIBLE_DEVICES contains {}."".format(
                            resources[""GPU""], gpu_ids))

    if ""GPU"" not in resources:
        # Try to automatically detect the number of GPUs.
        resources[""GPU""] = _autodetect_num_gpus()
        # Don't use more GPUs than allowed by CUDA_VISIBLE_DEVICES.
        if gpu_ids is not None:
            resources[""GPU""] = min(resources[""GPU""], len(gpu_ids))

    # Check types.
    for _, resource_quantity in resources.items():
        assert (isinstance(resource_quantity, int)
                or isinstance(resource_quantity, float))
        if (isinstance(resource_quantity, float)
                and not resource_quantity.is_integer()):
            raise ValueError(""Resource quantities must all be whole numbers."")

        if resource_quantity > ray_constants.MAX_RESOURCE_QUANTITY:
            raise ValueError(""Resource quantities must be at most {}."".format(
                ray_constants.MAX_RESOURCE_QUANTITY))

    return resources


def start_raylet(redis_address,
                 node_ip_address,
                 raylet_name,
                 plasma_store_name,
                 worker_path,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 redis_password=None,
                 use_valgrind=False,
                 use_profiler=False,
                 stdout_file=None,
                 stderr_file=None,
                 config=None,
                 include_java=False,
                 java_worker_options=None):
    """"""Start a raylet, which is a combined local scheduler and object manager.

    Args:
        redis_address (str): The address of the primary Redis server.
        node_ip_address (str): The IP address of this node.
        raylet_name (str): The name of the raylet socket to create.
        plasma_store_name (str): The name of the plasma store socket to connect
             to.
        worker_path (str): The path of the Python file that new worker
            processes will execute.
        num_cpus: The CPUs allocated for this raylet.
        num_gpus: The GPUs allocated for this raylet.
        resources: The custom resources allocated for this raylet.
        object_manager_port: The port to use for the object manager. If this is
            None, then the object manager will choose its own port.
        node_manager_port: The port to use for the node manager. If this is
            None, then the node manager will choose its own port.
        redis_password: The password to use when connecting to Redis.
        use_valgrind (bool): True if the raylet should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the raylet should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        config (dict|None): Optional Raylet configuration that will
            override defaults in RayConfig.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
    Returns:
        ProcessInfo for the process that was started.
    """"""
    config = config or {}
    config_str = "","".join([""{},{}"".format(*kv) for kv in config.items()])

    if use_valgrind and use_profiler:
        raise Exception(""Cannot use valgrind and profiler at the same time."")

    num_initial_workers = (num_cpus if num_cpus is not None else
                           multiprocessing.cpu_count())

    static_resources = check_and_update_resources(num_cpus, num_gpus,
                                                  resources)

    # Limit the number of workers that can be started in parallel by the
    # raylet. However, make sure it is at least 1.
    maximum_startup_concurrency = max(
        1, min(multiprocessing.cpu_count(), static_resources[""CPU""]))

    # Format the resource argument in a form like 'CPU,1.0,GPU,0,Custom,3'.
    resource_argument = "","".join(
        [""{},{}"".format(*kv) for kv in static_resources.items()])

    gcs_ip_address, gcs_port = redis_address.split("":"")

    if include_java is True:
        java_worker_options = (java_worker_options
                               or DEFAULT_JAVA_WORKER_OPTIONS)
        java_worker_command = build_java_worker_command(
            java_worker_options, redis_address, plasma_store_name, raylet_name)
    else:
        java_worker_command = """"

    # Create the command that the Raylet will use to start workers.
    start_worker_command = (""{} {} ""
                            ""--node-ip-address={} ""
                            ""--object-store-name={} ""
                            ""--raylet-name={} ""
                            ""--redis-address={} ""
                            ""--temp-dir={}"".format(
                                sys.executable, worker_path, node_ip_address,
                                plasma_store_name, raylet_name, redis_address,
                                get_temp_root()))
    if redis_password:
        start_worker_command += "" --redis-password {}"".format(redis_password)

    # If the object manager port is None, then use 0 to cause the object
    # manager to choose its own port.
    if object_manager_port is None:
        object_manager_port = 0
    # If the node manager port is None, then use 0 to cause the node manager
    # to choose its own port.
    if node_manager_port is None:
        node_manager_port = 0

    command = [
        RAYLET_EXECUTABLE,
        raylet_name,
        plasma_store_name,
        str(object_manager_port),
        str(node_manager_port),
        node_ip_address,
        gcs_ip_address,
        gcs_port,
        str(num_initial_workers),
        str(maximum_startup_concurrency),
        resource_argument,
        config_str,
        start_worker_command,
        java_worker_command,
        redis_password or """",
        get_temp_root(),
    ]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET,
        use_valgrind=use_valgrind,
        use_gdb=False,
        use_valgrind_profiler=use_profiler,
        use_perftools_profiler=(""RAYLET_PERFTOOLS_PATH"" in os.environ),
        stdout_file=stdout_file,
        stderr_file=stderr_file)

    return process_info


def build_java_worker_command(java_worker_options, redis_address,
                              plasma_store_name, raylet_name):
    """"""This method assembles the command used to start a Java worker.

    Args:
        java_worker_options (str): The command options for Java worker.
        redis_address (str): Redis address of GCS.
        plasma_store_name (str): The name of the plasma store socket to connect
           to.
        raylet_name (str): The name of the raylet socket to create.

    Returns:
        The command string for starting Java worker.
    """"""
    assert java_worker_options is not None

    command = ""java {} "".format(java_worker_options)
    if redis_address is not None:
        command += ""-Dray.redis.address={} "".format(redis_address)

    if plasma_store_name is not None:
        command += (
            ""-Dray.object-store.socket-name={} "".format(plasma_store_name))

    if raylet_name is not None:
        command += ""-Dray.raylet.socket-name={} "".format(raylet_name)

    command += ""-Dray.home={} "".format(RAY_HOME)
    command += ""-Dray.log-dir={} "".format(get_logs_dir_path())
    command += ""org.ray.runtime.runner.worker.DefaultWorker""

    return command


def determine_plasma_store_config(object_store_memory=None,
                                  plasma_directory=None,
                                  huge_pages=False):
    """"""Figure out how to configure the plasma object store.

    This will determine which directory to use for the plasma store (e.g.,
    /tmp or /dev/shm) and how much memory to start the store with. On Linux,
    we will try to use /dev/shm unless the shared memory file system is too
    small, in which case we will fall back to /tmp. If any of the object store
    memory or plasma directory parameters are specified by the user, then those
    values will be preserved.

    Args:
        object_store_memory (int): The user-specified object store memory
            parameter.
        plasma_directory (str): The user-specified plasma directory parameter.
        huge_pages (bool): The user-specified huge pages parameter.

    Returns:
        A tuple of the object store memory to use and the plasma directory to
            use. If either of these values is specified by the user, then that
            value will be preserved.
    """"""
    system_memory = ray.utils.get_system_memory()

    # Choose a default object store size.
    if object_store_memory is None:
        object_store_memory = int(system_memory * 0.4)
        # Cap memory to avoid memory waste and perf issues on large nodes
        if (object_store_memory >
                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES):
            logger.warning(
                ""Warning: Capping object memory store to {}GB. "".format(
                    ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES // 1e9)
                + ""To increase this further, specify `object_store_memory` ""
                ""when calling ray.init() or ray start."")
            object_store_memory = (
                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES)

    # Determine which directory to use. By default, use /tmp on MacOS and
    # /dev/shm on Linux, unless the shared-memory file system is too small,
    # in which case we default to /tmp on Linux.
    if plasma_directory is None:
        if sys.platform == ""linux"" or sys.platform == ""linux2"":
            shm_avail = ray.utils.get_shared_memory_bytes()
            # Compare the requested memory size to the memory available in
            # /dev/shm.
            if shm_avail > object_store_memory:
                plasma_directory = ""/dev/shm""
            else:
                plasma_directory = ""/tmp""
                logger.warning(
                    ""WARNING: The object store is using /tmp instead of ""
                    ""/dev/shm because /dev/shm has only {} bytes available. ""
                    ""This may slow down performance! You may be able to free ""
                    ""up space by deleting files in /dev/shm or terminating ""
                    ""any running plasma_store_server processes. If you are ""
                    ""inside a Docker container, you may need to pass an ""
                    ""argument with the flag '--shm-size' to 'docker run'."".
                    format(shm_avail))
        else:
            plasma_directory = ""/tmp""

        # Do some sanity checks.
        if object_store_memory > system_memory:
            raise Exception(
                ""The requested object store memory size is greater ""
                ""than the total available memory."")
    else:
        plasma_directory = os.path.abspath(plasma_directory)
        logger.warning(""WARNING: object_store_memory is not verified when ""
                       ""plasma_directory is set."")

    if not os.path.isdir(plasma_directory):
        raise Exception(
            ""The file {} does not exist or is not a directory."".format(
                plasma_directory))

    return object_store_memory, plasma_directory


def _start_plasma_store(plasma_store_memory,
                        use_valgrind=False,
                        use_profiler=False,
                        stdout_file=None,
                        stderr_file=None,
                        plasma_directory=None,
                        huge_pages=False,
                        socket_name=None):
    """"""Start a plasma store process.

    Args:
        plasma_store_memory (int): The amount of memory in bytes to start the
            plasma store with.
        use_valgrind (bool): True if the plasma store should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the plasma store should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: a boolean flag indicating whether to start the
            Object Store with hugetlbfs support. Requires plasma_directory.
        socket_name (str): If provided, it will specify the socket
            name used by the plasma store.

    Return:
        A tuple of the name of the plasma store socket and ProcessInfo for the
            plasma store process.
    """"""
    if use_valgrind and use_profiler:
        raise Exception(""Cannot use valgrind and profiler at the same time."")

    if huge_pages and not (sys.platform == ""linux""
                           or sys.platform == ""linux2""):
        raise Exception(""The huge_pages argument is only supported on ""
                        ""Linux."")

    if huge_pages and plasma_directory is None:
        raise Exception(""If huge_pages is True, then the ""
                        ""plasma_directory argument must be provided."")

    if not isinstance(plasma_store_memory, int):
        raise Exception(""plasma_store_memory should be an integer."")

    command = [
        PLASMA_STORE_EXECUTABLE, ""-s"", socket_name, ""-m"",
        str(plasma_store_memory)
    ]
    if plasma_directory is not None:
        command += [""-d"", plasma_directory]
    if huge_pages:
        command += [""-h""]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_PLASMA_STORE,
        use_valgrind=use_valgrind,
        use_valgrind_profiler=use_profiler,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_plasma_store(node_ip_address,
                       redis_address,
                       stdout_file=None,
                       stderr_file=None,
                       object_store_memory=None,
                       plasma_directory=None,
                       huge_pages=False,
                       plasma_store_socket_name=None):
    """"""This method starts an object store process.

    Args:
        node_ip_address (str): The IP address of the node running the object
            store.
        redis_address (str): The address of the Redis instance to connect to.
        stdout_file: A file handle opened for writing to redirect stdout
            to. If no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr
            to. If no redirection should happen, then this should be None.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    object_store_memory, plasma_directory = determine_plasma_store_config(
        object_store_memory, plasma_directory, huge_pages)

    if object_store_memory < ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES:
        raise ValueError(""Attempting to cap object store memory usage at {} ""
                         ""bytes, but the minimum allowed is {} bytes."".format(
                             object_store_memory,
                             ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES))

    # Print the object store memory using two decimal places.
    object_store_memory_str = (object_store_memory / 10**7) / 10**2
    logger.info(""Starting the Plasma object store with {} GB memory ""
                ""using {}."".format(object_store_memory_str, plasma_directory))
    # Start the Plasma store.
    process_info = _start_plasma_store(
        object_store_memory,
        use_profiler=RUN_PLASMA_STORE_PROFILER,
        stdout_file=stdout_file,
        stderr_file=stderr_file,
        plasma_directory=plasma_directory,
        huge_pages=huge_pages,
        socket_name=plasma_store_socket_name)

    return process_info


def start_worker(node_ip_address,
                 object_store_name,
                 raylet_name,
                 redis_address,
                 worker_path,
                 stdout_file=None,
                 stderr_file=None):
    """"""This method starts a worker process.

    Args:
        node_ip_address (str): The IP address of the node that this worker is
            running on.
        object_store_name (str): The socket name of the object store.
        raylet_name (str): The socket name of the raylet server.
        redis_address (str): The address that the Redis server is listening on.
        worker_path (str): The path of the source code which the worker process
            will run.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    command = [
        sys.executable, ""-u"", worker_path,
        ""--node-ip-address="" + node_ip_address,
        ""--object-store-name="" + object_store_name,
        ""--raylet-name="" + raylet_name,
        ""--redis-address="" + str(redis_address),
        ""--temp-dir="" + get_temp_root()
    ]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_WORKER,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_monitor(redis_address,
                  node_ip_address,
                  stdout_file=None,
                  stderr_file=None,
                  autoscaling_config=None,
                  redis_password=None):
    """"""Run a process to monitor the other processes.

    Args:
        redis_address (str): The address that the Redis server is listening on.
        node_ip_address: The IP address of the node that this process will run
            on.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        autoscaling_config: path to autoscaling config file.
        redis_password (str): The password of the redis server.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    monitor_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""monitor.py"")
    command = [
        sys.executable, ""-u"", monitor_path,
        ""--redis-address="" + str(redis_address)
    ]
    if autoscaling_config:
        command.append(""--autoscaling-config="" + str(autoscaling_config))
    if redis_password:
        command.append(""--redis-password="" + redis_password)
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_raylet_monitor(redis_address,
                         stdout_file=None,
                         stderr_file=None,
                         redis_password=None,
                         config=None):
    """"""Run a process to monitor the other processes.

    Args:
        redis_address (str): The address that the Redis server is listening on.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.
        config (dict|None): Optional configuration that will
            override defaults in RayConfig.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    gcs_ip_address, gcs_port = redis_address.split("":"")
    redis_password = redis_password or """"
    config = config or {}
    config_str = "","".join([""{},{}"".format(*kv) for kv in config.items()])
    command = [RAYLET_MONITOR_EXECUTABLE, gcs_ip_address, gcs_port, config_str]
    if redis_password:
        command += [redis_password]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info
/n/n/npython/ray/utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import functools
import hashlib
import inspect
import logging
import numpy as np
import os
import subprocess
import sys
import threading
import time
import uuid

import ray.gcs_utils
import ray.ray_constants as ray_constants


def _random_string():
    id_hash = hashlib.sha1()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest()
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes


def format_error_message(exception_message, task_exception=False):
    """"""Improve the formatting of an exception thrown by a remote function.

    This method takes a traceback from an exception and makes it nicer by
    removing a few uninformative lines and adding some space to indent the
    remaining lines nicely.

    Args:
        exception_message (str): A message generated by traceback.format_exc().

    Returns:
        A string of the formatted exception message.
    """"""
    lines = exception_message.split(""\n"")
    if task_exception:
        # For errors that occur inside of tasks, remove lines 1 and 2 which are
        # always the same, they just contain information about the worker code.
        lines = lines[0:1] + lines[3:]
        pass
    return ""\n"".join(lines)


def push_error_to_driver(worker, error_type, message, driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Args:
        worker: The worker to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    worker.raylet_client.push_error(driver_id, error_type, message,
                                    time.time())


def push_error_to_driver_through_redis(redis_client,
                                       error_type,
                                       message,
                                       driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Normally the push_error_to_driver function should be used. However, in some
    instances, the local scheduler client is not available, e.g., because the
    error happens in Python before the driver or worker has connected to the
    backend processes.

    Args:
        redis_client: The redis client to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    # Do everything in Python and through the Python Redis client instead
    # of through the raylet.
    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,
                                                       message, time.time())
    redis_client.execute_command(""RAY.TABLE_APPEND"",
                                 ray.gcs_utils.TablePrefix.ERROR_INFO,
                                 ray.gcs_utils.TablePubsub.ERROR_INFO,
                                 driver_id.binary(), error_data)


def is_cython(obj):
    """"""Check if an object is a Cython function or method""""""

    # TODO(suo): We could split these into two functions, one for Cython
    # functions and another for Cython methods.
    # TODO(suo): There doesn't appear to be a Cython function 'type' we can
    # check against via isinstance. Please correct me if I'm wrong.
    def check_cython(x):
        return type(x).__name__ == ""cython_function_or_method""

    # Check if function or method, respectively
    return check_cython(obj) or \
        (hasattr(obj, ""__func__"") and check_cython(obj.__func__))


def is_function_or_method(obj):
    """"""Check if an object is a function or method.

    Args:
        obj: The Python object in question.

    Returns:
        True if the object is an function or method.
    """"""
    return inspect.isfunction(obj) or inspect.ismethod(obj) or is_cython(obj)


def is_class_method(f):
    """"""Returns whether the given method is a class_method.""""""
    return hasattr(f, ""__self__"") and f.__self__ is not None


def random_string():
    """"""Generate a random string to use as an ID.

    Note that users may seed numpy, which could cause this function to generate
    duplicate IDs. Therefore, we need to seed numpy ourselves, but we can't
    interfere with the state of the user's random number generator, so we
    extract the state of the random number generator and reset it after we are
    done.

    TODO(rkn): If we want to later guarantee that these are generated in a
    deterministic manner, then we will need to make some changes here.

    Returns:
        A random byte string of length ray_constants.ID_SIZE.
    """"""
    # Get the state of the numpy random number generator.
    numpy_state = np.random.get_state()
    # Try to use true randomness.
    np.random.seed(None)
    # Generate the random ID.
    random_id = np.random.bytes(ray_constants.ID_SIZE)
    # Reset the state of the numpy random number generator.
    np.random.set_state(numpy_state)
    return random_id


def decode(byte_str, allow_none=False):
    """"""Make this unicode in Python 3, otherwise leave it as bytes.

    Args:
        byte_str: The byte string to decode.
        allow_none: If true, then we will allow byte_str to be None in which
            case we will return an empty string. TODO(rkn): Remove this flag.
            This is only here to simplify upgrading to flatbuffers 1.10.0.

    Returns:
        A byte string in Python 2 and a unicode string in Python 3.
    """"""
    if byte_str is None and allow_none:
        return """"

    if not isinstance(byte_str, bytes):
        raise ValueError(
            ""The argument {} must be a bytes object."".format(byte_str))
    if sys.version_info >= (3, 0):
        return byte_str.decode(""ascii"")
    else:
        return byte_str


def binary_to_object_id(binary_object_id):
    return ray.ObjectID(binary_object_id)


def binary_to_hex(identifier):
    hex_identifier = binascii.hexlify(identifier)
    if sys.version_info >= (3, 0):
        hex_identifier = hex_identifier.decode()
    return hex_identifier


def hex_to_binary(hex_identifier):
    return binascii.unhexlify(hex_identifier)


def get_cuda_visible_devices():
    """"""Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable.

    Returns:
        if CUDA_VISIBLE_DEVICES is set, this returns a list of integers with
            the IDs of the GPUs. If it is not set, this returns None.
    """"""
    gpu_ids_str = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    if gpu_ids_str is None:
        return None

    if gpu_ids_str == """":
        return []

    return [int(i) for i in gpu_ids_str.split("","")]


def set_cuda_visible_devices(gpu_ids):
    """"""Set the CUDA_VISIBLE_DEVICES environment variable.

    Args:
        gpu_ids: This is a list of integers representing GPU IDs.
    """"""
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join([str(i) for i in gpu_ids])


def resources_from_resource_arguments(default_num_cpus, default_num_gpus,
                                      default_resources, runtime_num_cpus,
                                      runtime_num_gpus, runtime_resources):
    """"""Determine a task's resource requirements.

    Args:
        default_num_cpus: The default number of CPUs required by this function
            or actor method.
        default_num_gpus: The default number of GPUs required by this function
            or actor method.
        default_resources: The default custom resources required by this
            function or actor method.
        runtime_num_cpus: The number of CPUs requested when the task was
            invoked.
        runtime_num_gpus: The number of GPUs requested when the task was
            invoked.
        runtime_resources: The custom resources requested when the task was
            invoked.

    Returns:
        A dictionary of the resource requirements for the task.
    """"""
    if runtime_resources is not None:
        resources = runtime_resources.copy()
    elif default_resources is not None:
        resources = default_resources.copy()
    else:
        resources = {}

    if ""CPU"" in resources or ""GPU"" in resources:
        raise ValueError(""The resources dictionary must not ""
                         ""contain the key 'CPU' or 'GPU'"")

    assert default_num_cpus is not None
    resources[""CPU""] = (default_num_cpus
                        if runtime_num_cpus is None else runtime_num_cpus)

    if runtime_num_gpus is not None:
        resources[""GPU""] = runtime_num_gpus
    elif default_num_gpus is not None:
        resources[""GPU""] = default_num_gpus

    return resources


_default_handler = None


def setup_logger(logging_level, logging_format):
    """"""Setup default logging for ray.""""""
    logger = logging.getLogger(""ray"")
    if type(logging_level) is str:
        logging_level = logging.getLevelName(logging_level.upper())
    logger.setLevel(logging_level)
    global _default_handler
    if _default_handler is None:
        _default_handler = logging.StreamHandler()
        logger.addHandler(_default_handler)
    _default_handler.setFormatter(logging.Formatter(logging_format))
    logger.propagate = False


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5bd44f8afcecbfb0db479ce230c790fc2c56569a/psutil/tests/test_linux.py#L132-L138  # noqa: E501
def vmstat(stat):
    """"""Run vmstat and get a particular statistic.

    Args:
        stat: The statistic that we are interested in retrieving.

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output([""vmstat"", ""-s""])
    stat = stat.encode(""ascii"")
    for line in out.split(b""\n""):
        line = line.strip()
        if stat in line:
            return int(line.split(b"" "")[0])
    raise ValueError(""Can't find {} in 'vmstat' output."".format(stat))


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5e90b0a7f3fccb177445a186cc4fac62cfadb510/psutil/tests/test_osx.py#L29-L38  # noqa: E501
def sysctl(command):
    """"""Run a sysctl command and parse the output.

    Args:
        command: A sysctl command with an argument, for example,
            [""sysctl"", ""hw.memsize""].

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output(command)
    result = out.split(b"" "")[1]
    try:
        return int(result)
    except ValueError:
        return result


def get_system_memory():
    """"""Return the total amount of system memory in bytes.

    Returns:
        The total amount of system memory in bytes.
    """"""
    # Try to accurately figure out the memory limit if we are in a docker
    # container. Note that this file is not specific to Docker and its value is
    # often much larger than the actual amount of memory.
    docker_limit = None
    memory_limit_filename = ""/sys/fs/cgroup/memory/memory.limit_in_bytes""
    if os.path.exists(memory_limit_filename):
        with open(memory_limit_filename, ""r"") as f:
            docker_limit = int(f.read())

    # Use psutil if it is available.
    psutil_memory_in_bytes = None
    try:
        import psutil
        psutil_memory_in_bytes = psutil.virtual_memory().total
    except ImportError:
        pass

    if psutil_memory_in_bytes is not None:
        memory_in_bytes = psutil_memory_in_bytes
    elif sys.platform == ""linux"" or sys.platform == ""linux2"":
        # Handle Linux.
        bytes_in_kilobyte = 1024
        memory_in_bytes = vmstat(""total memory"") * bytes_in_kilobyte
    else:
        # Handle MacOS.
        memory_in_bytes = sysctl([""sysctl"", ""hw.memsize""])

    if docker_limit is not None:
        return min(docker_limit, memory_in_bytes)
    else:
        return memory_in_bytes


def get_shared_memory_bytes():
    """"""Get the size of the shared memory file system.

    Returns:
        The size of the shared memory file system in bytes.
    """"""
    # Make sure this is only called on Linux.
    assert sys.platform == ""linux"" or sys.platform == ""linux2""

    shm_fd = os.open(""/dev/shm"", os.O_RDONLY)
    try:
        shm_fs_stats = os.fstatvfs(shm_fd)
        # The value shm_fs_stats.f_bsize is the block size and the
        # value shm_fs_stats.f_bavail is the number of available
        # blocks.
        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail
    finally:
        os.close(shm_fd)

    return shm_avail


def check_oversized_pickle(pickled, name, obj_type, worker):
    """"""Send a warning message if the pickled object is too large.

    Args:
        pickled: the pickled object.
        name: name of the pickled object.
        obj_type: type of the pickled object, can be 'function',
            'remote function', 'actor', or 'object'.
        worker: the worker used to send warning message.
    """"""
    length = len(pickled)
    if length <= ray_constants.PICKLE_OBJECT_WARNING_SIZE:
        return
    warning_message = (
        ""Warning: The {} {} has size {} when pickled. ""
        ""It will be stored in Redis, which could cause memory issues. ""
        ""This may mean that its definition uses a large array or other object.""
    ).format(obj_type, name, length)
    push_error_to_driver(
        worker,
        ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,
        warning_message,
        driver_id=worker.task_driver_id)


class _ThreadSafeProxy(object):
    """"""This class is used to create a thread-safe proxy for a given object.
        Every method call will be guarded with a lock.

    Attributes:
        orig_obj (object): the original object.
        lock (threading.Lock): the lock object.
        _wrapper_cache (dict): a cache from original object's methods to
            the proxy methods.
    """"""

    def __init__(self, orig_obj, lock):
        self.orig_obj = orig_obj
        self.lock = lock
        self._wrapper_cache = {}

    def __getattr__(self, attr):
        orig_attr = getattr(self.orig_obj, attr)
        if not callable(orig_attr):
            # If the original attr is a field, just return it.
            return orig_attr
        else:
            # If the orginal attr is a method,
            # return a wrapper that guards the original method with a lock.
            wrapper = self._wrapper_cache.get(attr)
            if wrapper is None:

                @functools.wraps(orig_attr)
                def _wrapper(*args, **kwargs):
                    with self.lock:
                        return orig_attr(*args, **kwargs)

                self._wrapper_cache[attr] = _wrapper
                wrapper = _wrapper
            return wrapper


def thread_safe_client(client, lock=None):
    """"""Create a thread-safe proxy which locks every method call
    for the given client.

    Args:
        client: the client object to be guarded.
        lock: the lock object that will be used to lock client's methods.
            If None, a new lock will be used.

    Returns:
        A thread-safe proxy for the given client.
    """"""
    if lock is None:
        lock = threading.Lock()
    return _ThreadSafeProxy(client, lock)


def is_main_thread():
    return threading.current_thread().getName() == ""MainThread""
/n/n/npython/ray/worker.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from contextlib import contextmanager
import atexit
import colorama
import faulthandler
import hashlib
import inspect
import logging
import numpy as np
import os
import redis
import signal
from six.moves import queue
import sys
import threading
import time
import traceback

# Ray modules
import pyarrow
import pyarrow.plasma as plasma
import ray.cloudpickle as pickle
import ray.experimental.state as state
import ray.gcs_utils
import ray.memory_monitor as memory_monitor
import ray.node
import ray.remote_function
import ray.serialization as serialization
import ray.services as services
import ray.signature
import ray.tempfile_services as tempfile_services
import ray.ray_constants as ray_constants
from ray import import_thread
from ray import ObjectID, DriverID, ActorID, ActorHandleID, ClientID, TaskID
from ray import profiling
from ray.function_manager import (FunctionActorManager, FunctionDescriptor)
import ray.parameter
from ray.utils import (check_oversized_pickle, is_cython, random_string,
                       thread_safe_client, setup_logger)

SCRIPT_MODE = 0
WORKER_MODE = 1
LOCAL_MODE = 2
PYTHON_MODE = 3

ERROR_KEY_PREFIX = b""Error:""

# Default resource requirements for actors when no resource requirements are
# specified.
DEFAULT_ACTOR_METHOD_CPUS_SIMPLE_CASE = 1
DEFAULT_ACTOR_CREATION_CPUS_SIMPLE_CASE = 0
# Default resource requirements for actors when some resource requirements are
# specified.
DEFAULT_ACTOR_METHOD_CPUS_SPECIFIED_CASE = 0
DEFAULT_ACTOR_CREATION_CPUS_SPECIFIED_CASE = 1

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)

try:
    import setproctitle
except ImportError:
    setproctitle = None


class RayTaskError(Exception):
    """"""An object used internally to represent a task that threw an exception.

    If a task throws an exception during execution, a RayTaskError is stored in
    the object store for each of the task's outputs. When an object is
    retrieved from the object store, the Python method that retrieved it checks
    to see if the object is a RayTaskError and if it is then an exception is
    thrown propagating the error message.

    Currently, we either use the exception attribute or the traceback attribute
    but not both.

    Attributes:
        function_name (str): The name of the function that failed and produced
            the RayTaskError.
        traceback_str (str): The traceback from the exception.
    """"""

    def __init__(self, function_name, traceback_str):
        """"""Initialize a RayTaskError.""""""
        if setproctitle:
            self.proctitle = setproctitle.getproctitle()
        else:
            self.proctitle = ""ray_worker""
        self.pid = os.getpid()
        self.host = os.uname()[1]
        self.function_name = function_name
        self.traceback_str = traceback_str
        assert traceback_str is not None

    def __str__(self):
        """"""Format a RayTaskError as a string.""""""
        lines = self.traceback_str.split(""\n"")
        out = []
        in_worker = False
        for line in lines:
            if line.startswith(""Traceback ""):
                out.append(""{}{}{} (pid={}, host={})"".format(
                    colorama.Fore.CYAN, self.proctitle, colorama.Fore.RESET,
                    self.pid, self.host))
            elif in_worker:
                in_worker = False
            elif ""ray/worker.py"" in line or ""ray/function_manager.py"" in line:
                in_worker = True
            else:
                out.append(line)
        return ""\n"".join(out)


class Worker(object):
    """"""A class used to define the control flow of a worker process.

    Note:
        The methods in this class are considered unexposed to the user. The
        functions outside of this class are considered exposed.

    Attributes:
        connected (bool): True if Ray has been started and False otherwise.
        mode: The mode of the worker. One of SCRIPT_MODE, LOCAL_MODE, and
            WORKER_MODE.
        cached_functions_to_run (List): A list of functions to run on all of
            the workers that should be exported as soon as connect is called.
        profiler: the profiler used to aggregate profiling information.
    """"""

    def __init__(self):
        """"""Initialize a Worker object.""""""
        self.connected = False
        self.mode = None
        self.cached_functions_to_run = []
        self.actor_init_error = None
        self.make_actor = None
        self.actors = {}
        self.actor_task_counter = 0
        # The number of threads Plasma should use when putting an object in the
        # object store.
        self.memcopy_threads = 12
        # When the worker is constructed. Record the original value of the
        # CUDA_VISIBLE_DEVICES environment variable.
        self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
        self.profiler = None
        self.memory_monitor = memory_monitor.MemoryMonitor()
        # A dictionary that maps from driver id to SerializationContext
        # TODO: clean up the SerializationContext once the job finished.
        self.serialization_context_map = {}
        self.function_actor_manager = FunctionActorManager(self)
        # Identity of the driver that this worker is processing.
        # It is a DriverID.
        self.task_driver_id = DriverID.nil()
        self._task_context = threading.local()
        # This event is checked regularly by all of the threads so that they
        # know when to exit.
        self.threads_stopped = threading.Event()

    @property
    def task_context(self):
        """"""A thread-local that contains the following attributes.

        current_task_id: For the main thread, this field is the ID of this
            worker's current running task; for other threads, this field is a
            fake random ID.
        task_index: The number of tasks that have been submitted from the
            current task.
        put_index: The number of objects that have been put from the current
            task.
        """"""
        if not hasattr(self._task_context, 'initialized'):
            # Initialize task_context for the current thread.
            if ray.utils.is_main_thread():
                # If this is running on the main thread, initialize it to
                # NIL. The actual value will set when the worker receives
                # a task from raylet backend.
                self._task_context.current_task_id = TaskID.nil()
            else:
                # If this is running on a separate thread, then the mapping
                # to the current task ID may not be correct. Generate a
                # random task ID so that the backend can differentiate
                # between different threads.
                self._task_context.current_task_id = TaskID(random_string())
                if getattr(self, '_multithreading_warned', False) is not True:
                    logger.warning(
                        ""Calling ray.get or ray.wait in a separate thread ""
                        ""may lead to deadlock if the main thread blocks on ""
                        ""this thread and there are not enough resources to ""
                        ""execute more tasks"")
                    self._multithreading_warned = True

            self._task_context.task_index = 0
            self._task_context.put_index = 1
            self._task_context.initialized = True
        return self._task_context

    @property
    def current_task_id(self):
        return self.task_context.current_task_id

    def mark_actor_init_failed(self, error):
        """"""Called to mark this actor as failed during initialization.""""""

        self.actor_init_error = error

    def reraise_actor_init_error(self):
        """"""Raises any previous actor initialization error.""""""

        if self.actor_init_error is not None:
            raise self.actor_init_error

    def get_serialization_context(self, driver_id):
        """"""Get the SerializationContext of the driver that this worker is processing.

        Args:
            driver_id: The ID of the driver that indicates which driver to get
                the serialization context for.

        Returns:
            The serialization context of the given driver.
        """"""
        if driver_id not in self.serialization_context_map:
            _initialize_serialization(driver_id)
        return self.serialization_context_map[driver_id]

    def check_connected(self):
        """"""Check if the worker is connected.

        Raises:
          Exception: An exception is raised if the worker is not connected.
        """"""
        if not self.connected:
            raise RayConnectionError(""Ray has not been started yet. You can ""
                                     ""start Ray with 'ray.init()'."")

    def set_mode(self, mode):
        """"""Set the mode of the worker.

        The mode SCRIPT_MODE should be used if this Worker is a driver that is
        being run as a Python script or interactively in a shell. It will print
        information about task failures.

        The mode WORKER_MODE should be used if this Worker is not a driver. It
        will not print information about tasks.

        The mode LOCAL_MODE should be used if this Worker is a driver and if
        you want to run the driver in a manner equivalent to serial Python for
        debugging purposes. It will not send remote function calls to the
        scheduler and will insead execute them in a blocking fashion.

        Args:
            mode: One of SCRIPT_MODE, WORKER_MODE, and LOCAL_MODE.
        """"""
        self.mode = mode

    def store_and_register(self, object_id, value, depth=100):
        """"""Store an object and attempt to register its class if needed.

        Args:
            object_id: The ID of the object to store.
            value: The value to put in the object store.
            depth: The maximum number of classes to recursively register.

        Raises:
            Exception: An exception is raised if the attempt to store the
                object fails. This can happen if there is already an object
                with the same ID in the object store or if the object store is
                full.
        """"""
        counter = 0
        while True:
            if counter == depth:
                raise Exception(""Ray exceeded the maximum number of classes ""
                                ""that it will recursively serialize when ""
                                ""attempting to serialize an object of ""
                                ""type {}."".format(type(value)))
            counter += 1
            try:
                self.plasma_client.put(
                    value,
                    object_id=pyarrow.plasma.ObjectID(object_id.binary()),
                    memcopy_threads=self.memcopy_threads,
                    serialization_context=self.get_serialization_context(
                        self.task_driver_id))
                break
            except pyarrow.SerializationCallbackError as e:
                try:
                    register_custom_serializer(
                        type(e.example_object), use_dict=True)
                    warning_message = (""WARNING: Serializing objects of type ""
                                       ""{} by expanding them as dictionaries ""
                                       ""of their fields. This behavior may ""
                                       ""be incorrect in some cases."".format(
                                           type(e.example_object)))
                    logger.debug(warning_message)
                except (serialization.RayNotDictionarySerializable,
                        serialization.CloudPickleError,
                        pickle.pickle.PicklingError, Exception):
                    # We also handle generic exceptions here because
                    # cloudpickle can fail with many different types of errors.
                    try:
                        register_custom_serializer(
                            type(e.example_object), use_pickle=True)
                        warning_message = (""WARNING: Falling back to ""
                                           ""serializing objects of type {} by ""
                                           ""using pickle. This may be ""
                                           ""inefficient."".format(
                                               type(e.example_object)))
                        logger.warning(warning_message)
                    except serialization.CloudPickleError:
                        register_custom_serializer(
                            type(e.example_object),
                            use_pickle=True,
                            local=True)
                        warning_message = (""WARNING: Pickling the class {} ""
                                           ""failed, so we are using pickle ""
                                           ""and only registering the class ""
                                           ""locally."".format(
                                               type(e.example_object)))
                        logger.warning(warning_message)

    def put_object(self, object_id, value):
        """"""Put value in the local object store with object id objectid.

        This assumes that the value for objectid has not yet been placed in the
        local object store.

        Args:
            object_id (object_id.ObjectID): The object ID of the value to be
                put.
            value: The value to put in the object store.

        Raises:
            Exception: An exception is raised if the attempt to store the
                object fails. This can happen if there is already an object
                with the same ID in the object store or if the object store is
                full.
        """"""
        # Make sure that the value is not an object ID.
        if isinstance(value, ObjectID):
            raise Exception(
                ""Calling 'put' on an ray.ObjectID is not allowed ""
                ""(similarly, returning an ray.ObjectID from a remote ""
                ""function is not allowed). If you really want to ""
                ""do this, you can wrap the ray.ObjectID in a list and ""
                ""call 'put' on it (or return it)."")

        # Serialize and put the object in the object store.
        try:
            self.store_and_register(object_id, value)
        except pyarrow.PlasmaObjectExists:
            # The object already exists in the object store, so there is no
            # need to add it again. TODO(rkn): We need to compare the hashes
            # and make sure that the objects are in fact the same. We also
            # should return an error code to the caller instead of printing a
            # message.
            logger.info(
                ""The object with ID {} already exists in the object store.""
                .format(object_id))
        except TypeError:
            # This error can happen because one of the members of the object
            # may not be serializable for cloudpickle. So we need these extra
            # fallbacks here to start from the beginning. Hopefully the object
            # could have a `__reduce__` method.
            register_custom_serializer(type(value), use_pickle=True)
            warning_message = (""WARNING: Serializing the class {} failed, ""
                               ""so are are falling back to cloudpickle.""
                               .format(type(value)))
            logger.warning(warning_message)
            self.store_and_register(object_id, value)

    def retrieve_and_deserialize(self, object_ids, timeout, error_timeout=10):
        start_time = time.time()
        # Only send the warning once.
        warning_sent = False
        while True:
            try:
                # We divide very large get requests into smaller get requests
                # so that a single get request doesn't block the store for a
                # long time, if the store is blocked, it can block the manager
                # as well as a consequence.
                results = []
                for i in range(0, len(object_ids),
                               ray._config.worker_get_request_size()):
                    results += self.plasma_client.get(
                        object_ids[i:(
                            i + ray._config.worker_get_request_size())],
                        timeout,
                        self.get_serialization_context(self.task_driver_id))
                return results
            except pyarrow.lib.ArrowInvalid:
                # TODO(ekl): the local scheduler could include relevant
                # metadata in the task kill case for a better error message
                invalid_error = RayTaskError(
                    ""<unknown>"",
                    ""Invalid return value: likely worker died or was killed ""
                    ""while executing the task; check previous logs or dmesg ""
                    ""for errors."")
                return [invalid_error] * len(object_ids)
            except pyarrow.DeserializationCallbackError:
                # Wait a little bit for the import thread to import the class.
                # If we currently have the worker lock, we need to release it
                # so that the import thread can acquire it.
                if self.mode == WORKER_MODE:
                    self.lock.release()
                time.sleep(0.01)
                if self.mode == WORKER_MODE:
                    self.lock.acquire()

                if time.time() - start_time > error_timeout:
                    warning_message = (""This worker or driver is waiting to ""
                                       ""receive a class definition so that it ""
                                       ""can deserialize an object from the ""
                                       ""object store. This may be fine, or it ""
                                       ""may be a bug."")
                    if not warning_sent:
                        ray.utils.push_error_to_driver(
                            self,
                            ray_constants.WAIT_FOR_CLASS_PUSH_ERROR,
                            warning_message,
                            driver_id=self.task_driver_id)
                    warning_sent = True

    def get_object(self, object_ids):
        """"""Get the value or values in the object store associated with the IDs.

        Return the values from the local object store for object_ids. This will
        block until all the values for object_ids have been written to the
        local object store.

        Args:
            object_ids (List[object_id.ObjectID]): A list of the object IDs
                whose values should be retrieved.
        """"""
        # Make sure that the values are object IDs.
        for object_id in object_ids:
            if not isinstance(object_id, ObjectID):
                raise Exception(
                    ""Attempting to call `get` on the value {}, ""
                    ""which is not an ray.ObjectID."".format(object_id))
        # Do an initial fetch for remote objects. We divide the fetch into
        # smaller fetches so as to not block the manager for a prolonged period
        # of time in a single call.
        plain_object_ids = [
            plasma.ObjectID(object_id.binary()) for object_id in object_ids
        ]
        for i in range(0, len(object_ids),
                       ray._config.worker_fetch_request_size()):
            self.raylet_client.fetch_or_reconstruct(
                object_ids[i:(i + ray._config.worker_fetch_request_size())],
                True)

        # Get the objects. We initially try to get the objects immediately.
        final_results = self.retrieve_and_deserialize(plain_object_ids, 0)
        # Construct a dictionary mapping object IDs that we haven't gotten yet
        # to their original index in the object_ids argument.
        unready_ids = {
            plain_object_ids[i].binary(): i
            for (i, val) in enumerate(final_results)
            if val is plasma.ObjectNotAvailable
        }

        if len(unready_ids) > 0:
            # Try reconstructing any objects we haven't gotten yet. Try to
            # get them until at least get_timeout_milliseconds
            # milliseconds passes, then repeat.
            while len(unready_ids) > 0:
                object_ids_to_fetch = [
                    plasma.ObjectID(unready_id)
                    for unready_id in unready_ids.keys()
                ]
                ray_object_ids_to_fetch = [
                    ObjectID(unready_id) for unready_id in unready_ids.keys()
                ]
                fetch_request_size = ray._config.worker_fetch_request_size()
                for i in range(0, len(object_ids_to_fetch),
                               fetch_request_size):
                    self.raylet_client.fetch_or_reconstruct(
                        ray_object_ids_to_fetch[i:(i + fetch_request_size)],
                        False,
                        self.current_task_id,
                    )
                results = self.retrieve_and_deserialize(
                    object_ids_to_fetch,
                    max([
                        ray._config.get_timeout_milliseconds(),
                        int(0.01 * len(unready_ids)),
                    ]),
                )
                # Remove any entries for objects we received during this
                # iteration so we don't retrieve the same object twice.
                for i, val in enumerate(results):
                    if val is not plasma.ObjectNotAvailable:
                        object_id = object_ids_to_fetch[i].binary()
                        index = unready_ids[object_id]
                        final_results[index] = val
                        unready_ids.pop(object_id)

            # If there were objects that we weren't able to get locally,
            # let the local scheduler know that we're now unblocked.
            self.raylet_client.notify_unblocked(self.current_task_id)

        assert len(final_results) == len(object_ids)
        return final_results

    def submit_task(self,
                    function_descriptor,
                    args,
                    actor_id=None,
                    actor_handle_id=None,
                    actor_counter=0,
                    is_actor_checkpoint_method=False,
                    actor_creation_id=None,
                    actor_creation_dummy_object_id=None,
                    max_actor_reconstructions=0,
                    execution_dependencies=None,
                    new_actor_handles=None,
                    num_return_vals=None,
                    resources=None,
                    placement_resources=None,
                    driver_id=None):
        """"""Submit a remote task to the scheduler.

        Tell the scheduler to schedule the execution of the function with
        function_descriptor with arguments args. Retrieve object IDs for the
        outputs of the function from the scheduler and immediately return them.

        Args:
            function_descriptor: The function descriptor to execute.
            args: The arguments to pass into the function. Arguments can be
                object IDs or they can be values. If they are values, they must
                be serializable objects.
            actor_id: The ID of the actor that this task is for.
            actor_counter: The counter of the actor task.
            is_actor_checkpoint_method: True if this is an actor checkpoint
                task and false otherwise.
            actor_creation_id: The ID of the actor to create, if this is an
                actor creation task.
            actor_creation_dummy_object_id: If this task is an actor method,
                then this argument is the dummy object ID associated with the
                actor creation task for the corresponding actor.
            execution_dependencies: The execution dependencies for this task.
            num_return_vals: The number of return values this function should
                have.
            resources: The resource requirements for this task.
            placement_resources: The resources required for placing the task.
                If this is not provided or if it is an empty dictionary, then
                the placement resources will be equal to resources.
            driver_id: The ID of the relevant driver. This is almost always the
                driver ID of the driver that is currently running. However, in
                the exceptional case that an actor task is being dispatched to
                an actor created by a different driver, this should be the
                driver ID of the driver that created the actor.

        Returns:
            The return object IDs for this task.
        """"""
        with profiling.profile(""submit_task"", worker=self):
            if actor_id is None:
                assert actor_handle_id is None
                actor_id = ActorID.nil()
                actor_handle_id = ActorHandleID.nil()
            else:
                assert actor_handle_id is not None

            if actor_creation_id is None:
                actor_creation_id = ActorID.nil()

            if actor_creation_dummy_object_id is None:
                actor_creation_dummy_object_id = ObjectID.nil()

            # Put large or complex arguments that are passed by value in the
            # object store first.
            args_for_local_scheduler = []
            for arg in args:
                if isinstance(arg, ObjectID):
                    args_for_local_scheduler.append(arg)
                elif ray._raylet.check_simple_value(arg):
                    args_for_local_scheduler.append(arg)
                else:
                    args_for_local_scheduler.append(put(arg))

            # By default, there are no execution dependencies.
            if execution_dependencies is None:
                execution_dependencies = []

            if new_actor_handles is None:
                new_actor_handles = []

            if driver_id is None:
                driver_id = self.task_driver_id

            if resources is None:
                raise ValueError(""The resources dictionary is required."")
            for value in resources.values():
                assert (isinstance(value, int) or isinstance(value, float))
                if value < 0:
                    raise ValueError(
                        ""Resource quantities must be nonnegative."")
                if (value >= 1 and isinstance(value, float)
                        and not value.is_integer()):
                    raise ValueError(
                        ""Resource quantities must all be whole numbers."")

            if placement_resources is None:
                placement_resources = {}

            # Increment the worker's task index to track how many tasks
            # have been submitted by the current task so far.
            self.task_context.task_index += 1
            # The parent task must be set for the submitted task.
            assert not self.current_task_id.is_nil()
            # Current driver id must not be nil when submitting a task.
            # Because every task must belong to a driver.
            assert not self.task_driver_id.is_nil()
            # Submit the task to local scheduler.
            function_descriptor_list = (
                function_descriptor.get_function_descriptor_list())
            assert isinstance(driver_id, DriverID)
            task = ray._raylet.Task(
                driver_id,
                function_descriptor_list,
                args_for_local_scheduler,
                num_return_vals,
                self.current_task_id,
                self.task_context.task_index,
                actor_creation_id,
                actor_creation_dummy_object_id,
                max_actor_reconstructions,
                actor_id,
                actor_handle_id,
                actor_counter,
                new_actor_handles,
                execution_dependencies,
                resources,
                placement_resources,
            )
            self.raylet_client.submit_task(task)

            return task.returns()

    def run_function_on_all_workers(self, function,
                                    run_on_other_drivers=False):
        """"""Run arbitrary code on all of the workers.

        This function will first be run on the driver, and then it will be
        exported to all of the workers to be run. It will also be run on any
        new workers that register later. If ray.init has not been called yet,
        then cache the function and export it later.

        Args:
            function (Callable): The function to run on all of the workers. It
                takes only one argument, a worker info dict. If it returns
                anything, its return values will not be used.
            run_on_other_drivers: The boolean that indicates whether we want to
                run this function on other drivers. One case is we may need to
                share objects across drivers.
        """"""
        # If ray.init has not been called yet, then cache the function and
        # export it when connect is called. Otherwise, run the function on all
        # workers.
        if self.mode is None:
            self.cached_functions_to_run.append(function)
        else:
            # Attempt to pickle the function before we need it. This could
            # fail, and it is more convenient if the failure happens before we
            # actually run the function locally.
            pickled_function = pickle.dumps(function)

            function_to_run_id = hashlib.sha1(pickled_function).digest()
            key = b""FunctionsToRun:"" + function_to_run_id
            # First run the function on the driver.
            # We always run the task locally.
            function({""worker"": self})
            # Check if the function has already been put into redis.
            function_exported = self.redis_client.setnx(b""Lock:"" + key, 1)
            if not function_exported:
                # In this case, the function has already been exported, so
                # we don't need to export it again.
                return

            check_oversized_pickle(pickled_function, function.__name__,
                                   ""function"", self)

            # Run the function on all workers.
            self.redis_client.hmset(
                key, {
                    ""driver_id"": self.task_driver_id.binary(),
                    ""function_id"": function_to_run_id,
                    ""function"": pickled_function,
                    ""run_on_other_drivers"": str(run_on_other_drivers)
                })
            self.redis_client.rpush(""Exports"", key)
            # TODO(rkn): If the worker fails after it calls setnx and before it
            # successfully completes the hmset and rpush, then the program will
            # most likely hang. This could be fixed by making these three
            # operations into a transaction (or by implementing a custom
            # command that does all three things).

    def _get_arguments_for_execution(self, function_name, serialized_args):
        """"""Retrieve the arguments for the remote function.

        This retrieves the values for the arguments to the remote function that
        were passed in as object IDs. Arguments that were passed by value are
        not changed. This is called by the worker that is executing the remote
        function.

        Args:
            function_name (str): The name of the remote function whose
                arguments are being retrieved.
            serialized_args (List): The arguments to the function. These are
                either strings representing serialized objects passed by value
                or they are ray.ObjectIDs.

        Returns:
            The retrieved arguments in addition to the arguments that were
                passed by value.

        Raises:
            RayTaskError: This exception is raised if a task that
                created one of the arguments failed.
        """"""
        arguments = []
        for (i, arg) in enumerate(serialized_args):
            if isinstance(arg, ObjectID):
                # get the object from the local object store
                argument = self.get_object([arg])[0]
                if isinstance(argument, RayTaskError):
                    raise argument
            else:
                # pass the argument by value
                argument = arg

            arguments.append(argument)
        return arguments

    def _store_outputs_in_object_store(self, object_ids, outputs):
        """"""Store the outputs of a remote function in the local object store.

        This stores the values that were returned by a remote function in the
        local object store. If any of the return values are object IDs, then
        these object IDs are aliased with the object IDs that the scheduler
        assigned for the return values. This is called by the worker that
        executes the remote function.

        Note:
            The arguments object_ids and outputs should have the same length.

        Args:
            object_ids (List[ObjectID]): The object IDs that were assigned to
                the outputs of the remote function call.
            outputs (Tuple): The value returned by the remote function. If the
                remote function was supposed to only return one value, then its
                output was wrapped in a tuple with one element prior to being
                passed into this function.
        """"""
        for i in range(len(object_ids)):
            if isinstance(outputs[i], ray.actor.ActorHandle):
                raise Exception(""Returning an actor handle from a remote ""
                                ""function is not allowed)."")

            self.put_object(object_ids[i], outputs[i])

    def _process_task(self, task, function_execution_info):
        """"""Execute a task assigned to this worker.

        This method deserializes a task from the scheduler, and attempts to
        execute the task. If the task succeeds, the outputs are stored in the
        local object store. If the task throws an exception, RayTaskError
        objects are stored in the object store to represent the failed task
        (these will be retrieved by calls to get or by subsequent tasks that
        use the outputs of this task).
        """"""
        assert self.current_task_id.is_nil()
        assert self.task_context.task_index == 0
        assert self.task_context.put_index == 1
        if task.actor_id().is_nil():
            # If this worker is not an actor, check that `task_driver_id`
            # was reset when the worker finished the previous task.
            assert self.task_driver_id.is_nil()
            # Set the driver ID of the current running task. This is
            # needed so that if the task throws an exception, we propagate
            # the error message to the correct driver.
            self.task_driver_id = task.driver_id()
        else:
            # If this worker is an actor, task_driver_id wasn't reset.
            # Check that current task's driver ID equals the previous one.
            assert self.task_driver_id == task.driver_id()

        self.task_context.current_task_id = task.task_id()

        function_descriptor = FunctionDescriptor.from_bytes_list(
            task.function_descriptor_list())
        args = task.arguments()
        return_object_ids = task.returns()
        if (not task.actor_id().is_nil()
                or not task.actor_creation_id().is_nil()):
            dummy_return_id = return_object_ids.pop()
        function_executor = function_execution_info.function
        function_name = function_execution_info.function_name

        # Get task arguments from the object store.
        try:
            if function_name != ""__ray_terminate__"":
                self.reraise_actor_init_error()
            self.memory_monitor.raise_if_low_memory()
            with profiling.profile(""task:deserialize_arguments"", worker=self):
                arguments = self._get_arguments_for_execution(
                    function_name, args)
        except RayTaskError as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))
            return
        except Exception as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))
            return

        # Execute the task.
        try:
            with profiling.profile(""task:execute"", worker=self):
                if (task.actor_id().is_nil()
                        and task.actor_creation_id().is_nil()):
                    outputs = function_executor(*arguments)
                else:
                    if not task.actor_id().is_nil():
                        key = task.actor_id()
                    else:
                        key = task.actor_creation_id()
                    outputs = function_executor(dummy_return_id,
                                                self.actors[key], *arguments)
        except Exception as e:
            # Determine whether the exception occured during a task, not an
            # actor method.
            task_exception = task.actor_id().is_nil()
            traceback_str = ray.utils.format_error_message(
                traceback.format_exc(), task_exception=task_exception)
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e, traceback_str)
            return

        # Store the outputs in the local object store.
        try:
            with profiling.profile(""task:store_outputs"", worker=self):
                # If this is an actor task, then the last object ID returned by
                # the task is a dummy output, not returned by the function
                # itself. Decrement to get the correct number of return values.
                num_returns = len(return_object_ids)
                if num_returns == 1:
                    outputs = (outputs, )
                self._store_outputs_in_object_store(return_object_ids, outputs)
        except Exception as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))

    def _handle_process_task_failure(self, function_descriptor,
                                     return_object_ids, error, backtrace):
        function_name = function_descriptor.function_name
        failure_object = RayTaskError(function_name, backtrace)
        failure_objects = [
            failure_object for _ in range(len(return_object_ids))
        ]
        self._store_outputs_in_object_store(return_object_ids, failure_objects)
        # Log the error message.
        ray.utils.push_error_to_driver(
            self,
            ray_constants.TASK_PUSH_ERROR,
            str(failure_object),
            driver_id=self.task_driver_id)
        # Mark the actor init as failed
        if not self.actor_id.is_nil() and function_name == ""__init__"":
            self.mark_actor_init_failed(error)

    def _wait_for_and_process_task(self, task):
        """"""Wait for a task to be ready and process the task.

        Args:
            task: The task to execute.
        """"""
        function_descriptor = FunctionDescriptor.from_bytes_list(
            task.function_descriptor_list())
        driver_id = task.driver_id()

        # TODO(rkn): It would be preferable for actor creation tasks to share
        # more of the code path with regular task execution.
        if not task.actor_creation_id().is_nil():
            assert self.actor_id.is_nil()
            self.actor_id = task.actor_creation_id()
            self.function_actor_manager.load_actor(driver_id,
                                                   function_descriptor)

        execution_info = self.function_actor_manager.get_execution_info(
            driver_id, function_descriptor)

        # Execute the task.
        # TODO(rkn): Consider acquiring this lock with a timeout and pushing a
        # warning to the user if we are waiting too long to acquire the lock
        # because that may indicate that the system is hanging, and it'd be
        # good to know where the system is hanging.
        with self.lock:
            function_name = execution_info.function_name
            extra_data = {
                ""name"": function_name,
                ""task_id"": task.task_id().hex()
            }
            if task.actor_id().is_nil():
                if task.actor_creation_id().is_nil():
                    title = ""ray_worker:{}()"".format(function_name)
                    next_title = ""ray_worker""
                else:
                    actor = self.actors[task.actor_creation_id()]
                    title = ""ray_{}:{}()"".format(actor.__class__.__name__,
                                                 function_name)
                    next_title = ""ray_{}"".format(actor.__class__.__name__)
            else:
                actor = self.actors[task.actor_id()]
                title = ""ray_{}:{}()"".format(actor.__class__.__name__,
                                             function_name)
                next_title = ""ray_{}"".format(actor.__class__.__name__)
            with profiling.profile(""task"", extra_data=extra_data, worker=self):
                with _changeproctitle(title, next_title):
                    self._process_task(task, execution_info)
                # Reset the state fields so the next task can run.
                self.task_context.current_task_id = TaskID.nil()
                self.task_context.task_index = 0
                self.task_context.put_index = 1
                if self.actor_id.is_nil():
                    # Don't need to reset task_driver_id if the worker is an
                    # actor. Because the following tasks should all have the
                    # same driver id.
                    self.task_driver_id = DriverID.nil()

        # Increase the task execution counter.
        self.function_actor_manager.increase_task_counter(
            driver_id, function_descriptor)

        reached_max_executions = (self.function_actor_manager.get_task_counter(
            driver_id, function_descriptor) == execution_info.max_calls)
        if reached_max_executions:
            self.raylet_client.disconnect()
            sys.exit(0)

    def _get_next_task_from_local_scheduler(self):
        """"""Get the next task from the local scheduler.

        Returns:
            A task from the local scheduler.
        """"""
        with profiling.profile(""worker_idle"", worker=self):
            task = self.raylet_client.get_task()

        # Automatically restrict the GPUs available to this task.
        ray.utils.set_cuda_visible_devices(ray.get_gpu_ids())

        return task

    def main_loop(self):
        """"""The main loop a worker runs to receive and execute tasks.""""""

        def exit(signum, frame):
            shutdown(worker=self)
            sys.exit(0)

        signal.signal(signal.SIGTERM, exit)

        while True:
            task = self._get_next_task_from_local_scheduler()
            self._wait_for_and_process_task(task)


def get_gpu_ids():
    """"""Get the IDs of the GPUs that are available to the worker.

    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker
    started up, then the IDs returned by this method will be a subset of the
    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range
    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.

    Returns:
        A list of GPU IDs.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(""ray.get_gpu_ids() currently does not work in PYTHON ""
                        ""MODE."")

    all_resource_ids = global_worker.raylet_client.resource_ids()
    assigned_ids = [
        resource_id for resource_id, _ in all_resource_ids.get(""GPU"", [])
    ]
    # If the user had already set CUDA_VISIBLE_DEVICES, then respect that (in
    # the sense that only GPU IDs that appear in CUDA_VISIBLE_DEVICES should be
    # returned).
    if global_worker.original_gpu_ids is not None:
        assigned_ids = [
            global_worker.original_gpu_ids[gpu_id] for gpu_id in assigned_ids
        ]

    return assigned_ids


def get_resource_ids():
    """"""Get the IDs of the resources that are available to the worker.

    Returns:
        A dictionary mapping the name of a resource to a list of pairs, where
        each pair consists of the ID of a resource and the fraction of that
        resource reserved for this worker.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(
            ""ray.get_resource_ids() currently does not work in PYTHON ""
            ""MODE."")

    return global_worker.raylet_client.resource_ids()


def _webui_url_helper(client):
    """"""Parsing for getting the url of the web UI.

    Args:
        client: A redis client to use to query the primary Redis shard.

    Returns:
        The URL of the web UI as a string.
    """"""
    result = client.hmget(""webui"", ""url"")[0]
    return ray.utils.decode(result) if result is not None else result


def get_webui_url():
    """"""Get the URL to access the web UI.

    Note that the URL does not specify which node the web UI is on.

    Returns:
        The URL of the web UI as a string.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(""ray.get_webui_url() currently does not work in ""
                        ""PYTHON MODE."")
    return _webui_url_helper(global_worker.redis_client)


global_worker = Worker()
""""""Worker: The global Worker object for this worker process.

We use a global Worker object to ensure that there is a single worker object
per worker process.
""""""

global_state = state.GlobalState()

_global_node = None
""""""ray.node.Node: The global node object that is created by ray.init().""""""


class RayConnectionError(Exception):
    pass


def print_failed_task(task_status):
    """"""Print information about failed tasks.

    Args:
        task_status (Dict): A dictionary containing the name, operationid, and
            error message for a failed task.
    """"""
    logger.error(""""""
      Error: Task failed
        Function Name: {}
        Task ID: {}
        Error Message: \n{}
    """""".format(task_status[""function_name""], task_status[""operationid""],
               task_status[""error_message""]))


def error_info(worker=global_worker):
    """"""Return information about failed tasks.""""""
    worker.check_connected()
    return (global_state.error_messages(job_id=worker.task_driver_id) +
            global_state.error_messages(job_id=DriverID.nil()))


def _initialize_serialization(driver_id, worker=global_worker):
    """"""Initialize the serialization library.

    This defines a custom serializer for object IDs and also tells ray to
    serialize several exception classes that we define for error handling.
    """"""
    serialization_context = pyarrow.default_serialization_context()
    # Tell the serialization context to use the cloudpickle version that we
    # ship with Ray.
    serialization_context.set_pickle(pickle.dumps, pickle.loads)
    pyarrow.register_torch_serialization_handlers(serialization_context)

    for id_type in ray._ID_TYPES:
        serialization_context.register_type(
            id_type,
            ""{}.{}"".format(id_type.__module__, id_type.__name__),
            pickle=True)

    def actor_handle_serializer(obj):
        return obj._serialization_helper(True)

    def actor_handle_deserializer(serialized_obj):
        new_handle = ray.actor.ActorHandle.__new__(ray.actor.ActorHandle)
        new_handle._deserialization_helper(serialized_obj, True)
        return new_handle

    # We register this serializer on each worker instead of calling
    # register_custom_serializer from the driver so that isinstance still
    # works.
    serialization_context.register_type(
        ray.actor.ActorHandle,
        ""ray.ActorHandle"",
        pickle=False,
        custom_serializer=actor_handle_serializer,
        custom_deserializer=actor_handle_deserializer)

    worker.serialization_context_map[driver_id] = serialization_context

    register_custom_serializer(
        RayTaskError,
        use_dict=True,
        local=True,
        driver_id=driver_id,
        class_id=""ray.RayTaskError"")
    # Tell Ray to serialize lambdas with pickle.
    register_custom_serializer(
        type(lambda: 0),
        use_pickle=True,
        local=True,
        driver_id=driver_id,
        class_id=""lambda"")
    # Tell Ray to serialize types with pickle.
    register_custom_serializer(
        type(int),
        use_pickle=True,
        local=True,
        driver_id=driver_id,
        class_id=""type"")
    # Tell Ray to serialize FunctionSignatures as dictionaries. This is
    # used when passing around actor handles.
    register_custom_serializer(
        ray.signature.FunctionSignature,
        use_dict=True,
        local=True,
        driver_id=driver_id,
        class_id=""ray.signature.FunctionSignature"")


def get_address_info_from_redis_helper(redis_address,
                                       node_ip_address,
                                       redis_password=None):
    redis_ip_address, redis_port = redis_address.split("":"")
    # For this command to work, some other client (on the same machine as
    # Redis) must have run ""CONFIG SET protected-mode no"".
    redis_client = redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=redis_password)

    client_table = ray.experimental.state.parse_client_table(redis_client)
    if len(client_table) == 0:
        raise Exception(
            ""Redis has started but no raylets have registered yet."")

    relevant_client = None
    for client_info in client_table:
        client_node_ip_address = client_info[""NodeManagerAddress""]
        if (client_node_ip_address == node_ip_address or
            (client_node_ip_address == ""127.0.0.1""
             and redis_ip_address == ray.services.get_node_ip_address())):
            relevant_client = client_info
            break
    if relevant_client is None:
        raise Exception(
            ""Redis has started but no raylets have registered yet."")

    return {
        ""node_ip_address"": node_ip_address,
        ""redis_address"": redis_address,
        ""object_store_address"": relevant_client[""ObjectStoreSocketName""],
        ""raylet_socket_name"": relevant_client[""RayletSocketName""],
        # Web UI should be running.
        ""webui_url"": _webui_url_helper(redis_client)
    }


def get_address_info_from_redis(redis_address,
                                node_ip_address,
                                num_retries=5,
                                redis_password=None):
    counter = 0
    while True:
        try:
            return get_address_info_from_redis_helper(
                redis_address, node_ip_address, redis_password=redis_password)
        except Exception:
            if counter == num_retries:
                raise
            # Some of the information may not be in Redis yet, so wait a little
            # bit.
            logger.warning(
                ""Some processes that the driver needs to connect to have ""
                ""not registered with Redis, so retrying. Have you run ""
                ""'ray start' on this node?"")
            time.sleep(1)
        counter += 1


def init(redis_address=None,
         num_cpus=None,
         num_gpus=None,
         resources=None,
         object_store_memory=None,
         redis_max_memory=None,
         log_to_driver=True,
         node_ip_address=None,
         object_id_seed=None,
         num_workers=None,
         local_mode=False,
         driver_mode=None,
         redirect_worker_output=True,
         redirect_output=True,
         ignore_reinit_error=False,
         num_redis_shards=None,
         redis_max_clients=None,
         redis_password=None,
         plasma_directory=None,
         huge_pages=False,
         include_webui=True,
         driver_id=None,
         configure_logging=True,
         logging_level=logging.INFO,
         logging_format=ray_constants.LOGGER_FORMAT,
         plasma_store_socket_name=None,
         raylet_socket_name=None,
         temp_dir=None,
         _internal_config=None,
         use_raylet=None):
    """"""Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases. Either a Ray cluster already exists and we
    just attach this driver to it, or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(redis_address=""123.45.67.89:6379"")

    Args:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        num_cpus (int): Number of cpus the user wishes all local schedulers to
            be configured with.
        num_gpus (int): Number of gpus the user wishes all local schedulers to
            be configured with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is capped at 20GB but can be
            set higher.
        redis_max_memory: The max amount of memory (in bytes) to allow each
            redis shard to use. Once the limit is exceeded, redis will start
            LRU eviction of entries. This only applies to the sharded redis
            tables (task, object, and profile tables). By default, this is
            capped at 10GB but can be set higher.
        log_to_driver (bool): If true, then output from all of the worker
            processes on all nodes will be directed to the driver.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        ignore_reinit_error: True if we should suppress errors from calling
            ray.init() a second time.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        driver_id: The ID of driver.
        configure_logging: True if allow the logging cofiguration here.
            Otherwise, the users may want to configure it by their own.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    if configure_logging:
        setup_logger(logging_level, logging_format)

    # Add the use_raylet option for backwards compatibility.
    if use_raylet is not None:
        if use_raylet:
            logger.warning(""WARNING: The use_raylet argument has been ""
                           ""deprecated. Please remove it."")
        else:
            raise DeprecationWarning(""The use_raylet argument is deprecated. ""
                                     ""Please remove it."")

    if driver_mode is not None:
        raise Exception(""The 'driver_mode' argument has been deprecated. ""
                        ""To run Ray in local mode, pass in local_mode=True."")
    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if setproctitle is None:
        logger.warning(
            ""WARNING: Not updating worker name since `setproctitle` is not ""
            ""installed. Install this with `pip install setproctitle` ""
            ""(or ray[debug]) to enable monitoring of worker processes."")

    if global_worker.connected:
        if ignore_reinit_error:
            logger.error(""Calling ray.init() again after it has already been ""
                         ""called."")
            return
        else:
            raise Exception(""Perhaps you called ray.init twice by accident? ""
                            ""This error can be suppressed by passing in ""
                            ""'ignore_reinit_error=True' or by calling ""
                            ""'ray.shutdown()' prior to 'ray.init()'."")

    # Convert hostnames to numerical IP address.
    if node_ip_address is not None:
        node_ip_address = services.address_to_ip(node_ip_address)
    if redis_address is not None:
        redis_address = services.address_to_ip(redis_address)

    address_info = {
        ""node_ip_address"": node_ip_address,
        ""redis_address"": redis_address
    }

    if driver_mode == LOCAL_MODE:
        # If starting Ray in LOCAL_MODE, don't start any other processes.
        pass
    elif redis_address is None:
        if node_ip_address is None:
            node_ip_address = ray.services.get_node_ip_address()
        if num_redis_shards is None:
            num_redis_shards = 1
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            num_workers=num_workers,
            object_id_seed=object_id_seed,
            local_mode=local_mode,
            driver_mode=driver_mode,
            redirect_worker_output=redirect_worker_output,
            redirect_output=redirect_output,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=num_redis_shards,
            redis_max_clients=redis_max_clients,
            redis_password=redis_password,
            plasma_directory=plasma_directory,
            huge_pages=huge_pages,
            include_webui=include_webui,
            object_store_memory=object_store_memory,
            redis_max_memory=redis_max_memory,
            plasma_store_socket_name=plasma_store_socket_name,
            raylet_socket_name=raylet_socket_name,
            temp_dir=temp_dir,
            _internal_config=_internal_config,
        )
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler.
        global _global_node
        _global_node = ray.node.Node(
            head=True, shutdown_at_exit=False, ray_params=ray_params)
        address_info[""redis_address""] = _global_node.redis_address
        address_info[
            ""object_store_address""] = _global_node.plasma_store_socket_name
        address_info[""webui_url""] = _global_node.webui_url
        address_info[""raylet_socket_name""] = _global_node.raylet_socket_name
    else:
        # In this case, we are connecting to an existing cluster.
        if num_workers is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""num_workers must not be provided."")
        if num_cpus is not None or num_gpus is not None:
            raise Exception(""When connecting to an existing cluster, num_cpus ""
                            ""and num_gpus must not be provided."")
        if resources is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""resources must not be provided."")
        if num_redis_shards is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""num_redis_shards must not be provided."")
        if redis_max_clients is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""redis_max_clients must not be provided."")
        if object_store_memory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""object_store_memory must not be provided."")
        if redis_max_memory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""redis_max_memory must not be provided."")
        if plasma_directory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""plasma_directory must not be provided."")
        if huge_pages:
            raise Exception(""When connecting to an existing cluster, ""
                            ""huge_pages must not be provided."")
        if temp_dir is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""temp_dir must not be provided."")
        if plasma_store_socket_name is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""plasma_store_socket_name must not be provided."")
        if raylet_socket_name is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""raylet_socket_name must not be provided."")
        if _internal_config is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""_internal_config must not be provided."")

        # Get the node IP address if one is not provided.

        if node_ip_address is None:
            node_ip_address = services.get_node_ip_address(redis_address)
        # Get the address info of the processes to connect to from Redis.
        address_info = get_address_info_from_redis(
            redis_address, node_ip_address, redis_password=redis_password)

    if driver_mode == LOCAL_MODE:
        driver_address_info = {}
    else:
        driver_address_info = {
            ""node_ip_address"": node_ip_address,
            ""redis_address"": address_info[""redis_address""],
            ""store_socket_name"": address_info[""object_store_address""],
            ""webui_url"": address_info[""webui_url""],
            ""raylet_socket_name"": address_info[""raylet_socket_name""],
        }

    # We only pass `temp_dir` to a worker (WORKER_MODE).
    # It can't be a worker here.
    connect(
        driver_address_info,
        redis_password=redis_password,
        object_id_seed=object_id_seed,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_id=driver_id)

    for hook in _post_init_hooks:
        hook()

    return address_info


# Functions to run as callback after a successful ray init
_post_init_hooks = []


def cleanup(worker=global_worker):
    raise DeprecationWarning(
        ""The function ray.worker.cleanup() has been deprecated. Instead, ""
        ""please call ray.shutdown()."")


def shutdown(worker=global_worker):
    """"""Disconnect the worker, and terminate processes started by ray.init().

    This will automatically run at the end when a Python process that uses Ray
    exits. It is ok to run this twice in a row. The primary use case for this
    function is to cleanup state between tests.

    Note that this will clear any remote function definitions, actor
    definitions, and existing actors, so if you wish to use any previously
    defined remote functions or actors after calling ray.shutdown(), then you
    need to redefine them. If they were defined in an imported module, then you
    will need to reload the module.
    """"""
    disconnect(worker)

    # Shut down the Ray processes.
    global _global_node
    if _global_node is not None:
        _global_node.kill_all_processes(check_alive=False, allow_graceful=True)
        _global_node = None

    worker.set_mode(None)


atexit.register(shutdown)

# Define a custom excepthook so that if the driver exits with an exception, we
# can push that exception to Redis.
normal_excepthook = sys.excepthook


def custom_excepthook(type, value, tb):
    # If this is a driver, push the exception to redis.
    if global_worker.mode == SCRIPT_MODE:
        error_message = """".join(traceback.format_tb(tb))
        global_worker.redis_client.hmset(b""Drivers:"" + global_worker.worker_id,
                                         {""exception"": error_message})
    # Call the normal excepthook.
    normal_excepthook(type, value, tb)


sys.excepthook = custom_excepthook

# The last time we raised a TaskError in this process. We use this value to
# suppress redundant error messages pushed from the workers.
last_task_error_raise_time = 0

# The max amount of seconds to wait before printing out an uncaught error.
UNCAUGHT_ERROR_GRACE_PERIOD = 5


def print_logs(redis_client, threads_stopped):
    """"""Prints log messages from workers on all of the nodes.

    Args:
        redis_client: A client to the primary Redis shard.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""
    pubsub_client = redis_client.pubsub(ignore_subscribe_messages=True)
    pubsub_client.subscribe(ray.gcs_utils.LOG_FILE_CHANNEL)
    try:
        # Keep track of the number of consecutive log messages that have been
        # received with no break in between. If this number grows continually,
        # then the worker is probably not able to process the log messages as
        # rapidly as they are coming in.
        num_consecutive_messages_received = 0
        while True:
            # Exit if we received a signal that we should stop.
            if threads_stopped.is_set():
                return

            msg = pubsub_client.get_message()
            if msg is None:
                num_consecutive_messages_received = 0
                threads_stopped.wait(timeout=0.01)
                continue
            num_consecutive_messages_received += 1
            print(ray.utils.decode(msg[""data""]), file=sys.stderr)

            if (num_consecutive_messages_received % 100 == 0
                    and num_consecutive_messages_received > 0):
                logger.warning(
                    ""The driver may not be able to keep up with the ""
                    ""stdout/stderr of the workers. To avoid forwarding logs ""
                    ""to the driver, use 'ray.init(log_to_driver=False)'."")
    finally:
        # Close the pubsub client to avoid leaking file descriptors.
        pubsub_client.close()


def print_error_messages_raylet(task_error_queue, threads_stopped):
    """"""Prints message received in the given output queue.

    This checks periodically if any un-raised errors occured in the background.

    Args:
        task_error_queue (queue.Queue): A queue used to receive errors from the
            thread that listens to Redis.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    while True:
        # Exit if we received a signal that we should stop.
        if threads_stopped.is_set():
            return

        try:
            error, t = task_error_queue.get(block=False)
        except queue.Empty:
            threads_stopped.wait(timeout=0.01)
            continue
        # Delay errors a little bit of time to attempt to suppress redundant
        # messages originating from the worker.
        while t + UNCAUGHT_ERROR_GRACE_PERIOD > time.time():
            threads_stopped.wait(timeout=1)
        if t < last_task_error_raise_time + UNCAUGHT_ERROR_GRACE_PERIOD:
            logger.debug(""Suppressing error from worker: {}"".format(error))
        else:
            logger.error(
                ""Possible unhandled error from worker: {}"".format(error))


def listen_error_messages_raylet(worker, task_error_queue, threads_stopped):
    """"""Listen to error messages in the background on the driver.

    This runs in a separate thread on the driver and pushes (error, time)
    tuples to the output queue.

    Args:
        worker: The worker class that this thread belongs to.
        task_error_queue (queue.Queue): A queue used to communicate with the
            thread that prints the errors found by this thread.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""
    worker.error_message_pubsub_client = worker.redis_client.pubsub(
        ignore_subscribe_messages=True)
    # Exports that are published after the call to
    # error_message_pubsub_client.subscribe and before the call to
    # error_message_pubsub_client.listen will still be processed in the loop.

    # Really we should just subscribe to the errors for this specific job.
    # However, currently all errors seem to be published on the same channel.
    error_pubsub_channel = str(
        ray.gcs_utils.TablePubsub.ERROR_INFO).encode(""ascii"")
    worker.error_message_pubsub_client.subscribe(error_pubsub_channel)
    # worker.error_message_pubsub_client.psubscribe(""*"")

    try:
        # Get the exports that occurred before the call to subscribe.
        with worker.lock:
            error_messages = global_state.error_messages(worker.task_driver_id)
            for error_message in error_messages:
                logger.error(error_message)

        while True:
            # Exit if we received a signal that we should stop.
            if threads_stopped.is_set():
                return

            msg = worker.error_message_pubsub_client.get_message()
            if msg is None:
                threads_stopped.wait(timeout=0.01)
                continue
            gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
                msg[""data""], 0)
            assert gcs_entry.EntriesLength() == 1
            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(
                gcs_entry.Entries(0), 0)
            job_id = error_data.JobId()
            if job_id not in [
                    worker.task_driver_id.binary(),
                    DriverID.nil().binary()
            ]:
                continue

            error_message = ray.utils.decode(error_data.ErrorMessage())
            if (ray.utils.decode(
                    error_data.Type()) == ray_constants.TASK_PUSH_ERROR):
                # Delay it a bit to see if we can suppress it
                task_error_queue.put((error_message, time.time()))
            else:
                logger.error(error_message)
    finally:
        # Close the pubsub client to avoid leaking file descriptors.
        worker.error_message_pubsub_client.close()


def is_initialized():
    """"""Check if ray.init has been called yet.

    Returns:
        True if ray.init has already been called and false otherwise.
    """"""
    return ray.worker.global_worker.connected


def connect(info,
            redis_password=None,
            object_id_seed=None,
            mode=WORKER_MODE,
            log_to_driver=False,
            worker=global_worker,
            driver_id=None):
    """"""Connect this worker to the local scheduler, to Plasma, and to Redis.

    Args:
        info (dict): A dictionary with address of the Redis server and the
            sockets of the plasma store and raylet.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        mode: The mode of the worker. One of SCRIPT_MODE, WORKER_MODE, and
            LOCAL_MODE.
        log_to_driver (bool): If true, then output from all of the worker
            processes on all nodes will be directed to the driver.
        worker: The ray.Worker instance.
        driver_id: The ID of driver. If it's None, then we will generate one.
    """"""
    # Do some basic checking to make sure we didn't call ray.init twice.
    error_message = ""Perhaps you called ray.init twice by accident?""
    assert not worker.connected, error_message
    assert worker.cached_functions_to_run is not None, error_message

    # Enable nice stack traces on SIGSEGV etc.
    if not faulthandler.is_enabled():
        faulthandler.enable(all_threads=False)

    worker.profiler = profiling.Profiler(worker, worker.threads_stopped)

    # Initialize some fields.
    if mode is WORKER_MODE:
        worker.worker_id = random_string()
        if setproctitle:
            setproctitle.setproctitle(""ray_worker"")
    else:
        # This is the code path of driver mode.
        if driver_id is None:
            driver_id = DriverID(random_string())

        if not isinstance(driver_id, DriverID):
            raise Exception(""The type of given driver id must be DriverID."")

        worker.worker_id = driver_id.binary()

    # When tasks are executed on remote workers in the context of multiple
    # drivers, the task driver ID is used to keep track of which driver is
    # responsible for the task so that error messages will be propagated to
    # the correct driver.
    if mode != WORKER_MODE:
        worker.task_driver_id = DriverID(worker.worker_id)

    # All workers start out as non-actors. A worker can be turned into an actor
    # after it is created.
    worker.actor_id = ActorID.nil()
    worker.connected = True
    worker.set_mode(mode)

    # If running Ray in LOCAL_MODE, there is no need to create call
    # create_worker or to start the worker service.
    if mode == LOCAL_MODE:
        return
    # Set the node IP address.
    worker.node_ip_address = info[""node_ip_address""]
    worker.redis_address = info[""redis_address""]

    # Create a Redis client.
    redis_ip_address, redis_port = info[""redis_address""].split("":"")
    # The Redis client can safely be shared between threads. However, that is
    # not true of Redis pubsub clients. See the documentation at
    # https://github.com/andymccurdy/redis-py#thread-safety.
    worker.redis_client = redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=redis_password)

    # For driver's check that the version information matches the version
    # information that the Ray cluster was started with.
    try:
        ray.services.check_version_info(worker.redis_client)
    except Exception as e:
        if mode == SCRIPT_MODE:
            raise e
        elif mode == WORKER_MODE:
            traceback_str = traceback.format_exc()
            ray.utils.push_error_to_driver_through_redis(
                worker.redis_client,
                ray_constants.VERSION_MISMATCH_PUSH_ERROR,
                traceback_str,
                driver_id=None)

    worker.lock = threading.Lock()

    # Create an object for interfacing with the global state.
    global_state._initialize_global_state(
        redis_ip_address, int(redis_port), redis_password=redis_password)

    # Register the worker with Redis.
    if mode == SCRIPT_MODE:
        # The concept of a driver is the same as the concept of a ""job"".
        # Register the driver/job with Redis here.
        import __main__ as main
        driver_info = {
            ""node_ip_address"": worker.node_ip_address,
            ""driver_id"": worker.worker_id,
            ""start_time"": time.time(),
            ""plasma_store_socket"": info[""store_socket_name""],
            ""raylet_socket"": info.get(""raylet_socket_name""),
            ""name"": (main.__file__
                     if hasattr(main, ""__file__"") else ""INTERACTIVE MODE"")
        }
        worker.redis_client.hmset(b""Drivers:"" + worker.worker_id, driver_info)
        if (not worker.redis_client.exists(""webui"")
                and info[""webui_url""] is not None):
            worker.redis_client.hmset(""webui"", {""url"": info[""webui_url""]})
        is_worker = False
    elif mode == WORKER_MODE:
        # Check the RedirectOutput key in Redis and based on its value redirect
        # worker output and error to their own files.
        # This key is set in services.py when Redis is started.
        redirect_worker_output_val = worker.redis_client.get(""RedirectOutput"")
        if (redirect_worker_output_val is not None
                and int(redirect_worker_output_val) == 1):
            redirect_worker_output = 1
        else:
            redirect_worker_output = 0
        if redirect_worker_output:
            log_stdout_file, log_stderr_file = (
                tempfile_services.new_worker_redirected_log_file(
                    worker.worker_id))
            # Redirect stdout/stderr at the file descriptor level. If we simply
            # set sys.stdout and sys.stderr, then logging from C++ can fail to
            # be redirected.
            os.dup2(log_stdout_file.fileno(), sys.stdout.fileno())
            os.dup2(log_stderr_file.fileno(), sys.stderr.fileno())
            # This should always be the first message to appear in the worker's
            # stdout and stderr log files. The string ""Ray worker pid:"" is
            # parsed in the log monitor process.
            print(""Ray worker pid: {}"".format(os.getpid()))
            print(""Ray worker pid: {}"".format(os.getpid()), file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

        # Register the worker with Redis.
        worker_dict = {
            ""node_ip_address"": worker.node_ip_address,
            ""plasma_store_socket"": info[""store_socket_name""],
        }
        if redirect_worker_output:
            worker_dict[""stdout_file""] = os.path.abspath(log_stdout_file.name)
            worker_dict[""stderr_file""] = os.path.abspath(log_stderr_file.name)
        worker.redis_client.hmset(b""Workers:"" + worker.worker_id, worker_dict)
        is_worker = True
    else:
        raise Exception(""This code should be unreachable."")

    # Create an object store client.
    worker.plasma_client = thread_safe_client(
        plasma.connect(info[""store_socket_name""], None, 0, 300))

    raylet_socket = info[""raylet_socket_name""]

    # If this is a driver, set the current task ID, the task driver ID, and set
    # the task index to 0.
    if mode == SCRIPT_MODE:
        # If the user provided an object_id_seed, then set the current task ID
        # deterministically based on that seed (without altering the state of
        # the user's random number generator). Otherwise, set the current task
        # ID randomly to avoid object ID collisions.
        numpy_state = np.random.get_state()
        if object_id_seed is not None:
            np.random.seed(object_id_seed)
        else:
            # Try to use true randomness.
            np.random.seed(None)
        # Reset the state of the numpy random number generator.
        np.random.set_state(numpy_state)

        # Create an entry for the driver task in the task table. This task is
        # added immediately with status RUNNING. This allows us to push errors
        # related to this driver task back to the driver.  For example, if the
        # driver creates an object that is later evicted, we should notify the
        # user that we're unable to reconstruct the object, since we cannot
        # rerun the driver.
        nil_actor_counter = 0

        function_descriptor = FunctionDescriptor.for_driver_task()
        driver_task = ray._raylet.Task(
            worker.task_driver_id,
            function_descriptor.get_function_descriptor_list(),
            [],  # arguments.
            0,  # num_returns.
            TaskID(random_string()),  # parent_task_id.
            0,  # parent_counter.
            ActorID.nil(),  # actor_creation_id.
            ObjectID.nil(),  # actor_creation_dummy_object_id.
            0,  # max_actor_reconstructions.
            ActorID.nil(),  # actor_id.
            ActorHandleID.nil(),  # actor_handle_id.
            nil_actor_counter,  # actor_counter.
            [],  # new_actor_handles.
            [],  # execution_dependencies.
            {""CPU"": 0},  # resource_map.
            {},  # placement_resource_map.
        )

        # Add the driver task to the task table.
        global_state._execute_command(driver_task.task_id(), ""RAY.TABLE_ADD"",
                                      ray.gcs_utils.TablePrefix.RAYLET_TASK,
                                      ray.gcs_utils.TablePubsub.RAYLET_TASK,
                                      driver_task.task_id().binary(),
                                      driver_task._serialized_raylet_task())

        # Set the driver's current task ID to the task ID assigned to the
        # driver task.
        worker.task_context.current_task_id = driver_task.task_id()

    worker.raylet_client = ray._raylet.RayletClient(
        raylet_socket,
        ClientID(worker.worker_id),
        is_worker,
        DriverID(worker.current_task_id.binary()),
    )

    # Start the import thread
    worker.import_thread = import_thread.ImportThread(worker, mode,
                                                      worker.threads_stopped)
    worker.import_thread.start()

    # If this is a driver running in SCRIPT_MODE, start a thread to print error
    # messages asynchronously in the background. Ideally the scheduler would
    # push messages to the driver's worker service, but we ran into bugs when
    # trying to properly shutdown the driver's worker service, so we are
    # temporarily using this implementation which constantly queries the
    # scheduler for new error messages.
    if mode == SCRIPT_MODE:
        q = queue.Queue()
        worker.listener_thread = threading.Thread(
            target=listen_error_messages_raylet,
            name=""ray_listen_error_messages"",
            args=(worker, q, worker.threads_stopped))
        worker.printer_thread = threading.Thread(
            target=print_error_messages_raylet,
            name=""ray_print_error_messages"",
            args=(q, worker.threads_stopped))
        worker.listener_thread.daemon = True
        worker.listener_thread.start()
        worker.printer_thread.daemon = True
        worker.printer_thread.start()
        if log_to_driver:
            worker.logger_thread = threading.Thread(
                target=print_logs,
                name=""ray_print_logs"",
                args=(worker.redis_client, worker.threads_stopped))
            worker.logger_thread.daemon = True
            worker.logger_thread.start()

    # If we are using the raylet code path and we are not in local mode, start
    # a background thread to periodically flush profiling data to the GCS.
    if mode != LOCAL_MODE:
        worker.profiler.start_flush_thread()

    if mode == SCRIPT_MODE:
        # Add the directory containing the script that is running to the Python
        # paths of the workers. Also add the current directory. Note that this
        # assumes that the directory structures on the machines in the clusters
        # are the same.
        script_directory = os.path.abspath(os.path.dirname(sys.argv[0]))
        current_directory = os.path.abspath(os.path.curdir)
        worker.run_function_on_all_workers(
            lambda worker_info: sys.path.insert(1, script_directory))
        worker.run_function_on_all_workers(
            lambda worker_info: sys.path.insert(1, current_directory))
        # TODO(rkn): Here we first export functions to run, then remote
        # functions. The order matters. For example, one of the functions to
        # run may set the Python path, which is needed to import a module used
        # to define a remote function. We may want to change the order to
        # simply be the order in which the exports were defined on the driver.
        # In addition, we will need to retain the ability to decide what the
        # first few exports are (mostly to set the Python path). Additionally,
        # note that the first exports to be defined on the driver will be the
        # ones defined in separate modules that are imported by the driver.
        # Export cached functions_to_run.
        for function in worker.cached_functions_to_run:
            worker.run_function_on_all_workers(function)
        # Export cached remote functions and actors to the workers.
        worker.function_actor_manager.export_cached()
    worker.cached_functions_to_run = None


def disconnect(worker=global_worker):
    """"""Disconnect this worker from the scheduler and object store.""""""
    # Reset the list of cached remote functions and actors so that if more
    # remote functions or actors are defined and then connect is called again,
    # the remote functions will be exported. This is mostly relevant for the
    # tests.
    if worker.connected:
        # Shutdown all of the threads that we've started. TODO(rkn): This
        # should be handled cleanly in the worker object's destructor and not
        # in this disconnect method.
        worker.threads_stopped.set()
        if hasattr(worker, ""import_thread""):
            worker.import_thread.join_import_thread()
        if hasattr(worker, ""profiler"") and hasattr(worker.profiler, ""t""):
            worker.profiler.join_flush_thread()
        if hasattr(worker, ""listener_thread""):
            worker.listener_thread.join()
        if hasattr(worker, ""printer_thread""):
            worker.printer_thread.join()
        if hasattr(worker, ""logger_thread""):
            worker.logger_thread.join()
        worker.threads_stopped.clear()

    worker.connected = False
    worker.cached_functions_to_run = []
    worker.function_actor_manager.reset_cache()
    worker.serialization_context_map.clear()

    if hasattr(worker, ""raylet_client""):
        del worker.raylet_client
    if hasattr(worker, ""plasma_client""):
        worker.plasma_client.disconnect()


@contextmanager
def _changeproctitle(title, next_title):
    if setproctitle:
        setproctitle.setproctitle(title)
    yield
    if setproctitle:
        setproctitle.setproctitle(next_title)


def _try_to_compute_deterministic_class_id(cls, depth=5):
    """"""Attempt to produce a deterministic class ID for a given class.

    The goal here is for the class ID to be the same when this is run on
    different worker processes. Pickling, loading, and pickling again seems to
    produce more consistent results than simply pickling. This is a bit crazy
    and could cause problems, in which case we should revert it and figure out
    something better.

    Args:
        cls: The class to produce an ID for.
        depth: The number of times to repeatedly try to load and dump the
            string while trying to reach a fixed point.

    Returns:
        A class ID for this class. We attempt to make the class ID the same
            when this function is run on different workers, but that is not
            guaranteed.

    Raises:
        Exception: This could raise an exception if cloudpickle raises an
            exception.
    """"""
    # Pickling, loading, and pickling again seems to produce more consistent
    # results than simply pickling. This is a bit
    class_id = pickle.dumps(cls)
    for _ in range(depth):
        new_class_id = pickle.dumps(pickle.loads(class_id))
        if new_class_id == class_id:
            # We appear to have reached a fix point, so use this as the ID.
            return hashlib.sha1(new_class_id).digest()
        class_id = new_class_id

    # We have not reached a fixed point, so we may end up with a different
    # class ID for this custom class on each worker, which could lead to the
    # same class definition being exported many many times.
    logger.warning(
        ""WARNING: Could not produce a deterministic class ID for class ""
        ""{}"".format(cls))
    return hashlib.sha1(new_class_id).digest()


def register_custom_serializer(cls,
                               use_pickle=False,
                               use_dict=False,
                               serializer=None,
                               deserializer=None,
                               local=False,
                               driver_id=None,
                               class_id=None,
                               worker=global_worker):
    """"""Enable serialization and deserialization for a particular class.

    This method runs the register_class function defined below on every worker,
    which will enable ray to properly serialize and deserialize objects of
    this class.

    Args:
        cls (type): The class that ray should use this custom serializer for.
        use_pickle (bool): If true, then objects of this class will be
            serialized using pickle.
        use_dict: If true, then objects of this class be serialized turning
            their __dict__ fields into a dictionary. Must be False if
            use_pickle is true.
        serializer: The custom serializer to use. This should be provided if
            and only if use_pickle and use_dict are False.
        deserializer: The custom deserializer to use. This should be provided
            if and only if use_pickle and use_dict are False.
        local: True if the serializers should only be registered on the current
            worker. This should usually be False.
        driver_id: ID of the driver that we want to register the class for.
        class_id: ID of the class that we are registering. If this is not
            specified, we will calculate a new one inside the function.

    Raises:
        Exception: An exception is raised if pickle=False and the class cannot
            be efficiently serialized by Ray. This can also raise an exception
            if use_dict is true and cls is not pickleable.
    """"""
    assert (serializer is None) == (deserializer is None), (
        ""The serializer/deserializer arguments must both be provided or ""
        ""both not be provided."")
    use_custom_serializer = (serializer is not None)

    assert use_custom_serializer + use_pickle + use_dict == 1, (
        ""Exactly one of use_pickle, use_dict, or serializer/deserializer must ""
        ""be specified."")

    if use_dict:
        # Raise an exception if cls cannot be serialized efficiently by Ray.
        serialization.check_serializable(cls)

    if class_id is None:
        if not local:
            # In this case, the class ID will be used to deduplicate the class
            # across workers. Note that cloudpickle unfortunately does not
            # produce deterministic strings, so these IDs could be different
            # on different workers. We could use something weaker like
            # cls.__name__, however that would run the risk of having
            # collisions.
            # TODO(rkn): We should improve this.
            try:
                # Attempt to produce a class ID that will be the same on each
                # worker. However, determinism is not guaranteed, and the
                # result may be different on different workers.
                class_id = _try_to_compute_deterministic_class_id(cls)
            except Exception:
                raise serialization.CloudPickleError(""Failed to pickle class ""
                                                     ""'{}'"".format(cls))
        else:
            # In this case, the class ID only needs to be meaningful on this
            # worker and not across workers.
            class_id = random_string()

        # Make sure class_id is a string.
        class_id = ray.utils.binary_to_hex(class_id)

    if driver_id is None:
        driver_id = worker.task_driver_id
    assert isinstance(driver_id, DriverID)

    def register_class_for_serialization(worker_info):
        # TODO(rkn): We need to be more thoughtful about what to do if custom
        # serializers have already been registered for class_id. In some cases,
        # we may want to use the last user-defined serializers and ignore
        # subsequent calls to register_custom_serializer that were made by the
        # system.

        serialization_context = worker_info[
            ""worker""].get_serialization_context(driver_id)
        serialization_context.register_type(
            cls,
            class_id,
            pickle=use_pickle,
            custom_serializer=serializer,
            custom_deserializer=deserializer)

    if not local:
        worker.run_function_on_all_workers(register_class_for_serialization)
    else:
        # Since we are pickling objects of this class, we don't actually need
        # to ship the class definition.
        register_class_for_serialization({""worker"": worker})


def get(object_ids, worker=global_worker):
    """"""Get a remote object or a list of remote objects from the object store.

    This method blocks until the object corresponding to the object ID is
    available in the local object store. If this object is not in the local
    object store, it will be shipped from an object store that has it (once the
    object has been created). If object_ids is a list, then the objects
    corresponding to each object in the list will be returned.

    Args:
        object_ids: Object ID of the object to get or a list of object IDs to
            get.

    Returns:
        A Python object or a list of Python objects.

    Raises:
        Exception: An exception is raised if the task that created the object
            or that created one of the objects raised an exception.
    """"""
    worker.check_connected()
    with profiling.profile(""ray.get"", worker=worker):
        if worker.mode == LOCAL_MODE:
            # In LOCAL_MODE, ray.get is the identity operation (the input will
            # actually be a value not an objectid).
            return object_ids
        global last_task_error_raise_time
        if isinstance(object_ids, list):
            values = worker.get_object(object_ids)
            for i, value in enumerate(values):
                if isinstance(value, RayTaskError):
                    last_task_error_raise_time = time.time()
                    raise value
            return values
        else:
            value = worker.get_object([object_ids])[0]
            if isinstance(value, RayTaskError):
                # If the result is a RayTaskError, then the task that created
                # this object failed, and we should propagate the error message
                # here.
                last_task_error_raise_time = time.time()
                raise value
            return value


def put(value, worker=global_worker):
    """"""Store an object in the object store.

    Args:
        value: The Python object to be stored.

    Returns:
        The object ID assigned to this value.
    """"""
    worker.check_connected()
    with profiling.profile(""ray.put"", worker=worker):
        if worker.mode == LOCAL_MODE:
            # In LOCAL_MODE, ray.put is the identity operation.
            return value
        object_id = ray._raylet.compute_put_id(
            worker.current_task_id,
            worker.task_context.put_index,
        )
        worker.put_object(object_id, value)
        worker.task_context.put_index += 1
        return object_id


def wait(object_ids, num_returns=1, timeout=None, worker=global_worker):
    """"""Return a list of IDs that are ready and a list of IDs that are not.

    .. warning::

        The **timeout** argument used to be in **milliseconds** (up through
        ``ray==0.6.1``) and now it is in **seconds**.

    If timeout is set, the function returns either when the requested number of
    IDs are ready or when the timeout is reached, whichever occurs first. If it
    is not set, the function simply waits until that number of objects is ready
    and returns that exact number of object IDs.

    This method returns two lists. The first list consists of object IDs that
    correspond to objects that are available in the object store. The second
    list corresponds to the rest of the object IDs (which may or may not be
    ready).

    Ordering of the input list of object IDs is preserved. That is, if A
    precedes B in the input list, and both are in the ready list, then A will
    precede B in the ready list. This also holds true if A and B are both in
    the remaining list.

    Args:
        object_ids (List[ObjectID]): List of object IDs for objects that may or
            may not be ready. Note that these IDs must be unique.
        num_returns (int): The number of object IDs that should be returned.
        timeout (float): The maximum amount of time in seconds to wait before
            returning.

    Returns:
        A list of object IDs that are ready and a list of the remaining object
        IDs.
    """"""

    if isinstance(object_ids, ObjectID):
        raise TypeError(
            ""wait() expected a list of ray.ObjectID, got a single ray.ObjectID""
        )

    if not isinstance(object_ids, list):
        raise TypeError(
            ""wait() expected a list of ray.ObjectID, got {}"".format(
                type(object_ids)))

    if isinstance(timeout, int) and timeout != 0:
        logger.warning(""The 'timeout' argument now requires seconds instead ""
                       ""of milliseconds. This message can be suppressed by ""
                       ""passing in a float."")

    if timeout is not None and timeout < 0:
        raise ValueError(""The 'timeout' argument must be nonnegative. ""
                         ""Received {}"".format(timeout))

    if worker.mode != LOCAL_MODE:
        for object_id in object_ids:
            if not isinstance(object_id, ObjectID):
                raise TypeError(""wait() expected a list of ray.ObjectID, ""
                                ""got list containing {}"".format(
                                    type(object_id)))

    worker.check_connected()
    # TODO(swang): Check main thread.
    with profiling.profile(""ray.wait"", worker=worker):
        # When Ray is run in LOCAL_MODE, all functions are run immediately,
        # so all objects in object_id are ready.
        if worker.mode == LOCAL_MODE:
            return object_ids[:num_returns], object_ids[num_returns:]

        # TODO(rkn): This is a temporary workaround for
        # https://github.com/ray-project/ray/issues/997. However, it should be
        # fixed in Arrow instead of here.
        if len(object_ids) == 0:
            return [], []

        if len(object_ids) != len(set(object_ids)):
            raise Exception(""Wait requires a list of unique object IDs."")
        if num_returns <= 0:
            raise Exception(
                ""Invalid number of objects to return %d."" % num_returns)
        if num_returns > len(object_ids):
            raise Exception(""num_returns cannot be greater than the number ""
                            ""of objects provided to ray.wait."")

        timeout = timeout if timeout is not None else 10**6
        timeout_milliseconds = int(timeout * 1000)
        ready_ids, remaining_ids = worker.raylet_client.wait(
            object_ids,
            num_returns,
            timeout_milliseconds,
            False,
            worker.current_task_id,
        )
        return ready_ids, remaining_ids


def _mode(worker=global_worker):
    """"""This is a wrapper around worker.mode.

    We use this wrapper so that in the remote decorator, we can call _mode()
    instead of worker.mode. The difference is that when we attempt to serialize
    remote functions, we don't attempt to serialize the worker object, which
    cannot be serialized.
    """"""
    return worker.mode


def get_global_worker():
    return global_worker


def make_decorator(num_return_vals=None,
                   num_cpus=None,
                   num_gpus=None,
                   resources=None,
                   max_calls=None,
                   checkpoint_interval=None,
                   max_reconstructions=None,
                   worker=None):
    def decorator(function_or_class):
        if (inspect.isfunction(function_or_class)
                or is_cython(function_or_class)):
            # Set the remote function default resources.
            if checkpoint_interval is not None:
                raise Exception(""The keyword 'checkpoint_interval' is not ""
                                ""allowed for remote functions."")
            if max_reconstructions is not None:
                raise Exception(""The keyword 'max_reconstructions' is not ""
                                ""allowed for remote functions."")

            return ray.remote_function.RemoteFunction(
                function_or_class, num_cpus, num_gpus, resources,
                num_return_vals, max_calls)

        if inspect.isclass(function_or_class):
            if num_return_vals is not None:
                raise Exception(""The keyword 'num_return_vals' is not allowed ""
                                ""for actors."")
            if max_calls is not None:
                raise Exception(""The keyword 'max_calls' is not allowed for ""
                                ""actors."")

            # Set the actor default resources.
            if num_cpus is None and num_gpus is None and resources is None:
                # In the default case, actors acquire no resources for
                # their lifetime, and actor methods will require 1 CPU.
                cpus_to_use = DEFAULT_ACTOR_CREATION_CPUS_SIMPLE_CASE
                actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SIMPLE_CASE
            else:
                # If any resources are specified, then all resources are
                # acquired for the actor's lifetime and no resources are
                # associated with methods.
                cpus_to_use = (DEFAULT_ACTOR_CREATION_CPUS_SPECIFIED_CASE
                               if num_cpus is None else num_cpus)
                actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SPECIFIED_CASE

            return worker.make_actor(function_or_class, cpus_to_use, num_gpus,
                                     resources, actor_method_cpus,
                                     checkpoint_interval, max_reconstructions)

        raise Exception(""The @ray.remote decorator must be applied to ""
                        ""either a function or to a class."")

    return decorator


def remote(*args, **kwargs):
    """"""Define a remote function or an actor class.

    This can be used with no arguments to define a remote function or actor as
    follows:

    .. code-block:: python

        @ray.remote
        def f():
            return 1

        @ray.remote
        class Foo(object):
            def method(self):
                return 1

    It can also be used with specific keyword arguments:

    * **num_return_vals:** This is only for *remote functions*. It specifies
      the number of object IDs returned by the remote function invocation.
    * **num_cpus:** The quantity of CPU cores to reserve for this task or for
      the lifetime of the actor.
    * **num_gpus:** The quantity of GPUs to reserve for this task or for the
      lifetime of the actor.
    * **resources:** The quantity of various custom resources to reserve for
      this task or for the lifetime of the actor. This is a dictionary mapping
      strings (resource names) to numbers.
    * **max_calls:** Only for *remote functions*. This specifies the maximum
      number of times that a given worker can execute the given remote function
      before it must exit (this can be used to address memory leaks in
      third-party libraries or to reclaim resources that cannot easily be
      released, e.g., GPU memory that was acquired by TensorFlow). By
      default this is infinite.
    * **max_reconstructions**: Only for *actors*. This specifies the maximum
      number of times that the actor should be reconstructed when it dies
      unexpectedly. The minimum valid value is 0 (default), which indicates
      that the actor doesn't need to be reconstructed. And the maximum valid
      value is ray.ray_constants.INFINITE_RECONSTRUCTIONS.

    This can be done as follows:

    .. code-block:: python

        @ray.remote(num_gpus=1, max_calls=1, num_return_vals=2)
        def f():
            return 1, 2

        @ray.remote(num_cpus=2, resources={""CustomResource"": 1})
        class Foo(object):
            def method(self):
                return 1
    """"""
    worker = get_global_worker()

    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        # This is the case where the decorator is just @ray.remote.
        return make_decorator(worker=worker)(args[0])

    # Parse the keyword arguments from the decorator.
    error_string = (""The @ray.remote decorator must be applied either ""
                    ""with no arguments and no parentheses, for example ""
                    ""'@ray.remote', or it must be applied using some of ""
                    ""the arguments 'num_return_vals', 'num_cpus', 'num_gpus', ""
                    ""'resources', 'max_calls', 'checkpoint_interval',""
                    ""or 'max_reconstructions', like ""
                    ""'@ray.remote(num_return_vals=2, ""
                    ""resources={\""CustomResource\"": 1})'."")
    assert len(args) == 0 and len(kwargs) > 0, error_string
    for key in kwargs:
        assert key in [
            ""num_return_vals"", ""num_cpus"", ""num_gpus"", ""resources"",
            ""max_calls"", ""checkpoint_interval"", ""max_reconstructions""
        ], error_string

    num_cpus = kwargs[""num_cpus""] if ""num_cpus"" in kwargs else None
    num_gpus = kwargs[""num_gpus""] if ""num_gpus"" in kwargs else None
    resources = kwargs.get(""resources"")
    if not isinstance(resources, dict) and resources is not None:
        raise Exception(""The 'resources' keyword argument must be a ""
                        ""dictionary, but received type {}."".format(
                            type(resources)))
    if resources is not None:
        assert ""CPU"" not in resources, ""Use the 'num_cpus' argument.""
        assert ""GPU"" not in resources, ""Use the 'num_gpus' argument.""

    # Handle other arguments.
    num_return_vals = kwargs.get(""num_return_vals"")
    max_calls = kwargs.get(""max_calls"")
    checkpoint_interval = kwargs.get(""checkpoint_interval"")
    max_reconstructions = kwargs.get(""max_reconstructions"")

    return make_decorator(
        num_return_vals=num_return_vals,
        num_cpus=num_cpus,
        num_gpus=num_gpus,
        resources=resources,
        max_calls=max_calls,
        checkpoint_interval=checkpoint_interval,
        max_reconstructions=max_reconstructions,
        worker=worker)
/n/n/ntest/runtest.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import logging
import os
import random
import re
import setproctitle
import shutil
import string
import subprocess
import sys
import tempfile
import threading
import time
from collections import defaultdict, namedtuple, OrderedDict
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import pickle
import pytest

import ray
import ray.test.cluster_utils
import ray.test.test_utils
from ray.utils import _random_string

logger = logging.getLogger(__name__)


def assert_equal(obj1, obj2):
    module_numpy = (type(obj1).__module__ == np.__name__
                    or type(obj2).__module__ == np.__name__)
    if module_numpy:
        empty_shape = ((hasattr(obj1, ""shape"") and obj1.shape == ())
                       or (hasattr(obj2, ""shape"") and obj2.shape == ()))
        if empty_shape:
            # This is a special case because currently np.testing.assert_equal
            # fails because we do not properly handle different numerical
            # types.
            assert obj1 == obj2, (""Objects {} and {} are ""
                                  ""different."".format(obj1, obj2))
        else:
            np.testing.assert_equal(obj1, obj2)
    elif hasattr(obj1, ""__dict__"") and hasattr(obj2, ""__dict__""):
        special_keys = [""_pytype_""]
        assert (set(list(obj1.__dict__.keys()) + special_keys) == set(
            list(obj2.__dict__.keys()) + special_keys)), (""Objects {} ""
                                                          ""and {} are ""
                                                          ""different."".format(
                                                              obj1, obj2))
        for key in obj1.__dict__.keys():
            if key not in special_keys:
                assert_equal(obj1.__dict__[key], obj2.__dict__[key])
    elif type(obj1) is dict or type(obj2) is dict:
        assert_equal(obj1.keys(), obj2.keys())
        for key in obj1.keys():
            assert_equal(obj1[key], obj2[key])
    elif type(obj1) is list or type(obj2) is list:
        assert len(obj1) == len(obj2), (""Objects {} and {} are lists with ""
                                        ""different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    elif type(obj1) is tuple or type(obj2) is tuple:
        assert len(obj1) == len(obj2), (""Objects {} and {} are tuples with ""
                                        ""different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    elif (ray.serialization.is_named_tuple(type(obj1))
          or ray.serialization.is_named_tuple(type(obj2))):
        assert len(obj1) == len(obj2), (""Objects {} and {} are named tuples ""
                                        ""with different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    else:
        assert obj1 == obj2, ""Objects {} and {} are different."".format(
            obj1, obj2)


if sys.version_info >= (3, 0):
    long_extras = [0, np.array([[""hi"", u""hi""], [1.3, 1]])]
else:

    long_extras = [
        long(0),  # noqa: E501,F821
        np.array([
            [""hi"", u""hi""],
            [1.3, long(1)]  # noqa: E501,F821
        ])
    ]

PRIMITIVE_OBJECTS = [
    0, 0.0, 0.9, 1 << 62, 1 << 100, 1 << 999, [1 << 100, [1 << 100]], ""a"",
    string.printable, ""\u262F"", u""hello world"", u""\xff\xfe\x9c\x001\x000\x00"",
    None, True, False, [], (), {},
    np.int8(3),
    np.int32(4),
    np.int64(5),
    np.uint8(3),
    np.uint32(4),
    np.uint64(5),
    np.float32(1.9),
    np.float64(1.9),
    np.zeros([100, 100]),
    np.random.normal(size=[100, 100]),
    np.array([""hi"", 3]),
    np.array([""hi"", 3], dtype=object)
] + long_extras

COMPLEX_OBJECTS = [
    [[[[[[[[[[[[]]]]]]]]]]]],
    {""obj{}"".format(i): np.random.normal(size=[100, 100])
     for i in range(10)},
    # {(): {(): {(): {(): {(): {(): {(): {(): {(): {(): {
    #      (): {(): {}}}}}}}}}}}}},
    (
        (((((((((), ), ), ), ), ), ), ), ), ),
    {
        ""a"": {
            ""b"": {
                ""c"": {
                    ""d"": {}
                }
            }
        }
    }
]


class Foo(object):
    def __init__(self, value=0):
        self.value = value

    def __hash__(self):
        return hash(self.value)

    def __eq__(self, other):
        return other.value == self.value


class Bar(object):
    def __init__(self):
        for i, val in enumerate(PRIMITIVE_OBJECTS + COMPLEX_OBJECTS):
            setattr(self, ""field{}"".format(i), val)


class Baz(object):
    def __init__(self):
        self.foo = Foo()
        self.bar = Bar()

    def method(self, arg):
        pass


class Qux(object):
    def __init__(self):
        self.objs = [Foo(), Bar(), Baz()]


class SubQux(Qux):
    def __init__(self):
        Qux.__init__(self)


class CustomError(Exception):
    pass


Point = namedtuple(""Point"", [""x"", ""y""])
NamedTupleExample = namedtuple(""Example"",
                               ""field1, field2, field3, field4, field5"")

CUSTOM_OBJECTS = [
    Exception(""Test object.""),
    CustomError(),
    Point(11, y=22),
    Foo(),
    Bar(),
    Baz(),  # Qux(), SubQux(),
    NamedTupleExample(1, 1.0, ""hi"", np.zeros([3, 5]), [1, 2, 3])
]

BASE_OBJECTS = PRIMITIVE_OBJECTS + COMPLEX_OBJECTS + CUSTOM_OBJECTS

LIST_OBJECTS = [[obj] for obj in BASE_OBJECTS]
TUPLE_OBJECTS = [(obj, ) for obj in BASE_OBJECTS]
# The check that type(obj).__module__ != ""numpy"" should be unnecessary, but
# otherwise this seems to fail on Mac OS X on Travis.
DICT_OBJECTS = (
    [{
        obj: obj
    } for obj in PRIMITIVE_OBJECTS
     if (obj.__hash__ is not None and type(obj).__module__ != ""numpy"")] + [{
         0: obj
     } for obj in BASE_OBJECTS] + [{
         Foo(123): Foo(456)
     }])

RAY_TEST_OBJECTS = BASE_OBJECTS + LIST_OBJECTS + TUPLE_OBJECTS + DICT_OBJECTS


@pytest.fixture
def ray_start():
    # Start the Ray processes.
    ray.init(num_cpus=1)
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()


@pytest.fixture
def shutdown_only():
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()


def test_passing_arguments_by_value(ray_start):
    @ray.remote
    def f(x):
        return x

    # Check that we can pass arguments by value to remote functions and
    # that they are uncorrupted.
    for obj in RAY_TEST_OBJECTS:
        assert_equal(obj, ray.get(f.remote(obj)))


def test_ray_recursive_objects(ray_start):
    class ClassA(object):
        pass

    # Make a list that contains itself.
    lst = []
    lst.append(lst)
    # Make an object that contains itself as a field.
    a1 = ClassA()
    a1.field = a1
    # Make two objects that contain each other as fields.
    a2 = ClassA()
    a3 = ClassA()
    a2.field = a3
    a3.field = a2
    # Make a dictionary that contains itself.
    d1 = {}
    d1[""key""] = d1
    # Create a list of recursive objects.
    recursive_objects = [lst, a1, a2, a3, d1]

    # Check that exceptions are thrown when we serialize the recursive
    # objects.
    for obj in recursive_objects:
        with pytest.raises(Exception):
            ray.put(obj)


def test_passing_arguments_by_value_out_of_the_box(ray_start):
    @ray.remote
    def f(x):
        return x

    # Test passing lambdas.

    def temp():
        return 1

    assert ray.get(f.remote(temp))() == 1
    assert ray.get(f.remote(lambda x: x + 1))(3) == 4

    # Test sets.
    assert ray.get(f.remote(set())) == set()
    s = {1, (1, 2, ""hi"")}
    assert ray.get(f.remote(s)) == s

    # Test types.
    assert ray.get(f.remote(int)) == int
    assert ray.get(f.remote(float)) == float
    assert ray.get(f.remote(str)) == str

    class Foo(object):
        def __init__(self):
            pass

    # Make sure that we can put and get a custom type. Note that the result
    # won't be ""equal"" to Foo.
    ray.get(ray.put(Foo))


def test_putting_object_that_closes_over_object_id(ray_start):
    # This test is here to prevent a regression of
    # https://github.com/ray-project/ray/issues/1317.

    class Foo(object):
        def __init__(self):
            self.val = ray.put(0)

        def method(self):
            f

    f = Foo()
    ray.put(f)


def test_put_get(shutdown_only):
    ray.init(num_cpus=0)

    for i in range(100):
        value_before = i * 10**6
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = i * 10**6 * 1.0
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = ""h"" * i
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = [1] * i
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after


def test_custom_serializers(shutdown_only):
    ray.init(num_cpus=1)

    class Foo(object):
        def __init__(self):
            self.x = 3

    def custom_serializer(obj):
        return 3, ""string1"", type(obj).__name__

    def custom_deserializer(serialized_obj):
        return serialized_obj, ""string2""

    ray.register_custom_serializer(
        Foo, serializer=custom_serializer, deserializer=custom_deserializer)

    assert ray.get(ray.put(Foo())) == ((3, ""string1"", Foo.__name__), ""string2"")

    class Bar(object):
        def __init__(self):
            self.x = 3

    ray.register_custom_serializer(
        Bar, serializer=custom_serializer, deserializer=custom_deserializer)

    @ray.remote
    def f():
        return Bar()

    assert ray.get(f.remote()) == ((3, ""string1"", Bar.__name__), ""string2"")


def test_serialization_final_fallback(ray_start):
    pytest.importorskip(""catboost"")
    # This test will only run when ""catboost"" is installed.
    from catboost import CatBoostClassifier

    model = CatBoostClassifier(
        iterations=2,
        depth=2,
        learning_rate=1,
        loss_function=""Logloss"",
        logging_level=""Verbose"")

    reconstructed_model = ray.get(ray.put(model))
    assert set(model.get_params().items()) == set(
        reconstructed_model.get_params().items())


def test_register_class(shutdown_only):
    ray.init(num_cpus=2)

    # Check that putting an object of a class that has not been registered
    # throws an exception.
    class TempClass(object):
        pass

    ray.get(ray.put(TempClass()))

    # Test subtypes of dictionaries.
    value_before = OrderedDict([(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    value_before = defaultdict(lambda: 0, [(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    value_before = defaultdict(lambda: [], [(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    # Test passing custom classes into remote functions from the driver.
    @ray.remote
    def f(x):
        return x

    foo = ray.get(f.remote(Foo(7)))
    assert foo == Foo(7)

    regex = re.compile(r""\d+\.\d*"")
    new_regex = ray.get(f.remote(regex))
    # This seems to fail on the system Python 3 that comes with
    # Ubuntu, so it is commented out for now:
    # assert regex == new_regex
    # Instead, we do this:
    assert regex.pattern == new_regex.pattern

    # Test returning custom classes created on workers.
    @ray.remote
    def g():
        return SubQux(), Qux()

    subqux, qux = ray.get(g.remote())
    assert subqux.objs[2].foo.value == 0

    # Test exporting custom class definitions from one worker to another
    # when the worker is blocked in a get.
    class NewTempClass(object):
        def __init__(self, value):
            self.value = value

    @ray.remote
    def h1(x):
        return NewTempClass(x)

    @ray.remote
    def h2(x):
        return ray.get(h1.remote(x))

    assert ray.get(h2.remote(10)).value == 10

    # Test registering multiple classes with the same name.
    @ray.remote(num_return_vals=3)
    def j():
        class Class0(object):
            def method0(self):
                pass

        c0 = Class0()

        class Class0(object):
            def method1(self):
                pass

        c1 = Class0()

        class Class0(object):
            def method2(self):
                pass

        c2 = Class0()

        return c0, c1, c2

    results = []
    for _ in range(5):
        results += j.remote()
    for i in range(len(results) // 3):
        c0, c1, c2 = ray.get(results[(3 * i):(3 * (i + 1))])

        c0.method0()
        c1.method1()
        c2.method2()

        assert not hasattr(c0, ""method1"")
        assert not hasattr(c0, ""method2"")
        assert not hasattr(c1, ""method0"")
        assert not hasattr(c1, ""method2"")
        assert not hasattr(c2, ""method0"")
        assert not hasattr(c2, ""method1"")

    @ray.remote
    def k():
        class Class0(object):
            def method0(self):
                pass

        c0 = Class0()

        class Class0(object):
            def method1(self):
                pass

        c1 = Class0()

        class Class0(object):
            def method2(self):
                pass

        c2 = Class0()

        return c0, c1, c2

    results = ray.get([k.remote() for _ in range(5)])
    for c0, c1, c2 in results:
        c0.method0()
        c1.method1()
        c2.method2()

        assert not hasattr(c0, ""method1"")
        assert not hasattr(c0, ""method2"")
        assert not hasattr(c1, ""method0"")
        assert not hasattr(c1, ""method2"")
        assert not hasattr(c2, ""method0"")
        assert not hasattr(c2, ""method1"")


def test_keyword_args(shutdown_only):
    @ray.remote
    def keyword_fct1(a, b=""hello""):
        return ""{} {}"".format(a, b)

    @ray.remote
    def keyword_fct2(a=""hello"", b=""world""):
        return ""{} {}"".format(a, b)

    @ray.remote
    def keyword_fct3(a, b, c=""hello"", d=""world""):
        return ""{} {} {} {}"".format(a, b, c, d)

    ray.init(num_cpus=1)

    x = keyword_fct1.remote(1)
    assert ray.get(x) == ""1 hello""
    x = keyword_fct1.remote(1, ""hi"")
    assert ray.get(x) == ""1 hi""
    x = keyword_fct1.remote(1, b=""world"")
    assert ray.get(x) == ""1 world""
    x = keyword_fct1.remote(a=1, b=""world"")
    assert ray.get(x) == ""1 world""

    x = keyword_fct2.remote(a=""w"", b=""hi"")
    assert ray.get(x) == ""w hi""
    x = keyword_fct2.remote(b=""hi"", a=""w"")
    assert ray.get(x) == ""w hi""
    x = keyword_fct2.remote(a=""w"")
    assert ray.get(x) == ""w world""
    x = keyword_fct2.remote(b=""hi"")
    assert ray.get(x) == ""hello hi""
    x = keyword_fct2.remote(""w"")
    assert ray.get(x) == ""w world""
    x = keyword_fct2.remote(""w"", ""hi"")
    assert ray.get(x) == ""w hi""

    x = keyword_fct3.remote(0, 1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, b=1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(a=0, b=1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, 1, d=""hi"", c=""w"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, 1, c=""w"")
    assert ray.get(x) == ""0 1 w world""
    x = keyword_fct3.remote(0, 1, d=""hi"")
    assert ray.get(x) == ""0 1 hello hi""
    x = keyword_fct3.remote(0, 1)
    assert ray.get(x) == ""0 1 hello world""
    x = keyword_fct3.remote(a=0, b=1)
    assert ray.get(x) == ""0 1 hello world""

    # Check that we cannot pass invalid keyword arguments to functions.
    @ray.remote
    def f1():
        return

    @ray.remote
    def f2(x, y=0, z=0):
        return

    # Make sure we get an exception if too many arguments are passed in.
    with pytest.raises(Exception):
        f1.remote(3)

    with pytest.raises(Exception):
        f1.remote(x=3)

    with pytest.raises(Exception):
        f2.remote(0, w=0)

    with pytest.raises(Exception):
        f2.remote(3, x=3)

    # Make sure we get an exception if too many arguments are passed in.
    with pytest.raises(Exception):
        f2.remote(1, 2, 3, 4)

    @ray.remote
    def f3(x):
        return x

    assert ray.get(f3.remote(4)) == 4


def test_variable_number_of_args(shutdown_only):
    @ray.remote
    def varargs_fct1(*a):
        return "" "".join(map(str, a))

    @ray.remote
    def varargs_fct2(a, *b):
        return "" "".join(map(str, b))

    try:

        @ray.remote
        def kwargs_throw_exception(**c):
            return ()

        kwargs_exception_thrown = False
    except Exception:
        kwargs_exception_thrown = True

    ray.init(num_cpus=1)

    x = varargs_fct1.remote(0, 1, 2)
    assert ray.get(x) == ""0 1 2""
    x = varargs_fct2.remote(0, 1, 2)
    assert ray.get(x) == ""1 2""

    assert kwargs_exception_thrown

    @ray.remote
    def f1(*args):
        return args

    @ray.remote
    def f2(x, y, *args):
        return x, y, args

    assert ray.get(f1.remote()) == ()
    assert ray.get(f1.remote(1)) == (1, )
    assert ray.get(f1.remote(1, 2, 3)) == (1, 2, 3)
    with pytest.raises(Exception):
        f2.remote()
    with pytest.raises(Exception):
        f2.remote(1)
    assert ray.get(f2.remote(1, 2)) == (1, 2, ())
    assert ray.get(f2.remote(1, 2, 3)) == (1, 2, (3, ))
    assert ray.get(f2.remote(1, 2, 3, 4)) == (1, 2, (3, 4))

    def testNoArgs(self):
        @ray.remote
        def no_op():
            pass

        self.init_ray()

        ray.get(no_op.remote())


def test_defining_remote_functions(shutdown_only):
    ray.init(num_cpus=3)

    # Test that we can define a remote function in the shell.
    @ray.remote
    def f(x):
        return x + 1

    assert ray.get(f.remote(0)) == 1

    # Test that we can redefine the remote function.
    @ray.remote
    def f(x):
        return x + 10

    while True:
        val = ray.get(f.remote(0))
        assert val in [1, 10]
        if val == 10:
            break
        else:
            logger.info(""Still using old definition of f, trying again."")

    # Test that we can close over plain old data.
    data = [
        np.zeros([3, 5]), (1, 2, ""a""), [0.0, 1.0, 1 << 62], 1 << 60, {
            ""a"": np.zeros(3)
        }
    ]

    @ray.remote
    def g():
        return data

    ray.get(g.remote())

    # Test that we can close over modules.
    @ray.remote
    def h():
        return np.zeros([3, 5])

    assert_equal(ray.get(h.remote()), np.zeros([3, 5]))

    @ray.remote
    def j():
        return time.time()

    ray.get(j.remote())

    # Test that we can define remote functions that call other remote
    # functions.
    @ray.remote
    def k(x):
        return x + 1

    @ray.remote
    def k2(x):
        return ray.get(k.remote(x))

    @ray.remote
    def m(x):
        return ray.get(k2.remote(x))

    assert ray.get(k.remote(1)) == 2
    assert ray.get(k2.remote(1)) == 2
    assert ray.get(m.remote(1)) == 2

    def test_submit_api(shutdown_only):
        ray.init(num_cpus=1, num_gpus=1, resources={""Custom"": 1})

        @ray.remote
        def f(n):
            return list(range(n))

        @ray.remote
        def g():
            return ray.get_gpu_ids()

        assert f._remote([0], num_return_vals=0) is None
        id1 = f._remote(args=[1], num_return_vals=1)
        assert ray.get(id1) == [0]
        id1, id2 = f._remote(args=[2], num_return_vals=2)
        assert ray.get([id1, id2]) == [0, 1]
        id1, id2, id3 = f._remote(args=[3], num_return_vals=3)
        assert ray.get([id1, id2, id3]) == [0, 1, 2]
        assert ray.get(
            g._remote(
                args=[], num_cpus=1, num_gpus=1,
                resources={""Custom"": 1})) == [0]
        infeasible_id = g._remote(args=[], resources={""NonexistentCustom"": 1})
        ready_ids, remaining_ids = ray.wait([infeasible_id], timeout=0.05)
        assert len(ready_ids) == 0
        assert len(remaining_ids) == 1

        @ray.remote
        class Actor(object):
            def __init__(self, x, y=0):
                self.x = x
                self.y = y

            def method(self, a, b=0):
                return self.x, self.y, a, b

            def gpu_ids(self):
                return ray.get_gpu_ids()

        a = Actor._remote(
            args=[0], kwargs={""y"": 1}, num_gpus=1, resources={""Custom"": 1})

        id1, id2, id3, id4 = a.method._remote(
            args=[""test""], kwargs={""b"": 2}, num_return_vals=4)
        assert ray.get([id1, id2, id3, id4]) == [0, 1, ""test"", 2]


def test_get_multiple(shutdown_only):
    ray.init(num_cpus=1)
    object_ids = [ray.put(i) for i in range(10)]
    assert ray.get(object_ids) == list(range(10))

    # Get a random choice of object IDs with duplicates.
    indices = list(np.random.choice(range(10), 5))
    indices += indices
    results = ray.get([object_ids[i] for i in indices])
    assert results == indices


def test_get_multiple_experimental(shutdown_only):
    ray.init(num_cpus=1)
    object_ids = [ray.put(i) for i in range(10)]

    object_ids_tuple = tuple(object_ids)
    assert ray.experimental.get(object_ids_tuple) == list(range(10))

    object_ids_nparray = np.array(object_ids)
    assert ray.experimental.get(object_ids_nparray) == list(range(10))


def test_get_dict(shutdown_only):
    ray.init(num_cpus=1)
    d = {str(i): ray.put(i) for i in range(5)}
    for i in range(5, 10):
        d[str(i)] = i
    result = ray.experimental.get(d)
    expected = {str(i): i for i in range(10)}
    assert result == expected


def test_wait(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    objectids = [f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    ready_ids, remaining_ids = ray.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3
    ready_ids, remaining_ids = ray.wait(objectids, num_returns=4)
    assert set(ready_ids) == set(objectids)
    assert remaining_ids == []

    objectids = [f.remote(0.5), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    start_time = time.time()
    ready_ids, remaining_ids = ray.wait(objectids, timeout=1.75, num_returns=4)
    assert time.time() - start_time < 2
    assert len(ready_ids) == 3
    assert len(remaining_ids) == 1
    ray.wait(objectids)
    objectids = [f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    start_time = time.time()
    ready_ids, remaining_ids = ray.wait(objectids, timeout=5.0)
    assert time.time() - start_time < 5
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3

    # Verify that calling wait with duplicate object IDs throws an
    # exception.
    x = ray.put(1)
    with pytest.raises(Exception):
        ray.wait([x, x])

    # Make sure it is possible to call wait with an empty list.
    ready_ids, remaining_ids = ray.wait([])
    assert ready_ids == []
    assert remaining_ids == []

    # Test semantics of num_returns with no timeout.
    oids = [ray.put(i) for i in range(10)]
    (found, rest) = ray.wait(oids, num_returns=2)
    assert len(found) == 2
    assert len(rest) == 8

    # Verify that incorrect usage raises a TypeError.
    x = ray.put(1)
    with pytest.raises(TypeError):
        ray.wait(x)
    with pytest.raises(TypeError):
        ray.wait(1)
    with pytest.raises(TypeError):
        ray.wait([1])


def test_wait_iterables(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    objectids = (f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5))
    ready_ids, remaining_ids = ray.experimental.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3

    objectids = np.array(
        [f.remote(1.0),
         f.remote(0.5),
         f.remote(0.5),
         f.remote(0.5)])
    ready_ids, remaining_ids = ray.experimental.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3


def test_multiple_waits_and_gets(shutdown_only):
    # It is important to use three workers here, so that the three tasks
    # launched in this experiment can run at the same time.
    ray.init(num_cpus=3)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    @ray.remote
    def g(l):
        # The argument l should be a list containing one object ID.
        ray.wait([l[0]])

    @ray.remote
    def h(l):
        # The argument l should be a list containing one object ID.
        ray.get(l[0])

    # Make sure that multiple wait requests involving the same object ID
    # all return.
    x = f.remote(1)
    ray.get([g.remote([x]), g.remote([x])])

    # Make sure that multiple get requests involving the same object ID all
    # return.
    x = f.remote(1)
    ray.get([h.remote([x]), h.remote([x])])


def test_caching_functions_to_run(shutdown_only):
    # Test that we export functions to run on all workers before the driver
    # is connected.
    def f(worker_info):
        sys.path.append(1)

    ray.worker.global_worker.run_function_on_all_workers(f)

    def f(worker_info):
        sys.path.append(2)

    ray.worker.global_worker.run_function_on_all_workers(f)

    def g(worker_info):
        sys.path.append(3)

    ray.worker.global_worker.run_function_on_all_workers(g)

    def f(worker_info):
        sys.path.append(4)

    ray.worker.global_worker.run_function_on_all_workers(f)

    ray.init(num_cpus=1)

    @ray.remote
    def get_state():
        time.sleep(1)
        return sys.path[-4], sys.path[-3], sys.path[-2], sys.path[-1]

    res1 = get_state.remote()
    res2 = get_state.remote()
    assert ray.get(res1) == (1, 2, 3, 4)
    assert ray.get(res2) == (1, 2, 3, 4)

    # Clean up the path on the workers.
    def f(worker_info):
        sys.path.pop()
        sys.path.pop()
        sys.path.pop()
        sys.path.pop()

    ray.worker.global_worker.run_function_on_all_workers(f)


def test_running_function_on_all_workers(shutdown_only):
    ray.init(num_cpus=1)

    def f(worker_info):
        sys.path.append(""fake_directory"")

    ray.worker.global_worker.run_function_on_all_workers(f)

    @ray.remote
    def get_path1():
        return sys.path

    assert ""fake_directory"" == ray.get(get_path1.remote())[-1]

    def f(worker_info):
        sys.path.pop(-1)

    ray.worker.global_worker.run_function_on_all_workers(f)

    # Create a second remote function to guarantee that when we call
    # get_path2.remote(), the second function to run will have been run on
    # the worker.
    @ray.remote
    def get_path2():
        return sys.path

    assert ""fake_directory"" not in ray.get(get_path2.remote())


def test_profiling_api(shutdown_only):
    ray.init(num_cpus=2)

    @ray.remote
    def f():
        with ray.profile(
                ""custom_event"",
                extra_data={""name"": ""custom name""}) as ray_prof:
            ray_prof.set_attribute(""key"", ""value"")

    ray.put(1)
    object_id = f.remote()
    ray.wait([object_id])
    ray.get(object_id)

    # Wait until all of the profiling information appears in the profile
    # table.
    timeout_seconds = 20
    start_time = time.time()
    while True:
        if time.time() - start_time > timeout_seconds:
            raise Exception(""Timed out while waiting for information in ""
                            ""profile table."")
        profile_data = ray.global_state.chrome_tracing_dump()
        event_types = {event[""cat""] for event in profile_data}
        expected_types = [
            ""worker_idle"",
            ""task"",
            ""task:deserialize_arguments"",
            ""task:execute"",
            ""task:store_outputs"",
            ""wait_for_function"",
            ""ray.get"",
            ""ray.put"",
            ""ray.wait"",
            ""submit_task"",
            ""fetch_and_run_function"",
            ""register_remote_function"",
            ""custom_event"",  # This is the custom one from ray.profile.
        ]

        if all(expected_type in event_types
               for expected_type in expected_types):
            break


@pytest.fixture()
def ray_start_cluster():
    cluster = ray.test.cluster_utils.Cluster()
    yield cluster

    # The code after the yield will run as teardown code.
    ray.shutdown()
    cluster.shutdown()


def test_object_transfer_dump(ray_start_cluster):
    cluster = ray_start_cluster

    num_nodes = 3
    # Set the inline object size to 0 to force all objects to be written to
    # plasma.
    config = json.dumps({""inline_object_max_size_bytes"": 0})
    for i in range(num_nodes):
        cluster.add_node(
            resources={str(i): 1},
            object_store_memory=10**9,
            _internal_config=config)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f(x):
        return

    # These objects will live on different nodes.
    object_ids = [
        f._remote(args=[1], resources={str(i): 1}) for i in range(num_nodes)
    ]

    # Broadcast each object from each machine to each other machine.
    for object_id in object_ids:
        ray.get([
            f._remote(args=[object_id], resources={str(i): 1})
            for i in range(num_nodes)
        ])

    # The profiling information only flushes once every second.
    time.sleep(1.1)

    transfer_dump = ray.global_state.chrome_tracing_object_transfer_dump()
    # Make sure the transfer dump can be serialized with JSON.
    json.loads(json.dumps(transfer_dump))
    assert len(transfer_dump) >= num_nodes**2
    assert len({
        event[""pid""]
        for event in transfer_dump if event[""name""] == ""transfer_receive""
    }) == num_nodes
    assert len({
        event[""pid""]
        for event in transfer_dump if event[""name""] == ""transfer_send""
    }) == num_nodes


def test_identical_function_names(shutdown_only):
    # Define a bunch of remote functions and make sure that we don't
    # accidentally call an older version.
    ray.init(num_cpus=1)

    num_calls = 200

    @ray.remote
    def f():
        return 1

    results1 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 2

    results2 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 3

    results3 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 4

    results4 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 5

    results5 = [f.remote() for _ in range(num_calls)]

    assert ray.get(results1) == num_calls * [1]
    assert ray.get(results2) == num_calls * [2]
    assert ray.get(results3) == num_calls * [3]
    assert ray.get(results4) == num_calls * [4]
    assert ray.get(results5) == num_calls * [5]

    @ray.remote
    def g():
        return 1

    @ray.remote  # noqa: F811
    def g():
        return 2

    @ray.remote  # noqa: F811
    def g():
        return 3

    @ray.remote  # noqa: F811
    def g():
        return 4

    @ray.remote  # noqa: F811
    def g():
        return 5

    result_values = ray.get([g.remote() for _ in range(num_calls)])
    assert result_values == num_calls * [5]


def test_illegal_api_calls(shutdown_only):
    ray.init(num_cpus=1)

    # Verify that we cannot call put on an ObjectID.
    x = ray.put(1)
    with pytest.raises(Exception):
        ray.put(x)
    # Verify that we cannot call get on a regular value.
    with pytest.raises(Exception):
        ray.get(3)


def test_multithreading(shutdown_only):
    # This test requires at least 2 CPUs to finish since the worker does not
    # relase resources when joining the threads.
    ray.init(num_cpus=2)

    def run_test_in_multi_threads(test_case, num_threads=20, num_repeats=50):
        """"""A helper function that runs test cases in multiple threads.""""""

        def wrapper():
            for _ in range(num_repeats):
                test_case()
                time.sleep(random.randint(0, 10) / 1000.0)
            return ""ok""

        executor = ThreadPoolExecutor(max_workers=num_threads)
        futures = [executor.submit(wrapper) for _ in range(num_threads)]
        for future in futures:
            assert future.result() == ""ok""

    @ray.remote
    def echo(value, delay_ms=0):
        if delay_ms > 0:
            time.sleep(delay_ms / 1000.0)
        return value

    @ray.remote
    class Echo(object):
        def echo(self, value):
            return value

    def test_api_in_multi_threads():
        """"""Test using Ray api in multiple threads.""""""

        # Test calling remote functions in multiple threads.
        def test_remote_call():
            value = random.randint(0, 1000000)
            result = ray.get(echo.remote(value))
            assert value == result

        run_test_in_multi_threads(test_remote_call)

        # Test multiple threads calling one actor.
        actor = Echo.remote()

        def test_call_actor():
            value = random.randint(0, 1000000)
            result = ray.get(actor.echo.remote(value))
            assert value == result

        run_test_in_multi_threads(test_call_actor)

        # Test put and get.
        def test_put_and_get():
            value = random.randint(0, 1000000)
            result = ray.get(ray.put(value))
            assert value == result

        run_test_in_multi_threads(test_put_and_get)

        # Test multiple threads waiting for objects.
        num_wait_objects = 10
        objects = [
            echo.remote(i, delay_ms=10) for i in range(num_wait_objects)
        ]

        def test_wait():
            ready, _ = ray.wait(
                objects,
                num_returns=len(objects),
                timeout=1000.0,
            )
            assert len(ready) == num_wait_objects
            assert ray.get(ready) == list(range(num_wait_objects))

        run_test_in_multi_threads(test_wait, num_repeats=1)

    # Run tests in a driver.
    test_api_in_multi_threads()

    # Run tests in a worker.
    @ray.remote
    def run_tests_in_worker():
        test_api_in_multi_threads()
        return ""ok""

    assert ray.get(run_tests_in_worker.remote()) == ""ok""

    # Test actor that runs background threads.
    @ray.remote
    class MultithreadedActor(object):
        def __init__(self):
            self.lock = threading.Lock()
            self.thread_results = []

        def background_thread(self, wait_objects):
            try:
                # Test wait
                ready, _ = ray.wait(
                    wait_objects,
                    num_returns=len(wait_objects),
                    timeout=1000.0,
                )
                assert len(ready) == len(wait_objects)
                for _ in range(50):
                    num = 20
                    # Test remote call
                    results = [echo.remote(i) for i in range(num)]
                    assert ray.get(results) == list(range(num))
                    # Test put and get
                    objects = [ray.put(i) for i in range(num)]
                    assert ray.get(objects) == list(range(num))
                    time.sleep(random.randint(0, 10) / 1000.0)
            except Exception as e:
                with self.lock:
                    self.thread_results.append(e)
            else:
                with self.lock:
                    self.thread_results.append(""ok"")

        def spawn(self):
            wait_objects = [echo.remote(i, delay_ms=10) for i in range(20)]
            self.threads = [
                threading.Thread(
                    target=self.background_thread, args=(wait_objects, ))
                for _ in range(20)
            ]
            [thread.start() for thread in self.threads]

        def join(self):
            [thread.join() for thread in self.threads]
            assert self.thread_results == [""ok""] * len(self.threads)
            return ""ok""

    actor = MultithreadedActor.remote()
    actor.spawn.remote()
    ray.get(actor.join.remote()) == ""ok""


def test_free_objects_multi_node(ray_start_cluster):
    # This test will do following:
    # 1. Create 3 raylets that each hold an actor.
    # 2. Each actor creates an object which is the deletion target.
    # 3. Invoke 64 methods on each actor to flush plasma client.
    # 4. After flushing, the plasma client releases the targets.
    # 5. Check that the deletion targets have been deleted.
    # Caution: if remote functions are used instead of actor methods,
    # one raylet may create more than one worker to execute the
    # tasks, so the flushing operations may be executed in different
    # workers and the plasma client holding the deletion target
    # may not be flushed.
    cluster = ray_start_cluster
    config = json.dumps({""object_manager_repeated_push_delay_ms"": 1000})
    for i in range(3):
        cluster.add_node(
            num_cpus=1,
            resources={""Custom{}"".format(i): 1},
            _internal_config=config)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote(resources={""Custom0"": 1})
    class ActorOnNode0(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""Custom1"": 1})
    class ActorOnNode1(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""Custom2"": 1})
    class ActorOnNode2(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    def create(actors):
        a = actors[0].get.remote()
        b = actors[1].get.remote()
        c = actors[2].get.remote()
        (l1, l2) = ray.wait([a, b, c], num_returns=3)
        assert len(l1) == 3
        assert len(l2) == 0
        return (a, b, c)

    def flush(actors):
        # Flush the Release History.
        # Current Plasma Client Cache will maintain 64-item list.
        # If the number changed, this will fail.
        logger.info(""Start Flush!"")
        for i in range(64):
            ray.get([actor.get.remote() for actor in actors])
        logger.info(""Flush finished!"")

    def run_one_test(actors, local_only):
        (a, b, c) = create(actors)
        # The three objects should be generated on different object stores.
        assert ray.get(a) != ray.get(b)
        assert ray.get(a) != ray.get(c)
        assert ray.get(c) != ray.get(b)
        ray.internal.free([a, b, c], local_only=local_only)
        flush(actors)
        return (a, b, c)

    actors = [
        ActorOnNode0.remote(),
        ActorOnNode1.remote(),
        ActorOnNode2.remote()
    ]
    # Case 1: run this local_only=False. All 3 objects will be deleted.
    (a, b, c) = run_one_test(actors, False)
    (l1, l2) = ray.wait([a, b, c], timeout=0.01, num_returns=1)
    # All the objects are deleted.
    assert len(l1) == 0
    assert len(l2) == 3
    # Case 2: run this local_only=True. Only 1 object will be deleted.
    (a, b, c) = run_one_test(actors, True)
    (l1, l2) = ray.wait([a, b, c], timeout=0.01, num_returns=3)
    # One object is deleted and 2 objects are not.
    assert len(l1) == 2
    assert len(l2) == 1
    # The deleted object will have the same store with the driver.
    local_return = ray.worker.global_worker.plasma_client.store_socket_name
    for object_id in l1:
        assert ray.get(object_id) != local_return


def test_local_mode(shutdown_only):
    @ray.remote
    def local_mode_f():
        return np.array([0, 0])

    @ray.remote
    def local_mode_g(x):
        x[0] = 1
        return x

    ray.init(local_mode=True)

    @ray.remote
    def f():
        return np.ones([3, 4, 5])

    xref = f.remote()
    # Remote functions should return by value.
    assert_equal(xref, np.ones([3, 4, 5]))
    # Check that ray.get is the identity.
    assert_equal(xref, ray.get(xref))
    y = np.random.normal(size=[11, 12])
    # Check that ray.put is the identity.
    assert_equal(y, ray.put(y))

    # Make sure objects are immutable, this example is why we need to copy
    # arguments before passing them into remote functions in python mode
    aref = local_mode_f.remote()
    assert_equal(aref, np.array([0, 0]))
    bref = local_mode_g.remote(aref)
    # Make sure local_mode_g does not mutate aref.
    assert_equal(aref, np.array([0, 0]))
    assert_equal(bref, np.array([1, 0]))

    # wait should return the first num_returns values passed in as the
    # first list and the remaining values as the second list
    num_returns = 5
    object_ids = [ray.put(i) for i in range(20)]
    ready, remaining = ray.wait(
        object_ids, num_returns=num_returns, timeout=None)
    assert_equal(ready, object_ids[:num_returns])
    assert_equal(remaining, object_ids[num_returns:])

    # Test actors in LOCAL_MODE.

    @ray.remote
    class LocalModeTestClass(object):
        def __init__(self, array):
            self.array = array

        def set_array(self, array):
            self.array = array

        def get_array(self):
            return self.array

        def modify_and_set_array(self, array):
            array[0] = -1
            self.array = array

    test_actor = LocalModeTestClass.remote(np.arange(10))
    # Remote actor functions should return by value
    assert_equal(test_actor.get_array.remote(), np.arange(10))

    test_array = np.arange(10)
    # Remote actor functions should not mutate arguments
    test_actor.modify_and_set_array.remote(test_array)
    assert_equal(test_array, np.arange(10))
    # Remote actor functions should keep state
    test_array[0] = -1
    assert_equal(test_array, test_actor.get_array.remote())

    # Check that actor handles work in Python mode.

    @ray.remote
    def use_actor_handle(handle):
        array = np.ones(10)
        handle.set_array.remote(array)
        assert np.alltrue(array == ray.get(handle.get_array.remote()))

    ray.get(use_actor_handle.remote(test_actor))


def test_resource_constraints(shutdown_only):
    num_workers = 20
    ray.init(num_cpus=10, num_gpus=2)

    @ray.remote(num_cpus=0)
    def get_worker_id():
        time.sleep(0.1)
        return os.getpid()

    # Attempt to wait for all of the workers to start up.
    while True:
        if len(
                set(
                    ray.get([
                        get_worker_id.remote() for _ in range(num_workers)
                    ]))) == num_workers:
            break

    time_buffer = 0.3

    # At most 10 copies of this can run at once.
    @ray.remote(num_cpus=1)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(10)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(11)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    @ray.remote(num_cpus=3)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(3)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(4)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    @ray.remote(num_gpus=1)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(2)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(3)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(4)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1


def test_multi_resource_constraints(shutdown_only):
    num_workers = 20
    ray.init(num_cpus=10, num_gpus=10)

    @ray.remote(num_cpus=0)
    def get_worker_id():
        time.sleep(0.1)
        return os.getpid()

    # Attempt to wait for all of the workers to start up.
    while True:
        if len(
                set(
                    ray.get([
                        get_worker_id.remote() for _ in range(num_workers)
                    ]))) == num_workers:
            break

    @ray.remote(num_cpus=1, num_gpus=9)
    def f(n):
        time.sleep(n)

    @ray.remote(num_cpus=9, num_gpus=1)
    def g(n):
        time.sleep(n)

    time_buffer = 0.3

    start_time = time.time()
    ray.get([f.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5), f.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([g.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([f.remote(0.5), f.remote(0.5), g.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1


def test_gpu_ids(shutdown_only):
    num_gpus = 10
    ray.init(num_cpus=10, num_gpus=num_gpus)

    @ray.remote(num_gpus=0)
    def f0():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 0
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=1)
    def f1():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 1
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=2)
    def f2():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 2
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=3)
    def f3():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 3
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=4)
    def f4():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 4
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=5)
    def f5():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 5
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    # Wait for all workers to start up.
    @ray.remote
    def f():
        time.sleep(0.1)
        return os.getpid()

    start_time = time.time()
    while True:
        if len(set(ray.get([f.remote() for _ in range(10)]))) == 10:
            break
        if time.time() > start_time + 10:
            raise Exception(""Timed out while waiting for workers to start ""
                            ""up."")

    list_of_ids = ray.get([f0.remote() for _ in range(10)])
    assert list_of_ids == 10 * [[]]

    list_of_ids = ray.get([f1.remote() for _ in range(10)])
    set_of_ids = {tuple(gpu_ids) for gpu_ids in list_of_ids}
    assert set_of_ids == {(i, ) for i in range(10)}

    list_of_ids = ray.get([f2.remote(), f4.remote(), f4.remote()])
    all_ids = [gpu_id for gpu_ids in list_of_ids for gpu_id in gpu_ids]
    assert set(all_ids) == set(range(10))

    remaining = [f5.remote() for _ in range(20)]
    for _ in range(10):
        t1 = time.time()
        ready, remaining = ray.wait(remaining, num_returns=2)
        t2 = time.time()
        # There are only 10 GPUs, and each task uses 2 GPUs, so there
        # should only be 2 tasks scheduled at a given time, so if we wait
        # for 2 tasks to finish, then it should take at least 0.1 seconds
        # for each pair of tasks to finish.
        assert t2 - t1 > 0.09
        list_of_ids = ray.get(ready)
        all_ids = [gpu_id for gpu_ids in list_of_ids for gpu_id in gpu_ids]
        # Commenting out the below assert because it seems to fail a lot.
        # assert set(all_ids) == set(range(10))

    # Test that actors have CUDA_VISIBLE_DEVICES set properly.

    @ray.remote
    class Actor0(object):
        def __init__(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 0
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            # Set self.x to make sure that we got here.
            self.x = 1

        def test(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 0
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            return self.x

    @ray.remote(num_gpus=1)
    class Actor1(object):
        def __init__(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            # Set self.x to make sure that we got here.
            self.x = 1

        def test(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            return self.x

    a0 = Actor0.remote()
    ray.get(a0.test.remote())

    a1 = Actor1.remote()
    ray.get(a1.test.remote())


def test_zero_cpus(shutdown_only):
    ray.init(num_cpus=0)

    @ray.remote(num_cpus=0)
    def f():
        return 1

    # The task should be able to execute.
    ray.get(f.remote())


def test_zero_cpus_actor(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=0)
    cluster.add_node(num_cpus=2)
    ray.init(redis_address=cluster.redis_address)

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote
    class Foo(object):
        def method(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    # Make sure tasks and actors run on the remote local scheduler.
    a = Foo.remote()
    assert ray.get(a.method.remote()) != local_plasma


def test_fractional_resources(shutdown_only):
    ray.init(num_cpus=6, num_gpus=3, resources={""Custom"": 1})

    @ray.remote(num_gpus=0.5)
    class Foo1(object):
        def method(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            return gpu_ids[0]

    foos = [Foo1.remote() for _ in range(6)]
    gpu_ids = ray.get([f.method.remote() for f in foos])
    for i in range(3):
        assert gpu_ids.count(i) == 2
    del foos

    @ray.remote
    class Foo2(object):
        def method(self):
            pass

    # Create an actor that requires 0.7 of the custom resource.
    f1 = Foo2._remote([], {}, resources={""Custom"": 0.7})
    ray.get(f1.method.remote())
    # Make sure that we cannot create an actor that requires 0.7 of the
    # custom resource. TODO(rkn): Re-enable this once ray.wait is
    # implemented.
    f2 = Foo2._remote([], {}, resources={""Custom"": 0.7})
    ready, _ = ray.wait([f2.method.remote()], timeout=0.5)
    assert len(ready) == 0
    # Make sure we can start an actor that requries only 0.3 of the custom
    # resource.
    f3 = Foo2._remote([], {}, resources={""Custom"": 0.3})
    ray.get(f3.method.remote())

    del f1, f3

    # Make sure that we get exceptions if we submit tasks that require a
    # fractional number of resources greater than 1.

    @ray.remote(num_cpus=1.5)
    def test():
        pass

    with pytest.raises(ValueError):
        test.remote()

    with pytest.raises(ValueError):
        Foo2._remote([], {}, resources={""Custom"": 1.5})


def test_multiple_local_schedulers(ray_start_cluster):
    # This test will define a bunch of tasks that can only be assigned to
    # specific local schedulers, and we will check that they are assigned
    # to the correct local schedulers.
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=11, num_gpus=0)
    cluster.add_node(num_cpus=5, num_gpus=5)
    cluster.add_node(num_cpus=10, num_gpus=1)
    ray.init(redis_address=cluster.redis_address)
    cluster.wait_for_nodes()

    # Define a bunch of remote functions that all return the socket name of
    # the plasma store. Since there is a one-to-one correspondence between
    # plasma stores and local schedulers (at least right now), this can be
    # used to identify which local scheduler the task was assigned to.

    # This must be run on the zeroth local scheduler.
    @ray.remote(num_cpus=11)
    def run_on_0():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the first local scheduler.
    @ray.remote(num_gpus=2)
    def run_on_1():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the second local scheduler.
    @ray.remote(num_cpus=6, num_gpus=1)
    def run_on_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This can be run anywhere.
    @ray.remote(num_cpus=0, num_gpus=0)
    def run_on_0_1_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the first or second local scheduler.
    @ray.remote(num_gpus=1)
    def run_on_1_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the zeroth or second local scheduler.
    @ray.remote(num_cpus=8)
    def run_on_0_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    def run_lots_of_tasks():
        names = []
        results = []
        for i in range(100):
            index = np.random.randint(6)
            if index == 0:
                names.append(""run_on_0"")
                results.append(run_on_0.remote())
            elif index == 1:
                names.append(""run_on_1"")
                results.append(run_on_1.remote())
            elif index == 2:
                names.append(""run_on_2"")
                results.append(run_on_2.remote())
            elif index == 3:
                names.append(""run_on_0_1_2"")
                results.append(run_on_0_1_2.remote())
            elif index == 4:
                names.append(""run_on_1_2"")
                results.append(run_on_1_2.remote())
            elif index == 5:
                names.append(""run_on_0_2"")
                results.append(run_on_0_2.remote())
        return names, results

    client_table = ray.global_state.client_table()
    store_names = []
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 0
    ]
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 5
    ]
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 1
    ]
    assert len(store_names) == 3

    def validate_names_and_results(names, results):
        for name, result in zip(names, ray.get(results)):
            if name == ""run_on_0"":
                assert result in [store_names[0]]
            elif name == ""run_on_1"":
                assert result in [store_names[1]]
            elif name == ""run_on_2"":
                assert result in [store_names[2]]
            elif name == ""run_on_0_1_2"":
                assert (result in [
                    store_names[0], store_names[1], store_names[2]
                ])
            elif name == ""run_on_1_2"":
                assert result in [store_names[1], store_names[2]]
            elif name == ""run_on_0_2"":
                assert result in [store_names[0], store_names[2]]
            else:
                raise Exception(""This should be unreachable."")
            assert set(ray.get(results)) == set(store_names)

    names, results = run_lots_of_tasks()
    validate_names_and_results(names, results)

    # Make sure the same thing works when this is nested inside of a task.

    @ray.remote
    def run_nested1():
        names, results = run_lots_of_tasks()
        return names, results

    @ray.remote
    def run_nested2():
        names, results = ray.get(run_nested1.remote())
        return names, results

    names, results = ray.get(run_nested2.remote())
    validate_names_and_results(names, results)


def test_custom_resources(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=3, resources={""CustomResource"": 0})
    cluster.add_node(num_cpus=3, resources={""CustomResource"": 1})
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource"": 1})
    def g():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource"": 1})
    def h():
        ray.get([f.remote() for _ in range(5)])
        return ray.worker.global_worker.plasma_client.store_socket_name

    # The f tasks should be scheduled on both local schedulers.
    assert len(set(ray.get([f.remote() for _ in range(50)]))) == 2

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    # The g tasks should be scheduled only on the second local scheduler.
    local_scheduler_ids = set(ray.get([g.remote() for _ in range(50)]))
    assert len(local_scheduler_ids) == 1
    assert list(local_scheduler_ids)[0] != local_plasma

    # Make sure that resource bookkeeping works when a task that uses a
    # custom resources gets blocked.
    ray.get([h.remote() for _ in range(5)])


def test_two_custom_resources(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(
        num_cpus=3, resources={
            ""CustomResource1"": 1,
            ""CustomResource2"": 2
        })
    cluster.add_node(
        num_cpus=3, resources={
            ""CustomResource1"": 3,
            ""CustomResource2"": 4
        })
    ray.init(redis_address=cluster.redis_address)

    @ray.remote(resources={""CustomResource1"": 1})
    def f():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource2"": 1})
    def g():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource1"": 1, ""CustomResource2"": 3})
    def h():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource1"": 4})
    def j():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource3"": 1})
    def k():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    # The f and g tasks should be scheduled on both local schedulers.
    assert len(set(ray.get([f.remote() for _ in range(50)]))) == 2
    assert len(set(ray.get([g.remote() for _ in range(50)]))) == 2

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    # The h tasks should be scheduled only on the second local scheduler.
    local_scheduler_ids = set(ray.get([h.remote() for _ in range(50)]))
    assert len(local_scheduler_ids) == 1
    assert list(local_scheduler_ids)[0] != local_plasma

    # Make sure that tasks with unsatisfied custom resource requirements do
    # not get scheduled.
    ready_ids, remaining_ids = ray.wait([j.remote(), k.remote()], timeout=0.5)
    assert ready_ids == []


def test_many_custom_resources(shutdown_only):
    num_custom_resources = 10000
    total_resources = {
        str(i): np.random.randint(1, 7)
        for i in range(num_custom_resources)
    }
    ray.init(num_cpus=5, resources=total_resources)

    def f():
        return 1

    remote_functions = []
    for _ in range(20):
        num_resources = np.random.randint(0, num_custom_resources + 1)
        permuted_resources = np.random.permutation(
            num_custom_resources)[:num_resources]
        random_resources = {
            str(i): total_resources[str(i)]
            for i in permuted_resources
        }
        remote_function = ray.remote(resources=random_resources)(f)
        remote_functions.append(remote_function)

    remote_functions.append(ray.remote(f))
    remote_functions.append(ray.remote(resources=total_resources)(f))

    results = []
    for remote_function in remote_functions:
        results.append(remote_function.remote())
        results.append(remote_function.remote())
        results.append(remote_function.remote())

    ray.get(results)


@pytest.fixture
def save_gpu_ids_shutdown_only():
    # Record the curent value of this environment variable so that we can
    # reset it after the test.
    original_gpu_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    yield None

    # The code after the yield will run as teardown code.
    ray.shutdown()
    # Reset the environment variable.
    if original_gpu_ids is not None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = original_gpu_ids
    else:
        del os.environ[""CUDA_VISIBLE_DEVICES""]


def test_specific_gpus(save_gpu_ids_shutdown_only):
    allowed_gpu_ids = [4, 5, 6]
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join(
        [str(i) for i in allowed_gpu_ids])
    ray.init(num_gpus=3)

    @ray.remote(num_gpus=1)
    def f():
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 1
        assert gpu_ids[0] in allowed_gpu_ids

    @ray.remote(num_gpus=2)
    def g():
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 2
        assert gpu_ids[0] in allowed_gpu_ids
        assert gpu_ids[1] in allowed_gpu_ids

    ray.get([f.remote() for _ in range(100)])
    ray.get([g.remote() for _ in range(100)])


def test_blocking_tasks(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(i, j):
        return (i, j)

    @ray.remote
    def g(i):
        # Each instance of g submits and blocks on the result of another
        # remote task.
        object_ids = [f.remote(i, j) for j in range(2)]
        return ray.get(object_ids)

    @ray.remote
    def h(i):
        # Each instance of g submits and blocks on the result of another
        # remote task using ray.wait.
        object_ids = [f.remote(i, j) for j in range(2)]
        return ray.wait(object_ids, num_returns=len(object_ids))

    ray.get([h.remote(i) for i in range(4)])

    @ray.remote
    def _sleep(i):
        time.sleep(0.01)
        return (i)

    @ray.remote
    def sleep():
        # Each instance of sleep submits and blocks on the result of
        # another remote task, which takes some time to execute.
        ray.get([_sleep.remote(i) for i in range(10)])

    ray.get(sleep.remote())


def test_max_call_tasks(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote(max_calls=1)
    def f():
        return os.getpid()

    pid = ray.get(f.remote())
    ray.test.test_utils.wait_for_pid_to_exit(pid)

    @ray.remote(max_calls=2)
    def f():
        return os.getpid()

    pid1 = ray.get(f.remote())
    pid2 = ray.get(f.remote())
    assert pid1 == pid2
    ray.test.test_utils.wait_for_pid_to_exit(pid1)


def attempt_to_load_balance(remote_function,
                            args,
                            total_tasks,
                            num_nodes,
                            minimum_count,
                            num_attempts=100):
    attempts = 0
    while attempts < num_attempts:
        locations = ray.get(
            [remote_function.remote(*args) for _ in range(total_tasks)])
        names = set(locations)
        counts = [locations.count(name) for name in names]
        logger.info(""Counts are {}."".format(counts))
        if (len(names) == num_nodes
                and all(count >= minimum_count for count in counts)):
            break
        attempts += 1
    assert attempts < num_attempts


def test_load_balancing(ray_start_cluster):
    # This test ensures that tasks are being assigned to all local
    # schedulers in a roughly equal manner.
    cluster = ray_start_cluster
    num_nodes = 3
    num_cpus = 7
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=num_cpus)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f():
        time.sleep(0.01)
        return ray.worker.global_worker.plasma_client.store_socket_name

    attempt_to_load_balance(f, [], 100, num_nodes, 10)
    attempt_to_load_balance(f, [], 1000, num_nodes, 100)


def test_load_balancing_with_dependencies(ray_start_cluster):
    # This test ensures that tasks are being assigned to all local
    # schedulers in a roughly equal manner even when the tasks have
    # dependencies.
    cluster = ray_start_cluster
    num_nodes = 3
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=1)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f(x):
        time.sleep(0.010)
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This object will be local to one of the local schedulers. Make sure
    # this doesn't prevent tasks from being scheduled on other local
    # schedulers.
    x = ray.put(np.zeros(1000000))

    attempt_to_load_balance(f, [x], 100, num_nodes, 25)


def wait_for_num_tasks(num_tasks, timeout=10):
    start_time = time.time()
    while time.time() - start_time < timeout:
        if len(ray.global_state.task_table()) >= num_tasks:
            return
        time.sleep(0.1)
    raise Exception(""Timed out while waiting for global state."")


def wait_for_num_objects(num_objects, timeout=10):
    start_time = time.time()
    while time.time() - start_time < timeout:
        if len(ray.global_state.object_table()) >= num_objects:
            return
        time.sleep(0.1)
    raise Exception(""Timed out while waiting for global state."")


@pytest.mark.skipif(
    os.environ.get(""RAY_USE_NEW_GCS"") == ""on"",
    reason=""New GCS API doesn't have a Python API yet."")
def test_global_state_api(shutdown_only):
    with pytest.raises(Exception):
        ray.global_state.object_table()

    with pytest.raises(Exception):
        ray.global_state.task_table()

    with pytest.raises(Exception):
        ray.global_state.client_table()

    with pytest.raises(Exception):
        ray.global_state.function_table()

    ray.init(num_cpus=5, num_gpus=3, resources={""CustomResource"": 1})

    resources = {""CPU"": 5, ""GPU"": 3, ""CustomResource"": 1}
    assert ray.global_state.cluster_resources() == resources

    assert ray.global_state.object_table() == {}

    driver_id = ray.experimental.state.binary_to_hex(
        ray.worker.global_worker.worker_id)
    driver_task_id = ray.worker.global_worker.current_task_id.hex()

    # One task is put in the task table which corresponds to this driver.
    wait_for_num_tasks(1)
    task_table = ray.global_state.task_table()
    assert len(task_table) == 1
    assert driver_task_id == list(task_table.keys())[0]
    task_spec = task_table[driver_task_id][""TaskSpec""]
    nil_id_hex = ray.ObjectID.nil().hex()

    assert task_spec[""TaskID""] == driver_task_id
    assert task_spec[""ActorID""] == nil_id_hex
    assert task_spec[""Args""] == []
    assert task_spec[""DriverID""] == driver_id
    assert task_spec[""FunctionID""] == nil_id_hex
    assert task_spec[""ReturnObjectIDs""] == []

    client_table = ray.global_state.client_table()
    node_ip_address = ray.worker.global_worker.node_ip_address

    assert len(client_table) == 1
    assert client_table[0][""NodeManagerAddress""] == node_ip_address

    @ray.remote
    def f(*xs):
        return 1

    x_id = ray.put(1)
    result_id = f.remote(1, ""hi"", x_id)

    # Wait for one additional task to complete.
    wait_for_num_tasks(1 + 1)
    task_table = ray.global_state.task_table()
    assert len(task_table) == 1 + 1
    task_id_set = set(task_table.keys())
    task_id_set.remove(driver_task_id)
    task_id = list(task_id_set)[0]

    function_table = ray.global_state.function_table()
    task_spec = task_table[task_id][""TaskSpec""]
    assert task_spec[""ActorID""] == nil_id_hex
    assert task_spec[""Args""] == [1, ""hi"", x_id]
    assert task_spec[""DriverID""] == driver_id
    assert task_spec[""ReturnObjectIDs""] == [result_id]
    function_table_entry = function_table[task_spec[""FunctionID""]]
    assert function_table_entry[""Name""] == ""runtest.f""
    assert function_table_entry[""DriverID""] == driver_id
    assert function_table_entry[""Module""] == ""runtest""

    assert task_table[task_id] == ray.global_state.task_table(task_id)

    # Wait for two objects, one for the x_id and one for result_id.
    wait_for_num_objects(2)

    def wait_for_object_table():
        timeout = 10
        start_time = time.time()
        while time.time() - start_time < timeout:
            object_table = ray.global_state.object_table()
            tables_ready = (object_table[x_id][""ManagerIDs""] is not None and
                            object_table[result_id][""ManagerIDs""] is not None)
            if tables_ready:
                return
            time.sleep(0.1)
        raise Exception(""Timed out while waiting for object table to ""
                        ""update."")

    object_table = ray.global_state.object_table()
    assert len(object_table) == 2

    assert object_table[x_id][""IsEviction""][0] is False

    assert object_table[result_id][""IsEviction""][0] is False

    assert object_table[x_id] == ray.global_state.object_table(x_id)
    object_table_entry = ray.global_state.object_table(result_id)
    assert object_table[result_id] == object_table_entry


# TODO(rkn): Pytest actually has tools for capturing stdout and stderr, so we
# should use those, but they seem to conflict with Ray's use of faulthandler.
class CaptureOutputAndError(object):
    """"""Capture stdout and stderr of some span.

    This can be used as follows.

        captured = {}
        with CaptureOutputAndError(captured):
            # Do stuff.
        # Access captured[""out""] and captured[""err""].
    """"""

    def __init__(self, captured_output_and_error):
        if sys.version_info >= (3, 0):
            import io
            self.output_buffer = io.StringIO()
            self.error_buffer = io.StringIO()
        else:
            import cStringIO
            self.output_buffer = cStringIO.StringIO()
            self.error_buffer = cStringIO.StringIO()
        self.captured_output_and_error = captured_output_and_error

    def __enter__(self):
        sys.stdout.flush()
        sys.stderr.flush()
        self.old_stdout = sys.stdout
        self.old_stderr = sys.stderr
        sys.stdout = self.output_buffer
        sys.stderr = self.error_buffer

    def __exit__(self, exc_type, exc_value, traceback):
        sys.stdout.flush()
        sys.stderr.flush()
        sys.stdout = self.old_stdout
        sys.stderr = self.old_stderr
        self.captured_output_and_error[""out""] = self.output_buffer.getvalue()
        self.captured_output_and_error[""err""] = self.error_buffer.getvalue()


def test_logging_to_driver(shutdown_only):
    ray.init(num_cpus=1, log_to_driver=True)

    @ray.remote
    def f():
        for i in range(100):
            print(i)
            print(100 + i, file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

    captured = {}
    with CaptureOutputAndError(captured):
        ray.get(f.remote())
        time.sleep(1)

    output_lines = captured[""out""]
    assert len(output_lines) == 0
    error_lines = captured[""err""]
    for i in range(200):
        assert str(i) in error_lines


def test_not_logging_to_driver(shutdown_only):
    ray.init(num_cpus=1, log_to_driver=False)

    @ray.remote
    def f():
        for i in range(100):
            print(i)
            print(100 + i, file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

    captured = {}
    with CaptureOutputAndError(captured):
        ray.get(f.remote())
        time.sleep(1)

    output_lines = captured[""out""]
    assert len(output_lines) == 0
    error_lines = captured[""err""]
    assert len(error_lines) == 0


@pytest.mark.skipif(
    os.environ.get(""RAY_USE_NEW_GCS"") == ""on"",
    reason=""New GCS API doesn't have a Python API yet."")
def test_workers(shutdown_only):
    num_workers = 3
    ray.init(redirect_worker_output=True, num_cpus=num_workers)

    @ray.remote
    def f():
        return id(ray.worker.global_worker), os.getpid()

    # Wait until all of the workers have started.
    worker_ids = set()
    while len(worker_ids) != num_workers:
        worker_ids = set(ray.get([f.remote() for _ in range(10)]))

    worker_info = ray.global_state.workers()
    assert len(worker_info) >= num_workers
    for worker_id, info in worker_info.items():
        assert ""node_ip_address"" in info
        assert ""plasma_store_socket"" in info
        assert ""stderr_file"" in info
        assert ""stdout_file"" in info


def test_specific_driver_id():
    dummy_driver_id = ray.DriverID(b""00112233445566778899"")
    ray.init(driver_id=dummy_driver_id)

    @ray.remote
    def f():
        return ray.worker.global_worker.task_driver_id.binary()

    assert_equal(dummy_driver_id.binary(), ray.worker.global_worker.worker_id)

    task_driver_id = ray.get(f.remote())
    assert_equal(dummy_driver_id.binary(), task_driver_id)

    ray.shutdown()


def test_object_id_properties():
    id_bytes = b""00112233445566778899""
    object_id = ray.ObjectID(id_bytes)
    assert object_id.binary() == id_bytes
    object_id = ray.ObjectID.nil()
    assert object_id.is_nil()
    with pytest.raises(ValueError, match=r"".*needs to have length 20.*""):
        ray.ObjectID(id_bytes + b""1234"")
    with pytest.raises(ValueError, match=r"".*needs to have length 20.*""):
        ray.ObjectID(b""0123456789"")
    object_id = ray.ObjectID(_random_string())
    assert not object_id.is_nil()
    assert object_id.binary() != id_bytes
    id_dumps = pickle.dumps(object_id)
    id_from_dumps = pickle.loads(id_dumps)
    assert id_from_dumps == object_id


@pytest.fixture
def shutdown_only_with_initialization_check():
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()
    assert not ray.is_initialized()


def test_initialized(shutdown_only_with_initialization_check):
    assert not ray.is_initialized()
    ray.init(num_cpus=0)
    assert ray.is_initialized()


def test_initialized_local_mode(shutdown_only_with_initialization_check):
    assert not ray.is_initialized()
    ray.init(num_cpus=0, local_mode=True)
    assert ray.is_initialized()


def test_wait_reconstruction(shutdown_only):
    ray.init(num_cpus=1, object_store_memory=10**8)

    @ray.remote
    def f():
        return np.zeros(6 * 10**7, dtype=np.uint8)

    x_id = f.remote()
    ray.wait([x_id])
    ray.wait([f.remote()])
    assert not ray.worker.global_worker.plasma_client.contains(
        ray.pyarrow.plasma.ObjectID(x_id.binary()))
    ready_ids, _ = ray.wait([x_id])
    assert len(ready_ids) == 1


def test_inline_objects(shutdown_only):
    config = json.dumps({""initial_reconstruction_timeout_milliseconds"": 200})
    ray.init(num_cpus=1, object_store_memory=10**7, _internal_config=config)

    @ray.remote
    class Actor(object):
        def create_inline_object(self):
            return ""inline""

        def create_non_inline_object(self):
            return 10000 * [1]

        def get(self):
            return

    a = Actor.remote()
    # Count the number of objects that were successfully inlined.
    inlined = 0
    for _ in range(100):
        inline_object = a.create_inline_object.remote()
        ray.get(inline_object)
        plasma_id = ray.pyarrow.plasma.ObjectID(inline_object.binary())
        ray.worker.global_worker.plasma_client.delete([plasma_id])
        # Make sure we can still get an inlined object created by an actor even
        # after it has been evicted.
        try:
            value = ray.get(inline_object)
            assert value == ""inline""
            inlined += 1
        except ray.worker.RayTaskError:
            pass
    # Make sure some objects were inlined. Some of them may not get inlined
    # because we evict the object soon after creating it.
    assert inlined > 0

    # Non-inlined objects are not able to be recreated after eviction.
    for _ in range(10):
        non_inline_object = a.create_non_inline_object.remote()
        ray.get(non_inline_object)
        plasma_id = ray.pyarrow.plasma.ObjectID(non_inline_object.binary())
        # This while loop is necessary because sometimes the object is still
        # there immediately after plasma_client.delete.
        while ray.worker.global_worker.plasma_client.contains(plasma_id):
            ray.worker.global_worker.plasma_client.delete([plasma_id])
        # Objects created by an actor that were evicted and larger than the
        # maximum inline object size cannot be retrieved or reconstructed.
        with pytest.raises(ray.worker.RayTaskError):
            ray.get(non_inline_object) == 10000 * [1]


def test_ray_setproctitle(shutdown_only):
    ray.init(num_cpus=2)

    @ray.remote
    class UniqueName(object):
        def __init__(self):
            assert setproctitle.getproctitle() == ""ray_UniqueName:__init__()""

        def f(self):
            assert setproctitle.getproctitle() == ""ray_UniqueName:f()""

    @ray.remote
    def unique_1():
        assert setproctitle.getproctitle() == ""ray_worker:runtest.unique_1()""

    actor = UniqueName.remote()
    ray.get(actor.f.remote())
    ray.get(unique_1.remote())


def test_duplicate_error_messages(shutdown_only):
    ray.init(num_cpus=0)

    driver_id = ray.DriverID.nil()
    error_data = ray.gcs_utils.construct_error_message(driver_id, ""test"",
                                                       ""message"", 0)

    # Push the same message to the GCS twice (they are the same because we
    # do not include a timestamp).

    r = ray.worker.global_worker.redis_client

    r.execute_command(""RAY.TABLE_APPEND"", ray.gcs_utils.TablePrefix.ERROR_INFO,
                      ray.gcs_utils.TablePubsub.ERROR_INFO, driver_id.binary(),
                      error_data)

    # Before https://github.com/ray-project/ray/pull/3316 this would
    # give an error
    r.execute_command(""RAY.TABLE_APPEND"", ray.gcs_utils.TablePrefix.ERROR_INFO,
                      ray.gcs_utils.TablePubsub.ERROR_INFO, driver_id.binary(),
                      error_data)


@pytest.mark.skipif(
    os.getenv(""TRAVIS"") is None,
    reason=""This test should only be run on Travis."")
def test_ray_stack(shutdown_only):
    ray.init(num_cpus=2)

    def unique_name_1():
        time.sleep(1000)

    @ray.remote
    def unique_name_2():
        time.sleep(1000)

    @ray.remote
    def unique_name_3():
        unique_name_1()

    unique_name_2.remote()
    unique_name_3.remote()

    success = False
    start_time = time.time()
    while time.time() - start_time < 30:
        # Attempt to parse the ""ray stack"" call.
        output = ray.utils.decode(subprocess.check_output([""ray"", ""stack""]))
        if (""unique_name_1"" in output and ""unique_name_2"" in output
                and ""unique_name_3"" in output):
            success = True
            break

    if not success:
        raise Exception(""Failed to find necessary information with ""
                        ""'ray stack'"")


def test_pandas_parquet_serialization():
    # Only test this if pandas is installed
    pytest.importorskip(""pandas"")

    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq

    tempdir = tempfile.mkdtemp()
    filename = os.path.join(tempdir, ""parquet-test"")
    pd.DataFrame({""col1"": [0, 1], ""col2"": [0, 1]}).to_parquet(filename)
    with open(os.path.join(tempdir, ""parquet-compression""), ""wb"") as f:
        table = pa.Table.from_arrays([pa.array([1, 2, 3])], [""hello""])
        pq.write_table(table, f, compression=""lz4"")
    # Clean up
    shutil.rmtree(tempdir)


def test_socket_dir_not_existing(shutdown_only):
    random_name = ray.ObjectID(_random_string()).hex()
    temp_raylet_socket_dir = ""/tmp/ray/tests/{}"".format(random_name)
    temp_raylet_socket_name = os.path.join(temp_raylet_socket_dir,
                                           ""raylet_socket"")
    ray.init(num_cpus=1, raylet_socket_name=temp_raylet_socket_name)
/n/n/ntest/tempfile_test.py/n/nimport os
import shutil
import time
import pytest
import ray
import ray.tempfile_services as tempfile_services


def test_conn_cluster():
    # plasma_store_socket_name
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"",
            plasma_store_socket_name=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""plasma_store_socket_name must not be provided."")

    # raylet_socket_name
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"",
            raylet_socket_name=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""raylet_socket_name must not be provided."")

    # temp_dir
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"", temp_dir=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""temp_dir must not be provided."")


def test_tempdir():
    ray.init(temp_dir=""/tmp/i_am_a_temp_dir"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_dir""), ""Specified temp dir not found.""
    ray.shutdown()
    shutil.rmtree(""/tmp/i_am_a_temp_dir"", ignore_errors=True)


def test_raylet_socket_name():
    ray.init(raylet_socket_name=""/tmp/i_am_a_temp_socket"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_socket""), ""Specified socket path not found.""
    ray.shutdown()
    try:
        os.remove(""/tmp/i_am_a_temp_socket"")
    except Exception:
        pass


def test_temp_plasma_store_socket():
    ray.init(plasma_store_socket_name=""/tmp/i_am_a_temp_socket"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_socket""), ""Specified socket path not found.""
    ray.shutdown()
    try:
        os.remove(""/tmp/i_am_a_temp_socket"")
    except Exception:
        pass


def test_raylet_tempfiles():
    ray.init(redirect_worker_output=False)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files == {
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    }  # with raylet logs
    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()

    ray.init(redirect_worker_output=True, num_cpus=0)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files == {
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    }  # with raylet logs
    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()

    ray.init(redirect_worker_output=True, num_cpus=2)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    time.sleep(3)  # wait workers to start
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files.issuperset({
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    })  # with raylet logs

    # Check numbers of worker log file.
    assert sum(
        1 for filename in log_files if filename.startswith(""worker"")) == 4

    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()
/n/n/n",0
59,59,ef527f84abf0cee7ac6ad832828ff92311440ee4,"/python/ray/import_thread.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import traceback

import redis

import ray
from ray import ray_constants
from ray import cloudpickle as pickle
from ray import profiling
from ray import utils


class ImportThread(object):
    """"""A thread used to import exports from the driver or other workers.

    Note:
    The driver also has an import thread, which is used only to
    import custom class definitions from calls to register_custom_serializer
    that happen under the hood on workers.

    Attributes:
        worker: the worker object in this process.
        mode: worker mode
        redis_client: the redis client used to query exports.
    """"""

    def __init__(self, worker, mode):
        self.worker = worker
        self.mode = mode
        self.redis_client = worker.redis_client

    def start(self):
        """"""Start the import thread.""""""
        t = threading.Thread(target=self._run, name=""ray_import_thread"")
        # Making the thread a daemon causes it to exit
        # when the main thread exits.
        t.daemon = True
        t.start()

    def _run(self):
        import_pubsub_client = self.redis_client.pubsub()
        # Exports that are published after the call to
        # import_pubsub_client.subscribe and before the call to
        # import_pubsub_client.listen will still be processed in the loop.
        import_pubsub_client.subscribe(""__keyspace@0__:Exports"")
        # Keep track of the number of imports that we've imported.
        num_imported = 0

        # Get the exports that occurred before the call to subscribe.
        with self.worker.lock:
            export_keys = self.redis_client.lrange(""Exports"", 0, -1)
            for key in export_keys:
                num_imported += 1
                self._process_key(key)
        try:
            for msg in import_pubsub_client.listen():
                with self.worker.lock:
                    if msg[""type""] == ""subscribe"":
                        continue
                    assert msg[""data""] == b""rpush""
                    num_imports = self.redis_client.llen(""Exports"")
                    assert num_imports >= num_imported
                    for i in range(num_imported, num_imports):
                        num_imported += 1
                        key = self.redis_client.lindex(""Exports"", i)
                        self._process_key(key)
        except redis.ConnectionError:
            # When Redis terminates the listen call will throw a
            # ConnectionError, which we catch here.
            pass

    def _process_key(self, key):
        """"""Process the given export key from redis.""""""
        # Handle the driver case first.
        if self.mode != ray.WORKER_MODE:
            if key.startswith(b""FunctionsToRun""):
                with profiling.profile(
                        ""fetch_and_run_function"", worker=self.worker):
                    self.fetch_and_execute_function_to_run(key)
            # Return because FunctionsToRun are the only things that
            # the driver should import.
            return

        if key.startswith(b""RemoteFunction""):
            with profiling.profile(
                    ""register_remote_function"", worker=self.worker):
                (self.worker.function_actor_manager.
                 fetch_and_register_remote_function(key))
        elif key.startswith(b""FunctionsToRun""):
            with profiling.profile(
                    ""fetch_and_run_function"", worker=self.worker):
                self.fetch_and_execute_function_to_run(key)
        elif key.startswith(b""ActorClass""):
            # Keep track of the fact that this actor class has been
            # exported so that we know it is safe to turn this worker
            # into an actor of that class.
            self.worker.function_actor_manager.imported_actor_classes.add(key)
        # TODO(rkn): We may need to bring back the case of
        # fetching actor classes here.
        else:
            raise Exception(""This code should be unreachable."")

    def fetch_and_execute_function_to_run(self, key):
        """"""Run on arbitrary function on the worker.""""""
        (driver_id, serialized_function,
         run_on_other_drivers) = self.redis_client.hmget(
             key, [""driver_id"", ""function"", ""run_on_other_drivers""])

        if (utils.decode(run_on_other_drivers) == ""False""
                and self.worker.mode == ray.SCRIPT_MODE
                and driver_id != self.worker.task_driver_id.binary()):
            return

        try:
            # Deserialize the function.
            function = pickle.loads(serialized_function)
            # Run the function.
            function({""worker"": self.worker})
        except Exception:
            # If an exception was thrown when the function was run, we record
            # the traceback and notify the scheduler of the failure.
            traceback_str = traceback.format_exc()
            # Log the error message.
            utils.push_error_to_driver(
                self.worker,
                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,
                traceback_str,
                driver_id=ray.DriverID(driver_id))
/n/n/n/python/ray/log_monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import redis
import time

import ray.ray_constants as ray_constants
from ray.services import get_ip_address
from ray.services import get_port
import ray.utils

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)


class LogMonitor(object):
    """"""A monitor process for monitoring Ray log files.

    Attributes:
        node_ip_address: The IP address of the node that the log monitor
            process is running on. This will be used to determine which log
            files to track.
        redis_client: A client used to communicate with the Redis server.
        log_files: A dictionary mapping the name of a log file to a list of
            strings representing its contents.
        log_file_handles: A dictionary mapping the name of a log file to a file
            handle for that file.
    """"""

    def __init__(self,
                 redis_ip_address,
                 redis_port,
                 node_ip_address,
                 redis_password=None):
        """"""Initialize the log monitor object.""""""
        self.node_ip_address = node_ip_address
        self.redis_client = redis.StrictRedis(
            host=redis_ip_address, port=redis_port, password=redis_password)
        self.log_files = {}
        self.log_file_handles = {}
        self.files_to_ignore = set()

    def update_log_filenames(self):
        """"""Get the most up-to-date list of log files to monitor from Redis.""""""
        num_current_log_files = len(self.log_files)
        new_log_filenames = self.redis_client.lrange(
            ""LOG_FILENAMES:{}"".format(self.node_ip_address),
            num_current_log_files, -1)
        for log_filename in new_log_filenames:
            logger.info(""Beginning to track file {}"".format(log_filename))
            assert log_filename not in self.log_files
            self.log_files[log_filename] = []

    def check_log_files_and_push_updates(self):
        """"""Get any changes to the log files and push updates to Redis.""""""
        for log_filename in self.log_files:
            if log_filename in self.log_file_handles:
                # Get any updates to the file.
                new_lines = []
                while True:
                    current_position = (
                        self.log_file_handles[log_filename].tell())
                    next_line = self.log_file_handles[log_filename].readline()
                    if next_line != """":
                        new_lines.append(next_line)
                    else:
                        self.log_file_handles[log_filename].seek(
                            current_position)
                        break

                # If there are any new lines, cache them and also push them to
                # Redis.
                if len(new_lines) > 0:
                    self.log_files[log_filename] += new_lines
                    redis_key = ""LOGFILE:{}:{}"".format(
                        self.node_ip_address, ray.utils.decode(log_filename))
                    self.redis_client.rpush(redis_key, *new_lines)

            # Pass if we already failed to open the log file.
            elif log_filename in self.files_to_ignore:
                pass

            # Try to open this file for the first time.
            else:
                try:
                    self.log_file_handles[log_filename] = open(
                        log_filename, ""r"")
                except IOError as e:
                    if e.errno == os.errno.EMFILE:
                        logger.warning(
                            ""Warning: Ignoring {} because there are too ""
                            ""many open files."".format(log_filename))
                    elif e.errno == os.errno.ENOENT:
                        logger.warning(""Warning: The file {} was not ""
                                       ""found."".format(log_filename))
                    else:
                        raise e

                    # Don't try to open this file any more.
                    self.files_to_ignore.add(log_filename)

    def run(self):
        """"""Run the log monitor.

        This will query Redis once every second to check if there are new log
        files to monitor. It will also store those log files in Redis.
        """"""
        while True:
            self.update_log_filenames()
            self.check_log_files_and_push_updates()
            time.sleep(1)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""log monitor to connect ""
                     ""to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""The address to use for Redis."")
    parser.add_argument(
        ""--node-ip-address"",
        required=True,
        type=str,
        help=""The IP address of the node this process is on."")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    ray.utils.setup_logger(args.logging_level, args.logging_format)

    redis_ip_address = get_ip_address(args.redis_address)
    redis_port = get_port(args.redis_address)

    log_monitor = LogMonitor(
        redis_ip_address,
        redis_port,
        args.node_ip_address,
        redis_password=args.redis_password)
    log_monitor.run()
/n/n/n/python/ray/monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import time
import traceback

import redis

import ray
from ray.autoscaler.autoscaler import LoadMetrics, StandardAutoscaler
import ray.cloudpickle as pickle
import ray.gcs_utils
import ray.utils
import ray.ray_constants as ray_constants
from ray.services import get_ip_address, get_port
from ray.utils import (binary_to_hex, binary_to_object_id, hex_to_binary,
                       setup_logger)

logger = logging.getLogger(__name__)


class Monitor(object):
    """"""A monitor for Ray processes.

    The monitor is in charge of cleaning up the tables in the global state
    after processes have died. The monitor is currently not responsible for
    detecting component failures.

    Attributes:
        redis: A connection to the Redis server.
        subscribe_client: A pubsub client for the Redis server. This is used to
            receive notifications about failed components.
    """"""

    def __init__(self,
                 redis_address,
                 redis_port,
                 autoscaling_config,
                 redis_password=None):
        # Initialize the Redis clients.
        self.state = ray.experimental.state.GlobalState()
        self.state._initialize_global_state(
            redis_address, redis_port, redis_password=redis_password)
        self.redis = redis.StrictRedis(
            host=redis_address, port=redis_port, db=0, password=redis_password)
        # Setup subscriptions to the primary Redis server and the Redis shards.
        self.primary_subscribe_client = self.redis.pubsub(
            ignore_subscribe_messages=True)
        # Keep a mapping from local scheduler client ID to IP address to use
        # for updating the load metrics.
        self.local_scheduler_id_to_ip_map = {}
        self.load_metrics = LoadMetrics()
        if autoscaling_config:
            self.autoscaler = StandardAutoscaler(autoscaling_config,
                                                 self.load_metrics)
        else:
            self.autoscaler = None

        # Experimental feature: GCS flushing.
        self.issue_gcs_flushes = ""RAY_USE_NEW_GCS"" in os.environ
        self.gcs_flush_policy = None
        if self.issue_gcs_flushes:
            # Data is stored under the first data shard, so we issue flushes to
            # that redis server.
            addr_port = self.redis.lrange(""RedisShards"", 0, -1)
            if len(addr_port) > 1:
                logger.warning(
                    ""Monitor: ""
                    ""TODO: if launching > 1 redis shard, flushing needs to ""
                    ""touch shards in parallel."")
                self.issue_gcs_flushes = False
            else:
                addr_port = addr_port[0].split(b"":"")
                self.redis_shard = redis.StrictRedis(
                    host=addr_port[0],
                    port=addr_port[1],
                    password=redis_password)
                try:
                    self.redis_shard.execute_command(""HEAD.FLUSH 0"")
                except redis.exceptions.ResponseError as e:
                    logger.info(
                        ""Monitor: ""
                        ""Turning off flushing due to exception: {}"".format(
                            str(e)))
                    self.issue_gcs_flushes = False

    def subscribe(self, channel):
        """"""Subscribe to the given channel on the primary Redis shard.

        Args:
            channel (str): The channel to subscribe to.

        Raises:
            Exception: An exception is raised if the subscription fails.
        """"""
        self.primary_subscribe_client.subscribe(channel)

    def xray_heartbeat_batch_handler(self, unused_channel, data):
        """"""Handle an xray heartbeat batch message from Redis.""""""

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        heartbeat_data = gcs_entries.Entries(0)

        message = (ray.gcs_utils.HeartbeatBatchTableData.
                   GetRootAsHeartbeatBatchTableData(heartbeat_data, 0))

        for j in range(message.BatchLength()):
            heartbeat_message = message.Batch(j)

            num_resources = heartbeat_message.ResourcesAvailableLabelLength()
            static_resources = {}
            dynamic_resources = {}
            for i in range(num_resources):
                dyn = heartbeat_message.ResourcesAvailableLabel(i)
                static = heartbeat_message.ResourcesTotalLabel(i)
                dynamic_resources[dyn] = (
                    heartbeat_message.ResourcesAvailableCapacity(i))
                static_resources[static] = (
                    heartbeat_message.ResourcesTotalCapacity(i))

            # Update the load metrics for this local scheduler.
            client_id = ray.utils.binary_to_hex(heartbeat_message.ClientId())
            ip = self.local_scheduler_id_to_ip_map.get(client_id)
            if ip:
                self.load_metrics.update(ip, static_resources,
                                         dynamic_resources)
            else:
                logger.warning(
                    ""Monitor: ""
                    ""could not find ip for client {}"".format(client_id))

    def _xray_clean_up_entries_for_driver(self, driver_id):
        """"""Remove this driver's object/task entries from redis.

        Removes control-state entries of all tasks and task return
        objects belonging to the driver.

        Args:
            driver_id: The driver id.
        """"""

        xray_task_table_prefix = (
            ray.gcs_utils.TablePrefix_RAYLET_TASK_string.encode(""ascii""))
        xray_object_table_prefix = (
            ray.gcs_utils.TablePrefix_OBJECT_string.encode(""ascii""))

        task_table_objects = self.state.task_table()
        driver_id_hex = binary_to_hex(driver_id)
        driver_task_id_bins = set()
        for task_id_hex, task_info in task_table_objects.items():
            task_table_object = task_info[""TaskSpec""]
            task_driver_id_hex = task_table_object[""DriverID""]
            if driver_id_hex != task_driver_id_hex:
                # Ignore tasks that aren't from this driver.
                continue
            driver_task_id_bins.add(hex_to_binary(task_id_hex))

        # Get objects associated with the driver.
        object_table_objects = self.state.object_table()
        driver_object_id_bins = set()
        for object_id, _ in object_table_objects.items():
            task_id_bin = ray._raylet.compute_task_id(object_id).binary()
            if task_id_bin in driver_task_id_bins:
                driver_object_id_bins.add(object_id.binary())

        def to_shard_index(id_bin):
            return binary_to_object_id(id_bin).redis_shard_hash() % len(
                self.state.redis_clients)

        # Form the redis keys to delete.
        sharded_keys = [[] for _ in range(len(self.state.redis_clients))]
        for task_id_bin in driver_task_id_bins:
            sharded_keys[to_shard_index(task_id_bin)].append(
                xray_task_table_prefix + task_id_bin)
        for object_id_bin in driver_object_id_bins:
            sharded_keys[to_shard_index(object_id_bin)].append(
                xray_object_table_prefix + object_id_bin)

        # Remove with best effort.
        for shard_index in range(len(sharded_keys)):
            keys = sharded_keys[shard_index]
            if len(keys) == 0:
                continue
            redis = self.state.redis_clients[shard_index]
            num_deleted = redis.delete(*keys)
            logger.info(""Monitor: ""
                        ""Removed {} dead redis entries of the ""
                        ""driver from redis shard {}."".format(
                            num_deleted, shard_index))
            if num_deleted != len(keys):
                logger.warning(""Monitor: ""
                               ""Failed to remove {} relevant redis ""
                               ""entries from redis shard {}."".format(
                                   len(keys) - num_deleted, shard_index))

    def xray_driver_removed_handler(self, unused_channel, data):
        """"""Handle a notification that a driver has been removed.

        Args:
            unused_channel: The message channel.
            data: The message data.
        """"""
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        driver_data = gcs_entries.Entries(0)
        message = ray.gcs_utils.DriverTableData.GetRootAsDriverTableData(
            driver_data, 0)
        driver_id = message.DriverId()
        logger.info(""Monitor: ""
                    ""XRay Driver {} has been removed."".format(
                        binary_to_hex(driver_id)))
        self._xray_clean_up_entries_for_driver(driver_id)

    def process_messages(self, max_messages=10000):
        """"""Process all messages ready in the subscription channels.

        This reads messages from the subscription channels and calls the
        appropriate handlers until there are no messages left.

        Args:
            max_messages: The maximum number of messages to process before
                returning.
        """"""
        subscribe_clients = [self.primary_subscribe_client]
        for subscribe_client in subscribe_clients:
            for _ in range(max_messages):
                message = subscribe_client.get_message()
                if message is None:
                    # Continue on to the next subscribe client.
                    break

                # Parse the message.
                channel = message[""channel""]
                data = message[""data""]

                # Determine the appropriate message handler.
                if channel == ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL:
                    # Similar functionality as local scheduler info channel
                    message_handler = self.xray_heartbeat_batch_handler
                elif channel == ray.gcs_utils.XRAY_DRIVER_CHANNEL:
                    # Handles driver death.
                    message_handler = self.xray_driver_removed_handler
                else:
                    raise Exception(""This code should be unreachable."")

                # Call the handler.
                message_handler(channel, data)

    def update_local_scheduler_map(self):
        local_schedulers = self.state.client_table()
        self.local_scheduler_id_to_ip_map = {}
        for local_scheduler_info in local_schedulers:
            client_id = local_scheduler_info.get(""DBClientID"") or \
                local_scheduler_info[""ClientID""]
            ip_address = (
                local_scheduler_info.get(""AuxAddress"")
                or local_scheduler_info[""NodeManagerAddress""]).split("":"")[0]
            self.local_scheduler_id_to_ip_map[client_id] = ip_address

    def _maybe_flush_gcs(self):
        """"""Experimental: issue a flush request to the GCS.

        The purpose of this feature is to control GCS memory usage.

        To activate this feature, Ray must be compiled with the flag
        RAY_USE_NEW_GCS set, and Ray must be started at run time with the flag
        as well.
        """"""
        if not self.issue_gcs_flushes:
            return
        if self.gcs_flush_policy is None:
            serialized = self.redis.get(""gcs_flushing_policy"")
            if serialized is None:
                # Client has not set any policy; by default flushing is off.
                return
            self.gcs_flush_policy = pickle.loads(serialized)

        if not self.gcs_flush_policy.should_flush(self.redis_shard):
            return

        max_entries_to_flush = self.gcs_flush_policy.num_entries_to_flush()
        num_flushed = self.redis_shard.execute_command(
            ""HEAD.FLUSH {}"".format(max_entries_to_flush))
        logger.info(""Monitor: num_flushed {}"".format(num_flushed))

        # This flushes event log and log files.
        ray.experimental.flush_redis_unsafe(self.redis)

        self.gcs_flush_policy.record_flush()

    def run(self):
        """"""Run the monitor.

        This function loops forever, checking for messages about dead database
        clients and cleaning up state accordingly.
        """"""
        # Initialize the subscription channel.
        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)
        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)

        # TODO(rkn): If there were any dead clients at startup, we should clean
        # up the associated state in the state tables.

        # Handle messages from the subscription channels.
        while True:
            # Update the mapping from local scheduler client ID to IP address.
            # This is only used to update the load metrics for the autoscaler.
            self.update_local_scheduler_map()

            # Process autoscaling actions
            if self.autoscaler:
                self.autoscaler.update()

            self._maybe_flush_gcs()

            # Process a round of messages.
            self.process_messages()

            # Wait for a heartbeat interval before processing the next round of
            # messages.
            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)

        # TODO(rkn): This infinite loop should be inside of a try/except block,
        # and if an exception is thrown we should push an error message to all
        # drivers.


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""monitor to connect to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""the address to use for Redis"")
    parser.add_argument(
        ""--autoscaling-config"",
        required=False,
        type=str,
        help=""the path to the autoscaling config file"")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    setup_logger(args.logging_level, args.logging_format)

    redis_ip_address = get_ip_address(args.redis_address)
    redis_port = get_port(args.redis_address)

    if args.autoscaling_config:
        autoscaling_config = os.path.expanduser(args.autoscaling_config)
    else:
        autoscaling_config = None

    monitor = Monitor(
        redis_ip_address,
        redis_port,
        autoscaling_config,
        redis_password=args.redis_password)

    try:
        monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = redis.StrictRedis(
            host=redis_ip_address,
            port=redis_port,
            password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = ""The monitor failed with the following error:\n{}"".format(
            traceback_str)
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)
        raise e
/n/n/n/python/ray/node.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import atexit
import json
import os
import logging
import signal
import threading
import time

import ray
import ray.ray_constants as ray_constants
from ray.tempfile_services import (
    get_logs_dir_path, get_object_store_socket_name, get_raylet_socket_name,
    new_log_monitor_log_file, new_monitor_log_file,
    new_raylet_monitor_log_file, new_plasma_store_log_file,
    new_raylet_log_file, new_webui_log_file, set_temp_root,
    try_to_create_directory)

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray configures it by default automatically
# using logging.basicConfig in its entry/init points.
logger = logging.getLogger(__name__)


class Node(object):
    """"""An encapsulation of the Ray processes on a single node.

    This class is responsible for starting Ray processes and killing them.

    Attributes:
        all_processes (dict): A mapping from process type (str) to a list of
            ProcessInfo objects. All lists have length one except for the Redis
            server list, which has multiple.
    """"""

    def __init__(self, ray_params, head=False, shutdown_at_exit=True):
        """"""Start a node.

        Args:
            ray_params (ray.params.RayParams): The parameters to use to
                configure the node.
            head (bool): True if this is the head node, which means it will
                start additional processes like the Redis servers, monitor
                processes, and web UI.
            shutdown_at_exit (bool): If true, a handler will be registered to
                shutdown the processes started here when the Python interpreter
                exits.
        """"""
        self.all_processes = {}

        ray_params.update_if_absent(
            node_ip_address=ray.services.get_node_ip_address(),
            include_log_monitor=True,
            resources={},
            include_webui=False,
            worker_path=os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                ""workers/default_worker.py""))

        if head:
            ray_params.update_if_absent(num_redis_shards=1, include_webui=True)
        else:
            redis_client = ray.services.create_redis_client(
                ray_params.redis_address, ray_params.redis_password)
            ray_params.include_java = (
                ray.services.include_java_from_redis(redis_client))

        self._ray_params = ray_params
        self._config = (json.loads(ray_params._internal_config)
                        if ray_params._internal_config else None)
        self._node_ip_address = ray_params.node_ip_address
        self._redis_address = ray_params.redis_address
        self._plasma_store_socket_name = None
        self._raylet_socket_name = None
        self._webui_url = None

        self.start_ray_processes()

        if shutdown_at_exit:
            atexit.register(lambda: self.kill_all_processes(
                check_alive=False, allow_graceful=True))

    @property
    def node_ip_address(self):
        """"""Get the cluster Redis address.""""""
        return self._node_ip_address

    @property
    def redis_address(self):
        """"""Get the cluster Redis address.""""""
        return self._redis_address

    @property
    def plasma_store_socket_name(self):
        """"""Get the node's plasma store socket name.""""""
        return self._plasma_store_socket_name

    @property
    def webui_url(self):
        """"""Get the cluster's web UI url.""""""
        return self._webui_url

    @property
    def raylet_socket_name(self):
        """"""Get the node's raylet socket name.""""""
        return self._raylet_socket_name

    def prepare_socket_file(self, socket_path):
        """"""Prepare the socket file for raylet and plasma.

        This method helps to prepare a socket file.
        1. Make the directory if the directory does not exist.
        2. If the socket file exists, raise exception.

        Args:
            socket_path (string): the socket file to prepare.
        """"""
        if not os.path.exists(socket_path):
            path = os.path.dirname(socket_path)
            if not os.path.isdir(path):
                try_to_create_directory(path)
        else:
            raise Exception(""Socket file {} exists!"".format(socket_path))

    def start_redis(self):
        """"""Start the Redis servers.""""""
        assert self._redis_address is None
        (self._redis_address, redis_shards,
         process_infos) = ray.services.start_redis(
             self._node_ip_address,
             port=self._ray_params.redis_port,
             redis_shard_ports=self._ray_params.redis_shard_ports,
             num_redis_shards=self._ray_params.num_redis_shards,
             redis_max_clients=self._ray_params.redis_max_clients,
             redirect_output=self._ray_params.redirect_output,
             redirect_worker_output=self._ray_params.redirect_worker_output,
             password=self._ray_params.redis_password,
             redis_max_memory=self._ray_params.redis_max_memory)
        assert (
            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (
            process_infos)

    def start_log_monitor(self):
        """"""Start the log monitor.""""""
        stdout_file, stderr_file = new_log_monitor_log_file()
        process_info = ray.services.start_log_monitor(
            self.redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_LOG_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] = [
            process_info
        ]

    def start_ui(self):
        """"""Start the web UI.""""""
        stdout_file, stderr_file = new_webui_log_file()
        self._webui_url, process_info = ray.services.start_ui(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        assert ray_constants.PROCESS_TYPE_WEB_UI not in self.all_processes
        if process_info is not None:
            self.all_processes[ray_constants.PROCESS_TYPE_WEB_UI] = [
                process_info
            ]

    def start_plasma_store(self):
        """"""Start the plasma store.""""""
        assert self._plasma_store_socket_name is None
        # If the user specified a socket name, use it.
        self._plasma_store_socket_name = (
            self._ray_params.plasma_store_socket_name
            or get_object_store_socket_name())
        self.prepare_socket_file(self._plasma_store_socket_name)
        stdout_file, stderr_file = (new_plasma_store_log_file(
            self._ray_params.redirect_output))
        process_info = ray.services.start_plasma_store(
            self._node_ip_address,
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            object_store_memory=self._ray_params.object_store_memory,
            plasma_directory=self._ray_params.plasma_directory,
            huge_pages=self._ray_params.huge_pages,
            plasma_store_socket_name=self._plasma_store_socket_name,
            redis_password=self._ray_params.redis_password)
        assert (
            ray_constants.PROCESS_TYPE_PLASMA_STORE not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_PLASMA_STORE] = [
            process_info
        ]

    def start_raylet(self, use_valgrind=False, use_profiler=False):
        """"""Start the raylet.

        Args:
            use_valgrind (bool): True if we should start the process in
                valgrind.
            use_profiler (bool): True if we should start the process in the
                valgrind profiler.
        """"""
        assert self._raylet_socket_name is None
        # If the user specified a socket name, use it.
        self._raylet_socket_name = (self._ray_params.raylet_socket_name
                                    or get_raylet_socket_name())
        self.prepare_socket_file(self._raylet_socket_name)
        stdout_file, stderr_file = new_raylet_log_file(
            redirect_output=self._ray_params.redirect_worker_output)
        process_info = ray.services.start_raylet(
            self._redis_address,
            self._node_ip_address,
            self._raylet_socket_name,
            self._plasma_store_socket_name,
            self._ray_params.worker_path,
            self._ray_params.num_cpus,
            self._ray_params.num_gpus,
            self._ray_params.resources,
            self._ray_params.object_manager_port,
            self._ray_params.node_manager_port,
            self._ray_params.redis_password,
            use_valgrind=use_valgrind,
            use_profiler=use_profiler,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            config=self._config,
            include_java=self._ray_params.include_java,
            java_worker_options=self._ray_params.java_worker_options,
        )
        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]

    def start_worker(self):
        """"""Start a worker process.""""""
        raise NotImplementedError

    def start_monitor(self):
        """"""Start the monitor.""""""
        stdout_file, stderr_file = new_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_monitor(
            self._redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            autoscaling_config=self._ray_params.autoscaling_config,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_MONITOR] = [process_info]

    def start_raylet_monitor(self):
        """"""Start the raylet monitor.""""""
        stdout_file, stderr_file = new_raylet_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_raylet_monitor(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password,
            config=self._config)
        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in
                self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [
            process_info
        ]

    def start_ray_processes(self):
        """"""Start all of the processes on the node.""""""
        set_temp_root(self._ray_params.temp_dir)
        logger.info(
            ""Process STDOUT and STDERR is being redirected to {}."".format(
                get_logs_dir_path()))

        # If this is the head node, start the relevant head node processes.
        if self._redis_address is None:
            self.start_redis()
            self.start_monitor()
            self.start_raylet_monitor()

        self.start_plasma_store()
        self.start_raylet()

        if self._ray_params.include_log_monitor:
            self.start_log_monitor()
        if self._ray_params.include_webui:
            self.start_ui()

    def _kill_process_type(self,
                           process_type,
                           allow_graceful=False,
                           check_alive=True,
                           wait=False):
        """"""Kill a process of a given type.

        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all
        of the Redis servers.

        If the process was started in valgrind, then we will raise an exception
        if the process has a non-zero exit code.

        Args:
            process_type: The type of the process to kill.
            allow_graceful (bool): Send a SIGTERM first and give the process
                time to exit gracefully. If that doesn't work, then use
                SIGKILL. We usually want to do this outside of tests.
            check_alive (bool): If true, then we expect the process to be alive
                and will raise an exception if the process is already dead.
            wait (bool): If true, then this method will not return until the
                process in question has exited.

        Raises:
            This process raises an exception in the following cases:
                1. The process had already died and check_alive is true.
                2. The process had been started in valgrind and had a non-zero
                   exit code.
        """"""
        process_infos = self.all_processes[process_type]
        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:
            assert len(process_infos) == 1
        for process_info in process_infos:
            process = process_info.process
            # Handle the case where the process has already exited.
            if process.poll() is not None:
                if check_alive:
                    raise Exception(""Attempting to kill a process of type ""
                                    ""'{}', but this process is already dead.""
                                    .format(process_type))
                else:
                    continue

            if process_info.use_valgrind:
                process.terminate()
                process.wait()
                if process.returncode != 0:
                    message = (""Valgrind detected some errors in process of ""
                               ""type {}. Error code {}."".format(
                                   process_type, process.returncode))
                    if process_info.stdout_file is not None:
                        with open(process_info.stdout_file, ""r"") as f:
                            message += ""\nPROCESS STDOUT:\n"" + f.read()
                    if process_info.stderr_file is not None:
                        with open(process_info.stderr_file, ""r"") as f:
                            message += ""\nPROCESS STDERR:\n"" + f.read()
                    raise Exception(message)
                continue

            if process_info.use_valgrind_profiler:
                # Give process signal to write profiler data.
                os.kill(process.pid, signal.SIGINT)
                # Wait for profiling data to be written.
                time.sleep(0.1)

            if allow_graceful:
                # Allow the process one second to exit gracefully.
                process.terminate()
                timer = threading.Timer(1, lambda process: process.kill(),
                                        [process])
                try:
                    timer.start()
                    process.wait()
                finally:
                    timer.cancel()

                if process.poll() is not None:
                    continue

            # If the process did not exit within one second, force kill it.
            process.kill()
            # The reason we usually don't call process.wait() here is that
            # there's some chance we'd end up waiting a really long time.
            if wait:
                process.wait()

        del self.all_processes[process_type]

    def kill_redis(self, check_alive=True):
        """"""Kill the Redis servers.

        Args:
            check_alive (bool): Raise an exception if any of the processes
                were already dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)

    def kill_plasma_store(self, check_alive=True):
        """"""Kill the plasma store.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)

    def kill_raylet(self, check_alive=True):
        """"""Kill the raylet.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET, check_alive=check_alive)

    def kill_log_monitor(self, check_alive=True):
        """"""Kill the log monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_LOG_MONITOR, check_alive=check_alive)

    def kill_monitor(self, check_alive=True):
        """"""Kill the monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_MONITOR, check_alive=check_alive)

    def kill_raylet_monitor(self, check_alive=True):
        """"""Kill the raylet monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET_MONITOR, check_alive=check_alive)

    def kill_all_processes(self, check_alive=True, allow_graceful=False):
        """"""Kill all of the processes.

        Note that This is slower than necessary because it calls kill, wait,
        kill, wait, ... instead of kill, kill, ..., wait, wait, ...

        Args:
            check_alive (bool): Raise an exception if any of the processes were
                already dead.
        """"""
        # Kill the raylet first. This is important for suppressing errors at
        # shutdown because we give the raylet a chance to exit gracefully and
        # clean up its child worker processes. If we were to kill the plasma
        # store (or Redis) first, that could cause the raylet to exit
        # ungracefully, leading to more verbose output from the workers.
        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:
            self._kill_process_type(
                ray_constants.PROCESS_TYPE_RAYLET,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

        # We call ""list"" to copy the keys because we are modifying the
        # dictionary while iterating over it.
        for process_type in list(self.all_processes.keys()):
            self._kill_process_type(
                process_type,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

    def live_processes(self):
        """"""Return a list of the live processes.

        Returns:
            A list of the live processes.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is None:
                    result.append((process_type, process_info.process))
        return result

    def dead_processes(self):
        """"""Return a list of the dead processes.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            A list of the dead processes ignoring the ones that have been
                explicitly killed.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is not None:
                    result.append((process_type, process_info.process))
        return result

    def any_processes_alive(self):
        """"""Return true if any processes are still alive.

        Returns:
            True if any process is still alive.
        """"""
        return any(self.live_processes())

    def remaining_processes_alive(self):
        """"""Return true if all remaining processes are still alive.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            True if any process that wasn't explicitly killed is still alive.
        """"""
        return not any(self.dead_processes())
/n/n/n/python/ray/parameter.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging

import ray.ray_constants as ray_constants


class RayParams(object):
    """"""A class used to store the parameters used by Ray.

    Attributes:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        redis_port (int): The port that the primary Redis shard should listen
            to. If None, then a random port will be chosen.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_cpus (int): Number of CPUs to configure the raylet with.
        num_gpus (int): Number of GPUs to configure the raylet with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries. This only applies to the
            sharded redis tables (task and object tables).
        object_manager_port int: The port to use for the object manager.
        node_manager_port: The port to use for the node manager.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        worker_path (str): The path of the source code that will be run by the
            worker.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        include_log_monitor (bool): If True, then start a log monitor to
            monitor the log files for all processes on this node and push their
            contents to Redis.
        autoscaling_config: path to autoscaling config file.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.
    """"""

    def __init__(self,
                 redis_address=None,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_store_memory=None,
                 redis_max_memory=None,
                 redis_port=None,
                 redis_shard_ports=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 node_ip_address=None,
                 object_id_seed=None,
                 num_workers=None,
                 local_mode=False,
                 driver_mode=None,
                 redirect_worker_output=False,
                 redirect_output=True,
                 num_redis_shards=None,
                 redis_max_clients=None,
                 redis_password=None,
                 plasma_directory=None,
                 worker_path=None,
                 huge_pages=False,
                 include_webui=None,
                 logging_level=logging.INFO,
                 logging_format=ray_constants.LOGGER_FORMAT,
                 plasma_store_socket_name=None,
                 raylet_socket_name=None,
                 temp_dir=None,
                 include_log_monitor=None,
                 autoscaling_config=None,
                 include_java=False,
                 java_worker_options=None,
                 _internal_config=None):
        self.object_id_seed = object_id_seed
        self.redis_address = redis_address
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.object_store_memory = object_store_memory
        self.redis_max_memory = redis_max_memory
        self.redis_port = redis_port
        self.redis_shard_ports = redis_shard_ports
        self.object_manager_port = object_manager_port
        self.node_manager_port = node_manager_port
        self.node_ip_address = node_ip_address
        self.num_workers = num_workers
        self.local_mode = local_mode
        self.driver_mode = driver_mode
        self.redirect_worker_output = redirect_worker_output
        self.redirect_output = redirect_output
        self.num_redis_shards = num_redis_shards
        self.redis_max_clients = redis_max_clients
        self.redis_password = redis_password
        self.plasma_directory = plasma_directory
        self.worker_path = worker_path
        self.huge_pages = huge_pages
        self.include_webui = include_webui
        self.plasma_store_socket_name = plasma_store_socket_name
        self.raylet_socket_name = raylet_socket_name
        self.temp_dir = temp_dir
        self.include_log_monitor = include_log_monitor
        self.autoscaling_config = autoscaling_config
        self.include_java = include_java
        self.java_worker_options = java_worker_options
        self._internal_config = _internal_config
        self._check_usage()

    def update(self, **kwargs):
        """"""Update the settings according to the keyword arguments.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update: %s"" % arg)

        self._check_usage()

    def update_if_absent(self, **kwargs):
        """"""Update the settings when the target fields are None.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                if getattr(self, arg) is None:
                    setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update_if_absent: %s"" % arg)

        self._check_usage()

    def _check_usage(self):
        if self.resources is not None:
            assert ""CPU"" not in self.resources, (
                ""'CPU' should not be included in the resource dictionary. Use ""
                ""num_cpus instead."")
            assert ""GPU"" not in self.resources, (
                ""'GPU' should not be included in the resource dictionary. Use ""
                ""num_gpus instead."")

        if self.num_workers is not None:
            raise ValueError(
                ""The 'num_workers' argument is deprecated. Please use ""
                ""'num_cpus' instead."")

        if self.include_java is None and self.java_worker_options is not None:
            raise ValueError(""Should not specify `java-worker-options` ""
                             ""without providing `include-java`."")
/n/n/n/python/ray/profiling.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import time
import threading
import traceback

import ray

LOG_POINT = 0
LOG_SPAN_START = 1
LOG_SPAN_END = 2


class _NullLogSpan(object):
    """"""A log span context manager that does nothing""""""

    def __enter__(self):
        pass

    def __exit__(self, type, value, tb):
        pass


NULL_LOG_SPAN = _NullLogSpan()


def profile(event_type, extra_data=None, worker=None):
    """"""Profile a span of time so that it appears in the timeline visualization.

    Note that this only works in the raylet code path.

    This function can be used as follows (both on the driver or within a task).

    .. code-block:: python

        with ray.profile(""custom event"", extra_data={'key': 'value'}):
            # Do some computation here.

    Optionally, a dictionary can be passed as the ""extra_data"" argument, and
    it can have keys ""name"" and ""cname"" if you want to override the default
    timeline display text and box color. Other values will appear at the bottom
    of the chrome tracing GUI when you click on the box corresponding to this
    profile span.

    Args:
        event_type: A string describing the type of the event.
        extra_data: This must be a dictionary mapping strings to strings. This
            data will be added to the json objects that are used to populate
            the timeline, so if you want to set a particular color, you can
            simply set the ""cname"" attribute to an appropriate color.
            Similarly, if you set the ""name"" attribute, then that will set the
            text displayed on the box in the timeline.

    Returns:
        An object that can profile a span of time via a ""with"" statement.
    """"""
    if worker is None:
        worker = ray.worker.global_worker
    return RayLogSpanRaylet(worker.profiler, event_type, extra_data=extra_data)


class Profiler(object):
    """"""A class that holds the profiling states.

    Attributes:
        worker: the worker to profile.
        events: the buffer of events.
        lock: the lock to protect access of events.
    """"""

    def __init__(self, worker):
        self.worker = worker
        self.events = []
        self.lock = threading.Lock()

    def start_flush_thread(self):
        t = threading.Thread(
            target=self._periodically_flush_profile_events,
            name=""ray_push_profiling_information"")
        # Making the thread a daemon causes it to exit when the main thread
        # exits.
        t.daemon = True
        t.start()

    def _periodically_flush_profile_events(self):
        """"""Drivers run this as a thread to flush profile data in the
        background.""""""
        # Note(rkn): This is run on a background thread in the driver. It uses
        # the local scheduler client. This should be ok because it doesn't read
        # from the local scheduler client and we have the GIL here. However,
        # if either of those things changes, then we could run into issues.
        try:
            while True:
                time.sleep(1)
                self.flush_profile_data()
        except AttributeError:
            # TODO(suquark): It is a bad idea to ignore ""AttributeError"".
            # It has caused some very unexpected behaviors when implementing
            # new features (related to AttributeError).

            # This is to suppress errors that occur at shutdown.
            pass

    def flush_profile_data(self):
        """"""Push the logged profiling data to the global control store.

        By default, profiling information for a given task won't appear in the
        timeline until after the task has completed. For very long-running
        tasks, we may want profiling information to appear more quickly.
        In such cases, this function can be called. Note that as an
        alternative, we could start a thread in the background on workers that
        calls this automatically.
        """"""
        with self.lock:
            events = self.events
            self.events = []

        if self.worker.mode == ray.WORKER_MODE:
            component_type = ""worker""
        else:
            component_type = ""driver""

        self.worker.raylet_client.push_profile_events(
            component_type, ray.UniqueID(self.worker.worker_id),
            self.worker.node_ip_address, events)

    def add_event(self, event):
        with self.lock:
            self.events.append(event)


class RayLogSpanRaylet(object):
    """"""An object used to enable logging a span of events with a with statement.

    Attributes:
        event_type (str): The type of the event being logged.
        extra_data: Additional information to log.
    """"""

    def __init__(self, profiler, event_type, extra_data=None):
        """"""Initialize a RayLogSpanRaylet object.""""""
        self.profiler = profiler
        self.event_type = event_type
        self.extra_data = extra_data if extra_data is not None else {}

    def set_attribute(self, key, value):
        """"""Add a key-value pair to the extra_data dict.

        This can be used to add attributes that are not available when
        ray.profile was called.

        Args:
            key: The attribute name.
            value: The attribute value.
        """"""
        if not isinstance(key, str) or not isinstance(value, str):
            raise ValueError(""The arguments 'key' and 'value' must both be ""
                             ""strings. Instead they are {} and {}."".format(
                                 key, value))
        self.extra_data[key] = value

    def __enter__(self):
        """"""Log the beginning of a span event.

        Returns:
            The object itself is returned so that if the block is opened using
                ""with ray.profile(...) as prof:"", we can call
                ""prof.set_attribute"" inside the block.
        """"""
        self.start_time = time.time()
        return self

    def __exit__(self, type, value, tb):
        """"""Log the end of a span event. Log any exception that occurred.""""""
        for key, value in self.extra_data.items():
            if not isinstance(key, str) or not isinstance(value, str):
                raise ValueError(""The extra_data argument must be a ""
                                 ""dictionary mapping strings to strings. ""
                                 ""Instead it is {}."".format(self.extra_data))

        if type is not None:
            extra_data = json.dumps({
                ""type"": str(type),
                ""value"": str(value),
                ""traceback"": str(traceback.format_exc()),
            })
        else:
            extra_data = json.dumps(self.extra_data)

        event = {
            ""event_type"": self.event_type,
            ""start_time"": self.start_time,
            ""end_time"": time.time(),
            ""extra_data"": extra_data,
        }

        self.profiler.add_event(event)
/n/n/n/python/ray/utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import functools
import hashlib
import inspect
import logging
import numpy as np
import os
import subprocess
import sys
import threading
import time
import uuid

import ray.gcs_utils
import ray.ray_constants as ray_constants


def _random_string():
    id_hash = hashlib.sha1()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest()
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes


def format_error_message(exception_message, task_exception=False):
    """"""Improve the formatting of an exception thrown by a remote function.

    This method takes a traceback from an exception and makes it nicer by
    removing a few uninformative lines and adding some space to indent the
    remaining lines nicely.

    Args:
        exception_message (str): A message generated by traceback.format_exc().

    Returns:
        A string of the formatted exception message.
    """"""
    lines = exception_message.split(""\n"")
    if task_exception:
        # For errors that occur inside of tasks, remove lines 1 and 2 which are
        # always the same, they just contain information about the worker code.
        lines = lines[0:1] + lines[3:]
        pass
    return ""\n"".join(lines)


def push_error_to_driver(worker, error_type, message, driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Args:
        worker: The worker to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    worker.raylet_client.push_error(driver_id, error_type, message,
                                    time.time())


def push_error_to_driver_through_redis(redis_client,
                                       error_type,
                                       message,
                                       driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Normally the push_error_to_driver function should be used. However, in some
    instances, the local scheduler client is not available, e.g., because the
    error happens in Python before the driver or worker has connected to the
    backend processes.

    Args:
        redis_client: The redis client to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    # Do everything in Python and through the Python Redis client instead
    # of through the raylet.
    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,
                                                       message, time.time())
    redis_client.execute_command(""RAY.TABLE_APPEND"",
                                 ray.gcs_utils.TablePrefix.ERROR_INFO,
                                 ray.gcs_utils.TablePubsub.ERROR_INFO,
                                 driver_id.binary(), error_data)


def is_cython(obj):
    """"""Check if an object is a Cython function or method""""""

    # TODO(suo): We could split these into two functions, one for Cython
    # functions and another for Cython methods.
    # TODO(suo): There doesn't appear to be a Cython function 'type' we can
    # check against via isinstance. Please correct me if I'm wrong.
    def check_cython(x):
        return type(x).__name__ == ""cython_function_or_method""

    # Check if function or method, respectively
    return check_cython(obj) or \
        (hasattr(obj, ""__func__"") and check_cython(obj.__func__))


def is_function_or_method(obj):
    """"""Check if an object is a function or method.

    Args:
        obj: The Python object in question.

    Returns:
        True if the object is an function or method.
    """"""
    return inspect.isfunction(obj) or inspect.ismethod(obj) or is_cython(obj)


def is_class_method(f):
    """"""Returns whether the given method is a class_method.""""""
    return hasattr(f, ""__self__"") and f.__self__ is not None


def random_string():
    """"""Generate a random string to use as an ID.

    Note that users may seed numpy, which could cause this function to generate
    duplicate IDs. Therefore, we need to seed numpy ourselves, but we can't
    interfere with the state of the user's random number generator, so we
    extract the state of the random number generator and reset it after we are
    done.

    TODO(rkn): If we want to later guarantee that these are generated in a
    deterministic manner, then we will need to make some changes here.

    Returns:
        A random byte string of length ray_constants.ID_SIZE.
    """"""
    # Get the state of the numpy random number generator.
    numpy_state = np.random.get_state()
    # Try to use true randomness.
    np.random.seed(None)
    # Generate the random ID.
    random_id = np.random.bytes(ray_constants.ID_SIZE)
    # Reset the state of the numpy random number generator.
    np.random.set_state(numpy_state)
    return random_id


def decode(byte_str, allow_none=False):
    """"""Make this unicode in Python 3, otherwise leave it as bytes.

    Args:
        byte_str: The byte string to decode.
        allow_none: If true, then we will allow byte_str to be None in which
            case we will return an empty string. TODO(rkn): Remove this flag.
            This is only here to simplify upgrading to flatbuffers 1.10.0.

    Returns:
        A byte string in Python 2 and a unicode string in Python 3.
    """"""
    if byte_str is None and allow_none:
        return """"

    if not isinstance(byte_str, bytes):
        raise ValueError(
            ""The argument {} must be a bytes object."".format(byte_str))
    if sys.version_info >= (3, 0):
        return byte_str.decode(""ascii"")
    else:
        return byte_str


def binary_to_object_id(binary_object_id):
    return ray.ObjectID(binary_object_id)


def binary_to_hex(identifier):
    hex_identifier = binascii.hexlify(identifier)
    if sys.version_info >= (3, 0):
        hex_identifier = hex_identifier.decode()
    return hex_identifier


def hex_to_binary(hex_identifier):
    return binascii.unhexlify(hex_identifier)


def get_cuda_visible_devices():
    """"""Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable.

    Returns:
        if CUDA_VISIBLE_DEVICES is set, this returns a list of integers with
            the IDs of the GPUs. If it is not set, this returns None.
    """"""
    gpu_ids_str = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    if gpu_ids_str is None:
        return None

    if gpu_ids_str == """":
        return []

    return [int(i) for i in gpu_ids_str.split("","")]


def set_cuda_visible_devices(gpu_ids):
    """"""Set the CUDA_VISIBLE_DEVICES environment variable.

    Args:
        gpu_ids: This is a list of integers representing GPU IDs.
    """"""
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join([str(i) for i in gpu_ids])


def resources_from_resource_arguments(default_num_cpus, default_num_gpus,
                                      default_resources, runtime_num_cpus,
                                      runtime_num_gpus, runtime_resources):
    """"""Determine a task's resource requirements.

    Args:
        default_num_cpus: The default number of CPUs required by this function
            or actor method.
        default_num_gpus: The default number of GPUs required by this function
            or actor method.
        default_resources: The default custom resources required by this
            function or actor method.
        runtime_num_cpus: The number of CPUs requested when the task was
            invoked.
        runtime_num_gpus: The number of GPUs requested when the task was
            invoked.
        runtime_resources: The custom resources requested when the task was
            invoked.

    Returns:
        A dictionary of the resource requirements for the task.
    """"""
    if runtime_resources is not None:
        resources = runtime_resources.copy()
    elif default_resources is not None:
        resources = default_resources.copy()
    else:
        resources = {}

    if ""CPU"" in resources or ""GPU"" in resources:
        raise ValueError(""The resources dictionary must not ""
                         ""contain the key 'CPU' or 'GPU'"")

    assert default_num_cpus is not None
    resources[""CPU""] = (default_num_cpus
                        if runtime_num_cpus is None else runtime_num_cpus)

    if runtime_num_gpus is not None:
        resources[""GPU""] = runtime_num_gpus
    elif default_num_gpus is not None:
        resources[""GPU""] = default_num_gpus

    return resources


_default_handler = None


def setup_logger(logging_level, logging_format):
    """"""Setup default logging for ray.""""""
    logger = logging.getLogger(""ray"")
    if type(logging_level) is str:
        logging_level = logging.getLevelName(logging_level.upper())
    logger.setLevel(logging_level)
    global _default_handler
    _default_handler = logging.StreamHandler()
    _default_handler.setFormatter(logging.Formatter(logging_format))
    logger.addHandler(_default_handler)
    logger.propagate = False


def try_update_handler(new_stream):
    global _default_handler
    logger = logging.getLogger(""ray"")
    if _default_handler:
        new_handler = logging.StreamHandler(stream=new_stream)
        new_handler.setFormatter(_default_handler.formatter)
        _default_handler.close()
        _default_handler = new_handler
        logger.addHandler(_default_handler)


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5bd44f8afcecbfb0db479ce230c790fc2c56569a/psutil/tests/test_linux.py#L132-L138  # noqa: E501
def vmstat(stat):
    """"""Run vmstat and get a particular statistic.

    Args:
        stat: The statistic that we are interested in retrieving.

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output([""vmstat"", ""-s""])
    stat = stat.encode(""ascii"")
    for line in out.split(b""\n""):
        line = line.strip()
        if stat in line:
            return int(line.split(b"" "")[0])
    raise ValueError(""Can't find {} in 'vmstat' output."".format(stat))


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5e90b0a7f3fccb177445a186cc4fac62cfadb510/psutil/tests/test_osx.py#L29-L38  # noqa: E501
def sysctl(command):
    """"""Run a sysctl command and parse the output.

    Args:
        command: A sysctl command with an argument, for example,
            [""sysctl"", ""hw.memsize""].

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output(command)
    result = out.split(b"" "")[1]
    try:
        return int(result)
    except ValueError:
        return result


def get_system_memory():
    """"""Return the total amount of system memory in bytes.

    Returns:
        The total amount of system memory in bytes.
    """"""
    # Try to accurately figure out the memory limit if we are in a docker
    # container. Note that this file is not specific to Docker and its value is
    # often much larger than the actual amount of memory.
    docker_limit = None
    memory_limit_filename = ""/sys/fs/cgroup/memory/memory.limit_in_bytes""
    if os.path.exists(memory_limit_filename):
        with open(memory_limit_filename, ""r"") as f:
            docker_limit = int(f.read())

    # Use psutil if it is available.
    psutil_memory_in_bytes = None
    try:
        import psutil
        psutil_memory_in_bytes = psutil.virtual_memory().total
    except ImportError:
        pass

    if psutil_memory_in_bytes is not None:
        memory_in_bytes = psutil_memory_in_bytes
    elif sys.platform == ""linux"" or sys.platform == ""linux2"":
        # Handle Linux.
        bytes_in_kilobyte = 1024
        memory_in_bytes = vmstat(""total memory"") * bytes_in_kilobyte
    else:
        # Handle MacOS.
        memory_in_bytes = sysctl([""sysctl"", ""hw.memsize""])

    if docker_limit is not None:
        return min(docker_limit, memory_in_bytes)
    else:
        return memory_in_bytes


def get_shared_memory_bytes():
    """"""Get the size of the shared memory file system.

    Returns:
        The size of the shared memory file system in bytes.
    """"""
    # Make sure this is only called on Linux.
    assert sys.platform == ""linux"" or sys.platform == ""linux2""

    shm_fd = os.open(""/dev/shm"", os.O_RDONLY)
    try:
        shm_fs_stats = os.fstatvfs(shm_fd)
        # The value shm_fs_stats.f_bsize is the block size and the
        # value shm_fs_stats.f_bavail is the number of available
        # blocks.
        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail
    finally:
        os.close(shm_fd)

    return shm_avail


def check_oversized_pickle(pickled, name, obj_type, worker):
    """"""Send a warning message if the pickled object is too large.

    Args:
        pickled: the pickled object.
        name: name of the pickled object.
        obj_type: type of the pickled object, can be 'function',
            'remote function', 'actor', or 'object'.
        worker: the worker used to send warning message.
    """"""
    length = len(pickled)
    if length <= ray_constants.PICKLE_OBJECT_WARNING_SIZE:
        return
    warning_message = (
        ""Warning: The {} {} has size {} when pickled. ""
        ""It will be stored in Redis, which could cause memory issues. ""
        ""This may mean that its definition uses a large array or other object.""
    ).format(obj_type, name, length)
    push_error_to_driver(
        worker,
        ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,
        warning_message,
        driver_id=worker.task_driver_id)


class _ThreadSafeProxy(object):
    """"""This class is used to create a thread-safe proxy for a given object.
        Every method call will be guarded with a lock.

    Attributes:
        orig_obj (object): the original object.
        lock (threading.Lock): the lock object.
        _wrapper_cache (dict): a cache from original object's methods to
            the proxy methods.
    """"""

    def __init__(self, orig_obj, lock):
        self.orig_obj = orig_obj
        self.lock = lock
        self._wrapper_cache = {}

    def __getattr__(self, attr):
        orig_attr = getattr(self.orig_obj, attr)
        if not callable(orig_attr):
            # If the original attr is a field, just return it.
            return orig_attr
        else:
            # If the orginal attr is a method,
            # return a wrapper that guards the original method with a lock.
            wrapper = self._wrapper_cache.get(attr)
            if wrapper is None:

                @functools.wraps(orig_attr)
                def _wrapper(*args, **kwargs):
                    with self.lock:
                        return orig_attr(*args, **kwargs)

                self._wrapper_cache[attr] = _wrapper
                wrapper = _wrapper
            return wrapper


def thread_safe_client(client, lock=None):
    """"""Create a thread-safe proxy which locks every method call
    for the given client.

    Args:
        client: the client object to be guarded.
        lock: the lock object that will be used to lock client's methods.
            If None, a new lock will be used.

    Returns:
        A thread-safe proxy for the given client.
    """"""
    if lock is None:
        lock = threading.Lock()
    return _ThreadSafeProxy(client, lock)


def is_main_thread():
    return threading.current_thread().getName() == ""MainThread""
/n/n/n",1
48,48,b2ddba994ca5e78fa5dcbc0e00d6171a44b0b338,"nyaa/views/account.py/n/nimport binascii
import time
from datetime import datetime
from ipaddress import ip_address

import flask

from nyaa import email, forms, models
from nyaa.extensions import db
from nyaa.utils import sha1_hash
from nyaa.views.users import get_activation_link, get_password_reset_link, get_serializer

app = flask.current_app
bp = flask.Blueprint('account', __name__)


@bp.route('/login', methods=['GET', 'POST'])
def login():
    if flask.g.user:
        return flask.redirect(redirect_url())

    form = forms.LoginForm(flask.request.form)
    if flask.request.method == 'POST' and form.validate():
        if app.config['MAINTENANCE_MODE'] and not app.config['MAINTENANCE_MODE_LOGINS']:
            flask.flash(flask.Markup('<strong>Logins are currently disabled.</strong>'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        username = form.username.data.strip()
        password = form.password.data
        user = models.User.by_username(username)

        if not user:
            user = models.User.by_email(username)

        if not user or password != user.password_hash:
            flask.flash(flask.Markup(
                '<strong>Login failed!</strong> Incorrect username or password.'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        if user.is_banned:
            ban_reason = models.Ban.banned(user.id, None).first().reason
            ban_str = ('<strong>Login failed!</strong> You are banned with the '
                       'reason ""{0}"" If you believe that this is a mistake, contact '
                       'a moderator on IRC.'.format(ban_reason))
            flask.flash(flask.Markup(ban_str), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        if user.status != models.UserStatusType.ACTIVE:
            flask.flash(flask.Markup(
                '<strong>Login failed!</strong> Account is not activated.'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        user.last_login_date = datetime.utcnow()
        user.last_login_ip = ip_address(flask.request.remote_addr).packed
        if not app.config['MAINTENANCE_MODE']:
            db.session.add(user)
            db.session.commit()

        flask.g.user = user
        flask.session['user_id'] = user.id
        flask.session.permanent = True
        flask.session.modified = True

        return flask.redirect(redirect_url())

    return flask.render_template('login.html', form=form)


@bp.route('/logout')
def logout():
    flask.g.user = None
    flask.session.permanent = False
    flask.session.modified = False

    response = flask.make_response(flask.redirect(redirect_url()))
    response.set_cookie(app.session_cookie_name, expires=0)
    return response


@bp.route('/register', methods=['GET', 'POST'])
def register():
    if flask.g.user:
        return flask.redirect(redirect_url())

    form = forms.RegisterForm(flask.request.form)
    if flask.request.method == 'POST' and form.validate():
        user = models.User(username=form.username.data.strip(),
                           email=form.email.data.strip(), password=form.password.data)
        user.registration_ip = ip_address(flask.request.remote_addr).packed
        user.last_login_ip = user.registration_ip
        db.session.add(user)
        db.session.commit()
        if models.RangeBan.is_rangebanned(user.registration_ip):
            flask.flash(flask.Markup('Your IP is blocked from creating new accounts. '
                                     'Please <a href=""{}"">ask a moderator</a> to manually '
                                     'activate your account <a href=""{}"">\'{}\'</a>.'
                                     .format(flask.url_for('site.help') + '#irchelp',
                                             flask.url_for('users.view_user',
                                                           user_name=user.username),
                                             user.username)), 'warning')
        else:
            if app.config['USE_EMAIL_VERIFICATION']:  # force verification, enable email
                send_verification_email(user)
                return flask.render_template('waiting.html')
            else:  # disable verification, set user as active and auto log in
                user.status = models.UserStatusType.ACTIVE
                db.session.add(user)
                db.session.commit()
                flask.g.user = user
                flask.session['user_id'] = user.id
                flask.session.permanent = True
                flask.session.modified = True
                return flask.redirect(redirect_url())

    return flask.render_template('register.html', form=form)


@bp.route('/password-reset/<payload>', methods=['GET', 'POST'])
@bp.route('/password-reset', methods=['GET', 'POST'])
def password_reset(payload=None):
    if not app.config['ALLOW_PASSWORD_RESET']:
        return flask.abort(404)

    if flask.g.user:
        return flask.redirect(redirect_url())

    if payload is None:
        form = forms.PasswordResetRequestForm(flask.request.form)
        if flask.request.method == 'POST' and form.validate():
            user = models.User.by_email(form.email.data.strip())
            if user:
                send_password_reset_request_email(user)

            flask.flash(flask.Markup(
                'A password reset request was sent to the provided email, '
                'if a matching account was found.'), 'info')
            return flask.redirect(flask.url_for('main.home'))
        return flask.render_template('password_reset_request.html', form=form)

    else:
        s = get_serializer()
        try:
            request_timestamp, pw_hash, user_id = s.loads(payload)
        except:
            return flask.abort(404)

        user = models.User.by_id(user_id)
        if not user:
            return flask.abort(404)

        # Timeout after six hours
        if (time.time() - request_timestamp) > 6 * 3600:
            return flask.abort(404)

        sha1_password_hash_hash = binascii.hexlify(sha1_hash(user.password_hash.hash)).decode()
        if pw_hash != sha1_password_hash_hash:
            return flask.abort(404)

        form = forms.PasswordResetForm(flask.request.form)
        if flask.request.method == 'POST' and form.validate():
            user.password_hash = form.password.data

            db.session.add(user)
            db.session.commit()

            send_password_reset_email(user)

            flask.flash(flask.Markup('Your password was reset. Log in now.'), 'info')
            return flask.redirect(flask.url_for('account.login'))
        return flask.render_template('password_reset.html', form=form)


@bp.route('/profile', methods=['GET', 'POST'])
def profile():
    if not flask.g.user:
        # so we don't get stuck in infinite loop when signing out
        return flask.redirect(flask.url_for('main.home'))

    form = forms.ProfileForm(flask.request.form)

    if flask.request.method == 'POST' and form.validate():
        user = flask.g.user
        new_email = form.email.data.strip()
        new_password = form.new_password.data

        if new_email:
            # enforce password check on email change too
            if form.current_password.data != user.password_hash:
                flask.flash(flask.Markup(
                    '<strong>Email change failed!</strong> Incorrect password.'), 'danger')
                return flask.redirect('/profile')
            user.email = form.email.data
            flask.flash(flask.Markup(
                '<strong>Email successfully changed!</strong>'), 'success')
        if new_password:
            if form.current_password.data != user.password_hash:
                flask.flash(flask.Markup(
                    '<strong>Password change failed!</strong> Incorrect password.'), 'danger')
                return flask.redirect('/profile')
            user.password_hash = form.new_password.data
            flask.flash(flask.Markup(
                '<strong>Password successfully changed!</strong>'), 'success')

        db.session.add(user)
        db.session.commit()

        flask.g.user = user
        return flask.redirect('/profile')

    return flask.render_template('profile.html', form=form)


def redirect_url():
    next_url = flask.request.args.get('next', '')
    referrer = flask.request.referrer or ''

    target_url = (
        # Use ?next= param if it's a local (/foo/bar) path
        (next_url.startswith('/') and next_url) or
        # Use referrer if it's on our own host
        (referrer.startswith(flask.request.host_url) and referrer)
    )

    # Return the target, avoiding infinite loops
    if target_url and target_url != flask.request.url:
        return target_url

    # Default to index
    return flask.url_for('main.home')


def send_verification_email(user):
    activation_link = get_activation_link(user)

    tmpl_context = {
        'activation_link': activation_link,
        'user': user
    }

    email_msg = email.EmailHolder(
        subject='Verify your {} account'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/verify.txt', **tmpl_context),
        html=flask.render_template('email/verify.html', **tmpl_context),
    )

    email.send_email(email_msg)


def send_password_reset_email(user):
    ''' Alert user that their password has been successfully reset '''

    email_msg = email.EmailHolder(
        subject='Your {} password has been reset'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/reset.txt', user=user),
        html=flask.render_template('email/reset.html', user=user),
    )

    email.send_email(email_msg)


def send_password_reset_request_email(user):
    ''' Send user a password reset link '''
    reset_link = get_password_reset_link(user)

    tmpl_context = {
        'reset_link': reset_link,
        'user': user
    }

    email_msg = email.EmailHolder(
        subject='{} password reset request'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/reset-request.txt', **tmpl_context),
        html=flask.render_template('email/reset-request.html', **tmpl_context),
    )

    email.send_email(email_msg)
/n/n/n",0
49,49,b2ddba994ca5e78fa5dcbc0e00d6171a44b0b338,"/nyaa/views/account.py/n/nimport binascii
import time
from datetime import datetime
from ipaddress import ip_address

import flask

from nyaa import email, forms, models
from nyaa.extensions import db
from nyaa.utils import sha1_hash
from nyaa.views.users import get_activation_link, get_password_reset_link, get_serializer

app = flask.current_app
bp = flask.Blueprint('account', __name__)


@bp.route('/login', methods=['GET', 'POST'])
def login():
    if flask.g.user:
        return flask.redirect(redirect_url())

    form = forms.LoginForm(flask.request.form)
    if flask.request.method == 'POST' and form.validate():
        if app.config['MAINTENANCE_MODE'] and not app.config['MAINTENANCE_MODE_LOGINS']:
            flask.flash(flask.Markup('<strong>Logins are currently disabled.</strong>'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        username = form.username.data.strip()
        password = form.password.data
        user = models.User.by_username(username)

        if not user:
            user = models.User.by_email(username)

        if not user or password != user.password_hash:
            flask.flash(flask.Markup(
                '<strong>Login failed!</strong> Incorrect username or password.'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        if user.is_banned:
            ban_reason = models.Ban.banned(user.id, None).first().reason
            ban_str = ('<strong>Login failed!</strong> You are banned with the '
                       'reason ""{0}"" If you believe that this is a mistake, contact '
                       'a moderator on IRC.'.format(ban_reason))
            flask.flash(flask.Markup(ban_str), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        if user.status != models.UserStatusType.ACTIVE:
            flask.flash(flask.Markup(
                '<strong>Login failed!</strong> Account is not activated.'), 'danger')
            return flask.redirect(flask.url_for('account.login'))

        user.last_login_date = datetime.utcnow()
        user.last_login_ip = ip_address(flask.request.remote_addr).packed
        if not app.config['MAINTENANCE_MODE']:
            db.session.add(user)
            db.session.commit()

        flask.g.user = user
        flask.session['user_id'] = user.id
        flask.session.permanent = True
        flask.session.modified = True

        return flask.redirect(redirect_url())

    return flask.render_template('login.html', form=form)


@bp.route('/logout')
def logout():
    flask.g.user = None
    flask.session.permanent = False
    flask.session.modified = False

    response = flask.make_response(flask.redirect(redirect_url()))
    response.set_cookie(app.session_cookie_name, expires=0)
    return response


@bp.route('/register', methods=['GET', 'POST'])
def register():
    if flask.g.user:
        return flask.redirect(redirect_url())

    form = forms.RegisterForm(flask.request.form)
    if flask.request.method == 'POST' and form.validate():
        user = models.User(username=form.username.data.strip(),
                           email=form.email.data.strip(), password=form.password.data)
        user.registration_ip = ip_address(flask.request.remote_addr).packed
        user.last_login_ip = user.registration_ip
        db.session.add(user)
        db.session.commit()
        if models.RangeBan.is_rangebanned(user.registration_ip):
            flask.flash(flask.Markup('Your IP is blocked from creating new accounts. '
                                     'Please <a href=""{}"">ask a moderator</a> to manually '
                                     'activate your account <a href=""{}"">\'{}\'</a>.'
                                     .format(flask.url_for('site.help') + '#irchelp',
                                             flask.url_for('users.view_user',
                                                           user_name=user.username),
                                             user.username)), 'warning')
        else:
            if app.config['USE_EMAIL_VERIFICATION']:  # force verification, enable email
                send_verification_email(user)
                return flask.render_template('waiting.html')
            else:  # disable verification, set user as active and auto log in
                user.status = models.UserStatusType.ACTIVE
                db.session.add(user)
                db.session.commit()
                flask.g.user = user
                flask.session['user_id'] = user.id
                flask.session.permanent = True
                flask.session.modified = True
                return flask.redirect(redirect_url())

    return flask.render_template('register.html', form=form)


@bp.route('/password-reset/<payload>', methods=['GET', 'POST'])
@bp.route('/password-reset', methods=['GET', 'POST'])
def password_reset(payload=None):
    if not app.config['ALLOW_PASSWORD_RESET']:
        return flask.abort(404)

    if flask.g.user:
        return flask.redirect(redirect_url())

    if payload is None:
        form = forms.PasswordResetRequestForm(flask.request.form)
        if flask.request.method == 'POST' and form.validate():
            user = models.User.by_email(form.email.data.strip())
            if user:
                send_password_reset_request_email(user)

            flask.flash(flask.Markup(
                'A password reset request was sent to the provided email, '
                'if a matching account was found.'), 'info')
            return flask.redirect(flask.url_for('main.home'))
        return flask.render_template('password_reset_request.html', form=form)

    else:
        s = get_serializer()
        try:
            request_timestamp, pw_hash, user_id = s.loads(payload)
        except:
            return flask.abort(404)

        user = models.User.by_id(user_id)
        if not user:
            return flask.abort(404)

        # Timeout after six hours
        if (time.time() - request_timestamp) > 6 * 3600:
            return flask.abort(404)

        sha1_password_hash_hash = binascii.hexlify(sha1_hash(user.password_hash.hash)).decode()
        if pw_hash != sha1_password_hash_hash:
            return flask.abort(404)

        form = forms.PasswordResetForm(flask.request.form)
        if flask.request.method == 'POST' and form.validate():
            user.password_hash = form.password.data

            db.session.add(user)
            db.session.commit()

            send_password_reset_email(user)

            flask.flash(flask.Markup('Your password was reset. Log in now.'), 'info')
            return flask.redirect(flask.url_for('account.login'))
        return flask.render_template('password_reset.html', form=form)


@bp.route('/profile', methods=['GET', 'POST'])
def profile():
    if not flask.g.user:
        # so we don't get stuck in infinite loop when signing out
        return flask.redirect(flask.url_for('main.home'))

    form = forms.ProfileForm(flask.request.form)

    if flask.request.method == 'POST' and form.validate():
        user = flask.g.user
        new_email = form.email.data.strip()
        new_password = form.new_password.data

        if new_email:
            # enforce password check on email change too
            if form.current_password.data != user.password_hash:
                flask.flash(flask.Markup(
                    '<strong>Email change failed!</strong> Incorrect password.'), 'danger')
                return flask.redirect('/profile')
            user.email = form.email.data
            flask.flash(flask.Markup(
                '<strong>Email successfully changed!</strong>'), 'success')
        if new_password:
            if form.current_password.data != user.password_hash:
                flask.flash(flask.Markup(
                    '<strong>Password change failed!</strong> Incorrect password.'), 'danger')
                return flask.redirect('/profile')
            user.password_hash = form.new_password.data
            flask.flash(flask.Markup(
                '<strong>Password successfully changed!</strong>'), 'success')

        db.session.add(user)
        db.session.commit()

        flask.g.user = user
        return flask.redirect('/profile')

    return flask.render_template('profile.html', form=form)


def redirect_url():
    home_url = flask.url_for('main.home')

    url = flask.request.args.get('next') or \
        flask.request.referrer or \
        home_url
    if url == flask.request.url:
        return home_url
    return url


def send_verification_email(user):
    activation_link = get_activation_link(user)

    tmpl_context = {
        'activation_link': activation_link,
        'user': user
    }

    email_msg = email.EmailHolder(
        subject='Verify your {} account'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/verify.txt', **tmpl_context),
        html=flask.render_template('email/verify.html', **tmpl_context),
    )

    email.send_email(email_msg)


def send_password_reset_email(user):
    ''' Alert user that their password has been successfully reset '''

    email_msg = email.EmailHolder(
        subject='Your {} password has been reset'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/reset.txt', user=user),
        html=flask.render_template('email/reset.html', user=user),
    )

    email.send_email(email_msg)


def send_password_reset_request_email(user):
    ''' Send user a password reset link '''
    reset_link = get_password_reset_link(user)

    tmpl_context = {
        'reset_link': reset_link,
        'user': user
    }

    email_msg = email.EmailHolder(
        subject='{} password reset request'.format(app.config['GLOBAL_SITE_NAME']),
        recipient=user,
        text=flask.render_template('email/reset-request.txt', **tmpl_context),
        html=flask.render_template('email/reset-request.html', **tmpl_context),
    )

    email.send_email(email_msg)
/n/n/n",1
36,36,84c6c5ac27627db8aa829ead02ec98e8afa94b1e,"common/djangoapps/student/helpers.py/n/n""""""Helpers for the student app. """"""
import logging
import mimetypes
import urllib
import urlparse
from datetime import datetime

from django.conf import settings
from django.core.urlresolvers import NoReverseMatch, reverse
from django.utils import http
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from pytz import UTC

import third_party_auth
from course_modes.models import CourseMode
from lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationDeadline
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import get_themes

# Enumeration of per-course verification statuses
# we display on the student dashboard.
VERIFY_STATUS_NEED_TO_VERIFY = ""verify_need_to_verify""
VERIFY_STATUS_SUBMITTED = ""verify_submitted""
VERIFY_STATUS_RESUBMITTED = ""re_verify_submitted""
VERIFY_STATUS_APPROVED = ""verify_approved""
VERIFY_STATUS_MISSED_DEADLINE = ""verify_missed_deadline""
VERIFY_STATUS_NEED_TO_REVERIFY = ""verify_need_to_reverify""

DISABLE_UNENROLL_CERT_STATES = [
    'generating',
    'ready',
]


log = logging.getLogger(__name__)


def check_verify_status_by_course(user, course_enrollments):
    """"""
    Determine the per-course verification statuses for a given user.

    The possible statuses are:
        * VERIFY_STATUS_NEED_TO_VERIFY: The student has not yet submitted photos for verification.
        * VERIFY_STATUS_SUBMITTED: The student has submitted photos for verification,
          but has have not yet been approved.
        * VERIFY_STATUS_RESUBMITTED: The student has re-submitted photos for re-verification while
          they still have an active but expiring ID verification
        * VERIFY_STATUS_APPROVED: The student has been successfully verified.
        * VERIFY_STATUS_MISSED_DEADLINE: The student did not submit photos within the course's deadline.
        * VERIFY_STATUS_NEED_TO_REVERIFY: The student has an active verification, but it is
            set to expire before the verification deadline for the course.

    It is is also possible that a course does NOT have a verification status if:
        * The user is not enrolled in a verified mode, meaning that the user didn't pay.
        * The course does not offer a verified mode.
        * The user submitted photos but an error occurred while verifying them.
        * The user submitted photos but the verification was denied.

    In the last two cases, we rely on messages in the sidebar rather than displaying
    messages for each course.

    Arguments:
        user (User): The currently logged-in user.
        course_enrollments (list[CourseEnrollment]): The courses the user is enrolled in.

    Returns:
        dict: Mapping of course keys verification status dictionaries.
            If no verification status is applicable to a course, it will not
            be included in the dictionary.
            The dictionaries have these keys:
                * status (str): One of the enumerated status codes.
                * days_until_deadline (int): Number of days until the verification deadline.
                * verification_good_until (str): Date string for the verification expiration date.

    """"""
    status_by_course = {}

    # Retrieve all verifications for the user, sorted in descending
    # order by submission datetime
    verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)

    # Check whether the user has an active or pending verification attempt
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending(
        user, queryset=verifications
    )

    # Retrieve expiration_datetime of most recent approved verification
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    expiration_datetime = SoftwareSecurePhotoVerification.get_expiration_datetime(user, verifications)
    verification_expiring_soon = SoftwareSecurePhotoVerification.is_verification_expiring_soon(expiration_datetime)

    # Retrieve verification deadlines for the enrolled courses
    enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments]
    course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)

    recent_verification_datetime = None

    for enrollment in course_enrollments:

        # If the user hasn't enrolled as verified, then the course
        # won't display state related to its verification status.
        if enrollment.mode in CourseMode.VERIFIED_MODES:

            # Retrieve the verification deadline associated with the course.
            # This could be None if the course doesn't have a deadline.
            deadline = course_deadlines.get(enrollment.course_id)

            relevant_verification = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, verifications)

            # Picking the max verification datetime on each iteration only with approved status
            if relevant_verification is not None and relevant_verification.status == ""approved"":
                recent_verification_datetime = max(
                    recent_verification_datetime if recent_verification_datetime is not None
                    else relevant_verification.expiration_datetime,
                    relevant_verification.expiration_datetime
                )

            # By default, don't show any status related to verification
            status = None

            # Check whether the user was approved or is awaiting approval
            if relevant_verification is not None:
                if relevant_verification.status == ""approved"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_APPROVED
                elif relevant_verification.status == ""submitted"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_RESUBMITTED
                    else:
                        status = VERIFY_STATUS_SUBMITTED

            # If the user didn't submit at all, then tell them they need to verify
            # If the deadline has already passed, then tell them they missed it.
            # If they submitted but something went wrong (error or denied),
            # then don't show any messaging next to the course, since we already
            # show messages related to this on the left sidebar.
            submitted = (
                relevant_verification is not None and
                relevant_verification.status not in [""created"", ""ready""]
            )
            if status is None and not submitted:
                if deadline is None or deadline > datetime.now(UTC):
                    if SoftwareSecurePhotoVerification.user_is_verified(user):
                        if verification_expiring_soon:
                            # The user has an active verification, but the verification
                            # is set to expire within ""EXPIRING_SOON_WINDOW"" days (default is 4 weeks).
                            # Tell the student to reverify.
                            status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_NEED_TO_VERIFY
                else:
                    # If a user currently has an active or pending verification,
                    # then they may have submitted an additional attempt after
                    # the verification deadline passed.  This can occur,
                    # for example, when the support team asks a student
                    # to reverify after the deadline so they can receive
                    # a verified certificate.
                    # In this case, we still want to show them as ""verified""
                    # on the dashboard.
                    if has_active_or_pending:
                        status = VERIFY_STATUS_APPROVED

                    # Otherwise, the student missed the deadline, so show
                    # them as ""honor"" (the kind of certificate they will receive).
                    else:
                        status = VERIFY_STATUS_MISSED_DEADLINE

            # Set the status for the course only if we're displaying some kind of message
            # Otherwise, leave the course out of the dictionary.
            if status is not None:
                days_until_deadline = None

                now = datetime.now(UTC)
                if deadline is not None and deadline > now:
                    days_until_deadline = (deadline - now).days

                status_by_course[enrollment.course_id] = {
                    'status': status,
                    'days_until_deadline': days_until_deadline
                }

    if recent_verification_datetime:
        for key, value in status_by_course.iteritems():  # pylint: disable=unused-variable
            status_by_course[key]['verification_good_until'] = recent_verification_datetime.strftime(""%m/%d/%Y"")

    return status_by_course


def auth_pipeline_urls(auth_entry, redirect_url=None):
    """"""Retrieve URLs for each enabled third-party auth provider.

    These URLs are used on the ""sign up"" and ""sign in"" buttons
    on the login/registration forms to allow users to begin
    authentication with a third-party provider.

    Optionally, we can redirect the user to an arbitrary
    url after auth completes successfully.  We use this
    to redirect the user to a page that required login,
    or to send users to the payment flow when enrolling
    in a course.

    Args:
        auth_entry (string): Either `pipeline.AUTH_ENTRY_LOGIN` or `pipeline.AUTH_ENTRY_REGISTER`

    Keyword Args:
        redirect_url (unicode): If provided, send users to this URL
            after they successfully authenticate.

    Returns:
        dict mapping provider IDs to URLs

    """"""
    if not third_party_auth.is_enabled():
        return {}

    return {
        provider.provider_id: third_party_auth.pipeline.get_login_url(
            provider.provider_id, auth_entry, redirect_url=redirect_url
        ) for provider in third_party_auth.provider.Registry.displayed_for_login()
    }


# Query string parameters that can be passed to the ""finish_auth"" view to manage
# things like auto-enrollment.
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in', 'purchase_workflow')


def get_next_url_for_login_page(request):
    """"""
    Determine the URL to redirect to following login/registration/third_party_auth

    The user is currently on a login or registration page.
    If 'course_id' is set, or other POST_AUTH_PARAMS, we will need to send the user to the
    /account/finish_auth/ view following login, which will take care of auto-enrollment in
    the specified course.

    Otherwise, we go to the ?next= query param or to the dashboard if nothing else is
    specified.

    If THIRD_PARTY_AUTH_HINT is set, then `tpa_hint=<hint>` is added as a query parameter.
    """"""
    redirect_to = get_redirect_to(request)
    if not redirect_to:
        try:
            redirect_to = reverse('dashboard')
        except NoReverseMatch:
            redirect_to = reverse('home')

    if any(param in request.GET for param in POST_AUTH_PARAMS):
        # Before we redirect to next/dashboard, we need to handle auto-enrollment:
        params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]
        params.append(('next', redirect_to))  # After auto-enrollment, user will be sent to payment page or to this URL
        redirect_to = '{}?{}'.format(reverse('finish_auth'), urllib.urlencode(params))
        # Note: if we are resuming a third party auth pipeline, then the next URL will already
        # be saved in the session as part of the pipeline state. That URL will take priority
        # over this one.

    # Append a tpa_hint query parameter, if one is configured
    tpa_hint = configuration_helpers.get_value(
        ""THIRD_PARTY_AUTH_HINT"",
        settings.FEATURES.get(""THIRD_PARTY_AUTH_HINT"", '')
    )
    if tpa_hint:
        # Don't add tpa_hint if we're already in the TPA pipeline (prevent infinite loop),
        # and don't overwrite any existing tpa_hint params (allow tpa_hint override).
        running_pipeline = third_party_auth.pipeline.get(request)
        (scheme, netloc, path, query, fragment) = list(urlparse.urlsplit(redirect_to))
        if not running_pipeline and 'tpa_hint' not in query:
            params = urlparse.parse_qs(query)
            params['tpa_hint'] = [tpa_hint]
            query = urllib.urlencode(params, doseq=True)
            redirect_to = urlparse.urlunsplit((scheme, netloc, path, query, fragment))

    return redirect_to


def get_redirect_to(request):
    """"""
    Determine the redirect url and return if safe
    :argument
        request: request object

    :returns: redirect url if safe else None
    """"""
    redirect_to = request.GET.get('next')
    header_accept = request.META.get('HTTP_ACCEPT', '')

    # If we get a redirect parameter, make sure it's safe i.e. not redirecting outside our domain.
    # Also make sure that it is not redirecting to a static asset and redirected page is web page
    # not a static file. As allowing assets to be pointed to by ""next"" allows 3rd party sites to
    # get information about a user on edx.org. In any such case drop the parameter.
    if redirect_to:
        mime_type, _ = mimetypes.guess_type(redirect_to, strict=False)
        if not http.is_safe_url(redirect_to):
            log.warning(
                u'Unsafe redirect parameter detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        elif 'text/html' not in header_accept:
            log.warning(
                u'Redirect to non html content %(content_type)r detected from %(user_agent)r'
                u' after login page: %(redirect_to)r',
                {
                    ""redirect_to"": redirect_to, ""content_type"": header_accept,
                    ""user_agent"": request.META.get('HTTP_USER_AGENT', '')
                }
            )
            redirect_to = None
        elif mime_type:
            log.warning(
                u'Redirect to url path with specified filed type %(mime_type)r not allowed: %(redirect_to)r',
                {""redirect_to"": redirect_to, ""mime_type"": mime_type}
            )
            redirect_to = None
        elif settings.STATIC_URL in redirect_to:
            log.warning(
                u'Redirect to static content detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        else:
            themes = get_themes()
            for theme in themes:
                if theme.theme_dir_name in redirect_to:
                    log.warning(
                        u'Redirect to theme content detected after login page: %(redirect_to)r',
                        {""redirect_to"": redirect_to}
                    )
                    redirect_to = None
                    break

    return redirect_to


def destroy_oauth_tokens(user):
    """"""
    Destroys ALL OAuth access and refresh tokens for the given user.
    """"""
    dop_access_token.objects.filter(user=user.id).delete()
    dop_refresh_token.objects.filter(user=user.id).delete()
    dot_access_token.objects.filter(user=user.id).delete()
    dot_refresh_token.objects.filter(user=user.id).delete()
/n/n/ncommon/djangoapps/student/tests/test_helpers.py/n/n"""""" Test Student helpers """"""

import logging

import ddt
from django.conf import settings
from django.contrib.sessions.middleware import SessionMiddleware
from django.core.urlresolvers import reverse
from django.test import TestCase
from django.test.client import RequestFactory
from django.test.utils import override_settings
from mock import patch
from testfixtures import LogCapture

from student.helpers import get_next_url_for_login_page
from openedx.core.djangoapps.site_configuration.tests.test_util import with_site_configuration_context

LOGGER_NAME = ""student.helpers""


@ddt.ddt
class TestLoginHelper(TestCase):
    """"""Test login helper methods.""""""
    static_url = settings.STATIC_URL

    def setUp(self):
        super(TestLoginHelper, self).setUp()
        self.request = RequestFactory()

    @staticmethod
    def _add_session(request):
        """"""Annotate the request object with a session""""""
        middleware = SessionMiddleware()
        middleware.process_request(request)
        request.session.save()

    @ddt.data(
        (""https://www.amazon.com"", ""text/html"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.amazon.com'""),
        (""favicon.ico"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'favicon.ico'""),
        (""https://www.test.com/test.jpg"", ""image/*"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.test.com/test.jpg'""),
        (static_url + ""dummy.png"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'"" + static_url +
         ""dummy.png"" + ""'""),
        (""test.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'test.png'""),
        (static_url + ""dummy.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'"" + static_url + ""dummy.png"" + ""'""),
    )
    @ddt.unpack
    def test_unsafe_next(self, unsafe_url, http_accept, user_agent, expected_log):
        """""" Test unsafe next parameter """"""
        with LogCapture(LOGGER_NAME, level=logging.WARNING) as logger:
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=unsafe_url))
            req.META[""HTTP_ACCEPT""] = http_accept  # pylint: disable=no-member
            req.META[""HTTP_USER_AGENT""] = user_agent  # pylint: disable=no-member
            get_next_url_for_login_page(req)
            logger.check(
                (LOGGER_NAME, ""WARNING"", expected_log)
            )

    def test_safe_next(self):
        """""" Test safe next parameter """"""
        req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=""/dashboard""))
        req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
        next_page = get_next_url_for_login_page(req)
        self.assertEqual(next_page, u'/dashboard')

    @patch('student.helpers.third_party_auth.pipeline.get')
    @ddt.data(
        # Test requests outside the TPA pipeline - tpa_hint should be added.
        (None, '/dashboard', '/dashboard', False),
        ('', '/dashboard', '/dashboard', False),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),
        ('saml-idp', '/dashboard', '/dashboard?tpa_hint=saml-idp', False),
        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),

        # Test requests inside the TPA pipeline - tpa_hint should not be added, preventing infinite loop.
        (None, '/dashboard', '/dashboard', True),
        ('', '/dashboard', '/dashboard', True),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
        ('saml-idp', '/dashboard', '/dashboard', True),
        # OK to leave tpa_hint overrides in place.
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
    )
    @ddt.unpack
    def test_third_party_auth_hint(self, tpa_hint, next_url, expected_url, running_pipeline, mock_running_pipeline):
        mock_running_pipeline.return_value = running_pipeline

        def validate_login():
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=next_url))
            req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
            self._add_session(req)
            next_page = get_next_url_for_login_page(req)
            self.assertEqual(next_page, expected_url)

        with override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()

        with with_site_configuration_context(configuration=dict(THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()
/n/n/nlms/djangoapps/student_account/test/test_views.py/n/n# -*- coding: utf-8 -*-
"""""" Tests for student account views. """"""

import logging
import re
from unittest import skipUnless
from urllib import urlencode

import ddt
import mock
from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.messages.middleware import MessageMiddleware
from django.core import mail
from django.core.files.uploadedfile import SimpleUploadedFile
from django.core.urlresolvers import reverse
from django.http import HttpRequest
from django.test import TestCase
from django.test.utils import override_settings
from edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory, RefreshTokenFactory
from edx_rest_api_client import exceptions
from nose.plugins.attrib import attr
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from testfixtures import LogCapture

from commerce.models import CommerceConfiguration
from commerce.tests import factories
from commerce.tests.mocks import mock_get_orders
from course_modes.models import CourseMode
from http.cookies import SimpleCookie
from openedx.core.djangoapps.oauth_dispatch.tests import factories as dot_factories
from openedx.core.djangoapps.programs.tests.mixins import ProgramsApiConfigMixin
from openedx.core.djangoapps.site_configuration.tests.mixins import SiteMixin
from openedx.core.djangoapps.theming.tests.test_util import with_comprehensive_theme_context
from openedx.core.djangoapps.user_api.accounts.api import activate_account, create_account
from openedx.core.djangolib.js_utils import dump_js_escaped_json
from openedx.core.djangolib.testing.utils import CacheIsolationTestCase
from student.tests.factories import UserFactory
from student_account.views import account_settings_context, get_user_orders
from third_party_auth.tests.testutil import ThirdPartyAuthTestMixin, simulate_running_pipeline
from util.testing import UrlResetMixin
from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase

LOGGER_NAME = 'audit'
User = get_user_model()  # pylint:disable=invalid-name


@ddt.ddt
class StudentAccountUpdateTest(CacheIsolationTestCase, UrlResetMixin):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = u""heisenberg""
    ALTERNATE_USERNAME = u""walt""
    OLD_PASSWORD = u""""
    NEW_PASSWORD = u""""
    OLD_EMAIL = u""walter@graymattertech.com""
    NEW_EMAIL = u""walt@savewalterwhite.com""

    INVALID_ATTEMPTS = 100
    INVALID_KEY = u""123abc""

    URLCONF_MODULES = ['student_accounts.urls']

    ENABLED_CACHES = ['default']

    def setUp(self):
        super(StudentAccountUpdateTest, self).setUp()

        # Create/activate a new account
        activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL)
        activate_account(activation_key)

        # Login
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertTrue(result)

    @skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in LMS')
    def test_password_change(self):
        # Request a password change while logged in, simulating
        # use of the password reset link from the account page
        response = self._change_password()
        self.assertEqual(response.status_code, 200)

        # Check that an email was sent
        self.assertEqual(len(mail.outbox), 1)

        # Retrieve the activation link from the email body
        email_body = mail.outbox[0].body
        result = re.search(r'(?P<url>https?://[^\s]+)', email_body)
        self.assertIsNot(result, None)
        activation_link = result.group('url')

        # Visit the activation link
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)

        # Submit a new password and follow the redirect to the success page
        response = self.client.post(
            activation_link,
            # These keys are from the form on the current password reset confirmation page.
            {'new_password1': self.NEW_PASSWORD, 'new_password2': self.NEW_PASSWORD},
            follow=True
        )
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""Your password has been reset."")

        # Log the user out to clear session data
        self.client.logout()

        # Verify that the new password can be used to log in
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

        # Try reusing the activation link to change the password again
        # Visit the activation link again.
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""This password reset link is invalid. It may have been used already."")

        self.client.logout()

        # Verify that the old password cannot be used to log in
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertFalse(result)

        # Verify that the new password continues to be valid
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

    @ddt.data(True, False)
    def test_password_change_logged_out(self, send_email):
        # Log the user out
        self.client.logout()

        # Request a password change while logged out, simulating
        # use of the password reset link from the login page
        if send_email:
            response = self._change_password(email=self.OLD_EMAIL)
            self.assertEqual(response.status_code, 200)
        else:
            # Don't send an email in the POST data, simulating
            # its (potentially accidental) omission in the POST
            # data sent from the login page
            response = self._change_password()
            self.assertEqual(response.status_code, 400)

    def test_access_token_invalidation_logged_out(self):
        self.client.logout()
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password(email=self.OLD_EMAIL)
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_access_token_invalidation_logged_in(self):
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password()
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_password_change_inactive_user(self):
        # Log out the user created during test setup
        self.client.logout()

        # Create a second user, but do not activate it
        create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)

        # Send the view the email address tied to the inactive user
        response = self._change_password(email=self.NEW_EMAIL)

        # Expect that the activation email is still sent,
        # since the user may have lost the original activation email.
        self.assertEqual(response.status_code, 200)
        self.assertEqual(len(mail.outbox), 1)

    def test_password_change_no_user(self):
        # Log out the user created during test setup
        self.client.logout()

        with LogCapture(LOGGER_NAME, level=logging.INFO) as logger:
            # Send the view an email address not tied to any user
            response = self._change_password(email=self.NEW_EMAIL)
            self.assertEqual(response.status_code, 200)
            logger.check((LOGGER_NAME, 'INFO', 'Invalid password reset attempt'))

    def test_password_change_rate_limited(self):
        # Log out the user created during test setup, to prevent the view from
        # selecting the logged-in user's email address over the email provided
        # in the POST data
        self.client.logout()

        # Make many consecutive bad requests in an attempt to trigger the rate limiter
        for __ in xrange(self.INVALID_ATTEMPTS):
            self._change_password(email=self.NEW_EMAIL)

        response = self._change_password(email=self.NEW_EMAIL)
        self.assertEqual(response.status_code, 403)

    @ddt.data(
        ('post', 'password_change_request', []),
    )
    @ddt.unpack
    def test_require_http_method(self, correct_method, url_name, args):
        wrong_methods = {'get', 'put', 'post', 'head', 'options', 'delete'} - {correct_method}
        url = reverse(url_name, args=args)

        for method in wrong_methods:
            response = getattr(self.client, method)(url)
            self.assertEqual(response.status_code, 405)

    def _change_password(self, email=None):
        """"""Request to change the user's password. """"""
        data = {}

        if email:
            data['email'] = email

        return self.client.post(path=reverse('password_change_request'), data=data)

    def _create_dop_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        client = ClientFactory()
        access_token = AccessTokenFactory(user=user, client=client)
        RefreshTokenFactory(user=user, client=client, access_token=access_token)

    def _create_dot_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        application = dot_factories.ApplicationFactory(user=user)
        access_token = dot_factories.AccessTokenFactory(user=user, application=application)
        dot_factories.RefreshTokenFactory(user=user, application=application, access_token=access_token)

    def assert_access_token_destroyed(self, user):
        """"""Assert all access tokens are destroyed.""""""
        self.assertFalse(dot_access_token.objects.filter(user=user).exists())
        self.assertFalse(dot_refresh_token.objects.filter(user=user).exists())
        self.assertFalse(dop_access_token.objects.filter(user=user).exists())
        self.assertFalse(dop_refresh_token.objects.filter(user=user).exists())


@attr(shard=3)
@ddt.ddt
class StudentAccountLoginAndRegistrationTest(ThirdPartyAuthTestMixin, UrlResetMixin, ModuleStoreTestCase):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = ""bob""
    EMAIL = ""bob@example.com""
    PASSWORD = ""password""

    URLCONF_MODULES = ['openedx.core.djangoapps.embargo']

    @mock.patch.dict(settings.FEATURES, {'EMBARGO': True})
    def setUp(self):
        super(StudentAccountLoginAndRegistrationTest, self).setUp()

        # Several third party auth providers are created for these tests:
        self.google_provider = self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)
        self.configure_dummy_provider(
            visible=True,
            enabled=True,
            icon_class='',
            icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width=""50"" height=""100""/></svg>'),
        )
        self.hidden_enabled_provider = self.configure_linkedin_provider(
            visible=False,
            enabled=True,
        )
        self.hidden_disabled_provider = self.configure_azure_ad_provider()

    @ddt.data(
        (""signin_user"", ""login""),
        (""register_user"", ""register""),
    )
    @ddt.unpack
    def test_login_and_registration_form(self, url_name, initial_mode):
        response = self.client.get(reverse(url_name))
        expected_data = '""initial_mode"": ""{mode}""'.format(mode=initial_mode)
        self.assertContains(response, expected_data)

    @ddt.data(""signin_user"", ""register_user"")
    def test_login_and_registration_form_already_authenticated(self, url_name):
        # Create/activate a new account and log in
        activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
        activate_account(activation_key)
        result = self.client.login(username=self.USERNAME, password=self.PASSWORD)
        self.assertTrue(result)

        # Verify that we're redirected to the dashboard
        response = self.client.get(reverse(url_name))
        self.assertRedirects(response, reverse(""dashboard""))

    @ddt.data(
        (None, ""signin_user""),
        (None, ""register_user""),
        (""edx.org"", ""signin_user""),
        (""edx.org"", ""register_user""),
    )
    @ddt.unpack
    def test_login_and_registration_form_signin_not_preserves_params(self, theme, url_name):
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
        ]

        # The response should not have a ""Sign In"" button with the URL
        # that preserves the querystring params
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params + [('next', '/dashboard')]))
        self.assertNotContains(response, expected_url)

        # Add additional parameters:
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination')
        ]

        # Verify that this parameter is also preserved
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params))
        self.assertNotContains(response, expected_url)

    @mock.patch.dict(settings.FEATURES, {""ENABLE_THIRD_PARTY_AUTH"": False})
    @ddt.data(""signin_user"", ""register_user"")
    def test_third_party_auth_disabled(self, url_name):
        response = self.client.get(reverse(url_name))
        self._assert_third_party_auth_data(response, None, None, [], None)

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        (""signin_user"", None, None, None),
        (""register_user"", None, None, None),
        (""signin_user"", ""google-oauth2"", ""Google"", None),
        (""register_user"", ""google-oauth2"", ""Google"", None),
        (""signin_user"", ""facebook"", ""Facebook"", None),
        (""register_user"", ""facebook"", ""Facebook"", None),
        (""signin_user"", ""dummy"", ""Dummy"", None),
        (""register_user"", ""dummy"", ""Dummy"", None),
        (
            ""signin_user"",
            ""google-oauth2"",
            ""Google"",
            {
                'name': 'FakeName',
                'logo': 'https://host.com/logo.jpg',
                'welcome_msg': 'No message'
            }
        )
    )
    @ddt.unpack
    def test_third_party_auth(
            self,
            url_name,
            current_backend,
            current_provider,
            expected_enterprise_customer_mock_attrs,
            enterprise_customer_mock
    ):
        params = [
            ('course_id', 'course-v1:Org+Course+Run'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination'),
        ]

        if expected_enterprise_customer_mock_attrs:
            expected_ec = mock.MagicMock(
                branding_configuration=mock.MagicMock(
                    logo=mock.MagicMock(
                        url=expected_enterprise_customer_mock_attrs['logo']
                    ),
                    welcome_message=expected_enterprise_customer_mock_attrs['welcome_msg']
                )
            )
            expected_ec.name = expected_enterprise_customer_mock_attrs['name']
        else:
            expected_ec = None

        enterprise_customer_mock.return_value = expected_ec

        # Simulate a running pipeline
        if current_backend is not None:
            pipeline_target = ""student_account.views.third_party_auth.pipeline""
            with simulate_running_pipeline(pipeline_target, current_backend):
                response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # Do NOT simulate a running pipeline
        else:
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # This relies on the THIRD_PARTY_AUTH configuration in the test settings
        expected_providers = [
            {
                ""id"": ""oa2-dummy"",
                ""name"": ""Dummy"",
                ""iconClass"": None,
                ""iconImage"": settings.MEDIA_URL + ""icon.svg"",
                ""loginUrl"": self._third_party_login_url(""dummy"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""dummy"", ""register"", params)
            },
            {
                ""id"": ""oa2-facebook"",
                ""name"": ""Facebook"",
                ""iconClass"": ""fa-facebook"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""facebook"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""facebook"", ""register"", params)
            },
            {
                ""id"": ""oa2-google-oauth2"",
                ""name"": ""Google"",
                ""iconClass"": ""fa-google-plus"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""google-oauth2"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""google-oauth2"", ""register"", params)
            },
        ]
        self._assert_third_party_auth_data(
            response,
            current_backend,
            current_provider,
            expected_providers,
            expected_ec
        )

    def test_hinted_login(self):
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        'signin_user',
        'register_user',
    )
    def test_settings_tpa_hinted_login(self, url_name):
        """"""
        Ensure that settings.FEATURES['THIRD_PARTY_AUTH_HINT'] can set third_party_auth_hint.
        """"""
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        # Even disabled providers in the query string will override THIRD_PARTY_AUTH_HINT
        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_settings_tpa_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled via settings.THIRD_PARTY_AUTH_HINT. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        ('signin_user', False, None, None, None),
        ('register_user', False, None, None, None),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('signin_user', True, 'Fake EC', None, None),
        ('register_user', True, 'Fake EC', None, None),
    )
    @ddt.unpack
    def test_enterprise_register(self, url_name, ec_present, ec_name, logo_url, welcome_message, mock_get_ec):
        """"""
        Verify that when an EnterpriseCustomer is received on the login and register views,
        the appropriate sidebar is rendered.
        """"""
        if ec_present:
            mock_ec = mock_get_ec.return_value
            mock_ec.name = ec_name
            if logo_url:
                mock_ec.branding_configuration.logo.url = logo_url
            else:
                mock_ec.branding_configuration.logo = None
            if welcome_message:
                mock_ec.branding_configuration.welcome_message = welcome_message
            else:
                del mock_ec.branding_configuration.welcome_message
        else:
            mock_get_ec.return_value = None

        response = self.client.get(reverse(url_name), HTTP_ACCEPT=""text/html"")

        enterprise_sidebar_div_id = u'enterprise-content-container'

        if not ec_present:
            self.assertNotContains(response, text=enterprise_sidebar_div_id)
        else:
            self.assertContains(response, text=enterprise_sidebar_div_id)
            if not welcome_message:
                welcome_message = settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
            expected_message = welcome_message.format(
                start_bold=u'<b>',
                end_bold=u'</b>',
                enterprise_name=ec_name,
                platform_name=settings.PLATFORM_NAME
            )
            self.assertContains(response, expected_message)
            if logo_url:
                self.assertContains(response, logo_url)

    def test_enterprise_cookie_delete(self):
        """"""
        Test that enterprise cookies are deleted in login/registration views.

        Cookies must be deleted in login/registration views so that *default* login/registration branding
        is displayed to subsequent requests from non-enterprise customers.
        """"""
        cookies = SimpleCookie()
        cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME] = 'test-enterprise-customer'
        response = self.client.get(reverse('signin_user'), HTTP_ACCEPT=""text/html"", cookies=cookies)

        self.assertIn(settings.ENTERPRISE_CUSTOMER_COOKIE_NAME, response.cookies)  # pylint:disable=no-member
        enterprise_cookie = response.cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME]  # pylint:disable=no-member

        self.assertEqual(enterprise_cookie['domain'], settings.BASE_COOKIE_DOMAIN)
        self.assertEqual(enterprise_cookie.value, '')

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_microsite_uses_old_login_page(self):
        # Retrieve the login page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""signin_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Log into your Test Site Account"")
        self.assertContains(resp, ""login-form"")

    def test_microsite_uses_old_register_page(self):
        # Retrieve the register page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""register_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Register for Test Site"")
        self.assertContains(resp, ""register-form"")

    def test_login_registration_xframe_protected(self):
        resp = self.client.get(
            reverse(""register_user""),
            {},
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'DENY')

        self.configure_lti_provider(name='Test', lti_hostname='localhost', lti_consumer_key='test_key', enabled=True)

        resp = self.client.get(
            reverse(""register_user""),
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'ALLOW')

    def _assert_third_party_auth_data(self, response, current_backend, current_provider, providers, expected_ec):
        """"""Verify that third party auth info is rendered correctly in a DOM data attribute. """"""
        finish_auth_url = None
        if current_backend:
            finish_auth_url = reverse(""social:complete"", kwargs={""backend"": current_backend}) + ""?""

        auth_info = {
            ""currentProvider"": current_provider,
            ""providers"": providers,
            ""secondaryProviders"": [],
            ""finishAuthUrl"": finish_auth_url,
            ""errorMessage"": None,
            ""registerFormSubmitButtonText"": ""Create Account"",
        }
        if expected_ec is not None:
            # If we set an EnterpriseCustomer, third-party auth providers ought to be hidden.
            auth_info['providers'] = []
        auth_info = dump_js_escaped_json(auth_info)

        expected_data = '""third_party_auth"": {auth_info}'.format(
            auth_info=auth_info
        )

        self.assertContains(response, expected_data)

    def _third_party_login_url(self, backend_name, auth_entry, login_params):
        """"""Construct the login URL to start third party authentication. """"""
        return u""{url}?auth_entry={auth_entry}&{param_str}"".format(
            url=reverse(""social:begin"", kwargs={""backend"": backend_name}),
            auth_entry=auth_entry,
            param_str=self._finish_auth_url_param(login_params),
        )

    def _finish_auth_url_param(self, params):
        """"""
        Make the next=... URL parameter that indicates where the user should go next.

        >>> _finish_auth_url_param([('next', '/dashboard')])
        '/account/finish_auth?next=%2Fdashboard'
        """"""
        return urlencode({
            'next': '/account/finish_auth?{}'.format(urlencode(params))
        })

    def test_english_by_default(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_unsupported_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""ts-zx"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_browser_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es"")

        self.assertEqual(response['Content-Language'], 'es-419')

    def test_browser_language_dialent(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es-es"")

        self.assertEqual(response['Content-Language'], 'es-es')


class AccountSettingsViewTest(ThirdPartyAuthTestMixin, TestCase, ProgramsApiConfigMixin):
    """""" Tests for the account settings view. """"""

    USERNAME = 'student'
    PASSWORD = 'password'
    FIELDS = [
        'country',
        'gender',
        'language',
        'level_of_education',
        'password',
        'year_of_birth',
        'preferred_language',
        'time_zone',
    ]

    @mock.patch(""django.conf.settings.MESSAGE_STORAGE"", 'django.contrib.messages.storage.cookie.CookieStorage')
    def setUp(self):
        super(AccountSettingsViewTest, self).setUp()
        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)
        CommerceConfiguration.objects.create(cache_ttl=10, enabled=True)
        self.client.login(username=self.USERNAME, password=self.PASSWORD)

        self.request = HttpRequest()
        self.request.user = self.user

        # For these tests, two third party auth providers are enabled by default:
        self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)

        # Python-social saves auth failure notifcations in Django messages.
        # See pipeline.get_duplicate_provider() for details.
        self.request.COOKIES = {}
        MessageMiddleware().process_request(self.request)
        messages.error(self.request, 'Facebook is already in use.', extra_tags='Auth facebook')

    def test_context(self):

        context = account_settings_context(self.request)

        user_accounts_api_url = reverse(""accounts_api"", kwargs={'username': self.user.username})
        self.assertEqual(context['user_accounts_api_url'], user_accounts_api_url)

        user_preferences_api_url = reverse('preferences_api', kwargs={'username': self.user.username})
        self.assertEqual(context['user_preferences_api_url'], user_preferences_api_url)

        for attribute in self.FIELDS:
            self.assertIn(attribute, context['fields'])

        self.assertEqual(
            context['user_accounts_api_url'], reverse(""accounts_api"", kwargs={'username': self.user.username})
        )
        self.assertEqual(
            context['user_preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username})
        )

        self.assertEqual(context['duplicate_provider'], 'facebook')
        self.assertEqual(context['auth']['providers'][0]['name'], 'Facebook')
        self.assertEqual(context['auth']['providers'][1]['name'], 'Google')

    def test_view(self):
        """"""
        Test that all fields are  visible
        """"""
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        for attribute in self.FIELDS:
            self.assertIn(attribute, response.content)

    def test_header_with_programs_listing_enabled(self):
        """"""
        Verify that tabs header will be shown while program listing is enabled.
        """"""
        self.create_programs_config()
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""tab-nav-item"">')

    def test_header_with_programs_listing_disabled(self):
        """"""
        Verify that nav header will be shown while program listing is disabled.
        """"""
        self.create_programs_config(enabled=False)
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""item nav-global-01"">')

    def test_commerce_order_detail(self):
        """"""
        Verify that get_user_orders returns the correct order data.
        """"""
        with mock_get_orders():
            order_detail = get_user_orders(self.user)

        for i, order in enumerate(mock_get_orders.default_response['results']):
            expected = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': 'Jan 01, 2016',
                'receipt_url': '/checkout/receipt/?order_number=' + order['number'],
                'lines': order['lines'],
            }
            self.assertEqual(order_detail[i], expected)

    def test_commerce_order_detail_exception(self):
        with mock_get_orders(exception=exceptions.HttpNotFoundError):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_incomplete_order_detail(self):
        response = {
            'results': [
                factories.OrderFactory(
                    status='Incomplete',
                    lines=[
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory()])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_order_history_with_no_product(self):
        response = {
            'results': [
                factories.OrderFactory(
                    lines=[
                        factories.OrderLineFactory(
                            product=None
                        ),
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory(
                                name='certificate_type',
                                value='verified'
                            )])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(len(order_detail), 1)


@override_settings(SITE_NAME=settings.MICROSITE_LOGISTRATION_HOSTNAME)
class MicrositeLogistrationTests(TestCase):
    """"""
    Test to validate that microsites can display the logistration page
    """"""

    def test_login_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    def test_registration_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_no_override(self):
        """"""
        Make sure we get the old style login/registration if we don't override
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)


class AccountCreationTestCaseWithSiteOverrides(SiteMixin, TestCase):
    """"""
    Test cases for Feature flag ALLOW_PUBLIC_ACCOUNT_CREATION which when
    turned off disables the account creation options in lms
    """"""

    def setUp(self):
        """"""Set up the tests""""""
        super(AccountCreationTestCaseWithSiteOverrides, self).setUp()

        # Set the feature flag ALLOW_PUBLIC_ACCOUNT_CREATION to False
        self.site_configuration_values = {
            'ALLOW_PUBLIC_ACCOUNT_CREATION': False
        }
        self.site_domain = 'testserver1.com'
        self.set_up_site(self.site_domain, self.site_configuration_values)

    def test_register_option_login_page(self):
        """"""
        Navigate to the login page and check the Register option is hidden when
        ALLOW_PUBLIC_ACCOUNT_CREATION flag is turned off
        """"""
        response = self.client.get(reverse('signin_user'))
        self.assertNotIn('<a class=""btn-neutral"" href=""/register?next=%2Fdashboard"">Register</a>',
                         response.content)
/n/n/nlms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.api import (
    RegistrationFormFactory,
    get_login_session_form,
    get_password_reset_form
)
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    if initial_mode == ""register"":
                        auth_entry = pipeline.AUTH_ENTRY_REGISTER
                    else:
                        auth_entry = pipeline.AUTH_ENTRY_LOGIN
                    return redirect(
                        pipeline.get_login_url(provider_id, auth_entry, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError) as ex:
            log.error(""Unknown tpa_hint provider: %s"", ex)

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    response = render_to_response('student_account/login_and_register.html', context)

    # Remove enterprise cookie so that subsequent requests show default login page.
    response.delete_cookie(
        configuration_helpers.get_value(""ENTERPRISE_CUSTOMER_COOKIE_NAME"", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),
        domain=configuration_helpers.get_value(""BASE_COOKIE_DOMAIN"", settings.BASE_COOKIE_DOMAIN),
    )

    return response


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
        ""registerFormSubmitButtonText"": _(""Create Account""),
    }

    if third_party_auth.is_enabled():
        enterprise_customer = enterprise_customer_for_request(request)
        if not enterprise_customer:
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # For enterprise (and later for everyone), we need to get explicit consent to the
                    # Terms of service instead of auto submitting the registration form outright.
                    if not enterprise_customer:
                        # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                        context[""autoSubmitRegForm""] = True
                    else:
                        context[""autoRegisterWelcomeMessage""] = (
                            'Thank you for joining {}. '
                            'Just a couple steps before you start learning!'
                        ).format(
                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)
                        )
                        context[""registerFormSubmitButtonText""] = _(""Continue"")

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""

    return {
        'password_reset': get_password_reset_form().to_json(),
        'login': get_login_session_form().to_json(),
        'registration': RegistrationFormFactory().get_registration_form(request).to_json()
    }


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",0
37,37,84c6c5ac27627db8aa829ead02ec98e8afa94b1e,"/lms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.api import (
    RegistrationFormFactory,
    get_login_session_form,
    get_password_reset_form
)
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    return redirect(
                        pipeline.get_login_url(provider_id, pipeline.AUTH_ENTRY_LOGIN, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError):
            pass

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    response = render_to_response('student_account/login_and_register.html', context)

    # Remove enterprise cookie so that subsequent requests show default login page.
    response.delete_cookie(
        configuration_helpers.get_value(""ENTERPRISE_CUSTOMER_COOKIE_NAME"", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),
        domain=configuration_helpers.get_value(""BASE_COOKIE_DOMAIN"", settings.BASE_COOKIE_DOMAIN),
    )

    return response


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
        ""registerFormSubmitButtonText"": _(""Create Account""),
    }

    if third_party_auth.is_enabled():
        enterprise_customer = enterprise_customer_for_request(request)
        if not enterprise_customer:
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # For enterprise (and later for everyone), we need to get explicit consent to the
                    # Terms of service instead of auto submitting the registration form outright.
                    if not enterprise_customer:
                        # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                        context[""autoSubmitRegForm""] = True
                    else:
                        context[""autoRegisterWelcomeMessage""] = (
                            'Thank you for joining {}. '
                            'Just a couple steps before you start learning!'
                        ).format(
                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)
                        )
                        context[""registerFormSubmitButtonText""] = _(""Continue"")

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""

    return {
        'password_reset': get_password_reset_form().to_json(),
        'login': get_login_session_form().to_json(),
        'registration': RegistrationFormFactory().get_registration_form(request).to_json()
    }


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",1
10,10,22b6ecb953bbf40f0394a8bfd41d71a3f16e3465,"mozilla_django_oidc/views.py/n/nimport time
try:
    from urllib.parse import urlencode
except ImportError:
    # Python < 3
    from urllib import urlencode

import django
from django.core.exceptions import SuspiciousOperation
from django.core.urlresolvers import reverse
from django.contrib import auth
from django.http import HttpResponseRedirect
from django.utils.crypto import get_random_string
from django.utils.http import is_safe_url
from django.utils.module_loading import import_string
from django.views.generic import View

from mozilla_django_oidc.utils import (
    absolutify,
    import_from_settings,
    is_authenticated,
)


class OIDCAuthenticationCallbackView(View):
    """"""OIDC client authentication callback HTTP endpoint""""""

    http_method_names = ['get']

    @property
    def failure_url(self):
        return import_from_settings('LOGIN_REDIRECT_URL_FAILURE', '/')

    @property
    def success_url(self):
        # Pull the next url from the session or settings--we don't need to
        # sanitize here because it should already have been sanitized.
        next_url = self.request.session.get('oidc_login_next', None)
        return next_url or import_from_settings('LOGIN_REDIRECT_URL', '/')

    def login_failure(self):
        return HttpResponseRedirect(self.failure_url)

    def login_success(self):
        auth.login(self.request, self.user)

        # Figure out when this id_token will expire. This is ignored unless you're
        # using the RenewIDToken middleware.
        expiration_interval = import_from_settings('OIDC_RENEW_ID_TOKEN_EXPIRY_SECONDS', 60 * 15)
        self.request.session['oidc_id_token_expiration'] = time.time() + expiration_interval

        return HttpResponseRedirect(self.success_url)

    def get(self, request):
        """"""Callback handler for OIDC authorization code flow""""""

        nonce = request.session.get('oidc_nonce')
        if nonce:
            # Make sure that nonce is not used twice
            del request.session['oidc_nonce']

        if 'code' in request.GET and 'state' in request.GET:
            kwargs = {
                'request': request,
                'nonce': nonce,
            }

            if 'oidc_state' not in request.session:
                return self.login_failure()

            if request.GET['state'] != request.session['oidc_state']:
                msg = 'Session `oidc_state` does not match the OIDC callback state'
                raise SuspiciousOperation(msg)

            self.user = auth.authenticate(**kwargs)

            if self.user and self.user.is_active:
                return self.login_success()
        return self.login_failure()


def get_next_url(request, redirect_field_name):
    """"""Retrieves next url from request

    Note: This verifies that the url is safe before returning it. If the url
    is not safe, this returns None.

    :arg HttpRequest request: the http request
    :arg str redirect_field_name: the name of the field holding the next url

    :returns: safe url or None

    """"""
    next_url = request.GET.get(redirect_field_name)
    if next_url:
        kwargs = {
            'url': next_url,
            'host': request.get_host()
        }
        # NOTE(willkg): Django 1.11+ allows us to require https, too.
        if django.VERSION >= (1, 11):
            kwargs['require_https'] = request.is_secure()
        is_safe = is_safe_url(**kwargs)
        if is_safe:
            return next_url
    return None


class OIDCAuthenticationRequestView(View):
    """"""OIDC client authentication HTTP endpoint""""""

    http_method_names = ['get']

    def __init__(self, *args, **kwargs):
        super(OIDCAuthenticationRequestView, self).__init__(*args, **kwargs)

        self.OIDC_OP_AUTH_ENDPOINT = import_from_settings('OIDC_OP_AUTHORIZATION_ENDPOINT')
        self.OIDC_RP_CLIENT_ID = import_from_settings('OIDC_RP_CLIENT_ID')

    def get(self, request):
        """"""OIDC client authentication initialization HTTP endpoint""""""
        state = get_random_string(import_from_settings('OIDC_STATE_SIZE', 32))
        redirect_field_name = import_from_settings('OIDC_REDIRECT_FIELD_NAME', 'next')

        params = {
            'response_type': 'code',
            'scope': 'openid',
            'client_id': self.OIDC_RP_CLIENT_ID,
            'redirect_uri': absolutify(
                request,
                reverse('oidc_authentication_callback')
            ),
            'state': state,
        }

        if import_from_settings('OIDC_USE_NONCE', True):
            nonce = get_random_string(import_from_settings('OIDC_NONCE_SIZE', 32))
            params.update({
                'nonce': nonce
            })
            request.session['oidc_nonce'] = nonce

        request.session['oidc_state'] = state
        request.session['oidc_login_next'] = get_next_url(request, redirect_field_name)

        query = urlencode(params)
        redirect_url = '{url}?{query}'.format(url=self.OIDC_OP_AUTH_ENDPOINT, query=query)
        return HttpResponseRedirect(redirect_url)


class OIDCLogoutView(View):
    """"""Logout helper view""""""

    http_method_names = ['get', 'post']

    @property
    def redirect_url(self):
        """"""Return the logout url defined in settings.""""""
        return import_from_settings('LOGOUT_REDIRECT_URL', '/')

    def post(self, request):
        """"""Log out the user.""""""
        logout_url = self.redirect_url

        if is_authenticated(request.user):
            # Check if a method exists to build the URL to log out the user
            # from the OP.
            logout_from_op = import_from_settings('OIDC_OP_LOGOUT_URL_METHOD', '')
            if logout_from_op:
                logout_url = import_string(logout_from_op)()

            # Log out the Django user, only if she was actually logged in.
            auth.logout(request)

        return HttpResponseRedirect(logout_url)
/n/n/ntests/test_views.py/n/ntry:
    from urllib.parse import parse_qs, urlparse
except ImportError:
    # Python < 3
    from urlparse import parse_qs, urlparse

from mock import patch

import django
from django.core.exceptions import SuspiciousOperation
from django.contrib.auth import get_user_model
from django.contrib.auth.models import AnonymousUser
from django.core.urlresolvers import reverse
from django.test import RequestFactory, TestCase, override_settings

from mozilla_django_oidc import views


User = get_user_model()


def my_custom_op_logout(*args, **kwargs):
    return 'http://example.com/logged/out'


class OIDCAuthorizationCallbackViewTestCase(TestCase):
    def setUp(self):
        self.factory = RequestFactory()

    @override_settings(LOGIN_REDIRECT_URL='/success')
    def test_get_auth_success(self):
        """"""Test successful callback request to RP.""""""
        user = User.objects.create_user('example_username')

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }
        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'example_state'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with patch('mozilla_django_oidc.views.auth.authenticate') as mock_auth:
            with patch('mozilla_django_oidc.views.auth.login') as mock_login:
                mock_auth.return_value = user
                response = callback_view(request)

                mock_auth.assert_called_once_with(nonce=None,
                                                  request=request)
                mock_login.assert_called_once_with(request, user)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/success')

    @override_settings(LOGIN_REDIRECT_URL='/success')
    def test_get_auth_success_next_url(self):
        """"""Test successful callback request to RP with custom `next` url.""""""
        user = User.objects.create_user('example_username')

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }
        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'example_state',
            'oidc_login_next': '/foobar'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with patch('mozilla_django_oidc.views.auth.authenticate') as mock_auth:
            with patch('mozilla_django_oidc.views.auth.login') as mock_login:
                mock_auth.return_value = user
                response = callback_view(request)

                mock_auth.assert_called_once_with(nonce=None,
                                                  request=request)
                mock_login.assert_called_once_with(request, user)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/foobar')

    @override_settings(LOGIN_REDIRECT_URL_FAILURE='/failure')
    def test_get_auth_failure_nonexisting_user(self):
        """"""Test unsuccessful authentication and redirect url.""""""
        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }

        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'example_state'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with patch('mozilla_django_oidc.views.auth.authenticate') as mock_auth:
            mock_auth.return_value = None
            response = callback_view(request)

            mock_auth.assert_called_once_with(nonce=None,
                                              request=request)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/failure')

    @override_settings(LOGIN_REDIRECT_URL_FAILURE='/failure')
    def test_get_auth_failure_inactive_user(self):
        """"""Test authentication failure attempt for an inactive user.""""""
        user = User.objects.create_user('example_username')
        user.is_active = False
        user.save()

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }

        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'example_state'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with patch('mozilla_django_oidc.views.auth.authenticate') as mock_auth:
            mock_auth.return_value = user
            response = callback_view(request)

            mock_auth.assert_called_once_with(request=request,
                                              nonce=None)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/failure')

    @override_settings(OIDC_USE_NONCE=False)
    @override_settings(LOGIN_REDIRECT_URL_FAILURE='/failure')
    def test_get_auth_dirty_data(self):
        """"""Test authentication attempt with wrong get data.""""""
        get_data = {
            'foo': 'bar',
        }

        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {}
        callback_view = views.OIDCAuthenticationCallbackView.as_view()
        response = callback_view(request)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/failure')

    @override_settings(LOGIN_REDIRECT_URL_FAILURE='/failure')
    def test_get_auth_failure_missing_session_state(self):
        """"""Test authentication failure attempt for an inactive user.""""""
        user = User.objects.create_user('example_username')
        user.is_active = False
        user.save()

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }

        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        response = callback_view(request)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/failure')

    @override_settings(LOGIN_REDIRECT_URL_FAILURE='/failure')
    def test_get_auth_failure_tampered_session_state(self):
        """"""Test authentication failure attempt for an inactive user.""""""
        user = User.objects.create_user('example_username')
        user.is_active = False
        user.save()

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }

        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'tampered_state'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with self.assertRaises(SuspiciousOperation) as context:
            callback_view(request)

        expected_error_message = 'Session `oidc_state` does not match the OIDC callback state'
        self.assertEqual(context.exception.args, (expected_error_message,))

    @override_settings(LOGIN_REDIRECT_URL='/success')
    def test_nonce_is_deleted(self):
        """"""Test Nonce is not in session.""""""
        user = User.objects.create_user('example_username')

        get_data = {
            'code': 'example_code',
            'state': 'example_state'
        }
        url = reverse('oidc_authentication_callback')
        request = self.factory.get(url, get_data)
        request.session = {
            'oidc_state': 'example_state',
            'oidc_nonce': 'example_nonce'
        }
        callback_view = views.OIDCAuthenticationCallbackView.as_view()

        with patch('mozilla_django_oidc.views.auth.authenticate') as mock_auth:
            with patch('mozilla_django_oidc.views.auth.login') as mock_login:
                mock_auth.return_value = user
                response = callback_view(request)

                mock_auth.assert_called_once_with(nonce='example_nonce',
                                                  request=request)
                mock_login.assert_called_once_with(request, user)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/success')
        self.assertTrue('oidc_nonce' not in request.session)


class GetNextURLTestCase(TestCase):
    def setUp(self):
        self.factory = RequestFactory()

    def build_request(self, next_url):
        return self.factory.get('/', data={'next': next_url})

    def test_no_param(self):
        req = self.factory.get('/')
        next_url = views.get_next_url(req, 'next')
        self.assertEqual(next_url, None)

    def test_non_next_param(self):
        req = self.factory.get('/', data={'redirectto': '/foo'})
        next_url = views.get_next_url(req, 'redirectto')
        self.assertEqual(next_url, '/foo')

    def test_good_urls(self):
        urls = [
            '/',
            '/foo',
            '/foo?bar=baz',
            'http://testserver/foo',
        ]
        for url in urls:
            req = self.build_request(next_url=url)
            next_url = views.get_next_url(req, 'next')

            self.assertEqual(next_url, url)

    def test_bad_urls(self):
        urls = [
            '',
            # NOTE(willkg): Test data taken from the Django is_safe_url tests.
            'http://example.com',
            'http:///example.com',
            'https://example.com',
            'ftp://example.com',
            r'\\example.com',
            r'\\\example.com',
            r'/\\/example.com',
            r'\\\example.com',
            r'\\example.com',
            r'\\//example.com',
            r'/\/example.com',
            r'\/example.com',
            r'/\example.com',
            'http:///example.com',
            r'http:/\//example.com',
            r'http:\/example.com',
            r'http:/\example.com',
            'javascript:alert(""XSS"")',
            '\njavascript:alert(x)',
            '\x08//example.com',
            r'http://otherserver\@example.com',
            r'http:\\testserver\@example.com',
            r'http://testserver\me:pass@example.com',
            r'http://testserver\@example.com',
            r'http:\\testserver\confirm\me@example.com',
            'http:999999999',
            'ftp:9999999999',
            '\n',
        ]
        for url in urls:
            req = self.build_request(next_url=url)
            next_url = views.get_next_url(req, 'next')

            self.assertEqual(next_url, None)

    def test_https(self):
        # If the request is for HTTPS and the next url is HTTPS, then that
        # works with all Djangos.
        req = self.factory.get(
            '/',
            data={'next': 'https://testserver/foo'},
            secure=True,
        )
        self.assertEquals(req.is_secure(), True)
        next_url = views.get_next_url(req, 'next')
        self.assertEqual(next_url, 'https://testserver/foo')

        # For Django 1.11+, if the request is for HTTPS and the next url is
        # HTTP, then that fails.
        if django.VERSION >= (1, 11):
            req = self.factory.get(
                '/',
                data={'next': 'http://testserver/foo'},
                secure=True,
            )
            self.assertEquals(req.is_secure(), True)
            next_url = views.get_next_url(req, 'next')
            self.assertEqual(next_url, None)


class OIDCAuthorizationRequestViewTestCase(TestCase):
    def setUp(self):
        self.factory = RequestFactory()

    @override_settings(OIDC_OP_AUTHORIZATION_ENDPOINT='https://server.example.com/auth')
    @override_settings(OIDC_RP_CLIENT_ID='example_id')
    @patch('mozilla_django_oidc.views.get_random_string')
    def test_get(self, mock_random_string):
        """"""Test initiation of a successful OIDC attempt.""""""
        mock_random_string.return_value = 'examplestring'
        url = reverse('oidc_authentication_init')
        request = self.factory.get(url)
        request.session = dict()
        login_view = views.OIDCAuthenticationRequestView.as_view()
        response = login_view(request)
        self.assertEqual(response.status_code, 302)

        o = urlparse(response.url)
        expected_query = {
            'response_type': ['code'],
            'scope': ['openid'],
            'client_id': ['example_id'],
            'redirect_uri': ['http://testserver/callback/'],
            'state': ['examplestring'],
            'nonce': ['examplestring']
        }
        self.assertDictEqual(parse_qs(o.query), expected_query)
        self.assertEqual(o.hostname, 'server.example.com')
        self.assertEqual(o.path, '/auth')

    @override_settings(OIDC_OP_AUTHORIZATION_ENDPOINT='https://server.example.com/auth')
    @override_settings(OIDC_RP_CLIENT_ID='example_id')
    def test_next_url(self):
        """"""Test that `next` url gets stored to user session.""""""
        url = reverse('oidc_authentication_init')
        request = self.factory.get('{url}?{params}'.format(url=url, params='next=/foo'))
        request.session = dict()
        login_view = views.OIDCAuthenticationRequestView.as_view()
        login_view(request)
        self.assertTrue('oidc_login_next' in request.session)
        self.assertEqual(request.session['oidc_login_next'], '/foo')

    @override_settings(OIDC_OP_AUTHORIZATION_ENDPOINT='https://server.example.com/auth')
    @override_settings(OIDC_RP_CLIENT_ID='example_id')
    def test_missing_next_url(self):
        """"""Test that `next` url gets invalidated in user session.""""""
        url = reverse('oidc_authentication_init')
        request = self.factory.get(url)
        request.session = {
            'oidc_login_next': 'foobar'
        }
        login_view = views.OIDCAuthenticationRequestView.as_view()
        login_view(request)
        self.assertTrue('oidc_login_next' in request.session)
        self.assertTrue(request.session['oidc_login_next'] is None)


class OIDCLogoutViewTestCase(TestCase):
    def setUp(self):
        self.factory = RequestFactory()

    @override_settings(LOGOUT_REDIRECT_URL='/example-logout')
    def test_get_anonymous_user(self):
        url = reverse('oidc_logout')
        request = self.factory.post(url)
        request.user = AnonymousUser()
        logout_view = views.OIDCLogoutView.as_view()

        response = logout_view(request)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/example-logout')

    @override_settings(LOGOUT_REDIRECT_URL='/example-logout')
    def test_post(self):
        user = User.objects.create_user('example_username')
        url = reverse('oidc_logout')
        request = self.factory.post(url)
        request.user = user
        logout_view = views.OIDCLogoutView.as_view()

        with patch('mozilla_django_oidc.views.auth.logout') as mock_logout:
            response = logout_view(request)
            mock_logout.assert_called_once_with(request)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/example-logout')

    @override_settings(LOGOUT_REDIRECT_URL='/example-logout')
    @override_settings(OIDC_OP_LOGOUT_URL_METHOD='tests.test_views.my_custom_op_logout')
    def test_post_with_OIDC_OP_LOGOUT_URL_METHOD(self):
        user = User.objects.create_user('example_username')
        url = reverse('oidc_logout')
        request = self.factory.post(url)
        request.user = user
        logout_view = views.OIDCLogoutView.as_view()

        with patch('mozilla_django_oidc.views.auth.logout') as mock_logout:
            response = logout_view(request)
            mock_logout.assert_called_once_with(request)

        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, 'http://example.com/logged/out')
/n/n/n",0
11,11,22b6ecb953bbf40f0394a8bfd41d71a3f16e3465,"/mozilla_django_oidc/views.py/n/nimport time
try:
    from urllib.parse import urlencode
except ImportError:
    # Python < 3
    from urllib import urlencode

from django.core.exceptions import SuspiciousOperation
from django.core.urlresolvers import reverse
from django.contrib import auth
from django.http import HttpResponseRedirect
from django.utils.crypto import get_random_string
from django.utils.module_loading import import_string
from django.views.generic import View

from mozilla_django_oidc.utils import (
    absolutify,
    import_from_settings,
    is_authenticated,
)


class OIDCAuthenticationCallbackView(View):
    """"""OIDC client authentication callback HTTP endpoint""""""

    http_method_names = ['get']

    @property
    def failure_url(self):
        return import_from_settings('LOGIN_REDIRECT_URL_FAILURE', '/')

    @property
    def success_url(self):
        next_url = self.request.session.get('oidc_login_next', None)
        return next_url or import_from_settings('LOGIN_REDIRECT_URL', '/')

    def login_failure(self):
        return HttpResponseRedirect(self.failure_url)

    def login_success(self):
        auth.login(self.request, self.user)

        # Figure out when this id_token will expire. This is ignored unless you're
        # using the RenewIDToken middleware.
        expiration_interval = import_from_settings('OIDC_RENEW_ID_TOKEN_EXPIRY_SECONDS', 60 * 15)
        self.request.session['oidc_id_token_expiration'] = time.time() + expiration_interval

        return HttpResponseRedirect(self.success_url)

    def get(self, request):
        """"""Callback handler for OIDC authorization code flow""""""

        nonce = request.session.get('oidc_nonce')
        if nonce:
            # Make sure that nonce is not used twice
            del request.session['oidc_nonce']

        if 'code' in request.GET and 'state' in request.GET:
            kwargs = {
                'request': request,
                'nonce': nonce,
            }

            if 'oidc_state' not in request.session:
                return self.login_failure()

            if request.GET['state'] != request.session['oidc_state']:
                msg = 'Session `oidc_state` does not match the OIDC callback state'
                raise SuspiciousOperation(msg)

            self.user = auth.authenticate(**kwargs)

            if self.user and self.user.is_active:
                return self.login_success()
        return self.login_failure()


class OIDCAuthenticationRequestView(View):
    """"""OIDC client authentication HTTP endpoint""""""

    http_method_names = ['get']

    def __init__(self, *args, **kwargs):
        super(OIDCAuthenticationRequestView, self).__init__(*args, **kwargs)

        self.OIDC_OP_AUTH_ENDPOINT = import_from_settings('OIDC_OP_AUTHORIZATION_ENDPOINT')
        self.OIDC_RP_CLIENT_ID = import_from_settings('OIDC_RP_CLIENT_ID')

    def get(self, request):
        """"""OIDC client authentication initialization HTTP endpoint""""""
        state = get_random_string(import_from_settings('OIDC_STATE_SIZE', 32))
        redirect_field_name = import_from_settings('OIDC_REDIRECT_FIELD_NAME', 'next')

        params = {
            'response_type': 'code',
            'scope': 'openid',
            'client_id': self.OIDC_RP_CLIENT_ID,
            'redirect_uri': absolutify(
                request,
                reverse('oidc_authentication_callback')
            ),
            'state': state,
        }

        if import_from_settings('OIDC_USE_NONCE', True):
            nonce = get_random_string(import_from_settings('OIDC_NONCE_SIZE', 32))
            params.update({
                'nonce': nonce
            })
            request.session['oidc_nonce'] = nonce

        request.session['oidc_state'] = state
        request.session['oidc_login_next'] = request.GET.get(redirect_field_name)

        query = urlencode(params)
        redirect_url = '{url}?{query}'.format(url=self.OIDC_OP_AUTH_ENDPOINT, query=query)
        return HttpResponseRedirect(redirect_url)


class OIDCLogoutView(View):
    """"""Logout helper view""""""

    http_method_names = ['get', 'post']

    @property
    def redirect_url(self):
        """"""Return the logout url defined in settings.""""""
        return import_from_settings('LOGOUT_REDIRECT_URL', '/')

    def post(self, request):
        """"""Log out the user.""""""
        logout_url = self.redirect_url

        if is_authenticated(request.user):
            # Check if a method exists to build the URL to log out the user
            # from the OP.
            logout_from_op = import_from_settings('OIDC_OP_LOGOUT_URL_METHOD', '')
            if logout_from_op:
                logout_url = import_string(logout_from_op)()

            # Log out the Django user, only if she was actually logged in.
            auth.logout(request)

        return HttpResponseRedirect(logout_url)
/n/n/n",1
30,30,03df8ce6bddc56b2487df3898758f4c1624d906f,"app/auth/views.py/n/nfrom urlparse import urlparse, urlunparse
from flask import render_template, redirect, request, url_for, flash, session
from flask.ext.login import login_user, logout_user, login_required, \
    current_user
from . import auth
from .. import db
from ..models import User
from ..email import send_email
from .forms import LoginForm, RegistrationForm, ChangePasswordForm,\
    PasswordResetRequestForm, PasswordResetForm, ChangeEmailForm


@auth.before_app_request
def before_request():
    if current_user.is_authenticated:
        current_user.ping()
        if not current_user.verify_auth_token(session['auth_token']):
            logout_user()
            flash('Your session has expired.')
            return redirect(url_for('auth.login'))
        if not current_user.confirmed \
                and request.endpoint[:5] != 'auth.' \
                and request.endpoint != 'static':
            return redirect(url_for('auth.unconfirmed'))


@auth.route('/unconfirmed')
def unconfirmed():
    if current_user.is_anonymous or current_user.confirmed:
        return redirect(url_for('main.index'))
    return render_template('auth/unconfirmed.html')


def sanitize_redirect_url(target):
    # Defeats open redirect attacks.
    if target:
        parts = list(urlparse(target)[2:])
        parts[0] = '/'.join(filter(None, parts[0].split('/')))
        return '/' + urlunparse(['', ''] + parts)


@auth.route('/login', methods=['GET', 'POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user is not None and user.verify_password(form.password.data):
            login_user(user, form.remember_me.data)
            session['auth_token'] = user.auth_token
            next = sanitize_redirect_url(request.args.get('next'))
            return redirect(next or url_for('main.index'))
        flash('Invalid username or password.')
    return render_template('auth/login.html', form=form)


@auth.route('/logout')
@login_required
def logout():
    logout_user()
    flash('You have logged out.')
    return redirect(url_for('main.index'))


@auth.route('/register', methods=['GET', 'POST'])
def register():
    form = RegistrationForm()
    if form.validate_on_submit():
        user = User(email=form.email.data,
                    username=form.username.data,
                    password=form.password.data)
        db.session.add(user)
        db.session.commit()
        token = user.generate_confirmation_token()
        send_email(user.email, 'Confirm Your Account',
                   'auth/email/confirm', user=user, token=token)
        flash('Check your inbox! A confirmation email has been sent.')
        return redirect(url_for('auth.login'))
    return render_template('auth/register.html', form=form)


@auth.route('/confirm/<token>')
@login_required
def confirm(token):
    if current_user.confirmed:
        return redirect(url_for('main.index'))
    if current_user.confirm(token):
        flash('Your account is confirmed. Thank you!')
    else:
        flash('The confirmation link is invalid or has expired.')
    return redirect(url_for('main.index'))


@auth.route('/confirm')
@login_required
def resend_confirmation():
    token = current_user.generate_confirmation_token()
    send_email(current_user.email, 'Confirm Your Account',
               'auth/email/confirm', user=current_user, token=token)
    flash('A new confirmation email has been sent.')
    return redirect(url_for('main.index'))


@auth.route('/change-password', methods=['GET', 'POST'])
@login_required
def change_password():
    form = ChangePasswordForm()
    if form.validate_on_submit():
        if current_user.verify_password(form.old_password.data):
            current_user.password = form.password.data
            db.session.add(current_user)
            session['auth_token'] = current_user.auth_token
            flash('Your password has been updated.')
            return redirect(url_for('main.index'))
        else:
            flash('Invalid password.')
    return render_template(""auth/change_password.html"", form=form)


@auth.route('/reset', methods=['GET', 'POST'])
def password_reset_request():
    if not current_user.is_anonymous:
        return redirect(url_for('main.index'))
    form = PasswordResetRequestForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user:
            token = user.generate_reset_token()
            send_email(user.email, 'Reset Your Password',
                       'auth/email/reset_password',
                       user=user, token=token,
                       next=request.args.get('next'))
        flash('An email with instructions for resetting your password has been '
              'sent.')
        return redirect(url_for('auth.login'))
    return render_template('auth/reset_password.html', form=form)


@auth.route('/reset/<token>', methods=['GET', 'POST'])
def password_reset(token):
    if not current_user.is_anonymous:
        return redirect(url_for('main.index'))
    form = PasswordResetForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user is None:
            return redirect(url_for('main.index'))
        if user.reset_password(token, form.password.data):
            flash('Your password has been updated.')
            return redirect(url_for('auth.login'))
        else:
            return redirect(url_for('main.index'))
    return render_template('auth/reset_password.html', form=form)


@auth.route('/change-email', methods=['GET', 'POST'])
@login_required
def change_email_request():
    form = ChangeEmailForm()
    if form.validate_on_submit():
        if current_user.verify_password(form.password.data):
            new_email = form.email.data
            token = current_user.generate_email_change_token(new_email)
            send_email(new_email, 'Confirm Your Email Address',
                       'auth/email/change_email',
                       user=current_user, token=token)
            flash('An email with instructions for confirming your new email '
                  'address has been sent.')
            return redirect(url_for('main.index'))
        else:
            flash('Invalid password.')
    return render_template(""auth/change_email.html"", form=form)


@auth.route('/change-email/<token>')
@login_required
def change_email(token):
    if current_user.change_email(token):
        session['auth_token'] = current_user.auth_token
        flash('Your email address has been updated.')
    else:
        flash('Invalid request.')
    return redirect(url_for('main.index'))
/n/n/n",0
31,31,03df8ce6bddc56b2487df3898758f4c1624d906f,"/app/auth/views.py/n/nfrom flask import render_template, redirect, request, url_for, flash, session
from flask.ext.login import login_user, logout_user, login_required, \
    current_user
from . import auth
from .. import db
from ..models import User
from ..email import send_email
from .forms import LoginForm, RegistrationForm, ChangePasswordForm,\
    PasswordResetRequestForm, PasswordResetForm, ChangeEmailForm


@auth.before_app_request
def before_request():
    if current_user.is_authenticated:
        current_user.ping()
        if not current_user.verify_auth_token(session['auth_token']):
            logout_user()
            flash('Your session has expired.')
            return redirect(url_for('auth.login'))
        if not current_user.confirmed \
                and request.endpoint[:5] != 'auth.' \
                and request.endpoint != 'static':
            return redirect(url_for('auth.unconfirmed'))


@auth.route('/unconfirmed')
def unconfirmed():
    if current_user.is_anonymous or current_user.confirmed:
        return redirect(url_for('main.index'))
    return render_template('auth/unconfirmed.html')


@auth.route('/login', methods=['GET', 'POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user is not None and user.verify_password(form.password.data):
            login_user(user, form.remember_me.data)
            session['auth_token'] = user.auth_token
            return redirect(request.args.get('next') or url_for('main.index'))
        flash('Invalid username or password.')
    return render_template('auth/login.html', form=form)


@auth.route('/logout')
@login_required
def logout():
    logout_user()
    flash('You have logged out.')
    return redirect(url_for('main.index'))


@auth.route('/register', methods=['GET', 'POST'])
def register():
    form = RegistrationForm()
    if form.validate_on_submit():
        user = User(email=form.email.data,
                    username=form.username.data,
                    password=form.password.data)
        db.session.add(user)
        db.session.commit()
        token = user.generate_confirmation_token()
        send_email(user.email, 'Confirm Your Account',
                   'auth/email/confirm', user=user, token=token)
        flash('Check your inbox! A confirmation email has been sent.')
        return redirect(url_for('auth.login'))
    return render_template('auth/register.html', form=form)


@auth.route('/confirm/<token>')
@login_required
def confirm(token):
    if current_user.confirmed:
        return redirect(url_for('main.index'))
    if current_user.confirm(token):
        flash('Your account is confirmed. Thank you!')
    else:
        flash('The confirmation link is invalid or has expired.')
    return redirect(url_for('main.index'))


@auth.route('/confirm')
@login_required
def resend_confirmation():
    token = current_user.generate_confirmation_token()
    send_email(current_user.email, 'Confirm Your Account',
               'auth/email/confirm', user=current_user, token=token)
    flash('A new confirmation email has been sent.')
    return redirect(url_for('main.index'))


@auth.route('/change-password', methods=['GET', 'POST'])
@login_required
def change_password():
    form = ChangePasswordForm()
    if form.validate_on_submit():
        if current_user.verify_password(form.old_password.data):
            current_user.password = form.password.data
            db.session.add(current_user)
            session['auth_token'] = current_user.auth_token
            flash('Your password has been updated.')
            return redirect(url_for('main.index'))
        else:
            flash('Invalid password.')
    return render_template(""auth/change_password.html"", form=form)


@auth.route('/reset', methods=['GET', 'POST'])
def password_reset_request():
    if not current_user.is_anonymous:
        return redirect(url_for('main.index'))
    form = PasswordResetRequestForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user:
            token = user.generate_reset_token()
            send_email(user.email, 'Reset Your Password',
                       'auth/email/reset_password',
                       user=user, token=token,
                       next=request.args.get('next'))
        flash('An email with instructions for resetting your password has been '
              'sent.')
        return redirect(url_for('auth.login'))
    return render_template('auth/reset_password.html', form=form)


@auth.route('/reset/<token>', methods=['GET', 'POST'])
def password_reset(token):
    if not current_user.is_anonymous:
        return redirect(url_for('main.index'))
    form = PasswordResetForm()
    if form.validate_on_submit():
        user = User.query.filter_by(email=form.email.data).first()
        if user is None:
            return redirect(url_for('main.index'))
        if user.reset_password(token, form.password.data):
            flash('Your password has been updated.')
            return redirect(url_for('auth.login'))
        else:
            return redirect(url_for('main.index'))
    return render_template('auth/reset_password.html', form=form)


@auth.route('/change-email', methods=['GET', 'POST'])
@login_required
def change_email_request():
    form = ChangeEmailForm()
    if form.validate_on_submit():
        if current_user.verify_password(form.password.data):
            new_email = form.email.data
            token = current_user.generate_email_change_token(new_email)
            send_email(new_email, 'Confirm Your Email Address',
                       'auth/email/change_email',
                       user=current_user, token=token)
            flash('An email with instructions for confirming your new email '
                  'address has been sent.')
            return redirect(url_for('main.index'))
        else:
            flash('Invalid password.')
    return render_template(""auth/change_email.html"", form=form)


@auth.route('/change-email/<token>')
@login_required
def change_email(token):
    if current_user.change_email(token):
        session['auth_token'] = current_user.auth_token
        flash('Your email address has been updated.')
    else:
        flash('Invalid request.')
    return redirect(url_for('main.index'))
/n/n/n",1
154,154,e3bccfc582beb57800c33e4f0afe01351733f2a5,"readthedocs/redirects/models.py/n/n# -*- coding: utf-8 -*-

""""""Django models for the redirects app.""""""

import logging
import re

from django.db import models
from django.utils.translation import ugettext
from django.utils.translation import ugettext_lazy as _

from readthedocs.core.resolver import resolve_path
from readthedocs.projects.models import Project

from .managers import RedirectManager


log = logging.getLogger(__name__)

HTTP_STATUS_CHOICES = (
    (301, _('301 - Permanent Redirect')),
    (302, _('302 - Temporary Redirect')),
)

STATUS_CHOICES = (
    (True, _('Active')),
    (False, _('Inactive')),
)

TYPE_CHOICES = (
    ('prefix', _('Prefix Redirect')),
    ('page', _('Page Redirect')),
    ('exact', _('Exact Redirect')),
    ('sphinx_html', _('Sphinx HTMLDir -> HTML')),
    ('sphinx_htmldir', _('Sphinx HTML -> HTMLDir')),
    # ('advanced', _('Advanced')),
)

# FIXME: this help_text message should be dynamic since ""Absolute path"" doesn't
# make sense for ""Prefix Redirects"" since the from URL is considered after the
# ``/$lang/$version/`` part. Also, there is a feature for the ""Exact
# Redirects"" that should be mentioned here: the usage of ``$rest``
from_url_helptext = _(
    'Absolute path, excluding the domain. '
    'Example: <b>/docs/</b>  or <b>/install.html</b>',
)
to_url_helptext = _(
    'Absolute or relative URL. Example: '
    '<b>/tutorial/install.html</b>',
)
redirect_type_helptext = _('The type of redirect you wish to use.')


class Redirect(models.Model):

    """"""A HTTP redirect associated with a Project.""""""

    project = models.ForeignKey(
        Project,
        verbose_name=_('Project'),
        related_name='redirects',
    )

    redirect_type = models.CharField(
        _('Redirect Type'),
        max_length=255,
        choices=TYPE_CHOICES,
        help_text=redirect_type_helptext,
    )

    from_url = models.CharField(
        _('From URL'),
        max_length=255,
        db_index=True,
        help_text=from_url_helptext,
        blank=True,
    )

    to_url = models.CharField(
        _('To URL'),
        max_length=255,
        db_index=True,
        help_text=to_url_helptext,
        blank=True,
    )

    http_status = models.SmallIntegerField(
        _('HTTP Status'),
        choices=HTTP_STATUS_CHOICES,
        default=301,
    )
    status = models.BooleanField(choices=STATUS_CHOICES, default=True)

    create_dt = models.DateTimeField(auto_now_add=True)
    update_dt = models.DateTimeField(auto_now=True)

    objects = RedirectManager()

    class Meta:
        verbose_name = _('redirect')
        verbose_name_plural = _('redirects')
        ordering = ('-update_dt',)

    def __str__(self):
        redirect_text = '{type}: {from_to_url}'
        if self.redirect_type in ['prefix', 'page', 'exact']:
            return redirect_text.format(
                type=self.get_redirect_type_display(),
                from_to_url=self.get_from_to_url_display(),
            )
        return ugettext(
            'Redirect: {}'.format(
                self.get_redirect_type_display(),
            ),
        )

    def get_from_to_url_display(self):
        if self.redirect_type in ['prefix', 'page', 'exact']:
            from_url = self.from_url
            to_url = self.to_url
            if self.redirect_type == 'prefix':
                to_url = '/{lang}/{version}/'.format(
                    lang=self.project.language,
                    version=self.project.default_version,
                )
            return '{from_url} -> {to_url}'.format(
                from_url=from_url,
                to_url=to_url,
            )
        return ''

    def get_full_path(self, filename, language=None, version_slug=None, allow_crossdomain=False):
        """"""
        Return a full path for a given filename.

        This will include version and language information. No protocol/domain
        is returned.
        """"""
        # Handle explicit http redirects
        if allow_crossdomain and re.match('^https?://', filename):
            return filename

        return resolve_path(
            project=self.project,
            language=language,
            version_slug=version_slug,
            filename=filename,
        )

    def get_redirect_path(self, path, language=None, version_slug=None):
        method = getattr(
            self,
            'redirect_{type}'.format(
                type=self.redirect_type,
            ),
        )
        return method(path, language=language, version_slug=version_slug)

    def redirect_prefix(self, path, language=None, version_slug=None):
        if path.startswith(self.from_url):
            log.debug('Redirecting %s', self)
            cut_path = re.sub('^%s' % self.from_url, '', path)

            to = self.get_full_path(
                filename=cut_path,
                language=language,
                version_slug=version_slug,
                allow_crossdomain=False,
            )
            return to

    def redirect_page(self, path, language=None, version_slug=None):
        if path == self.from_url:
            log.debug('Redirecting %s', self)
            to = self.get_full_path(
                filename=self.to_url.lstrip('/'),
                language=language,
                version_slug=version_slug,
                allow_crossdomain=True,
            )
            return to

    def redirect_exact(self, path, language=None, version_slug=None):
        full_path = path
        if language and version_slug:
            # reconstruct the full path for an exact redirect
            full_path = self.get_full_path(path, language, version_slug, allow_crossdomain=False)
        if full_path == self.from_url:
            log.debug('Redirecting %s', self)
            return self.to_url
        # Handle full sub-level redirects
        if '$rest' in self.from_url:
            match = self.from_url.split('$rest')[0]
            if full_path.startswith(match):
                cut_path = re.sub('^%s' % match, self.to_url, full_path)
                return cut_path

    def redirect_sphinx_html(self, path, language=None, version_slug=None):
        for ending in ['/', '/index.html']:
            if path.endswith(ending):
                log.debug('Redirecting %s', self)
                path = path[1:]  # Strip leading slash.
                to = re.sub(ending + '$', '.html', path)
                return self.get_full_path(
                    filename=to,
                    language=language,
                    version_slug=version_slug,
                    allow_crossdomain=False,
                )

    def redirect_sphinx_htmldir(self, path, language=None, version_slug=None):
        if path.endswith('.html'):
            log.debug('Redirecting %s', self)
            path = path[1:]  # Strip leading slash.
            to = re.sub('.html$', '/', path)
            return self.get_full_path(
                filename=to,
                language=language,
                version_slug=version_slug,
                allow_crossdomain=False,
            )
/n/n/nreadthedocs/redirects/utils.py/n/n""""""
Redirection view support.

This module allows for parsing a URL path, looking up redirects associated
with it in the database, and generating a redirect response.

These are not used directly as views; they are instead included into 404
handlers, so that redirects only take effect if no other view matches.
""""""
import logging
import re
from urllib.parse import urlparse, urlunparse

from django.http import HttpResponseRedirect

from readthedocs.constants import LANGUAGES_REGEX
from readthedocs.projects.models import Project


log = logging.getLogger(__name__)


def project_and_path_from_request(request, path):
    """"""
    Parse the project from a request path.

    Return a tuple (project, path) where `project` is a projects.Project if
    a matching project exists, and `path` is the unmatched remainder of the
    path.

    If the path does not match, or no matching project is found, then `project`
    will be ``None``.
    """"""
    if hasattr(request, 'slug'):
        project_slug = request.slug
    elif path.startswith('/docs/'):
        # In this case we use the docs without subdomains. So let's strip the
        # docs prefix.
        match = re.match(
            r'^/docs/(?P<project_slug>[^/]+)(?P<path>/.*)$',
            path,
        )
        if match:
            project_slug = match.groupdict()['project_slug']
            path = match.groupdict()['path']
        else:
            return None, path
    else:
        return None, path

    try:
        project = Project.objects.get(slug=project_slug)
    except Project.DoesNotExist:
        return None, path
    return project, path


def language_and_version_from_path(path):
    match = re.match(
        r'^/(?P<language>%s)/(?P<version_slug>[^/]+)(?P<path>/.*)$' % LANGUAGES_REGEX,
        path,
    )
    if match:
        language = match.groupdict()['language']
        version_slug = match.groupdict()['version_slug']
        path = match.groupdict()['path']
        return language, version_slug, path
    return None, None, path


def get_redirect_response(request, full_path):
    project, full_path = project_and_path_from_request(request, full_path)
    if not project:
        return None

    # Handle the special case of a path that looks like a fully qualified or scheme-relative URL
    full_path = '/' + full_path.lstrip('/')

    language = None
    version_slug = None
    schema, netloc, path, params, query, fragments = urlparse(full_path)
    if not project.single_version:
        language, version_slug, path = language_and_version_from_path(path)

    path = project.redirects.get_redirect_path(
        path=path, language=language, version_slug=version_slug
    )

    if path is None:
        return None

    new_path = urlunparse((schema, netloc, path, params, query, fragments))

    # Re-use the domain and protocol used in the current request.
    # Redirects shouldn't change the domain, version or language.
    # However, if the new_path is already an absolute URI, just use it
    new_path = request.build_absolute_uri(new_path)
    return HttpResponseRedirect(new_path)
/n/n/nreadthedocs/rtd_tests/tests/test_redirects.py/n/nimport logging

from django.http import Http404
from django.test import TestCase
from django.test.utils import override_settings
from django_dynamic_fixture import fixture, get
from mock import patch

from readthedocs.builds.constants import LATEST
from readthedocs.builds.models import Version
from readthedocs.projects.models import Project
from readthedocs.redirects.models import Redirect


@override_settings(PUBLIC_DOMAIN='readthedocs.org', USE_SUBDOMAIN=False, APPEND_SLASH=False)
class RedirectTests(TestCase):
    fixtures = ['eric', 'test_data']

    def setUp(self):
        logging.disable(logging.DEBUG)
        self.client.login(username='eric', password='test')
        self.client.post(
            '/dashboard/import/',
            {
                'repo_type': 'git', 'name': 'Pip',
                'tags': 'big, fucking, monkey', 'default_branch': '',
                'project_url': 'http://pip.rtfd.org',
                'repo': 'https://github.com/fail/sauce',
                'csrfmiddlewaretoken': '34af7c8a5ba84b84564403a280d9a9be',
                'default_version': LATEST,
                'privacy_level': 'public',
                'version_privacy_level': 'public',
                'description': 'wat',
                'documentation_type': 'sphinx',
            },
        )
        pip = Project.objects.get(slug='pip')
        pip.versions.create_latest()

    def test_proper_url_no_slash(self):
        r = self.client.get('/docs/pip')
        self.assertEqual(r.status_code, 404)

    def test_proper_url(self):
        r = self.client.get('/docs/pip/')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://readthedocs.org/docs/pip/en/latest/',
        )

    # Specific Page Redirects
    def test_proper_page_on_main_site(self):
        r = self.client.get('/docs/pip/page/test.html')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://readthedocs.org/docs/pip/en/latest/test.html',
        )

    def test_page_redirect_with_query_params(self):
        r = self.client.get('/docs/pip/page/test.html?foo=bar')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://readthedocs.org/docs/pip/en/latest/test.html?foo=bar'
        )

    # If slug is neither valid lang nor valid version, it should 404.
    # TODO: This should 404 directly, not redirect first
    def test_improper_url_with_nonexistent_slug(self):
        r = self.client.get('/docs/pip/nonexistent/')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_filename_only(self):
        r = self.client.get('/docs/pip/test.html')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_dir_file(self):
        r = self.client.get('/docs/pip/nonexistent_dir/bogus.html')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_dir_subdir_file(self):
        r = self.client.get('/docs/pip/nonexistent_dir/subdir/bogus.html')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_lang_file(self):
        r = self.client.get('/docs/pip/en/bogus.html')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_lang_subdir_file(self):
        r = self.client.get('/docs/pip/en/nonexistent_dir/bogus.html')
        self.assertEqual(r.status_code, 404)

    def test_improper_url_version_dir_file(self):
        r = self.client.get('/docs/pip/latest/nonexistent_dir/bogus.html')
        self.assertEqual(r.status_code, 404)

    # Subdomains
    @override_settings(USE_SUBDOMAIN=True)
    def test_proper_subdomain(self):
        r = self.client.get('/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_root_redirect_with_query_params(self):
        r = self.client.get('/?foo=bar', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://pip.readthedocs.org/en/latest/?foo=bar'
        )

    # Specific Page Redirects
    @override_settings(USE_SUBDOMAIN=True)
    def test_proper_page_on_subdomain(self):
        r = self.client.get('/page/test.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://pip.readthedocs.org/en/latest/test.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_improper_subdomain_filename_only(self):
        r = self.client.get('/test.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 404)


@override_settings(PUBLIC_DOMAIN='readthedocs.org', USE_SUBDOMAIN=False)
class RedirectAppTests(TestCase):
    fixtures = ['eric', 'test_data']

    def setUp(self):
        self.client.login(username='eric', password='test')
        self.client.post(
            '/dashboard/import/',
            {
                'repo_type': 'git', 'name': 'Pip',
                'tags': 'big, fucking, monkey', 'default_branch': '',
                'project_url': 'http://pip.rtfd.org',
                'repo': 'https://github.com/fail/sauce',
                'csrfmiddlewaretoken': '34af7c8a5ba84b84564403a280d9a9be',
                'default_version': LATEST,
                'privacy_level': 'public',
                'version_privacy_level': 'public',
                'description': 'wat',
                'documentation_type': 'sphinx',
            },
        )
        self.pip = Project.objects.get(slug='pip')
        self.pip.versions.create_latest()

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_prefix_infinite(self):
        """"""
        Avoid infinite redirects.

        If the URL hit is the same that the URL returned for redirection, we
        return a 404.

        These examples comes from this issue:
          * https://github.com/rtfd/readthedocs.org/issues/4673
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='prefix',
            from_url='/',
        )
        r = self.client.get('/redirect.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/redirect.html',
        )

        r = self.client.get('/redirect/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/redirect/',
        )

        r = self.client.get('/en/latest/redirect/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 404)

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_prefix_crossdomain(self):
        """"""
        Avoid redirecting to an external site unless the external site is in to_url
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='prefix',
            from_url='/',
        )

        r = self.client.get('http://testserver/http://my.host/path.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/http://my.host/path.html',
        )

        r = self.client.get('http://testserver//my.host/path.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/my.host/path.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_sphinx_htmldir_crossdomain(self):
        """"""
        Avoid redirecting to an external site unless the external site is in to_url
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_htmldir',
        )

        r = self.client.get('http://testserver/http://my.host/path.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/http://my.host/path/',
        )

        r = self.client.get('http://testserver//my.host/path.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/my.host/path/',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_sphinx_html_crossdomain(self):
        """"""
        Avoid redirecting to an external site unless the external site is in to_url
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_html',
        )

        r = self.client.get('http://testserver/http://my.host/path/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/http://my.host/path.html',
        )

        r = self.client.get('http://testserver//my.host/path/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/my.host/path.html',
        )

    def test_redirect_sphinx_htmldir_crossdomain_nosubdomain(self):
        """"""
        Avoid redirecting to an external site unless the external site is in to_url
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_htmldir',
        )

        r = self.client.get('/docs/pip/http://my.host/path.html')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://testserver/docs/pip/en/latest/http://my.host/path/',
        )

        r = self.client.get('/docs/pip//my.host/path.html')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://testserver/docs/pip/en/latest/my.host/path/',
        )

    def test_redirect_sphinx_html_crossdomain_nosubdomain(self):
        """"""
        Avoid redirecting to an external site unless the external site is in to_url
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_html',
        )

        r = self.client.get('/docs/pip//http://my.host/path/')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://testserver/docs/pip/en/latest/http://my.host/path.html',
        )

        r = self.client.get('/docs/pip//my.host/path/')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://testserver/docs/pip/en/latest/my.host/path.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_root(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='prefix', from_url='/woot/',
        )
        r = self.client.get('/woot/faq.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/faq.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_page(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='page',
            from_url='/install.html', to_url='/tutorial/install.html',
        )
        r = self.client.get('/install.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/tutorial/install.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_with_query_params(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='page',
            from_url='/install.html', to_url='/tutorial/install.html'
        )
        r = self.client.get(
            '/install.html?foo=bar', HTTP_HOST='pip.readthedocs.org'
        )
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://pip.readthedocs.org/en/latest/tutorial/install.html?foo=bar'
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_exact(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='exact',
            from_url='/en/latest/install.html', to_url='/en/latest/tutorial/install.html',
        )
        r = self.client.get('/en/latest/install.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/tutorial/install.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_exact_with_rest(self):
        """"""
        Exact redirects can have a ``$rest`` in the ``from_url``.

        Use case: we want to deprecate version ``2.0`` and replace it by
        ``3.0``. We write an exact redirect from ``/en/2.0/$rest`` to
        ``/en/3.0/``.
        """"""
        Redirect.objects.create(
            project=self.pip, redirect_type='exact',
            from_url='/en/latest/$rest', to_url='/en/version/', # change version
        )
        r = self.client.get('/en/latest/guides/install.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/version/guides/install.html',
        )

        Redirect.objects.create(
            project=self.pip, redirect_type='exact',
            from_url='/es/version/$rest', to_url='/en/master/', # change language and version
        )
        r = self.client.get('/es/version/guides/install.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/master/guides/install.html',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_inactive_version(self):
        """"""
        Inactive Version (``active=False``) should redirect properly.

        The function that servers the page should return 404 when serving a page
        of an inactive version and the redirect system should work.
        """"""
        version = get(
            Version,
            slug='oldversion',
            project=self.pip,
            active=False,
        )
        Redirect.objects.create(
            project=self.pip,
            redirect_type='exact',
            from_url='/en/oldversion/',
            to_url='/en/newversion/',
        )
        r = self.client.get('/en/oldversion/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/newversion/',
        )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_keeps_version_number(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='page',
            from_url='/how_to_install.html', to_url='/install.html',
        )
        with patch('readthedocs.core.views.serve._serve_symlink_docs') as _serve_docs:
            _serve_docs.side_effect = Http404()
            r = self.client.get(
                '/en/0.8.1/how_to_install.html',
                HTTP_HOST='pip.readthedocs.org',
            )
            self.assertEqual(r.status_code, 302)
            self.assertEqual(
                r['Location'],
                'http://pip.readthedocs.org/en/0.8.1/install.html',
            )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_keeps_language(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='page',
            from_url='/how_to_install.html', to_url='/install.html',
        )
        with patch('readthedocs.core.views.serve._serve_symlink_docs') as _serve_docs:
            _serve_docs.side_effect = Http404()
            r = self.client.get(
                '/de/0.8.1/how_to_install.html',
                HTTP_HOST='pip.readthedocs.org',
            )
            self.assertEqual(r.status_code, 302)
            self.assertEqual(
                r['Location'],
                'http://pip.readthedocs.org/de/0.8.1/install.html',
            )

    @override_settings(USE_SUBDOMAIN=True)
    def test_redirect_recognizes_custom_cname(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='page', from_url='/install.html',
            to_url='/tutorial/install.html',
        )
        r = self.client.get(
            '/install.html',
            HTTP_HOST='pip.pypa.io',
            HTTP_X_RTD_SLUG='pip',
        )
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'],
            'http://pip.pypa.io/en/latest/tutorial/install.html',
        )

    @override_settings(USE_SUBDOMAIN=True, PYTHON_MEDIA=True)
    def test_redirect_html(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_html',
        )
        r = self.client.get('/en/latest/faq/', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/faq.html',
        )

    @override_settings(USE_SUBDOMAIN=True, PYTHON_MEDIA=True)
    def test_redirect_html_index(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_html',
        )
        r = self.client.get('/en/latest/faq/index.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/faq.html',
        )

    @override_settings(USE_SUBDOMAIN=True, PYTHON_MEDIA=True)
    def test_redirect_htmldir(self):
        Redirect.objects.create(
            project=self.pip, redirect_type='sphinx_htmldir',
        )
        r = self.client.get('/en/latest/faq.html', HTTP_HOST='pip.readthedocs.org')
        self.assertEqual(r.status_code, 302)
        self.assertEqual(
            r['Location'], 'http://pip.readthedocs.org/en/latest/faq/',
        )


class CustomRedirectTests(TestCase):

    @classmethod
    def setUpTestData(cls):
        cls.pip = Project.objects.create(**{
            'repo_type': 'git',
            'name': 'Pip',
            'default_branch': '',
            'project_url': 'http://pip.rtfd.org',
            'repo': 'https://github.com/fail/sauce',
            'default_version': LATEST,
            'privacy_level': 'public',
            'version_privacy_level': 'public',
            'description': 'wat',
            'documentation_type': 'sphinx',
        })
        Redirect.objects.create(
            project=cls.pip,
            redirect_type='page',
            from_url='/install.html',
            to_url='/install.html#custom-fragment',
        )

    def test_redirect_fragment(self):
        redirect = Redirect.objects.get(project=self.pip)
        path = redirect.get_redirect_path('/install.html')
        expected_path = '/docs/pip/en/latest/install.html#custom-fragment'
        self.assertEqual(path, expected_path)


@override_settings(PUBLIC_DOMAIN='readthedocs.org', USE_SUBDOMAIN=False)
class RedirectBuildTests(TestCase):
    fixtures = ['eric', 'test_data']

    def setUp(self):
        self.project = get(
            Project,
            slug='project-1',
            documentation_type='sphinx',
            conf_py_file='test_conf.py',
            versions=[fixture()],
        )
        self.version = self.project.versions.all()[0]

    def test_redirect_list(self):
        r = self.client.get('/builds/project-1/')
        self.assertEqual(r.status_code, 301)
        self.assertEqual(r['Location'], '/projects/project-1/builds/')

    def test_redirect_detail(self):
        r = self.client.get('/builds/project-1/1337/')
        self.assertEqual(r.status_code, 301)
        self.assertEqual(r['Location'], '/projects/project-1/builds/1337/')


@override_settings(PUBLIC_DOMAIN='readthedocs.org', USE_SUBDOMAIN=False)
class GetFullPathTests(TestCase):
    fixtures = ['eric', 'test_data']

    def setUp(self):
        self.proj = Project.objects.get(slug='read-the-docs')
        self.redirect = get(Redirect, project=self.proj)

    def test_http_filenames_return_themselves(self):
        # If the crossdomain flag is False (default), then we don't redirect to a different host
        self.assertEqual(
            self.redirect.get_full_path('http://rtfd.org'),
            '/docs/read-the-docs/en/latest/http://rtfd.org',
        )

        self.assertEqual(
            self.redirect.get_full_path('http://rtfd.org', allow_crossdomain=True),
            'http://rtfd.org',
        )

    def test_redirects_no_subdomain(self):
        self.assertEqual(
            self.redirect.get_full_path('index.html'),
            '/docs/read-the-docs/en/latest/index.html',
        )

    @override_settings(
        USE_SUBDOMAIN=True, PRODUCTION_DOMAIN='rtfd.org',
    )
    def test_redirects_with_subdomain(self):
        self.assertEqual(
            self.redirect.get_full_path('faq.html'),
            '/en/latest/faq.html',
        )

    @override_settings(
        USE_SUBDOMAIN=True, PRODUCTION_DOMAIN='rtfd.org',
    )
    def test_single_version_with_subdomain(self):
        self.redirect.project.single_version = True
        self.assertEqual(
            self.redirect.get_full_path('faq.html'),
            '/faq.html',
        )

    def test_single_version_no_subdomain(self):
        self.redirect.project.single_version = True
        self.assertEqual(
            self.redirect.get_full_path('faq.html'),
            '/docs/read-the-docs/faq.html',
        )
/n/n/n",0
155,155,e3bccfc582beb57800c33e4f0afe01351733f2a5,"/readthedocs/redirects/models.py/n/n# -*- coding: utf-8 -*-

""""""Django models for the redirects app.""""""

import logging
import re

from django.db import models
from django.utils.translation import ugettext
from django.utils.translation import ugettext_lazy as _

from readthedocs.core.resolver import resolve_path
from readthedocs.projects.models import Project

from .managers import RedirectManager


log = logging.getLogger(__name__)

HTTP_STATUS_CHOICES = (
    (301, _('301 - Permanent Redirect')),
    (302, _('302 - Temporary Redirect')),
)

STATUS_CHOICES = (
    (True, _('Active')),
    (False, _('Inactive')),
)

TYPE_CHOICES = (
    ('prefix', _('Prefix Redirect')),
    ('page', _('Page Redirect')),
    ('exact', _('Exact Redirect')),
    ('sphinx_html', _('Sphinx HTMLDir -> HTML')),
    ('sphinx_htmldir', _('Sphinx HTML -> HTMLDir')),
    # ('advanced', _('Advanced')),
)

# FIXME: this help_text message should be dynamic since ""Absolute path"" doesn't
# make sense for ""Prefix Redirects"" since the from URL is considered after the
# ``/$lang/$version/`` part. Also, there is a feature for the ""Exact
# Redirects"" that should be mentioned here: the usage of ``$rest``
from_url_helptext = _(
    'Absolute path, excluding the domain. '
    'Example: <b>/docs/</b>  or <b>/install.html</b>',
)
to_url_helptext = _(
    'Absolute or relative URL. Example: '
    '<b>/tutorial/install.html</b>',
)
redirect_type_helptext = _('The type of redirect you wish to use.')


class Redirect(models.Model):

    """"""A HTTP redirect associated with a Project.""""""

    project = models.ForeignKey(
        Project,
        verbose_name=_('Project'),
        related_name='redirects',
    )

    redirect_type = models.CharField(
        _('Redirect Type'),
        max_length=255,
        choices=TYPE_CHOICES,
        help_text=redirect_type_helptext,
    )

    from_url = models.CharField(
        _('From URL'),
        max_length=255,
        db_index=True,
        help_text=from_url_helptext,
        blank=True,
    )

    to_url = models.CharField(
        _('To URL'),
        max_length=255,
        db_index=True,
        help_text=to_url_helptext,
        blank=True,
    )

    http_status = models.SmallIntegerField(
        _('HTTP Status'),
        choices=HTTP_STATUS_CHOICES,
        default=301,
    )
    status = models.BooleanField(choices=STATUS_CHOICES, default=True)

    create_dt = models.DateTimeField(auto_now_add=True)
    update_dt = models.DateTimeField(auto_now=True)

    objects = RedirectManager()

    class Meta:
        verbose_name = _('redirect')
        verbose_name_plural = _('redirects')
        ordering = ('-update_dt',)

    def __str__(self):
        redirect_text = '{type}: {from_to_url}'
        if self.redirect_type in ['prefix', 'page', 'exact']:
            return redirect_text.format(
                type=self.get_redirect_type_display(),
                from_to_url=self.get_from_to_url_display(),
            )
        return ugettext(
            'Redirect: {}'.format(
                self.get_redirect_type_display(),
            ),
        )

    def get_from_to_url_display(self):
        if self.redirect_type in ['prefix', 'page', 'exact']:
            from_url = self.from_url
            to_url = self.to_url
            if self.redirect_type == 'prefix':
                to_url = '/{lang}/{version}/'.format(
                    lang=self.project.language,
                    version=self.project.default_version,
                )
            return '{from_url} -> {to_url}'.format(
                from_url=from_url,
                to_url=to_url,
            )
        return ''

    def get_full_path(self, filename, language=None, version_slug=None):
        """"""
        Return a full path for a given filename.

        This will include version and language information. No protocol/domain
        is returned.
        """"""
        # Handle explicit http redirects
        if re.match('^https?://', filename):
            return filename

        return resolve_path(
            project=self.project,
            language=language,
            version_slug=version_slug,
            filename=filename,
        )

    def get_redirect_path(self, path, language=None, version_slug=None):
        method = getattr(
            self,
            'redirect_{type}'.format(
                type=self.redirect_type,
            ),
        )
        return method(path, language=language, version_slug=version_slug)

    def redirect_prefix(self, path, language=None, version_slug=None):
        if path.startswith(self.from_url):
            log.debug('Redirecting %s', self)
            cut_path = re.sub('^%s' % self.from_url, '', path)
            to = self.get_full_path(
                filename=cut_path,
                language=language,
                version_slug=version_slug,
            )
            return to

    def redirect_page(self, path, language=None, version_slug=None):
        if path == self.from_url:
            log.debug('Redirecting %s', self)
            to = self.get_full_path(
                filename=self.to_url.lstrip('/'),
                language=language,
                version_slug=version_slug,
            )
            return to

    def redirect_exact(self, path, language=None, version_slug=None):
        full_path = path
        if language and version_slug:
            # reconstruct the full path for an exact redirect
            full_path = self.get_full_path(path, language, version_slug)
        if full_path == self.from_url:
            log.debug('Redirecting %s', self)
            return self.to_url
        # Handle full sub-level redirects
        if '$rest' in self.from_url:
            match = self.from_url.split('$rest')[0]
            if full_path.startswith(match):
                cut_path = re.sub('^%s' % match, self.to_url, full_path)
                return cut_path

    def redirect_sphinx_html(self, path, language=None, version_slug=None):
        for ending in ['/', '/index.html']:
            if path.endswith(ending):
                log.debug('Redirecting %s', self)
                path = path[1:]  # Strip leading slash.
                to = re.sub(ending + '$', '.html', path)
                return self.get_full_path(
                    filename=to,
                    language=language,
                    version_slug=version_slug,
                )

    def redirect_sphinx_htmldir(self, path, language=None, version_slug=None):
        if path.endswith('.html'):
            log.debug('Redirecting %s', self)
            path = path[1:]  # Strip leading slash.
            to = re.sub('.html$', '/', path)
            return self.get_full_path(
                filename=to,
                language=language,
                version_slug=version_slug,
            )
/n/n/n",1
88,88,90cfcb6215a5e25ca2970c671bd4482fe8dd491c,"dashboard/forms/__init__.py/n/nfrom .forms import *
from dashboard.forms.list_presence_tag_form import *
from dashboard.forms.product_tag_form import *
/n/n/ndashboard/forms/forms.py/n/nfrom dal import autocomplete
from bootstrap_datepicker_plus import DatePickerInput

from django import forms
from django.forms import BaseInlineFormSet

from django.utils.translation import ugettext_lazy as _

from dashboard.models import *
from dashboard.utils import get_extracted_models


class DataGroupForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = DataGroup
        fields = ['name', 'description', 'url', 'group_type', 'downloaded_by',
                  'downloaded_at', 'download_script', 'data_source', 'csv']
        widgets = {'downloaded_at': DatePickerInput()}
        labels = {'csv': _('Register Records CSV File'),
                  'url': _('URL'), }

    def __init__(self, *args, **kwargs):
        qs = Script.objects.filter(script_type='DL')
        self.user = kwargs.pop('user', None)
        super(DataGroupForm, self).__init__(*args, **kwargs)
        self.fields['csv'].widget.attrs.update({'accept': '.csv'})
        self.fields['download_script'].queryset = qs


class ExtractionScriptForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='EX'),
        label=""Extraction Script"")
    weight_fraction_type = forms.ModelChoiceField(
        queryset=WeightFractionType.objects.all(),
        label=""Weight Fraction Type"",
        initial=""1"")
    extract_file = forms.FileField(label=""Extracted Text CSV File"")

    def __init__(self, *args, **kwargs):
        self.dg_type = kwargs.pop('dg_type', 0)
        self.user = kwargs.pop('user', None)
        super(ExtractionScriptForm, self).__init__(*args, **kwargs)
        self.fields['weight_fraction_type'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['extract_file'].widget.attrs.update({'accept': '.csv'})
        if self.dg_type in ['FU', 'CP']:
            del self.fields['weight_fraction_type']
        self.collapsed = True


class CleanCompDataForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='DC'),
        label=""Data Cleaning Script"",
        required=True)
    clean_comp_data_file = forms.FileField(label=""Clean Composition Data CSV File"",
                                           required=True)

    def __init__(self, *args, **kwargs):
        super(CleanCompDataForm, self).__init__(*args, **kwargs)
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['clean_comp_data_file'].widget.attrs.update(
            {'accept': '.csv'})
        self.collapsed = True


class DataSourceForm(forms.ModelForm):
    required_css_class = 'required'

    class Meta:
        model = DataSource
        fields = ['title', 'url', 'estimated_records', 'state', 'priority',
                  'description']


class PriorityForm(forms.ModelForm):
    class Meta:
        model = DataSource
        fields = ['priority']

    def __init__(self, *args, **kwargs):
        super(PriorityForm, self).__init__(*args, **kwargs)
        self.fields['priority'].label = ''
        self.fields['priority'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


class QANotesForm(forms.ModelForm):
    class Meta:
        model = QANotes
        fields = ['qa_notes']
        widgets = {
            'qa_notes': forms.Textarea(attrs={
                'id': 'qa-notes-textarea', 
            }),
        }
        labels = {
            'qa_notes': _('QA Notes (required if approving edited records)'),
        }


class ExtractedTextQAForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'data_document', 'qa_checked']


class ProductLinkForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    document_type = forms.ModelChoiceField(
        queryset=DocumentType.objects.all(),
        label=""Data Document Type"",
        required=True)
    return_url = forms.CharField()

    class Meta:
        model = Product
        fields = ['title', 'manufacturer',
                  'brand_name', 'upc', 'size', 'color']

    def __init__(self, *args, **kwargs):
        super(ProductLinkForm, self).__init__(*args, **kwargs)
        self.fields['return_url'].widget = forms.HiddenInput()


class ProductForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = Product
        fields = ['title', 'manufacturer', 'brand_name', 'size', 'color',
                  'model_number', 'short_description', 'long_description']


class ProductViewForm(ProductForm):
    class Meta(ProductForm.Meta):
        exclude = ('title', 'long_description',)

    def __init__(self, *args, **kwargs):
        super(ProductForm, self).__init__(*args, **kwargs)
        for f in self.fields:
            self.fields[f].disabled = True


class BasePUCForm(forms.ModelForm):
    puc = forms.ModelChoiceField(
        queryset=PUC.objects.all(),
        label='Category',
        widget=autocomplete.ModelSelect2(
            url='puc-autocomplete',
            attrs={'data-minimum-input-length': 3, })
    )


class ProductPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']


class HabitsPUCForm(BasePUCForm):
    class Meta:
        model = ExtractedHabitsAndPracticesToPUC
        fields = ['puc']


class BulkProductPUCForm(forms.ModelForm):
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput(),
                             required=True)

    class Meta:
        model = ProductToPUC
        fields = ['puc', 'id_pks']


class BulkPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']

    def __init__(self, *args, **kwargs):
        super(BulkPUCForm, self).__init__(*args, **kwargs)
        lbl = 'Select PUC for Attribute to Assign to Selected Products'
        self.fields['puc'].label = lbl
        self.fields['puc'].widget.attrs['onchange'] = 'form.submit();'


class BulkProductTagForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    tag = forms.ModelChoiceField(queryset=PUCTag.objects.none(),
                                 label='Attribute')
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput())

    class Meta:
        model = ProductToPUC
        fields = ['tag', 'id_pks']

    def __init__(self, *args, **kwargs):
        super(BulkProductTagForm, self).__init__(*args, **kwargs)
        lbl = 'Select Attribute to Assign to Selected Products'
        self.fields['tag'].label = lbl


class ExtractedTextForm(forms.ModelForm):
    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'doc_date', 'rev_num']

        widgets = {
            'data_document': forms.HiddenInput(),
            'extraction_script': forms.HiddenInput(),
        }

class ExtractedCPCatForm(ExtractedTextForm):

    class Meta:
        model = ExtractedCPCat
        fields = ['doc_date', 'cat_code', 'description_cpcat', 'cpcat_code', 'cpcat_sourcetype']


class ExtractedCPCatEditForm(ExtractedCPCatForm):

    class Meta(ExtractedCPCatForm.Meta):
        fields = ExtractedCPCatForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num', 'cpcat_code']


class ExtractedHHDocForm(ExtractedTextForm):

    class Meta:
        model = ExtractedHHDoc
        fields = ['hhe_report_number', 'study_location', 'naics_code', 'sampling_date', 'population_gender',
                  'population_age', 'population_other', 'occupation', 'facility']


class ExtractedHHDocEditForm(ExtractedHHDocForm):

    class Meta(ExtractedHHDocForm.Meta):
        fields = ExtractedHHDocForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num']


class DocumentTypeForm(forms.ModelForm):
    class Meta:
        model = DataDocument
        fields = ['document_type']

    def __init__(self, *args, **kwargs):
        super(DocumentTypeForm, self).__init__(*args, **kwargs)
        self.fields['document_type'].label = ''
        self.fields['document_type'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


def include_extract_form(dg):
    '''Returns the ExtractionScriptForm based on conditions of DataGroup
    type as well as whether all records are matched, but not extracted
    '''
    if not dg.type in ['FU', 'CO', 'CP']:
        return False
    if dg.all_matched() and not dg.all_extracted():
        return ExtractionScriptForm(dg_type=dg.type)
    else:
        return False


class ExtractedChemicalFormSet(BaseInlineFormSet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


class ExtractedChemicalForm(forms.ModelForm):
    def __init__(self, *args, **kwargs):
        super(ExtractedChemicalForm, self).__init__(*args, **kwargs)
        # the non-field properties need to be explicitly added
        if hasattr(self.instance, 'dsstox') and self.instance.dsstox is not None:
            self.fields['true_cas'] = forms.CharField(max_length=200)
            self.fields['true_cas'].initial = self.instance.dsstox.true_cas
            self.fields['true_cas'].disabled = True
            self.fields['true_chemname'] = forms.CharField(max_length=400)
            self.fields['true_chemname'].initial = self.instance.dsstox.true_chemname
            self.fields['true_chemname'].disabled = True
            self.fields['SID'] = forms.CharField(max_length=50)
            self.fields['SID'].initial = self.instance.dsstox.sid
            self.fields['SID'].disabled = True

    class Meta:
        model = ExtractedChemical
        fields = '__all__'


def include_clean_comp_data_form(dg):
    '''Returns the CleanCompDataForm based on conditions of DataGroup
    type = Composition and at least 1 document extracted
    '''
    if not dg.type in ['CO']:
        return False
    if dg.extracted_docs() > 0:
        return CleanCompDataForm()
    else:
        return False


def create_detail_formset(document, extra=1, can_delete=False, exclude=[]):
    '''Returns the pair of formsets that will be needed based on group_type.
    .                       ('CO'),('CP'),('FU'),('HP'),('HH')
    Parameters
        ----------
        document : DataDocument
            The parent DataDocument
        extra : integer
            How many empty forms should be created for new records
        can_delete : boolean
            whether a delete checkbox is included
        exclude : list
            which fields to leave out of the form
    .

    '''
    group_type = document.data_group.type
    parent, child = get_extracted_models(group_type)
    extracted = hasattr(document, 'extractedtext')

    def make_formset(parent_model, model,
                     formset=BaseInlineFormSet,
                     form=forms.ModelForm,
                     exclude=exclude):
        formset_fields = model.detail_fields()
        if exclude:
            formset_fields = [in_field for in_field in formset_fields if not in_field in exclude]
        return forms.inlineformset_factory(parent_model=parent_model,
                                           model=model,
                                           fields=formset_fields,
                                           formset=formset,  # this specifies a custom formset
                                           form=form,
                                           extra=extra,
                                           can_delete=can_delete)

    def one():  # for chemicals or unknown
        ChemicalFormSet = make_formset(
            parent_model=parent,
            model=child,
            formset=ExtractedChemicalFormSet,
            form=ExtractedChemicalForm
        )
        return (ExtractedTextForm, ChemicalFormSet)

    def two():  # for functional_use
        FunctionalUseFormSet = make_formset(parent, child)
        return (ExtractedTextForm, FunctionalUseFormSet)

    def three():  # for habits_and_practices
        HnPFormSet = make_formset(parent, child)
        return (ExtractedTextForm, HnPFormSet)

    def four():  # for extracted_list_presence
        ListPresenceFormSet = make_formset(parent, child)
        ParentForm = ExtractedCPCatForm if extracted else ExtractedCPCatEditForm


        return (ParentForm, ListPresenceFormSet)

    def five():  # for extracted_hh_rec
        HHFormSet = make_formset(parent, child)
        ParentForm = ExtractedHHDocForm if extracted else ExtractedHHDocEditForm
        return (ParentForm, HHFormSet)
    dg_types = {
        'CO': one,
        'UN': one,
        'FU': two,
        'HP': three,
        'CP': four,
        'HH': five,
    }
    func = dg_types.get(group_type, lambda: None)
    return func()

class DataDocumentForm(forms.ModelForm):
    required_css_class = 'required'

    class Meta:
        model = DataDocument
        fields = ['title', 'subtitle', 'document_type', 'note']

    @staticmethod
    def label_from_instance(obj):
        return f""{obj.title} (in Group Type: {obj.group_type})""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        qs_hotfix = (
            self.fields['document_type']
            .queryset
            .filter(pk=1)
        )
        self.fields['document_type'].queryset = (
            self.fields['document_type']
            .queryset
            .filter(group_type=self.instance.data_group.group_type)
        ) | qs_hotfix
        self.fields['document_type'].label_from_instance = self.label_from_instance
/n/n/ndashboard/forms/list_presence_tag_form.py/n/nfrom django import forms
from dashboard.models import ExtractedListPresence
from taggit.forms import TagWidget, TagField

class ExtractedListPresenceTagForm(forms.ModelForm):
    tags = TagField(label=""List Presence Keywords"", required=False)

    class Meta:
        model = ExtractedListPresence
        fields = ['tags']
        widgets = {
            'tags': TagWidget()
        }

    def __init__(self, *args, **kwargs):
        super(ExtractedListPresenceTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.attrs.update({'class':'mr-2 ml-2','size':'60'})
/n/n/ndashboard/forms/product_tag_form.py/n/nfrom django.forms import ModelForm
from taggit.forms import TagField

from dashboard.models import PUCToTag, Product
from dashboard.widgets import FilteredLabelWidget


class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))

    class Meta:
        model = Product
        fields = ['tags']

    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self
/n/n/ndashboard/migrations/0102_auto_20190411_1349.py/n/n# Generated by Django 2.1.7 on 2019-04-11 13:49

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ('dashboard', '0101_verbose_dsstox_names'),
    ]

    operations = [
        migrations.AlterField(
            model_name='script',
            name='url',
            field=models.CharField(blank=True, max_length=225, null=True,
                                   validators=[django.core.validators.URLValidator()]),
        ),
    ]
/n/n/ndashboard/migrations/0103_auto_20190411_1538.py/n/n# Generated by Django 2.1.7 on 2019-04-11 15:38

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ('dashboard', '0102_auto_20190411_1349'),
    ]

    operations = [
        migrations.AlterField(
            model_name='taxonomy',
            name='title',
            field=models.CharField(max_length=250),
        ),
    ]
/n/n/ndashboard/migrations/0104_auto_20190422_0933.py/n/n# Generated by Django 2.1.2 on 2019-04-22 09:33

from django.db import migrations, models
import django.db.models.deletion
import taggit.managers


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0103_auto_20190411_1538'),
    ]

    operations = [
        migrations.CreateModel(
            name='ExtractedListPresenceTag',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True, null=True)),
                ('updated_at', models.DateTimeField(auto_now=True, null=True)),
                ('name', models.CharField(max_length=100, unique=True, verbose_name='Name')),
                ('slug', models.SlugField(max_length=100, unique=True, verbose_name='Slug')),
            ],
            options={
                'verbose_name': 'ExtractedListPresence Keyword',
                'verbose_name_plural': 'ExtractedListPresence Keywords',
                'ordering': ('name',),
            },
        ),
        migrations.CreateModel(
            name='ExtractedListPresenceToTag',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True, null=True)),
                ('updated_at', models.DateTimeField(auto_now=True, null=True)),
                ('content_object', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='dashboard.ExtractedListPresence')),
                ('tag', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='dashboard_extractedlistpresencetotag_items', to='dashboard.ExtractedListPresenceTag')),
            ],
            options={
                'abstract': False,
            },
        ),
        migrations.AddField(
            model_name='extractedlistpresence',
            name='tags',
            field=taggit.managers.TaggableManager(blank=True, help_text='A set of keywords applicable to this Extracted List Presence', through='dashboard.ExtractedListPresenceToTag', to='dashboard.ExtractedListPresenceTag', verbose_name='Tags'),
        ),
    ]
/n/n/ndashboard/migrations/0105_document_subtitle.py/n/n# Generated by Django 2.1.2 on 2019-04-24 09:57

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('dashboard', '0104_auto_20190422_0933'),
    ]

    operations = [
        migrations.AddField(
            model_name='datadocument',
            name='subtitle',
            field=models.CharField(blank=True, default=None, max_length=250, null=True),
        ),
    ]
/n/n/ndashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.tag)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/ndashboard/models/__init__.py/n/nfrom .common_info import CommonInfo
from .data_source import DataSource
from .group_type import GroupType
from .data_group import DataGroup
from .document_type import DocumentType
from .data_document import DataDocument
from .ingredient import Ingredient
from .product import Product
from .source_category import SourceCategory
from .product_document import ProductDocument
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_habits_and_practices import ExtractedHabitsAndPractices
from .extracted_list_presence import ExtractedListPresence, ExtractedListPresenceTag, ExtractedListPresenceToTag
from .extracted_hhdoc import ExtractedHHDoc
from .extracted_hhrec import ExtractedHHRec
from .script import Script
from .dsstox_lookup import DSSToxLookup
from .qa_group import QAGroup
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .PUC import PUC, PUCToTag, PUCTag
from .product_to_tag import ProductToTag
from .product_to_puc import ProductToPUC
from .extracted_habits_and_practices_to_puc import ExtractedHabitsAndPracticesToPUC
from .qa_notes import QANotes
from .raw_chem import RawChem
from .taxonomy import Taxonomy
from .taxonomy_source import TaxonomySource
from .taxonomy_to_PUC import TaxonomyToPUC
/n/n/ndashboard/models/data_document.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.utils import timezone
from .document_type import DocumentType
from django.core.exceptions import ValidationError


class DataDocument(CommonInfo):
    """"""
    A DataDocument object is a single source of Factotum data. 

    ``filename``
        the name of the document's source file

    ``title``
        the title of the document

    ``subtitle``
        the subtitle of the document

    ``url``
        an optional URL to the document's remote source

    ``raw_category``

    ``data_group``
        the DataGroup object to which the document belongs. The
        type of the data group determines which document types the
        document might be among, and determines much of the available 
        relationships and behavior associated with the document's 
        extracted data
    
    ``products``
        Products are associated with the data document in a many-to-many relationship

    ``matched``
        When a source file for the document has been uploaded to the
        file system, the document is considered ""matched"" to that
        source file. 
    
    ``extracted``
        When the content of a data document has been extracted by manual data entry
        or by an extraction script, a new ExtractedText record is created
        with the DataDocument's id as its primary key. 
    
    ``document_type``
        each type of data group may only contain certain types of data documents. The
        clean() method checks to make sure that the assigned document type is among the
        types allowed by the group type

    ``organization``

    ``note``

    """"""

    filename = models.CharField(max_length=255)
    title = models.CharField(max_length=255)
    subtitle = models.CharField(null=True, blank=True, max_length=250, default=None)
    url = models.CharField(null=True, blank=True, max_length=275)
    raw_category = models.CharField(null=True, blank=True, max_length=100)
    data_group = models.ForeignKey('DataGroup', on_delete=models.CASCADE)
    products = models.ManyToManyField('Product', through='ProductDocument')
    matched = models.BooleanField(default=False)
    #############################################################
    #  T E C H N I C A L   D E B T 
    # Storing this as a boolean field might not be a good idea. If someone 
    # deletes an ExtractedText object in the admin panel or in the database,
    # the bit will not flip, so the document will remain ""extracted""
    # even in the absence of an ExtractedText object
    extracted = models.BooleanField(default=False)  
    # The is_extracted method below should replace this attribute
    #############################################################
    document_type = models.ForeignKey(DocumentType, on_delete=models.PROTECT,
                                                        null=True, blank=True)
    organization = models.CharField(max_length=255, blank=True)
    note = models.TextField(blank=True, null=True)

    class Meta:
        ordering = ['-id']

    def __str__(self):
        return str(self.title)
    
    @property
    def detail_page_editable(self):
        # this could be moved to settings
        return self.data_group.group_type.code in ['CP', 'HH', 'CO', ] 

    @property
    def is_extracted(self):
        return hasattr(self,'extractedtext')

    def get_absolute_url(self):
        return reverse('data_document', kwargs={'pk': self.pk})

    def get_abstract_filename(self):
        ext = self.filename.split('.')[-1] #maybe not all are PDF??
        return f'document_{self.pk}.{ext}'

    def pdf_url(self):
        dg = self.data_group
        fn = self.get_abstract_filename()
        return f'/media/{dg.fs_id}/pdf/{fn}'

    def clean(self):
        # the document_type must be one of the children types
        # of the datadocument's parent datagroup
        this_type = self.data_group.group_type
        qs_hotfix = DocumentType.objects.filter(pk=1)
        doc_types = DocumentType.objects.filter(group_type=this_type) | qs_hotfix
        if not self.document_type in doc_types:
            raise ValidationError(('The document type must be allowed by '
                                                    'the parent data group.'))
/n/n/ndashboard/models/data_group.py/n/nimport os
import shutil
import uuid
from factotum import settings
from pathlib import Path, PurePath

from django.db import models
from django.urls import reverse
from django.dispatch import receiver
from model_utils import FieldTracker
from django.core.exceptions import ValidationError

from .common_info import CommonInfo
from .group_type import GroupType
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_list_presence import ExtractedListPresence


# could be used for dynamically creating filename on instantiation
# in the 'upload_to' param on th FileField
def update_filename(instance, filename):
    name_fill_space = instance.name.replace(' ', '_')
    # potential space errors in name
    name = '{0}/{0}_{1}'.format(name_fill_space, filename)
    return name


def csv_upload_path(instance, filename):
    # potential space errors in name
    name = '{0}/{1}'.format(instance.fs_id, filename)
    return name

extract_models = {
    'CO': (ExtractedText, ExtractedChemical),
    'FU': (ExtractedText, ExtractedFunctionalUse),
    'CP': (ExtractedCPCat, ExtractedListPresence)
}


class DataGroup(CommonInfo):

    name = models.CharField(max_length=50)
    description = models.TextField(null=True, blank=True)
    downloaded_by = models.ForeignKey('auth.User',
                                    on_delete=models.SET_DEFAULT, default = 1)
    downloaded_at = models.DateTimeField()
    download_script = models.ForeignKey('Script',
                                    on_delete=models.SET_NULL, default=None,
                                    null=True, blank=True)
    data_source = models.ForeignKey('DataSource', on_delete=models.CASCADE)
    fs_id = models.UUIDField(default=uuid.uuid4, editable=False)
    csv = models.FileField(upload_to=csv_upload_path, null=True)
    zip_file = models.CharField(max_length=100)
    group_type = models.ForeignKey(GroupType, on_delete=models.SET_DEFAULT,
                                            default=1, null=True, blank=True)
    url = models.CharField(max_length=150, blank=True)

    tracker = FieldTracker()

    @property
    def type(self):
        return str(self.group_type.code)

    @property
    def is_composition(self):
        return self.type == 'CO'

    @property
    def is_habits_and_practices(self):
        return self.type == 'HP'

    @property
    def is_functional_use(self):
        return self.type == 'FU'

    @property
    def is_chemical_presence(self):
        return self.type == 'CP'

    @property
    def is_hh(self):
        return self.type == 'HH'


    def get_extract_models(self):
        '''returns a tuple with parent/child extract models'''
        return extract_models.get(self.type)

    def save(self, *args, **kwargs):
        super(DataGroup, self).save(*args, **kwargs)

    def matched_docs(self):
        return self.datadocument_set.filter(matched=True).count()

    def all_matched(self):
        return all(self.datadocument_set.values_list('matched', flat=True))

    def all_extracted(self):
        return all(self.datadocument_set.values_list('extracted', flat=True))

    def registered_docs(self):
        return self.datadocument_set.count()

    def extracted_docs(self):
        return self.datadocument_set.filter(extracted=True).count()

    def __str__(self):
        return self.name

    def get_absolute_url(self):
        return reverse('data_group_edit', kwargs={'pk': self.pk})

    def get_name_as_slug(self):
        return self.name.replace(' ', '_')

    def get_dg_folder(self):
        uuid_dir = f'{settings.MEDIA_ROOT}{str(self.fs_id)}'
        name_dir = f'{settings.MEDIA_ROOT}{self.get_name_as_slug()}'

        #this needs to handle missing csv files
        if bool(self.csv.name):
            # parse the media folder from the penultimate piece of csv file path
            p = PurePath(self.csv.path)
            csv_folder=p.parts[-2]
            csv_fullfolderpath   = f'{settings.MEDIA_ROOT}{csv_folder}'

        if os.path.isdir(uuid_dir):
            return uuid_dir # UUID-based folder
        elif bool(self.csv.name) and os.path.isdir(csv_fullfolderpath):
            return csv_fullfolderpath # csv path-based folder
        else:
            return 'no_folder_found'

    @property
    def dg_folder(self):
        '''This is a ""falsy"" property. If the folder cannot be found,
        dg.dg_folder evaluates to boolean False '''
        if self.get_dg_folder() != 'no_folder_found':
            return self.get_dg_folder()
        else:
            return False


    @property
    def csv_url(self):
        '''This is a ""falsy"" property. If the csv file cannot be found,
        dg.csv_url evaluates to boolean False '''
        try:
            self.csv.size
            csv_url = self.csv.url
        except ValueError:
            csv_url = False
        except:
            csv_url = False
        return csv_url


    @property
    def zip_url(self):
        '''This is a ""falsy"" property. If the zip file cannot be found,
        dg.zip_url evaluates to boolean False '''
        if self.get_zip_url()!='no_path_found':
            return(self.get_zip_url)
        else:
            return False
        

    def get_zip_url(self):
        # the path if the data group's folder was built from a UUID:
        uuid_path = f'{self.get_dg_folder()}/{str(self.fs_id)}.zip'
        # path if the data group's folder was built from old name-based method
        zip_file_path = f'{self.get_dg_folder()}/{self.get_name_as_slug()}.zip'
        if os.path.isfile(uuid_path):   # it is a newly-added data group
            zip_url = uuid_path
        elif os.path.isfile(zip_file_path): # it is a pre-UUID data group
            zip_url = zip_file_path
        else:
            zip_url = 'no_path_found'
        return zip_url


    def get_extracted_template_fieldnames(self):
        extract_fields = ['data_document_id','data_document_filename',
                            'prod_name', 'doc_date','rev_num', 'raw_category',
                            'raw_cas', 'raw_chem_name', 'report_funcuse']
        if self.type == 'FU':
            return extract_fields
        if self.type == 'CO':
            return extract_fields + ['raw_min_comp','raw_max_comp', 'unit_type',
                                        'ingredient_rank', 'raw_central_comp']
        if self.type == 'CP':
            for name in ['prod_name','rev_num','report_funcuse']:
                extract_fields.remove(name)
            return extract_fields + ['cat_code','description_cpcat',
                                    'cpcat_code','cpcat_sourcetype']

    def get_clean_comp_data_fieldnames(self):
        return ['id','lower_wf_analysis','central_wf_analysis', 'upper_wf_analysis']

    def clean_fields(self, exclude=None):
        super().clean_fields(exclude=exclude)
        if self.tracker.has_changed('group_type_id') and self.extracted_docs():
            msg = ""The Group Type may not be changed once extracted documents have been associated with the group.""
            raise ValidationError({'group_type': msg})


@receiver(models.signals.post_delete, sender=DataGroup)
def auto_delete_file_on_delete(sender, instance, **kwargs):
    """"""
    Deletes datagroup directory from filesystem
    when datagroup instance is deleted.
    """"""
    dg_folder = instance.get_dg_folder()
    if os.path.isdir(dg_folder):
        #print('deleting folder %s for data group %s'%(dg_folder, instance.pk))
        shutil.rmtree(dg_folder)
/n/n/ndashboard/models/extracted_chemical.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .raw_chem import RawChem


def validate_ingredient_rank(value):
    if value < 1 or value > 999:
        raise ValidationError(
            (f'Quantity {value} is not allowed'), params={'value': value},)


class ExtractedChemical(CommonInfo, RawChem):

    raw_cas_old = models.CharField(
        ""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                         null=True, blank=True)
    raw_min_comp = models.CharField(""Raw minimum composition"", max_length=100,
                                    null=True, blank=True)
    raw_max_comp = models.CharField(""Raw maximum composition"", max_length=100,
                                    null=True, blank=True)
    unit_type = models.ForeignKey(UnitType, on_delete=models.PROTECT)
    report_funcuse = models.CharField(""Reported functional use"", max_length=100,
                                      null=True, blank=True)
    weight_fraction_type = models.ForeignKey(WeightFractionType,
                                             on_delete=models.PROTECT, null=True, default='1')
    ingredient_rank = models.PositiveIntegerField(""Ingredient rank"", null=True, blank=True,
                                                  validators=[validate_ingredient_rank])
    raw_central_comp = models.CharField(""Raw central composition"", max_length=100, null=True, blank=True)

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @classmethod
    def detail_fields(cls):
        return ['extracted_text', 'raw_chem_name', 'raw_cas', 'raw_min_comp', 'raw_central_comp',
                'raw_max_comp', 'unit_type', 'weight_fraction_type', 'report_funcuse',
                'ingredient_rank', 'rawchem_ptr']

    def get_datadocument_url(self):
        return self.extracted_text.data_document.get_absolute_url()

    @property
    def data_document(self):
        return self.extracted_text.data_document

    def indexing(self):
        obj = ExtractedChemicalIndex(
            meta={'id': self.id},
            chem_name=self.raw_chem_name,
            raw_cas=self.raw_cas,
            raw_chem_name=self.raw_chem_name,
            facet_model_name='Extracted Chemical',
        )
        obj.save()
        return obj.to_dict(include_meta=True)

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def true_cas(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_cas
        else:
            return None

    @property
    def true_chemname(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_chemname
        else:
            return None

    @property
    def sid(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.sid
        else:
            return None
/n/n/ndashboard/models/extracted_functional_use.py/n/nfrom django.db import models
from .common_info import CommonInfo
from .raw_chem import RawChem

class ExtractedFunctionalUse(CommonInfo, RawChem):

    raw_cas_old = models.CharField(""Raw CAS"", max_length=50, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                  null=True, blank=True)
    report_funcuse = models.CharField(""Reported functional use"",
                                        max_length=100, null=True, blank=True)

    def __str__(self):
        return self.raw_chem_name

    @classmethod
    def detail_fields(cls):
        return ['extracted_text','raw_cas','raw_chem_name','report_funcuse']

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def data_document(self):
        return self.extracted_text.data_document
/n/n/ndashboard/models/extracted_list_presence.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager
from django.utils.translation import ugettext_lazy as _

from django.db import models
from .common_info import CommonInfo
from .raw_chem import RawChem


class ExtractedListPresence(CommonInfo, RawChem):
    raw_cas_old = models.CharField(""Raw CAS"", max_length=100,
                                        null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                        null=True, blank=True)
    qa_flag = models.BooleanField(default=False)
    tags = TaggableManager(through='dashboard.ExtractedListPresenceToTag',
                           to='dashboard.ExtractedListPresenceTag',
                           blank=True,
                           help_text='A set of keywords applicable to this Extracted List Presence')

    @classmethod
    def detail_fields(cls):
        return ['raw_cas','raw_chem_name']

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    def get_datadocument_url(self):
        return self.extracted_cpcat.data_document.get_absolute_url()

    def get_extractedtext(self):
        return self.extracted_cpcat.extractedtext_ptr
    
    @property
    def data_document(self):
        return self.extracted_text.data_document


class ExtractedListPresenceToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(ExtractedListPresence, on_delete=models.CASCADE)
    tag = models.ForeignKey('ExtractedListPresenceTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")

    def __str__(self):
        return str(self.content_object)


class ExtractedListPresenceTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""ExtractedListPresence Keyword"")
        verbose_name_plural = _(""ExtractedListPresence Keywords"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/ndashboard/models/extracted_text.py/n/nfrom model_utils.managers import InheritanceManager

from django.db import models
from django.urls import reverse

from .common_info import CommonInfo

    # this could potentially be used for 1:1 matching when uploading
    # coming in django v2.2!!
	# class Meta:
	# 	constraints = [
	# 		models.UniqueConstraint(fields=['prod_name','data_document'],
	# 								name='unique_assignment'),
	# 	]

class ExtractedText(CommonInfo):
    data_document = models.OneToOneField('DataDocument',on_delete=models.CASCADE,
                                                            primary_key=True)
    prod_name = models.CharField(max_length=500, null=True, blank=True)
    doc_date = models.CharField(max_length=25, null=True, blank=True)
    rev_num = models.CharField(max_length=50, null=True, blank=True)
    extraction_script = models.ForeignKey('Script', on_delete=models.CASCADE,
                                        limit_choices_to={'script_type': 'EX'})
    qa_checked = models.BooleanField(default=False, verbose_name=""QA approved"")
    qa_edited = models.BooleanField(default=False, verbose_name=""QA edited"")
    qa_approved_date = models.DateTimeField(null=True, blank=True,
                                                verbose_name=""QA approval date"")
    qa_approved_by = models.ForeignKey('auth.User', on_delete=models.SET_NULL,
                                                verbose_name = ""QA approved by"",
                                                null=True, blank=True,)
    qa_group = models.ForeignKey('QAGroup', verbose_name=""QA group"",
                                                     on_delete=models.SET_NULL,
                                                     null=True, blank=True)

    objects = InheritanceManager()


    def __str__(self):
        return str(self.data_document)

    def next_extracted_text_in_qa_group(self):
        nextid = 0
        # If the document is part of a Script-based QA Group, the 
        # next document is drawn from that group. If it is a CPCat
        # or HHE record, there is no next document
        extextnext = get_next_or_prev(ExtractedText.objects.filter(
            qa_group=self.qa_group, qa_checked=False), self, 'next')
        if extextnext:
            # Replace our item with the next one
            nextid = extextnext.pk
        if extextnext == self:
            nextid = 0
        return nextid
    
    def get_qa_index_path(self):
        """"""
        The type of data group to which the extracted text object belongs
        determines which QA index it will use.
        """"""
        group_type_code = self.data_document.data_group.group_type.code

        if group_type_code in ['CP','HH']:
            # TODO: change HH to its own path
            return reverse('qa_chemicalpresence_index')
        else:
            return reverse('qa_extractionscript_index')

    def get_extracted_records(self):
        return self.rawchem.all()

    def one_to_one_check(self, odict):
        '''
        Used in the upload of extracted text in the data_group_detail view, this
        returns a boolean to assure that there is a 1:1 relationship w/
        the Extracted{parent}, i.e. (Text/CPCat), and the DataDocument
        '''
        if hasattr(self, 'cat_code'):
            return self.cat_code != odict['cat_code']
        else:
            return self.prod_name != odict['prod_name']
    
    def is_approvable(self):
        '''
        Returns true or false to indicate whether the ExtractedText object
        can be approved. If the object has been edited, then there must be
        some related QA Notes.
        
        Note that if the ExtractedText object is missing a related QANotes
        record, self.qanotes will not return None. Instead it returns an 
        ObjectDoesNotExist error.
        https://stackoverflow.com/questions/3463240/check-if-onetoonefield-is-none-in-django
        
        The hasattr() method is the correct way to test for the presence of 
        a related record.

        It is not enough to test for the related record, though, because an empty
        qa_notes field is functionally equivalent to a missing QANotes record.
        
        '''

        if not self.qa_edited:
            return True
        elif self.qa_edited and not hasattr(self, 'qanotes'):
            return False
        elif self.qa_edited and hasattr(self, 'qanotes') and not bool(self.qanotes.qa_notes):
            return False
        else:
            return True
        


def get_next_or_prev(models, item, direction):
    '''
    Returns the next or previous item of
    a query-set for 'item'.

    'models' is a query-set containing all
    items of which 'item' is a part of.

    direction is 'next' or 'prev'

    '''
    getit = False
    if direction == 'prev':
        models = models.reverse()
    for m in models:
        if getit:
            return m
        if item == m:
            getit = True
    if getit:
        # This would happen when the last
        # item made getit True
        return models[0]
    return False
/n/n/ndashboard/models/ingredient.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .script import Script


def validate_wf_analysis(value):
    if value < 0 or value > 1:
        raise ValidationError(
            (f'Quantity {value} must be between 0 and 1'),params={'value': value})


class Ingredient(CommonInfo):
    lower_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                            null=True, blank=True,
                                            validators=[validate_wf_analysis])
    central_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                              null=True, blank=True,
                                              validators=[validate_wf_analysis])
    upper_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                            null=True, blank=True,
                                            validators=[validate_wf_analysis])

    script = models.ForeignKey(to=Script, on_delete=models.CASCADE,
                                                    null=True, blank=True)
                                                    
    rawchem_ptr = models.OneToOneField(related_name='ingredient', parent_link=True,
        on_delete=models.CASCADE, to='dashboard.RawChem')

    def __str__(self):
        return str(self.id)
/n/n/ndashboard/models/qa_notes.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from django.utils.translation import ugettext_lazy as _

from dashboard.models import ExtractedText


class QANotes(CommonInfo):
    extracted_text = models.OneToOneField(
        ExtractedText, on_delete=models.CASCADE)
    qa_notes = models.TextField(null=True, blank=True)

    def __str__(self):
        return self.qa_notes

    def clean(self):
        if (self.extracted_text.qa_edited and self.extracted_text.qa_checked) and not bool(self.qa_notes):
            print('About to raise a validation error')
            raise ValidationError(
                _('Before approving, please add a note explaining your edits to the extracted data'))
/n/n/ndashboard/models/raw_chem.py/n/nfrom django.db import models
from model_utils.managers import InheritanceManager
from django.apps import apps
from django.db.models.signals import pre_save

from model_utils import FieldTracker


class RawChem(models.Model):
    extracted_text = models.ForeignKey('ExtractedText', related_name = 'rawchem',
        on_delete=models.CASCADE, null=False, blank = False)

    raw_cas = models.CharField(""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name = models.CharField(""Raw chemical name"", max_length=500,
                                                        null=True, blank=True)
    temp_id = models.IntegerField(default=0, null=True, blank=True)
    temp_obj_name = models.CharField(max_length=255, null=True, blank=True)

    rid = models.CharField(max_length=50, null=True, blank=True)

    dsstox = models.ForeignKey('DSSToxLookup', related_name = 'curated_chemical', on_delete=models.PROTECT,
                                                    null=True, blank=True)

    objects = InheritanceManager()

    tracker = FieldTracker()

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @property
    def sid(self):
        '''If there is no DSSToxLookup record via the 
        curated_chemical relationship, it evaluates to boolean False '''
        try:
            return self.curated_chemical.sid
        except AttributeError:
            return False


    def get_data_document(self):
        '''Find the child object by trying each of the classes, then return the 
            datadocument id from it
            NOTE: this will be obsolete once we move the data_document 
            foreign key into RawChem in ticket 654
         '''
        id=self.id
        try:
            return apps.get_model('dashboard.ExtractedChemical').objects.get(rawchem_ptr=id).data_document
        except apps.get_model('dashboard.ExtractedChemical').DoesNotExist:
            try: 
                return apps.get_model('dashboard.ExtractedFunctionalUse').objects.get(rawchem_ptr=id).data_document
            except apps.get_model('dashboard.ExtractedFunctionalUse').DoesNotExist:
                try: 
                    return apps.get_model('dashboard.ExtractedListPresence').objects.get(rawchem_ptr=id).data_document
                except apps.get_model('dashboard.ExtractedListPresence').DoesNotExist: 
                    return False

    @staticmethod
    def pre_save(sender, **kwargs):
        instance = kwargs.get('instance')
        previous_raw_cas = instance.tracker.previous('raw_cas')
        previous_raw_chem_name = instance.tracker.previous('raw_chem_name')
       
        if instance.tracker.has_changed('raw_cas') or \
        instance.tracker.has_changed('raw_chem_name'):
            instance.dsstox = None

pre_save.connect(RawChem.pre_save, sender=RawChem)
/n/n/ndashboard/models/script.py/n/nimport math
from random import shuffle

from django.db import models
from django.urls import reverse
from django.core.validators import (URLValidator, MaxValueValidator, 
                                                    MinValueValidator)

from .common_info import CommonInfo
from .data_document import DataDocument


class Script(CommonInfo):

    TYPE_CHOICES = (('DL', 'download'),
                    ('EX', 'extraction'),
                    ('PC', 'product categorization'),
                    ('DC', 'data cleaning'))

    # Specify the share of a script's ExtractedText objects that must be
    # approved in order for the script's QA sat
    QA_COMPLETE_PERCENTAGE = 0.2


    title = models.CharField(max_length=50)
    url = models.CharField(max_length  = 225,
                            null       = True,
                            blank      = True,
                            validators = [URLValidator()])
    qa_begun = models.BooleanField(default=False)
    script_type = models.CharField( max_length = 2,
                                    choices    = TYPE_CHOICES,
                                    blank      = False,
                                    default    = 'EX')
    confidence = models.PositiveSmallIntegerField('Confidence', blank=True,
                                                validators=[
                                                        MaxValueValidator(100),
                                                        MinValueValidator(1)],
                                                                default=1)

    def __str__(self):
        return str(self.title)

    def get_absolute_url(self):
        return reverse('extraction_script_edit', kwargs={'pk': self.pk})

    def get_datadocument_count(self):
        return DataDocument.objects.filter(
                extractedtext__extraction_script=self.pk).count()

    def get_qa_complete_extractedtext_count(self):
        return DataDocument.objects.filter(extractedtext__qa_checked=True,
                            extractedtext__extraction_script=self.pk).count()

    def get_pct_checked(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return ""{0:.0f}%"".format(pct)

    def get_pct_checked_numeric(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return pct

    def qa_button_text(self):
        if self.get_qa_status():
            return ""QA Complete"" 
        elif self.qa_begun:
            return ""Continue QA""
        else:
            return ""Begin QA""

    def get_qa_status(self):
        """"""
        Compare the derived percent checked against the threshold constant
        Return true when the percent checked is above the threshold
        """"""
        return self.get_pct_checked_numeric() >= self.QA_COMPLETE_PERCENTAGE * 100

    def create_qa_group(self, force_doc_id=None):
        """"""
        Creates a QA Group for the specified Script object;
        Use all the related ExtractedText records or, if there are more than 100,
        select 20% of them. 
        """"""
        from .qa_group import QAGroup
        from .extracted_text import ExtractedText
        es = self
        # Handle cases where a QA group already exists for the script
        if QAGroup.objects.filter(extraction_script = es).count() == 1:
            # This is a valid state
            return QAGroup.objects.get(extraction_script = es)
        elif QAGroup.objects.filter(extraction_script = es).count() > 1:
            # this is a failure mode induced by the system's allowing
            # duplicate QA Groups to be created for a single script
            return QAGroup.objects.filter(extraction_script = es).first()

        
        # Create a new QA Group for the ExtractionScript es
        qa_group = QAGroup.objects.create(extraction_script=es)
        # Collect all the ExtractedText object keys that are related
        # to the Script being QA'd and have not yet been checked
        doc_text_ids = list(ExtractedText.objects.filter(extraction_script=es,
                                                    qa_checked=False
                                                    ).values_list('pk',
                                                                flat=True))
        # If there are fewer than 100 related records, they make up the entire QA Group
        if len(doc_text_ids) < 100 and len(doc_text_ids) > 0:
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids)
        # Otherwise sample 20 percent
        elif len(doc_text_ids) >= 100 :
            # Otherwise sample 20% of them
            random_20 = math.ceil(len(doc_text_ids)/5)
            shuffle(doc_text_ids)  # this is used to make random selection of texts
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids[:random_20])
        else:
            # If there are no related ExtractedText records, something has gone wrong
            # Don't make a new QA Group with zero ExtractedTexts
            # print('The Script has no related ExtractedText records')
            texts = None

        # Set the qa_group attribute of each ExtractedText record to the new QA Group    
        if texts is not None:
            for text in texts:
                text.qa_group = qa_group
                text.save()

        # If the force_doc_id argument was populated, make sure it gets assigned 
        # to the new QA Group
        if force_doc_id is not None and ExtractedText.objects.filter(pk=force_doc_id).exists():
            text = ExtractedText.objects.get(pk=force_doc_id)
            text.qa_group = qa_group
            text.save()
        
        return qa_group

        
/n/n/ndashboard/models/taxonomy.py/n/nfrom django.db import models
from .common_info import CommonInfo


class Taxonomy(CommonInfo):
    title = models.CharField(max_length=250, blank=False, null=False)
    description = models.TextField(null=True, blank=True)
    parent = models.ForeignKey('Taxonomy', on_delete=models.CASCADE,
                                                    null=True, blank=True)
    source = models.ForeignKey('TaxonomySource', on_delete=models.CASCADE)
    category_code = models.CharField(max_length=40, null=True, blank=True)
    last_edited_by = models.ForeignKey('auth.User',
                                                on_delete=models.SET_DEFAULT,
                                                default=1)
    product_category = models.ManyToManyField('PUC', through='TaxonomyToPUC')

    class Meta:
        verbose_name_plural = 'Taxonomies'

    def __str__(self):
        return str(self.title)
/n/n/ndashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        puc = self.objects.puc
        # import pdb; pdb.set_trace()

        allowedTag = PUCTag.objects.create(name='aerosol')
        PUCToTag.objects.create(tag=allowedTag,content_object=puc,assumed=False)

        assumedTag = PUCTag.objects.create(name='foamspray')
        PUCToTag.objects.create(tag=assumedTag,content_object=puc,assumed=True)

        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('General category,Product family,Product type,'
            'Allowed attributes,Assumed attributes,Description,PUC type,PUC level,Product count'))
        # check the PUC from loader
        row1 = csv_lines[1].split(',')
        self.assertEqual(len(row1), 9)
        self.assertEqual(row1[0], 'Test General Category')
        self.assertEqual(row1[1], 'Test Product Family')
        self.assertEqual(row1[2], 'Test Product Type')
        self.assertEqual(row1[3], 'aerosol; foamspray')
        self.assertEqual(row1[4], 'foamspray')
        self.assertEqual(row1[5], 'Test Product Description')
        self.assertEqual(row1[6], 'FO')
        self.assertEqual(row1[7], '3')
        self.assertEqual(row1[8], '0')

class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/ndashboard/tests/functional/test_datadocument_detail.py/n/nfrom lxml import html

from django.urls import reverse
from django.test import TestCase, override_settings
from django.core.exceptions import ObjectDoesNotExist

from dashboard.forms import *
from factotum.settings import EXTRA
from dashboard.tests.loader import *


@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_absent_extracted_text(self):
        # Check every data document and confirm that its detail page loads,
        # with or without a detail formset
        for dd in DataDocument.objects.all():
            ddid = dd.id
            resp = self.client.get('/datadocument/%s/' % ddid)
            self.assertEqual(resp.status_code, 200, 'The page must return a 200 status code')
            try:
                extracted_text = ExtractedText.objects.get(data_document=dd)
            except ExtractedText.DoesNotExist:
                #print(dd.id)
                self.assertContains(resp, 'No Extracted Text exists for this Data Document')
            else:
                self.assertContains(resp, '<h4>Extracted Text')

    def test_script_links(self):
        doc = DataDocument.objects.first()
        #response = self.client.get(f'/datadocument/{doc.pk}/')
        response = self.client.get(f'/datadocument/179486/')
        self.assertIn('Download Script',response.content.decode('utf-8'))
        self.assertIn('Extraction Script',response.content.decode('utf-8'))

    def test_product_card_location(self):
        response = self.client.get('/datadocument/179486/')
        html = response.content.decode('utf-8')
        e_idx = html.index('<h4>Extracted Text')
        p_idx = html.index('<h4 class=""d-inline"">Products')
        self.assertTrue(p_idx > e_idx, ('Product card should come after ' 
                                        'Extracted Text card'))

    def test_product_create_link(self):
        response = self.client.get('/datadocument/167497/')
        self.assertContains(response, '/link_product_form/167497/')
        data = {'title'        : ['New Product'],
                'upc'          : ['stub_1860'],
                'document_type': [1],
                'return_url'   : ['/datadocument/167497/']}
        response = self.client.post('/link_product_form/167497/', data=data)
        self.assertRedirects(response,'/datadocument/167497/')
        response = self.client.get(response.url)
        self.assertContains(response, 'New Product')

    def test_product_title_duplication(self):
        response = self.client.get('/datadocument/245401/')
        self.assertContains(response, '/link_product_form/245401/')
        # Add a new Product
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9100'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9100')
        self.assertContains(response, f'product/%s' % new_product.id )

        # Add another new Product with the same title
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9101'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9101')
        self.assertContains(response, f'product/%s' % new_product.id )

    def test_add_extracted(self):
        '''Check that the user has the ability to create an extracted record
        when the document doesn't yet have an extracted record for data 
        group types 'CP' and 'HH'
        '''
        doc = DataDocument.objects.get(pk=354784)
        self.assertFalse(doc.extracted, (""This document is matched ""
                                                    ""but not extracted""))
        data = {'hhe_report_number': ['47']}
        response = self.client.post('/extractedtext/edit/354784/', data=data,
                                                            follow=True)
        doc = DataDocument.objects.get(pk=354784)
        self.assertTrue(doc.extracted, ""This document is not extracted "")
        page = html.fromstring(response.content)
        hhe_no = page.xpath('//dd[contains(@class, ""hh-report-no"")]')[0].text
        self.assertIn('47', hhe_no)

    def test_delete(self):
        '''Checks if data document is deleted after POSTing to
        /datadocument/delete/<pk>
        '''
        post_uri = ""/datadocument/delete/""
        pk = 354784
        doc_exists = lambda: DataDocument.objects.filter(pk=pk).exists()
        self.assertTrue(doc_exists(), ""Document does not exist prior to delete attempt."")
        self.client.post(post_uri + str(pk) + ""/"")
        self.assertTrue(not doc_exists(), ""Document still exists after delete attempt."")


class TestDynamicDetailFormsets(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_get_extracted_records(self):
        ''' Confirm that each detail child object returned by the get_extracted_records
        function has the correct parent '''
        for et in ExtractedText.objects.all():
            for ex_child in et.get_extracted_records():
                child_model = ex_child.__class__ # the get_extracted_records function returns different classes
                self.assertEqual(et.pk , child_model.objects.get(pk=ex_child.pk).extracted_text.pk,
                    'The ExtractedChemical object with the returned child pk should have the correct extracted_text parent')

    def test_extractedsubclasses(self):
        ''' Confirm that the inheritance manager is returning appropriate
            subclass objects and ExtractedText base class objects 
         '''
        for doc in DataDocument.objects.all():
            try:
                extsub = ExtractedText.objects.get_subclass(data_document=doc)
                # A document with the CP data group type should be linked to 
                # ExtractedCPCat objects
                if doc.data_group.group_type.code=='CP':
                    self.assertEqual(type(extsub) , ExtractedCPCat)
                elif doc.data_group.group_type.code=='HH':
                    self.assertEqual(type(extsub) , ExtractedHHDoc)
                else:
                    self.assertEqual(type(extsub) , ExtractedText)
            except ObjectDoesNotExist:
                pass


    def test_every_extractedtext(self):
        ''''Loop through all the ExtractedText objects and confirm that the new
        create_detail_formset method returns forms based on the correct models
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd, EXTRA)
            child_formset = ChildForm(instance=et)
            # Compare the model of the child formset's QuerySet to the model
            # of the ExtractedText object's child objects
            dd_child_model  = get_extracted_models(dd.data_group.group_type.code)[1]
            childform_model = child_formset.__dict__.get('queryset').__dict__.get('model')
            self.assertEqual(dd_child_model, childform_model)

    def test_curated_chemical(self):
        ''''Confirm that if an ExtractedChemical record has been matched to DSSToxLookup, the 
            DSSToxLookup fields are displayed in the card
            This checks every data document.
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd)
            child_formset = ChildForm(instance=et)
            for form in child_formset.forms:
                if dd.data_group.type in ['CO','UN']:
                    ec = form.instance
                    if ec.dsstox is not None:
                        self.assertTrue( 'true_cas' in form.fields )
                        self.assertTrue( 'SID' in form.fields )
                    else:
                        self.assertFalse( 'true_cas' in form.fields )
                        self.assertFalse( 'SID' in form.fields )
                else:
                    self.assertFalse( 'true_cas' in form.fields )
            
    def test_num_forms(self):
        ''''Assure that the number of child forms is appropriate for the group type.
        '''
        for code, model in datadocument_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):

                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(
                                    reverse('data_document',kwargs={'pk': doc.pk})
                )
                num_forms = response.context['detail_formset'].total_form_count()
                children = model.objects.filter(
                                    extracted_text=doc.extractedtext
                ).count()

                if doc.detail_page_editable:
                    error = (f'{model.__module__} should have one more forms'
                                                                ' than instances')
                    self.assertEqual(num_forms, children + 1, error)
                else:
                    error = (f'{model.__module__} should have the same number'
                                                        ' of forms as instances')
                    self.assertEqual(num_forms, children, error)


    def test_listpresence_tags_form(self):
        ''''Assure that the list presence keywords appear for correct doc types and tags save
        '''
        for code, model in datadocument_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):
                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(reverse('data_document',kwargs={'pk': doc.pk}))
                response_html = html.fromstring(response.content.decode('utf8'))
                if code=='CP':
                    self.assertTrue(response_html.xpath('string(//*[@id=""id_tags""])'),
                              'Tag input should exist for Chemical Presence doc type')
                    self.assertEqual(ExtractedListPresenceTag.objects.count(), 0)
                    self.assertEqual(ExtractedListPresenceToTag.objects.count(), 0)
                    req = self.client.post(path=reverse('save_list_presence_tag_form',kwargs={'pk': doc.pk}),
                                           data={'tags':'pesticide,flavoring'})
                    self.assertEqual(ExtractedListPresenceTag.objects.count(), 2)
                    self.assertEqual(ExtractedListPresenceToTag.objects.count(), 2 * doc.extractedtext.objects.count())
                else:
                    self.assertFalse(response_html.xpath('string(//*[@id=""id_tags""])'),
                              'Tag input should only exist for Chemical Presence doc type')



/n/n/ndashboard/tests/functional/test_datadocument_form.py/n/nfrom django.utils import timezone
from django.test import RequestFactory, TestCase, override_settings

from dashboard.models import *
from dashboard.tests.loader import *
from dashboard.tests.mixins import DashboardFormFieldTestMixin
from dashboard.forms import DataDocumentForm

@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailFormTest(TestCase, DashboardFormFieldTestMixin):
    fixtures = fixtures_standard
    form = DataDocumentForm
    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')
    def test_field_exclusive_existence(self):
        self.fields_exclusive(['title', 'subtitle', 'document_type', 'note'])
    def test_post_fields(self):
        self.post_field('/datadocument/edit/', 'title', 'lol', pk=354784)
        self.post_field('/datadocument/edit/', 'subtitle', 'lol', pk=354784)
        self.post_field('/datadocument/edit/', 'document_type', 7, pk=5)
        self.post_field('/datadocument/edit/', 'note', 'lol', pk=354784)/n/n/ndashboard/tests/functional/test_datagroup_detail.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.auth.models import User
from django.db.models import Max

from dashboard.forms import *

from dashboard.models import *

class DataGroupDetailTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_detail_form_load(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(self.objects.doc.matched,
                    ('Document should start w/ matched False'))
        self.assertFalse(self.objects.doc.extracted,
                    ('Document should start w/ extracted False'))
        self.assertFalse(response.context['datagroup'].all_matched(),
                    ('UploadForm should be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(), (
                    'UploadForm should not be included in the page!'))
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(),
                    ('UploadForm should not be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_detail_template_fieldnames(self):
        pk = self.objects.dg.pk
        self.assertEqual(str(self.objects.dg.group_type),'Composition',
        'Type of DataGroup needs to be ""composition"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name',
                'report_funcuse','raw_min_comp','raw_max_comp', 'unit_type',
                'ingredient_rank', 'raw_central_comp'],
                ""Fieldnames passed are incorrect!"")
        self.objects.gt.title = 'Functional use'
        self.objects.gt.code = 'FU'
        self.objects.gt.save()
        self.assertEqual(str(self.objects.dg.group_type),'Functional use',
            'Type of DataGroup needs to be ""FU"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name','report_funcuse'],
                ""Fieldnames passed are incorrect!"")

    def test_unidentifed_group_type(self):
        pk = self.objects.dg.pk
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.gt.code = 'UN'
        self.objects.gt.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_bulk_create_products_form(self):
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        doc.matched = True
        self.objects.doc.matched = True
        doc.save()
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        self.assertIn('Bulk Create', response.content.decode(),
                            ""Bulk create button should be present."")
        p = Product.objects.create(upc='stub_47',data_source=self.objects.ds)
        ProductDocument.objects.create(document=doc, product=p)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
        'Product linked to all DataDocuments, no bulk_create needed.')
        self.objects.dg.group_type = GroupType.objects.create(
                                                title='Habits and practices')
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertNotIn('Bulk Create', response.content.decode(),
                            (""Bulk button shouldn't be present w/ ""
                            ""Habits and practices group_type.""))

    def test_bulk_create_post(self):
        '''test the POST to create Products and link if needed'''
        # create a new DataDocument with no Product
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        new_stub_id = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        response = self.client.post(f'/datagroup/{self.objects.dg.pk}/',
                                                                {'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Products linked to all DataDocuments, no bulk_create needed.')
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, 'unknown',
                                        'Title should be unknown in bulk_create')
        
        self.assertEqual(product.upc, f'stub_%s' % new_stub_id,
                                    'UPC should be created for second Product')

    def test_upload_note(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('Please limit upload to <600 documents at one time', response,
                      'Note to limit upload to <600 should be on the page')

    def test_extracted_count(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('0 extracted', response,
                      'Data Group should contain a count of 0 total extracted documents')
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('1 extracted', response,
                      'Data Group should contain a count of 1 total extracted documents')

    def test_delete_doc_button(self):
        url = f'/datagroup/{DataGroup.objects.first().id}/'
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-trash""></span>'
        self.assertIn(span, response,
                      'Trash button should be present if not matched.')
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-circle-check"" style=""color:green;""></span>'
        self.assertIn(span, response,
                      'Check should be present if matched.')

    def test_detail_table_headers(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertIn('<th>Product</th>', response,
                      'Data Group should have Product column.')
        fu = GroupType.objects.create(title='Functional use')
        self.objects.dg.group_type = fu
        self.objects.dg.save()
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertNotIn('<th>Product</th>', response,
                      'Data Group should have Product column.')

    def test_detail_datasource_link(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertContains(response,'<a href=""/datasource/',
                    msg_prefix='Should be able to get back to DataSource from here.')

    def test_edit_redirect(self):
        dgpk = self.objects.dg.pk
        dspk = str(self.objects.ds.pk)
        gtpk = str(self.objects.gt.pk)
        data = {'name': ['Changed Name'],
                'group_type': [gtpk],
                'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                'downloaded_at': ['08/20/2017'],
                'data_source': [dspk]}
        response = self.client.post(f'/datagroup/edit/{dgpk}/', data=data)
        self.assertEqual(response.status_code, 302,
                                         ""User is redirected to detail page."")
        self.assertEqual(response.url, f'/datagroup/{dgpk}/',
                                         ""Should go to detail page."")

class DataGroupDetailTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_download_raw_comp_data(self):
        # Ability to download, by data group, a csv file of raw extracted chemical composition data.
        # Download button would appear on data group detail page,
        # Download button would appear if any data documents have extracted text.
        # Only applies for data group type Composition. (group_type = 2)
        # Unidentified is excluded as of issue #502
        dg_co = DataGroup.objects.filter(group_type__code = 'CO').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_co.id)
        self.assertIn(b'Download Raw', resp.content)

        # Test download on all data groups with ExtractedChemicals, whether
        # they are CO or UN
        dg_ids = DataDocument.objects.filter(
            id__in=ExtractedChemical.objects.all().values('extracted_text_id')
            ).order_by().values_list('data_group_id',flat=True).distinct()

        for dg_id in dg_ids:
            #resp = self.client.get(f'/datagroup/%s/' % dg_id)
            resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_id)
            self.assertEqual(resp.status_code, 200)

        # File downloaded must include [specified fields]
        resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_ids[0])
        field_list = 'ExtractedChemical_id,raw_cas,raw_chem_name,raw_min_comp,raw_central_comp,raw_max_comp,unit_type'
        content = list(i.decode('utf-8') for i in resp.streaming_content)
        self.assertIn(field_list, content[1])
/n/n/ndashboard/tests/functional/test_extracted_qa.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import QAGroup, ExtractedText



class ExtractedQaTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_group_creation(self):
        # test the assignment of a qa_group to extracted text objects
        pk = self.objects.extext.pk
        self.assertIsNone(self.objects.extext.qa_group)
        self.assertEqual(len(QAGroup.objects.all()),0)
        pk = self.objects.extext.extraction_script.pk
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        self.assertEqual(response.status_code,200)
        qa_group = QAGroup.objects.get(
                        extraction_script=self.objects.extext.extraction_script)
        ext = ExtractedText.objects.get(qa_group=qa_group)
        self.assertIsNotNone(ext.qa_group)
        response = self.client.get(f'/qa/extractedtext/{ext.pk}/')

    def test_qa_approval_redirect(self):
        # first need to create a QAGroup w/ this get request.
        self.client.get(f'/qa/extractionscript/{self.objects.exscript.pk}/')
        pk = self.objects.extext.pk
        response = self.client.post(f'/extractedtext/approve/{pk}/')
        self.assertEqual(response.url, '/qa/extractionscript/',(""User should be redirected to ""
                                ""QA homepage after last extext is approved.""))
/n/n/ndashboard/tests/functional/test_faceted_search.py/n/nfrom django.test import TestCase
from django.test.client import Client
from lxml import html

from django.urls import resolve
from django.contrib.auth.models import User
from dashboard.tests.loader import fixtures_standard


class FacetedSearchTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.c = Client()

    def test_faceted_search_excludes_chemicals(self):
        response = self.c.get('/find/?q=ethyl')
        self.assertContains(response, 'Data Document')
        self.assertNotContains(response, 'Extracted Chemical')
        self.assertNotContains(response, 'DSSTox Substance')

    def test_faceted_search_returns_upc(self):
        response = self.c.get('/find/?q=avcat')
        self.assertContains(response, 'stub_1845')


    def test_group_type_facet(self):
        response = self.c.get('/find/?q=diatom')
        self.assertContains(response, 'Filter by Group Type')

        response = self.c.get('/find/?q=diatom&group_type=Composition')
        self.assertContains(response, 'Showing 1 - 20 of')

        response = self.c.get('/find/?q=diatom&group_type=BadGroupName')
        self.assertContains(response, 'Sorry, no result found')

    def test_faceted_search_renders_div(self):
        response = self.c.get('/find/?q=terro')
        self.assertNotContains(response, '<table')
        self.assertContains(response, '<div class=""results-wrapper"">')

    def test_product_facet_returns(self):
        response = self.c.get('/find/?q=insecticide')
        brands = response.content.count(b'name=""brand_name""')
        # default set to options = {""size"": 0} in /dashboard/views/search.py
        self.assertTrue(brands>10, ('There should be ~143 product returns '
                                                        'for this search term'))
/n/n/ndashboard/tests/functional/test_qa_seed_data.py/n/nfrom django.test import Client
from dashboard.tests.loader import *
from dashboard import views
from django.test import TestCase, override_settings, RequestFactory
from dashboard.models import DataDocument, Script, ExtractedText, ExtractedChemical, QAGroup
from django.db.models import Count


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestQaPage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_begin(self):
        """"""
        Check that starting the QA process flips the variable on the Script
        """"""
        self.assertFalse(Script.objects.get(pk=5).qa_begun,
                         'The Script should have qa_begun of False at the beginning')
        response = self.client.get('/qa/extractionscript/5/')
        self.assertTrue(Script.objects.get(pk=5).qa_begun,
                        'qa_begun should now be true')

    def test_new_qa_group_urls(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 15 has one ExtractedText object
        pk = 15
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertIn(f'/qa/extractedtext/{et.pk}/'.encode(), response.content)
        # After opening the URL, the following should be true:
        # One new QA group should be created
        group_count = QAGroup.objects.filter(extraction_script_id=pk).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(Script.objects.get(pk=15).qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        group_pk = QAGroup.objects.get(extraction_script_id=pk).pk
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertTrue(et.qa_group_id == group_pk)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/15/\'> Continue QA"".encode(), response.content)

    def test_qa_script_without_ext_text(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 9 has no ExtractedText objects
        pk = 9
        # a user will see no link on the QA index page, but it's still
        # possible to enter the URL
        response = self.client.get(f'/qa/extractionscript/{pk}/', follow=True)
        self.assertEqual(response.status_code, 200)

    def test_data_document_qa(self):
        # Open the QA page for a Composition ExtractedText record that has no QA group
        # and is in a Script with < 100 documents
        scr = Script.objects.annotate(num_ets=Count('extractedtext')).filter(
            num_ets__lt=100).filter(script_type='EX').first()
        pk = ExtractedText.objects.filter(qa_group=None).filter(extraction_script=scr
                                                                ).filter(
            data_document__data_group__group_type__code='CO').first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')

        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        group_count = QAGroup.objects.filter(extraction_script=scr).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(scr.qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        new_group = QAGroup.objects.get(extraction_script=scr)
        et = ExtractedText.objects.get(pk=pk)
        self.assertTrue(et.qa_group == new_group)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/{scr.pk}/\'> Continue QA"".encode(), response.content)

        # Open the QA page for an ExtractedText record that has no QA group and
        # is related to a script with over 100 documents
        scr = Script.objects.annotate(num_ets=Count(
            'extractedtext')).filter(num_ets__gt=100).first()
        pk = ExtractedText.objects.filter(extraction_script=scr).first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        new_group = QAGroup.objects.get(extraction_script=scr)

        # There should be a lot of ExtractedText records assigned to the QA Group
        initial_qa_count = ExtractedText.objects.filter(
            qa_group=new_group).count()
        self.assertTrue(initial_qa_count > 100)

        # Select a document that shares a Script with the
        # QA Group created above BUT DOES NOT BELONG TO THE QA GROUP
        pk = ExtractedText.objects.filter(
            extraction_script_id=scr.id).filter(qa_group=None).first().pk
        # Open its QA page via the /datdocument/qa path
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        # Make sure that the number of documents in the QA Group has increased
        self.assertGreater(ExtractedText.objects.filter(
            qa_group=new_group).count(), initial_qa_count)

    def test_habitsandpractices(self):
        # Begin from the QA index page
        response = self.client.get(f'/habitsandpractices/54/')
        self.assertContains(response, '<b>Add New Habit and Practice</b>')

    def test_dd_link(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractedtext/5', follow=True)
        self.assertIn(b'/datadocument/5', response.content)

    def test_approval(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Follow the first approval link
        response = self.client.get('/qa/extractedtext/7', follow=True)
        # print(response.context['extracted_text'])

    def test_detail_edits(self):
        '''
        After editing a detail (""child"") record, confirm that
        the save and approval functions work
        '''

        resp = self.client.get('/qa/extractedtext/7/')
        self.assertContains(resp, 'value=""dibutyl phthalate""', status_code=200)

        post_context = {
            'csrfmiddlewaretoken': ['BvtzIX6JjC5XkPWmAOduJllMTMdRLoVWeJtneuBVe5Bc3Js35EVsJunvII6vNFAy'],
            'rawchem-TOTAL_FORMS': ['2'],
            'rawchem-INITIAL_FORMS': ['1'],
            'rawchem-MIN_NUM_FORMS': ['0'],
            'rawchem-MAX_NUM_FORMS': ['1000'],
            'save': [''],
            'rawchem-0-extracted_text': ['7'],
            'rawchem-0-true_cas': ['84-74-2'],
            'rawchem-0-true_chemname': ['dibutyl phthalate'],
            'rawchem-0-SID': ['DTXSID2021781'],
            'rawchem-0-rawchem_ptr': ['4'],
            # change the raw_chem_name
            'rawchem-0-raw_chem_name': ['dibutyl phthalate edited'],
            'rawchem-0-raw_cas': ['84-74-2'],
            'rawchem-0-raw_min_comp': ['4'],
            'rawchem-0-raw_central_comp': [''],
            'rawchem-0-raw_max_comp': ['7'],
            'rawchem-0-unit_type': ['1'],
            'rawchem-0-report_funcuse': ['swell'],
            'rawchem-0-ingredient_rank': [''],
            'rawchem-1-extracted_text': ['7'],
            'rawchem-1-rawchem_ptr': [''],
            'rawchem-1-raw_chem_name': [''],
            'rawchem-1-raw_cas': [''],
            'rawchem-1-raw_min_comp': [''],
            'rawchem-1-raw_central_comp': [''],
            'rawchem-1-raw_max_comp': [''],
            'rawchem-1-unit_type': [''],
            'rawchem-1-report_funcuse': [''],
            'rawchem-1-ingredient_rank': ['']
        } 
        
        request = self.factory.post(
            path='/qa/extractedtext/7/', data=post_context)
        
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.save = [""""]
        resp = views.extracted_text_qa(pk=7, request=request)
        # The response has a 200 status code and contains the 
        # new edited value
        self.assertContains(resp, 'value=""dibutyl phthalate edited""', status_code=200)

        # Check the ORM objects here to make sure the editing has proceeded
        # correctly and the qa-related attributes are updated
        et = ExtractedText.objects.get(pk=7)
        self.assertEqual(et.qa_edited, True)

        # The RawChem object of interest is the one with the first detail form
        rc_key = post_context['rawchem-0-rawchem_ptr'][0]
        rc = RawChem.objects.get(pk=rc_key)

        # The raw_chem_name matches the new value
        self.assertEqual(rc.raw_chem_name, 'dibutyl phthalate edited')
        # The dsstox link has been broken
        self.assertFalse(rc.sid)
        
        # Change a different value and try to save it
        post_context.update({'rawchem-0-raw_central_comp': '6'})

        request = self.factory.post(
            path='/qa/extractedtext/7/', data=post_context)
        request.user = User.objects.get(username='Karyn')
        request.session = {}
        request.save = [""""]
        resp = views.extracted_text_qa(pk=7, request=request)
        # The second edit appears in the page
        self.assertContains(resp, 'name=""rawchem-0-raw_central_comp"" value=""6""', status_code=200)
        
        

        

    def test_hidden_fields(self):
        '''ExtractionScript 15 includes a functional use data group with pk = 5.
        Its QA page should hide the composition fields '''
        # Create the QA group by opening the Script's page
        response = self.client.get('/qa/extractionscript/15/', follow=True)
        # Open the DataGroup's first QA approval link
        response = self.client.get('/qa/extractedtext/5/', follow=True)
        # A raw_cas field should be in the page
        self.assertIn(
            b'<input type=""text"" name=""rawchem-1-raw_cas""', response.content)
        # There should not be any unit_type field in the functional use QA display
        self.assertNotIn(
            b'<input type=""text"" name=""rawchem-1-unit_type""', response.content)
        # The values shown should match the functional use record, not the chemical record
        self.assertIn(b'Functional Use Chem1', response.content)

        # Go back to a different ExtractionScript
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Open the QA page for a non-FunctionalUse document
        response = self.client.get('/qa/extractedtext/7/', follow=True)
        # This page should include a unit_type input form
        self.assertIn(b'rawchem-1-unit_type', response.content)

    def test_cpcat_qa(self):
        # Begin from the Chemical Presence QA index page
        response = self.client.get(f'/qa/chemicalpresence/')
        self.assertIn(
            f""/qa/chemicalpresencegroup/49/\'> View Chemical Presence Lists"".encode(), response.content)

        response = self.client.get(
            f'/qa/chemicalpresencegroup/49', follow=True)
        # The table should include the ""Begin QA"" link
        self.assertIn(
            f'/qa/extractedtext/254781/""> Begin QA'.encode(), response.content)

        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 0)
        response = self.client.get(f'/qa/extractedtext/254781/', follow=True)
        # Navigating to the extractedtext QA page should cause
        # the sampled child records to be flagged with qa_flag=True
        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 30)

        # The QA page should only show the flagged records
        elp_flagged = elps.filter(qa_flag=True).first()
        self.assertIn(elp_flagged.raw_cas.encode(), response.content)

        elp_not_flagged = elps.filter(qa_flag=False).first()
        self.assertNotIn(elp_not_flagged.raw_cas.encode(), response.content)

    def test_every_extractedtext_qa(self):
        # Attempt to open a QA page for every ExtractedText record
        for et in ExtractedText.objects.all():
            response = self.client.get(
                f'/qa/extractedtext/%s' % et.data_document_id, follow=True)
            if response.status_code != 200:
                print(et.data_document_id)
            self.assertEqual(response.status_code, 200)
/n/n/ndashboard/tests/integration/test_browser_edits.py/n/nfrom dashboard.tests.loader import *
from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from dashboard.models import *

def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.browser = load_browser()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note

        5. Approve
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781,  # Chemical Presence List
                       354783,  # HHE Report
                       ]:

            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()


            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')

            # Modify the first raw_chem_name field's value and save changes
            raw_chem_name_field.send_keys(' edited')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm that approval fails
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()

            # The page should include an error message like this one:
            '''
                <div class=""alert alert-danger alert-dismissible"" role=""alert"">
                <button type=""button"" class=""close"" data-dismiss=""alert"" aria-label=""Close"">
                    <span aria-hidden=""true""></span>
                </button>
                The extracted text
                could not be approved. Make sure that if the records have been edited,
                the QA Notes have been populated.
                </div>
            '''
            message_element = self.browser.find_element_by_xpath(
                '/html/body/div[1]/div[1]')
            self.assertIn('alert', message_element.get_attribute('role'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""qa-notes-textarea""]')
            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Save the notes
            btn_save_notes = self.browser.find_element_by_xpath(
                '//*[@id=""btn-save-notes""]')
            btn_save_notes.click()
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')


    def test_datadoc_add_extracted(self):
        '''
        Test that when a datadocument has no ExtractedText,
        the user can add one in the browser
        1.
        '''

        for doc_id in [155324   # CO record with no ExtractedText
                       ]:
            # QA Page
            dd_url = self.live_server_url + f'/datadocument/{doc_id}/'
            self.browser.get(dd_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]').click()

            # Verify that the modal window appears by finding the Cancel button
            # The modal window does not immediately appear, so the browser
            # should wait for the button to be clickable
            wait = WebDriverWait(self.browser, 10)
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            self.assertEqual(""Cancel"", cancel_button.text,
                             'The Cancel button should say Cancel')
            cancel_button.click()
            # Verify that no ExtractedText record was created
            self.assertEqual(0, ExtractedText.objects.filter(
                data_document_id=doc_id).count(),
                ""the count of ExtractedText records related to the \
                data document should be zero"")

            # Wait for the modal div to disappear
            edit_modal = wait.until(
                ec.invisibility_of_element(
                    (By.XPATH, '//*[@id=""extextModal""]')
                )
            )
            # Click the Add button again to reopen the editor
            add_button = self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]')
            add_button.click()
            # Once again, check that the controls on the modal form are clickable
            # before trying to interact with them
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            prod_name_box = self.browser.find_element_by_id(
                'id_prod_name')
            # Add a prod_name value to the box
            prod_name_box.send_keys('Fake Product')
            save_button = self.browser.find_element_by_id(
                'extracted-text-modal-save')
            save_button.click()
            # Confirm the presence of the new ExtractedText record
            et = ExtractedText.objects.get(data_document_id=doc_id)
            self.assertEqual('Fake Product', et.prod_name,
                             ""The prod_name of the new object should match what was entered"")


/n/n/ndashboard/tests/integration/test_user_experience.py/n/nfrom dashboard.tests.loader import *
from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from dashboard.models import *

def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestIntegration(StaticLiveServerTestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.browser = load_browser()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_hem(self):
        for i in range(27):
            ds = DataSource.objects.create(title=f'Test_DS_{i}')
        list_url = self.live_server_url + '/datasources/'
        self.browser.get(list_url)
        row_count = len(self.browser.find_elements_by_xpath(""//table[@id='sources']/tbody/tr""))
        self.assertEqual(row_count, 25, 'Should be 25 datasources in the table')
        # go to edit page from datasource list
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")
        detail_url = self.live_server_url + f'/datasource/{ds.pk}'
        self.browser.get(detail_url)
        # go to edit page from datasource detail
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), detail_url,
                         ""User should go back to detail view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

        num_pucs = len(PUC.objects.filter(kind='FO'))
        self.browser.get(self.live_server_url)
        import time
        time.sleep(3)  # or however long you think it'll take you to scroll down to bubble chart
        bubbles = self.browser.find_elements_by_class_name('bubble')
        self.assertEqual(num_pucs, len(bubbles), ('There should be a circle'
                                                  'drawn for every PUC'))

    def test_datagroup(self):
        list_url = self.live_server_url + '/datagroups/'
        self.browser.get(list_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")

        dg = DataGroup.objects.first()
        ds_detail_url = f'{self.live_server_url}/datasource/{dg.data_source.pk}'
        self.browser.get(ds_detail_url)
        self.browser.find_elements_by_xpath('//*[@title=""edit""]')[1].click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), ds_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        dg_detail_url = f'{self.live_server_url}/datagroup/{dg.pk}/'
        self.browser.get(dg_detail_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), dg_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        edit_url = f'{self.live_server_url}/datagroup/edit/{dg.pk}/'
        self.browser.get(edit_url)
        self.browser.find_element_by_name('cancel').click()
        self.assertIn('/datagroups/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

    def test_product(self):
        p = self.objects.p
        puc = self.objects.puc
        tag = self.objects.pt
        PUCToTag.objects.create(content_object=puc, tag=tag)
        ProductToPUC.objects.create(product=p, puc=puc)
        url = self.live_server_url + f'/product/{p.pk}/'
        self.browser.get(url)
        submit = self.browser.find_element_by_id('tag_submit')
        self.assertFalse(submit.is_enabled(), ""Button should be disabled"")
        tag = self.browser.find_element_by_class_name('taggit-tag')
        tag.click()
        self.assertTrue(submit.is_enabled(), ""Button should be enabled"")

    def test_field_exclusion(self):
        doc = self.objects.doc
        # The element should not appear on the QA page
        qa_url = self.live_server_url + f'/qa/extractedtext/{doc.pk}/'
        self.browser.get(qa_url)
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_cas""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_chemname""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-SID""]')
        # make sure the test can pick up one that should be there
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-raw_cas""]')
        except NoSuchElementException:
            self.fail(""Absence of raw_cas element raised exception"")

        # The element should appear on the datadocument page
        dd_url = self.live_server_url + f'/datadocument/{doc.pk}/'
        self.browser.get(dd_url)
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        except NoSuchElementException:
            self.fail(""Absence of weight_fraction_type element raised exception"")

/n/n/ndashboard/tests/loader.py/n/nfrom django.conf import settings
from django.utils import timezone
from django.contrib.auth.models import User
from dashboard.models import *

from selenium import webdriver
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException

fixtures_standard = [ '00_superuser',
                      '01_lookups',
                      '02_datasource',
                      '03_datagroup',
                      '04_PUC', 
                      '05_product',
                      '06_datadocument',
                      '07_rawchem_etc',
                      '08_script',
                      '09_productdocument',
                      '10_habits_and_practices',
                      '11_habits_and_practices_to_puc',
                      '12_product_to_puc',
                      '13_puc_tag'
                      ]

datadocument_models = {
                        'CO': ExtractedChemical,
                        'FU': ExtractedFunctionalUse,
                        'HP': ExtractedHabitsAndPractices,
                        'CP': ExtractedListPresence,
                        'HH': ExtractedHHRec
                    }


class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


def load_browser():
    chrome_options = Options()
    chrome_options.add_argument(""--headless"")
    if settings.TEST_BROWSER == 'firefox':
        return webdriver.Firefox()
    else:
        return webdriver.Chrome(executable_path=settings.CHROMEDRIVER_PATH, chrome_options=chrome_options)


def load_model_objects():
    user = User.objects.create_user(username='Karyn',
                                        password='specialP@55word')
    superuser = User.objects.create_superuser(username='SuperKaryn',
                                              password='specialP@55word',
                                              email='me@epa.gov')
    ds = DataSource.objects.create(title='Data Source for Test',
                                        estimated_records=2, state='AT',
                                        priority='HI')
    script = Script.objects.create(title='Test Download Script',
                                        url='http://www.epa.gov/',
                                        qa_begun=False, script_type='DL')
    exscript = Script.objects.create(title='Test Extraction Script',
                                   url='http://www.epa.gov/',
                                   qa_begun=False, script_type='EX')
    gt = GroupType.objects.create(title='Composition', code='CO')
    dg = DataGroup.objects.create(name='Data Group for Test',
                                        description='Testing...',
                                        data_source = ds,
                                        download_script=script,
                                        downloaded_by=user,
                                        downloaded_at=timezone.now(),
                                        group_type=gt,
                                        csv='register_records_matching.csv',
                                        url='https://www.epa.gov')
    dt = DocumentType.objects.create(title='MSDS',
                                    code='MS', group_type=gt)

    doc = DataDocument.objects.create(title='test document',
                                            data_group=dg,
                                            document_type=dt,
                                            filename='example.pdf')
    p = Product.objects.create(data_source=ds,
                                upc='Test UPC for ProductToPUC')

    puc = PUC.objects.create(gen_cat='Test General Category',
                              prod_fam='Test Product Family',
                              prod_type='Test Product Type',
                             description='Test Product Description',
                             last_edited_by = user,
                             kind='FO')

    extext = ExtractedText.objects.create(
                                    prod_name='Test Extracted Text Record',
                                    data_document=doc,
                                    extraction_script=exscript
                                    )
    ut = UnitType.objects.create(title='percent composition')
    wft = WeightFractionType.objects.create(title= 'reported', description= 'reported')
    ec = ExtractedChemical.objects.create(extracted_text=extext,
                                        unit_type=ut,
                                        weight_fraction_type = wft,
                                        raw_chem_name= 'Test Chem Name',
                                        raw_cas='test_cas'
                                        )
    rc = ec.rawchem_ptr
    ing = Ingredient.objects.create(lower_wf_analysis = 0.123456789012345,
                                    central_wf_analysis = 0.2,
                                    upper_wf_analysis = 1,
                                    script = script,
                                    rawchem_ptr = rc)
    
    pt = PUCTag.objects.create(name=""Test PUC Attribute"")
    pd = ProductDocument.objects.create(product=p, document=doc)
    ehp = ExtractedHabitsAndPractices.objects.create(extracted_text=extext,
                                                     product_surveyed='Test Product Surveyed',
                                                     prevalence='Continuous')


    return dotdict({'user':user,
                    'superuser':superuser,
                    'ds':ds,
                    'script':script,
                    'exscript':exscript,
                    'dg':dg,
                    'doc':doc,
                    'p':p,
                    'puc':puc,
                    'extext':extext,
                    'ut':ut,
                    'wft':wft,
                    'rc':rc,
                    'ec':ec,
                    'pt':pt,
                    'pd':pd,
                    'ing':ing,
                    'dt':dt,
                    'gt':gt,
                    'ehp':ehp
                    })
/n/n/ndashboard/tests/mixins.py/n/nclass DashboardFormFieldTestMixin(object):
    '''This class mixes in some useful methods for testing dashboard form fields.
    It expects a form to be set in self.
    '''
    def field_exists(self, field):
        '''Checks if a field exists in a Django ModelForm.
        Arguments:
            ``field``
                a string representing the field name
        '''
        self.assertTrue(field in self.form._meta.fields,
                        f'""{self.form.__name__}"""" does not include field ""{field}""')

    def fields_exclusive(self, fields):
        '''Checks if a Django ModelForm contains only certain fields
        Arguments:
            ``fields``
                an iterable of strings representing the field names
        '''
        s = set(self.form._meta.fields)
        for f in fields:
            self.field_exists(f)
            s.discard(f)
        self.assertTrue(not len(s),
                        f'""{self.form.__name__}"""" includes extra fields ""{s}""')

    def post_field(self, post_uri, field, data, pk=None):
        '''Checks if a Django ModelForm can correctly POST all its fields.
        Arguments
            ``post_uri``
                a string representing the relative root POST uri
            ``field``
                a string representing the field name
            ``data``
                data to post to field
        Keyword arguments:
            ``pk = None``
                A specific Model pk to test against. Otherwise,
                an arbitrary object will be chosen.
        '''
        # ensure trailing slash
        if post_uri[-1] != ""/"":
            post_uri += ""/""
        # get object
        qs = self.form._meta.model.objects
        if pk:
            obj = qs.get(pk=pk)
        else:
            obj = qs.first()
        # attempt to correct foreign key naming
        if field not in obj.__dict__ and field + ""_id"" in obj.__dict__:
            dict_field = field + ""_id""
        else:
            dict_field = field
        # generate data to post with
        post_data = {}
        for key in self.form._meta.fields:
            # non foreign key
            if key in obj.__dict__:
                value = obj.__dict__[key]
            # foreign key
            elif key + ""_id"" in obj.__dict__:
                value = obj.__dict__[key + ""_id""]
            # don't include nulls
            if (value is not None) or (value is None and key == field):
                post_data[key] = value
        self.assertTrue(post_data[field] != data,
                        f'Bad attempt to update ""{type(obj).__name__}"" with identical data on field ""{field}""')
        post_data[field] = data
        response = self.client.post(post_uri + str(obj.pk) + '/', post_data)
        obj.refresh_from_db()
        self.assertTrue(obj.__dict__[dict_field] == data,
                        f'POST error with object ""{type(obj).__name__}"" '
                        f'on field ""{field}"" ' 
                        f'at ""{post_uri}""')/n/n/ndashboard/tests/unit/test_extracted_text.py/n/nfrom django.test import TestCase
from django.utils import timezone
from django.contrib.auth.models import User
from django.core.exceptions import ValidationError, ObjectDoesNotExist


from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard.models import ExtractedText, QANotes


class ExtractedTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()

    def test_extracted_doc_date_validation(self):
        # check validation for proper length string
        text = ExtractedText(doc_date= 'Wednesday, January 21, 2014',
                                data_document=self.objects.doc,
                                extraction_script=self.objects.script)
        self.assertRaises(ValidationError, text.clean())
        # check validation not thrown for arbitrary date string less than 25 chars
        text = ExtractedText(doc_date= 'January 1984',
                             data_document=self.objects.doc,
                             extraction_script=self.objects.script)
        try:
            text.clean()
        except ValidationError:
            self.fail(""clean() raised ExceptionType unexpectedly!"")

        # check validation not thrown if doc_date is null
        text = ExtractedText(data_document=self.objects.doc,
                                extraction_script=self.objects.script)
        try:
            text.clean()
        except ValidationError:
            self.fail(""clean() raised ExceptionType unexpectedly!"")

    def test_long_qa_notes(self):
        self.objects.extext.qa_edited = True
        note = QANotes.objects.create(extracted_text=self.objects.extext)
        self.assertEqual(note.qa_notes, None)
        note.qa_notes = ""A short QA note""
        try:
            note.clean()
        except Exception as ex:
            template = ""An exception of type {0} occurred. Arguments:\n{1!r}""
            message = template.format(type(ex).__name__, ex.args)

        long_note = 'A long QA note' * 200
        note.qa_notes = long_note
        try:
            note.clean()
        except Exception as ex:
            template = ""An exception of type {0} occurred. Arguments:\n{1!r}""
            message = template.format(type(ex).__name__, ex.args)


class ExtractedTestWithSeedData(TestCase):

    fixtures = fixtures_standard

    def test_approvable(self):
        for et in ExtractedText.objects.filter(qa_edited=False):
            self.assertTrue(et.is_approvable(),'The record should be approvable')
            # Turn it into an edited record
            et.qa_edited = True
            et.save()

            self.assertFalse(et.is_approvable(),'Now the record should not be approvable')
            qa = QANotes.objects.create(extracted_text=et,qa_notes='Some notes have been added')

            self.assertTrue(et.is_approvable(),'The record should be approvable after adding a note')
            # Make the qa_notes field blank
            qa.qa_notes = ''
            qa.save()
            self.assertFalse(et.is_approvable(),
                'The record should not be approvable if the notes exist but are blank')



            
                
/n/n/ndashboard/tests/unit/test_habits_n_practices.py/n/nfrom django.urls import resolve
from django.test import TestCase

from dashboard import views
from dashboard.forms import create_detail_formset
from dashboard.tests.loader import load_model_objects



class HabitViewTest(TestCase):
    multi_db = True
    def setUp(self):
        self.objects = load_model_objects()


    def test_habitsandpractices(self):
        found = resolve(f'/habitsandpractices/{self.objects.doc.pk}/')
        self.assertEqual(found.func, views.habitsandpractices)

    def test_link_habitandpractice_to_puc(self):
        found = resolve(f'/link_habitandpractice_to_puc/{self.objects.ehp.pk}/')
        self.assertEqual(found.func, views.link_habitsandpractices)

    def test_product_surveyed_field(self):
        self.objects.gt.code = 'HP'
        self.objects.gt.save()
        _, HnPFormSet = create_detail_formset(self.objects.doc)
        data = {'habits-TOTAL_FORMS':'2',
                'habits-INITIAL_FORMS':'1',
                'habits-MIN_NUM_FORMS':'0',
                'habits-MAX_NUM_FORMS':'1000',
                'habits-0-id': self.objects.ehp.pk,
                'habits-0-product_surveyed':'',
        }
        hp_formset = HnPFormSet(data, prefix='habits')
        self.assertFalse(hp_formset.is_valid())

        data = {'habits-TOTAL_FORMS':'2',
                'habits-INITIAL_FORMS':'1',
                'habits-MIN_NUM_FORMS':'0',
                'habits-MAX_NUM_FORMS':'1000',
                'habits-0-id': self.objects.ehp.pk,
                'habits-0-product_surveyed':'monster trucks',
        }
        hp_formset = HnPFormSet(data, prefix='habits')

        self.assertTrue(hp_formset.is_valid())

    def test_edit_hnp_detail(self):
        self.objects.exscript.title = 'Manual (dummy)'
        self.objects.exscript.save()
        self.client.login(username='Karyn', password='specialP@55word')
        pk = self.objects.doc.pk
        response = self.client.get(f'/habitsandpractices/{pk}/')
        self.assertNotContains(response, 'Raw Category', html=True)

        # Ensure there are Cancel and Back buttons with the correct URL to return to the DG detail page
        self.assertContains(response, f'href=""/datagroup/{self.objects.dg.pk}/"" role=""button"">Cancel</a>')
        self.assertContains(response, f'href=""/datagroup/{self.objects.dg.pk}/"" role=""button"">Back</a>')

        # Ensure that the URL above responds correctly
        response2 = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertContains(response2, 'Data Group Detail: Data Group for Test')
/n/n/ndashboard/tests/unit/test_models.py/n/nimport csv
from django.utils import timezone
from django.test import TestCase
from django.db.models import Count
from dashboard.models import *
from dashboard.tests.loader import *
from django.db.models import Q


def create_data_documents(data_group, source_type, pdfs):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdfs,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


def create_data_documents_with_txt(data_group, source_type, pdf_txt):
    '''Used to imitate the creation of new DataDocuments from CSV'''
    dds = []
    with open('./sample_files/register_records_matching_with_txt.csv', 'r') as dg_csv:
        table = csv.DictReader(dg_csv)
        for line in table: # read every csv line, create docs for each
            if line['title'] == '': # updates title in line object
                line['title'] = line['filename'].split('.')[0]
            dd = DataDocument.objects.create(filename=line['filename'],
                                            title=line['title'],
                                            document_type=DocumentType.objects.get(
                                                Q(code='MS') & Q(group_type_id= data_group.group_type_id)
                                                ),
                                            url=line['url'],
                                            organization=line['organization'],
                                            matched = line['filename'] in pdf_txt,
                                            data_group=data_group)
            dd.save()
            dds.append(dd)
        return dds


class ModelsTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')
        self.pdfs = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0c68ab16-2065-4d9b-a8f2-e428eb192465.pdf']
        self.pdf_txt = ['0bf5755e-3a08-4024-9d2f-0ea155a9bd17.pdf',
                        '0bf5755e-3a08-4024-9d2f-0ea155a9bd17.txt']

    def test_object_creation(self):
        self.assertTrue(isinstance(self.objects.ds, DataSource))
        self.assertTrue(isinstance(self.objects.script, Script))
        self.assertTrue(isinstance(self.objects.extext, ExtractedText))
        self.assertTrue(isinstance(self.objects.ec, ExtractedChemical))
        self.assertTrue(isinstance(self.objects.ing, Ingredient))
        self.assertTrue(isinstance(self.objects.p, Product))
        self.assertTrue(isinstance(self.objects.pd, ProductDocument))
        self.assertTrue(isinstance(self.objects.pt, PUCTag))

    def test_datagroup(self):
        self.assertTrue(isinstance(self.objects.dg, DataGroup))

        self.assertEqual(str(self.objects.dg), self.objects.dg.name)
        self.assertEqual('https://www.epa.gov', self.objects.dg.url)

    def test_object_properties(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)
        self.assertTrue(hasattr(PUCToTag,'assumed'))
        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents(self.objects.dg,self.objects.st, self.pdfs)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdfs), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded pdf
        fn = docs[0].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))
        # DownloadScript
        self.assertEqual(str(self.objects.script), 'Test Download Script')
        # ExtractedText
        self.assertEqual(str(self.objects.extext),
                                    'test document')
        # RawChem
        self.assertEqual(str(self.objects.rc), 'Test Chem Name')
        # ExtractedChemical
        self.assertEqual(str(self.objects.ec), 'Test Chem Name')

    def test_product_attribute(self):
        self.assertEqual(ProductToTag.objects.count(), 0)
        p2t = ProductToTag.objects.create(content_object=self.objects.p,
                                            tag=self.objects.pt)
        self.assertEqual(ProductToTag.objects.count(), 1)

    def test_data_group(self):
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        self.assertFalse(self.objects.dg.all_matched())
        self.assertFalse(self.objects.dg.all_extracted())
        doc.matched = True
        doc.save()
        self.assertFalse(self.objects.dg.all_matched())
        self.objects.doc.matched = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_matched())
        doc.extracted = True
        doc.save()
        self.assertFalse(self.objects.dg.all_extracted())
        self.objects.doc.extracted = True
        self.objects.doc.save()
        self.assertTrue(self.objects.dg.all_extracted())

    def test_extracted_habits_and_practices(self):
        puc2 = PUC.objects.create(gen_cat='Test General Category',
                                 prod_fam='Test Product Family',
                                 prod_type='Test Product Type',
                                 description='Test Product Description',
                                 last_edited_by = self.objects.user)
        self.assertEqual(ExtractedHabitsAndPractices.objects.count(), 1)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 0)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=self.objects.puc)
        e2p = ExtractedHabitsAndPracticesToPUC.objects.create(extracted_habits_and_practices=self.objects.ehp,
                                                              PUC=puc2)
        self.assertEqual(ExtractedHabitsAndPracticesToPUC.objects.count(), 2)

    def test_data_document_organization(self):
        self.assertEqual(self.objects.doc.organization, '')
        self.objects.doc.organization = 'Test Organization'
        self.objects.doc.save()
        self.assertEqual(DataDocument.objects.filter(organization='Test Organization').count(), 1)

    def test_data_document_filename(self):
        pk = self.objects.doc.pk
        self.assertEqual(self.objects.doc.get_abstract_filename(),
                        f'document_{pk}.pdf',
                        'This is used in the FileSystem naming convention.')

    def test_dg_with_txt(self):
        # Test properties of objects
        # DataSource
        self.assertEqual(str(self.objects.ds), self.objects.ds.title)

        # DataDocuments
        # Confirm that one of the data documents appears in the data group
        # show page after upload from CSV
        docs = create_data_documents_with_txt(self.objects.dg,self.objects.st, self.pdf_txt)
        self.assertEqual(len(docs),2, ('Only 2 records should be created!'))
        dg_response = self.client.get(f'/datagroup/{str(self.objects.dg.pk)}/')
        self.assertIn(b'NUTRA', dg_response.content)
        self.assertEqual(len(self.pdf_txt), 2)
        # Confirm that the two data documents in the csv file are matches to
        # the pdfs via their file names
        self.assertEqual(self.objects.dg.matched_docs(), 2)
        # Test a link to an uploaded text file
        fn = docs[1].get_abstract_filename()
        u = ""{0}/pdf/{1}"".format(self.objects.dg.fs_id, fn).encode('utf-8')
        self.assertIn(u, dg_response.content, (
                                    'link to PDF should be in HTML!'))

    def test_script_fields(self):
        fields = ['title','url','qa_begun','script_type','confidence']
        for fld in fields:
            self.assertIn(fld, Script.__dict__, (f'{fld} '
                                                'should be in Script model.'))
        url = next(f for f in Script._meta.fields if f.name == 'url')
        self.assertTrue(url.max_length == 225, (""'url' field should be of ""
                                                                ""length 225""))

    def test_taxonomy_fields(self):
        fields = ['title','description','parent','source', 'category_code', 
                    'last_edited_by','created_at','updated_at']
        taxonomy_fields = [f.name for f in Taxonomy._meta.fields]   
        for fld in fields:
            self.assertIn(fld, taxonomy_fields, (f'{fld} '
                                                'should be in Taxonomy model.'))
        title = next(f for f in Taxonomy._meta.fields if f.name == 'title')
        self.assertTrue(title.max_length == 250, (""'title' field should have ""
                                                        ""max length of 250""))

class PUCModelTest(TestCase):

    fixtures = fixtures_standard

    def test_puc_fields(self):
        fields = ['kind','gen_cat','prod_fam','prod_type','description',
                'last_edited_by','products','extracted_habits_and_practices',
                'tags']
        for fld in fields:
            self.assertIn(fld, PUC.__dict__, f'{fld} should be in PUC model.')

    def test_get_the_kids(self):
        '''Level 1 and 2 PUCs should accumulate lower level PUCs.
        '''
        puc = PUC.objects.get(pk=20) # PUC w/ only gen_cat value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=6) # PUC w/ gen_cat and prod_fam value
        self.assertGreater(len(puc.get_the_kids()),1, ('PUC should have more'
                                                        'than one child PUCs'))
        puc = PUC.objects.get(pk=126) # PUC w/ ALL values
        self.assertEqual(len(puc.get_the_kids()),1, ('PUC should only have '
                                                        'itself associated'))

    def test_puc_category_defaults(self):
        '''Assert that the prod_fam and prod_type are nulled w/ an
        empty string and not NULL.
        '''
        k = User.objects.get(username='Karyn')
        puc = PUC.objects.create(last_edited_by=k)
        self.assertTrue(puc.prod_fam == '')
        self.assertTrue(puc.prod_type == '')

    def test_product_counts(self):
        '''Make sure the product_count property
        returns the same thing as the num_products annotation'''
        pucs = PUC.objects.all().annotate(num_products=Count('products'))
        # pucs 1-3 have products associated with them
        self.assertEqual(pucs.get(pk=1).num_products , PUC.objects.get(pk=1).product_count)


class DataGroupFilesTest(TestCase):

    fixtures = fixtures_standard

    def test_filefield_properties(self):
        dg5 = DataGroup.objects.get(pk=5) # this datagroup has no csv value
        dg6 = DataGroup.objects.get(pk=6) # this one has a csv value, but no file
        dg50 = DataGroup.objects.get(pk=50) # this one has a /media/ folder

        # All of the falsy properties should return False rather than errors
        self.assertFalse(dg5.dg_folder)
        self.assertFalse(dg5.zip_url)

        self.assertFalse(dg6.dg_folder)
        self.assertFalse(dg6.zip_url)

        # 50 is the only datagroup that has a linked file in the /media folder
        self.assertTrue(dg50.dg_folder == dg50.get_dg_folder())
        self.assertFalse(dg50.zip_url)


class DataDocumentTest(TestCase):

    fixtures = fixtures_standard

    def test_datadocument_note(self):

        datadocument = DataDocument(filename=""MyFile.pdf"",
                                    title=""My Title"",
                                    data_group=DataGroup.objects.first(),
                                    note=""Some long note."")
        datadocument.save()
        self.assertTrue(datadocument.note, ""Some long note."")






/n/n/ndashboard/urls.py/n/nfrom django.urls import include, path
from django.conf import settings
from django.conf.urls.static import static

import dashboard.views.qa
from . import views

urlpatterns = [
    path('', views.index,                   name='index'),
    path('datasources/', views.data_source_list,
                                            name='data_source_list'),
    path('datasource/<int:pk>', views.data_source_detail,
                                            name='data_source_detail'),
    path('datasource/new/', views.data_source_create,
                                            name='data_source_new'),
    path('datasource/edit/<int:pk>/', views.data_source_update,
                                            name='data_source_edit'),
    path('datasource/delete/<int:pk>/', views.data_source_delete,
                                            name='data_source_delete'),
    path('datagroups/', views.data_group_list,
                                            name='data_group_list'),
    path('datagroup/<int:pk>/', views.data_group_detail,
                                            name='data_group_detail'),
    path('datagroup/docs_csv/<int:pk>/', views.dg_dd_csv_view,
                                            name='dg_dd_csv_view'),
    path('datagroup/pdfs_zipped/<int:pk>/', views.dg_pdfs_zip_view,
                                            name='dg_pdfs_zip_view'),
    path('datagroup/raw_extracted_records/<int:pk>/', views.dg_raw_extracted_records,
                                            name='dg_raw_extracted_records'),
    path('datasource/<int:pk>/datagroup_new/', views.data_group_create,
                                            name='data_group_new'),
    path('datagroup/<int:pk>/registered_records.csv', views.data_group_registered_records_csv,
                                            name=""registered_records.csv""),
    path('datagroup/edit/<int:pk>/', views.data_group_update,
                                            name='data_group_edit'),
    path('datagroup/delete/<int:pk>/', views.data_group_delete,
                                            name='data_group_delete'),
    path('datadocument/delete/<int:pk>/', views.data_document_delete,
                                            name='data_document_delete'),
    path('datadocument/note/<int:pk>/', views.data_document_note,
                                            name='data_document_note'),
    path('product_curation/', views.product_curation_index,
                                            name='product_curation'),
    path('chemical_curation/', views.chemical_curation_index,
         name='chemical_curation'),
    path('category_assignment/<int:pk>/', views.category_assignment,
                                            name='category_assignment'),
    path('link_product_list/<int:pk>/', views.link_product_list,
                                            name='link_product_list'),
    path('link_product_form/<int:pk>/', views.link_product_form,
                                            name='link_product_form'),
    path('qa/extractionscript/', views.qa_extractionscript_index,
                                            name='qa_extractionscript_index'),
    path('qa/extractionscript/<int:pk>/', dashboard.views.qa.qa_extraction_script,
                                            name='qa_extraction_script'),
    path('qa/extractedtext/<int:pk>/', dashboard.views.qa.extracted_text_qa,
                                            name='extracted_text_qa'),
    path('extractionscript/<int:pk>/', views.extraction_script_detail,
                                            name='extraction_script_detail'),
    path('qa/chemicalpresence/', views.qa_chemicalpresence_index,
                                            name='qa_chemicalpresence_index'),
    path('qa/chemicalpresencegroup/<int:pk>/', views.qa_chemicalpresence_group,
                                            name='qa_chemical_presence_group'),
    path('bulk_product_puc/', views.bulk_assign_puc_to_product,
                                            name='bulk_product_puc'),
    path('bulk_product_tag/', views.bulk_assign_tag_to_products,
                                            name='bulk_product_tag'),
    path('product_puc/<int:pk>/', views.assign_puc_to_product,
                                            name='product_puc'),
    path('product_puc_delete/<int:pk>/', views.detach_puc_from_product,
                                            name='product_puc_delete'),
    path('puc-autocomplete/', views.puc_autocomplete.PUCAutocomplete.as_view(),
                                            name='puc-autocomplete'),
    path('product/<int:pk>/', views.product_detail,
                                            name='product_detail'),
    path('product/edit/<int:pk>/', views.product_update,
                                            name='product_edit'),
    path('product/delete/<int:pk>/', views.product_delete,
                                            name='product_delete'),
    path('products/', views.product_list,  name='product_list'),
    path('datadocument/<int:pk>/', views.data_document_detail,
                                            name='data_document'),
    path('save_type/<int:pk>/', views.save_doc_form,
                                            name='save_doc_form'),
    path('save_ext/<int:pk>/', views.save_ext_form,
                                            name='save_ext_form'),
    path('save_list_presence_tags/<int:pk>/', views.save_list_presence_tag_form,
                                            name='save_list_presence_tag_form'),
    path('search/', include('haystack.urls')),
    path('find/', views.search.FacetedSearchView.as_view(),
                                            name='haystack_search'),
    path('p_json/', views.product_ajax,     name='p_ajax_url'),
    path('pucs/', views.puc_list,           name='puc_list'),
    path('dl_pucs/', views.download_PUCs,   name='download_PUCs'),
    path('dl_raw_chems/', views.download_raw_chems,  
                                            name='download_raw_chems'),
    path('dsstox_lookup/<int:pk>/', views.dsstox_lookup_detail,
                                            name='dsstox_lookup'),
    path('habitsandpractices/<int:pk>/', views.habitsandpractices,
                                            name='habitsandpractices'),
    path('link_habitandpractice_to_puc/<int:pk>/', views.link_habitsandpractices,
                                            name='link_habitsandpractices'),
    path('get_data/', views.get_data,      name='get_data'),
    path('dl_chem_summary/', views.download_chem_stats,
                                            name='download_chem_stats'),
    path('upload/dtxsid_csv/', views.upload_dtxsid_csv,
                                            name='upload_dtxsid_csv'),
    path('get_data/get_dsstox_csv_template/', views.get_data_dsstox_csv_template,
                                            name='get_data_dsstox_csv_template'),
    path('datagroup/diagnostics/<int:pk>/',   views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('datagroup/diagnostics/',          views.data_group_diagnostics,
                                            name='data_group_diagnostics'),
    path('extractedtext/edit/<int:pk>/',   views.extracted_text_edit,
                                            name='extracted_text_edit'),
    path('extractedchild/edit/<int:pk>/',   views.extracted_child_edit,
                                            name='extracted_child_edit'),
    path('datadocument/edit/<int:pk>/',   views.data_document_edit,
                                            name='data_document_edit'),
    path('qanotes/save/<int:pk>/',   views.save_qa_notes,
                                            name='save_qa_notes'),
    path('extractedtext/approve/<int:pk>/',   views.approve_extracted_text,
                                            name='approve_extracted_text'),
]

if settings.DEBUG is True:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
/n/n/ndashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['General category','Product family','Product type','Allowed attributes','Assumed attributes','Description','PUC type','PUC level','Product count']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                # list(puc.get_allowed_tags()),
                '; '.join([str(allowedTag) for allowedTag in puc.puctotag_set.all()]),
                '; '.join([str(assumedTag) for assumedTag in puc.puctotag_set.filter(assumed=True)]),
                puc.description, 
                puc.kind,
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/ndashboard/views/data_document.py/n/nfrom django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404
from django.core.exceptions import ObjectDoesNotExist
from djqscsv import render_to_csv_response

from dashboard.forms import *
from dashboard.forms import ExtractedListPresenceTagForm
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if doc.detail_page_editable else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=bool(edit))
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if code == 'CP':
        # although keywords display as if at the datadocument level, they are
        # attached to each list_presence record. To display, we're getting the
        # tags associated with the first list_presence record, but on saving
        # (in save_list_presence_tag_form()) we loop over the whole set
        try:
            list_presence = doc.extractedtext.rawchem.select_subclasses('extractedlistpresence').first()
            list_presence_tag_form = ExtractedListPresenceTagForm(instance=list_presence)
            context.update({'list_presence_tag_form': list_presence_tag_form})
        except ObjectDoesNotExist:
            pass
    if doc.is_extracted:
        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk)
        child_formset = ChildFormSet(instance=extracted_text)
        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True
        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)

@login_required()
def save_list_presence_tag_form(request, pk):
    referer = request.POST.get('referer', 'data_document')
    extracted_text = get_object_or_404(ExtractedText, pk=pk)
    for extracted_list_presence in extracted_text.rawchem.select_subclasses('extractedlistpresence'):
        tag_form = ExtractedListPresenceTagForm(request.POST or None, instance=extracted_list_presence)
        if tag_form.is_valid():
            tag_form.save()
    return redirect(referer, pk=pk)

@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})

@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)

@login_required
def data_document_edit(request, pk, template_name=('data_document/'
                                                    'data_document_form.html')):
    datadocument = get_object_or_404(DataDocument, pk=pk)
    form = DataDocumentForm(request.POST or None, instance=datadocument)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_document', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/ndashboard/views/data_group.py/n/nimport csv
import zipfile
from djqscsv import render_to_csv_response
from pathlib import Path

from django.conf import settings
from django.core.files import File
from django.core.exceptions import ValidationError
from django.core.files.storage import FileSystemStorage
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404
from django.http import HttpResponse
from django.core.paginator import Paginator

from dashboard.models import *
from dashboard.forms import (DataGroupForm,
                                   ExtractionScriptForm,
                                   CleanCompDataForm,
                                   create_detail_formset,
                                   include_extract_form,
                                   include_clean_comp_data_form)
from dashboard.utils import get_extracted_models, clean_dict, update_fields
from django.db.models import Max


@login_required()
def data_group_list(request, template_name='data_group/datagroup_list.html'):
    datagroup = DataGroup.objects.all()
    data = {}
    data['object_list'] = datagroup
    return render(request, template_name, data)

@login_required()
def data_group_detail(request, pk,
                      template_name='data_group/datagroup_detail.html'):
    dg = get_object_or_404(DataGroup, pk=pk, )
    dg.doc_types = DocumentType.objects.filter(group_type=dg.group_type)
    docs = dg.datadocument_set.get_queryset()#this needs to be updated after matching...
    prod_link = ProductDocument.objects.filter(document__in=docs)
    page = request.GET.get('page')
    paginator = Paginator(docs, 50) # TODO: make this dynamic someday in its own ticket
    store = settings.MEDIA_URL + str(dg.fs_id)
    ext = ExtractedText.objects.filter(data_document_id__in=docs).first()
    context = { 'datagroup'      : dg,
                'documents'      : paginator.page(1 if page is None else page),
                'all_documents'  : docs, # this used for template download
                'extract_fields' : dg.get_extracted_template_fieldnames(),
                'ext_err'        : {},
                'clean_comp_err'        : {},
                'extract_form'   : include_extract_form(dg),
                'clean_comp_data_form'   : include_clean_comp_data_form(dg),
                'bulk'           : len(docs) - len(prod_link),
                'msg'            : '',
                }
    if request.method == 'POST' and 'upload' in request.POST:
        # match filename to pdf name
        matched_files = [f for d in docs for f
                in request.FILES.getlist('multifiles') if f.name == d.filename]
        if not matched_files:
            context['msg'] = ('There are no matching records in the '
                                                        'selected directory.')
            return render(request, template_name, context)
        zf = zipfile.ZipFile(dg.zip_file, 'a', zipfile.ZIP_DEFLATED)
        while matched_files:
            f = matched_files.pop(0)
            doc = DataDocument.objects.get(filename=f.name,
                                            data_group=dg.pk)
            if doc.matched:
                continue
            doc.matched = True
            doc.save()
            fs = FileSystemStorage(store + '/pdf')
            afn = doc.get_abstract_filename()
            fs.save(afn, f)
            zf.write(store + '/pdf/' + afn, afn)
        zf.close()
        form = include_extract_form(dg)
        # update docs so it appears in the template table w/ ""matched"" docs
        context['all_documents'] = dg.datadocument_set.get_queryset()
        context['extract_form'] = form
        context['msg'] = 'Matching records uploaded successfully.'
    if request.method == 'POST' and 'extract_button' in request.POST:
        extract_form = ExtractionScriptForm(request.POST,
                                                request.FILES,dg_type=dg.type)
        if extract_form.is_valid():
            csv_file = request.FILES.get('extract_file')
            script_pk = int(request.POST['script_selection'])
            script = Script.objects.get(pk=script_pk)
            info = [x.decode('ascii','ignore') for x in csv_file.readlines()]
            table = csv.DictReader(info)
            missing =  list(set(dg.get_extracted_template_fieldnames())-
                                                        set(table.fieldnames))
            if missing: #column names are NOT a match, send back to user
                context['msg'] = ('The following columns need to be added or '
                                            f'renamed in the csv: {missing}')
                return render(request, template_name, context)
            good_records = []
            ext_parent, ext_child = get_extracted_models(dg.type)
            for i, row in enumerate(csv.DictReader(info)):
                d = docs.get(pk=int(row['data_document_id']))
                d.raw_category = row.pop('raw_category')
                wft = request.POST.get('weight_fraction_type', None)
                if wft: # this signifies 'Composition' type
                    w = 'weight_fraction_type'
                    row[w] = WeightFractionType.objects.get(pk=int(wft))
                    unit_type_id = int(row['unit_type'])
                    row['unit_type'] = UnitType.objects.get(pk=unit_type_id)
                    rank = row['ingredient_rank']
                    row['ingredient_rank'] = None if rank == '' else rank
                ext, created = ext_parent.objects.get_or_create(data_document=d,
                                                    extraction_script=script)
                if not created and ext.one_to_one_check(row):
                    # check that there is a 1:1 relation ExtParent and DataDoc
                    col = 'cat_code' if hasattr(ext,'cat_code') else 'prod_name' 
                    err_msg = ['must be 1:1 with ""data_document_id"".']
                    context['ext_err'][i+1] = {col: err_msg}
                if created:
                    update_fields(row, ext)
                row['extracted_text'] = ext
                if (ext_child == ExtractedListPresence):
                    row['extracted_cpcat'] = ext.extractedtext_ptr
                row = clean_dict(row, ext_child)
                try:
                    ext.full_clean()
                    ext.save()
                    record = ext_child(**row)
                    record.full_clean()
                    good_records.append((d,ext,record))
                except ValidationError as e:
                    context['ext_err'][i+1] = e.message_dict
            if context['ext_err']: # if errors, send back with errors
                [e[1].delete() for e in good_records] # delete any created exts
                return render(request, template_name, context)
            if not context['ext_err']:  # no saving until all errors are removed
                for doc,text,record in good_records:
                    doc.extracted = True
                    doc.save()
                    text.save()
                    record.save()
                fs = FileSystemStorage(store)
                fs.save(str(dg)+'_extracted.csv', csv_file)
                context['msg'] = (f'{len(good_records)} extracted records '
                                                    'uploaded successfully.')
                context['extract_form'] = include_extract_form(dg)
    if request.method == 'POST' and 'bulk' in request.POST:
        # get the set of documents that have not been matched
        a = set(docs.values_list('pk',flat=True))
        b = set(prod_link.values_list('document_id',flat=True))
        # DataDocs to make products for...
        docs_needing_products = DataDocument.objects.filter(pk__in=list(a-b))
        stub = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        for doc in docs_needing_products:
            # Try to name the new product from the ExtractedText record's prod_name
            try:
                ext = ExtractedText.objects.get(data_document_id=doc.id)
                if ext:
                    if ext.prod_name:
                        new_prod_title = ext.prod_name
                    else:
                        new_prod_title = None
            except ExtractedText.DoesNotExist:
                new_prod_title = None
            # If the ExtractedText record can't provide a title, use the DataDocument's title
            if not new_prod_title:
                if doc.title:
                    new_prod_title = '%s stub' % doc.title
                else:
                    new_prod_title = 'unknown'
            product = Product.objects.create(
                                    title=new_prod_title,
                                    upc=f'stub_{stub}',
                                    data_source_id=doc.data_group.data_source_id
                                    )
            ProductDocument.objects.create(product=product, document=doc)
            stub += 1
        context['bulk'] = 0
    if request.method == 'POST' and 'clean_comp_data_button' in request.POST:
        clean_comp_data_form = CleanCompDataForm(request.POST, request.FILES)
        if clean_comp_data_form.is_valid():
            script_pk = int(request.POST['script_selection'])
            script = Script.objects.get(pk=script_pk)
            csv_file = request.FILES.get('clean_comp_data_file')
            info = [x.decode('ascii','ignore') for x in csv_file.readlines()]
            table = csv.DictReader(info)
            missing =  list(set(dg.get_clean_comp_data_fieldnames())-
                                                        set(table.fieldnames))
            if missing: #column names are NOT a match, send back to user
                context['clean_comp_data_form'].collapsed = False
                context['msg'] = ('The following columns need to be added or '
                                            f'renamed in the csv: {missing}')
                return render(request, template_name, context)

            good_records = []
            for i, row in enumerate(csv.DictReader(info)):
                try:
                    extracted_chemical = ExtractedChemical.objects.get(rawchem_ptr=int(row['id']))
                except ExtractedChemical.DoesNotExist as e:
                    extracted_chemical = None
                    context['clean_comp_err'][i + 1] = {'id': ['No ExtractedChemical matches rawchem_ptr_id ' + row['id'], ]}
                    print('No ExtractedChemical matches rawchem_ptr_id %s' % row['id'])
                try:
                    ingredient = Ingredient.objects.get(rawchem_ptr=extracted_chemical.rawchem_ptr)
                except Ingredient.DoesNotExist as e:
                    ingredient = Ingredient(rawchem_ptr=extracted_chemical.rawchem_ptr)
                ingredient.lower_wf_analysis = row['lower_wf_analysis']
                ingredient.central_wf_analysis = row['central_wf_analysis']
                ingredient.upper_wf_analysis = row['upper_wf_analysis']
                ingredient.script = script
                try:
                    ingredient.full_clean()
                except ValidationError as e:
                    context['clean_comp_err'][i+1] = e.message_dict
                good_records.append(ingredient)
            if context['clean_comp_err']: # if errors, send back with errors
                context['clean_comp_data_form'].collapsed = False
                return render(request, template_name, context)
            if not context['clean_comp_err']:  # no saving until all errors are removed
                for ingredient in good_records:
                    ingredient.save()
                context['msg'] = (f'{len(good_records)} clean composition data records '
                                                    'uploaded successfully.')
                context['clean_comp_data_form'] = include_clean_comp_data_form(dg)
        else:
            context['clean_comp_data_form'].collapsed = False

    return render(request, template_name, context)


@login_required()
def data_group_create(request, pk,
                        template_name='data_group/datagroup_form.html'):
    datasource = get_object_or_404(DataSource, pk=pk)
    group_key = DataGroup.objects.filter(data_source=datasource).count() + 1
    default_name = '{} {}'.format(datasource.title, group_key)
    header = 'Create New Data Group For Data Source ""' + str(datasource) + '""'
    initial_values = {'downloaded_by' : request.user,
                      'name'          : default_name,
                      'data_source'   : datasource}
    if request.method == 'POST':
        form = DataGroupForm(request.POST, request.FILES,
                             user    = request.user,
                             initial = initial_values)
        if form.is_valid():
            # what's the pk of the newly created datagroup?
            datagroup = form.save()
            info = [x.decode('ascii',
                             'ignore') for x in datagroup.csv.readlines()]
            table = csv.DictReader(info)
            good_fields = ['filename','title','document_type',
                                                    'url','organization']
            if not table.fieldnames == good_fields:
                datagroup.csv.close()
                datagroup.delete()
                return render(request, template_name,
                              {'field_error': table.fieldnames,
                              'good_fields': good_fields,
                               'form': form})
            text = ['DataDocument_id,' + ','.join(table.fieldnames)+'\n']
            errors = []
            filenames = []
            count = 0
            for line in table: # read every csv line, create docs for each
                count+=1
                doc_type = DocumentType.objects.get(pk=1)
                code = line['document_type']
                if line['filename'] == '' :
                    errors.append([count,""Filename can't be empty!""])
                    continue
                if len(line['filename'])>255:
                    errors.append([count,""Filename too long!""])
                    continue
                if line['filename'] in filenames:
                    errors.append([count, ""Duplicate filename found in csv""])
                    continue
                if line['title'] == '': # updates title in line object
                    line['title'] = line['filename'].split('.')[0]
                if code == '':
                    errors.append([count,
                                    ""'document_type' field can't be empty""])
                    continue
                if DocumentType.objects.filter(group_type=datagroup.group_type,
                                                            code=code).exists():
                    doc_type = DocumentType.objects.get(
                                    group_type=datagroup.group_type,code=code)
                else:
                    errors.append([count,""DocumentType code doesn't exist.""])

                filenames.append(line['filename'])
                doc=DataDocument(filename=line['filename'],
                                 title=line['title'],
                                 document_type=doc_type,
                                 url=line['url'],
                                 organization=line['organization'],
                                 data_group=datagroup)
                doc.save()
                # update line to hold the pk for writeout
                text.append(str(doc.pk)+','+ ','.join(line.values())+'\n')
            if errors:
                datagroup.csv.close()
                datagroup.delete()
                return render(request, template_name, {'line_errors': errors,
                                                       'form': form})
            #Save the DG to make sure the pk exists
            datagroup.save()
            #Let's even write the csv first
            with open(datagroup.csv.path,'w') as f:
                myfile = File(f)
                myfile.write(''.join(text))
            #Let's explicitly use the full path for the actually writing of the zipfile
            new_zip_name = Path(settings.MEDIA_URL + ""/"" + str(datagroup.fs_id) + ""/"" + str(datagroup.fs_id) + "".zip"")
            new_zip_path = Path(settings.MEDIA_ROOT + ""/"" + str(datagroup.fs_id) + ""/"" + str(datagroup.fs_id) + "".zip"")
            zf = zipfile.ZipFile(str(new_zip_path), 'w',
                                 zipfile.ZIP_DEFLATED)
            datagroup.zip_file = new_zip_name
            zf.close()
            datagroup.save()
            return redirect('data_group_detail', pk=datagroup.id)
    else:
        groups = GroupType.objects.all()
        for group in groups:
            group.codes = DocumentType.objects.filter(group_type=group)
        form = DataGroupForm(user=request.user, initial=initial_values)
    context = {'form': form, 'header': header,
                'datasource': datasource, 'groups' : groups}
    return render(request, template_name, context)


@login_required()
def data_group_update(request, pk, template_name='data_group/datagroup_form.html'):
    # TODO: Resolve whether this form save ought to also update Datadocuments
    #  in the case the ""Register Records CSV file"" is updated.
    datagroup = get_object_or_404(DataGroup, pk=pk)
    form = DataGroupForm(request.POST or None, instance=datagroup)
    header = f'Update Data Group for Data Source ""{datagroup.data_source}""'
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_group_detail', pk=datagroup.id)
    form.referer = request.META.get('HTTP_REFERER', None)
    if datagroup.extracted_docs():
        form.fields['group_type'].disabled = True
    groups = GroupType.objects.all()
    for group in groups:
            group.codes = DocumentType.objects.filter(group_type=group)
    return render(request, template_name, {'datagroup': datagroup, 
                                            'form': form,
                                            'header': header,
                                            'groups': groups})

@login_required()
def data_group_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    datagroup = get_object_or_404(DataGroup, pk=pk)
    if request.method == 'POST':
        datagroup.delete()
        return redirect('data_group_list')
    return render(request, template_name, {'object': datagroup})

@login_required
def dg_pdfs_zip_view(request, pk):
    dg = DataGroup.objects.get(pk=pk)
    #print('opening zip file from %s' % dg.get_zip_url())
    zip_file_name = f'{dg.fs_id}.zip'
    zip_file = open(dg.get_zip_url(), 'rb')
    response = HttpResponse(zip_file, content_type='application/zip')
    response['Content-Disposition'] = 'attachment; filename=%s' % zip_file_name
    return response

@login_required
def data_group_registered_records_csv(request, pk):
    columnlist = ['filename','title','document_type','url','organization']
    dg = DataGroup.objects.filter(pk=pk).first()
    if dg:
        columnlist.insert(0, ""id"")
        qs = DataDocument.objects.filter(data_group_id=pk).values(*columnlist)
        return render_to_csv_response(qs, filename=(dg.get_name_as_slug() +
                                                    ""_registered_records.csv""),
                                  field_header_map={""id"": ""DataDocument_id""},
                                  use_verbose_names=False)
    else:
        qs = DataDocument.objects.filter(data_group_id=0).values(*columnlist)
        return render_to_csv_response(qs, filename=""registered_records.csv"",
                                        use_verbose_names=False)

@login_required()
def habitsandpractices(request, pk,
                      template_name='data_group/habitsandpractices.html'):
    doc = get_object_or_404(DataDocument, pk=pk, )
    script = Script.objects.get(title='Manual (dummy)', script_type='EX')
    extext, created = ExtractedText.objects.get_or_create(data_document=doc,
                                                    extraction_script=script)
    if created:
        extext.doc_date = 'please add...'
    ExtractedTextForm, HPFormSet = create_detail_formset(doc)
    # print(extext.pk)
    ext_form = ExtractedTextForm(request.POST or None, instance=extext)
    hp_formset = HPFormSet(request.POST or None, instance=extext, prefix='habits')
    context = {   'doc'         : doc,
                  'ext_form'    : ext_form,
                  'hp_formset'  : hp_formset,
                  }
    if request.method == 'POST' and 'save' in request.POST:
        if hp_formset.is_valid():
            hp_formset.save()
        if ext_form.is_valid():
            ext_form.save()
        doc.extracted = True
        doc.save()
        context = {   'doc'         : doc,
                      'ext_form'    : ext_form,
                      'hp_formset'  : hp_formset,
                      }
    return render(request, template_name, context)

@login_required
def dg_raw_extracted_records(request, pk):
    columnlist = ['extracted_text_id','id','raw_cas','raw_chem_name','raw_min_comp','raw_central_comp','raw_max_comp','unit_type__title']
    dg = DataGroup.objects.get(pk=pk)
    et = ExtractedText.objects.filter(data_document__data_group = dg).first()
    if et:
        dg_name = dg.get_name_as_slug()
        qs = ExtractedChemical.objects.filter(extracted_text__data_document__data_group_id=pk).values(*columnlist)
        #print('Writing %s records to csv' % len(qs) )
        return render_to_csv_response(qs, filename=(dg_name +
                                                    ""_raw_extracted_records.csv""),
                                  field_header_map={""id"": ""ExtractedChemical_id""},
                                  use_verbose_names=False)
    else:
        qs = ExtractedChemical.objects.filter(extracted_text__data_document__id=pk).values(*columnlist)
        return render_to_csv_response(qs, filename='raw_extracted_records.csv' ,
                                        use_verbose_names=False)
/n/n/ndashboard/views/data_source.py/n/nfrom django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from dashboard.forms import DataSourceForm, PriorityForm
from dashboard.models import DataSource, DataGroup, DataDocument
from django.db.models import Count, Q



@login_required()
def data_source_list(request, template_name='data_source/datasource_list.html'):
    datasources = DataSource.objects.all()
    ds_list, frm_list = [], []
    for ds in datasources:
        frm_list.append(PriorityForm(request.POST or None, instance=ds))
    registered = Count('datagroup__datadocument') 
    uploaded   = Count('datagroup__datadocument', filter=Q(datagroup__datadocument__matched=True))
    extracted  = Count('datagroup__datadocument__extractedtext')
    ds_list    = DataSource.objects.annotate(registered=registered).annotate(uploaded=uploaded, extracted=extracted)
    out = zip(ds_list, frm_list)
    if request.method == 'POST':
        datasource = DataSource.objects.get(pk=request.POST['ds_pk'])
        form = PriorityForm(request.POST or None, instance=datasource)
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
            return redirect('data_source_list')
    return render(request, template_name, {'object_list': out})


@login_required()
def data_source_detail(request, pk,
                        template_name='data_source/datasource_detail.html'):
    datasource = get_object_or_404(DataSource, pk=pk, )
    docs = DataDocument.objects.filter(data_group__in=DataGroup.objects.filter(data_source=datasource))
    datasource.registered = (len(docs)/float(datasource.estimated_records))*100
    datasource.uploaded = (len(docs.filter(matched=True))/float(
                                            datasource.estimated_records))*100

    form = PriorityForm(request.POST or None, instance=datasource)
    if request.method == 'POST':
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
    datagroup_list = DataGroup.objects.filter(data_source=pk)
    context =     {'object':             datasource,
                'datagroup_list':    datagroup_list,
                'form':             form}
    return render(request, template_name, context)


@login_required()
def data_source_create(request, template_name=('data_source/'
                                                'datasource_form.html')):
    form = DataSourceForm(request.POST or None)
    if form.is_valid():
        form.save()
        return redirect('data_source_list')
    return render(request, template_name, {'form': form})


@login_required()
def data_source_update(request, pk, template_name=('data_source/'
                                                    'datasource_form.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    form = DataSourceForm(request.POST or None, instance=datasource)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_source_detail', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})

@login_required()
def data_source_delete(request, pk,
                        template_name=('data_source/'
                                        'datasource_confirm_delete.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    if request.method == 'POST':
        datasource.delete()
        return redirect('data_source_list')
    return render(request, template_name, {'object': datasource})
/n/n/ndashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, F

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp


/n/n/ndashboard/views/habits_n_practices.py/n/nfrom django.shortcuts import (render, get_object_or_404,
                              HttpResponseRedirect)
from django.contrib.auth.decorators import login_required

from dashboard.models import *
from dashboard.forms import HabitsPUCForm, create_detail_formset


@login_required()
def habitsandpractices(request, pk,
                      template_name='data_group/habitsandpractices.html'):
    doc = get_object_or_404(DataDocument, pk=pk, )
    script = Script.objects.get(title='Manual (dummy)', script_type='EX')
    extext, created = ExtractedText.objects.get_or_create(data_document=doc,
                                                    extraction_script=script)
    if created:
        extext.doc_date = 'please add...'
    ExtractedTextForm, HnPFormSet = create_detail_formset(doc)
    ext_form = ExtractedTextForm(request.POST or None, instance=extext)
    hp_formset = HnPFormSet(request.POST or None,
                            instance=extext, prefix='habits')
    if request.method == 'POST' and 'save' in request.POST:
        if hp_formset.is_valid() and ext_form.is_valid():
            if not doc.extracted:
                doc.extracted = True
                doc.save()
            hp_formset.save()
            ext_form.save()
            return HttpResponseRedirect(f'/habitsandpractices/{doc.pk}')
    context = {   'doc'         : doc,
                  'ext_form'    : ext_form,
                  'hp_formset'  : hp_formset,
                  }
    return render(request, template_name, context)


@login_required()
def link_habitsandpractices(request, pk,
                        template_name='data_group/habitsandpractices_to_puc.html'):
    hnp = get_object_or_404(ExtractedHabitsAndPractices, pk=pk, )
    form = HabitsPUCForm()
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(id=form['puc'].value())
            # make sure the PUC link doesn't already exist
            if not ExtractedHabitsAndPracticesToPUC.objects.filter(
                    PUC=puc,
                    extracted_habits_and_practices=hnp).exists():
                ExtractedHabitsAndPracticesToPUC.objects.create(
                        PUC=puc,
                        extracted_habits_and_practices=hnp
                )
                form = HabitsPUCForm()
    linked = ExtractedHabitsAndPracticesToPUC.objects.filter(
                    extracted_habits_and_practices=hnp).values('PUC')
    hnp_puc = PUC.objects.filter(pk__in=linked)
    print(hnp_puc)
    context = {'hnp': hnp,
                'form': form,
                'hnp_puc': hnp_puc,
    }
    return render(request, template_name, context)
/n/n/ndashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, ProductTagForm,
                                   BulkProductPUCForm, BulkProductTagForm,
                                   BulkPUCForm, ProductForm)
from django.core.paginator import Paginator
from django.db.models import Max



@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs
    return render(request, template_name, {'data_sources': data_sources})


@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})


@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})


@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})


@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)


@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})


@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter(Q(title__icontains=q) | Q(brand_name__icontains=q)).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})


@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })


@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')


@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/ndashboard/views/qa.py/n/nfrom django.shortcuts import render, redirect, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.contrib import messages
from django.db.models import Count, Q
from django.http import HttpResponseRedirect
from django.core.exceptions import ValidationError, MultipleObjectsReturned, ObjectDoesNotExist
from django.urls import reverse
from django.utils import timezone
import json

from dashboard.forms import create_detail_formset, QANotesForm, DocumentTypeForm
from dashboard.models import Script, DataGroup, DataDocument,\
    ExtractedCPCat, ExtractedText, ExtractedListPresence,\
    QAGroup, QANotes, DocumentType
from factotum.settings import EXTRA
from django import forms
from django.utils.http import is_safe_url


@login_required()
def qa_extractionscript_index(request, template_name='qa/extraction_script_index.html'):
    datadocument_count = Count('extractedtext__extraction_script')
    qa_complete_extractedtext_count = Count('extractedtext', filter=Q(extractedtext__qa_checked=True))
    extraction_scripts = Script.objects.\
        annotate(datadocument_count=datadocument_count).\
        annotate(qa_complete_extractedtext_count=qa_complete_extractedtext_count).\
        filter(script_type='EX')

    return render(request, template_name, {'extraction_scripts': extraction_scripts})

@login_required()
def qa_chemicalpresence_index(request, template_name='qa/chemical_presence_index.html'):
    datagroups = DataGroup.objects.filter(group_type__code='CP').\
        annotate(datadocument_count=Count('datadocument'))

    return render(request, template_name, {'datagroups': datagroups})

@login_required()
def qa_chemicalpresence_group(request, pk, template_name='qa/chemical_presence.html'):
    datagroup = DataGroup.objects.get(pk=pk)
    if datagroup.group_type.code != 'CP':
        raise ValidationError('This DataGroup is not of a ChemicalPresence type')
    extractedcpcats = ExtractedCPCat.objects.filter(data_document__data_group=datagroup)
    return render(request, template_name, {'datagroup':datagroup, 'extractedcpcats':extractedcpcats})

def prep_cp_for_qa(extractedcpcat):
    '''
    Given an ExtractedCPCat object, select a sample of its ExtractedListPresence children
    for QA review.
    '''
    from random import shuffle
    QA_RECORDS_PER_DOCUMENT = 30

    if extractedcpcat.rawchem.count() > 0:
        list_presence_count = extractedcpcat.rawchem.count()
    else:
        return
    elps = extractedcpcat.rawchem.select_subclasses()
    non_qa_list_presence_ids = list(elps.filter(extractedlistpresence__qa_flag=False).values_list('pk',flat=True))

    # total number of qa-flagged listpresence objects
    list_presence_qa_count = elps.filter(extractedlistpresence__qa_flag=True).count()

    # if less than 30 records (or all records in set) flagged for QA, make up the difference
    if list_presence_qa_count < QA_RECORDS_PER_DOCUMENT and list_presence_qa_count < list_presence_count:
        random_x = QA_RECORDS_PER_DOCUMENT - list_presence_qa_count
        shuffle(non_qa_list_presence_ids)
        list_presence = ExtractedListPresence.objects.filter(pk__in=non_qa_list_presence_ids[:random_x])
        for lp in list_presence:
            lp.qa_flag = True
            lp.save()
    return

 


@login_required()
def qa_extraction_script(request, pk,
                         template_name='qa/extraction_script.html'):
    """"""
    The user reviews the extracted text and checks whether it was properly converted to data
    """"""
    es = get_object_or_404(Script, pk=pk)
    # If the Script has no related ExtractedText objects, redirect back to the QA index
    if ExtractedText.objects.filter(extraction_script = es).count() == 0 :
        return redirect('/qa/extractionscript/')
    # Check whether QA has begun for the script
    if es.qa_group.count() > 0:
        # if the QA process has begun, there will already be one QA Group
        # associated with the Script.
        try:
            # get the QA Group
            qa_group = QAGroup.objects.get(extraction_script=es,
                                           qa_complete=False)
        except MultipleObjectsReturned:
            qa_group = QAGroup.objects.filter(extraction_script=es,
                                              qa_complete=False).first()
        except ObjectDoesNotExist:
            print('No QA Group was found matching Extraction Script %s' % es.pk)

        texts = ExtractedText.objects.filter(qa_group=qa_group,
                                             qa_checked=False)
        return render(request, template_name, {'extractionscript': es,
                                               'extractedtexts': texts,
                                               'qagroup': qa_group})
    else:
        qa_group = es.create_qa_group()
        es.qa_begun = True
        es.save()
    # Collect all the ExtractedText objects in the QA Group
    texts = ExtractedText.objects.filter(qa_group=qa_group)

    return render(request, template_name, {'extractionscript': es,
                                           'extractedtexts': texts,
                                           'qagroup': qa_group})


def hide_dsstox_fields(formset):
    # Hide the curated DSSToxLookup fields in the formset if they appear
    for form in formset:
        for dssfield in ['true_cas','true_chemname','SID']:
            if dssfield in form.fields:
                form.fields[dssfield].widget = forms.HiddenInput()


@login_required()
def extracted_text_qa(request, pk,
                      template_name='qa/extracted_text_qa.html', nextid=0):
    """"""
    Detailed view of an ExtractedText object, where the user can approve the
    record, edit its ExtractedChemical objects, skip to the next ExtractedText
    in the QA group, or exit to the index page.
    This view processes objects of different models with different QA workflows. 
    The qa_focus variable is used to indicate whether an ExtractedText object is
    part of a QA Group, as with Composition records, or if the DataDocument/ExtractedText
    is its own QA Group, as with ExtractedCPCat and ExtractedHHDoc records.  
    """"""
    extext = get_object_or_404(
	        ExtractedText.objects.select_subclasses(), pk=pk)
    
    doc = DataDocument.objects.get(pk=pk)
    exscript = extext.extraction_script
    group_type_code = extext.data_document.data_group.group_type.code

    if group_type_code in ['CP','HH']:
        qa_focus = 'doc'
        #
        # Document-focused QA process
        #
        # If the object is an ExtractedCPCat record, there will be no Script
        # associated with it and no QA Group
        prep_cp_for_qa(extext)

        stats = ''
        qa_home_page = f'qa/chemicalpresencegroup/%s/' % extext.data_document.data_group.id
    else:
        qa_focus = 'script'
        #
        # Extraction Script-focused QA process
        #
        # when not coming from extraction_script page, the document's script might not have 
        # a QA Group yet. 
        if not extext.qa_group:
            # create the qa group with the optional ExtractedText pk argument
            # so that the ExtractedText gets added to the QA group even if the
            # group uses a random sample
            qa_group = exscript.create_qa_group(pk)
            exscript.qa_begun = True
            exscript.save()
            extext.qa_group = qa_group
            extext.save()
        # get the next unapproved Extracted Text object
        # Its ID will populate the URL for the ""Skip"" button
        if extext.qa_checked:  # if ExtractedText object's QA process done, use 0
            nextid = 0
        else:
            nextid = extext.next_extracted_text_in_qa_group()
        # derive number of approved records and remaining unapproved in QA Group
        a = extext.qa_group.get_approved_doc_count()
        r = ExtractedText.objects.filter(qa_group=extext.qa_group).count() - a
        stats = '%s document(s) approved, %s documents remaining' % (a, r)

    referer = 'data_document' if 'datadocument' in request.path else 'qa_extraction_script'

    # Create the formset factory for the extracted records
    # The model used for the formset depends on whether the
    # extracted text object matches a data document()
    # The QA view should exclude the weight_fraction_type field.
    ParentForm, ChildForm = create_detail_formset(
        doc, EXTRA, can_delete=True, 
        exclude=['weight_fraction_type', 'true_cas', 'true_chemname', 'sid'])
    # extext = extext.pull_out_cp()
    ext_form = ParentForm(instance=extext)
    detail_formset = ChildForm(instance=extext)
    
    # If the document is CPCat or HHE type, the display should only show the
    # child records where qa_flag = True
    if qa_focus == 'doc' and hasattr(detail_formset.get_queryset().model, 'qa_flag'):
        qs = detail_formset.get_queryset().filter(qa_flag=True)
        detail_formset._queryset = qs
    
    # This code is being repeated in the GET and POST blocks
    # 
    # Hide all the DSSToxLookup fields 
    hide_dsstox_fields(detail_formset)

    # Add CSS selector classes to each form
    for form in detail_formset:
        for field in form.fields:
            form.fields[field].widget.attrs.update(
                {'class': f'detail-control form-control %s' % doc.data_group.type}
            )

    note, created = QANotes.objects.get_or_create(extracted_text=extext)
    notesform = QANotesForm(instance=note)

    # Allow the user to edit the data document type
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    # the form class overrides the label, so over-override it
    document_type_form.fields['document_type'].label = 'Data document type:'

    context = {
        'extracted_text': extext,
        'doc': doc,
        'script': exscript,
        'stats': stats,
        'nextid': nextid,
        'detail_formset': detail_formset,
        'notesform': notesform,
        'ext_form': ext_form,
        'referer': referer,
        'document_type_form': document_type_form
    }

    if request.method == 'POST' and 'save' in request.POST:
        # The save action only applies to the child records and QA properties,
        # no need to save the ExtractedText form
        ParentForm, ChildForm = create_detail_formset(
            doc, EXTRA, can_delete=True, exclude=['weight_fraction_type'])
        # extext = extext.pull_out_cp()
        detail_formset = ChildForm(request.POST, instance=extext)

        if detail_formset.has_changed():
            if detail_formset.is_valid() :
                detail_formset.save()
                extext.qa_edited = True
                extext.save()
                # rebuild the formset after saving it
                detail_formset = ChildForm(instance=extext)
            else:
                pass
                # print(detail_formset.errors)
                # TODO: iterate through this dict of errors and map each error to
                # the corresponding form in the template for rendering

            context['detail_formset'] = detail_formset
            context['ext_form'] = ext_form

        # This code is being repeated in the GET and POST blocks
        # 
        # Hide all the DSSToxLookup fields 
        hide_dsstox_fields(detail_formset)

        # Add CSS selector classes to each form
        for form in detail_formset:
            for field in form.fields:
                form.fields[field].widget.attrs.update(
                    {'class': f'detail-control form-control %s' % doc.data_group.type}
                )

    return render(request, template_name, context)


@login_required()
def save_qa_notes(request, pk):
    '''
    This is an endpoint that serves the AJAX call
    '''

    if request.method == 'POST':
        #print(""Saving {request.POST.get('qa_note_text')} to the object"")
        qa_note_text = request.POST.get('qa_note_text')

        response_data = {}
        
        et = ExtractedText.objects.get(pk=pk)

        qa, created = QANotes.objects.get_or_create(extracted_text=et)
        qa.qa_notes = qa_note_text
        qa.save()

        response_data['result'] = 'QA Note edits successfully saved'
        response_data['qa_note_text'] = qa.qa_notes
        #print(response_data)
        return HttpResponse(
            json.dumps(response_data),
            content_type=""application/json""
        )
    else:
        return HttpResponse(
            json.dumps({""not a POST request"": ""this will not happen""}),
            content_type=""application/json""
        )

@login_required()
def approve_extracted_text(request, pk):
    '''
    This is an endpoint that processes the ExtractedText approval
    '''
    extext = get_object_or_404(
	        ExtractedText.objects.select_subclasses(), pk=pk)
    nextpk = extext.next_extracted_text_in_qa_group()
    
    
    if request.method == 'POST':
        if extext.is_approvable():
            extext.qa_approved_date = timezone.now()
            extext.qa_approved_by = request.user
            extext.qa_checked = True
            extext.save()
            # The ExtractedText record is now approved.
            # Redirect to the appropriate page.

            referer = request.POST.get('referer', '') 
            if referer == 'data_document':
                # The user got to the QA page from a data document detail page,
                # so return there
                redirect_to = reverse(referer, kwargs={'pk': pk})
            elif not nextpk == 0:
                redirect_to = reverse('extracted_text_qa', args=[(nextpk)])
            elif nextpk == 0:
                # return to the top of the most local QA stack.
                # that may be the list of ExtractionScripts or 
                # the list of Chemical Presence Data Groups
                redirect_to = extext.get_qa_index_path()

            messages.success(request, ""The extracted text has been approved!"")
            if is_safe_url(url=redirect_to, allowed_hosts=request.get_host()):
                return HttpResponseRedirect(redirect_to)
            else:
                return HttpResponseRedirect(
                    reverse('extracted_text_qa', args=[(pk)]))
        else:
            # The ExtractedText record cannot be approved.
            # Return to the QA page and display an error.
            messages.error(request, ""The extracted text \
                could not be approved. Make sure that if the records have been edited,\
                the QA Notes have been populated."")
            return HttpResponseRedirect(
                    reverse('extracted_text_qa', args=[(pk)]))

        
        

/n/n/ndashboard/widgets.py/n/nfrom taggit_labels.widgets import LabelWidget


class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]
/n/n/nfactotum/settings.py/n/n""""""
Django settings for factotum project.

Generated by 'django-admin startproject' using Django 1.11.3.

For more information on this file, see
https://docs.djangoproject.com/en/1.11/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.11/ref/settings/
""""""

import sys
import os
from .settings_secret import *

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/1.11/howto/deployment/checklist/

# Application definition

INSTALLED_APPS = [
    'dal',
    'dal_select2',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'dashboard.apps.DashboardConfig',
    'api.apps.ApiConfig',
    'bootstrap_datepicker_plus',
    'widget_tweaks',
    'django.contrib.humanize',
    'factotum',
    'debug_toolbar',
    'haystack',
    'haystack_elasticsearch',
    'taggit',
    'taggit_labels'
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'debug_toolbar.middleware.DebugToolbarMiddleware',
]

ROOT_URLCONF = 'factotum.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
                'django.template.context_processors.media',
            ],
        },
    },
]

WSGI_APPLICATION = 'factotum.wsgi.application'


# IPs allowed to see django-debug-toolbar output.
INTERNAL_IPS = ('127.0.0.1',)

# Password validation
# https://docs.djangoproject.com/en/1.11/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/1.11/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'America/New_York'

USE_I18N = True

USE_L10N = True

USE_TZ = False

STATIC_URL = '/static/'
STATICFILES_DIRS = [
    os.path.join(BASE_DIR, 'static'),
]

DEBUG = True

LOGIN_REDIRECT_URL = 'index'
LOGIN_URL = 'login'

MEDIA_URL = 'media/'
MEDIA_ROOT = os.path.join(BASE_DIR, 'media/')

HAYSTACK_CONN = 'default'

TAGGIT_CASE_INSENSITIVE = True

EXTRA = 1

from django.contrib.messages import constants as messages

MESSAGE_TAGS = {
    messages.DEBUG: 'alert-info',
    messages.INFO: 'alert-info',
    messages.SUCCESS: 'alert-success',
    messages.WARNING: 'alert-warning',
    messages.ERROR: 'alert-danger',
}/n/n/n",0
89,89,90cfcb6215a5e25ca2970c671bd4482fe8dd491c,"/dashboard/forms.py/n/nfrom dal import autocomplete
from bootstrap_datepicker_plus import DatePickerInput

from django import forms
from django.forms import BaseInlineFormSet

from django.utils.translation import ugettext_lazy as _

from dashboard.models import *
from django.db.models import F
from dashboard.utils import get_extracted_models


class DataGroupForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = DataGroup
        fields = ['name', 'description', 'url', 'group_type', 'downloaded_by',
                  'downloaded_at', 'download_script', 'data_source', 'csv']
        widgets = {'downloaded_at': DatePickerInput()}
        labels = {'csv': _('Register Records CSV File'),
                  'url': _('URL'), }

    def __init__(self, *args, **kwargs):
        qs = Script.objects.filter(script_type='DL')
        self.user = kwargs.pop('user', None)
        super(DataGroupForm, self).__init__(*args, **kwargs)
        self.fields['csv'].widget.attrs.update({'accept': '.csv'})
        self.fields['download_script'].queryset = qs


class ExtractionScriptForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='EX'),
        label=""Extraction Script"")
    weight_fraction_type = forms.ModelChoiceField(
        queryset=WeightFractionType.objects.all(),
        label=""Weight Fraction Type"",
        initial=""1"")
    extract_file = forms.FileField(label=""Extracted Text CSV File"")

    def __init__(self, *args, **kwargs):
        self.dg_type = kwargs.pop('dg_type', 0)
        self.user = kwargs.pop('user', None)
        super(ExtractionScriptForm, self).__init__(*args, **kwargs)
        self.fields['weight_fraction_type'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['extract_file'].widget.attrs.update({'accept': '.csv'})
        if self.dg_type in ['FU', 'CP']:
            del self.fields['weight_fraction_type']
        self.collapsed = True


class CleanCompDataForm(forms.Form):
    required_css_class = 'required'  # adds to label tag
    script_selection = forms.ModelChoiceField(
        queryset=Script.objects.filter(script_type='DC'),
        label=""Data Cleaning Script"",
        required=True)
    clean_comp_data_file = forms.FileField(label=""Clean Composition Data CSV File"",
                                           required=True)

    def __init__(self, *args, **kwargs):
        super(CleanCompDataForm, self).__init__(*args, **kwargs)
        self.fields['script_selection'].widget.attrs.update(
            {'style': 'height:2.75rem; !important'})
        self.fields['clean_comp_data_file'].widget.attrs.update(
            {'accept': '.csv'})
        self.collapsed = True


class DataSourceForm(forms.ModelForm):
    required_css_class = 'required'

    class Meta:
        model = DataSource
        fields = ['title', 'url', 'estimated_records', 'state', 'priority',
                  'description']


class PriorityForm(forms.ModelForm):
    class Meta:
        model = DataSource
        fields = ['priority']

    def __init__(self, *args, **kwargs):
        super(PriorityForm, self).__init__(*args, **kwargs)
        self.fields['priority'].label = ''
        self.fields['priority'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


class QANotesForm(forms.ModelForm):
    class Meta:
        model = QANotes
        fields = ['qa_notes']
        widgets = {
            'qa_notes': forms.Textarea,
        }
        labels = {
            'qa_notes': _('QA Notes (required if approving edited records)'),
        }


class ExtractedTextQAForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'data_document', 'qa_checked']


class ProductLinkForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    document_type = forms.ModelChoiceField(
        queryset=DocumentType.objects.all(),
        label=""Data Document Type"",
        required=True)
    return_url = forms.CharField()

    class Meta:
        model = Product
        fields = ['title', 'manufacturer',
                  'brand_name', 'upc', 'size', 'color']

    def __init__(self, *args, **kwargs):
        super(ProductLinkForm, self).__init__(*args, **kwargs)
        self.fields['return_url'].widget = forms.HiddenInput()


class ProductForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag

    class Meta:
        model = Product
        fields = ['title', 'manufacturer', 'brand_name', 'size', 'color',
                  'model_number', 'short_description', 'long_description']


class ProductViewForm(ProductForm):
    class Meta(ProductForm.Meta):
        exclude = ('title', 'long_description',)

    def __init__(self, *args, **kwargs):
        super(ProductForm, self).__init__(*args, **kwargs)
        for f in self.fields:
            self.fields[f].disabled = True


class BasePUCForm(forms.ModelForm):
    puc = forms.ModelChoiceField(
        queryset=PUC.objects.all(),
        label='Category',
        widget=autocomplete.ModelSelect2(
            url='puc-autocomplete',
            attrs={'data-minimum-input-length': 3, })
    )


class ProductPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']


class HabitsPUCForm(BasePUCForm):
    class Meta:
        model = ExtractedHabitsAndPracticesToPUC
        fields = ['puc']


class BulkProductPUCForm(forms.ModelForm):
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput(),
                             required=True)

    class Meta:
        model = ProductToPUC
        fields = ['puc', 'id_pks']


class BulkPUCForm(BasePUCForm):
    class Meta:
        model = ProductToPUC
        fields = ['puc']

    def __init__(self, *args, **kwargs):
        super(BulkPUCForm, self).__init__(*args, **kwargs)
        lbl = 'Select PUC for Attribute to Assign to Selected Products'
        self.fields['puc'].label = lbl
        self.fields['puc'].widget.attrs['onchange'] = 'form.submit();'


class BulkProductTagForm(forms.ModelForm):
    required_css_class = 'required'  # adds to label tag
    tag = forms.ModelChoiceField(queryset=PUCTag.objects.none(),
                                 label='Attribute')
    id_pks = forms.CharField(label='Product Titles',
                             widget=forms.HiddenInput())

    class Meta:
        model = ProductToPUC
        fields = ['tag', 'id_pks']

    def __init__(self, *args, **kwargs):
        super(BulkProductTagForm, self).__init__(*args, **kwargs)
        lbl = 'Select Attribute to Assign to Selected Products'
        self.fields['tag'].label = lbl


class ExtractedTextForm(forms.ModelForm):
    class Meta:
        model = ExtractedText
        fields = ['prod_name', 'doc_date', 'rev_num']

        widgets = {
            'data_document': forms.HiddenInput(),
            'extraction_script': forms.HiddenInput(),
        }


class ExtractedCPCatForm(ExtractedTextForm):

    class Meta:
        model = ExtractedCPCat
        fields = ['doc_date', 'cat_code',
                  'description_cpcat', 'cpcat_sourcetype']


class ExtractedCPCatEditForm(ExtractedCPCatForm):

    class Meta(ExtractedCPCatForm.Meta):
        fields = ExtractedCPCatForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num', 'cpcat_code']


class ExtractedHHDocForm(ExtractedTextForm):

    class Meta:
        model = ExtractedHHDoc
        fields = ['hhe_report_number', 'study_location', 'naics_code', 'sampling_date', 'population_gender',
                  'population_age', 'population_other', 'occupation', 'facility']


class ExtractedHHDocEditForm(ExtractedHHDocForm):

    class Meta(ExtractedHHDocForm.Meta):
        fields = ExtractedHHDocForm.Meta.fields + \
            ['prod_name', 'doc_date', 'rev_num']


class DocumentTypeForm(forms.ModelForm):
    class Meta:
        model = DataDocument
        fields = ['document_type']

    def __init__(self, *args, **kwargs):
        super(DocumentTypeForm, self).__init__(*args, **kwargs)
        self.fields['document_type'].label = ''
        self.fields['document_type'].widget.attrs.update({
            'onchange': 'form.submit();'
        })


def include_extract_form(dg):
    '''Returns the ExtractionScriptForm based on conditions of DataGroup
    type as well as whether all records are matched, but not extracted
    '''
    if not dg.type in ['FU', 'CO', 'CP']:
        return False
    if dg.all_matched() and not dg.all_extracted():
        return ExtractionScriptForm(dg_type=dg.type)
    else:
        return False


class ExtractedChemicalFormSet(BaseInlineFormSet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


class ExtractedChemicalForm(forms.ModelForm):
    def __init__(self, *args, **kwargs):
        super(ExtractedChemicalForm, self).__init__(*args, **kwargs)
        # the non-field properties need to be explicitly added
        if hasattr(self.instance, 'dsstox') and self.instance.dsstox is not None:
            self.fields['true_cas'] = forms.CharField(max_length=200)
            self.fields['true_cas'].initial = self.instance.dsstox.true_cas
            self.fields['true_cas'].disabled = True
            self.fields['true_chemname'] = forms.CharField(max_length=400)
            self.fields['true_chemname'].initial = self.instance.dsstox.true_chemname
            self.fields['true_chemname'].disabled = True
            self.fields['SID'] = forms.CharField(max_length=50)
            self.fields['SID'].initial = self.instance.dsstox.sid
            self.fields['SID'].disabled = True

    class Meta:
        model = ExtractedChemical
        fields = '__all__'


def include_clean_comp_data_form(dg):
    '''Returns the CleanCompDataForm based on conditions of DataGroup
    type = Composition and at least 1 document extracted
    '''
    if not dg.type in ['CO']:
        return False
    if dg.extracted_docs() > 0:
        return CleanCompDataForm()
    else:
        return False


def create_detail_formset(document, extra=1, can_delete=False, exclude=[]):
    '''Returns the pair of formsets that will be needed based on group_type.
    .                       ('CO'),('CP'),('FU'),('HP'),('HH')
    Parameters
        ----------
        document : DataDocument
            The parent DataDocument
        extra : integer
            How many empty forms should be created for new records
        can_delete : boolean
            whether a delete checkbox is included
        exclude : list
            which fields to leave out of the form
    .

    '''
    group_type = document.data_group.type
    parent, child = get_extracted_models(group_type)
    extracted = hasattr(document, 'extractedtext')

    def make_formset(parent_model, model,
                     formset=BaseInlineFormSet,
                     form=forms.ModelForm,
                     exclude=exclude):
        formset_fields = model.detail_fields()
        if exclude:
            formset_fields = [in_field for in_field in formset_fields if not in_field in exclude]
        return forms.inlineformset_factory(parent_model=parent_model,
                                           model=model,
                                           fields=formset_fields,
                                           formset=formset,  # this specifies a custom formset
                                           form=form,
                                           extra=extra,
                                           can_delete=can_delete)

    def one():  # for chemicals or unknown
        ChemicalFormSet = make_formset(
            parent_model=parent,
            model=child,
            formset=ExtractedChemicalFormSet,
            form=ExtractedChemicalForm
        )
        return (ExtractedTextForm, ChemicalFormSet)

    def two():  # for functional_use
        FunctionalUseFormSet = make_formset(parent, child)
        return (ExtractedTextForm, FunctionalUseFormSet)

    def three():  # for habits_and_practices
        HnPFormSet = make_formset(parent, child)
        return (ExtractedTextForm, HnPFormSet)

    def four():  # for extracted_list_presence
        ListPresenceFormSet = make_formset(parent, child)
        ParentForm = ExtractedCPCatForm if extracted else ExtractedCPCatEditForm


        return (ParentForm, ListPresenceFormSet)

    def five():  # for extracted_hh_rec
        HHFormSet = make_formset(parent, child)
        ParentForm = ExtractedHHDocForm if extracted else ExtractedHHDocEditForm
        return (ParentForm, HHFormSet)
    dg_types = {
        'CO': one,
        'UN': one,
        'FU': two,
        'HP': three,
        'CP': four,
        'HH': five,
    }
    func = dg_types.get(group_type, lambda: None)
    return func()
/n/n/n/dashboard/models/PUC.py/n/nfrom taggit.models import TaggedItemBase, TagBase
from taggit.managers import TaggableManager

from django.db import models
from django.urls import reverse
from django.utils.translation import ugettext_lazy as _

from .common_info import CommonInfo
from .extracted_habits_and_practices_to_puc import (
                                            ExtractedHabitsAndPracticesToPUC)
from .extracted_habits_and_practices import ExtractedHabitsAndPractices


class PUC(CommonInfo):
    KIND_CHOICES = (
        ('UN', 'unknown'),
        ('FO', 'formulations'),
        ('AR', 'articles'),
        ('OC', 'occupational'))

    kind = models.CharField(max_length=2, blank=True, default='UN',
                             choices=KIND_CHOICES)
    gen_cat = models.CharField(max_length=50, blank=False)
    prod_fam = models.CharField(max_length=50, blank=True, default='')
    prod_type = models.CharField(max_length=100, blank=True, default='')
    description = models.TextField(null=False, blank=False)
    last_edited_by = models.ForeignKey('auth.User', on_delete=models.CASCADE,
                                                                    default=1)
    products = models.ManyToManyField('Product', through='ProductToPUC')
    extracted_habits_and_practices = models.ManyToManyField(
                        'dashboard.ExtractedHabitsAndPractices',
                        through='dashboard.ExtractedHabitsAndPracticesToPUC')
    tags = TaggableManager(through='dashboard.PUCToTag',
                           to='dashboard.PUCTag',
                           blank=True,
                           help_text='A set of PUC Attributes applicable to this PUC')

    class Meta:
        ordering = ['gen_cat', 'prod_fam', 'prod_type']
        verbose_name_plural = 'PUCs'

    def __str__(self):
        cats = [self.gen_cat, self.prod_fam, self.prod_type]
        return ' - '.join(cat for cat in cats if cat is not None)

    def natural_key(self):
        return self.gen_cat

    def tag_list(self, obj):
        return u"", "".join(o.name for o in obj.tags.all())


    def get_level(self):
        if self.is_level_one:
            return 1
        if self.is_level_two:
            return 2
        else:
            return 3


    @property
    def is_level_one(self): # gen_cat only
        return self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_two(self): # no prod_type
        return not self.prod_fam is '' and self.prod_type is ''

    @property
    def is_level_three(self): # most granular PUC
        return not self.prod_fam is '' and not self.prod_type is ''

    def get_the_kids(self):
        if self.is_level_one:
            return PUC.objects.filter(gen_cat=self.gen_cat)
        if self.is_level_two:
            return PUC.objects.filter(gen_cat=self.gen_cat,
                                        prod_fam=self.prod_fam)
        if self.is_level_three:
            return PUC.objects.filter(pk=self.pk)

    @property
    def product_count(self):
        '''Don't use this in large querysets. It uses a SQL query for each 
        PUC record. '''
        return self.products.count()

    @property
    def admin_url(self):
        return reverse('admin:dashboard_puc_change', args=(self.pk,))
        
    def get_assumed_tags(self):
        '''Queryset of used to filter which PUCs a Product can have '''
        qs = PUCToTag.objects.filter(content_object=self, assumed=True)
        return PUCTag.objects.filter(dashboard_puctotag_items__in=qs)


class PUCToTag(TaggedItemBase, CommonInfo):
    content_object = models.ForeignKey(PUC, on_delete=models.CASCADE)
    tag = models.ForeignKey('PUCTag', on_delete=models.CASCADE,
                            related_name=""%(app_label)s_%(class)s_items"")
    assumed = models.BooleanField(default=False)

    def __str__(self):
        return str(self.content_object)


class PUCTag(TagBase, CommonInfo):

    class Meta:
        verbose_name = _(""PUC Attribute"")
        verbose_name_plural = _(""PUC Attributes"")
        ordering = ('name',)

    def __str__(self):
        return self.name
/n/n/n/dashboard/models/__init__.py/n/nfrom .common_info import CommonInfo
from .data_source import DataSource
from .group_type import GroupType
from .data_group import DataGroup
from .document_type import DocumentType
from .data_document import DataDocument
from .ingredient import Ingredient
from .product import Product
from .source_category import SourceCategory
from .product_document import ProductDocument
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_habits_and_practices import ExtractedHabitsAndPractices
from .extracted_list_presence import ExtractedListPresence
from .extracted_hhdoc import ExtractedHHDoc
from .extracted_hhrec import ExtractedHHRec
from .script import Script
from .dsstox_lookup import DSSToxLookup
from .qa_group import QAGroup
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .PUC import PUC, PUCToTag, PUCTag
from .product_to_tag import ProductToTag
from .product_to_puc import ProductToPUC
from .extracted_habits_and_practices_to_puc import ExtractedHabitsAndPracticesToPUC
from .qa_notes import QANotes
from .raw_chem import RawChem
from .taxonomy import Taxonomy
from .taxonomy_source import TaxonomySource
from .taxonomy_to_PUC import TaxonomyToPUC
/n/n/n/dashboard/models/data_document.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.utils import timezone
from .document_type import DocumentType
from django.core.exceptions import ValidationError


class DataDocument(CommonInfo):
    """"""
    A DataDocument object is a single source of Factotum data. 

    ``filename``
        the name of the document's source file

    ``title``
        the title of the document
    
    ``url``
        an optional URL to the document's remote source

    ``raw_category``

    ``data_group``
        the DataGroup object to which the document belongs. The
        type of the data group determines which document types the
        document might be among, and determines much of the available 
        relationships and behavior associated with the document's 
        extracted data
    
    ``products``
        Products are associated with the data document in a many-to-many relationship

    ``matched``
        When a source file for the document has been uploaded to the
        file system, the document is considered ""matched"" to that
        source file. 
    
    ``extracted``
        When the content of a data document has been extracted by manual data entry
        or by an extraction script, a new ExtractedText record is created
        with the DataDocument's id as its primary key. 
    
    ``document_type``
        each type of data group may only contain certain types of data documents. The
        clean() method checks to make sure that the assigned document type is among the
        types allowed by the group type

    ``organization``

    ``note``

    """"""

    filename = models.CharField(max_length=255)
    title = models.CharField(max_length=255)
    url = models.CharField(null=True, blank=True, max_length=275)
    raw_category = models.CharField(null=True, blank=True, max_length=100)
    data_group = models.ForeignKey('DataGroup', on_delete=models.CASCADE)
    products = models.ManyToManyField('Product', through='ProductDocument')
    matched = models.BooleanField(default=False)
    #############################################################
    #  T E C H N I C A L   D E B T 
    # Storing this as a boolean field might not be a good idea. If someone 
    # deletes an ExtractedText object in the admin panel or in the database,
    # the bit will not flip, so the document will remain ""extracted""
    # even in the absence of an ExtractedText object
    extracted = models.BooleanField(default=False)  
    # The is_extracted method below should replace this attribute
    #############################################################
    document_type = models.ForeignKey(DocumentType, on_delete=models.PROTECT,
                                                        null=True, blank=True)
    organization = models.CharField(max_length=255, blank=True)
    note = models.TextField(blank=True, null=True)

    class Meta:
        ordering = ['-id']

    def __str__(self):
        return str(self.title)
    
    @property
    def detail_page_editable(self):
        # this could be moved to settings
        return self.data_group.group_type.code in ['CP', 'HH', 'CO', ] 

    @property
    def is_extracted(self):
        return hasattr(self,'extractedtext')

    def get_absolute_url(self):
        return reverse('data_document', kwargs={'pk': self.pk})

    def get_abstract_filename(self):
        ext = self.filename.split('.')[-1] #maybe not all are PDF??
        return f'document_{self.pk}.{ext}'

    def pdf_url(self):
        dg = self.data_group
        fn = self.get_abstract_filename()
        return f'/media/{dg.fs_id}/pdf/{fn}'

    def clean(self):
        # the document_type must be one of the children types
        # of the datadocument's parent datagroup
        this_type = self.data_group.group_type
        doc_types = DocumentType.objects.filter(group_type=this_type)
        if not self.document_type in doc_types:
            raise ValidationError(('The document type must be allowed by '
                                                    'the parent data group.'))
/n/n/n/dashboard/models/data_group.py/n/nimport os
import shutil
import uuid
from factotum import settings
from pathlib import Path, PurePath

from django.db import models
from .common_info import CommonInfo
from django.urls import reverse
from django.db.models.signals import pre_save
from django.dispatch import receiver
from model_utils import FieldTracker
from django.core.exceptions import ValidationError

from .group_type import GroupType
from .extracted_text import ExtractedText
from .extracted_cpcat import ExtractedCPCat
from .extracted_chemical import ExtractedChemical
from .extracted_functional_use import ExtractedFunctionalUse
from .extracted_list_presence import ExtractedListPresence

# could be used for dynamically creating filename on instantiation
# in the 'upload_to' param on th FileField
def update_filename(instance, filename):
    name_fill_space = instance.name.replace(' ', '_')
    # potential space errors in name
    name = '{0}/{0}_{1}'.format(name_fill_space, filename)
    return name


def csv_upload_path(instance, filename):
    # potential space errors in name
    name = '{0}/{1}'.format(instance.fs_id, filename)
    return name

extract_models = {
    'CO': (ExtractedText, ExtractedChemical),
    'FU': (ExtractedText, ExtractedFunctionalUse),
    'CP': (ExtractedCPCat, ExtractedListPresence)
}



class DataGroup(CommonInfo):

    name = models.CharField(max_length=50)
    description = models.TextField(null=True, blank=True)
    downloaded_by = models.ForeignKey('auth.User',
                                    on_delete=models.SET_DEFAULT, default = 1)
    downloaded_at = models.DateTimeField()
    download_script = models.ForeignKey('Script',
                                    on_delete=models.SET_NULL, default=None,
                                    null=True, blank=True)
    data_source = models.ForeignKey('DataSource', on_delete=models.CASCADE)
    fs_id = models.UUIDField(default=uuid.uuid4, editable=False)
    csv = models.FileField(upload_to=csv_upload_path, null=True)
    zip_file = models.CharField(max_length=100)
    group_type = models.ForeignKey(GroupType, on_delete=models.SET_DEFAULT,
                                            default=1, null=True, blank=True)
    url = models.CharField(max_length=150, blank=True)

    tracker = FieldTracker()

    @property
    def type(self):
        return str(self.group_type.code)

    @property
    def is_composition(self):
        return self.type == 'CO'

    @property
    def is_habits_and_practices(self):
        return self.type == 'HP'

    @property
    def is_functional_use(self):
        return self.type == 'FU'

    @property
    def is_chemical_presence(self):
        return self.type == 'CP'

    @property
    def is_hh(self):
        return self.type == 'HH'


    def get_extract_models(self):
        '''returns a tuple with parent/child extract models'''
        return extract_models.get(self.type)

    def save(self, *args, **kwargs):
        super(DataGroup, self).save(*args, **kwargs)

    def matched_docs(self):
        return self.datadocument_set.filter(matched=True).count()

    def all_matched(self):
        return all(self.datadocument_set.values_list('matched', flat=True))

    def all_extracted(self):
        return all(self.datadocument_set.values_list('extracted', flat=True))

    def registered_docs(self):
        return self.datadocument_set.count()

    def extracted_docs(self):
        return self.datadocument_set.filter(extracted=True).count()

    def __str__(self):
        return self.name

    def get_absolute_url(self):
        return reverse('data_group_edit', kwargs={'pk': self.pk})

    def get_name_as_slug(self):
        return self.name.replace(' ', '_')

    def get_dg_folder(self):
        uuid_dir = f'{settings.MEDIA_ROOT}{str(self.fs_id)}'
        name_dir = f'{settings.MEDIA_ROOT}{self.get_name_as_slug()}'

        #this needs to handle missing csv files
        if bool(self.csv.name):
            # parse the media folder from the penultimate piece of csv file path
            p = PurePath(self.csv.path)
            csv_folder=p.parts[-2]
            csv_fullfolderpath   = f'{settings.MEDIA_ROOT}{csv_folder}'

        if os.path.isdir(uuid_dir):
            return uuid_dir # UUID-based folder
        elif bool(self.csv.name) and os.path.isdir(csv_fullfolderpath):
            return csv_fullfolderpath # csv path-based folder
        else:
            return 'no_folder_found'

    @property
    def dg_folder(self):
        '''This is a ""falsy"" property. If the folder cannot be found,
        dg.dg_folder evaluates to boolean False '''
        if self.get_dg_folder() != 'no_folder_found':
            return self.get_dg_folder()
        else:
            return False


    @property
    def csv_url(self):
        '''This is a ""falsy"" property. If the csv file cannot be found,
        dg.csv_url evaluates to boolean False '''
        try:
            self.csv.size
            csv_url = self.csv.url
        except ValueError:
            csv_url = False
        except:
            csv_url = False
        return csv_url


    @property
    def zip_url(self):
        '''This is a ""falsy"" property. If the zip file cannot be found,
        dg.zip_url evaluates to boolean False '''
        if self.get_zip_url()!='no_path_found':
            return(self.get_zip_url)
        else:
            return False
        

    def get_zip_url(self):
        # the path if the data group's folder was built from a UUID:
        uuid_path = f'{self.get_dg_folder()}/{str(self.fs_id)}.zip'
        # path if the data group's folder was built from old name-based method
        zip_file_path = f'{self.get_dg_folder()}/{self.get_name_as_slug()}.zip'
        if os.path.isfile(uuid_path):   # it is a newly-added data group
            zip_url = uuid_path
        elif os.path.isfile(zip_file_path): # it is a pre-UUID data group
            zip_url = zip_file_path
        else:
            zip_url = 'no_path_found'
        return zip_url


    def get_extracted_template_fieldnames(self):
        extract_fields = ['data_document_id','data_document_filename',
                            'prod_name', 'doc_date','rev_num', 'raw_category',
                            'raw_cas', 'raw_chem_name', 'report_funcuse']
        if self.type == 'FU':
            return extract_fields
        if self.type == 'CO':
            return extract_fields + ['raw_min_comp','raw_max_comp', 'unit_type',
                                        'ingredient_rank', 'raw_central_comp']
        if self.type == 'CP':
            for name in ['prod_name','rev_num','report_funcuse']:
                extract_fields.remove(name)
            return extract_fields + ['cat_code','description_cpcat',
                                    'cpcat_code','cpcat_sourcetype']

    def get_clean_comp_data_fieldnames(self):
        return ['id','lower_wf_analysis','central_wf_analysis', 'upper_wf_analysis']

    def clean_fields(self, exclude=None):
        super().clean_fields(exclude=exclude)
        if self.tracker.has_changed('group_type_id') and self.extracted_docs():
            msg = ""The Group Type may not be changed once extracted documents have been associated with the group.""
            raise ValidationError({'group_type': msg})


@receiver(models.signals.post_delete, sender=DataGroup)
def auto_delete_file_on_delete(sender, instance, **kwargs):
    """"""
    Deletes datagroup directory from filesystem
    when datagroup instance is deleted.
    """"""
    dg_folder = instance.get_dg_folder()
    if os.path.isdir(dg_folder):
        #print('deleting folder %s for data group %s'%(dg_folder, instance.pk))
        shutil.rmtree(dg_folder)
/n/n/n/dashboard/models/extracted_chemical.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .extracted_text import ExtractedText
from .unit_type import UnitType
from .weight_fraction_type import WeightFractionType
from .raw_chem import RawChem


def validate_ingredient_rank(value):
    if value < 1 or value > 999:
        raise ValidationError(
            (f'Quantity {value} is not allowed'), params={'value': value},)


class ExtractedChemical(CommonInfo, RawChem):

    raw_cas_old = models.CharField(
        ""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                         null=True, blank=True)
    raw_min_comp = models.CharField(""Raw minimum composition"", max_length=100,
                                    null=True, blank=True)
    raw_max_comp = models.CharField(""Raw maximum composition"", max_length=100,
                                    null=True, blank=True)
    unit_type = models.ForeignKey(UnitType, on_delete=models.PROTECT)
    report_funcuse = models.CharField(""Reported functional use"", max_length=100,
                                      null=True, blank=True)
    weight_fraction_type = models.ForeignKey(WeightFractionType,
                                             on_delete=models.PROTECT, null=True, default='1')
    ingredient_rank = models.PositiveIntegerField(""Ingredient rank"", null=True, blank=True,
                                                  validators=[validate_ingredient_rank])
    raw_central_comp = models.CharField(""Raw central composition"", max_length=100, null=True, blank=True)

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @classmethod
    def detail_fields(cls):
        return ['extracted_text', 'raw_chem_name', 'raw_cas', 'raw_min_comp', 'raw_central_comp',
                'raw_max_comp', 'unit_type', 'weight_fraction_type', 'report_funcuse',
                'ingredient_rank', 'rawchem_ptr']

    def get_datadocument_url(self):
        return self.extracted_text.data_document.get_absolute_url()

    @property
    def data_document(self):
        return self.extracted_text.data_document

    def indexing(self):
        obj = ExtractedChemicalIndex(
            meta={'id': self.id},
            chem_name=self.raw_chem_name,
            raw_cas=self.raw_cas,
            raw_chem_name=self.raw_chem_name,
            facet_model_name='Extracted Chemical',
        )
        obj.save()
        return obj.to_dict(include_meta=True)

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def true_cas(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_cas
        else:
            return None

    @property
    def true_chemname(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.true_chemname
        else:
            return None

    @property
    def sid(self):
        if hasattr(self, 'curated_chemical') and self.curated_chemical is not None:
            return self.curated_chemical.sid
        else:
            return None
/n/n/n/dashboard/models/extracted_functional_use.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .extracted_text import ExtractedText
from .raw_chem import RawChem

class ExtractedFunctionalUse(CommonInfo, RawChem):

    raw_cas_old = models.CharField(""Raw CAS"", max_length=50, null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                  null=True, blank=True)
    report_funcuse = models.CharField(""Reported functional use"",
                                        max_length=100, null=True, blank=True)

    def __str__(self):
        return self.raw_chem_name

    @classmethod
    def detail_fields(cls):
        return ['extracted_text','raw_cas','raw_chem_name','report_funcuse']

    def get_extractedtext(self):
        return self.extracted_text

    @property
    def data_document(self):
        return self.extracted_text.data_document
/n/n/n/dashboard/models/extracted_list_presence.py/n/nfrom django.db import models

from dashboard.models import CommonInfo
from .raw_chem import RawChem

class ExtractedListPresence(CommonInfo, RawChem):

    raw_cas_old = models.CharField(""Raw CAS"", max_length=100,
                                        null=True, blank=True)
    raw_chem_name_old = models.CharField(""Raw chemical name"", max_length=500,
                                        null=True, blank=True)
    qa_flag = models.BooleanField(default=False)

    @classmethod
    def detail_fields(cls):
        return ['raw_cas','raw_chem_name']

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    def get_datadocument_url(self):
        return self.extracted_cpcat.data_document.get_absolute_url()

    def get_extractedtext(self):
        return self.extracted_cpcat.extractedtext_ptr
    
    @property
    def data_document(self):
        return self.extracted_text.data_document
/n/n/n/dashboard/models/extracted_text.py/n/nfrom itertools import chain
from datetime import datetime
from model_utils.managers import InheritanceManager

from django.db import models
from django.core.exceptions import ValidationError
from django import forms
from django.urls import reverse


from .common_info import CommonInfo

    # this could potentially be used for 1:1 matching when uploading
    # coming in django v2.2!!
	# class Meta:
	# 	constraints = [
	# 		models.UniqueConstraint(fields=['prod_name','data_document'],
	# 								name='unique_assignment'),
	# 	]

class ExtractedText(CommonInfo):
    data_document = models.OneToOneField('DataDocument',on_delete=models.CASCADE,
                                                            primary_key=True)
    prod_name = models.CharField(max_length=500, null=True, blank=True)
    doc_date = models.CharField(max_length=25, null=True, blank=True)
    rev_num = models.CharField(max_length=50, null=True, blank=True)
    extraction_script = models.ForeignKey('Script', on_delete=models.CASCADE,
                                        limit_choices_to={'script_type': 'EX'})
    qa_checked = models.BooleanField(default=False, verbose_name=""QA approved"")
    qa_edited = models.BooleanField(default=False, verbose_name=""QA edited"")
    qa_approved_date = models.DateTimeField(null=True, blank=True,
                                                verbose_name=""QA approval date"")
    qa_approved_by = models.ForeignKey('auth.User', on_delete=models.SET_NULL,
                                                verbose_name = ""QA approved by"",
                                                null=True, blank=True,)
    qa_group = models.ForeignKey('QAGroup', verbose_name=""QA group"",
                                                     on_delete=models.SET_NULL,
                                                     null=True, blank=True)

    objects = InheritanceManager()


    def __str__(self):
        return str(self.data_document)

    def next_extracted_text_in_qa_group(self):
        nextid = 0
        # If the document is part of a Script-based QA Group, the 
        # next document is drawn from that group. If it is a CPCat
        # or HHE record, there is no next document
        extextnext = get_next_or_prev(ExtractedText.objects.filter(
            qa_group=self.qa_group, qa_checked=False), self, 'next')
        if extextnext:
            # Replace our item with the next one
            nextid = extextnext.pk
        if extextnext == self:
            nextid = 0
        return nextid
    
    def get_qa_index_path(self):
        """"""
        The type of data group to which the extracted text object belongs
        determines which QA index it will use.
        """"""
        group_type_code = self.data_document.data_group.group_type.code

        if group_type_code in ['CP','HH']:
            # TODO: change HH to its own path
            return reverse('qa_chemicalpresence_index')
        else:
            return reverse('qa_extractionscript_index')


    def fetch_extracted_records(self):
        return self.rawchem.all()

    def pull_out_cp(self):
        if hasattr(self, 'extractedcpcat'):
            return self.extractedcpcat
        else:
            return self

    def pull_out_hh(self):
        if hasattr(self, 'extractedhhdoc'):
            return self.extractedhhdoc
        else:
            return self

    def one_to_one_check(self, odict):
        '''
        Used in the upload of extracted text in the data_group_detail view, this
        returns a boolean to assure that there is a 1:1 relationship w/
        the Extracted{parent}, i.e. (Text/CPCat), and the DataDocument
        '''
        if hasattr(self, 'cat_code'):
            return self.cat_code != odict['cat_code']
        else:
            return self.prod_name != odict['prod_name']




def get_next_or_prev(models, item, direction):
    '''
    Returns the next or previous item of
    a query-set for 'item'.

    'models' is a query-set containing all
    items of which 'item' is a part of.

    direction is 'next' or 'prev'

    '''
    getit = False
    if direction == 'prev':
        models = models.reverse()
    for m in models:
        if getit:
            return m
        if item == m:
            getit = True
    if getit:
        # This would happen when the last
        # item made getit True
        return models[0]
    return False
/n/n/n/dashboard/models/ingredient.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from .weight_fraction_type import WeightFractionType
from .extracted_chemical import ExtractedChemical
from .script import Script


def validate_wf_analysis(value):
    if value < 0 or value > 1:
        raise ValidationError(
            (f'Quantity {value} must be between 0 and 1'),params={'value': value})


class Ingredient(CommonInfo):
    lower_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                            null=True, blank=True,
                                            validators=[validate_wf_analysis])
    central_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                              null=True, blank=True,
                                              validators=[validate_wf_analysis])
    upper_wf_analysis = models.DecimalField(max_digits=16, decimal_places=15,
                                            null=True, blank=True,
                                            validators=[validate_wf_analysis])

    script = models.ForeignKey(to=Script, on_delete=models.CASCADE,
                                                    null=True, blank=True)
                                                    
    rawchem_ptr = models.OneToOneField(related_name='ingredient', parent_link=True,
        on_delete=models.CASCADE, to='dashboard.RawChem')

    def __str__(self):
        return str(self.id)
/n/n/n/dashboard/models/qa_notes.py/n/nfrom django.db import models
from .common_info import CommonInfo
from django.core.exceptions import ValidationError
from django.utils.translation import ugettext_lazy as _

from dashboard.models import ExtractedText


class QANotes(CommonInfo):
    extracted_text = models.OneToOneField(ExtractedText, on_delete=models.CASCADE)
    qa_notes = models.TextField(null=True, blank=True)

    def __str__(self):
        return 'Notes for {}'.format(self.extracted_text)

    def clean(self):
        if self.extracted_text.qa_edited and not self.qa_notes:
            raise ValidationError(
                    _('Before approving, please add a note explaining your edits to the extracted data'))
/n/n/n/dashboard/models/raw_chem.py/n/nfrom django.db import models
from .dsstox_lookup import DSSToxLookup
from .extracted_text import ExtractedText
from model_utils.managers import InheritanceManager
from django.apps import apps
from django.db.models.signals import pre_save
from django.dispatch import receiver

from model_utils import FieldTracker


class RawChem(models.Model):
    extracted_text = models.ForeignKey(ExtractedText, related_name = 'rawchem', 
        on_delete=models.CASCADE, null=False, blank = False)

    raw_cas = models.CharField(""Raw CAS"", max_length=100, null=True, blank=True)
    raw_chem_name = models.CharField(""Raw chemical name"", max_length=500,
                                                        null=True, blank=True)
    temp_id = models.IntegerField(default=0, null=True, blank=True)
    temp_obj_name = models.CharField(max_length=255, null=True, blank=True)

    rid = models.CharField(max_length=50, null=True, blank=True)

    dsstox = models.ForeignKey(DSSToxLookup, related_name = 'curated_chemical', on_delete=models.PROTECT,
                                                    null=True, blank=True)

    objects = InheritanceManager()

    tracker = FieldTracker()

    def __str__(self):
        return str(self.raw_chem_name) if self.raw_chem_name else ''

    @property
    def sid(self):
        '''If there is no DSSToxLookup record via the 
        curated_chemical relationship, it evaluates to boolean False '''
        try:
            return self.curated_chemical.sid
        except AttributeError:
            return False


    def get_data_document(self):
        '''Find the child object by trying each of the classes, then return the 
            datadocument id from it
            NOTE: this will be obsolete once we move the data_document 
            foreign key into RawChem in ticket 654
         '''
        id=self.id
        try:
            return apps.get_model('dashboard.ExtractedChemical').objects.get(rawchem_ptr=id).data_document
        except apps.get_model('dashboard.ExtractedChemical').DoesNotExist:
            try: 
                return apps.get_model('dashboard.ExtractedFunctionalUse').objects.get(rawchem_ptr=id).data_document
            except apps.get_model('dashboard.ExtractedFunctionalUse').DoesNotExist:
                try: 
                    return apps.get_model('dashboard.ExtractedListPresence').objects.get(rawchem_ptr=id).data_document
                except apps.get_model('dashboard.ExtractedListPresence').DoesNotExist: 
                    return False

    @staticmethod
    def pre_save(sender, **kwargs):
        instance = kwargs.get('instance')
        previous_raw_cas = instance.tracker.previous('raw_cas')
        previous_raw_chem_name = instance.tracker.previous('raw_chem_name')
       
        if instance.tracker.has_changed('raw_cas') or \
        instance.tracker.has_changed('raw_chem_name'):
            instance.dsstox = None

pre_save.connect(RawChem.pre_save, sender=RawChem)
/n/n/n/dashboard/models/script.py/n/nimport math
from random import shuffle

from django.db import models
from django.urls import reverse
from django.core.validators import (URLValidator, MaxValueValidator, 
                                                    MinValueValidator)

from .common_info import CommonInfo
from .data_document import DataDocument


class Script(CommonInfo):

    TYPE_CHOICES = (('DL', 'download'),
                    ('EX', 'extraction'),
                    ('PC', 'product categorization'),
                    ('DC', 'data cleaning'))

    # Specify the share of a script's ExtractedText objects that must be
    # approved in order for the script's QA sat
    QA_COMPLETE_PERCENTAGE = 0.2


    title = models.CharField(max_length=50)
    url = models.CharField(max_length  = 100,
                            null       = True,
                            blank      = True,
                            validators = [URLValidator()])
    qa_begun = models.BooleanField(default=False)
    script_type = models.CharField( max_length = 2,
                                    choices    = TYPE_CHOICES,
                                    blank      = False,
                                    default    = 'EX')
    confidence = models.PositiveSmallIntegerField('Confidence', blank=True,
                                                validators=[
                                                        MaxValueValidator(100),
                                                        MinValueValidator(1)],
                                                                default=1)

    def __str__(self):
        return str(self.title)

    def get_absolute_url(self):
        return reverse('extraction_script_edit', kwargs={'pk': self.pk})

    def get_datadocument_count(self):
        return DataDocument.objects.filter(
                extractedtext__extraction_script=self.pk).count()

    def get_qa_complete_extractedtext_count(self):
        return DataDocument.objects.filter(extractedtext__qa_checked=True,
                            extractedtext__extraction_script=self.pk).count()

    def get_pct_checked(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return ""{0:.0f}%"".format(pct)

    def get_pct_checked_numeric(self):
        count = self.get_datadocument_count()
        pct = (0 if count == 0 else (
                      self.get_qa_complete_extractedtext_count() / count * 100))
        return pct

    def qa_button_text(self):
        if self.get_qa_status():
            return ""QA Complete"" 
        elif self.qa_begun:
            return ""Continue QA""
        else:
            return ""Begin QA""

    def get_qa_status(self):
        """"""
        Compare the derived percent checked against the threshold constant
        Return true when the percent checked is above the threshold
        """"""
        return self.get_pct_checked_numeric() >= self.QA_COMPLETE_PERCENTAGE * 100

    def create_qa_group(self, force_doc_id=None):
        """"""
        Creates a QA Group for the specified Script object;
        Use all the related ExtractedText records or, if there are more than 100,
        select 20% of them. 
        """"""
        from .qa_group import QAGroup
        from .extracted_text import ExtractedText
        es = self
        # Handle cases where a QA group already exists for the script
        if QAGroup.objects.filter(extraction_script = es).count() == 1:
            # This is a valid state
            return QAGroup.objects.get(extraction_script = es)
        elif QAGroup.objects.filter(extraction_script = es).count() > 1:
            # this is a failure mode induced by the system's allowing
            # duplicate QA Groups to be created for a single script
            return QAGroup.objects.filter(extraction_script = es).first()

        
        # Create a new QA Group for the ExtractionScript es
        qa_group = QAGroup.objects.create(extraction_script=es)
        # Collect all the ExtractedText object keys that are related
        # to the Script being QA'd and have not yet been checked
        doc_text_ids = list(ExtractedText.objects.filter(extraction_script=es,
                                                    qa_checked=False
                                                    ).values_list('pk',
                                                                flat=True))
        # If there are fewer than 100 related records, they make up the entire QA Group
        if len(doc_text_ids) < 100 and len(doc_text_ids) > 0:
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids)
        # Otherwise sample 20 percent
        elif len(doc_text_ids) >= 100 :
            # Otherwise sample 20% of them
            random_20 = math.ceil(len(doc_text_ids)/5)
            shuffle(doc_text_ids)  # this is used to make random selection of texts
            texts = ExtractedText.objects.filter(pk__in=doc_text_ids[:random_20])
        else:
            # If there are no related ExtractedText records, something has gone wrong
            # Don't make a new QA Group with zero ExtractedTexts
            # print('The Script has no related ExtractedText records')
            texts = None

        # Set the qa_group attribute of each ExtractedText record to the new QA Group    
        if texts is not None:
            for text in texts:
                text.qa_group = qa_group
                text.save()

        # If the force_doc_id argument was populated, make sure it gets assigned 
        # to the new QA Group
        if force_doc_id is not None and ExtractedText.objects.filter(pk=force_doc_id).exists():
            text = ExtractedText.objects.get(pk=force_doc_id)
            text.qa_group = qa_group
            text.save()
        
        return qa_group

        
/n/n/n/dashboard/models/taxonomy.py/n/nfrom django.db import models
from .common_info import CommonInfo


class Taxonomy(CommonInfo):
    title = models.CharField(max_length=100, blank=False, null=False)
    description = models.TextField(null=True, blank=True)
    parent = models.ForeignKey('Taxonomy', on_delete=models.CASCADE,
                                                    null=True, blank=True)
    source = models.ForeignKey('TaxonomySource', on_delete=models.CASCADE)
    category_code = models.CharField(max_length=40, null=True, blank=True)
    last_edited_by = models.ForeignKey('auth.User',
                                                on_delete=models.SET_DEFAULT,
                                                default=1)
    product_category = models.ManyToManyField('PUC', through='TaxonomyToPUC')

    class Meta:
        verbose_name_plural = 'Taxonomies'

    def __str__(self):
        return str(self.title)
/n/n/n/dashboard/tests/functional/test_dashboard.py/n/nimport csv
import time
from lxml import html

from django.urls import resolve
from django.test import TestCase

from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard import views
from dashboard.models import *


class DashboardTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        # self.test_start = time.time()

    # def tearDown(self):
    #     self.test_elapsed = time.time() - self.test_start
    #     print('\nFinished with ' + self._testMethodName + ' in {:.2f}s'.format(self.test_elapsed))

    def test_public_navbar(self):
        self.client.logout()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('factotum', response_html.xpath('string(/html/body/nav//a[@href=""/""]/text())'),
                      'The app name factotum should appear in the public navbar')
        self.assertNotIn('QA', response_html.xpath('string(/html/body/nav//a[@href=""/qa/extractionscript/""])'),
                         'The link to /qa/ should not appear in the public navbar')

    def test_logged_in_navbar(self):
        self.client.login(username='Karyn', password='specialP@55word')
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        self.assertIn('QA', response_html.xpath('string(//*[@id=""navbarQADropdownMenuLink""])'),
                      'The link to /qa/ must be in the logged-in navbar')
        found = resolve('/qa/extractionscript/')
        self.assertEqual(found.func, views.qa_extractionscript_index)

    def test_percent_extracted_text_doc(self):
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('0%', extracted_doc_count)

        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        extracted_doc_count = response_html.xpath(
            '/html/body/div[1]/div[1]/div[4]/div/div')[0].text
        self.assertEqual('100%', extracted_doc_count)

    def test_PUC_download(self):
        p = self.objects.puc
        puc_line = (p.gen_cat + ',' + p.prod_fam + ',' + p.prod_type + ',' + p.description +
                    ',' + str(p.get_level()) + ',' + str(p.product_count))
        # get csv
        response = self.client.get('/dl_pucs/')
        self.assertEqual(response.status_code, 200)
        csv_lines = response.content.decode('ascii').split('\r\n')
        # check header
        self.assertEqual(csv_lines[0], ('gen_cat,prod_fam,prod_type,description,'
                                        'PUC_type,num_prods'))
        # check the PUC from loader
        self.assertEqual(csv_lines[1], puc_line)


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_chemical_card(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('DSS Tox Chemicals', response,
                      'Where is the DSS Tox Chemicals card???')
        response_html = html.fromstring(response)
        num_dss = int(response_html.xpath('//*[@name=""dsstox""]')[0].text)
        dss_table_count = DSSToxLookup.objects.count()
        self.assertEqual(num_dss, dss_table_count,
                         'The number shown should match the number of records in DSSToxLookup')


class DashboardTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def test_producttopuc_counts(self):
        response = self.client.get('/').content.decode('utf8')
        self.assertIn('Products Linked To PUC', response,
                      'Where is the Products Linked to PUC card???')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)

        orm_prod_puc_count = ProductToPUC.objects.values(
            'product_id').distinct().count()
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign an already-assigned product to a different PUC with a different method
        # and confirm that the count has not changed
        p2puc = ProductToPUC.objects.first()
        p2puc.id = None
        p2puc.classification_method = 'MB'
        p2puc.puc_id = 21
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count,
                         'The page should show %s Products linked to PUCs' % orm_prod_puc_count)

        # Assign a previously unassigned product to a different PUC with a different method
        # and confirm that the count has gone up
        assigned_prods = ProductToPUC.objects.values_list('product_id')
        # print(assigned_prods)
        prod = Product.objects.exclude(id__in=assigned_prods).first()
        puc21 = PUC.objects.get(id=21)
        p2puc = ProductToPUC.objects.create(
            product=prod, puc=puc21, classification_method='MA')
        p2puc.save()

        response = self.client.get('/').content.decode('utf8')
        response_html = html.fromstring(response)
        num_prods = int(response_html.xpath(
            '//*[@name=""product_with_puc_count""]')[0].text)
        self.assertEqual(num_prods, orm_prod_puc_count + 1,
                         'The page should show %s Products linked to PUCs' % str(orm_prod_puc_count + 1))
/n/n/n/dashboard/tests/functional/test_datadocument_detail.py/n/nfrom lxml import html

from django.test import Client
from django.urls import reverse
from django.test import TestCase, override_settings
from django.core.exceptions import ObjectDoesNotExist

from dashboard.forms import *
from factotum.settings import EXTRA
from dashboard.tests.loader import *


@override_settings(ALLOWED_HOSTS=['testserver'])
class DataDocumentDetailTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_absent_extracted_text(self):
        # Check every data document and confirm that its detail page loads,
        # with or without a detail formset
        for dd in DataDocument.objects.all():
            ddid = dd.id
            resp = self.client.get('/datadocument/%s/' % ddid)
            self.assertEqual(resp.status_code, 200, 'The page must return a 200 status code')
            try:
                extracted_text = ExtractedText.objects.get(data_document=dd)
            except ExtractedText.DoesNotExist:
                #print(dd.id)
                self.assertContains(resp, 'No Extracted Text exists for this Data Document')
            else:
                self.assertContains(resp, '<h4>Extracted Text')

    def test_script_links(self):
        doc = DataDocument.objects.first()
        #response = self.client.get(f'/datadocument/{doc.pk}/')
        response = self.client.get(f'/datadocument/179486/')
        self.assertIn('Download Script',response.content.decode('utf-8'))
        self.assertIn('Extraction Script',response.content.decode('utf-8'))

    def test_product_card_location(self):
        response = self.client.get('/datadocument/179486/')
        html = response.content.decode('utf-8')
        e_idx = html.index('<h4>Extracted Text')
        p_idx = html.index('<h4 class=""d-inline"">Products')
        self.assertTrue(p_idx > e_idx, ('Product card should come after ' 
                                        'Extracted Text card'))

    def test_product_create_link(self):
        response = self.client.get('/datadocument/167497/')
        self.assertContains(response, '/link_product_form/167497/')
        data = {'title'        : ['New Product'],
                'upc'          : ['stub_1860'],
                'document_type': [1],
                'return_url'   : ['/datadocument/167497/']}
        response = self.client.post('/link_product_form/167497/', data=data)
        self.assertRedirects(response,'/datadocument/167497/')
        response = self.client.get(response.url)
        self.assertContains(response, 'New Product')

    def test_product_title_duplication(self):
        response = self.client.get('/datadocument/245401/')
        self.assertContains(response, '/link_product_form/245401/')
        # Add a new Product
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9100'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9100')
        self.assertContains(response, f'product/%s' % new_product.id )

        # Add another new Product with the same title
        data = {'title'        : ['Product Title'],
                'upc'          : ['stub_9101'],
                'document_type': [1],
                'return_url'   : ['/datadocument/245401/']}
        response = self.client.post('/link_product_form/245401/', data=data)
        self.assertRedirects(response,'/datadocument/245401/')
        response = self.client.get(response.url)
        new_product = Product.objects.get(upc='stub_9101')
        self.assertContains(response, f'product/%s' % new_product.id )

    def test_add_extracted(self):
        '''Check that the user has the ability to create an extracted record
        when the document doesn't yet have an extracted record for data 
        group types 'CP' and 'HH'
        '''
        doc = DataDocument.objects.get(pk=354784)
        self.assertFalse(doc.extracted, (""This document is matched ""
                                                    ""but not extracted""))
        data = {'hhe_report_number': ['47']}
        response = self.client.post('/extractedtext/edit/354784/', data=data,
                                                            follow=True)
        doc = DataDocument.objects.get(pk=354784)
        self.assertTrue(doc.extracted, ""This document is not extracted "")
        page = html.fromstring(response.content)
        hhe_no = page.xpath('//dd[contains(@class, ""hh-report-no"")]')[0].text
        self.assertIn('47', hhe_no)


class TestDynamicDetailFormsets(TestCase):
    fixtures = fixtures_standard

    def setUp(self):

        self.client.login(username='Karyn', password='specialP@55word')

    def test_fetch_extracted_records(self):
        ''' Confirm that each detail child object returned by the fetch_extracted_records
        function has the correct parent '''
        for et in ExtractedText.objects.all():
            #print('Fetching extracted child records from %s: %s ' % (et.pk , et))
            for ex_child in et.fetch_extracted_records():
                child_model = ex_child.__class__ # the fetch_extracted_records function returns different classes
                #print('    %s: %s' % (ex_child.__class__.__name__ , ex_child ))
                self.assertEqual(et.pk , child_model.objects.get(pk=ex_child.pk).extracted_text.pk,
                    'The ExtractedChemical object with the returned child pk should have the correct extracted_text parent')

    def test_extractedsubclasses(self):
        ''' Confirm that the inheritance manager is returning appropriate
            subclass objects and ExtractedText base class objects 
         '''
        for doc in DataDocument.objects.all():
            try:
                extsub = ExtractedText.objects.get_subclass(data_document=doc)
                # A document with the CP data group type should be linked to 
                # ExtractedCPCat objects
                if doc.data_group.group_type.code=='CP':
                    #print(f'%s %s %s' % (doc.id, extsub, type(extsub)))
                    self.assertEqual(type(extsub) , ExtractedCPCat)
                elif doc.data_group.group_type.code=='HH':
                    self.assertEqual(type(extsub) , ExtractedHHDoc)
                else:
                    self.assertEqual(type(extsub) , ExtractedText)
            except ObjectDoesNotExist:
                pass
                #print('No extracted text for data document %s' % doc.id)


    def test_every_extractedtext(self):
        ''''Loop through all the ExtractedText objects and confirm that the new
        create_detail_formset method returns forms based on the correct models
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd, EXTRA)
            extracted_text_form = ParentForm(instance=et)
            child_formset = ChildForm(instance=et)
            # Compare the model of the child formset's QuerySet to the model
            # of the ExtractedText object's child objects
            dd_child_model  = get_extracted_models(dd.data_group.group_type.code)[1]
            childform_model = child_formset.__dict__.get('queryset').__dict__.get('model')
            self.assertEqual(dd_child_model, childform_model)

    def test_curated_chemical(self):
        ''''Confirm that if an ExtractedChemical record has been matched to DSSToxLookup, the 
            DSSToxLookup fields are displayed in the card
            This checks every data document.
        '''
        for et in ExtractedText.objects.all():
            dd = et.data_document
            ParentForm, ChildForm = create_detail_formset(dd)
            child_formset = ChildForm(instance=et)
            #print('Data doc %s , Group Type: %s ' % (dd.id, dd.data_group.type ))
            for form in child_formset.forms:
                if dd.data_group.type in ['CO','UN']:
                    ec = form.instance
                    if ec.dsstox is not None:
                        self.assertTrue( 'true_cas' in form.fields )
                        self.assertTrue( 'SID' in form.fields )
                    else:
                        self.assertFalse( 'true_cas' in form.fields )
                        self.assertFalse( 'SID' in form.fields )
                else:
                    self.assertFalse( 'true_cas' in form.fields )
            
    def test_num_forms(self):
        ''''Assure that the number of child forms is appropriate for the group
        type.
        '''
        group_models = {
                        'CO': ExtractedChemical,
                        'FU': ExtractedFunctionalUse,
                        'HP': ExtractedHabitsAndPractices,
                        'CP': ExtractedListPresence,
                        'HH': ExtractedHHRec
        }
        for code, model in group_models.items():
            if DataDocument.objects.filter(
                                document_type__group_type__code=code,
                                extractedtext__isnull=False
            ):

                doc = DataDocument.objects.filter(
                                    document_type__group_type__code=code,
                                    extractedtext__isnull=False
                ).first()
                response = self.client.get(
                                    reverse('data_document',kwargs={'pk': doc.pk})
                )
                num_forms = response.context['detail_formset'].total_form_count()
                children = model.objects.filter(
                                    extracted_text=doc.extractedtext
                ).count()

                if doc.detail_page_editable:
                    error = (f'{model.__module__} should have one more forms'
                                                                ' than instances')
                    self.assertEqual(num_forms, children + 1, error)
                else:
                    error = (f'{model.__module__} should have the same number'
                                                        ' of forms as instances')
                    self.assertEqual(num_forms, children, error)


/n/n/n/dashboard/tests/functional/test_datagroup_detail.py/n/nfrom lxml import html
from importlib import import_module

from django.test import Client
from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from dashboard.views.data_group import ExtractionScriptForm, DataGroupForm
from django.core.files.uploadedfile import SimpleUploadedFile
from django.contrib.auth.models import User
from django.test import Client
from importlib import import_module
from django.db.models import Max

from dashboard.forms import *

from dashboard.models import *

class DataGroupDetailTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_detail_form_load(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(self.objects.doc.matched,
                    ('Document should start w/ matched False'))
        self.assertFalse(self.objects.doc.extracted,
                    ('Document should start w/ extracted False'))
        self.assertFalse(response.context['datagroup'].all_matched(),
                    ('UploadForm should be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(), (
                    'UploadForm should not be included in the page!'))
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertTrue(response.context['datagroup'].all_matched(),
                    ('UploadForm should not be included in the page!'))
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_detail_template_fieldnames(self):
        pk = self.objects.dg.pk
        self.assertEqual(str(self.objects.dg.group_type),'Composition',
        'Type of DataGroup needs to be ""composition"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name',
                'report_funcuse','raw_min_comp','raw_max_comp', 'unit_type',
                'ingredient_rank', 'raw_central_comp'],
                ""Fieldnames passed are incorrect!"")
        self.objects.gt.title = 'Functional use'
        self.objects.gt.code = 'FU'
        self.objects.gt.save()
        self.assertEqual(str(self.objects.dg.group_type),'Functional use',
            'Type of DataGroup needs to be ""FU"" for this test.')
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertEqual(response.context['extract_fields'],
                ['data_document_id','data_document_filename',
                'prod_name','doc_date','rev_num', 'raw_category',
                 'raw_cas', 'raw_chem_name','report_funcuse'],
                ""Fieldnames passed are incorrect!"")

    def test_unidentifed_group_type(self):
        pk = self.objects.dg.pk
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertIsInstance(response.context['extract_form'],
                                            ExtractionScriptForm,
                    ('ExtractForm should be included in the page!'))
        self.objects.gt.code = 'UN'
        self.objects.gt.save()
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertFalse(response.context['extract_form'],
                    ('ExtractForm should not be included in the page!'))

    def test_bulk_create_products_form(self):
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
                'Product linked to all DataDocuments, no bulk_create needed.')
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        doc.matched = True
        self.objects.doc.matched = True
        doc.save()
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        self.assertIn('Bulk Create', response.content.decode(),
                            ""Bulk create button should be present."")
        p = Product.objects.create(upc='stub_47',data_source=self.objects.ds)
        ProductDocument.objects.create(document=doc, product=p)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 0,
        'Product linked to all DataDocuments, no bulk_create needed.')
        self.objects.dg.group_type = GroupType.objects.create(
                                                title='Habits and practices')
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertNotIn('Bulk Create', response.content.decode(),
                            (""Bulk button shouldn't be present w/ ""
                            ""Habits and practices group_type.""))

    def test_bulk_create_post(self):
        '''test the POST to create Products and link if needed'''
        # create a new DataDocument with no Product
        doc = DataDocument.objects.create(data_group=self.objects.dg)
        response = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertEqual(response.context['bulk'], 1,
                'Not all DataDocuments linked to Product, bulk_create needed')
        new_stub_id = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        response = self.client.post(f'/datagroup/{self.objects.dg.pk}/',
                                                                {'bulk':1})
        self.assertEqual(response.context['bulk'], 0,
                'Products linked to all DataDocuments, no bulk_create needed.')
        product = ProductDocument.objects.get(document=doc).product
        self.assertEqual(product.title, 'unknown',
                                        'Title should be unknown in bulk_create')
        
        self.assertEqual(product.upc, f'stub_%s' % new_stub_id,
                                    'UPC should be created for second Product')

    def test_upload_note(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('Please limit upload to <600 documents at one time', response,
                      'Note to limit upload to <600 should be on the page')

    def test_extracted_count(self):
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('0 extracted', response,
                      'Data Group should contain a count of 0 total extracted documents')
        self.objects.doc.extracted = True
        self.objects.doc.save()
        response = self.client.get(f'/datagroup/{DataGroup.objects.first().id}/').content.decode('utf8')
        self.assertIn('1 extracted', response,
                      'Data Group should contain a count of 1 total extracted documents')

    def test_delete_doc_button(self):
        url = f'/datagroup/{DataGroup.objects.first().id}/'
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-trash""></span>'
        self.assertIn(span, response,
                      'Trash button should be present if not matched.')
        self.objects.doc.matched = True
        self.objects.doc.save()
        response = self.client.get(url).content.decode('utf8')
        span = '<span class=""oi oi-circle-check"" style=""color:green;""></span>'
        self.assertIn(span, response,
                      'Check should be present if matched.')

    def test_detail_table_headers(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertIn('<th>Product</th>', response,
                      'Data Group should have Product column.')
        fu = GroupType.objects.create(title='Functional use')
        self.objects.dg.group_type = fu
        self.objects.dg.save()
        response = self.client.get(f'/datagroup/{pk}/').content.decode('utf8')
        self.assertNotIn('<th>Product</th>', response,
                      'Data Group should have Product column.')

    def test_detail_datasource_link(self):
        pk = self.objects.dg.pk
        response = self.client.get(f'/datagroup/{pk}/')
        self.assertContains(response,'<a href=""/datasource/',
                    msg_prefix='Should be able to get back to DataSource from here.')

    def test_edit_redirect(self):
        dgpk = self.objects.dg.pk
        dspk = str(self.objects.ds.pk)
        gtpk = str(self.objects.gt.pk)
        data = {'name': ['Changed Name'],
                'group_type': [gtpk],
                'downloaded_by': [str(User.objects.get(username='Karyn').pk)],
                'downloaded_at': ['08/20/2017'],
                'data_source': [dspk]}
        response = self.client.post(f'/datagroup/edit/{dgpk}/', data=data)
        self.assertEqual(response.status_code, 302,
                                         ""User is redirected to detail page."")
        self.assertEqual(response.url, f'/datagroup/{dgpk}/',
                                         ""Should go to detail page."")

class DataGroupDetailTestWithFixtures(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.client.login(username='Karyn', password='specialP@55word')

    def test_download_raw_comp_data(self):
        # Ability to download, by data group, a csv file of raw extracted chemical composition data.
        # Download button would appear on data group detail page,
        # Download button would appear if any data documents have extracted text.
        # Only applies for data group type Composition. (group_type = 2)
        # Unidentified is excluded as of issue #502
        dg_co = DataGroup.objects.filter(group_type__code = 'CO').first()
        resp = self.client.get(f'/datagroup/%s/' % dg_co.id)
        self.assertIn(b'Download Raw', resp.content)

        # Test download on all data groups with ExtractedChemicals, whether
        # they are CO or UN
        dg_ids = DataDocument.objects.filter(
            id__in=ExtractedChemical.objects.all().values('extracted_text_id')
            ).order_by().values_list('data_group_id',flat=True).distinct()

        for dg_id in dg_ids:
            #resp = self.client.get(f'/datagroup/%s/' % dg_id)
            resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_id)
            self.assertEqual(resp.status_code, 200)

        # File downloaded must include [specified fields]
        resp = self.client.get(f'/datagroup/raw_extracted_records/%s/' % dg_ids[0])
        field_list = 'ExtractedChemical_id,raw_cas,raw_chem_name,raw_min_comp,raw_central_comp,raw_max_comp,unit_type'
        content = list(i.decode('utf-8') for i in resp.streaming_content)
        self.assertIn(field_list, content[1])
/n/n/n/dashboard/tests/functional/test_extracted_qa.py/n/nfrom django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import QAGroup, ExtractedText



class ExtractedQaTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_group_creation(self):
        # test the assignment of a qa_group to extracted text objects
        pk = self.objects.extext.pk
        self.assertIsNone(self.objects.extext.qa_group)
        self.assertEqual(len(QAGroup.objects.all()),0)
        pk = self.objects.extext.extraction_script.pk
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        self.assertEqual(response.status_code,200)
        qa_group = QAGroup.objects.get(
                        extraction_script=self.objects.extext.extraction_script)
        ext = ExtractedText.objects.get(qa_group=qa_group)
        self.assertIsNotNone(ext.qa_group)
        response = self.client.get(f'/qa/extractedtext/{ext.pk}/')

    def test_qa_approval_redirect(self):
        # first need to create a QAGroup w/ this get request.
        self.client.get(f'/qa/extractionscript/{self.objects.exscript.pk}/')
        pk = self.objects.extext.pk
        response = self.client.post(f'/qa/extractedtext/{pk}/',{'approve':[47]})
        self.assertEqual(response.url, '/qa/extractionscript/',(""User should be redirected to ""
                                ""QA homepage after last extext is approved.""))
/n/n/n/dashboard/tests/functional/test_faceted_search.py/n/nfrom django.test import TestCase
from django.test.client import Client
from lxml import html

from django.urls import resolve
from django.contrib.auth.models import User
from dashboard.tests.loader import fixtures_standard


class FacetedSearchTest(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.c = Client()

    def test_faceted_search_excludes_chemicals(self):
        response = self.c.get('/find/?q=ethyl')
        self.assertContains(response, 'Data Document')
        self.assertNotContains(response, 'Extracted Chemical')
        self.assertNotContains(response, 'DSSTox Substance')

    def test_faceted_search_returns_upc(self):
        response = self.c.get('/find/?q=avcat')
        self.assertContains(response, 'stub_1845')


    def test_group_type_facet(self):
        response = self.c.get('/find/?q=diatom')
        self.assertContains(response, 'Filter by Group Type')

        response = self.c.get('/find/?q=diatom&group_type=Unidentified')
        self.assertContains(response, 'Showing 1 - 20 of')

        response = self.c.get('/find/?q=diatom&group_type=BadGroupName')
        self.assertContains(response, 'Sorry, no result found')

    def test_faceted_search_renders_div(self):
        response = self.c.get('/find/?q=terro')
        self.assertNotContains(response, '<table')
        self.assertContains(response, '<div class=""results-wrapper"">')

    def test_product_facet_returns(self):
        response = self.c.get('/find/?q=insecticide')
        brands = response.content.count(b'name=""brand_name""')
        # default set to options = {""size"": 0} in /dashboard/views/search.py
        self.assertTrue(brands>10, ('There should be ~143 product returns '
                                                        'for this search term'))
/n/n/n/dashboard/tests/functional/test_qa_seed_data.py/n/nfrom django.test import Client
from dashboard.tests.loader import *
from django.test import TestCase, override_settings, RequestFactory
from dashboard.models import DataDocument, Script, ExtractedText, ExtractedChemical, QAGroup
from django.db.models import Count


@override_settings(ALLOWED_HOSTS=['testserver'])
class TestQaPage(TestCase):
    fixtures = fixtures_standard

    def setUp(self):
        self.factory = RequestFactory()
        self.client.login(username='Karyn', password='specialP@55word')

    def test_qa_begin(self):
        """"""
        Check that starting the QA process flips the variable on the Script
        """"""
        self.assertFalse(Script.objects.get(pk=5).qa_begun,
                         'The Script should have qa_begun of False at the beginning')
        response = self.client.get('/qa/extractionscript/5/')
        self.assertTrue(Script.objects.get(pk=5).qa_begun,
                        'qa_begun should now be true')

    def test_new_qa_group_urls(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 15 has one ExtractedText object
        pk = 15
        response = self.client.get(f'/qa/extractionscript/{pk}/')
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertIn(f'/qa/extractedtext/{et.pk}/'.encode(), response.content)
        # After opening the URL, the following should be true:
        # One new QA group should be created
        group_count = QAGroup.objects.filter(extraction_script_id=pk).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(Script.objects.get(pk=15).qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        group_pk = QAGroup.objects.get(extraction_script_id=pk).pk
        et = ExtractedText.objects.filter(extraction_script=pk).first()
        self.assertTrue(et.qa_group_id == group_pk)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/15/\'> Continue QA"".encode(), response.content)

    def test_qa_script_without_ext_text(self):
        # Begin from the QA index page
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""/qa/extractionscript/15/'> Begin QA"".encode(), response.content)
        # Script 9 has no ExtractedText objects
        pk = 9
        # a user will see no link on the QA index page, but it's still
        # possible to enter the URL
        response = self.client.get(f'/qa/extractionscript/{pk}/', follow=True)
        self.assertEqual(response.status_code, 200)

    def test_data_document_qa(self):
        # Open the QA page for a Composition ExtractedText record that has no QA group
        # and is in a Script with < 100 documents
        scr = Script.objects.annotate(num_ets=Count('extractedtext')).filter(
            num_ets__lt=100).filter(script_type='EX').first()
        pk = ExtractedText.objects.filter(qa_group=None).filter(extraction_script=scr
                                                                ).filter(
            data_document__data_group__group_type__code='CO').first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')

        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        group_count = QAGroup.objects.filter(extraction_script=scr).count()
        self.assertTrue(group_count == 1)
        # The ExtractionScript's qa_begun property should be set to True
        self.assertTrue(scr.qa_begun)
        # The ExtractedText object should be assigned to the QA Group
        new_group = QAGroup.objects.get(extraction_script=scr)
        et = ExtractedText.objects.get(pk=pk)
        self.assertTrue(et.qa_group == new_group)
        # The link on the QA index page should now say ""Continue QA""
        response = self.client.get(f'/qa/extractionscript/')
        self.assertIn(
            f""'/qa/extractionscript/{scr.pk}/\'> Continue QA"".encode(), response.content)

        # Open the QA page for an ExtractedText record that has no QA group and
        # is related to a script with over 100 documents
        scr = Script.objects.annotate(num_ets=Count(
            'extractedtext')).filter(num_ets__gt=100).first()
        pk = ExtractedText.objects.filter(extraction_script=scr).first().pk
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        scr = ExtractedText.objects.get(pk=pk).extraction_script
        # After opening the QA link from the data document detail page, the
        # following should be true:
        # One new QA group should be created
        new_group = QAGroup.objects.get(extraction_script=scr)

        # There should be a lot of ExtractedText records assigned to the QA Group
        initial_qa_count = ExtractedText.objects.filter(
            qa_group=new_group).count()
        self.assertTrue(initial_qa_count > 100)

        # Select a document that shares a Script with the
        # QA Group created above BUT DOES NOT BELONG TO THE QA GROUP
        pk = ExtractedText.objects.filter(
            extraction_script_id=scr.id).filter(qa_group=None).first().pk
        # Open its QA page via the /datdocument/qa path
        response = self.client.get(f'/qa/extractedtext/{pk}/')
        # Make sure that the number of documents in the QA Group has increased
        self.assertGreater(ExtractedText.objects.filter(
            qa_group=new_group).count(), initial_qa_count)

    def test_habitsandpractices(self):
        # Begin from the QA index page
        response = self.client.get(f'/habitsandpractices/54/')
        self.assertContains(response, '<b>Add New Habit and Practice</b>')

    def test_dd_link(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractedtext/5', follow=True)
        self.assertIn(b'/datadocument/5', response.content)

    def test_approval(self):
        # Open the Script page to create a QA Group
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Follow the first approval link
        response = self.client.get('/qa/extractedtext/7', follow=True)
        # print(response.context['extracted_text'])

    def test_hidden_fields(self):
        '''ExtractionScript 15 includes a functional use data group with pk = 5.
        Its QA page should hide the composition fields '''
        # Create the QA group by opening the Script's page
        response = self.client.get('/qa/extractionscript/15/', follow=True)
        # Open the DataGroup's first QA approval link
        response = self.client.get('/qa/extractedtext/5/', follow=True)
        # A raw_cas field should be in the page
        self.assertIn(
            b'<input type=""text"" name=""rawchem-1-raw_cas""', response.content)
        # There should not be any unit_type field in the functional use QA display
        self.assertNotIn(
            b'<input type=""text"" name=""rawchem-1-unit_type""', response.content)
        # The values shown should match the functional use record, not the chemical record
        self.assertIn(b'Functional Use Chem1', response.content)

        # Go back to a different ExtractionScript
        response = self.client.get('/qa/extractionscript/5', follow=True)
        # Open the QA page for a non-FunctionalUse document
        response = self.client.get('/qa/extractedtext/7/', follow=True)
        # This page should include a unit_type input form
        self.assertIn(b'rawchem-1-unit_type', response.content)

    def test_cpcat_qa(self):
        # Begin from the Chemical Presence QA index page
        response = self.client.get(f'/qa/chemicalpresence/')
        self.assertIn(
            f""/qa/chemicalpresencegroup/49/\'> View Chemical Presence Lists"".encode(), response.content)

        response = self.client.get(
            f'/qa/chemicalpresencegroup/49', follow=True)
        # The table should include the ""Begin QA"" link
        self.assertIn(
            f'/qa/extractedtext/254781/""> Begin QA'.encode(), response.content)

        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 0)
        response = self.client.get(f'/qa/extractedtext/254781/', follow=True)
        # Navigating to the extractedtext QA page should cause
        # the sampled child records to be flagged with qa_flag=True
        elps = ExtractedListPresence.objects.filter(
            extracted_text__data_document_id=254781)
        self.assertEqual(elps.filter(qa_flag=True).count(), 30)

        # The QA page should only show the flagged records
        elp_flagged = elps.filter(qa_flag=True).first()
        self.assertIn(elp_flagged.raw_cas.encode(), response.content)

        elp_not_flagged = elps.filter(qa_flag=False).first()
        self.assertNotIn(elp_not_flagged.raw_cas.encode(), response.content)

    def test_every_extractedtext_qa(self):
        # Attempt to open a QA page for every ExtractedText record
        for et in ExtractedText.objects.all():
            response = self.client.get(f'/qa/extractedtext/%s' % et.data_document_id, follow=True)
            if response.status_code != 200:
                print(et.data_document_id)
            self.assertEqual(response.status_code, 200)/n/n/n/dashboard/tests/integration/test_browser_edits.py/n/nfrom lxml import html

from django.test import TestCase
from dashboard.tests.loader import load_model_objects, fixtures_standard
from django.contrib.staticfiles.testing import StaticLiveServerTestCase

from dashboard.models import *
from selenium import webdriver
from django.conf import settings
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as ec


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestEditsWithSeedData(StaticLiveServerTestCase):
    fixtures = fixtures_standard

    def setUp(self):
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_break_curation(self):
        '''
        Changing the raw_cas or raw_chemname on a RawChem record with a related DssToxLookup should cause
        the relationship to be deleted.
        '''
        # currently uses a single data document
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            rc_id = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]').get_attribute('value')
            true_cas = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-true_cas""]').get_attribute('value')
            rc = RawChem.objects.get(pk=rc_id)
            self.assertEqual(true_cas, rc.dsstox.true_cas,
                             'The displayed True CAS should match the object attribute')
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_cas""]')
            raw_cas_input.send_keys('changed cas')
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()
            rc = RawChem.objects.get(pk=rc_id)   # reload the rawchem record
            self.assertEqual(
                None, rc.dsstox, 'The same rawchem record should now have nothing in its dsstox link')

    def test_new_chem(self):
        '''
        Adding a new ExtractedChemical without a unit type should return a validation error
        '''
        # currently ""loops"" over just a single data document. Other cases can be added
        ets_with_curation = ExtractedText.objects.filter(
            rawchem__dsstox__isnull=False).filter(pk=245401)
        for et in ets_with_curation:
            doc_qa_link = f'/qa/extractedtext/%s/' % et.data_document_id
            self.browser.get(self.live_server_url + doc_qa_link)

            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            # edit the Raw CAS field
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # Save the edits
            save_button.send_keys(""\n"")
            # Check for the error message after clicking Save
            wait.until(ec.visibility_of(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')))
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertTrue(""errorlist"" in card_div.get_attribute(""innerHTML""))

            # Try editing a new record correctly
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()
            # wait for the Save button to be clickable
            wait = WebDriverWait(self.browser, 10)
            save_button = wait.until(
                ec.element_to_be_clickable((By.XPATH, ""//*[@id='save']"")))
            raw_cas_input = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]')
            raw_cas_input.send_keys('test raw cas')
            # The unit_type field is the only required one
            unit_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-unit_type""]'))
            unit_type_select.select_by_index(1)

            save_button.send_keys(""\n"")
            # Check for the absence of an error message after clicking Save
            parent_div = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-1-raw_cas""]/parent::*')
            card_div = parent_div.find_element_by_xpath(
                '../..')
            self.assertFalse(
                ""errorlist"" in card_div.get_attribute(""innerHTML""))

    def test_redirects(self):
        '''
        Editing the data document type should return the user to the page on which the edits were made
        '''
        for doc_id in [7]:
            # QA Page
            doc_qa_link = f'/qa/extractedtext/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_qa_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            option = doc_type_select.first_selected_option
            doc_type_select.select_by_visible_text(""ingredient disclosure"")
            self.assertIn(doc_qa_link, self.browser.current_url)

            # Data Document Detail Page
            doc_detail_link = f'/datadocument/%s/' % doc_id
            self.browser.get(self.live_server_url + doc_detail_link)
            doc_type_select = Select(self.browser.find_element_by_xpath(
                '//*[@id=""id_document_type""]'))
            doc_type_select.select_by_visible_text(""MSDS"")
            self.assertIn(doc_detail_link, self.browser.current_url)

    def test_qa_approval(self):
        '''
        Test the QA process in the browser
        1. Open the QA page for an ExtractedText record
        2. Edit one of the child records
        3. Attempt to approve the document without a QA note
        4. Add a note

        5. Approve
        '''
        for doc_id in [7,      # Composition
                       5,      # Functional Use
                       254781,  # Chemical Presence List
                       354783,  # HHE Report
                       ]:

            # QA Page
            qa_url = self.live_server_url + f'/qa/extractedtext/{doc_id}/'
            self.browser.get(qa_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-toggle-edit""]').click()

            # Modify the first raw_chem_name field's value

            raw_chem = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-raw_chem_name""]')
            # Wait for the field to be editable
            wait = WebDriverWait(self.browser, 10)
            raw_chem_name_field = wait.until(ec.element_to_be_clickable(
                (By.XPATH, ""//*[@id='id_rawchem-0-raw_chem_name']"")))

            old_raw_chem_name = raw_chem_name_field.get_attribute('value')

            # Get the detailed child record's ID
            rawchem_id_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_rawchem-0-rawchem_ptr""]')
            rawchem_id = rawchem_id_field.get_attribute('value')
            # print(rawchem_id)

            raw_chem_name_field.send_keys(' edited')
            # save changes
            self.browser.find_element_by_xpath('//*[@id=""save""]').click()

            # Confirm the changes in the ORM
            rc = RawChem.objects.get(pk=rawchem_id)
            self.assertEqual(rc.raw_chem_name, f'%s edited' %
                             old_raw_chem_name, 'The raw_chem_name field should have changed')

            et = ExtractedText.objects.get(pk=doc_id)
            # print(et.data_document.data_group.group_type)
            self.assertTrue(
                et.qa_edited, 'The qa_edited attribute should be True')

            # Click Approve without any notes and confirm validation failure
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            # The QA notes field should be invalid
            qa_notes_field = self.browser.find_element_by_xpath(
                '//*[@id=""id_qa_notes""]')
            self.assertIn('is-invalid', qa_notes_field.get_attribute('class'))
            et.refresh_from_db()
            self.assertFalse(
                et.qa_checked, 'The qa_checked attribute should be False')

            # Add the mandatory QA note
            qa_notes_field.send_keys('Some QA Notes')
            # Click ""Approve"" again
            self.browser.find_element_by_xpath('//*[@id=""approve""]').click()
            et.refresh_from_db()
            self.assertTrue(
                et.qa_checked, 'The qa_checked attribute should be True')


    def test_datadoc_add_extracted(self):
        '''
        Test that when a datadocument has no ExtractedText,
        the user can add one in the browser
        1.
        '''

        for doc_id in [155324   # CO record with no ExtractedText
                       ]:
            # QA Page
            dd_url = self.live_server_url + f'/datadocument/{doc_id}/'
            self.browser.get(dd_url)
            # Activate the edit mode
            self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]').click()

            # Verify that the modal window appears by finding the Cancel button
            # The modal window does not immediately appear, so the browser
            # should wait for the button to be clickable
            wait = WebDriverWait(self.browser, 10)
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            self.assertEqual(""Cancel"", cancel_button.text,
                             'The Cancel button should say Cancel')
            cancel_button.click()
            # Verify that no ExtractedText record was created
            self.assertEqual(0, ExtractedText.objects.filter(
                data_document_id=doc_id).count(),
                ""the count of ExtractedText records related to the \
                data document should be zero"")

            # Wait for the modal div to disappear
            edit_modal = wait.until(
                ec.invisibility_of_element(
                    (By.XPATH, '//*[@id=""extextModal""]')
                )
            )
            # Click the Add button again to reopen the editor
            add_button = self.browser.find_element_by_xpath(
                '//*[@id=""btn-add-or-edit-extracted-text""]')
            add_button.click()
            # Once again, check that the controls on the modal form are clickable
            # before trying to interact with them
            cancel_button = wait.until(
                ec.element_to_be_clickable(
                    (By.XPATH, ""//*[@id='extracted-text-modal-cancel']"")
                )
            )
            prod_name_box = self.browser.find_element_by_id(
                'id_prod_name')
            # Add a prod_name value to the box
            prod_name_box.send_keys('Fake Product')
            save_button = self.browser.find_element_by_id(
                'extracted-text-modal-save')
            save_button.click()
            # Confirm the presence of the new ExtractedText record
            et = ExtractedText.objects.get(data_document_id=doc_id)
            self.assertEqual('Fake Product', et.prod_name,
                             ""The prod_name of the new object should match what was entered"")


/n/n/n/dashboard/tests/integration/test_user_experience.py/n/nfrom lxml import html
from django.test import TestCase
from dashboard.tests.loader import load_model_objects
from dashboard.models import *
import os
import csv
import time
import unittest
import collections
import json
import re
from selenium import webdriver
from selenium.webdriver.support.select import Select
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from django.conf import settings
from django.contrib.staticfiles.testing import StaticLiveServerTestCase
from dashboard.models import *


def log_karyn_in(object):
    '''
    Log user in for further testing.
    '''
    object.browser.get(object.live_server_url + '/login/')
    body = object.browser.find_element_by_tag_name('body')
    object.assertIn('Please sign in', body.text)
    username_input = object.browser.find_element_by_name(""username"")
    username_input.send_keys('Karyn')
    password_input = object.browser.find_element_by_name(""password"")
    password_input.send_keys('specialP@55word')
    object.browser.find_element_by_class_name('btn').click()


class TestIntegration(StaticLiveServerTestCase):

    def setUp(self):
        self.objects = load_model_objects()
        if settings.TEST_BROWSER == 'firefox':
            self.browser = webdriver.Firefox()
        else:
            self.browser = webdriver.Chrome()
        log_karyn_in(self)

    def tearDown(self):
        self.browser.quit()

    def test_hem(self):
        for i in range(27):
            ds = DataSource.objects.create(title=f'Test_DS_{i}')
        list_url = self.live_server_url + '/datasources/'
        self.browser.get(list_url)
        row_count = len(self.browser.find_elements_by_xpath(""//table[@id='sources']/tbody/tr""))
        self.assertEqual(row_count, 25, 'Should be 25 datasources in the table')
        # go to edit page from datasource list
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")
        detail_url = self.live_server_url + f'/datasource/{ds.pk}'
        self.browser.get(detail_url)
        # go to edit page from datasource detail
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), detail_url,
                         ""User should go back to detail view when clicking cancel"")
        self.browser.find_element_by_name('submit').click()
        self.assertIn('/datasource/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

        num_pucs = len(PUC.objects.filter(kind='FO'))
        self.browser.get(self.live_server_url)
        import time
        time.sleep(3)  # or however long you think it'll take you to scroll down to bubble chart
        bubbles = self.browser.find_elements_by_class_name('bubble')
        self.assertEqual(num_pucs, len(bubbles), ('There should be a circle'
                                                  'drawn for every PUC'))

    def test_datagroup(self):
        list_url = self.live_server_url + '/datagroups/'
        self.browser.get(list_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), list_url,
                         ""User should go back to list view when clicking cancel"")

        dg = DataGroup.objects.first()
        ds_detail_url = f'{self.live_server_url}/datasource/{dg.data_source.pk}'
        self.browser.get(ds_detail_url)
        self.browser.find_elements_by_xpath('//*[@title=""edit""]')[1].click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), ds_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        dg_detail_url = f'{self.live_server_url}/datagroup/{dg.pk}/'
        self.browser.get(dg_detail_url)
        self.browser.find_element_by_xpath('//*[@title=""edit""]').click()
        btn = self.browser.find_element_by_name('cancel')
        self.assertEqual(btn.get_attribute(""href""), dg_detail_url,
                         ""User should go back to detail view when clicking cancel"")

        edit_url = f'{self.live_server_url}/datagroup/edit/{dg.pk}/'
        self.browser.get(edit_url)
        self.browser.find_element_by_name('cancel').click()
        self.assertIn('/datagroups/', self.browser.current_url,
                      ""User should always return to detail page after submit"")

    def test_product(self):
        p = self.objects.p
        puc = self.objects.puc
        tag = self.objects.pt
        PUCToTag.objects.create(content_object=puc, tag=tag)
        ProductToPUC.objects.create(product=p, puc=puc)
        url = self.live_server_url + f'/product/{p.pk}/'
        self.browser.get(url)
        submit = self.browser.find_element_by_id('tag_submit')
        self.assertFalse(submit.is_enabled(), ""Button should be disabled"")
        tag = self.browser.find_element_by_class_name('taggit-tag')
        tag.click()
        self.assertTrue(submit.is_enabled(), ""Button should be enabled"")

    def test_field_exclusion(self):
        doc = self.objects.doc
        # The element should not appear on the QA page
        qa_url = self.live_server_url + f'/qa/extractedtext/{doc.pk}/'
        self.browser.get(qa_url)
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_cas""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-true_chemname""]')
        with self.assertRaises(NoSuchElementException):
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-SID""]')
        # make sure the test can pick up one that should be there
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-raw_cas""]')
        except NoSuchElementException:
            self.fail(""Absence of raw_cas element raised exception"")

        # The element should appear on the datadocument page
        dd_url = self.live_server_url + f'/datadocument/{doc.pk}/'
        self.browser.get(dd_url)
        try:
            self.browser.find_element_by_xpath('//*[@id=""id_rawchem-0-weight_fraction_type""]')
        except NoSuchElementException:
            self.fail(""Absence of weight_fraction_type element raised exception"")

/n/n/n/dashboard/tests/loader.py/n/nfrom django.utils import timezone
from django.contrib.auth.models import User

from dashboard.models import *

fixtures_standard = [ '00_superuser',
                      '01_lookups',
                      '02_datasource',
                      '03_datagroup',
                      '04_PUC', 
                      '05_product',
                      '06_datadocument',
                      '07_rawchem_etc',
                       '08_script',
                    '09_productdocument',  
                    '10_habits_and_practices',
                     '11_habits_and_practices_to_puc',
                      '12_product_to_puc',
                        '13_puc_tag'
                        ]

class dotdict(dict):
    """"""dot.notation access to dictionary attributes""""""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

def load_model_objects():
    user = User.objects.create_user(username='Karyn',
                                        password='specialP@55word')
    superuser = User.objects.create_superuser(username='SuperKaryn',
                                              password='specialP@55word',
                                              email='me@epa.gov')
    ds = DataSource.objects.create(title='Data Source for Test',
                                        estimated_records=2, state='AT',
                                        priority='HI')
    script = Script.objects.create(title='Test Download Script',
                                        url='http://www.epa.gov/',
                                        qa_begun=False, script_type='DL')
    exscript = Script.objects.create(title='Test Extraction Script',
                                   url='http://www.epa.gov/',
                                   qa_begun=False, script_type='EX')
    gt = GroupType.objects.create(title='Composition', code='CO')
    dg = DataGroup.objects.create(name='Data Group for Test',
                                        description='Testing...',
                                        data_source = ds,
                                        download_script=script,
                                        downloaded_by=user,
                                        downloaded_at=timezone.now(),
                                        group_type=gt,
                                        csv='register_records_matching.csv',
                                        url='https://www.epa.gov')
    dt = DocumentType.objects.create(title='MSDS',
                                    code='MS', group_type=gt)

    doc = DataDocument.objects.create(title='test document',
                                            data_group=dg,
                                            document_type=dt,
                                            filename='example.pdf')
    p = Product.objects.create(data_source=ds,
                                upc='Test UPC for ProductToPUC')

    puc = PUC.objects.create(gen_cat='Test General Category',
                              prod_fam='Test Product Family',
                              prod_type='Test Product Type',
                             description='Test Product Description',
                             last_edited_by = user,
                             kind='FO')

    extext = ExtractedText.objects.create(
                                    prod_name='Test Extracted Text Record',
                                    data_document=doc,
                                    extraction_script=exscript
                                    )
    ut = UnitType.objects.create(title='percent composition')
    wft = WeightFractionType.objects.create(title= 'reported', description= 'reported')
    ec = ExtractedChemical.objects.create(extracted_text=extext,
                                        unit_type=ut,
                                        weight_fraction_type = wft,
                                        raw_chem_name= 'Test Chem Name',
                                        raw_cas='test_cas'
                                        )
    rc = ec.rawchem_ptr
    ing = Ingredient.objects.create(lower_wf_analysis = 0.123456789012345,
                                    central_wf_analysis = 0.2,
                                    upper_wf_analysis = 1,
                                    script = script,
                                    rawchem_ptr = rc)
    
    pt = PUCTag.objects.create(name=""Test PUC Attribute"")
    pd = ProductDocument.objects.create(product=p, document=doc)
    ehp = ExtractedHabitsAndPractices.objects.create(extracted_text=extext,
                                                     product_surveyed='Test Product Surveyed',
                                                     prevalence='Continuous')


    return dotdict({'user':user,
                    'superuser':superuser,
                    'ds':ds,
                    'script':script,
                    'exscript':exscript,
                    'dg':dg,
                    'doc':doc,
                    'p':p,
                    'puc':puc,
                    'extext':extext,
                    'ut':ut,
                    'wft':wft,
                    'rc':rc,
                    'ec':ec,
                    'pt':pt,
                    'pd':pd,
                    'ing':ing,
                    'dt':dt,
                    'gt':gt,
                    'ehp':ehp
                    })
/n/n/n/dashboard/tests/unit/test_extracted_text.py/n/nfrom django.test import TestCase
from django.utils import timezone
from django.contrib.auth.models import User
from django.core.exceptions import ValidationError

from dashboard.tests.loader import load_model_objects
from dashboard.models import ExtractedText, QANotes


class ExtractedTest(TestCase):

    def setUp(self):
        self.objects = load_model_objects()

    def test_extracted_doc_date_validation(self):
        # check validation for proper length string
        text = ExtractedText(doc_date= 'Wednesday, January 21, 2014',
                                data_document=self.objects.doc,
                                extraction_script=self.objects.script)
        self.assertRaises(ValidationError, text.clean())
        # check validation not thrown for arbitrary date string less than 25 chars
        text = ExtractedText(doc_date= 'January 1984',
                             data_document=self.objects.doc,
                             extraction_script=self.objects.script)
        try:
            text.clean()
        except ValidationError:
            self.fail(""clean() raised ExceptionType unexpectedly!"")

        # check validation not thrown if doc_date is null
        text = ExtractedText(data_document=self.objects.doc,
                                extraction_script=self.objects.script)
        try:
            text.clean()
        except ValidationError:
            self.fail(""clean() raised ExceptionType unexpectedly!"")

    def test_extracted_text_qa_notes(self):
        self.objects.extext.qa_edited = True
        note = QANotes.objects.create(extracted_text=self.objects.extext)
        self.assertEqual(note.qa_notes, None)
        self.assertRaises(ValidationError, note.clean)

    def test_long_qa_notes(self):
        self.objects.extext.qa_edited = True
        note = QANotes.objects.create(extracted_text=self.objects.extext)
        self.assertEqual(note.qa_notes, None)
        note.qa_notes = ""A short QA note""
        try:
            note.clean()
        except Exception as ex:
            template = ""An exception of type {0} occurred. Arguments:\n{1!r}""
            message = template.format(type(ex).__name__, ex.args)

        long_note = 'A long QA note' * 200
        note.qa_notes = long_note
        try:
            note.clean()
        except Exception as ex:
            template = ""An exception of type {0} occurred. Arguments:\n{1!r}""
            message = template.format(type(ex).__name__, ex.args)
/n/n/n/dashboard/tests/unit/test_habits_n_practices.py/n/nfrom django.urls import resolve
from django.test import TestCase
from django.http import HttpRequest

from lxml import html

from dashboard import views
from dashboard.models import *
from dashboard.forms import create_detail_formset
from dashboard.tests.loader import load_model_objects



class HabitViewTest(TestCase):
    multi_db = True
    def setUp(self):
        self.objects = load_model_objects()


    def test_habitsandpractices(self):
        found = resolve(f'/habitsandpractices/{self.objects.doc.pk}/')
        self.assertEqual(found.func, views.habitsandpractices)

    def test_link_habitandpractice_to_puc(self):
        found = resolve(f'/link_habitandpractice_to_puc/{self.objects.ehp.pk}/')
        self.assertEqual(found.func, views.link_habitsandpractices)

    def test_product_surveyed_field(self):
        self.objects.gt.code = 'HP'
        self.objects.gt.save()
        _, HnPFormSet = create_detail_formset(self.objects.doc)
        data = {'habits-TOTAL_FORMS':'2',
                'habits-INITIAL_FORMS':'1',
                'habits-MIN_NUM_FORMS':'0',
                'habits-MAX_NUM_FORMS':'1000',
                'habits-0-id': self.objects.ehp.pk,
                'habits-0-product_surveyed':'',
        }
        hp_formset = HnPFormSet(data, prefix='habits')
        self.assertFalse(hp_formset.is_valid())

        data = {'habits-TOTAL_FORMS':'2',
                'habits-INITIAL_FORMS':'1',
                'habits-MIN_NUM_FORMS':'0',
                'habits-MAX_NUM_FORMS':'1000',
                'habits-0-id': self.objects.ehp.pk,
                'habits-0-product_surveyed':'monster trucks',
        }
        hp_formset = HnPFormSet(data, prefix='habits')

        self.assertTrue(hp_formset.is_valid())

    def test_edit_hnp_detail(self):
        self.objects.exscript.title = 'Manual (dummy)'
        self.objects.exscript.save()
        self.client.login(username='Karyn', password='specialP@55word')
        pk = self.objects.doc.pk
        response = self.client.get(f'/habitsandpractices/{pk}/')
        self.assertNotContains(response, 'Raw Category', html=True)

        # Ensure there are Cancel and Back buttons with the correct URL to return to the DG detail page
        self.assertContains(response, f'href=""/datagroup/{self.objects.dg.pk}/"" role=""button"">Cancel</a>')
        self.assertContains(response, f'href=""/datagroup/{self.objects.dg.pk}/"" role=""button"">Back</a>')

        # Ensure that the URL above responds correctly
        response2 = self.client.get(f'/datagroup/{self.objects.dg.pk}/')
        self.assertContains(response2, 'Data Group Detail: Data Group for Test')
/n/n/n/dashboard/views/dashboard.py/n/nimport csv
import datetime
from dateutil.relativedelta import relativedelta

from django.http import HttpResponse
from django.shortcuts import render
from django.db.models import Count, F, DateField, DateTimeField
from django.db.models.functions import Trunc
from django.contrib.auth.decorators import login_required

from dashboard.models import *

from dashboard.models import *

current_date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d')
chart_start_datetime = datetime.datetime(datetime.datetime.now().year - 1, min(12,datetime.datetime.now().month + 1), 1)


def index(request):
    stats = {}
    stats['datagroup_count'] = DataGroup.objects.count()
    stats['datasource_count'] = DataSource.objects.count()

    stats['datadocument_count'] = DataDocument.objects.count()
    stats['datadocument_with_extracted_text_percent'] =\
        DataDocument.objects.filter(extracted = True).count()/DataDocument.objects.count()*100
    stats['datadocument_count_by_date'] = datadocument_count_by_date()
    stats['datadocument_count_by_month'] = datadocument_count_by_month()
    stats['product_count'] = Product.objects.count()
    stats['dss_tox_count'] = DSSToxLookup.objects.count()
    stats['chemical_count'] = ExtractedChemical.objects.count()
    stats['product_with_puc_count'] = ProductToPUC.objects.values('product_id').distinct().count()
    stats['product_with_puc_count_by_month'] = product_with_puc_count_by_month()
    return render(request, 'dashboard/index.html', stats)


def datadocument_count_by_date():
    # Datasets to populate linechart with document-upload statistics
    # Number of datadocuments, both overall and by type, that have been uploaded as of each date
    select_upload_date = {""upload_date"": """"""date(dashboard_datadocument.created_at)""""""}
    document_stats = {}
    document_stats['all'] = list(DataDocument.objects.extra(select=select_upload_date) \
                                 .values('upload_date') \
                                 .annotate(document_count = Count('id')) \
                                 .order_by('upload_date'))
    document_stats_by_type = DataDocument.objects.extra(select=select_upload_date) \
        .values('upload_date') \
        .annotate(source_type = F('document_type__title'), document_count = Count('id')) \
        .order_by('upload_date')
    document_stats['product'] = list(document_stats_by_type.filter(source_type = 'product'))
    document_stats['msds_sds'] = list(document_stats_by_type.filter(source_type = 'msds/sds'))
    for type in {'all'}:
        document_count = 0
        for item in document_stats[type]:
            if isinstance(item['upload_date'], datetime.date):
                item['upload_date'] = datetime.date.strftime((item['upload_date']), '%Y-%m-%d')
            document_count += item['document_count']
            item['document_count'] = document_count
        # if final record isn't for current date, create one
        for item in document_stats[type][len(document_stats[type])-1:]:
            if item['upload_date'] != current_date:
                document_stats[type].append({'upload_date': current_date
                                                , 'document_count': document_count})
    return document_stats


def datadocument_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year
    document_stats = list(DataDocument.objects.filter(created_at__gte=chart_start_datetime)\
        .annotate(upload_month = (Trunc('created_at', 'month', output_field=DateTimeField()))) \
        .values('upload_month') \
        .annotate(document_count = (Count('id'))) \
        .values('document_count', 'upload_month') \
        .order_by('upload_month'))
    if len(document_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(document_stats) or document_stats[i]['upload_month'] != chart_month:
                document_stats.insert(i, {'document_count': '0', 'upload_month': chart_month})
    return document_stats


def product_with_puc_count_by_month():
    # GROUP BY issue solved with https://stackoverflow.com/questions/8746014/django-group-by-date-day-month-year

    product_stats = list(ProductToPUC.objects
        .filter(created_at__gte=chart_start_datetime)
        .annotate(
            puc_assigned_month = (Trunc('created_at', 'month', output_field=DateField()))
        )
        .values('puc_assigned_month')
        .annotate(product_count=Count('product', distinct=True))
        .order_by('puc_assigned_month')
        )

    if len(product_stats) < 12:
        for i in range(0, 12):
            chart_month = chart_start_datetime + relativedelta(months=i)
            if i + 1 > len(product_stats) or product_stats[i]['puc_assigned_month'] != chart_month:
                product_stats.insert(i, {'product_count': '0', 'puc_assigned_month': chart_month})
    return product_stats


def download_PUCs(request):
    '''This view gets called every time we call the index view and is used to
    populate the bubble plot. It is also used to download all of the PUCs in 
    csv form. The ""bubbles"" parameter in the request will either be ""True"" or 
    ""None"", it's worth noting that if when making the call to here from the 
    index page we were to use ?bubbles=False it would also give us the filtered
    PUCs because the if expression is just checking whether that parameter is 
    there.
    '''
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""PUCs.csv""'
    bubbles = request.GET.get('bubbles')
    writer = csv.writer(response)
    cols = ['gen_cat','prod_fam','prod_type','description','PUC_type','num_prods']
    writer.writerow(cols)
    pucs = PUC.objects.filter(kind='FO') if bubbles else PUC.objects.all()
    for puc in pucs:
        row = [ puc.gen_cat,
                puc.prod_fam, 
                puc.prod_type, 
                puc.description, 
                puc.get_level(), 
                puc.product_count
                ]
        writer.writerow(row)

    return response
/n/n/n/dashboard/views/data_document.py/n/nfrom django import forms
from django.http import HttpResponse
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from djqscsv import render_to_csv_response

from dashboard.forms import *
# if this goes to 0, tests will fail because of what num form we search for
from factotum.settings import EXTRA
from dashboard.models import *


@login_required()
def data_document_detail(request, pk):
    template_name = 'data_document/data_document_detail.html'
    doc = get_object_or_404(DataDocument, pk=pk, )
    code = doc.data_group.group_type.code
    edit = 1 if doc.detail_page_editable else 0
    # edit adds an extra record to the formset, but is also a switch in the
    # template and to add the delete input, this will only work if we add one at
    # a time...
    ParentForm, ChildFormSet = create_detail_formset(
        doc, extra=edit, can_delete=edit)
    document_type_form = DocumentTypeForm(request.POST or None, instance=doc)
    qs = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    document_type_form.fields['document_type'].queryset = qs
    context = {'doc': doc,
               'edit': edit,
               'document_type_form': document_type_form}
    if doc.is_extracted:

        extracted_text = ExtractedText.objects.get_subclass(pk=doc.pk) 
        extracted_text_form = ParentForm(instance=extracted_text)
        child_formset = ChildFormSet(instance=extracted_text)

        if not edit:
            for form in child_formset.forms:
                for field in form.fields:
                    form.fields[field].widget.attrs['disabled'] = True

        context.update(
            {'edit_text_form': ParentForm(instance=extracted_text),
             'extracted_text': extracted_text,
             'detail_formset': child_formset}
        )

        colors = ['#d6d6a6', '#dfcaa9', '#d8e5bf'] * 47
        color = (hex for hex in colors)
        for form in child_formset.forms:
            form.color = next(color)
    else:
        context['edit_text_form'] = ParentForm()
    return render(request, template_name, context)


@login_required()
def save_doc_form(request, pk):
    '''Writes changes to the data document form 
    
    The request object should have a 'referer' key to redirect the 
    browser to the appropriate place after saving the edits

    Invoked by changing the document type in the data document detail view or the
    extracted text QA page template
    '''

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    document_type_form = DocumentTypeForm(request.POST, instance=doc)
    if document_type_form.is_valid() and document_type_form.has_changed():
        document_type_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_note(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    doc_note = request.POST['dd_note']
    doc.note = doc_note
    doc.save()
    return redirect('data_document', pk=pk)


@login_required()
def save_ext_form(request, pk):
    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ExtractedTextForm, _ = create_detail_formset(doc)
    extracted_text = ExtractedText.objects.get_subclass(pk=pk)
    ext_text_form = ExtractedTextForm(request.POST, instance=extracted_text)
    if ext_text_form.is_valid() and ext_text_form.has_changed():
        ext_text_form.save()
    return redirect(referer, pk=pk)


@login_required()
def data_document_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    doc = get_object_or_404(DataDocument, pk=pk)
    datagroup_id = doc.data_group_id
    if request.method == 'POST':
        doc.delete()
        return redirect('data_group_detail', pk=datagroup_id)
    return render(request, template_name, {'object': doc})


@login_required
def dg_dd_csv_view(request, pk):
    qs = DataDocument.objects.filter(data_group_id=pk)
    filename = DataGroup.objects.get(pk=pk).name
    return render_to_csv_response(qs, filename=filename, append_datestamp=True)


@login_required
def data_document_edit(request, pk):

    referer = request.POST['referer'] if request.POST['referer'] else 'data_document'
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        return redirect(referer, pk=doc.pk)
    else:
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_text_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    ParentForm, _ = create_detail_formset(doc, extra=0, can_delete=False)
    model = ParentForm.Meta.model
    script = Script.objects.get(title__icontains='Manual (dummy)', script_type='EX')
    exttext, _ = model.objects.get_or_create(extraction_script=script,
                                             data_document_id=pk)
    form = ParentForm(request.POST, instance=exttext)
    if form.is_valid():
        form.save()
        doc.extracted = True
        doc.save()
        return redirect('data_document', pk=doc.pk)
    else:
        extext.delete()
        return HttpResponse(""Houston, we have a problem."")


@login_required
def extracted_child_edit(request, pk):
    doc = get_object_or_404(DataDocument, pk=pk)
    _, ChildFormSet = create_detail_formset(doc, extra=1, can_delete=True)
    formset = ChildFormSet(request.POST, instance=doc.extractedtext)
    if formset.is_valid():
        formset.save()
    return redirect('data_document', pk=doc.pk)
/n/n/n/dashboard/views/data_group.py/n/nimport os
import csv
import zipfile
from itertools import islice
from collections import OrderedDict
from djqscsv import render_to_csv_response
from pathlib import Path

from django import forms
from django.urls import reverse
from django.conf import settings
from django.core.files import File
from django.core.exceptions import ValidationError
from django.core.files.storage import FileSystemStorage
from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404
from django.http import HttpResponse
from django.core.paginator import Paginator

from dashboard.models import *
from dashboard.forms import (DataGroupForm,
                             ExtractionScriptForm,
                             CleanCompDataForm,
                             create_detail_formset,
                             include_extract_form,
                             include_clean_comp_data_form)
from dashboard.utils import get_extracted_models, clean_dict, update_fields
from django.db.models import Max


@login_required()
def data_group_list(request, template_name='data_group/datagroup_list.html'):
    datagroup = DataGroup.objects.all()
    data = {}
    data['object_list'] = datagroup
    return render(request, template_name, data)

@login_required()
def data_group_detail(request, pk,
                      template_name='data_group/datagroup_detail.html'):
    dg = get_object_or_404(DataGroup, pk=pk, )
    dg.doc_types = DocumentType.objects.filter(group_type=dg.group_type)
    docs = dg.datadocument_set.get_queryset()#this needs to be updated after matching...
    prod_link = ProductDocument.objects.filter(document__in=docs)
    page = request.GET.get('page')
    paginator = Paginator(docs, 50) # TODO: make this dynamic someday in its own ticket
    store = settings.MEDIA_URL + str(dg.fs_id)
    ext = ExtractedText.objects.filter(data_document_id__in=docs).first()
    context = { 'datagroup'      : dg,
                'documents'      : paginator.page(1 if page is None else page),
                'all_documents'  : docs, # this used for template download
                'extract_fields' : dg.get_extracted_template_fieldnames(),
                'ext_err'        : {},
                'clean_comp_err'        : {},
                'extract_form'   : include_extract_form(dg),
                'clean_comp_data_form'   : include_clean_comp_data_form(dg),
                'bulk'           : len(docs) - len(prod_link),
                'msg'            : '',
                }
    if request.method == 'POST' and 'upload' in request.POST:
        # match filename to pdf name
        matched_files = [f for d in docs for f
                in request.FILES.getlist('multifiles') if f.name == d.filename]
        if not matched_files:
            context['msg'] = ('There are no matching records in the '
                                                        'selected directory.')
            return render(request, template_name, context)
        zf = zipfile.ZipFile(dg.zip_file, 'a', zipfile.ZIP_DEFLATED)
        while matched_files:
            f = matched_files.pop(0)
            doc = DataDocument.objects.get(filename=f.name,
                                            data_group=dg.pk)
            if doc.matched:
                continue
            doc.matched = True
            doc.save()
            fs = FileSystemStorage(store + '/pdf')
            afn = doc.get_abstract_filename()
            fs.save(afn, f)
            zf.write(store + '/pdf/' + afn, afn)
        zf.close()
        form = include_extract_form(dg)
        # update docs so it appears in the template table w/ ""matched"" docs
        context['all_documents'] = dg.datadocument_set.get_queryset()
        context['extract_form'] = form
        context['msg'] = 'Matching records uploaded successfully.'
    if request.method == 'POST' and 'extract_button' in request.POST:
        extract_form = ExtractionScriptForm(request.POST,
                                                request.FILES,dg_type=dg.type)
        if extract_form.is_valid():
            csv_file = request.FILES.get('extract_file')
            script_pk = int(request.POST['script_selection'])
            script = Script.objects.get(pk=script_pk)
            info = [x.decode('ascii','ignore') for x in csv_file.readlines()]
            table = csv.DictReader(info)
            missing =  list(set(dg.get_extracted_template_fieldnames())-
                                                        set(table.fieldnames))
            if missing: #column names are NOT a match, send back to user
                context['msg'] = ('The following columns need to be added or '
                                            f'renamed in the csv: {missing}')
                return render(request, template_name, context)
            good_records = []
            ext_parent, ext_child = get_extracted_models(dg.type)
            for i, row in enumerate(csv.DictReader(info)):
                d = docs.get(pk=int(row['data_document_id']))
                d.raw_category = row.pop('raw_category')
                wft = request.POST.get('weight_fraction_type', None)
                if wft: # this signifies 'Composition' type
                    w = 'weight_fraction_type'
                    row[w] = WeightFractionType.objects.get(pk=int(wft))
                    unit_type_id = int(row['unit_type'])
                    row['unit_type'] = UnitType.objects.get(pk=unit_type_id)
                    rank = row['ingredient_rank']
                    row['ingredient_rank'] = None if rank == '' else rank
                ext, created = ext_parent.objects.get_or_create(data_document=d,
                                                    extraction_script=script)
                if not created and ext.one_to_one_check(row):
                    # check that there is a 1:1 relation ExtParent and DataDoc
                    col = 'cat_code' if hasattr(ext,'cat_code') else 'prod_name' 
                    err_msg = ['must be 1:1 with ""data_document_id"".']
                    context['ext_err'][i+1] = {col: err_msg}
                if created:
                    update_fields(row, ext)
                row['extracted_text'] = ext
                if (ext_child == ExtractedListPresence):
                    row['extracted_cpcat'] = ext.extractedtext_ptr
                row = clean_dict(row, ext_child)
                try:
                    ext.full_clean()
                    ext.save()
                    record = ext_child(**row)
                    record.full_clean()
                    good_records.append((d,ext,record))
                except ValidationError as e:
                    context['ext_err'][i+1] = e.message_dict
            if context['ext_err']: # if errors, send back with errors
                [e[1].delete() for e in good_records] # delete any created exts
                return render(request, template_name, context)
            if not context['ext_err']:  # no saving until all errors are removed
                for doc,text,record in good_records:
                    doc.extracted = True
                    doc.save()
                    text.save()
                    record.save()
                fs = FileSystemStorage(store)
                fs.save(str(dg)+'_extracted.csv', csv_file)
                context['msg'] = (f'{len(good_records)} extracted records '
                                                    'uploaded successfully.')
                context['extract_form'] = include_extract_form(dg)
    if request.method == 'POST' and 'bulk' in request.POST:
        # get the set of documents that have not been matched
        a = set(docs.values_list('pk',flat=True))
        b = set(prod_link.values_list('document_id',flat=True))
        # DataDocs to make products for...
        docs_needing_products = DataDocument.objects.filter(pk__in=list(a-b))
        stub = Product.objects.all().aggregate(Max('id'))[""id__max""] + 1
        for doc in docs_needing_products:
            # Try to name the new product from the ExtractedText record's prod_name
            try:
                ext = ExtractedText.objects.get(data_document_id=doc.id)
                if ext:
                    if ext.prod_name:
                        new_prod_title = ext.prod_name
                    else:
                        new_prod_title = None
            except ExtractedText.DoesNotExist:
                new_prod_title = None
            # If the ExtractedText record can't provide a title, use the DataDocument's title
            if not new_prod_title:
                if doc.title:
                    new_prod_title = '%s stub' % doc.title
                else:
                    new_prod_title = 'unknown'
            product = Product.objects.create(
                                    title=new_prod_title,
                                    upc=f'stub_{stub}',
                                    data_source_id=doc.data_group.data_source_id
                                    )
            ProductDocument.objects.create(product=product, document=doc)
            stub += 1
        context['bulk'] = 0
    if request.method == 'POST' and 'clean_comp_data_button' in request.POST:
        clean_comp_data_form = CleanCompDataForm(request.POST, request.FILES)
        if clean_comp_data_form.is_valid():
            script_pk = int(request.POST['script_selection'])
            script = Script.objects.get(pk=script_pk)
            csv_file = request.FILES.get('clean_comp_data_file')
            info = [x.decode('ascii','ignore') for x in csv_file.readlines()]
            table = csv.DictReader(info)
            missing =  list(set(dg.get_clean_comp_data_fieldnames())-
                                                        set(table.fieldnames))
            if missing: #column names are NOT a match, send back to user
                context['clean_comp_data_form'].collapsed = False
                context['msg'] = ('The following columns need to be added or '
                                            f'renamed in the csv: {missing}')
                return render(request, template_name, context)

            good_records = []
            for i, row in enumerate(csv.DictReader(info)):
                try:
                    extracted_chemical = ExtractedChemical.objects.get(rawchem_ptr=int(row['id']))
                except ExtractedChemical.DoesNotExist as e:
                    extracted_chemical = None
                    context['clean_comp_err'][i + 1] = {'id': ['No ExtractedChemical matches rawchem_ptr_id ' + row['id'], ]}
                    print('No ExtractedChemical matches rawchem_ptr_id %s' % row['id'])
                try:
                    ingredient = Ingredient.objects.get(rawchem_ptr=extracted_chemical.rawchem_ptr)
                except Ingredient.DoesNotExist as e:
                    ingredient = Ingredient(rawchem_ptr=extracted_chemical.rawchem_ptr)
                ingredient.lower_wf_analysis = row['lower_wf_analysis']
                ingredient.central_wf_analysis = row['central_wf_analysis']
                ingredient.upper_wf_analysis = row['upper_wf_analysis']
                ingredient.script = script
                try:
                    ingredient.full_clean()
                except ValidationError as e:
                    context['clean_comp_err'][i+1] = e.message_dict
                good_records.append(ingredient)
            if context['clean_comp_err']: # if errors, send back with errors
                context['clean_comp_data_form'].collapsed = False
                return render(request, template_name, context)
            if not context['clean_comp_err']:  # no saving until all errors are removed
                for ingredient in good_records:
                    ingredient.save()
                context['msg'] = (f'{len(good_records)} clean composition data records '
                                                    'uploaded successfully.')
                context['clean_comp_data_form'] = include_clean_comp_data_form(dg)
        else:
            context['clean_comp_data_form'].collapsed = False

    return render(request, template_name, context)


@login_required()
def data_group_create(request, pk,
                        template_name='data_group/datagroup_form.html'):
    datasource = get_object_or_404(DataSource, pk=pk)
    group_key = DataGroup.objects.filter(data_source=datasource).count() + 1
    default_name = '{} {}'.format(datasource.title, group_key)
    header = 'Create New Data Group For Data Source ""' + str(datasource) + '""'
    initial_values = {'downloaded_by' : request.user,
                      'name'          : default_name,
                      'data_source'   : datasource}
    if request.method == 'POST':
        form = DataGroupForm(request.POST, request.FILES,
                             user    = request.user,
                             initial = initial_values)
        if form.is_valid():
            # what's the pk of the newly created datagroup?
            datagroup = form.save()
            info = [x.decode('ascii',
                             'ignore') for x in datagroup.csv.readlines()]
            table = csv.DictReader(info)
            good_fields = ['filename','title','document_type',
                                                    'url','organization']
            if not table.fieldnames == good_fields:
                datagroup.csv.close()
                datagroup.delete()
                return render(request, template_name,
                              {'field_error': table.fieldnames,
                              'good_fields': good_fields,
                               'form': form})
            text = ['DataDocument_id,' + ','.join(table.fieldnames)+'\n']
            errors = []
            filenames = []
            count = 0
            for line in table: # read every csv line, create docs for each
                count+=1
                doc_type = DocumentType.objects.get(pk=1)
                code = line['document_type']
                if line['filename'] == '' :
                    errors.append([count,""Filename can't be empty!""])
                    continue
                if len(line['filename'])>255:
                    errors.append([count,""Filename too long!""])
                    continue
                if line['filename'] in filenames:
                    errors.append([count, ""Duplicate filename found in csv""])
                    continue
                if line['title'] == '': # updates title in line object
                    line['title'] = line['filename'].split('.')[0]
                if code == '':
                    errors.append([count,
                                    ""'document_type' field can't be empty""])
                    continue
                if DocumentType.objects.filter(group_type=datagroup.group_type,
                                                            code=code).exists():
                    doc_type = DocumentType.objects.get(
                                    group_type=datagroup.group_type,code=code)
                else:
                    errors.append([count,""DocumentType code doesn't exist.""])

                filenames.append(line['filename'])
                doc=DataDocument(filename=line['filename'],
                                 title=line['title'],
                                 document_type=doc_type,
                                 url=line['url'],
                                 organization=line['organization'],
                                 data_group=datagroup)
                doc.save()
                # update line to hold the pk for writeout
                text.append(str(doc.pk)+','+ ','.join(line.values())+'\n')
            if errors:
                datagroup.csv.close()
                datagroup.delete()
                return render(request, template_name, {'line_errors': errors,
                                                       'form': form})
            #Save the DG to make sure the pk exists
            datagroup.save()
            #Let's even write the csv first
            with open(datagroup.csv.path,'w') as f:
                myfile = File(f)
                myfile.write(''.join(text))
            #Let's explicitly use the full path for the actually writing of the zipfile
            new_zip_name = Path(settings.MEDIA_URL + ""/"" + str(datagroup.fs_id) + ""/"" + str(datagroup.fs_id) + "".zip"")
            new_zip_path = Path(settings.MEDIA_ROOT + ""/"" + str(datagroup.fs_id) + ""/"" + str(datagroup.fs_id) + "".zip"")
            zf = zipfile.ZipFile(str(new_zip_path), 'w',
                                 zipfile.ZIP_DEFLATED)
            datagroup.zip_file = new_zip_name
            zf.close()
            datagroup.save()
            return redirect('data_group_detail', pk=datagroup.id)
    else:
        groups = GroupType.objects.all()
        for group in groups:
            group.codes = DocumentType.objects.filter(group_type=group)
        form = DataGroupForm(user=request.user, initial=initial_values)
    context = {'form': form, 'header': header,
                'datasource': datasource, 'groups' : groups}
    return render(request, template_name, context)


@login_required()
def data_group_update(request, pk, template_name='data_group/datagroup_form.html'):
    # TODO: Resolve whether this form save ought to also update Datadocuments
    #  in the case the ""Register Records CSV file"" is updated.
    datagroup = get_object_or_404(DataGroup, pk=pk)
    form = DataGroupForm(request.POST or None, instance=datagroup)
    header = f'Update Data Group for Data Source ""{datagroup.data_source}""'
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_group_detail', pk=datagroup.id)
    form.referer = request.META.get('HTTP_REFERER', None)
    if datagroup.extracted_docs():
        form.fields['group_type'].disabled = True
    groups = GroupType.objects.all()
    for group in groups:
            group.codes = DocumentType.objects.filter(group_type=group)
    return render(request, template_name, {'datagroup': datagroup, 
                                            'form': form,
                                            'header': header,
                                            'groups': groups})

@login_required()
def data_group_delete(request, pk, template_name='data_source/datasource_confirm_delete.html'):
    datagroup = get_object_or_404(DataGroup, pk=pk)
    if request.method == 'POST':
        datagroup.delete()
        return redirect('data_group_list')
    return render(request, template_name, {'object': datagroup})

@login_required
def dg_pdfs_zip_view(request, pk):
    dg = DataGroup.objects.get(pk=pk)
    #print('opening zip file from %s' % dg.get_zip_url())
    zip_file_name = f'{dg.fs_id}.zip'
    zip_file = open(dg.get_zip_url(), 'rb')
    response = HttpResponse(zip_file, content_type='application/zip')
    response['Content-Disposition'] = 'attachment; filename=%s' % zip_file_name
    return response

@login_required
def data_group_registered_records_csv(request, pk):
    columnlist = ['filename','title','document_type','url','organization']
    dg = DataGroup.objects.filter(pk=pk).first()
    if dg:
        columnlist.insert(0, ""id"")
        qs = DataDocument.objects.filter(data_group_id=pk).values(*columnlist)
        return render_to_csv_response(qs, filename=(dg.get_name_as_slug() +
                                                    ""_registered_records.csv""),
                                  field_header_map={""id"": ""DataDocument_id""},
                                  use_verbose_names=False)
    else:
        qs = DataDocument.objects.filter(data_group_id=0).values(*columnlist)
        return render_to_csv_response(qs, filename=""registered_records.csv"",
                                        use_verbose_names=False)

@login_required()
def habitsandpractices(request, pk,
                      template_name='data_group/habitsandpractices.html'):
    doc = get_object_or_404(DataDocument, pk=pk, )
    script = Script.objects.get(title='Manual (dummy)', script_type='EX')
    extext, created = ExtractedText.objects.get_or_create(data_document=doc,
                                                    extraction_script=script)
    if created:
        extext.doc_date = 'please add...'
    ExtractedTextForm, HPFormSet = create_detail_formset(doc)
    # print(extext.pk)
    ext_form = ExtractedTextForm(request.POST or None, instance=extext)
    hp_formset = HPFormSet(request.POST or None, instance=extext, prefix='habits')
    context = {   'doc'         : doc,
                  'ext_form'    : ext_form,
                  'hp_formset'  : hp_formset,
                  }
    if request.method == 'POST' and 'save' in request.POST:
        if hp_formset.is_valid():
            hp_formset.save()
        if ext_form.is_valid():
            ext_form.save()
        doc.extracted = True
        doc.save()
        context = {   'doc'         : doc,
                      'ext_form'    : ext_form,
                      'hp_formset'  : hp_formset,
                      }
    return render(request, template_name, context)

@login_required
def dg_raw_extracted_records(request, pk):
    columnlist = ['extracted_text_id','id','raw_cas','raw_chem_name','raw_min_comp','raw_central_comp','raw_max_comp','unit_type__title']
    dg = DataGroup.objects.get(pk=pk)
    et = ExtractedText.objects.filter(data_document__data_group = dg).first()
    if et:
        dg_name = dg.get_name_as_slug()
        qs = ExtractedChemical.objects.filter(extracted_text__data_document__data_group_id=pk).values(*columnlist)
        #print('Writing %s records to csv' % len(qs) )
        return render_to_csv_response(qs, filename=(dg_name +
                                                    ""_raw_extracted_records.csv""),
                                  field_header_map={""id"": ""ExtractedChemical_id""},
                                  use_verbose_names=False)
    else:
        qs = ExtractedChemical.objects.filter(extracted_text__data_document__id=pk).values(*columnlist)
        return render_to_csv_response(qs, filename='raw_extracted_records.csv' ,
                                        use_verbose_names=False)
/n/n/n/dashboard/views/data_source.py/n/nfrom datetime import datetime

from django.contrib.auth.decorators import login_required
from django.shortcuts import render, redirect, get_object_or_404

from dashboard.forms import DataSourceForm, PriorityForm
from dashboard.models import DataSource, DataGroup, DataDocument
from .data_group import DataGroupForm
from django.db.models import Count, Q



@login_required()
def data_source_list(request, template_name='data_source/datasource_list.html'):
    datasources = DataSource.objects.all()
    ds_list, frm_list = [], []
    for ds in datasources:
        frm_list.append(PriorityForm(request.POST or None, instance=ds))
    registered = Count('datagroup__datadocument') 
    uploaded   = Count('datagroup__datadocument', filter=Q(datagroup__datadocument__matched=True))
    extracted  = Count('datagroup__datadocument__extractedtext')
    ds_list    = DataSource.objects.annotate(registered=registered).annotate(uploaded=uploaded, extracted=extracted)
    out = zip(ds_list, frm_list)
    if request.method == 'POST':
        datasource = DataSource.objects.get(pk=request.POST['ds_pk'])
        form = PriorityForm(request.POST or None, instance=datasource)
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
            return redirect('data_source_list')
    return render(request, template_name, {'object_list': out})


@login_required()
def data_source_detail(request, pk,
                        template_name='data_source/datasource_detail.html'):
    datasource = get_object_or_404(DataSource, pk=pk, )
    docs = DataDocument.objects.filter(data_group__in=DataGroup.objects.filter(data_source=datasource))
    datasource.registered = (len(docs)/float(datasource.estimated_records))*100
    datasource.uploaded = (len(docs.filter(matched=True))/float(
                                            datasource.estimated_records))*100

    form = PriorityForm(request.POST or None, instance=datasource)
    if request.method == 'POST':
        if form.is_valid():
            priority = form.cleaned_data['priority']
            datasource.priority = priority
            datasource.save()
    datagroup_list = DataGroup.objects.filter(data_source=pk)
    context =     {'object':             datasource,
                'datagroup_list':    datagroup_list,
                'form':             form}
    return render(request, template_name, context)


@login_required()
def data_source_create(request, template_name=('data_source/'
                                                'datasource_form.html')):
    form = DataSourceForm(request.POST or None)
    if form.is_valid():
        form.save()
        return redirect('data_source_list')
    return render(request, template_name, {'form': form})


@login_required()
def data_source_update(request, pk, template_name=('data_source/'
                                                    'datasource_form.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    form = DataSourceForm(request.POST or None, instance=datasource)
    if form.is_valid():
        if form.has_changed():
            form.save()
        return redirect('data_source_detail', pk=pk)
    form.referer = request.META.get('HTTP_REFERER', None)
    return render(request, template_name, {'form': form})

@login_required()
def data_source_delete(request, pk,
                        template_name=('data_source/'
                                        'datasource_confirm_delete.html')):
    datasource = get_object_or_404(DataSource, pk=pk)
    if request.method == 'POST':
        datasource.delete()
        return redirect('data_source_list')
    return render(request, template_name, {'object': datasource})
/n/n/n/dashboard/views/get_data.py/n/nimport csv
import logging
import datetime

from django import forms
from django.db import connection
from django.urls import reverse
from django.http import HttpResponse, HttpResponseRedirect
from django.contrib import messages
from django.shortcuts import render
from django.db.models import Count, Q, Value, IntegerField, Subquery, OuterRef, F, Sum
from django.forms.models import model_to_dict

from dashboard.models import *
from dashboard.forms import HabitsPUCForm


def get_data(request, template_name='get_data/get_data.html'):
    hnp = None
    form = HabitsPUCForm()
    context = { 'hnp' : hnp,
                'form': form,
                'first': None,
                }
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(pk=form['puc'].value())
            pucs = puc.get_the_kids()
            link_table = ExtractedHabitsAndPracticesToPUC
            links = link_table.objects.filter(PUC__in=pucs).values_list(
                                            'extracted_habits_and_practices',
                                            flat=True)
            hnp = ExtractedHabitsAndPractices.objects.filter(pk__in=links)
            context['form'] = form
            context['hnp'] = hnp if len(hnp)>0 else 0
            if len(hnp)>0:
                context['first'] = hnp[0].pk
    return render(request, template_name, context)


def stats_by_dtxsids(dtxs):
    """"""
    PUCS.n
    The number of unique PUCs (product categories) the chemical is associated with
    datadocs.n
    ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    the chemical is appears in""
    datadocs_w_wf.n
    ""The number of data documents with associated weight fraction data
    that the chemical appears in (weight fraction data may be reported or predicted data,
     i.e., predicted from an ingredient list)""
    products.n
    ""The number of products the chemical appears in, where a product is defined as a
    product entry in Factotum.""
    """"""
    # print('List of DTXSIDs provided:')
    # print(dtxs)


    # The number of unique PUCs (product categories) the chemical is associated with
    pucs_n = DSSToxLookup.objects.filter(sid__in=dtxs).\
        annotate(pucs_n=Count('curated_chemical__extracted_text__data_document__product__puc')).\
        values('sid','pucs_n').order_by()

    # ""The number of data documents (e.g.  MSDS, SDS, ingredient list, product label)
    # the chemical appears in
    dds_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
        annotate(sid=F('dsstox__sid'), dds_n=Count('extracted_text__data_document')).\
        values('sid','dds_n').order_by()

    #print('dds_n:')
    #print(dds_n)

    # The number of data documents with associated weight fraction data
    # that the chemical appears in (weight fraction data may be reported or predicted data,
    # i.e., predicted from an ingredient list)
    # This query only applies to ExtractedChemical objects, so the RawChem model can be bypassed
    wf_ecs = ExtractedChemical.objects.filter(dsstox__sid__in=dtxs).filter(
                Q(raw_max_comp__isnull=False) |
                Q(raw_min_comp__isnull=False) |
                Q(raw_central_comp__isnull=False)
            )
    dds_wf_n = DSSToxLookup.objects.filter(sid__in=dtxs).filter(curated_chemical__in=wf_ecs).\
        annotate(dds_wf_n=Count('curated_chemical__extracted_text_id', distinct=True)).\
        order_by().values('sid','dds_wf_n')






    # The number of products the chemical appears in, where a product is defined as a
    # product entry in Factotum.
    products_n = RawChem.objects.filter(dsstox__sid__in=dtxs).values('dsstox__sid').\
       annotate(products_n=Count('extracted_text__data_document__product')).\
       annotate(sid=F('dsstox__sid')).values('sid', 'products_n')

    # build a list of stats, starting with the pucs_n object
    stats = pucs_n\
    .annotate(dds_n=Value(-1, output_field=IntegerField())) \
    .annotate(dds_wf_n=Value(-1, output_field=IntegerField())) \
    .annotate(products_n=Value(-1, output_field=IntegerField())) 

    for row in stats:
        row['dds_n'] = int(dds_n.get(sid=row['sid'])['dds_n'] or 0)

        if not dds_wf_n.filter(sid=row['sid']):
            row['dds_wf_n'] = 0
        else:
            row['dds_wf_n'] = int(dds_wf_n.get(sid=row['sid'])['dds_wf_n'] or 0)
            
        row['products_n'] = int(products_n.get(sid=row['sid'])['products_n'] or 0)
        
    return stats

def download_chem_stats(stats):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""chem_summary_metrics_%s.csv""' % (datetime.datetime.now().strftime(""%Y%m%d""))

    writer = csv.writer(response)
    writer.writerow(['DTXSID',  'pucs_n', 'dds_n', 'dds_wf_n', 'products_n'])
    for stat in stats:
        writer.writerow([stat['sid'], stat['pucs_n'], stat['dds_n'], stat['dds_wf_n'], stat['products_n']])

    return response

def get_data_dsstox_csv_template(request):
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename=""dsstox_lookup_template.csv""'
    writer = csv.writer(response)
    writer.writerow(['DTXSID'])
    return response


def upload_dtxsid_csv(request):
    data = {}
    if ""GET"" == request.method:
        return render(request, ""get_data/get_data.html"", data)
    # if not GET, then proceed
    try:
        csv_file = request.FILES[""csv_file""]
        if not csv_file.name.endswith('.csv'):
            messages.error(request,'File is not CSV type')
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))
        #if file is too large, return
        if csv_file.multiple_chunks():
            messages.error(request,""Uploaded file is too big (%.2f MB)."" % (csv_file.size/(1000*1000),))
            return HttpResponseRedirect(reverse(""upload_dtxsid_csv""))

        file_data = csv_file.read().decode(""utf-8"")

        lines = file_data.split(""\n"")
        #loop over the lines
        dtxsids = []
        for line in lines:
            #print(line)
            if DSSToxLookup.objects.filter(sid=str.strip(line)).count() > 0:
                dtxsids.append(str.strip(line)) # only add DTXSIDs that appear in the database

    except Exception as e:
        logging.getLogger(""error_logger"").error(""Unable to upload file. ""+repr(e))
        messages.error(request,""Unable to upload file. ""+repr(e))

    stats = stats_by_dtxsids(dtxsids)
    #stats  = {'pucs_n': 0, 'dds_n': 0, 'dds_wf_n': 0, 'products_n': 0}
    resp = download_chem_stats(stats)
    #print(resp)
    return resp


/n/n/n/dashboard/views/habits_n_practices.py/n/nfrom dal import autocomplete

from django.shortcuts import (render, redirect, get_object_or_404,
                                                HttpResponseRedirect)
from django.utils.translation import ugettext_lazy as _
from django.contrib.auth.decorators import login_required

from dashboard.models import *
from dashboard.forms import HabitsPUCForm, create_detail_formset


@login_required()
def habitsandpractices(request, pk,
                      template_name='data_group/habitsandpractices.html'):
    doc = get_object_or_404(DataDocument, pk=pk, )
    script = Script.objects.get(title='Manual (dummy)', script_type='EX')
    extext, created = ExtractedText.objects.get_or_create(data_document=doc,
                                                    extraction_script=script)
    if created:
        extext.doc_date = 'please add...'
    ExtractedTextForm, HnPFormSet = create_detail_formset(doc)
    ext_form = ExtractedTextForm(request.POST or None, instance=extext)
    hp_formset = HnPFormSet(request.POST or None,
                            instance=extext, prefix='habits')
    if request.method == 'POST' and 'save' in request.POST:
        if hp_formset.is_valid() and ext_form.is_valid():
            if not doc.extracted:
                doc.extracted = True
                doc.save()
            hp_formset.save()
            ext_form.save()
            return HttpResponseRedirect(f'/habitsandpractices/{doc.pk}')
    context = {   'doc'         : doc,
                  'ext_form'    : ext_form,
                  'hp_formset'  : hp_formset,
                  }
    return render(request, template_name, context)


@login_required()
def link_habitsandpractices(request, pk,
                        template_name='data_group/habitsandpractices_to_puc.html'):
    hnp = get_object_or_404(ExtractedHabitsAndPractices, pk=pk, )
    form = HabitsPUCForm()
    if request.method == 'POST':
        form = HabitsPUCForm(request.POST)
        if form.is_valid():
            puc = PUC.objects.get(id=form['puc'].value())
            # make sure the PUC link doesn't already exist
            if not ExtractedHabitsAndPracticesToPUC.objects.filter(
                    PUC=puc,
                    extracted_habits_and_practices=hnp).exists():
                ExtractedHabitsAndPracticesToPUC.objects.create(
                        PUC=puc,
                        extracted_habits_and_practices=hnp
                )
                form = HabitsPUCForm()
    linked = ExtractedHabitsAndPracticesToPUC.objects.filter(
                    extracted_habits_and_practices=hnp).values('PUC')
    hnp_puc = PUC.objects.filter(pk__in=linked)
    print(hnp_puc)
    context = {'hnp': hnp,
                'form': form,
                'hnp_puc': hnp_puc,
    }
    return render(request, template_name, context)
/n/n/n/dashboard/views/product_curation.py/n/nfrom urllib import parse

from django.urls import resolve
from django.utils import timezone, safestring
from django.shortcuts import redirect
from django.db.models import Count, Q
from django.shortcuts import render, get_object_or_404
from django.contrib.auth.decorators import login_required
from django.forms import ModelForm
from dashboard.models import *
from dashboard.forms import (ProductPUCForm, ProductLinkForm, 
                            BulkProductPUCForm, BulkProductTagForm, 
                            BulkPUCForm, ProductForm)
from taggit.forms import TagField
from taggit_labels.widgets import LabelWidget
from django.core.paginator import Paginator
from django.db.models import Max


class FilteredLabelWidget(LabelWidget):
    # overriding django-taggit-label function to display subset of tags
    def tag_list(self, tags):
        # must set form_instance in form __init__()
        puc = self.form_instance.instance.get_uber_puc() or None
        qs = self.model.objects.filter(content_object=puc,assumed=False)
        filtered = [unassumed.tag for unassumed in qs]
        return [(tag.name, 'selected taggit-tag' if tag.name in tags else 'taggit-tag')
                for tag in filtered]


class ProductTagForm(ModelForm):
    tags = TagField(required=False, widget=FilteredLabelWidget(model=PUCToTag))

    class Meta:
        model = Product
        fields = ['tags']

    def __init__(self, *args, **kwargs):
        super(ProductTagForm, self).__init__(*args, **kwargs)
        self.fields['tags'].widget.form_instance = self


@login_required()
def product_curation_index(request, template_name='product_curation/product_curation_index.html'):
    # List of all data sources which have had at least 1 data
    # document matched to a registered record
    data_sources = DataSource.objects.annotate(uploaded=Count('datagroup__datadocument'))\
        .filter(uploaded__gt=0)
    # A separate queryset of data sources and their related products without PUCs assigned
    # Changed in issue 232. Instead of filtering products based on their prod_cat being null,
    #   we now exclude all products that have a product_id contained in the ProductToPUC object set
    qs_no_puc = Product.objects.values('data_source').exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).\
        filter(data_source__isnull=False).annotate(no_category=Count('id')).order_by('data_source')
    # Convert the queryset to a list
    list_no_puc = [ds_no_puc for ds_no_puc in qs_no_puc]

    for ds in data_sources:
        try:
            ds.no_category = next((item for item in list_no_puc if item[""data_source""] == ds.id), False)['no_category']
        except:
            ds.no_category = 0
        dgs = ds.datagroup_set.all()
        for dg in dgs:
            dg.unlinked = dg.datadocument_set.count() - dg.datadocument_set.filter(productdocument__document__isnull=False).count()
        ds.data_groups = dgs
    return render(request, template_name, {'data_sources': data_sources})


@login_required()
def category_assignment(request, pk, template_name=('product_curation/'
                                                'category_assignment.html')):
    """"""Deliver a datasource and its associated products""""""
    ds = DataSource.objects.get(pk=pk)
    products = ds.source.exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))).order_by('-created_at')
    return render(request, template_name, {'datasource': ds, 'products': products})


@login_required()
def link_product_list(request,  pk, template_name='product_curation/link_product_list.html'):
    dg = DataGroup.objects.get(pk=pk)
    documents = dg.datadocument_set.filter(productdocument__document__isnull=True)
    npage = 20 # TODO: make this dynamic someday in its own ticket
    paginator = Paginator(documents, npage) # Show npage data documents per page
    page = request.GET.get('page')
    page = 1 if page is None else page
    docs_page = paginator.page(page)
    return render(request, template_name, {'documents':docs_page, 'datagroup':dg})


@login_required()
def link_product_form(request, pk, template_name=('product_curation/'
                                                    'link_product_form.html')):
    doc = DataDocument.objects.get(pk=pk)
    ds_id = doc.data_group.data_source_id
    initial = {   'upc': ('stub_' + str(Product.objects.all().aggregate(Max('id'))[""id__max""] + 1)),
        'document_type': doc.document_type,
           'return_url': request.META.get('HTTP_REFERER')}
    form = ProductLinkForm(initial=initial)
    # limit document type options to those matching parent datagroup group_type
    queryset = DocumentType.objects.filter(group_type=doc.data_group.group_type)
    form.fields['document_type'].queryset = queryset
    if request.method == 'POST':
        form = ProductLinkForm(request.POST or None)
        if form.is_valid():
            upc = form['upc'].value()
            title = form['title'].value()
            product, created = Product.objects.get_or_create(upc=upc,
                                                        data_source_id = ds_id)
            if created:
                product.title = title
                product.manufacturer = form['manufacturer'].value()
                product.brand_name = form['brand_name'].value()
                product.upc = form['upc'].value()
                product.size = form['size'].value()
                product.color = form['color'].value()
                product.save()
            if not ProductDocument.objects.filter(document=doc,
                                                    product=product).exists():
                p = ProductDocument(product=product, document=doc)
                p.save()
            document_type = form['document_type'].value()
            if document_type != doc.document_type: # update if user changes
                doc.document_type = DocumentType.objects.get(pk=document_type)
                doc.save()
            if 'datadocument' in form['return_url'].value():
                return redirect('data_document', pk=doc.pk)
            else:
                return redirect('link_product_list', pk=doc.data_group.pk)
        else:
            pass #form is invalid
    return render(request, template_name,{'document': doc, 'form': form})


@login_required()
def detach_puc_from_product(request, pk):
    p = Product.objects.get(pk=pk)
    pp = ProductToPUC.objects.get(product=p)
    pp.delete()
    return redirect('product_detail', pk=p.pk)


@login_required()
def bulk_assign_tag_to_products(request):
    template_name = 'product_curation/bulk_product_tag.html'
    products = {}
    msg = ''
    puc_form = BulkPUCForm(request.POST or None)
    form = BulkProductTagForm()
    if puc_form['puc'].value():
        puc = PUC.objects.get(pk = puc_form['puc'].value())
        assumed_tags = puc.get_assumed_tags()
        puc2tags = (PUCToTag.objects.filter(content_object=puc,assumed=False).
                                                values_list('tag', flat=True))
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        prod2pucs = (ProductToPUC.objects.filter(puc = puc).
                                        values_list('product_id', flat=True))
        products = Product.objects.filter(id__in=prod2pucs)
    if request.method == 'POST' and 'save' in request.POST:
        form = BulkProductTagForm(request.POST or None)
        form.fields['tag'].queryset = PUCTag.objects.filter(id__in=puc2tags)
        if form.is_valid():
            assign_tag = PUCTag.objects.filter(id=form['tag'].value())
            tags = assumed_tags | assign_tag
            product_ids = form['id_pks'].value().split("","")
            for id in product_ids:
                product = Product.objects.get(id=id)
                #add the assumed tags to the update
                for tag in tags:
                    ProductToTag.objects.update_or_create(tag=tag,
                                                        content_object=product)
            puc_form = BulkPUCForm()
            form = BulkProductTagForm()
            tag = assign_tag[0]
            msg = f'The ""{tag.name}"" Attribute was assigned to {len(product_ids)} Product(s).'
            if assumed_tags:
                msg += (' Along with the assumed tags: '
                            f'{"" | "".join(x.name for x in assumed_tags)}')
            products = {}
    return render(request, template_name, {'products': products,
                                            'puc_form': puc_form,
                                            'form': form, 
                                            'msg': msg})


@login_required()
def bulk_assign_puc_to_product(request, template_name=('product_curation/'
                                                      'bulk_product_puc.html')):
    max_products_returned = 50
    q = safestring.mark_safe(request.GET.get('q', '')).lstrip()
    if q > '':
        p = (Product.objects
            .filter( Q(title__icontains=q) | Q(brand_name__icontains=q) )
            .exclude(id__in=(ProductToPUC.objects.values_list('product_id', flat=True))
            )[:max_products_returned])
        full_p_count = Product.objects.filter(Q(title__icontains=q) | Q(brand_name__icontains=q)).count()
    else:
        p = {}
        full_p_count = 0
    form = BulkProductPUCForm(request.POST or None)
    if form.is_valid():
        puc = PUC.objects.get(id=form['puc'].value())
        product_ids = form['id_pks'].value().split("","")
        for id in product_ids:
            product = Product.objects.get(id=id)
            ProductToPUC.objects.create(puc=puc, product=product, classification_method='MB',
                                    puc_assigned_usr=request.user)
    form['puc'].label = 'PUC to Assign to Selected Products'
    return render(request, template_name, {'products': p, 'q': q, 'form': form, 'full_p_count': full_p_count})


@login_required()
def assign_puc_to_product(request, pk, template_name=('product_curation/'
                                                      'product_puc.html')):
    p = Product.objects.get(pk=pk)
    p2p = ProductToPUC.objects.filter(classification_method='MA', product=p).first()
    form = ProductPUCForm(request.POST or None, instance=p2p)
    if form.is_valid():
        if p2p:
            p2p.save()
        else:
            puc = PUC.objects.get(id=form['puc'].value())
            p2p = ProductToPUC.objects.create(puc=puc, product=p, classification_method='MA',
                                        puc_assigned_usr=request.user)
        referer = request.POST.get('referer') if request.POST.get('referer') else 'category_assignment'
        pk = p2p.product.pk if referer == 'product_detail' else p2p.product.data_source.pk
        return redirect(referer, pk=pk)
    form.referer = resolve(parse.urlparse(request.META['HTTP_REFERER']).path).url_name\
        if 'HTTP_REFERER' in request.META else 'category_assignment'
    form.referer_pk = p.id if form.referer == 'product_detail' else p.data_source.id
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_detail(request, pk):
    template_name = 'product_curation/product_detail.html'
    p = get_object_or_404(Product, pk=pk, )
    tagform = ProductTagForm(request.POST or None, instance=p)
    tagform['tags'].label = ''
    puc = p.get_uber_puc()
    assumed_tags = puc.get_assumed_tags() if puc else PUCTag.objects.none()
    if tagform.is_valid():
        tagform.save()
    docs = p.datadocument_set.order_by('-created_at')
    return render(request, template_name, {'product'      : p,
                                            'puc'         : puc,
                                            'tagform'     : tagform,
                                            'docs'        : docs,
                                            'assumed_tags': assumed_tags
                                            })


@login_required()
def product_update(request, pk, template_name=('product_curation/'
                                               'product_edit.html')):
    p = Product.objects.get(pk=pk)
    form = ProductForm(request.POST or None, instance=p)
    if form.is_valid():
        form.save()
        return redirect('product_detail', pk=p.pk)
    return render(request, template_name,{'product': p, 'form': form})


@login_required()
def product_delete(request, pk):
    p = Product.objects.get(pk=pk)
    p.delete()
    return redirect('product_curation')


@login_required()
def product_list(request):
    template_name = 'product_curation/products.html'
    products = Product.objects.all()
    data = {}
    data['products'] = products
    return render(request, template_name, data)
/n/n/n",1
124,124,58b7e09b805fd6e6adb1e90a466f126bd2e07216,"cs490/beta/evaluate.py/n/n# This attempts to execute a student's python function against any number of
# test cases.
# We attempt to catch common mistakes by static anlysis of student source code
# that fails to compile, and then further static analysis of an AST given a
# successful compile but unsuccessful result.
# We also attempt to (minimally) guard against malicious code by stripping
# the global environment and enforcing only a single function definition.
# This also includes disallowing the user to (easily) output over stdout, which
# might allow student code to effectively publish their own grade to the caller.
# Of course, this is not foolproof as os.write(1, 'str') is hard to guard
# against if the student code does manage to get access to __import__.

import argparse, ast, json, sys, keyword, math, io

# Constants.
global_whitelist = ['__doc__', '__package__']
# Mostly a copy of all __builtins__.__dict__'s keys.
# Normally present values that are removed are denoted by a comment
# NOTE: Keep this list in alphabetical order
# LAST UPDATED FOR VERSION 3.5.2
builtins_whitelist = [
  'abs',
  'all',
  'any',
  'ArithmeticError',
  'ascii',
  'AssertionError',
  'AttributeError',
  'BaseException',
  'bin',
  'BlockingIOError',
  'bool',
  'BrokenPipeError',
  'BufferError',
  '__build_class__',
  'bytearray',
  'bytes',
  'BytesWarning',
  'callable',
  'ChildProcessError',
  'chr',
  'classmethod',
  #'compile',    # Avoid new code creation.
  'complex',
  'ConnectionAbortedError',
  'ConnectionError',
  'ConnectionRefusedError',
  'ConnectionResetError',
  #'copyright',  # Extraneous constant from module site.
  #'credits',    # ibid module site.
  #'__debug__',  # No reason to use debug mode on student code.
  'delattr',
  'DeprecationWarning',
  'dict',
  'dir',
  'divmod',
  '__doc__',
  'Ellipsis',
  'enumerate',
  'EnvironmentError',
  'EOFError',
  #'eval',       # ibid code creation.
  'Exception',
  #'exec',       # ibid code creation.
  #'exit',       # ibid module site.
  'False',
  'FileExistsError',
  'FileNotFoundError',
  'filter',
  'float',
  'FloatingPointError',
  'format',
  'frozenset',
  'FutureWarning',
  'GeneratorExit',
  'getatter',
  'globals',
  'hasattr',
  'hash',
  #'help',       # ibid module site.
  'hex',
  'id',
  #'__import__', # ibid code creation.
  'ImportError',
  'ImportWarning',
  'IndentationError',
  'IndexError',
  'input',
  'int',
  'InterruptedError',
  'IOError',
  'IsADirectoryError',
  'isinstance',
  'issubclass',
  'iter',
  'KeyboardInterrupt',
  'KeyError',
  'len',
  #'license',    # ibid module site.
  'list',
  #'__loader__', # ibid code creation.
  'locals',
  'LookupError',
  'map',
  'max',
  'MemoryError',
  'memoryview',
  'min',
  '__name__',
  'NameError',
  'next',
  'None',
  'NotADirectoryError',
  'NotImplemented',
  'NotImplementedError',
  'object',
  'oct',
  #'open',      # Avoid file i/o.
  'ord',
  'OSError',
  'OverflowError',
  '__package__',
  'PendingDeprecationWarning',
  'PermissionError',
  'pow',
  'print',
  'ProcessLookupError',
  'property',
  #'quit',      # ibid module site.
  'range',
  'RecursionError',
  'ReferenceError',
  'repr',
  'ResourceWarning',
  'reversed',
  'round',
  'RuntimeError',
  'RuntimeWarning',
  'set',
  'setattr',
  'slice',
  'sorted',
  #'__spec__',  # ibid code creation.
  'staticmethod',
  'StopAsyncIteration',
  'StopIteration',
  'str',
  'sum',
  'super',
  'SyntaxError',
  'SyntaxWarning',
  'SystemExit',
  'TabError',
  'TimeoutError',
  'True',
  'tuple',
  'type',
  'TypeError',
  'UnboundLocalError',
  'UnicodeEncodeError',
  'UnicodeWarning',
  'UserWarning',
  'ValueError',
  'vars',
  'Warning',
  'ZeroDivisionError',
  'zip',
]

# ContextManager Class for redirecting standard streams.
class RedirectIO(object):
  def __init__(self, stdin=''):
    # stdin: desired contents of stdin.
    self.stdin_mock  = io.StringIO(stdin)
    self.stdout_mock = io.StringIO()
    self.stderr_mock = io.StringIO()

    self.sys_stdin, self.sys_stdout, self.sys_stderr = (None,)*3

  def __enter__(self):
    # Save current objects, while replacing with the mocks
    self.sys_stdin,  sys.stdin  = sys.stdin,  self.stdin_mock
    self.sys_stdout, sys.stdout = sys.stdout, self.stdout_mock
    self.sys_stderr, sys.stderr = sys.stderr, self.stderr_mock

  def __exit__(self, exc_type, exc_value, traceback):
    # Return to original state
    sys.stdin  = self.sys_stdin
    sys.stdout = self.sys_stdout
    sys.stderr = self.sys_stderr

    return False  # Do not suppress exceptions

  def get_output(self):
    # Returns the contents of both mocked output streams.
    return (self.stdout_mock.getvalue(), self.stderr_mock.getvalue())

## OUTPUT FORMAT
# The output of this script will be written to stdout, and will take the form:
# {
#   'score': dd,
#   'deductions': [
#     { 'points': dd, 'reason': str }, ...
#   ]
# }

def output_json(points, deductions):
  # Write out the proper json output to stdout.
  score = points - sum(d['points'] for d in deductions)
  if score < 0: score = 0
  print(json.dumps({'score': score, 'deductions': deductions}))

def dock_points(deductions, points, reason):
  deductions.append({'points': points, 'reason': reason})

def fix_syntax_err(code, err):
  # Try to fix the syntax error found in the code. Returns fixed code or None.
  # TODO: Come up with some rules for fixing syntax errors.
  return None

def grade(code_obj, name, points, test_case_objs, vlevel = 0):
  # Grades the given implementation of name, code_obj, with a maximum score
  # of points on each test_case_obj. Returns a list of deductions.
  deductions = []  # Reasons we took off points.
  points_per_case = points // len(test_case_objs)

  # For capturing output written to sys.std* file objects.
  # NOTE: If the target code manages to get a hold of os.write or an equivalent,
  #       this will not be able to intercept direct calls using a file
  #       descriptor. i.e. os.write(1, 'haha, gotcha!');
  capture_io = RedirectIO();

  # Instrument globals and locals. This is not leak-proof by anymeans, but
  # helps to limit the attack surface.
  instr_globals = {
    k: globals()[k]
    for k in global_whitelist
    if k in globals()
  }
  instr_globals['__name__'] = name  # Module shouldn't run with name as __main__
  instr_globals['__builtins__'] = {
    k: __builtins__.__dict__[k]
    for k in builtins_whitelist
    if k in __builtins__.__dict__
  }
  instr_locals = {}

  # Run the function declaration from the student.
  try:
    with capture_io:
      exec(code_obj, instr_globals, instr_locals)
  except BaseException as e:
    if vlevel >= 1: print(repr(e), file=sys.stderr)
    # We'll consider any exception while defining the function to be a 0.
    dock_points(deductions, points, 'unable to execute function')
    return deductions

  for i, test_case_obj in enumerate(test_case_objs):
    try:
      with capture_io:
        result = eval(test_case_obj, instr_globals, instr_locals)
      if not result:
        dock_points(deductions, points_per_case, 'failed test case %d' % i)
        # TODO: Consider checking if the function evaluated to None, in which
        #       case, checking capture_io.get_output() may reveal a student
        #       incorrectly printed the result instead of returning it, in which
        #       case they may be given partial credit.
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      # We'll consider any exception while executing a test case to be wrong.
      dock_points(deductions, points_per_case,
                  'exception during test case %d' % i)

  return deductions


def main():
  ## INPUT ARGUMENTS
  parser = argparse.ArgumentParser(description=""Grade student code against ""
          ""given test cases. Results will be written over stdout in JSON"")
  parser.add_argument('-n', '--name', required=True,
          help=""The name of the function the student was supposed to implement"")
  parser.add_argument('-p', '--points', type=int, default=0,
          help=""The number of points this question is worth. This argument is ""
               ""only used when -c is passed, and defaults to 0"")
  parser.add_argument('-c', '--code',
          help=""The student's code submission. Make sure to carefully escape ""
               ""this argument as a single string. If this argument is ommitted ""
               ""then this program just checks the validity of the test cases. ""
               ""The exit status indicates the validity of the cases"")
  parser.add_argument('-t', '--test_case', required=True, action='append',
          help=""The test cases to run the students code against. Each test case ""
               ""must take the form of of a function call without the function ""
               ""name followed by a comparison to a return value. For example ""
               ""``(1, 2) == 3'' or ``(1, 2, (3, 4), *[5, 6], last=8) == None''"")
  parser.add_argument('-v', '--verbose',  action='count', default=0,
          help=""Specifies verbositiy level. Each time this flag is specified, ""
               ""the count goes up by one. Level 1 or greater outputs additional ""
               ""information about exceptions that occur"")


  # Process input arguments.
  args = parser.parse_args()
  vlevel = args.verbose

  # Ensure that name is a valid identifier (not a keyword).
  name = args.name
  if not name.isidentifier() or keyword.iskeyword(name):
    raise ValueError('Function name is not a valid identifier')

  test_case_objs = []
  # Handle test cases (make sure they are valid)
  for i, test_case in enumerate(args.test_case):
    # Prepend the function name to create a valid expression comparing the result
    # of calling the function with a value.
    test_case = name + test_case
    # Try parsing this into a single expression ast.
    try:
      expr = ast.parse(test_case, mode='eval')
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      expr = None

    if not expr:
      raise ValueError('Failed to parse test case %d' % i)

    # Validate format of test case.
    # For now we will be pretty strict, we expect exactly:
    # One comparison expression:
    #   where left is a single function call.
    #   and there is only one comparator
    #     which must be a simple literal:
    #       Dict, Set, Num, Str, Bytes, NameConstant, List, Tuple.
    #     or simple operators applied to the above:
    #       BinOp, UnaryOp.
    valid = False
    if type(expr) == ast.Expression:
      comp = expr.body
      if type(comp) == ast.Compare and len(comp.ops) == len(comp.comparators) == 1:
        # We have a valid comparison between exactly two exprs.
        left = comp.left
        right = comp.comparators[0]

        # Validate left side of comparison.
        left_valid = False
        if type(left) == ast.Call:
          if type(left.func) == ast.Name and left.func.id == name:
            # We appear to be calling the right function, TODO: walk args.
            left_valid = True

        # Validate right side of comparison (only if left side was okay).
        if left_valid:
          # TODO: walk nested exprs as needed.
          if type(right) in [ast.Num, ast.Str, ast.Bytes, ast.NameConstant,
                             ast.Dict, ast.Set, ast.List, ast.Tuple,
                             ast.BinOp, ast.UnaryOp]:
            valid = True

    if not valid:
      raise ValueError('Invalid formatting for test case %d' % i)

    # Finally, we just have to try compiling the test case.
    try:
      obj = compile(expr, '<unknown>', 'eval')
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      obj = None

    if not obj:
      raise ValueError('Cannot compile test case %d' % i)

    test_case_objs.append(obj)

  # Handle student code.
  code = args.code
  if not code:
    # We're just supposed to test the validity of the test cases when code is None.
    # TODO: Consider trying to evaluate the test cases to ensure that in addition to
    #       compiling, they actually evaluate without throwing exceptions.
    return

  # Bookkeeping for score.
  deductions = []

  # Try to construct an AST.
  tree = None
  while not tree:
    try:
      tree = ast.parse(code)
    except SyntaxError as se:
      fixed = fix_syntax_err(code, se)
      if not fixed:
        if vlevel >= 1: print(repr(se), file=sys.stderr)
        dock_points(deductions, args.points, 'syntax error')
        break  # Couldn't fix error.
      code = fixed
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      dock_points(deductions, args.points, 'failed to parse code')
      break    # Unknown exception.

  if not tree:
    output_json(args.points, deductions)
    return

  # Validate the tree. Should be a single module containing a single function.
  valid = False
  if type(tree) == ast.Module and len(tree.body) == 1:
    fdef = tree.body[0]
    if type(fdef) in [ast.FunctionDef, ast.AsyncFunctionDef]:
      if fdef.name != name:
        fdef.name = name
        # TODO: Solidify point values.
        dock_points(deductions, 1, 'misnamed function')
      # TODO: Consider checking number of arguments and stuff.
      valid = True

  if not valid:
    dock_points(deductions, args.points, 'not just a single function definition')
    output_json(args.points, deductions)
    return

  # TODO: Consider doing more work on the AST to help catch common errors.
  # TODO: Consider multiple alterations to tree to fix common errors.

  # Attempt to compile current code tree.
  try:
    code_obj = compile(tree, '<unknown>', 'exec')
  except BaseException as e:
    if vlevel >= 1: print(repr(e), file=sys.stderr)
    code_obj = None

  # Currently no way to recover from failed compile if tree parsed okay.
  # Full marks off.
  if not code_obj:
    dock_points(deductions, args.points, 'failed to compile code')
    output_json(args.points, deductions)
    return

  # Actually grade the students submission!
  deductions += grade(code_obj, name, args.points, test_case_objs, vlevel)
  output_json(args.points, deductions)

if __name__ == '__main__':
  try:
    main()
  except Exception as e:
    sys.exit(str(e))
/n/n/n",0
125,125,58b7e09b805fd6e6adb1e90a466f126bd2e07216,"/cs490/beta/evaluate.py/n/n# This attempts to execute a student's python function against any number of
# test cases.
# We attempt to catch common mistakes by static anlysis of student source code
# that fails to compile, and then further static analysis of an AST given a
# successful compile but unsuccessful result.
# We also attempt to (minimally) guard against malicious code by stripping
# the global environment and enforcing only a single function definition.
# This also includes disallowing the user to (easily) output over stdout, which
# might allow student code to effectively publish their own grade to the caller.
# Of course, this is not foolproof as os.write(1, 'str') is hard to guard
# against if the student code does manage to get access to __import__.

import argparse, ast, json, sys, keyword, math

# Constants.
global_whitelist = ['__doc__', '__package__']
# Mostly a copy of all __builtins__.__dict__'s keys.
# Normally present values that are removed are denoted by a comment
# NOTE: Keep this list in alphabetical order
# LAST UPDATED FOR VERSION 3.5.2
builtins_whitelist = [
  'abs',
  'all',
  'any',
  'ArithmeticError',
  'ascii',
  'AssertionError',
  'AttributeError',
  'BaseException',
  'bin',
  'BlockingIOError',
  'bool',
  'BrokenPipeError',
  'BufferError',
  '__build_class__',
  'bytearray',
  'bytes',
  'BytesWarning',
  'callable',
  'ChildProcessError',
  'chr',
  'classmethod',
  #'compile',    # Avoid new code creation.
  'complex',
  'ConnectionAbortedError',
  'ConnectionError',
  'ConnectionRefusedError',
  'ConnectionResetError',
  #'copyright',  # Extraneous constant from module site.
  #'credits',    # ibid module site.
  #'__debug__',  # No reason to use debug mode on student code.
  'delattr',
  'DeprecationWarning',
  'dict',
  'dir',
  'divmod',
  '__doc__',
  'Ellipsis',
  'enumerate',
  'EnvironmentError',
  'EOFError',
  #'eval',       # ibid code creation.
  'Exception',
  #'exec',       # ibid code creation.
  #'exit',       # ibid module site.
  'False',
  'FileExistsError',
  'FileNotFoundError',
  'filter',
  'float',
  'FloatingPointError',
  'format',
  'frozenset',
  'FutureWarning',
  'GeneratorExit',
  'getatter',
  'globals',
  'hasattr',
  'hash',
  #'help',       # ibid module site.
  'hex',
  'id',
  #'__import__', # ibid code creation.
  'ImportError',
  'ImportWarning',
  'IndentationError',
  'IndexError',
  'input',
  'int',
  'InterruptedError',
  'IOError',
  'IsADirectoryError',
  'isinstance',
  'issubclass',
  'iter',
  'KeyboardInterrupt',
  'KeyError',
  'len',
  #'license',    # ibid module site.
  'list',
  #'__loader__', # ibid code creation.
  'locals',
  'LookupError',
  'map',
  'max',
  'MemoryError',
  'memoryview',
  'min',
  '__name__',
  'NameError',
  'next',
  'None',
  'NotADirectoryError',
  'NotImplemented',
  'NotImplementedError',
  'object',
  'oct',
  #'open',      # Avoid file i/o.
  'ord',
  'OSError',
  'OverflowError',
  '__package__',
  'PendingDeprecationWarning',
  'PermissionError',
  'pow',
  'print',
  'ProcessLookupError',
  'property',
  #'quit',      # ibid module site.
  'range',
  'RecursionError',
  'ReferenceError',
  'repr',
  'ResourceWarning',
  'reversed',
  'round',
  'RuntimeError',
  'RuntimeWarning',
  'set',
  'setattr',
  'slice',
  'sorted',
  #'__spec__',  # ibid code creation.
  'staticmethod',
  'StopAsyncIteration',
  'StopIteration',
  'str',
  'sum',
  'super',
  'SyntaxError',
  'SyntaxWarning',
  'SystemExit',
  'TabError',
  'TimeoutError',
  'True',
  'tuple',
  'type',
  'TypeError',
  'UnboundLocalError',
  'UnicodeEncodeError',
  'UnicodeWarning',
  'UserWarning',
  'ValueError',
  'vars',
  'Warning',
  'ZeroDivisionError',
  'zip',
]

## OUTPUT FORMAT
# The output of this script will be written to stdout, and will take the form:
# {
#   'score': dd,
#   'deductions': [
#     { 'points': dd, 'reason': str }, ...
#   ]
# }

def output_json(points, deductions):
  # Write out the proper json output to stdout.
  score = points - sum(d['points'] for d in deductions)
  if score < 0: score = 0
  print(json.dumps({'score': score, 'deductions': deductions}))

def dock_points(deductions, points, reason):
  deductions.append({'points': points, 'reason': reason})

def fix_syntax_err(code, err):
  # Try to fix the syntax error found in the code. Returns fixed code or None.
  # TODO: Come up with some rules for fixing syntax errors.
  return None

def grade(code_obj, name, points, test_case_objs, vlevel = 0):
  # Grades the given implementation of name, code_obj, with a maximum score
  # of points on each test_case_obj. Returns a list of deductions.
  deductions = []  # Reasons we took off points.
  points_per_case = points // len(test_case_objs)

  # Instrument globals and locals. This is not leak-proof by anymeans, but
  # helps to limit the attack surface.
  instr_globals = {
    k: globals()[k]
    for k in global_whitelist
    if k in globals()
  }
  instr_globals['__name__'] = name  # Module shouldn't run with name as __main__
  instr_globals['__builtins__'] = {
    k: __builtins__.__dict__[k]
    for k in builtins_whitelist
    if k in __builtins__.__dict__
  }
  instr_locals = {}

  # TODO: instrument stdin, stdout, and stderr for these testcases.
  # TODO: if output is captured from stdout, consider comparing it against desired result

  # Run the function declaration from the student.
  try:
    exec(code_obj, instr_globals, instr_locals)
  except BaseException as e:
    if vlevel >= 1: print(repr(e), file=sys.stderr)
    # We'll consider any exception while defining the function to be a 0.
    dock_points(deductions, points, 'unable to execute function')
    return deductions

  for i, test_case_obj in enumerate(test_case_objs):
    try:
      result = eval(test_case_obj, instr_globals, instr_locals)
      if not result:
        dock_points(deductions, points_per_case, 'failed test case %d' % i)
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      # We'll consider any exception while executing a test case to be wrong.
      dock_points(deductions, points_per_case, 'exception during test case %d' % i)

  return deductions


def main():
  ## INPUT ARGUMENTS
  parser = argparse.ArgumentParser(description=""Grade student code against ""
          ""given test cases. Results will be written over stdout in JSON"")
  parser.add_argument('-n', '--name', required=True,
          help=""The name of the function the student was supposed to implement"")
  parser.add_argument('-p', '--points', type=int, default=0,
          help=""The number of points this question is worth. This argument is ""
               ""only used when -c is passed, and defaults to 0"")
  parser.add_argument('-c', '--code',
          help=""The student's code submission. Make sure to carefully escape ""
               ""this argument as a single string. If this argument is ommitted ""
               ""then this program just checks the validity of the test cases. ""
               ""The exit status indicates the validity of the cases"")
  parser.add_argument('-t', '--test_case', required=True, action='append',
          help=""The test cases to run the students code against. Each test case ""
               ""must take the form of of a function call without the function ""
               ""name followed by a comparison to a return value. For example ""
               ""``(1, 2) == 3'' or ``(1, 2, (3, 4), *[5, 6], last=8) == None''"")
  parser.add_argument('-v', '--verbose',  action='count', default=0,
          help=""Specifies verbositiy level. Each time this flag is specified, ""
               ""the count goes up by one. Level 1 or greater outputs additional ""
               ""information about exceptions that occur"")


  # Process input arguments.
  args = parser.parse_args()
  vlevel = args.verbose

  # Ensure that name is a valid identifier (not a keyword).
  name = args.name
  if not name.isidentifier() or keyword.iskeyword(name):
    raise ValueError('Function name is not a valid identifier')

  test_case_objs = []
  # Handle test cases (make sure they are valid)
  for i, test_case in enumerate(args.test_case):
    # Prepend the function name to create a valid expression comparing the result
    # of calling the function with a value.
    test_case = name + test_case
    # Try parsing this into a single expression ast.
    try:
      expr = ast.parse(test_case, mode='eval')
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      expr = None

    if not expr:
      raise ValueError('Failed to parse test case %d' % i)

    # Validate format of test case.
    # For now we will be pretty strict, we expect exactly:
    # One comparison expression:
    #   where left is a single function call.
    #   and there is only one comparator
    #     which must be a simple literal:
    #       Dict, Set, Num, Str, Bytes, NameConstant, List, Tuple.
    #     or simple operators applied to the above:
    #       BinOp, UnaryOp.
    valid = False
    if type(expr) == ast.Expression:
      comp = expr.body
      if type(comp) == ast.Compare and len(comp.ops) == len(comp.comparators) == 1:
        # We have a valid comparison between exactly two exprs.
        left = comp.left
        right = comp.comparators[0]

        # Validate left side of comparison.
        left_valid = False
        if type(left) == ast.Call:
          if type(left.func) == ast.Name and left.func.id == name:
            # We appear to be calling the right function, TODO: walk args.
            left_valid = True

        # Validate right side of comparison (only if left side was okay).
        if left_valid:
          # TODO: walk nested exprs as needed.
          if type(right) in [ast.Num, ast.Str, ast.Bytes, ast.NameConstant,
                             ast.Dict, ast.Set, ast.List, ast.Tuple,
                             ast.BinOp, ast.UnaryOp]:
            valid = True

    if not valid:
      raise ValueError('Invalid formatting for test case %d' % i)

    # Finally, we just have to try compiling the test case.
    try:
      obj = compile(expr, '<unknown>', 'eval')
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      obj = None

    if not obj:
      raise ValueError('Cannot compile test case %d' % i)

    test_case_objs.append(obj)

  # Handle student code.
  code = args.code
  if not code:
    # We're just supposed to test the validity of the test cases when code is None.
    # TODO: Consider trying to evaluate the test cases to ensure that in addition to
    #       compiling, they actually evaluate without throwing exceptions.
    return

  # Bookkeeping for score.
  deductions = []

  # Try to construct an AST.
  tree = None
  while not tree:
    try:
      tree = ast.parse(code)
    except SyntaxError as se:
      fixed = fix_syntax_err(code, se)
      if not fixed:
        if vlevel >= 1: print(repr(se), file=sys.stderr)
        dock_points(deductions, args.points, 'syntax error')
        break  # Couldn't fix error.
      code = fixed
    except BaseException as e:
      if vlevel >= 1: print(repr(e), file=sys.stderr)
      dock_points(deductions, args.points, 'failed to parse code')
      break    # Unknown exception.

  if not tree:
    output_json(args.points, deductions)
    return

  # Validate the tree. Should be a single module containing a single function.
  valid = False
  if type(tree) == ast.Module and len(tree.body) == 1:
    fdef = tree.body[0]
    if type(fdef) in [ast.FunctionDef, ast.AsyncFunctionDef]:
      if fdef.name != name:
        fdef.name = name
        # TODO: Solidify point values.
        dock_points(deductions, 1, 'misnamed function')
      # TODO: Consider checking number of arguments and stuff.
      valid = True

  if not valid:
    dock_points(deductions, args.points, 'not just a single function definition')
    output_json(args.points, deductions)
    return

  # TODO: Consider doing more work on the AST to help catch common errors.
  # TODO: Consider multiple alterations to tree to fix common errors.

  # Attempt to compile current code tree.
  try:
    code_obj = compile(tree, '<unknown>', 'exec')
  except BaseException as e:
    if vlevel >= 1: print(repr(e), file=sys.stderr)
    code_obj = None

  # Currently no way to recover from failed compile if tree parsed okay.
  # Full marks off.
  if not code_obj:
    dock_points(deductions, args.points, 'failed to compile code')
    output_json(args.points, deductions)
    return

  # Actually grade the students submission!
  deductions += grade(code_obj, name, args.points, test_case_objs, vlevel)
  output_json(args.points, deductions)

if __name__ == '__main__':
  try:
    main()
  except Exception as e:
    sys.exit(str(e))
/n/n/n",1
72,72,d98b8549ad2327f2f61310b4294095fbd46c8ed2,"chemie/elections/tests/test_views.py/n/nimport pytest
from django.shortcuts import reverse


@pytest.mark.django_db
def test_user_not_logged_in(client):
    request = client.get(reverse('elections:vote'))
    assert request.status_code == 302
    assert reverse('login') in request.url


@pytest.mark.django_db
def test_election_not_open(client, create_user):
    user = create_user
    client.login(username=user.username, password='defaultpassword')

    request = client.get(reverse('elections:vote'))
    assert 'Valget har ikke pnet enda' in request.content.decode('utf-8')
    assert '<a class=""button"" href=""/elections/results"">' in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url


@pytest.mark.django_db
def test_election_is_open(client, create_user, create_election_with_positions):
    election, positions = create_election_with_positions
    election.is_open = True
    election.save()
    user = create_user

    client.login(username=user.username, password='defaultpassword')
    request = client.get(reverse('elections:vote'))
    assert request.status_code is 200
    assert ""Klargjres til valg"" in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
/n/n/n",0
73,73,d98b8549ad2327f2f61310b4294095fbd46c8ed2,"/chemie/elections/tests/test_view.py/n/nimport pytest
from django.shortcuts import reverse
from



@pytest.mark.django_db
def test_user_not_logged_in(client,create_user):
    request = client.get(reverse('elections:vote'))
    assert request.status_code == 302
    assert reverse('login') in request.url

@pytest.mark.django_db
def test_election_not_open(client,create_user):
    user = create_user
    client.login(username=user.username, password='defaultpassword')

    request = client.get(reverse('elections:vote'))
    assert ""Valget har ikke pnet enda"" in request.content.decode('utf-8')
    assert '<a class=""button"" href=""/elections/results"">' in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url


@pytest.mark.django_db
def test_election_is_open(client,create_user, create_election_with_positions):
    election, positions = create_election_with_positions
    election = Election.objects.create()
    client.login(username=user.username, password='defaultpassword')
    request = client.get(reverse('elections:vote'))
    assert reverse('elections:vote') == request.url
    assert ""Klargjres til valg"" in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url




/n/n/n",1
130,130,c73c7dc18384435d01d9a7c4591074884216e202,"apps/domain/views.py/n/nfrom django.http import HttpResponseRedirect
from django.shortcuts import redirect
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.geoip import GeoIP
from lib.vt import VT
from lib.threatminer import ThreatMiner
import socket
import re
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit

class IndexView(TemplateView):
    template_name = 'domain/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            domain = request.GET.get('keyword')
            if self.is_valid_domain(domain):
                return HttpResponseRedirect(domain)
            else:
                return redirect('domain:index')
        context = self.get_context_data()
        return self.render_to_response(context)

    def is_valid_domain(self, value):
        compiled_pattern = re.compile('\w+\.\w+')
        if compiled_pattern.match(value):
            return True
        else:
            return False

class DetailView(TemplateView):
    template_name = 'domain/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        domain = self.kwargs['pk']
        try:
            context['geoip'] = GeoIP().lookup(domain)
        except Exception as e:
            print(e)
            pass
        try:
            context['ipaddress'] = socket.gethostbyname(domain)
        except Exception as e:
            pass

        vt = VT()
        context['vt_domain'] = vt.getDomainReport(domain)

        tm = ThreatMiner()
        context['tm_url'] = tm.getURIFromDomain(domain)
        context['tm_sample'] = tm.getSamplesFromDomain(domain)
        context['tm_report'] = tm.getReportFromDomain(domain)

        context['bls'] = blacklist.objects.filter(Q(domain=domain)|Q(url__contains=domain))
        count = context['bls'].count()
        if count > 0:
            context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=domain)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=domain)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=domain)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=domain)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

/n/n/napps/exploit/views.py/n/nfrom django.views.generic import ListView, DetailView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q
from .models import Exploit
from .forms import SearchForm, TargetForm
from datetime import datetime, timezone, timedelta

class IndexView(PaginationMixin, ListView):
    template_name = 'exploit/index.html'
    context_object_name = 'exs'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        search_form = SearchForm(self.request.GET)
        context['search_form'] = search_form
        target_form = TargetForm()
        context['target_form'] = target_form
        count = self.object_list.count()
        context['count'] = count
        context['30_day_labels'] = self.thirty_day_labels()
        context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = Exploit.objects.order_by('-datetime')
        keyword = self.request.GET.get('keyword')
        source = self.request.GET.get('source')
        if keyword is not None:
            query = query.filter(Q(text__icontains=keyword)).order_by('-datetime')
        if source is not None:
            query = query.filter(source=source)
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(datetime__gte=from_date, datetime__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

class DetailView(DetailView):
    model = Exploit
    template_name = 'exploit/detail.html'
/n/n/napps/filehash/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import redirect
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.vt import VT
from lib.threatminer import ThreatMiner
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit
import re

class IndexView(TemplateView):
    template_name = 'filehash/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            filehash = request.GET.get('keyword')
            if self.is_valid_hash(filehash):
                return HttpResponseRedirect(filehash)
            else:
                return redirect('filehash:index')
        context = self.get_context_data()
        return self.render_to_response(context)

    def is_valid_hash(self, value):
        compiled_pattern_md5 = re.compile(r'(?=(\b[a-fA-F0-9]{32}\b))')
        compiled_pattern_sha1 = re.compile(r'(?=(\b[a-fA-F0-9]{40}\b))')
        compiled_pattern_sha256 = re.compile(r'(?=(\b[a-fA-F0-9]{64}\b))')
        if compiled_pattern_md5.match(value) or compiled_pattern_sha1.match(value) or compiled_pattern_sha256.match(value):
            return True
        else:
            return False

class DetailView(TemplateView):
    template_name = 'filehash/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        filehash = self.kwargs['pk']

        vt = VT()
        context['vt_hash'] = vt.getFileReport(filehash)
        context['vt_behavior'] = vt.getFileBehavior(filehash)

        tm = ThreatMiner()
        context['tm_meta'] = tm.getMetaFromSample(filehash)
        context['tm_http'] = tm.getHttpFromSample(filehash)
        context['tm_host'] = tm.getHostsFromSample(filehash)
        context['tm_av'] = tm.getAVFromSample(filehash)
        context['tm_report'] = tm.getReportFromSample(filehash)

        #context['bls'] = blacklist.objects.filter(Q(url__contains=filehash))
        #count = context['bls'].count()
        #if count > 0:
        #    context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=filehash)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=filehash)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=filehash)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=filehash)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

def getpcap(request, pk):
    response = HttpResponse(VT().getPcap(pk), content_type=""application/vnd.tcpdump.pcap"")
    response[""Content-Disposition""] = ""filename=%s.pcap"" % pk
    return response

/n/n/napps/ip/forms.py/n/nfrom django import forms

class SearchForm(forms.Form):
    keyword = forms.GenericIPAddressField(label='', required=True, protocol='IPv4')
    keyword.widget.attrs['class'] = 'form-control mr-sm-2 my-2'
    keyword.widget.attrs['placeholder'] = 'Lookup IP Address'

/n/n/napps/ip/views.py/n/nfrom django.http import HttpResponseRedirect
from django.shortcuts import redirect
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.geoip import GeoIP
from lib.vt import VT
from lib.threatminer import ThreatMiner
import socket
import ipaddress
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit

class IndexView(TemplateView):
    template_name = 'ip/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            ip = request.GET.get('keyword')
            if self.is_valid_ip(ip):
                return HttpResponseRedirect(ip)
            else:
                return redirect('ip:index')
        context = self.get_context_data()
        return self.render_to_response(context)

    def is_valid_ip(self, value):
        try:
            ipaddress.ip_address(value)
            return True
        except ValueError:
            return False

class DetailView(TemplateView):
    template_name = 'ip/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        ip = self.kwargs['pk']
        context['geoip'] = GeoIP().lookup(ip)
        try:
            context['domain'] = socket.gethostbyaddr(ip)[0]
        except Exception as e:
            pass

        vt = VT()
        context['vt_ip'] = vt.getIPReport(ip)

        tm = ThreatMiner()
        context['tm_url'] = tm.getURIFromIP(ip)
        context['tm_sample'] = tm.getSamplesFromIP(ip)
        context['tm_report'] = tm.getReportFromIP(ip)

        context['bls'] = blacklist.objects.filter(Q(ip=ip)|Q(url__contains=ip))
        count = context['bls'].count()
        if count > 0:
            context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=ip)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=ip)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=ip)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=ip)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

/n/n/napps/threat/views.py/n/nfrom django.views.generic import ListView, DetailView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q
from .models import Event, Attribute, Org, Tag, Object, ObjectReference
from .forms import EventSearchForm, AttributeSearchForm
from datetime import datetime, timezone, timedelta

class EventListView(PaginationMixin, ListView):
    model = Event
    template_name = 'threat/event_list.html'
    context_object_name = 'events'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['count'] = self.object_list.count()
        context['alltag'] = Tag.objects.order_by('id')
        taglist = self.request.GET.getlist('tag')
        context['tags'] = Tag.objects.filter(id__in=taglist)
        search_form = EventSearchForm(self.request.GET)
        context['search_form'] = search_form
        context['30_day_labels'] = self.thirty_day_labels()
        context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = Event.objects.order_by('-publish_timestamp')
        tag = self.request.GET.get('tag')
        if tag is not None:
            query = query.filter(tags__id=tag)
        org = self.request.GET.get('org')
        if org is not None:
            query = query.filter(orgc=org)
        level = self.request.GET.get('level')
        if level is not None:
            query = query.filter(threat_level_id=level)
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(info__icontains=keyword)).order_by('-publish_timestamp')
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(publish_timestamp__gte=from_date, publish_timestamp__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

class EventDetailView(PaginationMixin, ListView):
    model = Attribute
    template_name = 'threat/event_detail.html'
    context_object_name = 'attributes'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        pk = self.kwargs['pk']
        event_obj = Event.objects.get(pk=pk)
        objects_obj = Object.objects.filter(event=pk)
        context = super().get_context_data(**kwargs)
        context['event'] = event_obj
        context['objects'] = objects_obj
        context['categories'] = event_obj.getUniqCategory()
        context['types'] = event_obj.getUniqType()
        context['count'] = self.object_list.count()
        return context

    def get_queryset(self):
        pk = self.kwargs['pk']
        query = Attribute.objects.filter(event=pk).order_by('id')
        category = self.request.GET.get('category')
        type = self.request.GET.get('type')
        if category is not None:
            query = query.filter(category=category)
        if type is not None:
            query = query.filter(type=type)
        return query

class AttributeListView(PaginationMixin, ListView):
    model = Attribute
    template_name = 'threat/attribute_list.html'
    context_object_name = 'attributes'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        attr = Attribute.objects.all()
        context = super().get_context_data(**kwargs)
        context['categories'] = attr.values_list('category', flat=True).order_by('category').distinct()
        context['types'] = attr.values_list('type', flat=True).order_by('type').distinct()
        context['count'] = self.object_list.count()
        search_form = AttributeSearchForm(self.request.GET)
        context['search_form'] = search_form
        #context['30_day_labels'] = self.thirty_day_labels()
        #context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = Attribute.objects.order_by('-timestamp')
        category = self.request.GET.get('category')
        type = self.request.GET.get('type')
        if category is not None:
            query = query.filter(category=category)
        if type is not None:
            query = query.filter(type=type)
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(value__icontains=keyword)).order_by('-timestamp')
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(timestamp__gte=from_date, timestamp__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

class OrgListView(ListView):
    model = Org
    template_name = 'threat/org_list.html'
    context_object_name = 'orgs'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        count = self.object_list.count()
        context['count'] = count
        return context

    def get_queryset(self):
        query = Org.objects.order_by('id')
        return query

class TagListView(ListView):
    model = Tag
    template_name = 'threat/tag_list.html'
    context_object_name = 'tags'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        count = self.object_list.count()
        context['count'] = count
        return context

    def get_queryset(self):
        query = Tag.objects.order_by('id')
        return query

/n/n/napps/threat_hunter/views.py/n/nfrom django.http import HttpResponse
from django.shortcuts import get_object_or_404, render, redirect
from django.views.generic import ListView, DetailView
from django.views.generic.edit import CreateView, UpdateView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q, Count
from .models import Hunt
from apps.threat.models import Event
from .forms import HuntForm
import csv
from io import StringIO, BytesIO
from codecs import BOM_UTF8
from pytz import timezone
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPConnection

class IndexView(PaginationMixin, ListView):
    model = Hunt
    template_name = 'threat_hunter/index.html'
    context_object_name = 'hunts'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self):
        query = Hunt.objects.order_by('id')
        query = query.annotate(count=Count('events'))
        return query

class EventListView(PaginationMixin, ListView):
    model = Event
    template_name = 'threat_hunter/event_list.html'
    context_object_name = 'events'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request, pk):
        pk = self.kwargs['pk']
        query = Event.objects.filter(Q(id__in=Hunt(id=pk).events.all())).order_by('-publish_timestamp')
        return query

    def get(self, request, pk):
        self.object_list = self.get_queryset(request, pk)
        context = self.get_context_data()
        return render(request, 'threat_hunter/event_list.html', context)

class HuntCreateView(CreateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'threat_hunter/hunt_form.html'

    def get_success_url(self):
        self.object.run()
        return '/threat_hunter'
        
class HuntUpdateView(UpdateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'threat_hunter/hunt_edit_form.html'

    def get_success_url(self):
        self.object.run()
        return '/threat_hunter'

def hunt_del(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.delete()
    return redirect('threat_hunter:index')

def hunt_export(request, pk):
    stream = StringIO()
    writer = csv.writer(stream)
    header = ['#published', 'date', 'info', 'level', 'attribute_count', 'org']
    writer.writerow(header)
    for event in Event.objects.filter(id__in=Hunt(id=pk).events.all()).order_by('publish_timestamp'):
        dt = event.publish_timestamp.astimezone(timezone('Asia/Tokyo'))
        row = [dt, event.date, event.info, event.get_threat_level_id_display(), event.attribute_count, event.org.name]
        writer.writerow(row)
    b_stream = BytesIO(BOM_UTF8 + stream.getvalue().encode('utf8'))
    response = HttpResponse(b_stream.getvalue(), content_type=""text/csv"")
    response[""Content-Disposition""] = ""filename=hunter%s.csv"" % pk
    return response

def hunt_switch_notice(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.notice == True:
        hunt.setNoticeFalse()
    else:
        hunt.setNoticeTrue()
    hunt.run()
    return redirect('threat_hunter:index')

def hunt_switch_enable(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.enable == True:
        hunt.setDisable()
    else:
        hunt.setEnable()
        hunt.run()
    return redirect('threat_hunter:index')

/n/n/napps/twitter/views.py/n/nfrom django.views.generic import ListView, DetailView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q
from .models import tweet
from .forms import SearchForm
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPSConnection
from datetime import datetime, timezone, timedelta

class IndexView(PaginationMixin, ListView):
    template_name = 'twitter/index.html'
    context_object_name = 'tws'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        search_form = SearchForm(self.request.GET)
        context['search_form'] = search_form
        count = self.object_list.count()
        context['count'] = count
        context['30_day_labels'] = self.thirty_day_labels()
        context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = tweet.objects.order_by('-datetime')
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(text__icontains=keyword)).order_by('-datetime')
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(datetime__gte=from_date, datetime__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

def expand_url(request):
    url = request.GET.get('url', None)
    exurl = expand(url)
    while exurl != url:
        url = exurl
        exurl = expand(url)
    return JsonResponse({'url': exurl})

def expand(url):
    o = urlparse(url)
    con = HTTPSConnection(o.netloc)
    con.request('HEAD', o.path)
    res = con.getresponse()
    if res.getheader('location') == None:
        return url
    return res.getheader('location')
/n/n/napps/twitter_hunter/views.py/n/nfrom django.http import HttpResponse
from django.shortcuts import get_object_or_404, render, redirect
from django.views.generic import ListView, DetailView
from django.views.generic.edit import CreateView, UpdateView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q, Count
from .models import tweet, Hunt
from .forms import HuntForm
import csv
from io import StringIO, BytesIO
from codecs import BOM_UTF8
from pytz import timezone
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPSConnection

class IndexView(PaginationMixin, ListView):
    template_name = 'twitter_hunter/index.html'
    context_object_name = 'hts'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request):
        query = Hunt.objects.order_by('id')
        query = query.annotate(count=Count('tweet'))
        return query

    def get(self, request):
        self.object_list = self.get_queryset(request)
        context = self.get_context_data()
        return render(request, 'twitter_hunter/index.html', context)

class TweetsView(PaginationMixin, ListView):
    template_name = 'twitter_hunter/tweets.html'
    context_object_name = 'tws'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request, pk):
        query = tweet.objects.filter(hunt_id=Hunt(id=pk)).order_by('-datetime')
        return query

    def get(self, request, pk):
        self.object_list = self.get_queryset(request, pk)
        context = self.get_context_data()
        return render(request, 'twitter_hunter/tweets.html', context)

class HuntCreateView(CreateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'twitter_hunter/hunt_form.html'

    def get_success_url(self):
        self.object.start()
        return '/twitter_hunter'

class HuntUpdateView(UpdateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'twitter_hunter/hunt_edit_form.html'

    def get_success_url(self):
        self.object.restart()
        return '/twitter_hunter'

def hunt_del(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.stop()
    hunt.delete()
    return redirect('twitter_hunter:index')

def hunt_export(request, pk):
    stream = StringIO()
    writer = csv.writer(stream)
    header = ['#datetime', 'user', 'screen_name', 'text']
    writer.writerow(header)
    for tw in tweet.objects.filter(hunt_id=Hunt(id=pk)).order_by('datetime'):
        dt = tw.datetime.astimezone(timezone('Asia/Tokyo'))
        row = [dt, tw.user, tw.screen_name, tw.text]
        writer.writerow(row)
    b_stream = BytesIO(BOM_UTF8 + stream.getvalue().encode('utf8'))
    response = HttpResponse(b_stream.getvalue(), content_type=""text/csv"")
    response[""Content-Disposition""] = ""filename=hunter%s.csv"" % pk
    return response

def hunt_switch_notice(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.stop()
    if hunt.notice == True:
        hunt.setNoticeFalse()
    else:
        hunt.setNoticeTrue()
    hunt.start()
    return redirect('twitter_hunter:index')

def hunt_switch_enable(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.enable == True:
        hunt.setDisable()
        hunt.stop()
    else:
        hunt.setEnable()
        hunt.start()
    return redirect('twitter_hunter:index')

def expand_url(request):
    url = request.GET.get('url', None)
    exurl = expand(url)
    while exurl != url:
        url = exurl
        exurl = expand(url)
    return JsonResponse({'url': exurl})

def expand(url):
    o = urlparse(url)
    con = HTTPSConnection(o.netloc)
    con.request('HEAD', o.path)
    res = con.getresponse()
    if res.getheader('location') == None:
        return url
    return res.getheader('location')

/n/n/napps/url/forms.py/n/nfrom django import forms

class SearchForm(forms.Form):
    keyword = forms.URLField(label='', max_length=100, required=True)
    keyword.widget.attrs['class'] = 'form-control mr-sm-2 my-2'
    keyword.widget.attrs['placeholder'] = 'Lookup URL'
/n/n/napps/url/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import redirect
from django.views.generic import TemplateView, DetailView
from django.urls import reverse
from django.utils.http import urlquote
from .forms import SearchForm
from lib.vt import VT
import os
import subprocess
import hashlib
import requests
import imgkit
import shutil
import re
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit
from django.conf import settings

class IndexView(TemplateView):
    template_name = 'url/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            url = request.GET.get('keyword')
            if self.is_valid_url(url):
                return HttpResponseRedirect(reverse(""url:index"") + urlquote(url, safe='') + '/')
            else:
                return redirect('url:index')
        context = self.get_context_data()
        return self.render_to_response(context)

    def is_valid_url(self, value):
        compiled_pattern = re.compile('http(s)?://\w+')
        if compiled_pattern.match(value):
            return True
        else:
            return False

class DetailView(TemplateView):
    template_name = 'url/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        url = self.kwargs['pk']

        response = self.getResponse(url)
        if response is not None:
            context['response_code'] = response.status_code
            if ""content-type"" in response.headers:
                context['content_type'] = response.headers[""content-type""]
                context['response_sha256'] = self.getHash(response)
                context['title'] = self.getTitle(response)
            if ""last-modified"" in response.headers:
                context['last_modified'] = response.headers[""last-modified""]
            if ""server"" in response.headers:
                context['server'] = response.headers[""server""]
            if ""content-length"" in response.headers:
                context['content_length'] = response.headers[""content-length""]
        context['imagefile'] = self.getImage(url)
        context['websrc'] = self.getSrc(url)

        vt = VT()
        context['vt_url'] = vt.getURLReport(url)

        context['bls'] = blacklist.objects.filter(Q(url__contains=url))
        count = context['bls'].count()
        if count > 0:
            context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=url)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=url)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=url)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=url)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

    def getResponse(self, url):
        ua = ""Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv 11.0) like Gecko""
        headers = {'User-Agent': ua}
        try:
            res = requests.get(url, headers=headers, verify=False)
        except Exception as e:
            return
        res.encoding = res.apparent_encoding
        return res

    def getHash(self, res):
        if 'text/html' in res.headers[""content-type""]:
            sha256 = hashlib.sha256(res.text.encode('utf-8')).hexdigest()
        else:
            sha256 = hashlib.sha256(res.content).hexdigest()
        return sha256

    def getTitle(self, res):
        title = ''
        if 'text/html' in res.headers[""content-type""]:
            if '<title>' in res.text:
                title = res.text.split('<title>')[1].split('</title>')[0]
            elif '<TITLE>' in res.text:
                title = res.text.split('<TITLE>')[1].split('</TITLE>')[0]
        return title

    def getImage(self, url):
        imagehash = hashlib.md5(url.encode('utf-8')).hexdigest()
        if settings.STATIC_ROOT is None:
            filepath = settings.STATICFILES_DIRS[0] + ""webimg/"" + imagehash + "".png""
        else:
            filepath = settings.STATIC_ROOT + ""webimg/"" + imagehash + "".png""
        path = ""static/webimg/"" + imagehash + "".png""
        options = {
            'quiet': '',
        }
        if not os.path.exists(filepath):
            try:
                imgkit.from_url(url, filepath, options=options)
            except Exception as e:
                return
        return path

    def getSrc(self, url):
        imagehash = hashlib.md5(url.encode('utf-8')).hexdigest()
        if settings.STATIC_ROOT is None:
            filepath = settings.STATICFILES_DIRS[0] + ""websrc/"" + imagehash
        else:
            filepath = settings.STATIC_ROOT + ""websrc/"" + imagehash

        if not os.path.exists(filepath):
            ua = ""Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv 11.0) like Gecko""
            headers = {
                'User-Agent': ua
            }
            try:
                res = requests.get(url, headers=headers, verify=False)
            except Exception as e:
                return
            if 'text/html' in res.headers['content-type']:
                with open(filepath, 'w') as fp:
                    fp.write(res.text)
            else:
                with open(filepath, 'wb') as fp:
                    fp.write(res.content)
        return imagehash

class CodeView(TemplateView):
    template_name = 'url/code.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        if settings.STATIC_ROOT is None:
            srcpath = settings.STATICFILES_DIRS[0] + 'websrc/' + self.kwargs['pk']
        else:
            srcpath = settings.STATIC_ROOT + 'websrc/' + self.kwargs['pk']
        f = open(srcpath, 'r')
        context['websrc'] = f.read()
        f.close()
        return context

def getContents(request, pk):
    if not re.compile(r'(?=(\b[a-fA-F0-9]{32}\b))').match(pk):
        return redirect('index')
    if settings.STATIC_ROOT is None:
        filepath = settings.STATICFILES_DIRS[0] + 'websrc/' + pk
    else:
        filepath = settings.STATIC_ROOT + 'websrc/' + pk
    f = open(filepath, 'rb')
    contents = f.read()
    f.close()
    response = HttpResponse(contents)
    response[""Content-Disposition""] = ""filename=%s"" % pk
    return response
/n/n/n",0
131,131,c73c7dc18384435d01d9a7c4591074884216e202,"/apps/domain/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.geoip import GeoIP
from lib.vt import VT
from lib.threatminer import ThreatMiner
import socket
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit

class IndexView(TemplateView):
    template_name = 'domain/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            domain = request.GET.get('keyword')
            return HttpResponseRedirect(domain)
        context = self.get_context_data()
        return self.render_to_response(context)

class DetailView(TemplateView):
    template_name = 'domain/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        domain = self.kwargs['pk']
        try:
            context['geoip'] = GeoIP().lookup(domain)
        except Exception as e:
            print(e)
            pass
        try:
            context['ipaddress'] = socket.gethostbyname(domain)
        except Exception as e:
            pass

        vt = VT()
        context['vt_domain'] = vt.getDomainReport(domain)

        tm = ThreatMiner()
        context['tm_url'] = tm.getURIFromDomain(domain)
        context['tm_sample'] = tm.getSamplesFromDomain(domain)
        context['tm_report'] = tm.getReportFromDomain(domain)

        context['bls'] = blacklist.objects.filter(Q(domain=domain)|Q(url__contains=domain))
        count = context['bls'].count()
        if count > 0:
            context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=domain)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=domain)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=domain)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=domain)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

/n/n/n/apps/filehash/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.vt import VT
from lib.threatminer import ThreatMiner
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit

class IndexView(TemplateView):
    template_name = 'filehash/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            filehash = request.GET.get('keyword')
            return HttpResponseRedirect(filehash)
        context = self.get_context_data()
        return self.render_to_response(context)

class DetailView(TemplateView):
    template_name = 'filehash/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        filehash = self.kwargs['pk']

        vt = VT()
        context['vt_hash'] = vt.getFileReport(filehash)
        context['vt_behavior'] = vt.getFileBehavior(filehash)

        tm = ThreatMiner()
        context['tm_meta'] = tm.getMetaFromSample(filehash)
        context['tm_http'] = tm.getHttpFromSample(filehash)
        context['tm_host'] = tm.getHostsFromSample(filehash)
        context['tm_av'] = tm.getAVFromSample(filehash)
        context['tm_report'] = tm.getReportFromSample(filehash)

        #context['bls'] = blacklist.objects.filter(Q(url__contains=filehash))
        #count = context['bls'].count()
        #if count > 0:
        #    context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=filehash)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=filehash)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=filehash)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=filehash)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

def getpcap(request, pk):
    response = HttpResponse(VT().getPcap(pk), content_type=""application/vnd.tcpdump.pcap"")
    response[""Content-Disposition""] = ""filename=%s.pcap"" % pk
    return response

/n/n/n/apps/ip/forms.py/n/nfrom django import forms

class SearchForm(forms.Form):
    keyword = forms.CharField(label='', max_length=100, required=True)
    keyword.widget.attrs['class'] = 'form-control mr-sm-2 my-2'
    keyword.widget.attrs['placeholder'] = 'Lookup IP Address'
/n/n/n/apps/ip/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render
from django.views.generic import TemplateView, DetailView
from .forms import SearchForm
from lib.geoip import GeoIP
from lib.vt import VT
from lib.threatminer import ThreatMiner
import socket
from django.db.models import Q
from apps.threat.models import Event, Attribute
from apps.reputation.models import blacklist
from apps.twitter.models import tweet
from apps.exploit.models import Exploit

class IndexView(TemplateView):
    template_name = 'ip/index.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        return context

    def get(self, request, **kwargs):
        if request.GET.get('keyword'):
            ip = request.GET.get('keyword')
            return HttpResponseRedirect(ip)
        context = self.get_context_data()
        return self.render_to_response(context)

class DetailView(TemplateView):
    template_name = 'ip/detail.html'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['search_form'] = SearchForm()
        ip = self.kwargs['pk']
        context['geoip'] = GeoIP().lookup(ip)
        try:
            context['domain'] = socket.gethostbyaddr(ip)[0]
        except Exception as e:
            pass

        vt = VT()
        context['vt_ip'] = vt.getIPReport(ip)

        tm = ThreatMiner()
        context['tm_url'] = tm.getURIFromIP(ip)
        context['tm_sample'] = tm.getSamplesFromIP(ip)
        context['tm_report'] = tm.getReportFromIP(ip)

        context['bls'] = blacklist.objects.filter(Q(ip=ip)|Q(url__contains=ip))
        count = context['bls'].count()
        if count > 0:
            context['bls_count'] = count
        context['events'] = Event.objects.filter(Q(info__icontains=ip)).order_by('-publish_timestamp')
        count = context['events'].count()
        if count > 0:
            context['events_count'] = count
        context['attributes'] = Attribute.objects.filter(Q(value__icontains=ip)).order_by('-timestamp')
        count = context['attributes'].count()
        if count > 0:
            context['attributes_count'] = count
        context['tws'] = tweet.objects.filter(Q(text__icontains=ip)).order_by('-datetime')
        count = context['tws'].count()
        if count > 0:
            context['tws_count'] = count
        context['exs'] = Exploit.objects.filter(Q(text__icontains=ip)).order_by('-datetime')
        count = context['exs'].count()
        if count > 0:
            context['exs_count'] = count

        return context

/n/n/n/apps/threat/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render
from django.views.generic import ListView, DetailView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q
from .models import Event, Attribute, Org, Tag, Object, ObjectReference
from .forms import EventSearchForm, AttributeSearchForm
from datetime import datetime, timezone, timedelta

class EventListView(PaginationMixin, ListView):
    model = Event
    template_name = 'threat/event_list.html'
    context_object_name = 'events'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        context['count'] = self.object_list.count()
        context['alltag'] = Tag.objects.order_by('id')
        taglist = self.request.GET.getlist('tag')
        context['tags'] = Tag.objects.filter(id__in=taglist)
        search_form = EventSearchForm(self.request.GET)
        context['search_form'] = search_form
        context['30_day_labels'] = self.thirty_day_labels()
        context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = Event.objects.order_by('-publish_timestamp')
        tag = self.request.GET.get('tag')
        if tag is not None:
            query = query.filter(tags__id=tag)
        org = self.request.GET.get('org')
        if org is not None:
            query = query.filter(orgc=org)
        level = self.request.GET.get('level')
        if level is not None:
            query = query.filter(threat_level_id=level)
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(info__icontains=keyword)).order_by('-publish_timestamp')
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(publish_timestamp__gte=from_date, publish_timestamp__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

class EventDetailView(PaginationMixin, ListView):
    model = Attribute
    template_name = 'threat/event_detail.html'
    context_object_name = 'attributes'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        pk = self.kwargs['pk']
        event_obj = Event.objects.get(pk=pk)
        objects_obj = Object.objects.filter(event=pk)
        context = super().get_context_data(**kwargs)
        context['event'] = event_obj
        context['objects'] = objects_obj
        context['categories'] = event_obj.getUniqCategory()
        context['types'] = event_obj.getUniqType()
        context['count'] = self.object_list.count()
        return context

    def get_queryset(self):
        pk = self.kwargs['pk']
        query = Attribute.objects.filter(event=pk).order_by('id')
        category = self.request.GET.get('category')
        type = self.request.GET.get('type')
        if category is not None:
            query = query.filter(category=category)
        if type is not None:
            query = query.filter(type=type)
        return query

class AttributeListView(PaginationMixin, ListView):
    model = Attribute
    template_name = 'threat/attribute_list.html'
    context_object_name = 'attributes'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        attr = Attribute.objects.all()
        context = super().get_context_data(**kwargs)
        context['categories'] = attr.values_list('category', flat=True).order_by('category').distinct()
        context['types'] = attr.values_list('type', flat=True).order_by('type').distinct()
        context['count'] = self.object_list.count()
        search_form = AttributeSearchForm(self.request.GET)
        context['search_form'] = search_form
        #context['30_day_labels'] = self.thirty_day_labels()
        #context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = Attribute.objects.order_by('-timestamp')
        category = self.request.GET.get('category')
        type = self.request.GET.get('type')
        if category is not None:
            query = query.filter(category=category)
        if type is not None:
            query = query.filter(type=type)
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(value__icontains=keyword)).order_by('-timestamp')
        return query

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(timestamp__gte=from_date, timestamp__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

class OrgListView(ListView):
    model = Org
    template_name = 'threat/org_list.html'
    context_object_name = 'orgs'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        count = self.object_list.count()
        context['count'] = count
        return context

    def get_queryset(self):
        query = Org.objects.order_by('id')
        return query

class TagListView(ListView):
    model = Tag
    template_name = 'threat/tag_list.html'
    context_object_name = 'tags'

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        count = self.object_list.count()
        context['count'] = count
        return context

    def get_queryset(self):
        query = Tag.objects.order_by('id')
        return query

/n/n/n/apps/threat_hunter/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render, redirect
from django.views.generic import ListView, DetailView
from django.views.generic.edit import CreateView, UpdateView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q, Count
from .models import Hunt
from apps.threat.models import Event
from .forms import HuntForm
import csv
from io import StringIO, BytesIO
from codecs import BOM_UTF8
from pytz import timezone
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPConnection

class IndexView(PaginationMixin, ListView):
    model = Hunt
    template_name = 'threat_hunter/index.html'
    context_object_name = 'hunts'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self):
        query = Hunt.objects.order_by('id')
        query = query.annotate(count=Count('events'))
        return query

class EventListView(PaginationMixin, ListView):
    model = Event
    template_name = 'threat_hunter/event_list.html'
    context_object_name = 'events'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request, pk):
        pk = self.kwargs['pk']
        query = Event.objects.filter(Q(id__in=Hunt(id=pk).events.all())).order_by('-publish_timestamp')
        return query

    def get(self, request, pk):
        self.object_list = self.get_queryset(request, pk)
        context = self.get_context_data()
        return render(request, 'threat_hunter/event_list.html', context)

class HuntCreateView(CreateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'threat_hunter/hunt_form.html'

    def get_success_url(self):
        self.object.run()
        return '/threat_hunter'
        
class HuntUpdateView(UpdateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'threat_hunter/hunt_edit_form.html'

    def get_success_url(self):
        self.object.run()
        return '/threat_hunter'

def hunt_del(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.delete()
    return redirect('threat_hunter:index')

def hunt_export(request, pk):
    stream = StringIO()
    writer = csv.writer(stream)
    header = ['#published', 'date', 'info', 'level', 'attribute_count', 'org']
    writer.writerow(header)
    for event in Event.objects.filter(id__in=Hunt(id=pk).events.all()).order_by('publish_timestamp'):
        dt = event.publish_timestamp.astimezone(timezone('Asia/Tokyo'))
        row = [dt, event.date, event.info, event.get_threat_level_id_display(), event.attribute_count, event.org.name]
        writer.writerow(row)
    b_stream = BytesIO(BOM_UTF8 + stream.getvalue().encode('utf8'))
    response = HttpResponse(b_stream.getvalue(), content_type=""text/csv"")
    response[""Content-Disposition""] = ""filename=hunter%s.csv"" % pk
    return response

def hunt_switch_notice(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.notice == True:
        hunt.setNoticeFalse()
    else:
        hunt.setNoticeTrue()
    hunt.run()
    return redirect('threat_hunter:index')

def hunt_switch_enable(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.enable == True:
        hunt.setDisable()
    else:
        hunt.setEnable()
        hunt.run()
    return redirect('threat_hunter:index')

/n/n/n/apps/twitter/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render
from django.views.generic import ListView, DetailView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q
from .models import tweet
from .forms import SearchForm
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPSConnection
from datetime import datetime, timezone, timedelta

class IndexView(PaginationMixin, ListView):
    template_name = 'twitter/index.html'
    context_object_name = 'tws'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        search_form = SearchForm(self.request.GET)
        context['search_form'] = search_form
        count = self.object_list.count()
        context['count'] = count
        context['30_day_labels'] = self.thirty_day_labels()
        context['30_day_data'] = self.thirty_day_data()
        return context

    def get_queryset(self):
        query = tweet.objects.order_by('-datetime')
        keyword = self.request.GET.get('keyword')
        if keyword is not None:
            query = query.filter(Q(text__icontains=keyword)).order_by('-datetime')
        return query

#    def post(self, request):
#        search_form = SearchForm(request.POST)
#        if not search_form.is_valid():
#            return HttpResponseRedirect('/')
#        self.object_list = self.get_queryset()
#        context = self.get_context_data()
#        context['search_form'] = search_form
#        return render(request, 'twitter/index.html', context)

    def thirty_day_data(self):
        data = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            from_date = today - timedelta(days=day)
            to_date = today - timedelta(days=day-1)
            count = self.object_list.filter(datetime__gte=from_date, datetime__lte=to_date).count()
            data.append(count)
        return data

    def thirty_day_labels(self):
        labels = []
        today = datetime.now(timezone(timedelta(hours=+9), 'JST'))
        today = today.replace(hour=0, minute=0, second=0, microsecond=0)
        for day in range(30)[::-1]:
            date = today - timedelta(days=day)
            label = date.strftime('%Y-%m-%d')
            labels.append(label)
        return labels

def expand_url(request):
    url = request.GET.get('url', None)
    exurl = expand(url)
    while exurl != url:
        url = exurl
        exurl = expand(url)
    return JsonResponse({'url': exurl})

def expand(url):
    o = urlparse(url)
    con = HTTPSConnection(o.netloc)
    con.request('HEAD', o.path)
    res = con.getresponse()
    if res.getheader('location') == None:
        return url
    return res.getheader('location')
/n/n/n/apps/twitter_hunter/views.py/n/nfrom django.http import HttpResponse, HttpResponseRedirect
from django.shortcuts import get_object_or_404, render, redirect
from django.views.generic import ListView, DetailView
from django.views.generic.edit import CreateView, UpdateView
from pure_pagination.mixins import PaginationMixin
from django.db.models import Q, Count
from .models import tweet, Hunt
from .forms import HuntForm
import csv
from io import StringIO, BytesIO
from codecs import BOM_UTF8
from pytz import timezone
from django.http import JsonResponse
from urllib.parse import urlparse
from http.client import HTTPSConnection

class IndexView(PaginationMixin, ListView):
    template_name = 'twitter_hunter/index.html'
    context_object_name = 'hts'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request):
        query = Hunt.objects.order_by('id')
        query = query.annotate(count=Count('tweet'))
        return query

    def get(self, request):
        self.object_list = self.get_queryset(request)
        context = self.get_context_data()
        return render(request, 'twitter_hunter/index.html', context)

class TweetsView(PaginationMixin, ListView):
    template_name = 'twitter_hunter/tweets.html'
    context_object_name = 'tws'
    paginate_by = 30

    def get_context_data(self, **kwargs):
        context = super().get_context_data(**kwargs)
        return context

    def get_queryset(self, request, pk):
        query = tweet.objects.filter(hunt_id=Hunt(id=pk)).order_by('-datetime')
        return query

    def get(self, request, pk):
        self.object_list = self.get_queryset(request, pk)
        context = self.get_context_data()
        return render(request, 'twitter_hunter/tweets.html', context)

#def hunt_add(request):
#    hunt = Hunt()
#    if request.method == 'POST':
#        form = HuntForm(request.POST, instance=hunt)
#        if form.is_valid():
#            hunt = form.save(commit=False)
#            hunt.save()
#            hunt.start()
#            return redirect('twitter_hunter:index')
#    else:
#        form = HuntForm(instance=hunt)
#    return render(request, 'twitter_hunter/hunt_edit.html', dict(form=form))

class HuntCreateView(CreateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'twitter_hunter/hunt_form.html'

    def get_success_url(self):
        self.object.start()
        return '/twitter_hunter'

class HuntUpdateView(UpdateView):
    model = Hunt
    form_class = HuntForm
    template_name = 'twitter_hunter/hunt_edit_form.html'

    def get_success_url(self):
        self.object.restart()
        return '/twitter_hunter'

def hunt_del(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.stop()
    hunt.delete()
    return redirect('twitter_hunter:index')

def hunt_export(request, pk):
    stream = StringIO()
    writer = csv.writer(stream)
    header = ['#datetime', 'user', 'screen_name', 'text']
    writer.writerow(header)
    for tw in tweet.objects.filter(hunt_id=Hunt(id=pk)).order_by('datetime'):
        dt = tw.datetime.astimezone(timezone('Asia/Tokyo'))
        row = [dt, tw.user, tw.screen_name, tw.text]
        writer.writerow(row)
    b_stream = BytesIO(BOM_UTF8 + stream.getvalue().encode('utf8'))
    response = HttpResponse(b_stream.getvalue(), content_type=""text/csv"")
    response[""Content-Disposition""] = ""filename=hunter%s.csv"" % pk
    return response

def hunt_switch_notice(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    hunt.stop()
    if hunt.notice == True:
        hunt.setNoticeFalse()
    else:
        hunt.setNoticeTrue()
    hunt.start()
    return redirect('twitter_hunter:index')

def hunt_switch_enable(request, pk):
    hunt = get_object_or_404(Hunt, id=pk)
    if hunt.enable == True:
        hunt.setDisable()
        hunt.stop()
    else:
        hunt.setEnable()
        hunt.start()
    return redirect('twitter_hunter:index')

def expand_url(request):
    url = request.GET.get('url', None)
    exurl = expand(url)
    while exurl != url:
        url = exurl
        exurl = expand(url)
    return JsonResponse({'url': exurl})

def expand(url):
    o = urlparse(url)
    con = HTTPSConnection(o.netloc)
    con.request('HEAD', o.path)
    res = con.getresponse()
    if res.getheader('location') == None:
        return url
    return res.getheader('location')

/n/n/n/apps/url/forms.py/n/nfrom django import forms

class SearchForm(forms.Form):
    keyword = forms.CharField(label='', max_length=100, required=True)
    keyword.widget.attrs['class'] = 'form-control mr-sm-2 my-2'
    keyword.widget.attrs['placeholder'] = 'Lookup URL'
/n/n/n",1
22,22,be720513bcc09df7b0787bc6bde033023933061f,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
23,23,be720513bcc09df7b0787bc6bde033023933061f,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
134,134,001ff508081a893d0cf81df1214dbd234606c360,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path)).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nfrom __future__ import unicode_literals

import mimetypes
import unittest
from os import path

from django.conf.urls.static import static
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, filename))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/non_existing_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super(StaticHelperTest, self).setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super(StaticHelperTest, self).tearDown()
        urls.urlpatterns = self._old_views_urlpatterns


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0
135,135,001ff508081a893d0cf81df1214dbd234606c360,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
from __future__ import unicode_literals

import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.six.moves.urllib.parse import unquote
from django.utils.translation import ugettext as _, ugettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(unquote(path))
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = ugettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1
8,8,a1f948b468b6621083a03b0d53432341b7a4d753,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes
import unittest
from os import path
from urllib.parse import quote

from django.conf.urls.static import static
from django.core.exceptions import ImproperlyConfigured
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/nonexistent_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super().setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super().tearDown()
        urls.urlpatterns = self._old_views_urlpatterns

    def test_prefix(self):
        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')

    @override_settings(DEBUG=False)
    def test_debug_off(self):
        """"""No URLs are served if DEBUG=False.""""""
        self.assertEqual(static('test'), [])

    def test_empty_prefix(self):
        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):
            static('')

    def test_special_prefix(self):
        """"""No URLs are served if prefix contains '://'.""""""
        self.assertEqual(static('http://'), [])


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0
9,9,a1f948b468b6621083a03b0d53432341b7a4d753,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path)
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1
78,78,0e6427c6dd44c434849c8c80f8bb3ff36013dd43,"JSHUtil/IORedirect.py/n/nimport sys
import os
import contextlib

def IO_redirection(cmd):
    """"""
    Redirect input/output based characters < > >> in the command.
    < will be redirecting input, > will be output and >> is output append
    """"""
    infile = None
    inputred = False
    outfile = None
    outred = False
    appfile = None
    appred = False
    #check for input redirection first
    if(""<"" in cmd):
        infile, inputred = SetUpInputRedirect(cmd)

    if("">"" in cmd):
        outfile, outred = SetUpOutputRedirect(cmd)

    if("">>"" in cmd):
        appfile, appred = SetUpAppendRedirect(cmd)

    return (inputred, outred, appred, infile, outfile, appfile)

def SetUpInputRedirect(cmd):
    return _find_redirect(""<"", cmd)

def SetUpOutputRedirect(cmd):
    return _find_redirect("">"", cmd)

def SetUpAppendRedirect(cmd):
    return _find_redirect("">>"", cmd)

def _find_redirect(symbol, cmd):
    i = cmd.index(symbol)
    rfile = cmd[i + 1]
    del cmd[i + 1]
    del cmd[i]
    return rfile, True
/n/n/n",0
79,79,0e6427c6dd44c434849c8c80f8bb3ff36013dd43,"/JSHUtil/IORedirect.py/n/nimport sys
import os
import contextlib

def IO_redirection(cmd):
    """"""
    Redirect input/output based characters < > >> in the command.
    < will be redirecting input, > will be output and >> is output append
    """"""
    #check for input redirection first
    if(""<"" in cmd):
        SetUpInputRedirect(cmd)

    if("">"" in cmd):
        SetUpOutputRedirect(cmd)

    if("">>"" in cmd):
        SetUpAppendRedirect(cmd)

def SetUpInputRedirect(cmd):
    import pdb; pdb.set_trace()
    pass

def SetUpOutputRedirect(cmd):
    i = cmd.index("">"")
    sys.stdout = open(cmd[i + 1], ""w"")
    del cmd[i + 1]
    del cmd[i]
    print(""this is a test"")

def SetUpAppendRedirect(cmd):
    import pdb; pdb.set_trace()
    pass
/n/n/n",1
162,162,97306a12d3deff023be84c72ba2457e99d2f196b,"ckanext/data_qld/actions.py/n/nimport ckan.lib.base as base
import ckan.lib.helpers as helpers
import ckan.lib.mailer as mailer
import ckan.model as model
import ckan.plugins as plugins
import ckanext.datarequests.db as db
import ckanext.datarequests.validator as validator
import datetime
import logging
from pylons import config

import constants

c = plugins.toolkit.c
log = logging.getLogger(__name__)
tk = plugins.toolkit

# Avoid user_show lag
USERS_CACHE = {}


def _get_user(user_id):
    try:
        if user_id in USERS_CACHE:
            return USERS_CACHE[user_id]
        else:
            user = tk.get_action('user_show')({'ignore_auth': True}, {'id': user_id})
            USERS_CACHE[user_id] = user
            return user
    except Exception as e:
        log.warn(e)


def _get_organization(organization_id):
    try:
        organization_show = tk.get_action('organization_show')
        return organization_show({'ignore_auth': True}, {'id': organization_id})
    except Exception as e:
        log.warn(e)


def _get_package(package_id):
    try:
        package_show = tk.get_action('package_show')
        return package_show({'ignore_auth': True}, {'id': package_id})
    except Exception as e:
        log.warn(e)


def _dictize_datarequest(datarequest):
    # Transform time
    open_time = str(datarequest.open_time)
    # Close time can be None and the transformation is only needed when the
    # fields contains a valid date
    close_time = datarequest.close_time
    close_time = str(close_time) if close_time else close_time

    # Convert the data request into a dict
    data_dict = {
        'id': datarequest.id,
        'user_id': datarequest.user_id,
        'title': datarequest.title,
        'description': datarequest.description,
        'organization_id': datarequest.organization_id,
        'open_time': open_time,
        'accepted_dataset_id': datarequest.accepted_dataset_id,
        'close_time': close_time,
        'closed': datarequest.closed,
        'user': _get_user(datarequest.user_id),
        'organization': None,
        'accepted_dataset': None,
        'followers': 0,
        'dataset_url': helpers.url_for(controller='ckanext.datarequests.controllers.ui_controller:DataRequestsUI',
                                       action='show', id=datarequest.id, qualified=True)
    }

    if datarequest.organization_id:
        data_dict['organization'] = _get_organization(datarequest.organization_id)

    if datarequest.accepted_dataset_id:
        data_dict['accepted_dataset'] = _get_package(datarequest.accepted_dataset_id)

    data_dict['followers'] = db.DataRequestFollower.get_datarequest_followers_number(
        datarequest_id=datarequest.id)

    return data_dict


def _undictize_datarequest_basic(data_request, data_dict):
    data_request.title = data_dict['title']
    data_request.description = data_dict['description']
    organization = data_dict['organization_id']
    data_request.organization_id = organization if organization else None


def _send_mail(user_ids, action_type, datarequest, job_title):
    for user_id in user_ids:
        try:
            user_data = model.User.get(user_id)
            extra_vars = {
                'datarequest': datarequest,
                'user': user_data,
                'site_title': config.get('ckan.site_title'),
                'site_url': config.get('ckan.site_url')
            }
            subject = base.render_jinja2('emails/subjects/{0}.txt'.format(action_type), extra_vars)
            body = base.render_jinja2('emails/bodies/{0}.txt'.format(action_type), extra_vars)
            tk.enqueue_job(mailer.mail_user, [user_data, subject, body], title=job_title)
        except Exception:
            logging.exception(""Error sending notification to {0}"".format(user_id))


def _get_admin_users_from_organasition(datarequest_dict):
    # Data QLD modification.
    users = set([user['id'] for user in datarequest_dict['organization']['users'] if user.get('capacity') == 'admin'])
    return users


# Copied from ckanext.datarequests.actions. Please keep up to date with any extension updates
@tk.chained_action
def create_datarequest(original_action, context, data_dict):
    """"""
    Action to create a new data request. The function checks the access rights
    of the user before creating the data request. If the user is not allowed
    a NotAuthorized exception will be risen.

    In addition, you should note that the parameters will be checked and an
    exception (ValidationError) will be risen if some of these parameters are
    not valid.

    Data QLD modification
    Will send email notification to users of assigned organisation with admin access

    :param title: The title of the data request
    :type title: string

    :param description: A brief description for your data request
    :type description: string

    :param organiztion_id: The ID of the organization you want to asign the
        data request (optional).
    :type organization_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict
    """"""

    model = context['model']
    session = context['session']

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.CREATE_DATAREQUEST, context, data_dict)

    # Validate data
    validator.validate_datarequest(context, data_dict)

    # Store the data
    data_req = db.DataRequest()
    _undictize_datarequest_basic(data_req, data_dict)
    data_req.user_id = context['auth_user_obj'].id
    data_req.open_time = datetime.datetime.now()

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    if datarequest_dict['organization']:
        # Data QLD modification
        users = _get_admin_users_from_organasition(datarequest_dict)
        users.discard(context['auth_user_obj'].id)
        _send_mail(users, 'new_datarequest_organisation', datarequest_dict, 'Data Request Created Email')

    return datarequest_dict


#  Copied from ckanext.datarequests.actions. Please keep up to date with any action updates
@tk.chained_action
def update_datarequest(original_action, context, data_dict):
    """"""
    Action to update a data request. The function checks the access rights of
    the user before updating the data request. If the user is not allowed
    a NotAuthorized exception will be risen.

    In addition, you should note that the parameters will be checked and an
    exception (ValidationError) will be risen if some of these parameters are
    invalid.

    Data QLD modification
    Will send email notification if organisation was changed to users of assigned organisation with admin access

    :param id: The ID of the data request to be updated
    :type id: string

    :param title: The title of the data request
    :type title: string

    :param description: A brief description for your data request
    :type description: string

    :param organiztion_id: The ID of the organization you want to asign the
        data request.
    :type organization_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict
    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.UPDATE_DATAREQUEST, context, data_dict)

    # Get the initial data
    result = db.DataRequest.get(id=datarequest_id)
    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    data_req = result[0]

    # Avoid the validator to return an error when the user does not change the title
    context['avoid_existing_title_check'] = data_req.title == data_dict['title']

    # Validate data
    validator.validate_datarequest(context, data_dict)

    # Data QLD modification
    organisation_updated = data_req.organization_id != data_dict['organization_id']
    if organisation_updated:
        unassigned_organisation_id = data_req.organization_id

    # Set the data provided by the user in the data_red
    _undictize_datarequest_basic(data_req, data_dict)

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    if datarequest_dict['organization'] and organisation_updated:
        # Data QLD modification
        # Email Admin users of the assigned organisation
        users = _get_admin_users_from_organasition(datarequest_dict)
        users.discard(context['auth_user_obj'].id)
        _send_mail(users, 'new_datarequest_organisation', datarequest_dict, 'Data Request Assigned Email')
        # Email Admin users of unassigned organisation
        org_dict = {
            'organization': _get_organization(unassigned_organisation_id)
        }
        users = _get_admin_users_from_organasition(org_dict)
        users.discard(context['auth_user_obj'].id)
        _send_mail(users, 'unassigned_datarequest_organisation', datarequest_dict, 'Data Request Unassigned Email')

    return datarequest_dict


#  Copied from ckanext.datarequests.actions. Please keep up to date with any action updates
@tk.chained_action
def close_datarequest(original_action, context, data_dict):
    """"""
    Action to close a data request. Access rights will be checked before
    closing the data request. If the user is not allowed, a NotAuthorized
    exception will be risen.

    Data QLD modification
    Will send email notification to the data request creator

    :param id: The ID of the data request to be closed
    :type id: string

    :param accepted_dataset_id: The ID of the dataset accepted as solution
        for this data request
    :type accepted_dataset_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict

    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    # Check id
    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.CLOSE_DATAREQUEST, context, data_dict)

    # Get the data request
    result = db.DataRequest.get(id=datarequest_id)
    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    # Validate data
    validator.validate_datarequest_closing(context, data_dict)

    data_req = result[0]

    # Was the data request previously closed?
    if data_req.closed:
        raise tk.ValidationError([tk._('This Data Request is already closed')])

    data_req.closed = True
    data_req.accepted_dataset_id = data_dict.get('accepted_dataset_id', None)
    data_req.close_time = datetime.datetime.now()

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    # Mailing
    users = [data_req.user_id]
    _send_mail(users, 'close_datarequest_creator', datarequest_dict, 'Data Request Closed Send Email')

    return datarequest_dict


def open_datarequest(context, data_dict):
    """"""
    Action to open a data request. Access rights will be checked before
    opening the data request. If the user is not allowed, a NotAuthorized
    exception will be risen.

    :param id: The ID of the data request to be closed
    :type id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict

    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    # Check id
    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

        # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.OPEN_DATAREQUEST, context, data_dict)

    # Get the data request
    result = db.DataRequest.get(id=datarequest_id)

    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    data_req = result[0]
    data_req.closed = False
    data_req.accepted_dataset_id = None
    data_req.close_time = None

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    # Mailing
    users = [data_req.user_id]
    # Creator email
    _send_mail(users, 'open_datarequest_creator', datarequest_dict, 'Data Request Opened Creator Email')
    if datarequest_dict['organization']:
        users = _get_admin_users_from_organasition(datarequest_dict)
        # Admins of organisation email
        _send_mail(users, 'open_datarequest_organisation', datarequest_dict, 'Data Request Opened Admins Email')

    return datarequest_dict
/n/n/n",0
163,163,97306a12d3deff023be84c72ba2457e99d2f196b,"/ckanext/data_qld/actions.py/n/nimport ckan.lib.base as base
import ckan.lib.helpers as helpers
import ckan.lib.mailer as mailer
import ckan.model as model
import ckan.plugins as plugins
import ckanext.datarequests.db as db
import ckanext.datarequests.validator as validator
import datetime
import logging
from pylons import config

import constants

c = plugins.toolkit.c
log = logging.getLogger(__name__)
tk = plugins.toolkit

# Avoid user_show lag
USERS_CACHE = {}


def _get_user(user_id):
    try:
        if user_id in USERS_CACHE:
            return USERS_CACHE[user_id]
        else:
            user = tk.get_action('user_show')({'ignore_auth': True}, {'id': user_id})
            USERS_CACHE[user_id] = user
            return user
    except Exception as e:
        log.warn(e)


def _get_organization(organization_id):
    try:
        organization_show = tk.get_action('organization_show')
        return organization_show({'ignore_auth': True}, {'id': organization_id})
    except Exception as e:
        log.warn(e)


def _get_package(package_id):
    try:
        package_show = tk.get_action('package_show')
        return package_show({'ignore_auth': True}, {'id': package_id})
    except Exception as e:
        log.warn(e)


def _dictize_datarequest(datarequest):
    # Transform time
    open_time = str(datarequest.open_time)
    # Close time can be None and the transformation is only needed when the
    # fields contains a valid date
    close_time = datarequest.close_time
    close_time = str(close_time) if close_time else close_time

    # Convert the data request into a dict
    data_dict = {
        'id': datarequest.id,
        'user_id': datarequest.user_id,
        'title': datarequest.title,
        'description': datarequest.description,
        'organization_id': datarequest.organization_id,
        'open_time': open_time,
        'accepted_dataset_id': datarequest.accepted_dataset_id,
        'close_time': close_time,
        'closed': datarequest.closed,
        'user': _get_user(datarequest.user_id),
        'organization': None,
        'accepted_dataset': None,
        'followers': 0,
        'dataset_url': helpers.url_for(controller='ckanext.datarequests.controllers.ui_controller:DataRequestsUI',
                                       action='show', id=datarequest.id, qualified=True)
    }

    if datarequest.organization_id:
        data_dict['organization'] = _get_organization(datarequest.organization_id)

    if datarequest.accepted_dataset_id:
        data_dict['accepted_dataset'] = _get_package(datarequest.accepted_dataset_id)

    data_dict['followers'] = db.DataRequestFollower.get_datarequest_followers_number(
        datarequest_id=datarequest.id)

    return data_dict


def _undictize_datarequest_basic(data_request, data_dict):
    data_request.title = data_dict['title']
    data_request.description = data_dict['description']
    organization = data_dict['organization_id']
    data_request.organization_id = organization if organization else None


def _send_mail(user_ids, action_type, datarequest):
    for user_id in user_ids:
        try:
            user_data = model.User.get(user_id)
            extra_vars = {
                'datarequest': datarequest,
                'user': user_data,
                'site_title': config.get('ckan.site_title'),
                'site_url': config.get('ckan.site_url')
            }

            subject = base.render_jinja2('emails/subjects/{0}.txt'.format(action_type), extra_vars)
            body = base.render_jinja2('emails/bodies/{0}.txt'.format(action_type), extra_vars)

            mailer.mail_user(user_data, subject, body)

        except Exception:
            logging.exception(""Error sending notification to {0}"".format(user_id))


def _get_admin_users_from_organasition(datarequest_dict):
    # Data QLD modification.
    users = set([user['id'] for user in datarequest_dict['organization']['users'] if user.get('capacity') == 'admin'])
    return users


# Copied from ckanext.datarequests.actions. Please keep up to date with any extension updates
@tk.chained_action
def create_datarequest(original_action, context, data_dict):
    """"""
    Action to create a new data request. The function checks the access rights
    of the user before creating the data request. If the user is not allowed
    a NotAuthorized exception will be risen.

    In addition, you should note that the parameters will be checked and an
    exception (ValidationError) will be risen if some of these parameters are
    not valid.

    Data QLD modification
    Will send email notification to users of assigned organisation with admin access

    :param title: The title of the data request
    :type title: string

    :param description: A brief description for your data request
    :type description: string

    :param organiztion_id: The ID of the organization you want to asign the
        data request (optional).
    :type organization_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict
    """"""

    model = context['model']
    session = context['session']

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.CREATE_DATAREQUEST, context, data_dict)

    # Validate data
    validator.validate_datarequest(context, data_dict)

    # Store the data
    data_req = db.DataRequest()
    _undictize_datarequest_basic(data_req, data_dict)
    data_req.user_id = context['auth_user_obj'].id
    data_req.open_time = datetime.datetime.now()

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    if datarequest_dict['organization']:
        # Data QLD modification
        users = _get_admin_users_from_organasition(datarequest_dict)
        users.discard(context['auth_user_obj'].id)
        tk.enqueue_job(_send_mail, [users, 'new_datarequest_organisation', datarequest_dict], title=u'Data Request Created Email')

    return datarequest_dict


#  Copied from ckanext.datarequests.actions. Please keep up to date with any action updates
@tk.chained_action
def update_datarequest(original_action, context, data_dict):
    """"""
    Action to update a data request. The function checks the access rights of
    the user before updating the data request. If the user is not allowed
    a NotAuthorized exception will be risen.

    In addition, you should note that the parameters will be checked and an
    exception (ValidationError) will be risen if some of these parameters are
    invalid.

    Data QLD modification
    Will send email notification if organisation was changed to users of assigned organisation with admin access

    :param id: The ID of the data request to be updated
    :type id: string

    :param title: The title of the data request
    :type title: string

    :param description: A brief description for your data request
    :type description: string

    :param organiztion_id: The ID of the organization you want to asign the
        data request.
    :type organization_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict
    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.UPDATE_DATAREQUEST, context, data_dict)

    # Get the initial data
    result = db.DataRequest.get(id=datarequest_id)
    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    data_req = result[0]

    # Avoid the validator to return an error when the user does not change the title
    context['avoid_existing_title_check'] = data_req.title == data_dict['title']

    # Validate data
    validator.validate_datarequest(context, data_dict)

    # Data QLD modification
    organisation_updated = data_req.organization_id != data_dict['organization_id']
    if organisation_updated:
        unassigned_organisation_id = data_req.organization_id

    # Set the data provided by the user in the data_red
    _undictize_datarequest_basic(data_req, data_dict)

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    if datarequest_dict['organization'] and organisation_updated:
        # Data QLD modification
        # Email Admin users of the assigned organisation
        users = _get_admin_users_from_organasition(datarequest_dict)
        users.discard(context['auth_user_obj'].id)
        tk.enqueue_job(_send_mail, [users, 'new_datarequest_organisation', datarequest_dict], title=u'Data Request Assigned Email')
        # Email Admin users of unassigned organisation
        org_dict = {
            'organization': _get_organization(unassigned_organisation_id)
        }
        users = _get_admin_users_from_organasition(org_dict)
        users.discard(context['auth_user_obj'].id)
        tk.enqueue_job(_send_mail, [users, 'unassigned_datarequest_organisation', datarequest_dict], title=u'Data Request Unassigned Email')

    return datarequest_dict


#  Copied from ckanext.datarequests.actions. Please keep up to date with any action updates
@tk.chained_action
def close_datarequest(original_action, context, data_dict):
    """"""
    Action to close a data request. Access rights will be checked before
    closing the data request. If the user is not allowed, a NotAuthorized
    exception will be risen.

    Data QLD modification
    Will send email notification to the data request creator

    :param id: The ID of the data request to be closed
    :type id: string

    :param accepted_dataset_id: The ID of the dataset accepted as solution
        for this data request
    :type accepted_dataset_id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict

    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    # Check id
    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

    # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.CLOSE_DATAREQUEST, context, data_dict)

    # Get the data request
    result = db.DataRequest.get(id=datarequest_id)
    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    # Validate data
    validator.validate_datarequest_closing(context, data_dict)

    data_req = result[0]

    # Was the data request previously closed?
    if data_req.closed:
        raise tk.ValidationError([tk._('This Data Request is already closed')])

    data_req.closed = True
    data_req.accepted_dataset_id = data_dict.get('accepted_dataset_id', None)
    data_req.close_time = datetime.datetime.now()

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    # Mailing
    users = [data_req.user_id]
    tk.enqueue_job(_send_mail, [users, 'close_datarequest_creator', datarequest_dict], title=u'Data Request Closed Send Email')

    return datarequest_dict


def open_datarequest(context, data_dict):
    """"""
    Action to open a data request. Access rights will be checked before
    opening the data request. If the user is not allowed, a NotAuthorized
    exception will be risen.

    :param id: The ID of the data request to be closed
    :type id: string

    :returns: A dict with the data request (id, user_id, title, description,
        organization_id, open_time, accepted_dataset, close_time, closed,
        followers)
    :rtype: dict

    """"""

    model = context['model']
    session = context['session']
    datarequest_id = data_dict.get('id', '')

    # Check id
    if not datarequest_id:
        raise tk.ValidationError(tk._('Data Request ID has not been included'))

        # Init the data base
    db.init_db(model)

    # Check access
    tk.check_access(constants.OPEN_DATAREQUEST, context, data_dict)

    # Get the data request
    result = db.DataRequest.get(id=datarequest_id)

    if not result:
        raise tk.ObjectNotFound(tk._('Data Request %s not found in the data base') % datarequest_id)

    data_req = result[0]
    data_req.closed = False
    data_req.accepted_dataset_id = None
    data_req.close_time = None

    session.add(data_req)
    session.commit()

    datarequest_dict = _dictize_datarequest(data_req)

    # Mailing
    users = [data_req.user_id]
    # Creator email
    tk.enqueue_job(_send_mail, [users, 'open_datarequest_creator', datarequest_dict], title=u'Data Request Opened Creator Email')
    if datarequest_dict['organization']:
        users = _get_admin_users_from_organasition(datarequest_dict)
        # Admins of organisation email
        tk.enqueue_job(_send_mail, [users, 'open_datarequest_organisation', datarequest_dict], title=u'Data Request Opened Admins Email')

    return datarequest_dict
/n/n/n",1
16,16,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
17,17,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
122,122,5b27ed28614c11178a2c7d8120dc5db99abee175,"src/eduid_webapp/authn/acs_actions.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
from __future__ import absolute_import

from time import time
from saml2.ident import code
from flask import session, request, redirect, current_app
from eduid_common.authn.loa import get_loa
from eduid_webapp.authn.acs_registry import acs_action
from eduid_webapp.authn.helpers import verify_relay_state


@acs_action('login-action')
def login_action(session_info, user):
    """"""
    Upon successful login in the IdP, store login info in the session
    and redirect back to the app that asked for authn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    current_app.logger.info(""User {!r} logging in."".format(user))
    session['_saml2_session_name_id'] = code(session_info['name_id'])
    session['eduPersonPrincipalName'] = user.eppn
    session['user_eppn'] = user.eppn
    loa = get_loa(current_app.config.get('AVAILABLE_LOA'), session_info)
    session['eduPersonAssurance'] = loa
    session.persist()

    # redirect the user to the view where he came from
    relay_state = verify_relay_state(request.form.get('RelayState', '/'))
    current_app.logger.debug('Redirecting to the RelayState: ' + relay_state)
    response = redirect(location=relay_state)
    session.set_cookie(response)
    current_app.logger.info('Redirecting user {!r} to {!r}'.format(user, relay_state))
    return response


@acs_action('change-password-action')
def chpass_action(session_info, user):
    """"""
    Upon successful reauthn in the IdP,
    set a timestamp in the session (key reauthn-for-chpass)
    and redirect back to the app that asked for reauthn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    return _reauthn('reauthn-for-chpass', session_info, user)


@acs_action('terminate-account-action')
def term_account_action(session_info, user):
    """"""
    Upon successful reauthn in the IdP,
    set a timestamp in the session (key reauthn-for-termination)
    and redirect back to the app that asked for reauthn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    return _reauthn('reauthn-for-termination', session_info, user)


def _reauthn(reason, session_info, user):

    current_app.logger.info(""Reauthenticating user {!r} for {!r}."".format(user, reason))
    session['_saml2_session_name_id'] = code(session_info['name_id'])
    session[reason] = int(time())
    session.persist()

    # redirect the user to the view where he came from
    relay_state = verify_relay_state(request.form.get('RelayState', '/'))
    current_app.logger.debug('Redirecting to the RelayState: ' + relay_state)
    return redirect(location=relay_state)
/n/n/nsrc/eduid_webapp/authn/helpers.py/n/n# -*- coding: utf-8 -*-
from __future__ import absolute_import

import time
import re
from flask import current_app
from hashlib import sha256

__author__ = 'lundberg'


def verify_auth_token(eppn, token, nonce, timestamp, generator=sha256):
    """"""
    Authenticate a user who just signed up, for user convenience.

    Authentication is done using a shared secret in the configuration of the
    authn and signup applications. The signup application can effectively
    log a new user in.

    :param eppn: the identifier of the user as string
    :param token: authentication token as string
    :param nonce: a public nonce for this authentication request as string
    :param timestamp: unixtime of signup application as hex string
    :param generator: hash function to use (default: SHA-256)
    :return: bool, True on valid authentication
    """"""
    current_app.logger.debug('Trying to authenticate user {} with auth token {}'.format(eppn, token))
    shared_key = current_app.config.get('TOKEN_LOGIN_SHARED_KEY')

    # check timestamp to make sure it is within -300..900 seconds from now
    now = int(time.time())
    ts = int(timestamp, 16)
    if (ts < now - 300) or (ts > now + 900):
        current_app.logger.debug('Auth token timestamp {} out of bounds ({} seconds from {})'.format(
            timestamp, ts - now, now))
        return False

    # verify there is a long enough nonce
    if len(nonce) < 16:
        current_app.logger.warning('Auth token nonce {} too short'.format(nonce))
        return False

    # verify token format
    expected = generator('{0}|{1}|{2}|{3}'.format(
        shared_key, eppn, nonce, timestamp)).hexdigest()
    if len(expected) != len(token):
        current_app.logger.warning('Auth token bad length')
        return False

    # constant time comparision of the hash, courtesy of
    # http://rdist.root.org/2009/05/28/timing-attack-in-google-keyczar-library/
    result = 0
    for x, y in zip(expected, token):
        result |= ord(x) ^ ord(y)
    current_app.logger.debug('Auth token match result: {}'.format(result == 0))
    return result == 0


def verify_relay_state(relay_state, safe_default='/'):
    """"""
    :param relay_state: Next url
    :param safe_default: The default if relay state is found unsafe

    :type safe_default: six.string_types
    :type relay_state: six.string_types

    :return: Safe relay state
    :rtype: six.string_types
    """"""
    current_app.logger.debug('Checking if relay state {} is safe'.format(relay_state))
    url_scheme = current_app.config['PREFERRED_URL_SCHEME']
    safe_domain = current_app.config['SAFE_RELAY_DOMAIN']
    safe_relay_regex = '^{}\:\/\/.*{}|^\/\w*'.format(url_scheme, safe_domain)
    if re.match(safe_relay_regex, relay_state):
        return relay_state
    current_app.logger.warning('Caught unsafe relay state: {}. Using safe relay state: {}.'.format(relay_state,
                                                                                                   safe_default))
    return safe_default
/n/n/nsrc/eduid_webapp/authn/tests/test_authn.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

import os
import time
import json
import base64
from hashlib import sha256

from werkzeug.exceptions import NotFound
from werkzeug.http import dump_cookie
from flask import session
from flask import Blueprint
from saml2.s_utils import deflate_and_base64_encode

from eduid_userdb.user import User
from eduid_userdb.data_samples import NEW_COMPLETED_SIGNUP_USER_EXAMPLE
from eduid_common.api.testing import EduidAPITestCase
from eduid_common.authn.cache import OutstandingQueriesCache
from eduid_common.authn.utils import get_location, no_authn_views
from eduid_common.authn.eduid_saml2 import get_authn_request
from eduid_common.authn.tests.responses import (auth_response,
                                                logout_response,
                                                logout_request)
from eduid_webapp.authn.app import authn_init_app
from eduid_common.api.app import eduid_init_app

try:
    from urllib import quote_plus
except ImportError:  # Python3
    from urllib.parse import quote_plus

import logging
logger = logging.getLogger(__name__)

HERE = os.path.abspath(os.path.dirname(__file__))


class AuthnAPITestBase(EduidAPITestCase):

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'SAML2_LOGIN_REDIRECT_URL': '/',
            'SAML2_LOGOUT_REDIRECT_URL': '/logged-out',
            'SAML2_SETTINGS_MODULE': saml_config,
            'TOKEN_LOGIN_SHARED_KEY': 'shared_secret',
            'TOKEN_LOGIN_SUCCESS_REDIRECT_URL': 'http://test.localhost/success',
            'TOKEN_LOGIN_FAILURE_REDIRECT_URL': 'http://test.localhost/failure',
            'SAFE_RELAY_DOMAIN': 'test.localhost'
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return authn_init_app('test.localhost', config)

    def add_outstanding_query(self, came_from):
        """"""
        Add a SAML2 authentication query to the queries cache.
        To be used before accessing the assertion consumer service.

        :param came_from: url to redirect back the client
                          after finishing with the authn service.
        :type came_from: str

        :return: the session token corresponding to the query
        :rtype: str
        """"""
        with self.app.test_request_context('/login'):
            self.app.dispatch_request()
            oq_cache = OutstandingQueriesCache(session)
            oq_cache.set(session.token, came_from)
            session.persist()
            return session.token

    def login(self, eppn, came_from):
        """"""
        Add a SAML2 authentication query to the queries cache,
        build a cookie with a session id corresponding to the added query,
        build a SAML2 authn response for the added query,
        and send both to the assertion consumer service,
        so that the user is logged in (the session corresponding to the cookie
        has her eppn).
        This method returns the cookie that has to be sent with any
        subsequent request that needs to be athenticated.

        :param eppn: the eppn of the user to be logged in
        :type eppn: str
        :param came_from: url to redirect back the client
                          after finishing with the authn service.
        :type came_from: str

        :return: the cookie corresponding to the authn session
        :rtype: str
        """"""
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)
        saml_response = auth_response(session_id, eppn)

        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': came_from}):

            response1 = self.app.dispatch_request()
            cookie = response1.headers['Set-Cookie']
            return cookie

    def authn(self, url, force_authn=False, next_url='/'):
        """"""
        Common code for the tests that need to send an authentication request.
        This checks that the client is redirected to the idp.

        :param url: the url of the desired authentication mode.
        :type url: str
        :param force_authn: whether to force reauthentication for an already
                            authenticated client
        :type force_authn: bool
        :param next_url: Next url
        :type next_url: six.string_types
        """"""
        with self.app.test_client() as c:
            resp = c.get('{}?next={}'.format(url, next_url))
            authn_req = get_location(get_authn_request(self.app.config,
                                                       session, next_url, None,
                                                       force_authn=force_authn))
            idp_url = authn_req.split('?')[0]
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(idp_url))
            relay_state = resp.location.split('&')[-1]
            quoted_next = 'RelayState={}'.format(quote_plus(next_url))
            self.assertEqual(quoted_next, relay_state)

    def acs(self, url, eppn, check_fn, came_from='/camefrom/'):
        """"""
        common code for the tests that need to access the assertion consumer service
        and then check the side effects of this access.

        :param url: the url of the desired authentication mode.
        :type url: str
        :param eppn: the eppn of the user to access the service
        :type eppn: str
        :param check_fn: the function that checks the side effects after accessing the acs
        :type check_fn: callable
        :param came_from: Relay state
        :type came_from: six.string_types
        """"""
        with self.app.test_client() as c:
            resp = c.get(url)
            cookie = resp.headers['Set-Cookie']
            token = session._session.token
            authr = auth_response(token, eppn)

        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(authr),
                                                 'RelayState': came_from}):

            oq_cache = OutstandingQueriesCache(session)
            oq_cache.set(token, came_from)

            resp = self.app.dispatch_request()

            self.assertEquals(resp.status_code, 302)
            self.assertEquals(resp.location, came_from)
            check_fn()

    def dump_session_cookie(self, session_id):
        """"""
        Get a cookie corresponding to an authenticated session.

        :param session_id: the token for the session
        :type session_id: str

        :return: the cookie
        """"""
        return dump_cookie(self.app.config.get('SESSION_COOKIE_NAME'), session_id,
                           max_age=float(self.app.config.get('PERMANENT_SESSION_LIFETIME')),
                           path=self.app.config.get('SESSION_COOKIE_PATH'),
                           domain=self.app.config.get('SESSION_COOKIE_DOMAIN'))


class AuthnAPITestCase(AuthnAPITestBase):
    """"""
    Tests to check the different modes of authentication.
    """"""

    def init_data(self):
        """"""
        Called from the parent class, so we can extend data initialized.
        """"""
        test_user = User(data=NEW_COMPLETED_SIGNUP_USER_EXAMPLE)  # eppn hubba-fooo
        self.app.central_userdb.save(test_user, check_sync=False)

    def test_login_authn(self):
        self.authn('/login')

    def test_login_authn_good_relay_state(self):
        self.authn('/login', next_url='http://test.localhost/profile/')

    def test_login_authn_bad_relay_state(self):
        with self.assertRaises(AssertionError):
            self.authn('/login', next_url='http://bad.localhost/evil/')

    def test_chpass_authn(self):
        self.authn('/chpass', force_authn=True)

    def test_terminate_authn(self):
        self.authn('/terminate', force_authn=True)

    def test_login_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            eppn = 'hubba-bubba'
            self.assertEquals(session['eduPersonPrincipalName'], eppn)

        self.acs('/login', eppn, _check)

    def test_login_assertion_consumer_service_good_relay_state(self):
        eppn = 'hubba-bubba'

        def _check():
            eppn = 'hubba-bubba'
            self.assertEquals(session['eduPersonPrincipalName'], eppn)

        self.acs('/login', eppn, _check, came_from='http://test.localhost/profile/')

    def test_login_assertion_consumer_service_bad_relay_state(self):
        eppn = 'hubba-bubba'

        def _check():
            eppn = 'hubba-bubba'
            self.assertEquals(session['eduPersonPrincipalName'], eppn)

        with self.assertRaises(AssertionError):
            self.acs('/login', eppn, _check, came_from='http://bad.localhost/evil/')

    def test_chpass_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            self.assertIn('reauthn-for-chpass', session)
            then = session['reauthn-for-chpass']
            now = int(time.time())
            self.assertTrue(now - then < 5)

        self.acs('/chpass', eppn, _check)

    def test_terminate_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            self.assertIn('reauthn-for-termination', session)
            then = session['reauthn-for-termination']
            now = int(time.time())
            self.assertTrue(now - then < 5)

        self.acs('/terminate', eppn, _check)

    def test_token_login_new_user(self):
        eppn = 'hubba-fooo'
        shared_key = self.app.config['TOKEN_LOGIN_SHARED_KEY']
        timestamp = '{:x}'.format(int(time.time()))
        nonce = os.urandom(16).encode('hex')
        token = sha256(""{0}|{1}|{2}|{3}"".format(shared_key, eppn, nonce, timestamp)).hexdigest()

        data = {
            'eppn': eppn,
            'token': token,
            'nonce': nonce,
            'ts': timestamp
        }

        with self.app.test_client() as c:
            resp = c.post('/token-login', data=data)
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_LOGIN_SUCCESS_REDIRECT_URL']))

    def test_token_login_old_user(self):
        eppn = 'hubba-bubba'
        shared_key = self.app.config['TOKEN_LOGIN_SHARED_KEY']
        timestamp = '{:x}'.format(int(time.time()))
        nonce = os.urandom(16).encode('hex')
        token = sha256(""{0}|{1}|{2}|{3}"".format(shared_key, eppn, nonce, timestamp)).hexdigest()

        data = {
            'eppn': eppn,
            'token': token,
            'nonce': nonce,
            'ts': timestamp
        }

        with self.app.test_client() as c:
            resp = c.post('/token-login', data=data)
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_LOGIN_FAILURE_REDIRECT_URL']))


class UnAuthnAPITestCase(EduidAPITestCase):

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'TOKEN_SERVICE_URL': 'http://login',
            'SAML2_SETTINGS_MODULE': saml_config,
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return eduid_init_app('testing', config)

    def test_no_cookie(self):
        with self.app.test_client() as c:
            resp = c.get('/')
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_SERVICE_URL']))

    def test_cookie(self):
        token = ('a7MPUEQQLAEEQEAQDGJOXKAMFM467EUW6HCETFI4VP5JCU3CDVJDQZSHMXAOSC'
                 'U25WPZA66NY5ZVAA4RPCVMHBQBJSVGYQPPLZNIBTP3Y')
        sessid = ('fb1f42420b0109020203325d750185673df252de388932a3957f522a6c43a'
                  'a47')
        self.redis_instance.conn.set(sessid, json.dumps({'v1': {'id': '0'}}))

        eppn = self.test_user_data['eduPersonPrincipalName']
        with self.session_cookie(self.browser, eppn) as c:
            self.assertRaises(NotFound, c.get, '/')


class NoAuthnAPITestCase(EduidAPITestCase):

    def setUp(self):
        super(NoAuthnAPITestCase, self).setUp()
        test_views = Blueprint('testing', __name__)

        @test_views.route('/test')
        def test():
            return 'OK'

        @test_views.route('/test3')
        def test3():
            return 'OK'

        self.app.register_blueprint(test_views)

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'TOKEN_SERVICE_URL': 'http://login',
            'SAML2_SETTINGS_MODULE': saml_config,
            'NO_AUTHN_URLS': ['^/test$'],
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return eduid_init_app('testing', config)

    def test_no_authn(self):
        with self.app.test_client() as c:
            resp = c.get('/test')
            self.assertEqual(resp.status_code, 200)

    def test_authn(self):
        with self.app.test_client() as c:
            resp = c.get('/test2')
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_SERVICE_URL']))

    def test_no_authn_util(self):
        no_authn_urls_before = [path for path in self.app.config['NO_AUTHN_URLS']]
        no_authn_path = '/test3'
        no_authn_views(self.app, [no_authn_path])
        self.assertEqual(no_authn_urls_before + ['^{!s}$'.format(no_authn_path)], self.app.config['NO_AUTHN_URLS'])

        with self.app.test_client() as c:
            resp = c.get('/test3')
            self.assertEqual(resp.status_code, 200)


class LogoutRequestTests(AuthnAPITestBase):

    def test_metadataview(self):
        with self.app.test_client() as c:
            response = c.get('/saml2-metadata')
            self.assertEqual(response.status, '200 OK')

    def test_logout_nologgedin(self):
        eppn = 'hubba-bubba'
        csrft = 'csrf token'
        with self.app.test_request_context('/logout', method='POST',
                                           data={'csrf': csrft}):
            session['_csrft_'] = csrft
            session['user_eppn'] = eppn
            session['eduPersonPrincipalName'] = eppn
            response = self.app.dispatch_request()
            self.assertEqual(response.status, '200 OK')
            self.assertIn(self.app.config['SAML2_LOGOUT_REDIRECT_URL'],
                          json.loads(response.data)['payload']['location'])

    def test_logout_loggedin(self):
        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        cookie = self.login(eppn, came_from)

        csrft = 'csrf token'
        with self.app.test_request_context('/logout', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'csrf': csrft}):
            session['_csrft_'] = csrft
            response2 = self.app.dispatch_request()
            self.assertEqual(response2.status, '200 OK')
            self.assertIn('https://idp.example.com/simplesaml/saml2/idp/'
                          'SingleLogoutService.php',
                          json.loads(response2.data)['payload']['location'])

    def test_logout_service_startingSP(self):

        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': deflate_and_base64_encode(
                                            logout_response(session_id)
                                           ),
                                               'RelayState': '/testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)

    def test_logout_service_startingSP_already_logout(self):

        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           data={'SAMLResponse': deflate_and_base64_encode(
                                               logout_response(session_id)
                                           ),
                                               'RelayState': '/testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)

    def test_logout_service_startingIDP(self):

        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        saml_response = auth_response(session_id, eppn)

        # Log in through IDP SAMLResponse
        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': '/testing-relay-state',
                                                 }):
            response = self.app.dispatch_request()

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLRequest': deflate_and_base64_encode(
                                               logout_request(session_id)
                                           ),
                                               'RelayState': '/testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('https://idp.example.com/simplesaml/saml2/idp/'
                          'SingleLogoutService.php?SAMLResponse=', response.location)

    def test_logout_service_startingIDP_no_subject_id(self):

        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        saml_response = auth_response(session_id, eppn)

        # Log in through IDP SAMLResponse
        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': '/testing-relay-state',
                                                 }):
            response = self.app.dispatch_request()

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLRequest': deflate_and_base64_encode(
                                               logout_request(session_id)
                                           ),
                                               'RelayState': '/testing-relay-state',
                                           }):
            del session['_saml2_session_name_id']
            session.persist()
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)
/n/n/nsrc/eduid_webapp/authn/views.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

from saml2 import BINDING_HTTP_REDIRECT
from saml2.ident import decode
from saml2.client import Saml2Client
from saml2.response import LogoutResponse
from saml2.metadata import entity_descriptor
from werkzeug.exceptions import Forbidden
from flask import request, session, redirect, abort, make_response
from flask import current_app, Blueprint

from eduid_common.api.decorators import MarshalWith
from eduid_common.authn.utils import get_location
from eduid_common.authn.loa import get_loa
from eduid_common.authn.eduid_saml2 import get_authn_request, get_authn_response
from eduid_common.authn.eduid_saml2 import authenticate
from eduid_common.authn.cache import IdentityCache, StateCache
from eduid_webapp.authn.acs_registry import get_action, schedule_action
from eduid_webapp.authn.helpers import verify_auth_token, verify_relay_state
from eduid_webapp.authn.schemas import LogoutPayload, LogoutResponseSchema


authn_views = Blueprint('authn', __name__)


@authn_views.route('/login')
def login():
    """"""
    login view, redirects to SAML2 IdP
    """"""
    return _authn('login-action')


@authn_views.route('/chpass')
def chpass():
    """"""
    Reauthn view, sends a SAML2 reauthn request to the IdP.
    """"""
    return _authn('change-password-action', force_authn=True)


@authn_views.route('/terminate')
def terminate():
    """"""
    Reauthn view, sends a SAML2 reauthn request to the IdP.
    """"""
    return _authn('terminate-account-action', force_authn=True)


def _authn(action, force_authn=False):
    redirect_url = current_app.config.get('SAML2_LOGIN_REDIRECT_URL', '/')
    relay_state = verify_relay_state(request.args.get('next', redirect_url), redirect_url)
    idps = current_app.saml2_config.getattr('idp')
    assert len(idps) == 1
    idp = idps.keys()[0]
    idp = request.args.get('idp', idp)
    loa = request.args.get('required_loa', None)
    authn_request = get_authn_request(current_app.config, session,
                                      relay_state, idp, required_loa=loa,
                                      force_authn=force_authn)
    schedule_action(action)
    current_app.logger.info('Redirecting the user to the IdP for ' + action)
    return redirect(get_location(authn_request))


@authn_views.route('/saml2-acs', methods=['POST'])
def assertion_consumer_service():
    """"""
    Assertion consumer service, receives POSTs from SAML2 IdP's
    """"""

    if 'SAMLResponse' not in request.form:
        abort(400)

    xmlstr = request.form['SAMLResponse']
    session_info = get_authn_response(current_app.config, session, xmlstr)
    current_app.logger.debug('Trying to locate the user authenticated by the IdP')
    user = authenticate(current_app, session_info)

    if user is None:
        current_app.logger.error('Could not find the user identified by the IdP')
        raise Forbidden(""Access not authorized"")

    action = get_action()
    return action(session_info, user)


def _get_name_id(session):
    """"""
    Get the SAML2 NameID of the currently logged in user.
    :param session: The current session object
    :return: NameID
    :rtype: saml2.saml.NameID | None
    """"""
    try:
        return decode(session['_saml2_session_name_id'])
    except KeyError:
        return None


@authn_views.route('/logout', methods=['POST'])
@MarshalWith(LogoutResponseSchema)
def logout():
    """"""
    SAML Logout Request initiator.
    This view initiates the SAML2 Logout request
    using the pysaml2 library to create the LogoutRequest.
    """"""
    eppn = session.get('user_eppn')

    if eppn is None:
        current_app.logger.info('Session cookie has expired, no logout action needed')
        location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
        return LogoutPayload().dump({'location': location}).data

    user = current_app.central_userdb.get_user_by_eppn(eppn)

    current_app.logger.debug('Logout process started for user {!r}'.format(user))
    state = StateCache(session)
    identity = IdentityCache(session)

    client = Saml2Client(current_app.saml2_config,
                         state_cache=state,
                         identity_cache=identity)

    subject_id = _get_name_id(session)
    if subject_id is None:
        current_app.logger.warning(
            'The session does not contain '
            'the subject id for user {!r}'.format(user))
        location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')

    else:
        logouts = client.global_logout(subject_id)
        loresponse = logouts.values()[0]
        # loresponse is a dict for REDIRECT binding, and LogoutResponse for SOAP binding
        if isinstance(loresponse, LogoutResponse):
            if loresponse.status_ok():
                current_app.logger.debug('Performing local logout for {!r}'.format(user))
                session.clear()
                location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
                location = verify_relay_state(request.form.get('RelayState', location), location)
                return LogoutPayload().dump({'location': location}).data
            else:
                abort(500)
        headers_tuple = loresponse[1]['headers']
        location = headers_tuple[0][1]
        current_app.logger.info('Redirecting to {!r} to continue the logout process '
                                'for user {!r}'.format(location, user))

    state.sync()
    return LogoutPayload().dump({'location': location}).data


@authn_views.route('/saml2-ls', methods=['POST'])
def logout_service():
    """"""SAML Logout Response endpoint
    The IdP will send the logout response to this view,
    which will process it with pysaml2 help and log the user
    out.
    Note that the IdP can request a logout even when
    we didn't initiate the process as a single logout
    request started by another SP.
    """"""
    current_app.logger.debug('Logout service started')

    state = StateCache(session)
    identity = IdentityCache(session)
    client = Saml2Client(current_app.saml2_config,
                         state_cache=state,
                         identity_cache=identity)

    logout_redirect_url = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
    next_page = session.get('next', logout_redirect_url)
    next_page = request.args.get('next', next_page)
    next_page = request.form.get('RelayState', next_page)
    next_page = verify_relay_state(next_page, logout_redirect_url)

    if 'SAMLResponse' in request.form: # we started the logout
        current_app.logger.debug('Receiving a logout response from the IdP')
        response = client.parse_logout_request_response(
            request.form['SAMLResponse'],
            BINDING_HTTP_REDIRECT
        )
        state.sync()
        if response and response.status_ok():
            session.clear()
            return redirect(next_page)
        else:
            current_app.logger.error('Unknown error during the logout')
            abort(400)

    # logout started by the IdP
    elif 'SAMLRequest' in request.form:
        current_app.logger.debug('Receiving a logout request from the IdP')
        subject_id = _get_name_id(session)
        if subject_id is None:
            current_app.logger.warning(
                'The session does not contain the subject id for user {0} '
                'Performing local logout'.format(
                    session['eduPersonPrincipalName']
                )
            )
            session.clear()
            return redirect(next_page)
        else:
            http_info = client.handle_logout_request(
                request.form['SAMLRequest'],
                subject_id,
                BINDING_HTTP_REDIRECT,
                relay_state=request.form['RelayState']
            )
            state.sync()
            location = get_location(http_info)
            session.clear()
            return redirect(location)
    current_app.logger.error('No SAMLResponse or SAMLRequest parameter found')
    abort(400)


@authn_views.route('/token-login', methods=['POST'])
def token_login():
    current_app.logger.debug('Starting token login')
    location_on_fail = current_app.config.get('TOKEN_LOGIN_FAILURE_REDIRECT_URL')
    location_on_success = current_app.config.get('TOKEN_LOGIN_SUCCESS_REDIRECT_URL')

    eppn = request.form.get('eppn')
    token = request.form.get('token')
    nonce = request.form.get('nonce')
    timestamp = request.form.get('ts')
    loa = get_loa(current_app.config.get('AVAILABLE_LOA'), None)  # With no session_info lowest loa will be returned

    if verify_auth_token(eppn=eppn, token=token, nonce=nonce, timestamp=timestamp):
        try:
            user = current_app.central_userdb.get_user_by_eppn(eppn)
            if user.locked_identity.count > 0:
                # This user has previously verified their account and is not new, this should not happen.
                current_app.logger.error('Not new user {} tried to log in using token login'.format(user))
                return redirect(location_on_fail)
            session['eduPersonPrincipalName'] = user.eppn
            session['user_eppn'] = user.eppn
            session['eduPersonAssurance'] = loa
            session.persist()

            response = redirect(location_on_success)
            session.set_cookie(response)
            current_app.logger.info('Successful token login, redirecting user {} to {}'.format(user,
                                                                                               location_on_success))
            return response
        except current_app.central_userdb.exceptions.UserDoesNotExist:
            current_app.logger.error('No user with eduPersonPrincipalName = {} found'.format(eppn))
        except current_app.central_userdb.exceptions.MultipleUsersReturned:
            current_app.logger.error(""There are more than one user with eduPersonPrincipalName = {}"".format(eppn))

    current_app.logger.info('Token login failed, redirecting user to {}'.format(location_on_fail))
    return redirect(location_on_fail)


@authn_views.route('/saml2-metadata')
def metadata():
    """"""
    Returns an XML with the SAML 2.0 metadata for this
    SP as configured in the saml2_settings.py file.
    """"""
    metadata = entity_descriptor(current_app.saml2_config)
    response = make_response(metadata.to_string(), 200)
    response.headers['Content-Type'] = ""text/xml; charset=utf8""
    return response
/n/n/nsrc/eduid_webapp/settings/common.py/n/n# -*- coding: utf-8 -*-

#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

# ==============#
# Flask config #
# ==============#

# enable/disable debug mode
DEBUG = False

# enable/disable testing mode
TESTING = False

# explicitly enable or disable the propagation of exceptions.
# If not set or explicitly set to None this is implicitly true if either
# TESTING or DEBUG is true.
PROPAGATE_EXCEPTIONS = None

# By default if the application is in debug mode the request context is not
# popped on exceptions to enable debuggers to introspect the data. This can be
# disabled by this key. You can also use this setting to force-enable it for non
# debug execution which might be useful to debug production applications (but
# also very risky).
PRESERVE_CONTEXT_ON_EXCEPTION = False

# the secret key
SECRET_KEY = ''

# the name of the session cookie
SESSION_COOKIE_NAME = 'sessid'

# the domain for the session cookie. If this is not set, the cookie will
# be valid for all subdomains of SERVER_NAME.
SESSION_COOKIE_DOMAIN = 'eduid.se'

# the path for the session cookie. If this is not set the cookie will be valid
# for all of APPLICATION_ROOT or if that is not set for '/'.
SESSION_COOKIE_PATH = '/'

# controls if the cookie should be set with the httponly flag. Defaults to True
SESSION_COOKIE_HTTPONLY = False

# controls if the cookie should be set with the secure flag. Defaults to False
SESSION_COOKIE_SECURE = False

# the lifetime of a permanent session as datetime.timedelta object.
# Starting with Flask 0.8 this can also be an integer representing seconds.
PERMANENT_SESSION_LIFETIME = 3600

# enable/disable x-sendfile
# USE_X_SENDFILE = False

# the name of the logger
LOGGER_NAME = 'eduid_webapp'

# the name and port number of the server. Required for subdomain support (e.g.: 'myapp.dev:5000') Note that localhost
# does not support subdomains so setting this to localhost does not help. Setting a SERVER_NAME also by default
# enables URL generation without a request context but with an application context.
SERVER_NAME = None

# If the application does not occupy a whole domain or subdomain this can be set to the path where the application is
# configured to live. This is for session cookie as path value. If domains are used, this should be None.
APPLICATION_ROOT = None

# If set to a value in bytes, Flask will reject incoming requests with a
# content length greater than this by returning a 413 status code.
# MAX_CONTENT_LENGTH

# Default cache control max age to use with send_static_file() (the default
# static file handler) and send_file(), in seconds. Override this value on a
# per-file basis using the get_send_file_max_age() hook on Flask or Blueprint,
# respectively. Defaults to 43200 (12 hours).
SEND_FILE_MAX_AGE_DEFAULT = 43200

# If this is set to True Flask will not execute the error handlers of HTTP
# exceptions but instead treat the exception like any other and bubble it through
# the exception stack. This is helpful for hairy debugging situations where you
# have to find out where an HTTP exception is coming from.
TRAP_HTTP_EXCEPTIONS = False

# Werkzeugs internal data structures that deal with request specific data
# will raise special key errors that are also bad request exceptions. Likewise
# many operations can implicitly fail with a BadRequest exception for
# consistency. Since its nice for debugging to know why exactly it failed this
# flag can be used to debug those situations. If this config is set to True you
# will get a regular traceback instead.
TRAP_BAD_REQUEST_ERRORS = False

# The URL scheme that should be used for URL generation if no URL scheme is
# available. This defaults to http.
PREFERRED_URL_SCHEME = 'https'

# By default Flask serialize object to ascii-encoded JSON. If this is set to
# False Flask will not encode to ASCII and output strings as-is and return
# unicode strings. jsonfiy will automatically encode it in utf-8 then for
# transport for instance.
JSON_AS_ASCII = False

# By default Flask will serialize JSON objects in a way that the keys are
# ordered. This is done in order to ensure that independent of the hash seed of
# the dictionary the return value will be consistent to not trash external HTTP
# caches. You can override the default behavior by changing this variable. This
# is not recommended but might give you a performance improvement on the cost of
# cachability.
# JSON_SORT_KEYS = True

# If this is set to True (the default) jsonify responses will be pretty printed
# if they are not requested by an XMLHttpRequest object (controlled by the
# X-Requested-With header)
# JSONIFY_PRETTYPRINT_REGULAR

# Whitelist of URLs that do not need authentication. Unauthenticated requests
# for these URLs will be served, rather than redirected to the authn service.
# The list is a list of regex that are matched against the path of the
# requested URL ex. ^/test$.
NO_AUTHN_URLS = []


# ================#
#  mongodb config #
# ================#

MONGO_URI = 'mongodb://'


# ==============#
#  redis config #
# ==============#

REDIS_HOST = ''
REDIS_PORT = 6379
REDIS_DB = 0
REDIS_SENTINEL_HOSTS = ''
REDIS_SENTINEL_SERVICE_NAME = ''


# =======#
#  SAML2 #
# =======#

SAML2_LOGIN_REDIRECT_URL = '/'
SAML2_SETTINGS_MODULE = ''
SAML2_LOGOUT_REDIRECT_URL = 'https://www.eduid.se/'
SAML2_USER_MAIN_ATTRIBUTE = 'eduPersonPrincipalName'
SAML2_STRIP_SAML_USER_SUFFIX = '@eduid.se'

# ===============#
#  AUTHN SERVICE #
# ===============#

TOKEN_SERVICE_URL = 'https://'
SAFE_RELAY_DOMAIN = 'eduid.se'

# ===============#
#  TEMPLATE DATA #
# ===============#
EDUID_SITE_NAME = 'eduID'
EDUID_SITE_URL = 'https://www.eduid.se'
EDUID_STATIC_URL = 'https://www.eduid.se/static/'

# ===============#
#  BABEL         #
# ===============#

# Try to guess the language from the user accept header the browser transmits. The best match wins.
SUPPORTED_LANGUAGES = ['sv', 'en']
/n/n/n",0
123,123,5b27ed28614c11178a2c7d8120dc5db99abee175,"/src/eduid_webapp/authn/acs_actions.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

from time import time
from saml2.ident import code
from flask import session, request, redirect, current_app
from eduid_common.authn.loa import get_loa
from eduid_webapp.authn.acs_registry import acs_action


@acs_action('login-action')
def login_action(session_info, user):
    """"""
    Upon successful login in the IdP, store login info in the session
    and redirect back to the app that asked for authn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    current_app.logger.info(""User {!r} logging in."".format(user))
    session['_saml2_session_name_id'] = code(session_info['name_id'])
    session['eduPersonPrincipalName'] = user.eppn
    session['user_eppn'] = user.eppn
    loa = get_loa(current_app.config.get('AVAILABLE_LOA'), session_info)
    session['eduPersonAssurance'] = loa
    session.persist()

    # redirect the user to the view where he came from
    relay_state = request.form.get('RelayState', '/')
    current_app.logger.debug('Redirecting to the RelayState: ' + relay_state)
    response = redirect(location=relay_state)
    session.set_cookie(response)
    current_app.logger.info('Redirecting user {!r} to {!r}'.format(user, relay_state))
    return response


@acs_action('change-password-action')
def chpass_action(session_info, user):
    """"""
    Upon successful reauthn in the IdP,
    set a timestamp in the session (key reauthn-for-chpass)
    and redirect back to the app that asked for reauthn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    return _reauthn('reauthn-for-chpass', session_info, user)


@acs_action('terminate-account-action')
def term_account_action(session_info, user):
    """"""
    Upon successful reauthn in the IdP,
    set a timestamp in the session (key reauthn-for-termination)
    and redirect back to the app that asked for reauthn.

    :param session_info: the SAML session info
    :type session_info: dict

    :param user: the authenticated user
    :type user: eduid_userdb.User
    """"""
    return _reauthn('reauthn-for-termination', session_info, user)


def _reauthn(reason, session_info, user):

    current_app.logger.info(""Reauthenticating user {!r} for {!r}."".format(user, reason))
    session['_saml2_session_name_id'] = code(session_info['name_id'])
    session[reason] = int(time())
    session.persist()

    # redirect the user to the view where he came from
    relay_state = request.form.get('RelayState', '/')
    current_app.logger.debug('Redirecting to the RelayState: ' + relay_state)
    return redirect(location=relay_state)
/n/n/n/src/eduid_webapp/authn/tests/test_authn.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

import os
import time
import json
import base64
from hashlib import sha256

from werkzeug.exceptions import NotFound
from werkzeug.http import dump_cookie
from flask import session
from flask import Blueprint
from saml2.s_utils import deflate_and_base64_encode

from eduid_userdb.user import User
from eduid_userdb.data_samples import NEW_COMPLETED_SIGNUP_USER_EXAMPLE
from eduid_common.api.testing import EduidAPITestCase
from eduid_common.authn.cache import OutstandingQueriesCache
from eduid_common.authn.utils import get_location, no_authn_views
from eduid_common.authn.eduid_saml2 import get_authn_request
from eduid_common.authn.tests.responses import (auth_response,
                                                logout_response,
                                                logout_request)
from eduid_webapp.authn.app import authn_init_app
from eduid_common.api.app import eduid_init_app


import logging
logger = logging.getLogger(__name__)

HERE = os.path.abspath(os.path.dirname(__file__))


class AuthnAPITestBase(EduidAPITestCase):

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'SAML2_LOGIN_REDIRECT_URL': '/',
            'SAML2_LOGOUT_REDIRECT_URL': '/logged-out',
            'SAML2_SETTINGS_MODULE': saml_config,
            'TOKEN_LOGIN_SHARED_KEY': 'shared_secret',
            'TOKEN_LOGIN_SUCCESS_REDIRECT_URL': 'http://test.localhost/success',
            'TOKEN_LOGIN_FAILURE_REDIRECT_URL': 'http://test.localhost/failure'
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return authn_init_app('test.localhost', config)

    def add_outstanding_query(self, came_from):
        """"""
        Add a SAML2 authentication query to the queries cache.
        To be used before accessing the assertion consumer service.

        :param came_from: url to redirect back the client
                          after finishing with the authn service.
        :type came_from: str

        :return: the session token corresponding to the query
        :rtype: str
        """"""
        with self.app.test_request_context('/login'):
            self.app.dispatch_request()
            oq_cache = OutstandingQueriesCache(session)
            oq_cache.set(session.token, came_from)
            session.persist()
            return session.token

    def login(self, eppn, came_from):
        """"""
        Add a SAML2 authentication query to the queries cache,
        build a cookie with a session id corresponding to the added query,
        build a SAML2 authn response for the added query,
        and send both to the assertion consumer service,
        so that the user is logged in (the session corresponding to the cookie
        has her eppn).
        This method returns the cookie that has to be sent with any
        subsequent request that needs to be athenticated.

        :param eppn: the eppn of the user to be logged in
        :type eppn: str
        :param came_from: url to redirect back the client
                          after finishing with the authn service.
        :type came_from: str

        :return: the cookie corresponding to the authn session
        :rtype: str
        """"""
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)
        saml_response = auth_response(session_id, eppn)

        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': came_from}):

            response1 = self.app.dispatch_request()
            cookie = response1.headers['Set-Cookie']
            return cookie

    def authn(self, url, force_authn=False):
        """"""
        Common code for the tests that need to send an authentication request.
        This checks that the client is redirected to the idp.

        :param url: the url of the desired authentication mode.
        :type url: str
        :param force_authn: whether to force reauthentication for an already
                            authenticated client
        :type force_authn: bool
        """"""
        with self.app.test_client() as c:
            resp = c.get(url)
            authn_req = get_location(get_authn_request(self.app.config,
                                                       session, '/', None,
                                                       force_authn=force_authn))
            idp_url = authn_req.split('?')[0]
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(idp_url))

    def acs(self, url, eppn, check_fn):
        """"""
        common code for the tests that need to access the assertion consumer service
        and then check the side effects of this access.

        :param url: the url of the desired authentication mode.
        :type url: str
        :param eppn: the eppn of the user to access the service
        :type eppn: str
        :param check_fn: the function that checks the side effects after accessing the acs
        :type check_fn: callable
        """"""
        came_from = '/camefrom/'
        with self.app.test_client() as c:
            resp = c.get(url)
            cookie = resp.headers['Set-Cookie']
            token = session._session.token
            authr = auth_response(token, eppn)

        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(authr),
                                                 'RelayState': came_from}):

            oq_cache = OutstandingQueriesCache(session)
            oq_cache.set(token, came_from)

            resp = self.app.dispatch_request()

            self.assertEquals(resp.status_code, 302)
            self.assertEquals(resp.location, came_from)
            check_fn()

    def dump_session_cookie(self, session_id):
        """"""
        Get a cookie corresponding to an authenticated session.

        :param session_id: the token for the session
        :type session_id: str

        :return: the cookie
        """"""
        return dump_cookie(self.app.config.get('SESSION_COOKIE_NAME'), session_id,
                           max_age=float(self.app.config.get('PERMANENT_SESSION_LIFETIME')),
                           path=self.app.config.get('SESSION_COOKIE_PATH'),
                           domain=self.app.config.get('SESSION_COOKIE_DOMAIN'))


class AuthnAPITestCase(AuthnAPITestBase):
    """"""
    Tests to check the different modes of authentication.
    """"""

    def init_data(self):
        """"""
        Called from the parent class, so we can extend data initialized.
        """"""
        test_user = User(data=NEW_COMPLETED_SIGNUP_USER_EXAMPLE)  # eppn hubba-fooo
        self.app.central_userdb.save(test_user, check_sync=False)

    def test_login_authn(self):
        self.authn('/login')

    def test_chpass_authn(self):
        self.authn('/chpass', force_authn=True)

    def test_terminate_authn(self):
        self.authn('/terminate', force_authn=True)

    def test_login_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            eppn = 'hubba-bubba'
            self.assertEquals(session['eduPersonPrincipalName'], eppn)

        self.acs('/login', eppn, _check)

    def test_chpass_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            self.assertIn('reauthn-for-chpass', session)
            then = session['reauthn-for-chpass']
            now = int(time.time())
            self.assertTrue(now - then < 5)

        self.acs('/chpass', eppn, _check)

    def test_terminate_assertion_consumer_service(self):
        eppn = 'hubba-bubba'

        def _check():
            self.assertIn('reauthn-for-termination', session)
            then = session['reauthn-for-termination']
            now = int(time.time())
            self.assertTrue(now - then < 5)

        self.acs('/terminate', eppn, _check)

    def test_token_login_new_user(self):
        eppn = 'hubba-fooo'
        shared_key = self.app.config['TOKEN_LOGIN_SHARED_KEY']
        timestamp = '{:x}'.format(int(time.time()))
        nonce = os.urandom(16).encode('hex')
        token = sha256(""{0}|{1}|{2}|{3}"".format(shared_key, eppn, nonce, timestamp)).hexdigest()

        data = {
            'eppn': eppn,
            'token': token,
            'nonce': nonce,
            'ts': timestamp
        }

        with self.app.test_client() as c:
            resp = c.post('/token-login', data=data)
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_LOGIN_SUCCESS_REDIRECT_URL']))

    def test_token_login_old_user(self):
        eppn = 'hubba-bubba'
        shared_key = self.app.config['TOKEN_LOGIN_SHARED_KEY']
        timestamp = '{:x}'.format(int(time.time()))
        nonce = os.urandom(16).encode('hex')
        token = sha256(""{0}|{1}|{2}|{3}"".format(shared_key, eppn, nonce, timestamp)).hexdigest()

        data = {
            'eppn': eppn,
            'token': token,
            'nonce': nonce,
            'ts': timestamp
        }

        with self.app.test_client() as c:
            resp = c.post('/token-login', data=data)
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_LOGIN_FAILURE_REDIRECT_URL']))


class UnAuthnAPITestCase(EduidAPITestCase):

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'TOKEN_SERVICE_URL': 'http://login',
            'SAML2_SETTINGS_MODULE': saml_config,
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return eduid_init_app('testing', config)

    def test_no_cookie(self):
        with self.app.test_client() as c:
            resp = c.get('/')
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_SERVICE_URL']))

    def test_cookie(self):
        token = ('a7MPUEQQLAEEQEAQDGJOXKAMFM467EUW6HCETFI4VP5JCU3CDVJDQZSHMXAOSC'
                 'U25WPZA66NY5ZVAA4RPCVMHBQBJSVGYQPPLZNIBTP3Y')
        sessid = ('fb1f42420b0109020203325d750185673df252de388932a3957f522a6c43a'
                  'a47')
        self.redis_instance.conn.set(sessid, json.dumps({'v1': {'id': '0'}}))

        eppn = self.test_user_data['eduPersonPrincipalName']
        with self.session_cookie(self.browser, eppn) as c:
            self.assertRaises(NotFound, c.get, '/')


class NoAuthnAPITestCase(EduidAPITestCase):

    def setUp(self):
        super(NoAuthnAPITestCase, self).setUp()
        test_views = Blueprint('testing', __name__)

        @test_views.route('/test')
        def test():
            return 'OK'

        @test_views.route('/test3')
        def test3():
            return 'OK'

        self.app.register_blueprint(test_views)

    def update_config(self, config):
        """"""
        Called from the parent class, so that we can update the configuration
        according to the needs of this test case.
        """"""
        saml_config = os.path.join(HERE, 'saml2_settings.py')
        config.update({
            'TOKEN_SERVICE_URL': 'http://login',
            'SAML2_SETTINGS_MODULE': saml_config,
            'NO_AUTHN_URLS': ['^/test$'],
            })
        return config

    def load_app(self, config):
        """"""
        Called from the parent class, so we can provide the appropriate flask
        app for this test case.
        """"""
        return eduid_init_app('testing', config)

    def test_no_authn(self):
        with self.app.test_client() as c:
            resp = c.get('/test')
            self.assertEqual(resp.status_code, 200)

    def test_authn(self):
        with self.app.test_client() as c:
            resp = c.get('/test2')
            self.assertEqual(resp.status_code, 302)
            self.assertTrue(resp.location.startswith(self.app.config['TOKEN_SERVICE_URL']))

    def test_no_authn_util(self):
        no_authn_urls_before = [path for path in self.app.config['NO_AUTHN_URLS']]
        no_authn_path = '/test3'
        no_authn_views(self.app, [no_authn_path])
        self.assertEqual(no_authn_urls_before + ['^{!s}$'.format(no_authn_path)], self.app.config['NO_AUTHN_URLS'])

        with self.app.test_client() as c:
            resp = c.get('/test3')
            self.assertEqual(resp.status_code, 200)


class LogoutRequestTests(AuthnAPITestBase):

    def test_metadataview(self):
        with self.app.test_client() as c:
            response = c.get('/saml2-metadata')
            self.assertEqual(response.status, '200 OK')

    def test_logout_nologgedin(self):
        eppn = 'hubba-bubba'
        csrft = 'csrf token'
        with self.app.test_request_context('/logout', method='POST',
                                           data={'csrf': csrft}):
            session['_csrft_'] = csrft
            session['user_eppn'] = eppn
            session['eduPersonPrincipalName'] = eppn
            response = self.app.dispatch_request()
            self.assertEqual(response.status, '200 OK')
            self.assertIn(self.app.config['SAML2_LOGOUT_REDIRECT_URL'],
                          json.loads(response.data)['payload']['location'])

    def test_logout_loggedin(self):
        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        cookie = self.login(eppn, came_from)

        csrft = 'csrf token'
        with self.app.test_request_context('/logout', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'csrf': csrft}):
            session['_csrft_'] = csrft
            response2 = self.app.dispatch_request()
            self.assertEqual(response2.status, '200 OK')
            self.assertIn('https://idp.example.com/simplesaml/saml2/idp/'
                          'SingleLogoutService.php',
                          json.loads(response2.data)['payload']['location'])

    def test_logout_service_startingSP(self):

        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': deflate_and_base64_encode(
                                            logout_response(session_id)
                                           ),
                                               'RelayState': 'testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)

    def test_logout_service_startingSP_already_logout(self):

        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           data={'SAMLResponse': deflate_and_base64_encode(
                                               logout_response(session_id)
                                           ),
                                               'RelayState': 'testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)

    def test_logout_service_startingIDP(self):

        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        saml_response = auth_response(session_id, eppn)

        # Log in through IDP SAMLResponse
        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': 'testing-relay-state',
                                                 }):
            response = self.app.dispatch_request()

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLRequest': deflate_and_base64_encode(
                                               logout_request(session_id)
                                           ),
                                               'RelayState': 'testing-relay-state',
                                           }):
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('https://idp.example.com/simplesaml/saml2/idp/'
                          'SingleLogoutService.php?SAMLResponse=', response.location)

    def test_logout_service_startingIDP_no_subject_id(self):

        eppn = 'hubba-bubba'
        came_from = '/afterlogin/'
        session_id = self.add_outstanding_query(came_from)
        cookie = self.dump_session_cookie(session_id)

        saml_response = auth_response(session_id, eppn)

        # Log in through IDP SAMLResponse
        with self.app.test_request_context('/saml2-acs', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLResponse': base64.b64encode(saml_response),
                                                 'RelayState': 'testing-relay-state',
                                                 }):
            response = self.app.dispatch_request()

        with self.app.test_request_context('/saml2-ls', method='POST',
                                           headers={'Cookie': cookie},
                                           data={'SAMLRequest': deflate_and_base64_encode(
                                               logout_request(session_id)
                                           ),
                                               'RelayState': 'testing-relay-state',
                                           }):
            del session['_saml2_session_name_id']
            session.persist()
            response = self.app.dispatch_request()

            self.assertEqual(response.status, '302 FOUND')
            self.assertIn('testing-relay-state', response.location)
/n/n/n/src/eduid_webapp/authn/views.py/n/n#
# Copyright (c) 2016 NORDUnet A/S
# All rights reserved.
#
#   Redistribution and use in source and binary forms, with or
#   without modification, are permitted provided that the following
#   conditions are met:
#
#     1. Redistributions of source code must retain the above copyright
#        notice, this list of conditions and the following disclaimer.
#     2. Redistributions in binary form must reproduce the above
#        copyright notice, this list of conditions and the following
#        disclaimer in the documentation and/or other materials provided
#        with the distribution.
#     3. Neither the name of the NORDUnet nor the names of its
#        contributors may be used to endorse or promote products derived
#        from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
# COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#

from saml2 import BINDING_HTTP_REDIRECT
from saml2.ident import decode
from saml2.client import Saml2Client
from saml2.response import LogoutResponse
from saml2.metadata import entity_descriptor
from werkzeug.exceptions import Forbidden
from flask import request, session, redirect, abort, make_response
from flask import current_app, Blueprint

from eduid_common.api.decorators import MarshalWith
from eduid_common.authn.utils import get_location
from eduid_common.authn.loa import get_loa
from eduid_common.authn.eduid_saml2 import get_authn_request, get_authn_response
from eduid_common.authn.eduid_saml2 import authenticate
from eduid_common.authn.cache import IdentityCache, StateCache
from eduid_webapp.authn.acs_registry import get_action, schedule_action
from eduid_webapp.authn.helpers import verify_auth_token
from eduid_webapp.authn.schemas import LogoutPayload, LogoutResponseSchema



authn_views = Blueprint('authn', __name__)


@authn_views.route('/login')
def login():
    """"""
    login view, redirects to SAML2 IdP
    """"""
    return _authn('login-action')


@authn_views.route('/chpass')
def chpass():
    """"""
    Reauthn view, sends a SAML2 reauthn request to the IdP.
    """"""
    return _authn('change-password-action', force_authn=True)


@authn_views.route('/terminate')
def terminate():
    """"""
    Reauthn view, sends a SAML2 reauthn request to the IdP.
    """"""
    return _authn('terminate-account-action', force_authn=True)


def _authn(action, force_authn=False):
    redirect_url = current_app.config.get('SAML2_LOGIN_REDIRECT_URL', '/')
    relay_state = request.args.get('next', redirect_url)
    idps = current_app.saml2_config.getattr('idp')
    assert len(idps) == 1
    idp = idps.keys()[0]
    idp = request.args.get('idp', idp)
    loa = request.args.get('required_loa', None)
    authn_request = get_authn_request(current_app.config, session,
                                      relay_state, idp, required_loa=loa,
                                      force_authn=force_authn)
    schedule_action(action)
    current_app.logger.info('Redirecting the user to the IdP for ' + action)
    return redirect(get_location(authn_request))


@authn_views.route('/saml2-acs', methods=['POST'])
def assertion_consumer_service():
    """"""
    Assertion consumer service, receives POSTs from SAML2 IdP's
    """"""

    if 'SAMLResponse' not in request.form:
        abort(400)

    xmlstr = request.form['SAMLResponse']
    session_info = get_authn_response(current_app.config, session, xmlstr)
    current_app.logger.debug('Trying to locate the user authenticated by the IdP')
    user = authenticate(current_app, session_info)

    if user is None:
        current_app.logger.error('Could not find the user identified by the IdP')
        raise Forbidden(""Access not authorized"")

    action = get_action()
    return action(session_info, user)


def _get_name_id(session):
    """"""
    Get the SAML2 NameID of the currently logged in user.
    :param session: The current session object
    :return: NameID
    :rtype: saml2.saml.NameID | None
    """"""
    try:
        return decode(session['_saml2_session_name_id'])
    except KeyError:
        return None


@authn_views.route('/logout', methods=['POST'])
@MarshalWith(LogoutResponseSchema)
def logout():
    """"""
    SAML Logout Request initiator.
    This view initiates the SAML2 Logout request
    using the pysaml2 library to create the LogoutRequest.
    """"""
    eppn = session.get('user_eppn')

    if eppn is None:
        current_app.logger.info('Session cookie has expired, no logout action needed')
        location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
        return LogoutPayload().dump({'location': location}).data

    user = current_app.central_userdb.get_user_by_eppn(eppn)

    current_app.logger.debug('Logout process started for user {!r}'.format(user))
    state = StateCache(session)
    identity = IdentityCache(session)

    client = Saml2Client(current_app.saml2_config,
                         state_cache=state,
                         identity_cache=identity)

    subject_id = _get_name_id(session)
    if subject_id is None:
        current_app.logger.warning(
            'The session does not contain '
            'the subject id for user {!r}'.format(user))
        location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')

    else:
        logouts = client.global_logout(subject_id)
        loresponse = logouts.values()[0]
        # loresponse is a dict for REDIRECT binding, and LogoutResponse for SOAP binding
        if isinstance(loresponse, LogoutResponse):
            if loresponse.status_ok():
                current_app.logger.debug('Performing local logout for {!r}'.format(user))
                session.clear()
                location = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
                location = request.form.get('RelayState', location)
                return LogoutPayload().dump({'location': location}).data
            else:
                abort(500)
        headers_tuple = loresponse[1]['headers']
        location = headers_tuple[0][1]
        current_app.logger.info('Redirecting to {!r} to continue the logout process '
                                'for user {!r}'.format(location, user))

    state.sync()
    return LogoutPayload().dump({'location': location}).data


@authn_views.route('/saml2-ls', methods=['POST'])
def logout_service():
    """"""SAML Logout Response endpoint
    The IdP will send the logout response to this view,
    which will process it with pysaml2 help and log the user
    out.
    Note that the IdP can request a logout even when
    we didn't initiate the process as a single logout
    request started by another SP.
    """"""
    current_app.logger.debug('Logout service started')

    state = StateCache(session)
    identity = IdentityCache(session)
    client = Saml2Client(current_app.saml2_config,
                         state_cache=state,
                         identity_cache=identity)

    logout_redirect_url = current_app.config.get('SAML2_LOGOUT_REDIRECT_URL')
    next_page = session.get('next', logout_redirect_url)
    next_page = request.args.get('next', next_page)
    next_page = request.form.get('RelayState', next_page)

    if 'SAMLResponse' in request.form: # we started the logout
        current_app.logger.debug('Receiving a logout response from the IdP')
        response = client.parse_logout_request_response(
            request.form['SAMLResponse'],
            BINDING_HTTP_REDIRECT
        )
        state.sync()
        if response and response.status_ok():
            session.clear()
            return redirect(next_page)
        else:
            current_app.logger.error('Unknown error during the logout')
            abort(400)

    # logout started by the IdP
    elif 'SAMLRequest' in request.form:
        current_app.logger.debug('Receiving a logout request from the IdP')
        subject_id = _get_name_id(session)
        if subject_id is None:
            current_app.logger.warning(
                'The session does not contain the subject id for user {0} '
                'Performing local logout'.format(
                    session['eduPersonPrincipalName']
                )
            )
            session.clear()
            return redirect(next_page)
        else:
            http_info = client.handle_logout_request(
                request.form['SAMLRequest'],
                subject_id,
                BINDING_HTTP_REDIRECT,
                relay_state=request.form['RelayState']
            )
            state.sync()
            location = get_location(http_info)
            session.clear()
            return redirect(location)
    current_app.logger.error('No SAMLResponse or SAMLRequest parameter found')
    abort(400)


@authn_views.route('/token-login', methods=['POST'])
def token_login():
    current_app.logger.debug('Starting token login')
    location_on_fail = current_app.config.get('TOKEN_LOGIN_FAILURE_REDIRECT_URL')
    location_on_success = current_app.config.get('TOKEN_LOGIN_SUCCESS_REDIRECT_URL')

    eppn = request.form.get('eppn')
    token = request.form.get('token')
    nonce = request.form.get('nonce')
    timestamp = request.form.get('ts')
    loa = get_loa(current_app.config.get('AVAILABLE_LOA'), None)  # With no session_info lowest loa will be returned

    if verify_auth_token(eppn=eppn, token=token, nonce=nonce, timestamp=timestamp):
        try:
            user = current_app.central_userdb.get_user_by_eppn(eppn)
            if user.locked_identity.count > 0:
                # This user has previously verified their account and is not new, this should not happen.
                current_app.logger.error('Not new user {} tried to log in using token login'.format(user))
                return redirect(location_on_fail)
            session['eduPersonPrincipalName'] = user.eppn
            session['user_eppn'] = user.eppn
            session['eduPersonAssurance'] = loa
            session.persist()

            response = redirect(location_on_success)
            session.set_cookie(response)
            current_app.logger.info('Successful token login, redirecting user {} to {}'.format(user,
                                                                                               location_on_success))
            return response
        except current_app.central_userdb.exceptions.UserDoesNotExist:
            current_app.logger.error('No user with eduPersonPrincipalName = {} found'.format(eppn))
        except current_app.central_userdb.exceptions.MultipleUsersReturned:
            current_app.logger.error(""There are more than one user with eduPersonPrincipalName = {}"".format(eppn))

    current_app.logger.info('Token login failed, redirecting user to {}'.format(location_on_fail))
    return redirect(location_on_fail)


@authn_views.route('/saml2-metadata')
def metadata():
    """"""
    Returns an XML with the SAML 2.0 metadata for this
    SP as configured in the saml2_settings.py file.
    """"""
    metadata = entity_descriptor(current_app.saml2_config)
    response = make_response(metadata.to_string(), 200)
    response.headers['Content-Type'] = ""text/xml; charset=utf8""
    return response
/n/n/n",1
100,100,050077c00b7be9148370ab12ba022fe3479dd3ad,"realpal/apps/chat/api.py/n/nimport logging
import json
import os
from urllib.parse import urlparse

from channels import Group
from rest_framework.generics import CreateAPIView, UpdateAPIView
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework import status


from django.conf import settings
from django.core.urlresolvers import reverse

from realpal.apps.chat.serializers import MessageSerializer, RoomSerializer
from realpal.apps.chat.consumers import get_room_group_channel
from realpal.apps.chat.models import Message, Room

logger = logging.getLogger(__name__)


class MessageCreateAPIView(CreateAPIView):
    """"""
    Creates a new message object with a file attachment

    Returns on the socket

        {
            'id': ""id"",
            'sent_by':'user_id',
            'room':""room_id"",
            'text':message.txt,
            'file_name': message.attachment,
            'file_link': message.attachment.path
        }
    """"""
    model = Message
    serializer_class = MessageSerializer
    permission_classes = [IsAuthenticated, ]

    def create(self, request, *args, **kwargs):
        room_id = self.request.data.get('room')
        try:
            self.room = Room.objects.get(pk=room_id)
            self.request.data['sent_by'] = self.request.user.id
            self.request.data['room'] = self.room.id
            self.request.data['text'] = self.request.data.get('message')
            serializer = self.get_serializer(data=request.data)
            self.perform_create(serializer)
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        except Room.DoesNotExist:
            return Response(status=status.HTTP_400_BAD_REQUEST)

    def perform_create(self, serializer):
        serializer.is_valid(self)
        instance = serializer.save(sent_by=self.request.user, room=self.room)
        if not settings.IS_TESTING:
            data = {
                'id': instance.id.__str__(),
                'timestamp': instance.time_ago,
                'timestamp_string': instance.timestamp_string,
                'user_handle': self.request.user.full_name,
                'user_type': self.request.user.user_type,
                'message': instance.text,
                'file_name': os.path.basename(urlparse(instance.attachment.path).path) if instance.attachment else None,
                'file_link': instance.file_download_link if instance.attachment else None,
            }
            group_channel = get_room_group_channel(instance.room.id)
            self.push_socket_update(group_channel, data)

    @staticmethod
    def push_socket_update(group_channel, data):
        Group(group_channel).send({""text"": json.dumps(data)})


class RoomUpdateAPIView(UpdateAPIView):
    model = Room
    serializer_class = RoomSerializer
    permission_classes = [IsAuthenticated, ]

    def patch(self, request, *args, **kwargs):
        room_id = self.request.data.get('id', 0)
        try:
            room = Room.objects.get(pk=room_id)
            if room.agent == self.request.user:
                room.agent = None
                room.save()
                data = {
                    'client_room_id': room.id,
                    'client_room_link': reverse('chat:chat-room', args=(room.id,)),
                    'client_details': room.client.full_name,
                }
                return Response(data=data, status=status.HTTP_200_OK)
            else:
                return Response(status=status.HTTP_403_FORBIDDEN)
        except Room.DoesNotExist:
            return Response(status=status.HTTP_400_BAD_REQUEST)


/n/n/nrealpal/apps/chat/serializers.py/n/nfrom rest_framework import serializers

from .models import Message, Room


class MessageSerializer(serializers.ModelSerializer):
    """"""
    Message Serializer class
    """"""

    class Meta:
        model = Message
        fields = ('sent_by', 'room', 'text', 'attachment')


class RoomSerializer(serializers.ModelSerializer):
    """"""
    Room Serializer class
    """"""

    class Meta:
        model = Room
        fields = ('id', 'client', 'agent')
        read_only_fields = ('client',)
/n/n/nrealpal/apps/chat/tests/test_api.py/n/nimport json
import tempfile

from PIL import Image
from rest_framework import status
from django.urls import reverse
from django.conf import settings
from django.core.files.storage import default_storage
from rest_framework.test import APITestCase

from realpal.apps.chat.models import Room, Message
from realpal.apps.users.models import User
from realpal.apps.users.constants import AGENT_USER, CLIENT_USER


class MessageFileUploadTest(APITestCase):
    api_url_name = 'chat:chat-file'  # url with namespace

    # Generated files path for later destruction in tear down
    saved_attachment_path = ''
    temp_file_path = ''

    def setUp(self):
        self.user = User.objects.create(
            email='test@test.com'
        )
        self.room = Room.objects.get(client=self.user).id

    def test_post(self):
        """"""
        saves a message with a file attachment
        """"""
        self.url = reverse(self.api_url_name)

        # test unauthenticated
        response = self.client.post(self.url, json.dumps({}), content_type='application/json')
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        self.client.force_login(user=self.user)

        # Set the mode to binary and read so it can be decoded as binary

        image = Image.new('RGB', (100, 100))

        tmp_file = tempfile.NamedTemporaryFile(dir=settings.MEDIA_ROOT, prefix='testy', suffix='.jpeg', delete=False)
        image.save(tmp_file, format='jpeg')

        # Store generated image path for later destruction in tear down
        self.temp_file_path = tmp_file.name

        with open(tmp_file.name, 'rb') as f:
            data = {
                'attachment': f,
                'room': self.room,
                'text': 'Group Rules'
            }
            response = self.client.post(self.url, data, format='multipart')

        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

        # test if message attachment was generated in the media folder
        saved_message_attachment = Message.objects.get(room=self.room)

        self.saved_attachment_path = saved_message_attachment.attachment.path

        self.assertTrue(default_storage.exists(self.saved_attachment_path))

    def tearDown(self):
        default_storage.delete(self.saved_attachment_path)
        default_storage.delete(self.temp_file_path)
        super().tearDown()


class RoomTests(APITestCase):
    api_url_name = 'chat:update-room'

    def setUp(self):
        self.client_user = User.objects.create(
            email='client@test.com',
            username='client',
            user_type=CLIENT_USER
        )
        self.agent = User.objects.create(
            email='agent@test.com',
            username='agent',
            user_type=AGENT_USER
        )
        self.room = Room.objects.get(client=self.client_user)
        self.room.agent = self.agent
        self.room.save()

    def test_unassign_client(self):
        self.url = reverse(self.api_url_name)

        # test unauthenticated
        response = self.client.patch(self.url, data=json.dumps({}), content_type='application/json')
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        self.client.force_login(user=self.agent)
        data = {
            'id': self.room.id,
            'client': self.client_user.id,
            'agent': self.agent.id
        }
        response = self.client.patch(self.url, data=json.dumps(data), content_type='application/json')
        self.assertEqual(response.status_code, status.HTTP_200_OK)
/n/n/nrealpal/apps/chat/urls.py/n/nfrom django.conf.urls import url

from realpal.apps.chat.views import ChatRoomView
from realpal.apps.chat.api import MessageCreateAPIView, RoomUpdateAPIView

urlpatterns = [
    url(r'^$', ChatRoomView.as_view(), name='chat-room'),
    url(r'^(?P<room_id>[0-9]+)/', ChatRoomView.as_view(), name='chat-room'),
    url(r'^room/update/$', RoomUpdateAPIView.as_view(), name='update-room'),
    url(r'^file/$', MessageCreateAPIView.as_view(), name='chat-file'),

]
/n/n/nrealpal/apps/users/tests/test_views.py/n/nfrom django.test import RequestFactory
from django.shortcuts import reverse
from test_plus.test import TestCase
from django.test import Client
from realpal.apps.users.constants import *
from realpal.apps.users.views import UserRedirectView, UserUpdateView


class BaseUserTestCase(TestCase):
    def setUp(self):
        self.user = self.make_user()
        self.factory = RequestFactory()


class TestUserRedirectView(BaseUserTestCase):
    client = Client()

    def test_get_redirect_url(self):
        # Instantiate the view directly. Never do this outside a test!
        view = UserRedirectView()
        # Generate a fake request
        request = self.factory.get('/fake-url')
        # Attach the user to the request
        request.user = self.user
        # Attach the request to the view
        view.request = request
        # Expect: '/users/testuser/', as that is the default username for
        #   self.make_user()
        self.assertEqual(
            view.get_redirect_url(),
            '/users/testuser/'
        )


class TestUserUpdateView(BaseUserTestCase):
    def setUp(self):
        # call BaseUserTestCase.setUp()
        super(TestUserUpdateView, self).setUp()
        # Instantiate the view directly. Never do this outside a test!
        self.view = UserUpdateView()
        # Generate a fake request
        request = self.factory.get('/fake-url')
        # Attach the user to the request
        request.user = self.user
        # Attach the request to the view
        self.view.request = request

    def test_get_success_url(self):
        # Expect: '/users/testuser/', as that is the default username for
        #   self.make_user()
        self.assertEqual(
            self.view.get_success_url(),
            '/users/~update/#success'
        )

    def test_get_object(self):
        # Expect: self.user, as that is the request's user object
        self.assertEqual(
            self.view.get_object(),
            self.user
        )

    def test_updating_user_info(self):
        update_url = reverse('users:update')
        data = {
            'purchase_step_form': {'purchase_step': PS_DAP},
            'marital_status_form': {'status': SC_SI},
            'first_home_form': {'firsthome': True},
            'house_type_form': {'house_type': HT_SF, 'house_age': HA_15, 'house_cond': HC_SL},
            'city_form': {'preferred_city': ''},
            'max_budget_form': {'budget': 1200.59},
            'current_rent_form': {'current_rent': 321.49},
            'how_soon_form': {'how_soon': HS_3},
            'personal_profile_form': {
                'first_name': 'TestFirstName',
                'last_name': 'TestLastName',
                'zipcode': '10118',
                'phone_number': '+263771819478',
                'email': 'test_email@gmail.com',
            },
        }

        # let's login first since this view is only reachable after login
        self.client_user.login(username='testuser', password='password')

        # test to see if we reached the user update profile page
        self.assertTemplateUsed('users/update.html')

        for form in data:
            data_to_pass = data[form]
            data[form][form] = 'Update'
            response = self.client_user.post(update_url, data_to_pass)
            self.assertEqual(response.status_code, 302)
            self.assertTemplateUsed('users/update.html')

        # test to see that trying to update with incorrect data will never save the new data
        data = {'purchase_step': 8}  # 8 is not a valid option
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().purchase_step, PS_DAP)  # the default

        # testing marital status update
        data = {'status': 8}  # 8 is not a valid option
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().status, None)

        # testing house type update
        data = {'house_type': 8, 'house_age': 8, 'house_cond': 8}  # 8 is not a valid option
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().house_type, None)
        self.assertEqual(self.view.get_object().house_age, None)
        self.assertEqual(self.view.get_object().house_cond, None)

        # testing budget update
        data = {'budget': 'TEXT'}  # 8 is not a valid option
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().budget, None)

        # testing current rent update
        data = {'current_rent': 'TEXT'}  # TEXT is not a valid number
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().current_rent, None)

        # testing how soon update
        data = {'how_soon': 8}  # 8 is not a valid option
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().how_soon, None)

        # testing profile update
        data = {
                   'first_name': 'TestFirstName',
                   'last_name': 'TestLastName',
                   'zipcode': '10118',
                   'phone_number': '+26334465657456774567',  # number too long
                   'email': 'test_email@gmail.com',
               }
        self.client_user.post(update_url, data)
        self.assertEqual(self.view.get_object().first_name, '')
        self.assertEqual(self.view.get_object().zipcode, None)
        self.assertEqual(self.view.get_object().email, 'testuser')
/n/n/n",0
101,101,050077c00b7be9148370ab12ba022fe3479dd3ad,"/realpal/apps/chat/api.py/n/nimport arrow
import logging
import json
import os
from urllib.parse import urlparse

from channels import Group
from rest_framework.generics import CreateAPIView
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework import status

from django.conf import settings
from realpal.apps.chat.models import Message
from realpal.apps.chat.serializers import MessageSerializer
from realpal.apps.chat.consumers import get_room_group_channel
from realpal.apps.chat.models import Room

logger = logging.getLogger(__name__)


class MessageCreateAPIView(CreateAPIView):
    """"""
    Creates a new message object with a file attachment

    Returns on the socket

        {
            'id': ""id"",
            'sent_by':'user_id',
            'room':""room_id"",
            'text':message.txt,
            'file_name': message.attachment,
            'file_link': message.attachment.path
        }
    """"""
    model = Message
    serializer_class = MessageSerializer
    permission_classes = [IsAuthenticated, ]

    def create(self, request, *args, **kwargs):
        room_id = self.request.data.get('room')
        try:
            self.room = Room.objects.get(pk=room_id)
            self.request.data['sent_by'] = self.request.user.id
            self.request.data['room'] = self.room.id
            self.request.data['text'] = self.request.data.get('message')
            serializer = self.get_serializer(data=request.data)
            self.perform_create(serializer)
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        except Room.DoesNotExist:
            return Response(status=status.HTTP_400_BAD_REQUEST)

    def perform_create(self, serializer):
        serializer.is_valid(self)
        instance = serializer.save(sent_by=self.request.user, room=self.room)
        if not settings.IS_TESTING:
            data = {
                'id': instance.id.__str__(),
                'timestamp': instance.time_ago,
                'timestamp_string': instance.timestamp_string,
                'user_handle': self.request.user.full_name,
                'user_type': self.request.user.user_type,
                'message': instance.text,
                'file_name': os.path.basename(urlparse(instance.attachment.path).path) if instance.attachment else None,
                'file_link': instance.file_download_link if instance.attachment else None,
            }
            group_channel = get_room_group_channel(instance.room.id)
            self.push_socket_update(group_channel, data)

    @staticmethod
    def push_socket_update(group_channel, data):
        Group(group_channel).send({""text"": json.dumps(data)})
/n/n/n/realpal/apps/chat/serializers.py/n/nfrom rest_framework import serializers
from rest_framework.validators import ValidationError

from .models import Message


class MessageSerializer(serializers.ModelSerializer):
    """"""
    Message Serializer class
    """"""

    class Meta:
        model = Message
        fields = ('sent_by', 'room', 'text', 'attachment')
/n/n/n/realpal/apps/chat/urls.py/n/nfrom django.conf.urls import url

from realpal.apps.chat.views import ChatRoomView
from realpal.apps.chat.api import MessageCreateAPIView

urlpatterns = [
    url(r'^$', ChatRoomView.as_view(), name='chat-room'),
    url(r'^(?P<room_id>[0-9]+)/', ChatRoomView.as_view(), name='chat-room'),
    url(r'^file/$', MessageCreateAPIView.as_view(), name='chat-file'),

]
/n/n/n/realpal/apps/users/tests/test_views.py/n/nfrom django.test import RequestFactory
from django.shortcuts import reverse
from test_plus.test import TestCase
from django.test import Client
from realpal.apps.users.constants import *
from realpal.apps.users.views import UserRedirectView, UserUpdateView


class BaseUserTestCase(TestCase):
    def setUp(self):
        self.user = self.make_user()
        self.factory = RequestFactory()


class TestUserRedirectView(BaseUserTestCase):
    client = Client()

    def test_get_redirect_url(self):
        # Instantiate the view directly. Never do this outside a test!
        view = UserRedirectView()
        # Generate a fake request
        request = self.factory.get('/fake-url')
        # Attach the user to the request
        request.user = self.user
        # Attach the request to the view
        view.request = request
        # Expect: '/users/testuser/', as that is the default username for
        #   self.make_user()
        self.assertEqual(
            view.get_redirect_url(),
            '/users/testuser/'
        )


class TestUserUpdateView(BaseUserTestCase):
    def setUp(self):
        # call BaseUserTestCase.setUp()
        super(TestUserUpdateView, self).setUp()
        # Instantiate the view directly. Never do this outside a test!
        self.view = UserUpdateView()
        # Generate a fake request
        request = self.factory.get('/fake-url')
        # Attach the user to the request
        request.user = self.user
        # Attach the request to the view
        self.view.request = request

    def test_get_success_url(self):
        # Expect: '/users/testuser/', as that is the default username for
        #   self.make_user()
        self.assertEqual(
            self.view.get_success_url(),
            '/users/~update/#success'
        )

    def test_get_object(self):
        # Expect: self.user, as that is the request's user object
        self.assertEqual(
            self.view.get_object(),
            self.user
        )

    def test_updating_user_info(self):
        update_url = reverse('users:update')
        data = {
            'purchase_step_form': {'purchase_step': PS_DAP},
            'marital_status_form': {'status': SC_SI},
            'first_home_form': {'firsthome': True},
            'house_type_form': {'house_type': HT_SF, 'house_age': HA_15, 'house_cond': HC_SL},
            'city_form': {'preferred_city': ''},
            'max_budget_form': {'budget': 1200.59},
            'current_rent_form': {'current_rent': 321.49},
            'how_soon_form': {'how_soon': HS_3},
            'personal_profile_form': {
                'first_name': 'TestFirstName',
                'last_name': 'TestLastName',
                'zipcode': '10118',
                'phone_number': '+263771819478',
                'email': 'test_email@gmail.com',
            },
        }

        # let's login first since this view is only reachable after login
        self.client.login(username='testuser', password='password')

        # test to see if we reached the user update profile page
        self.assertTemplateUsed('users/update.html')

        for form in data:
            data_to_pass = data[form]
            data[form][form] = 'Update'
            response = self.client.post(update_url, data_to_pass)
            self.assertEqual(response.status_code, 302)
            self.assertTemplateUsed('users/update.html')

        # test to see that trying to update with incorrect data will never save the new data
        data = {'purchase_step': 8}  # 8 is not a valid option
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().purchase_step, PS_DAP)  # the default

        # testing marital status update
        data = {'status': 8}  # 8 is not a valid option
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().status, None)

        # testing house type update
        data = {'house_type': 8, 'house_age': 8, 'house_cond': 8}  # 8 is not a valid option
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().house_type, None)
        self.assertEqual(self.view.get_object().house_age, None)
        self.assertEqual(self.view.get_object().house_cond, None)

        # testing budget update
        data = {'budget': 'TEXT'}  # 8 is not a valid option
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().budget, None)

        # testing current rent update
        data = {'current_rent': 'TEXT'}  # TEXT is not a valid number
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().current_rent, None)

        # testing how soon update
        data = {'how_soon': 8}  # 8 is not a valid option
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().how_soon, None)

        # testing profile update
        data = {
                   'first_name': 'TestFirstName',
                   'last_name': 'TestLastName',
                   'zipcode': '10118',
                   'phone_number': '+26334465657456774567',  # number too long
                   'email': 'test_email@gmail.com',
               }
        self.client.post(update_url, data)
        self.assertEqual(self.view.get_object().first_name, '')
        self.assertEqual(self.view.get_object().zipcode, None)
        self.assertEqual(self.view.get_object().email, 'testuser')
/n/n/n",1
74,74,130aae1da13304c4d4afc79f6c3dad4872eaa9df,"chemie/elections/tests/test_views.py/n/nimport pytest
from django.shortcuts import reverse


@pytest.mark.django_db
def test_user_not_logged_in(client):
    request = client.get(reverse('elections:vote'))
    assert request.status_code == 302
    assert reverse('login') in request.url


@pytest.mark.django_db
def test_election_not_open(client, create_user):
    user = create_user
    client.login(username=user.username, password='defaultpassword')

    request = client.get(reverse('elections:vote'))
    assert 'Valget har ikke pnet enda' in request.content.decode('utf-8')
    assert '<a class=""button"" href=""/elections/results"">' in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url


@pytest.mark.django_db
def test_election_is_open(client, create_user, create_election_with_positions):
    election, positions = create_election_with_positions
    election.is_open = True
    election.save()
    user = create_user

    client.login(username=user.username, password='defaultpassword')
    request = client.get(reverse('elections:vote'))
    assert request.status_code is 200
    assert ""Klargjres til valg"" in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
/n/n/n",0
75,75,130aae1da13304c4d4afc79f6c3dad4872eaa9df,"/chemie/elections/tests/test_view.py/n/nimport pytest
from django.shortcuts import reverse
from



@pytest.mark.django_db
def test_user_not_logged_in(client,create_user):
    request = client.get(reverse('elections:vote'))
    assert request.status_code == 302
    assert reverse('login') in request.url

@pytest.mark.django_db
def test_election_not_open(client,create_user):
    user = create_user
    client.login(username=user.username, password='defaultpassword')

    request = client.get(reverse('elections:vote'))
    assert ""Valget har ikke pnet enda"" in request.content.decode('utf-8')
    assert '<a class=""button"" href=""/elections/results"">' in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url


@pytest.mark.django_db
def test_election_is_open(client,create_user, create_election_with_positions):
    election, positions = create_election_with_positions
    election = Election.objects.create()
    client.login(username=user.username, password='defaultpassword')
    request = client.get(reverse('elections:vote'))
    assert reverse('elections:vote') == request.url
    assert ""Klargjres til valg"" in request.content.decode('utf-8')

    request = client.get(reverse('elections:has_voted'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url
    request = client.get(reverse('elections:voting'))
    assert request.status_code == 302
    assert reverse('elections:vote') == request.url




/n/n/n",1
166,166,feb74b4f391db4b5add7a4f4960c554fb706f9d6,"gui/control_application.py/n/nimport os
import queue
import subprocess
import sys
import traceback
from contextlib import redirect_stdout

from PyQt5.QtCore import pyqtSlot
from PyQt5.QtGui import QTextCursor
from PyQt5.QtWidgets import QVBoxLayout, QMainWindow, QWidget, QTextEdit, QInputDialog, QMessageBox, QApplication

from matisse import Matisse
from .handled_decorators import handled_function, handled_slot
from .logging_stream import LoggingStream
from .status_monitor import StatusMonitor
from .threading import ExitFlag, LoggingThread


class ControlApplication(QApplication):
    EXIT_CODE_RESTART = 42  # Answer to the Ultimate Question of Life, the Universe, and Everything

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Non-handled functions only here
        self.setup_logging()
        self.setup_window()
        self.setup_menus()
        self.setup_action_listeners()
        # Set up the log window first to display log output
        self.setup_log_window()

        # Handled functions can go here
        self.setup_matisse()
        self.setup_widgets()

        # Other setup
        self.aboutToQuit.connect(self.clean_up)

        container = QWidget()
        container.setLayout(self.layout)
        self.window.setCentralWidget(container)
        self.window.show()

    def setup_logging(self):
        self.log_area = QTextEdit()
        self.log_area.setReadOnly(True)

        # Create the queue that holds all log messages and the input stream that writes them
        self.log_queue = queue.Queue()
        self.log_stream = LoggingStream(self.log_queue)
        # Create a thread to manage receiving log messages
        # TODO: Subclass QTextEdit and keep the logging thread there
        self.log_thread = LoggingThread(self.log_queue, parent=self)
        self.log_thread.message_received.connect(self.log)
        self.log_thread.start()
        # Set up a context manager to redirect stdout to the log window
        self.log_redirector = redirect_stdout(self.log_stream)
        self.log_redirector.__enter__()

    def setup_window(self):
        self.window = window = QMainWindow()
        self.layout = QVBoxLayout()
        window.setWindowTitle('Matisse Controller')
        window.resize(600, 200)

    def setup_menus(self):
        menu_bar = self.window.menuBar()

        console_menu = menu_bar.addMenu('Console')
        self.clear_log_area_action = console_menu.addAction('Clear Log')
        self.open_idle_action = console_menu.addAction('Open Python Shell...')
        self.restart_action = console_menu.addAction('Restart')

        set_menu = menu_bar.addMenu('Set')
        self.set_wavelength_action = set_menu.addAction('Wavelength')
        self.set_bifi_approx_wavelength_action = set_menu.addAction('BiFi Approx. Wavelength')
        self.set_bifi_motor_pos_action = set_menu.addAction('BiFi Motor Position')
        self.set_thin_eta_motor_pos_action = set_menu.addAction('Thin Etalon Motor Position')

        scan_menu = menu_bar.addMenu('Scan')
        self.bifi_scan_action = scan_menu.addAction('Birefringent Filter')
        self.thin_eta_scan_action = scan_menu.addAction('Thin Etalon')

        lock_menu = menu_bar.addMenu('Lock')
        self.lock_all_action = lock_menu.addAction('Lock All')
        self.lock_all_action.setCheckable(True)
        self.lock_slow_piezo_action = lock_menu.addAction('Lock Slow Piezo')
        self.lock_slow_piezo_action.setCheckable(True)
        self.lock_thin_etalon_action = lock_menu.addAction('Lock Thin Etalon')
        self.lock_thin_etalon_action.setCheckable(True)
        self.lock_piezo_etalon_action = lock_menu.addAction('Lock Piezo Etalon')
        self.lock_piezo_etalon_action.setCheckable(True)
        self.lock_fast_piezo_action = lock_menu.addAction('Lock Fast Piezo')
        self.lock_fast_piezo_action.setCheckable(True)

        tools_menu = menu_bar.addMenu('Tools')

        self.lock_actions = [self.lock_slow_piezo_action, self.lock_thin_etalon_action, self.lock_piezo_etalon_action,
                             self.lock_fast_piezo_action]

    def setup_action_listeners(self):
        # Console
        self.clear_log_area_action.triggered.connect(self.clear_log_area)
        self.open_idle_action.triggered.connect(self.open_idle)
        self.restart_action.triggered.connect(self.restart)

        # Set
        self.set_wavelength_action.triggered.connect(self.set_wavelength_dialog)
        self.set_bifi_approx_wavelength_action.triggered.connect(self.set_bifi_approx_wavelength_dialog)
        self.set_bifi_motor_pos_action.triggered.connect(self.set_bifi_motor_pos_dialog)
        self.set_thin_eta_motor_pos_action.triggered.connect(self.set_thin_eta_motor_pos_dialog)

        # Scan
        self.bifi_scan_action.triggered.connect(self.start_bifi_scan)
        self.thin_eta_scan_action.triggered.connect(self.start_thin_etalon_scan)

        # Lock
        self.lock_all_action.triggered.connect(self.toggle_lock_all)
        self.lock_slow_piezo_action.triggered.connect(self.toggle_slow_piezo_lock)
        self.lock_thin_etalon_action.triggered.connect(self.toggle_thin_etalon_lock)
        self.lock_piezo_etalon_action.triggered.connect(self.toggle_piezo_etalon_lock)
        self.lock_fast_piezo_action.triggered.connect(self.toggle_fast_piezo_lock)

        # Tools

    def setup_log_window(self):
        self.layout.addWidget(self.log_area)

    @handled_function
    def setup_widgets(self):
        self.status_monitor_queue = queue.Queue(maxsize=1)
        self.status_monitor = StatusMonitor(self.matisse, self.status_monitor_queue)
        self.layout.addWidget(self.status_monitor)

    @handled_function
    def setup_matisse(self):
        try:
            self.matisse: Matisse = Matisse(device_id=sys.argv[1], wavemeter_port=sys.argv[2])
        except Exception as err:
            self.matisse: Matisse = None
            raise err

    @pyqtSlot()
    def clean_up(self):
        self.status_monitor_queue.put(ExitFlag())
        self.status_monitor.update_thread.wait()
        self.log_queue.put(ExitFlag())
        self.log_thread.wait()
        self.log_redirector.__exit__(None, None, None)

    @pyqtSlot(str)
    def log(self, message):
        self.log_area.moveCursor(QTextCursor.End)
        self.log_area.insertPlainText(message)

    def error_dialog(self):
        stack = list(traceback.format_exception(*sys.exc_info()))
        # Pick length of longest line in stack, with a cutoff at 185
        desired_width = min(max([len(line) for line in stack]), 185)
        description = stack.pop()
        print(description, end='')
        # Remove entries for handled_function decorator, for clarity
        stack = filter(lambda item: os.path.join('gui', 'handled_decorators.py') not in item, stack)
        dialog = QMessageBox(icon=QMessageBox.Critical)
        dialog.setWindowTitle('Error')
        # Adding the underscores is a hack to resize the QMessageBox because it's not normally resizable.
        # This looks good in Windows, haven't tested other platforms. Sorry :(
        dialog.setText(f""{description + '_' * desired_width}\n\n{''.join(stack)}"")
        dialog.exec()

    @handled_slot(bool)
    def clear_log_area(self, checked):
        self.log_area.clear()

    @handled_slot(bool)
    def open_idle(self, checked):
        print('Opening IDLE.')
        subprocess.Popen('python -m idlelib -t ""Matisse Controller - Python Shell"" -c ""from matisse import Matisse; ' +
                         f""matisse = Matisse(device_id='{sys.argv[1]}', wavemeter_port='{sys.argv[2]}'); "" +
                         f""print(\\\""Access the Matisse using 'matisse.[method]'\\\"")\"""")

    @handled_slot(bool)
    def restart(self, checked):
        self.exit(self.EXIT_CODE_RESTART)

    @handled_slot(bool)
    def set_wavelength_dialog(self, checked):
        target_wavelength, success = QInputDialog.getDouble(self.window,
                                                            title='Set Wavelength',
                                                            label='Wavelength (nm): ',
                                                            value=self.matisse.target_wavelength)
        if success:
            print(f""Setting wavelength to {target_wavelength} nm..."")
            self.matisse.set_wavelength(target_wavelength)

    @handled_slot(bool)
    def set_bifi_approx_wavelength_dialog(self, checked):
        target_wavelength, success = QInputDialog.getDouble(self.window,
                                                            title='Set Approx. Wavelength',
                                                            label='Wavelength (nm): ',
                                                            value=self.matisse.query('MOTBI:WL?', numeric_result=True))
        if success:
            print(f""Setting BiFi approximate wavelength to {target_wavelength} nm..."")
            self.matisse.set_bifi_wavelength(target_wavelength)

    @handled_slot(bool)
    def set_bifi_motor_pos_dialog(self, checked):
        target_pos, success = QInputDialog.getInt(self.window,
                                                  title='Set BiFi Motor Position',
                                                  label='Absolute Position:',
                                                  value=self.matisse.query('MOTBI:POS?', numeric_result=True))
        if success:
            print(f""Setting BiFi motor position to {target_pos}."")
            self.matisse.set_bifi_motor_pos(target_pos)

    @handled_slot(bool)
    def set_thin_eta_motor_pos_dialog(self, checked):
        target_pos, success = QInputDialog.getInt(self.window,
                                                  title='Set Thin Etalon Motor Position',
                                                  label='Absolute Position:',
                                                  value=self.matisse.query('MOTTE:POS?', numeric_result=True))
        if success:
            print(f""Setting thin etalon motor position to {target_pos}."")
            self.matisse.set_thin_etalon_motor_pos(target_pos)

    @handled_slot(bool)
    def start_bifi_scan(self, checked):
        print('Starting BiFi scan...')
        self.matisse.birefringent_filter_scan()

    @handled_slot(bool)
    def start_thin_etalon_scan(self, checked):
        print('Starting thin etalon scan...')
        self.matisse.thin_etalon_scan()

    @handled_slot(bool)
    def toggle_lock_all(self, checked):
        if checked:
            for action in self.lock_actions:
                if not action.isChecked():
                    action.trigger()
            if all([action.isChecked() for action in self.lock_actions]):
                [action.setEnabled(False) for action in self.lock_actions]
            else:
                self.lock_all_action.setChecked(False)
                print(""Couldn't lock all laser components."")
        else:
            for action in reversed(self.lock_actions):
                action.trigger()
                action.setEnabled(True)

    @handled_slot(bool)
    def toggle_slow_piezo_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} slow piezo."")
        self.lock_slow_piezo_action.setChecked(not checked)
        self.matisse.set_slow_piezo_lock(checked)
        self.lock_slow_piezo_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_thin_etalon_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} thin etalon."")
        self.lock_thin_etalon_action.setChecked(not checked)
        self.matisse.set_thin_etalon_lock(checked)
        self.lock_thin_etalon_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_piezo_etalon_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} piezo etalon."")
        self.lock_piezo_etalon_action.setChecked(not checked)
        self.matisse.set_piezo_etalon_lock(checked)
        self.lock_piezo_etalon_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_fast_piezo_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} fast piezo."")
        self.lock_fast_piezo_action.setChecked(not checked)
        self.matisse.set_piezo_etalon_lock(checked)
        self.lock_fast_piezo_action.setChecked(checked)
/n/n/nmain.py/n/nimport sys

from gui import ControlApplication

# TODO: Assert that the user has provided these values
sys.argv.append('USB0::0x17E7::0x0102::07-40-01::INSTR')
sys.argv.append('COM5')
print(sys.argv)

exit_code = ControlApplication.EXIT_CODE_RESTART
while exit_code == ControlApplication.EXIT_CODE_RESTART:
    gui = ControlApplication([])
    exit_code = gui.exec()
    del gui
/n/n/n",0
167,167,feb74b4f391db4b5add7a4f4960c554fb706f9d6,"/gui/control_application.py/n/nimport os
import queue
import subprocess
import sys
import traceback

from PyQt5.QtCore import pyqtSlot
from PyQt5.QtGui import QTextCursor
from PyQt5.QtWidgets import QVBoxLayout, QMainWindow, QWidget, QTextEdit, QInputDialog, QMessageBox, QApplication

from matisse import Matisse
from .handled_decorators import handled_function, handled_slot
from .logging_stream import LoggingStream
from .status_monitor import StatusMonitor
from .threading import ExitFlag, LoggingThread


class ControlApplication(QApplication):
    EXIT_CODE_RESTART = 42  # Answer to the Ultimate Question of Life, the Universe, and Everything

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Non-handled functions only here
        self.setup_logging()
        self.setup_window()
        self.setup_menus()
        self.setup_action_listeners()
        # Set up the log window first to display log output
        self.setup_log_window()

        # Handled functions can go here
        self.setup_matisse()
        self.setup_widgets()

        # Other setup
        self.aboutToQuit.connect(self.clean_up)

        container = QWidget()
        container.setLayout(self.layout)
        self.window.setCentralWidget(container)
        self.window.show()

    def setup_logging(self):
        self.log_area = QTextEdit()
        self.log_area.setReadOnly(True)

        # Create the queue that holds all log messages and the input stream that writes them
        self.log_queue = queue.Queue()
        self.log_stream = LoggingStream(self.log_queue)
        # Create a thread to manage receiving log messages
        # TODO: Subclass QTextEdit and keep the logging thread there
        self.log_thread = LoggingThread(self.log_queue, parent=self)
        self.log_thread.message_received.connect(self.log)
        self.log_thread.start()

    def setup_window(self):
        self.window = window = QMainWindow()
        self.layout = QVBoxLayout()
        window.setWindowTitle('Matisse Controller')
        window.resize(600, 200)

    def setup_menus(self):
        menu_bar = self.window.menuBar()

        console_menu = menu_bar.addMenu('Console')
        self.clear_log_area_action = console_menu.addAction('Clear Log')
        self.open_idle_action = console_menu.addAction('Open Python Shell...')
        self.restart_action = console_menu.addAction('Restart')

        set_menu = menu_bar.addMenu('Set')
        self.set_wavelength_action = set_menu.addAction('Wavelength')
        self.set_bifi_approx_wavelength_action = set_menu.addAction('BiFi Approx. Wavelength')
        self.set_bifi_motor_pos_action = set_menu.addAction('BiFi Motor Position')
        self.set_thin_eta_motor_pos_action = set_menu.addAction('Thin Etalon Motor Position')

        scan_menu = menu_bar.addMenu('Scan')
        self.bifi_scan_action = scan_menu.addAction('Birefringent Filter')
        self.thin_eta_scan_action = scan_menu.addAction('Thin Etalon')

        lock_menu = menu_bar.addMenu('Lock')
        self.lock_all_action = lock_menu.addAction('Lock All')
        self.lock_all_action.setCheckable(True)
        self.lock_slow_piezo_action = lock_menu.addAction('Lock Slow Piezo')
        self.lock_slow_piezo_action.setCheckable(True)
        self.lock_thin_etalon_action = lock_menu.addAction('Lock Thin Etalon')
        self.lock_thin_etalon_action.setCheckable(True)
        self.lock_piezo_etalon_action = lock_menu.addAction('Lock Piezo Etalon')
        self.lock_piezo_etalon_action.setCheckable(True)
        self.lock_fast_piezo_action = lock_menu.addAction('Lock Fast Piezo')
        self.lock_fast_piezo_action.setCheckable(True)

        tools_menu = menu_bar.addMenu('Tools')

        self.lock_actions = [self.lock_slow_piezo_action, self.lock_thin_etalon_action, self.lock_piezo_etalon_action,
                             self.lock_fast_piezo_action]

    def setup_action_listeners(self):
        # Console
        self.clear_log_area_action.triggered.connect(self.clear_log_area)
        self.open_idle_action.triggered.connect(self.open_idle)
        self.restart_action.triggered.connect(self.restart)

        # Set
        self.set_wavelength_action.triggered.connect(self.set_wavelength_dialog)
        self.set_bifi_approx_wavelength_action.triggered.connect(self.set_bifi_approx_wavelength_dialog)
        self.set_bifi_motor_pos_action.triggered.connect(self.set_bifi_motor_pos_dialog)
        self.set_thin_eta_motor_pos_action.triggered.connect(self.set_thin_eta_motor_pos_dialog)

        # Scan
        self.bifi_scan_action.triggered.connect(self.start_bifi_scan)
        self.thin_eta_scan_action.triggered.connect(self.start_thin_etalon_scan)

        # Lock
        self.lock_all_action.triggered.connect(self.toggle_lock_all)
        self.lock_slow_piezo_action.triggered.connect(self.toggle_slow_piezo_lock)
        self.lock_thin_etalon_action.triggered.connect(self.toggle_thin_etalon_lock)
        self.lock_piezo_etalon_action.triggered.connect(self.toggle_piezo_etalon_lock)
        self.lock_fast_piezo_action.triggered.connect(self.toggle_fast_piezo_lock)

        # Tools

    def setup_log_window(self):
        self.layout.addWidget(self.log_area)

    @handled_function
    def setup_widgets(self):
        self.status_monitor_queue = queue.Queue(maxsize=1)
        self.status_monitor = StatusMonitor(self.matisse, self.status_monitor_queue)
        self.layout.addWidget(self.status_monitor)

    @handled_function
    def setup_matisse(self):
        try:
            self.matisse: Matisse = Matisse(device_id=sys.argv[1], wavemeter_port=sys.argv[2])
        except Exception as err:
            self.matisse: Matisse = None
            raise err

    @pyqtSlot()
    def clean_up(self):
        self.status_monitor_queue.put(ExitFlag())
        self.status_monitor.update_thread.wait()
        self.log_queue.put(ExitFlag())
        self.log_thread.wait()

    @pyqtSlot(str)
    def log(self, message):
        self.log_area.moveCursor(QTextCursor.End)
        self.log_area.insertPlainText(message)

    def error_dialog(self):
        stack = list(traceback.format_exception(*sys.exc_info()))
        # Pick length of longest line in stack, with a cutoff at 185
        desired_width = min(max([len(line) for line in stack]), 185)
        description = stack.pop()
        print(description, end='')
        # Remove entries for handled_function decorator, for clarity
        stack = filter(lambda item: os.path.join('gui', 'handled_decorators.py') not in item, stack)
        dialog = QMessageBox(icon=QMessageBox.Critical)
        dialog.setWindowTitle('Error')
        # Adding the underscores is a hack to resize the QMessageBox because it's not normally resizable.
        # This looks good in Windows, haven't tested other platforms. Sorry :(
        dialog.setText(f""{description + '_' * desired_width}\n\n{''.join(stack)}"")
        dialog.exec()

    @handled_slot(bool)
    def clear_log_area(self, checked):
        self.log_area.clear()

    @handled_slot(bool)
    def open_idle(self, checked):
        print('Opening IDLE.')
        subprocess.Popen('python -m idlelib -t ""Matisse Controller - Python Shell"" -c ""from matisse import Matisse; ' +
                         'matisse = Matisse(); print(\'Access the Matisse using \\\'matisse.[method]\\\'\')""')

    @handled_slot(bool)
    def restart(self, checked):
        self.exit(self.EXIT_CODE_RESTART)

    @handled_slot(bool)
    def set_wavelength_dialog(self, checked):
        target_wavelength, success = QInputDialog.getDouble(self.window,
                                                            title='Set Wavelength',
                                                            label='Wavelength (nm): ',
                                                            value=self.matisse.target_wavelength)
        if success:
            print(f""Setting wavelength to {target_wavelength} nm..."")
            self.matisse.set_wavelength(target_wavelength)

    @handled_slot(bool)
    def set_bifi_approx_wavelength_dialog(self, checked):
        target_wavelength, success = QInputDialog.getDouble(self.window,
                                                            title='Set Approx. Wavelength',
                                                            label='Wavelength (nm): ',
                                                            value=self.matisse.query('MOTBI:WL?', numeric_result=True))
        if success:
            print(f""Setting BiFi approximate wavelength to {target_wavelength} nm..."")
            self.matisse.set_bifi_wavelength(target_wavelength)

    @handled_slot(bool)
    def set_bifi_motor_pos_dialog(self, checked):
        target_pos, success = QInputDialog.getInt(self.window,
                                                  title='Set BiFi Motor Position',
                                                  label='Absolute Position:',
                                                  value=self.matisse.query('MOTBI:POS?', numeric_result=True))
        if success:
            print(f""Setting BiFi motor position to {target_pos}."")
            self.matisse.set_bifi_motor_pos(target_pos)

    @handled_slot(bool)
    def set_thin_eta_motor_pos_dialog(self, checked):
        target_pos, success = QInputDialog.getInt(self.window,
                                                  title='Set Thin Etalon Motor Position',
                                                  label='Absolute Position:',
                                                  value=self.matisse.query('MOTTE:POS?', numeric_result=True))
        if success:
            print(f""Setting thin etalon motor position to {target_pos}."")
            self.matisse.set_thin_etalon_motor_pos(target_pos)

    @handled_slot(bool)
    def start_bifi_scan(self, checked):
        print('Starting BiFi scan...')
        self.matisse.birefringent_filter_scan()

    @handled_slot(bool)
    def start_thin_etalon_scan(self, checked):
        print('Starting thin etalon scan...')
        self.matisse.thin_etalon_scan()

    @handled_slot(bool)
    def toggle_lock_all(self, checked):
        if checked:
            for action in self.lock_actions:
                if not action.isChecked():
                    action.trigger()
            if all([action.isChecked() for action in self.lock_actions]):
                [action.setEnabled(False) for action in self.lock_actions]
            else:
                self.lock_all_action.setChecked(False)
                print(""Couldn't lock all laser components."")
        else:
            for action in reversed(self.lock_actions):
                action.trigger()
                action.setEnabled(True)

    @handled_slot(bool)
    def toggle_slow_piezo_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} slow piezo."")
        self.lock_slow_piezo_action.setChecked(not checked)
        self.matisse.set_slow_piezo_lock(checked)
        self.lock_slow_piezo_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_thin_etalon_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} thin etalon."")
        self.lock_thin_etalon_action.setChecked(not checked)
        self.matisse.set_thin_etalon_lock(checked)
        self.lock_thin_etalon_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_piezo_etalon_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} piezo etalon."")
        self.lock_piezo_etalon_action.setChecked(not checked)
        self.matisse.set_piezo_etalon_lock(checked)
        self.lock_piezo_etalon_action.setChecked(checked)

    @handled_slot(bool)
    def toggle_fast_piezo_lock(self, checked):
        print(f""{'Locking' if checked else 'Unlocking'} fast piezo."")
        self.lock_fast_piezo_action.setChecked(not checked)
        self.matisse.set_piezo_etalon_lock(checked)
        self.lock_fast_piezo_action.setChecked(checked)
/n/n/n/main.py/n/nimport sys
from contextlib import redirect_stdout

from gui import ControlApplication

# TODO: Assert that the user has provided these values
sys.argv.append('USB0::0x17E7::0x0102::07-40-01::INSTR')
sys.argv.append('COM5')
print(sys.argv)

exit_code = ControlApplication.EXIT_CODE_RESTART
while exit_code == ControlApplication.EXIT_CODE_RESTART:
    gui = ControlApplication([])
    with redirect_stdout(gui.log_stream):
        exit_code = gui.exec()
    del gui
/n/n/n",1
0,0,a1f948b468b6621083a03b0d53432341b7a4d753,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes
import unittest
from os import path
from urllib.parse import quote

from django.conf.urls.static import static
from django.core.exceptions import ImproperlyConfigured
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/nonexistent_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super().setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super().tearDown()
        urls.urlpatterns = self._old_views_urlpatterns

    def test_prefix(self):
        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')

    @override_settings(DEBUG=False)
    def test_debug_off(self):
        """"""No URLs are served if DEBUG=False.""""""
        self.assertEqual(static('test'), [])

    def test_empty_prefix(self):
        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):
            static('')

    def test_special_prefix(self):
        """"""No URLs are served if prefix contains '://'.""""""
        self.assertEqual(static('http://'), [])


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0
1,1,a1f948b468b6621083a03b0d53432341b7a4d753,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path)
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1
76,76,a5cf6aaf22fc4fac709237f40e869d7ecf690b01,"dfsh.py/n/n#!/usr/bin/env python3

import os
import sys

import contextlib

import enum
from enum import Enum

def main():
    sh = Shell()
    sh.run()

class Shell:
    '''
    The main shell class.
    '''

    def __init__(self):
        self.builtins = {
            'exit': self._builtin_exit,
            'pwd': self._builtin_pwd,
            'cd': self._builtin_cd
        }

    def run(self):
        '''
        Run the shell.
        '''

        while True:
            try:
                line = self.readline()
                self.execute(line)
            except EOFError:
                sys.exit(0)

    def readline(self):
        '''
        Read a command from stdin to execute.

        Returns:
            A raw string read from stdin.
        '''

        while True:
            raw = input('$ ')
            if len(raw) > 0:
                return raw

    def execute(self, raw):
        '''
        Execute a command in the form of a raw string.
        '''

        tokens = Tokenizer(raw)

        parser = Parser(tokens)
        root = parser.parse()
        if root:
            root.execute(self.builtins)
            root.wait()

    # various shell builtins
    def _builtin_exit(self, name, n=0):
        sys.exit(n)

    def _builtin_pwd(self, name):
        wd = os.getcwd()
        print(wd)

    def _builtin_cd(self, name, d):
        os.chdir(d)

class TokenType(Enum):
    '''
    Token types that are recognized by the Tokenizer.
    '''

    WORD = enum.auto()
    REDIRECT_OUT = enum.auto()
    REDIRECT_APPEND = enum.auto()
    REDIRECT_IN = enum.auto()
    PIPE = enum.auto()
    COMMAND_END = enum.auto()
    EOF = enum.auto()
    UNKNOWN = enum.auto()

class Token:
    '''
    A string with an assigned meaning.

    Args:
        ttype: The token meaning.
        lexeme: The token value (optional).
        position: The location of the token in the stream.
    '''

    def __init__(self, ttype, lexeme=None, position=None):
        self.lexeme = lexeme
        self.ttype = ttype
        self.position = position

class Tokenizer:
    '''
    Performs lexical analysis on a raw string.

    Args:
        string: The raw string on which to operate.
    '''

    def __init__(self, string):
        self.string = string
        self.position = -1
        self.char = None

        self.read()

    def read(self):
        '''
        Read a single char from the stream and store it in self.char.

        Returns:
            The value of self.char.
        '''

        self.position += 1
        if self.position < len(self.string):
            self.char = self.string[self.position]
        else:
            self.char = None
        return self.char

    def token(self):
        '''
        Read a single token from the stream.

        Returns:
            The generated token.

        Throws:
            May throw a ValueError in the case that the input is malformed and
            a token cannot be correctly generated from it.
        '''

        # ignore whitespace
        while self.char and self.char.isspace():
            self.read()

        if self.char == None:
            # end-of-file
            return Token(TokenType.EOF, None, self.position)
        elif self.char == '>':
            start = self.position
            if self.read() == '>':
                self.read()
                return Token(TokenType.REDIRECT_APPEND, None, start)
            else:
                return Token(TokenType.REDIRECT_OUT, None, start)
        elif self.char == '<':
            token = Token(TokenType.REDIRECT_IN, None, self.position)
            self.read()
            return token
        elif self.char == '|':
            token = Token(TokenType.PIPE, None, self.position)
            self.read()
            return token
        elif self.char == ';':
            token = Token(TokenType.COMMAND_END, None, self.position)
            self.read()
            return token
        elif self.char in '\'""':
            # quoted word
            end = self.char
            self.read()

            start = self.position
            value = []
            while self.char and self.char != end:
                value.append(self.char)
                self.read()
            if self.char is None:
                raise ValueError('unexpected end of line while reading quoted word')
            else:
                self.read()

            return Token(TokenType.WORD, ''.join(value), start)
        elif self.char.isprintable():
            # single word
            start = self.position
            value = []
            while self.char and self.char.isprintable() and not self.char.isspace():
                value.append(self.char)
                self.read()

            return Token(TokenType.WORD, ''.join(value), start)
        else:
            # unknown
            token = Token(TokenType.UNKNOWN, self.char, self.position)
            self.read()
            return token

    def __iter__(self):
        '''
        Utility iterator to allow easy creation of a stream of tokens.
        '''

        while True:
            token = self.token()
            yield token

            if token.ttype == TokenType.EOF: break

class Parser:
    '''
    Parses a stream of tokens into an Abstract Syntax Tree for later execution.

    Args:
        tokens: The stream of tokens.
    '''

    def __init__(self, tokens):
        self.tokens = iter(tokens)
        self.token = None
        self.last = None

        self.next()

    def parse(self):
        '''
        Parse the stream of tokens.

        Returns:
            The root node of the Abstract Syntax Tree.

        Throws:
            May throw a ValueError in the case that the stream of tokens is
            malformed.
        '''

        root = self.commands()
        self.expect(TokenType.EOF)

        return root

    def commands(self):
        base = self.command()
        if self.accept(TokenType.COMMAND_END):
            other = self.commands()
            if base and other:
                return DoubleNode(base, other)
            else:
                return other
        else:
            return base

    def command(self):
        if self.accept(TokenType.WORD):
            command = self.last.lexeme

            args = []
            while self.accept(TokenType.WORD):
                args.append(self.last.lexeme)

            node = CommandNode(command, args)

            redirs = self.redirections()
            if redirs:
                node = RedirectionsNode(node, redirs)

            if self.accept(TokenType.PIPE):
                return PipeNode(node, self.command())
            else:
                return node
        else:
            return None

    def redirections(self):
        redirs = []
        redir = self.redirection()
        while redir:
            redirs.append(redir)
            redir = self.redirection()

        if len(redirs) > 0:
            return RedirectionsHelper(redirs)
        else:
            return None

    def redirection(self):
        # TODO: recognize other types of redirections
        if self.accept(TokenType.REDIRECT_OUT):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(1, (filename, os.O_CREAT | os.O_WRONLY | os.O_TRUNC))
        elif self.accept(TokenType.REDIRECT_APPEND):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(1, (filename, os.O_CREAT | os.O_WRONLY | os.O_APPEND))
        elif self.accept(TokenType.REDIRECT_IN):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(0, (filename, os.O_RDONLY))
        else:
            return None

    def next(self):
        self.last = self.token
        self.token = next(self.tokens, None)
        return self.token

    def accept(self, ttype):
        if self.token and self.token.ttype == ttype:
            self.next()
            return self.last
        else:
            return None

    def expect(self, ttype):
        result = self.accept(ttype)
        if result:
            return result
        else:
            raise ValueError(f'expected token to be {ttype}, instead got {self.token.ttype}')

class Node:
    '''
    A single node in the Abstract Syntax Tree.
    '''

    def execute(self, builtins):
        '''
        Execute the node.

        Args:
            builtins: A dict of builtin commands.
        '''

        pass

    def wait(self):
        '''
        Wait for the execution of the node to finish.
        '''

        pass

class DoubleNode(Node):
    '''
    A node that executes two nodes sequentially.

    Args:
        first: The first node to execute.
        second: The second node to execute.
    '''

    def __init__(self, first, second):
        self.first = first
        self.second = second

    def execute(self, *args):
        self.first.execute(*args)
        self.first.wait()

        self.second.execute(*args)
        self.second.wait()

class PipeNode(DoubleNode):
    '''
    A node that forwards the output of one node to the input of another.

    Args:
        first: The node to pipe the output from.
        second: The node to pipe the input into.
    '''

    def __init__(self, first, second):
        self.first = first
        self.second = second

    def execute(self, builtins):
        read, write = os.pipe()
        inp = RedirectionHelper(0, read)
        outp = RedirectionHelper(1, write)

        # TODO: close inp in child process
        with outp:
            self.first.execute(builtins)
        outp.close()

        with inp:
            self.second.execute(builtins)

    def wait(self):
        self.first.wait()
        self.second.wait()

class CommandNode(Node):
    '''
    A node that contains a single shell command.

    Args:
        command: The name of the executable to run (will be looked up in PATH).
        args: The arguments to be passed to the executable.
    '''

    def __init__(self, command, args):
        self.command = command

        self.args = args
        self.args.insert(0, command)

        self.pid = None

    def execute(self, builtins):
        if self.command in builtins:
            builtins[self.command](*self.args)
        else:
            pid = os.fork()
            if pid == 0:
                # child process
                os.execv(self.full_command, self.args)
            else:
                # parent process
                self.pid = pid

    def wait(self):
        if self.pid:
            os.waitpid(self.pid, 0)

    @property
    def full_command(self):
        if os.path.exists(self.command):
            return self.command

        path = os.environ['PATH'].split(':')
        for di in path:
            cmd = os.path.join(di, self.command)
            if os.path.exists(cmd):
                return cmd

        raise FileNotFoundError('command not found')

class RedirectionsNode(Node):
    '''
    A node that performs a number of IO redirections.

    Args:
        base: The base node to operate on.
        redirections: The redirections to apply.
    '''

    def __init__(self, base, redirections):
        self.base = base
        self.redirections = redirections

    def execute(self, builtins):
        with self.redirections:
            self.base.execute(builtins)

    def wait(self):
        self.base.wait()

class RedirectionHelper:
    '''
    Helps perform a single file redirection.

    Args:
        fd: The file descriptor to modify.
        newfd: The new file descriptor.
    '''

    def __init__(self, fd, newfd):
        self.fd = fd
        self.backup = os.dup(fd)

        try:
            if len(newfd) == 2:
                filename, mode = newfd
                self.newfd = os.open(filename, mode, 0o644)
            elif len(newfd) == 3:
                filename, mode, permissions = newfd
                self.newfd = os.open(filename, mode, permissions)
            else:
                raise ValueError('invalid file open parameters')
        except TypeError:
            self.newfd = newfd

    def close(self):
        os.close(self.newfd)

    def __enter__(self):
        os.dup2(self.newfd, self.fd)

    def __exit__(self, type, value, traceback):
        os.dup2(self.backup, self.fd)

class RedirectionsHelper:
    '''
    Helps perform multiple file redirections.

    Args:
        redirections: A list of redirections.
    '''

    def __init__(self, redirections):
        self.redirections = redirections

        self.stack = None

    def __enter__(self):
        if len(self.redirections) > 0:
            self.stack = contextlib.ExitStack()
            for redirection in self.redirections:
                self.stack.enter_context(redirection)

    def __exit__(self, type, value, traceback):
        if self.stack:
            self.stack.close()
            self.stack = None

if __name__ == ""__main__"":
    main()
/n/n/n",0
77,77,a5cf6aaf22fc4fac709237f40e869d7ecf690b01,"/dfsh.py/n/n#!/usr/bin/env python3

import os
import sys

import contextlib

import enum
from enum import Enum

def main():
    sh = Shell()
    sh.run()

class Shell:
    '''
    The main shell class.
    '''

    def __init__(self):
        self.builtins = {
            'exit': self._builtin_exit,
            'pwd': self._builtin_pwd,
            'cd': self._builtin_cd
        }

    def run(self):
        '''
        Run the shell.
        '''

        while True:
            try:
                line = self.readline()
                self.execute(line)
            except EOFError:
                sys.exit(0)

    def readline(self):
        '''
        Read a command from stdin to execute.

        Returns:
            A raw string read from stdin.
        '''

        while True:
            raw = input('$ ')
            if len(raw) > 0:
                return raw

    def execute(self, raw):
        '''
        Execute a command in the form of a raw string.
        '''

        tokens = Tokenizer(raw)

        parser = Parser(tokens)
        root = parser.parse()
        if root:
            root.execute(self.builtins)
            root.wait()

    # various shell builtins
    def _builtin_exit(self, name, n=0):
        sys.exit(n)

    def _builtin_pwd(self, name):
        wd = os.getcwd()
        print(wd)

    def _builtin_cd(self, name, d):
        os.chdir(d)

class TokenType(Enum):
    '''
    Token types that are recognized by the Tokenizer.
    '''

    WORD = enum.auto()
    REDIRECT_OUT = enum.auto()
    REDIRECT_APPEND = enum.auto()
    REDIRECT_IN = enum.auto()
    PIPE = enum.auto()
    COMMAND_END = enum.auto()
    EOF = enum.auto()
    UNKNOWN = enum.auto()

class Token:
    '''
    A string with an assigned meaning.

    Args:
        ttype: The token meaning.
        lexeme: The token value (optional).
        position: The location of the token in the stream.
    '''

    def __init__(self, ttype, lexeme=None, position=None):
        self.lexeme = lexeme
        self.ttype = ttype
        self.position = position

class Tokenizer:
    '''
    Performs lexical analysis on a raw string.

    Args:
        string: The raw string on which to operate.
    '''

    def __init__(self, string):
        self.string = string
        self.position = -1
        self.char = None

        self.read()

    def read(self):
        '''
        Read a single char from the stream and store it in self.char.

        Returns:
            The value of self.char.
        '''

        self.position += 1
        if self.position < len(self.string):
            self.char = self.string[self.position]
        else:
            self.char = None
        return self.char

    def token(self):
        '''
        Read a single token from the stream.

        Returns:
            The generated token.

        Throws:
            May throw a ValueError in the case that the input is malformed and
            a token cannot be correctly generated from it.
        '''

        # ignore whitespace
        while self.char and self.char.isspace():
            self.read()

        if self.char == None:
            # end-of-file
            return Token(TokenType.EOF, None, self.position)
        elif self.char == '>':
            start = self.position
            if self.read() == '>':
                self.read()
                return Token(TokenType.REDIRECT_APPEND, None, start)
            else:
                return Token(TokenType.REDIRECT_OUT, None, start)
        elif self.char == '<':
            token = Token(TokenType.REDIRECT_IN, None, self.position)
            self.read()
            return token
        elif self.char == '|':
            token = Token(TokenType.PIPE, None, self.position)
            self.read()
            return token
        elif self.char == ';':
            token = Token(TokenType.COMMAND_END, None, self.position)
            self.read()
            return token
        elif self.char in '\'""':
            # quoted word
            end = self.char
            self.read()

            start = self.position
            value = []
            while self.char and self.char != end:
                value.append(self.char)
                self.read()
            if self.char is None:
                raise ValueError('unexpected end of line while reading quoted word')
            else:
                self.read()

            return Token(TokenType.WORD, ''.join(value), start)
        elif self.char.isprintable():
            # single word
            start = self.position
            value = []
            while self.char and self.char.isprintable() and not self.char.isspace():
                value.append(self.char)
                self.read()

            return Token(TokenType.WORD, ''.join(value), start)
        else:
            # unknown
            token = Token(TokenType.UNKNOWN, self.char, self.position)
            self.read()
            return token

    def __iter__(self):
        '''
        Utility iterator to allow easy creation of a stream of tokens.
        '''

        while True:
            token = self.token()
            yield token

            if token.ttype == TokenType.EOF: break

class Parser:
    '''
    Parses a stream of tokens into an Abstract Syntax Tree for later execution.

    Args:
        tokens: The stream of tokens.
    '''

    def __init__(self, tokens):
        self.tokens = iter(tokens)
        self.token = None
        self.last = None

        self.next()

    def parse(self):
        '''
        Parse the stream of tokens.

        Returns:
            The root node of the Abstract Syntax Tree.

        Throws:
            May throw a ValueError in the case that the stream of tokens is
            malformed.
        '''

        root = self.commands()
        self.expect(TokenType.EOF)

        return root

    def commands(self):
        base = self.command()
        if self.accept(TokenType.COMMAND_END):
            other = self.commands()
            if base and other:
                return DoubleNode(base, other)
            else:
                return other
        else:
            return base

    def command(self):
        if self.accept(TokenType.WORD):
            command = self.last.lexeme

            args = []
            while self.accept(TokenType.WORD):
                args.append(self.last.lexeme)

            node = CommandNode(command, args)

            redirs = self.redirections()
            if redirs:
                node = RedirectionsNode(node, redirs)

            if self.accept(TokenType.PIPE):
                return PipeNode(node, self.command())
            else:
                return node
        else:
            return None

    def redirections(self):
        redirs = []
        redir = self.redirection()
        while redir:
            redirs.append(redir)
            redir = self.redirection()

        if len(redirs) > 0:
            return RedirectionsHelper(redirs)
        else:
            return None

    def redirection(self):
        # TODO: recognize other types of redirections
        if self.accept(TokenType.REDIRECT_OUT):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(1, (filename, os.O_CREAT | os.O_WRONLY | os.O_TRUNC))
        elif self.accept(TokenType.REDIRECT_APPEND):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(1, (filename, os.O_CREAT | os.O_WRONLY | os.O_APPEND))
        elif self.accept(TokenType.REDIRECT_IN):
            filename = self.expect(TokenType.WORD).lexeme
            return RedirectionHelper(0, (filename, os.O_RDONLY))
        else:
            return None

    def next(self):
        self.last = self.token
        self.token = next(self.tokens, None)
        return self.token

    def accept(self, ttype):
        if self.token and self.token.ttype == ttype:
            self.next()
            return self.last
        else:
            return None

    def expect(self, ttype):
        result = self.accept(ttype)
        if result:
            return result
        else:
            raise ValueError(f'expected token to be {ttype}, instead got {self.token.ttype}')

class Node:
    '''
    A single node in the Abstract Syntax Tree.
    '''

    def execute(self, builtins):
        '''
        Execute the node.

        Args:
            builtins: A dict of builtin commands.
        '''

        pass

    def wait(self):
        '''
        Wait for the execution of the node to finish.
        '''

        pass

class DoubleNode(Node):
    '''
    A node that executes two nodes sequentially.

    Args:
        first: The first node to execute.
        second: The second node to execute.
    '''

    def __init__(self, first, second):
        self.first = first
        self.second = second

    def execute(self, *args):
        self.first.execute(*args)
        self.first.wait()

        self.second.execute(*args)
        self.second.wait()

class PipeNode(DoubleNode):
    '''
    A node that forwards the output of one node to the input of another.

    Args:
        first: The node to pipe the output from.
        second: The node to pipe the input into.
    '''

    def __init__(self, first, second):
        self.first = first
        self.second = second

    def execute(self, builtins):
        read, write = os.pipe()
        inp = RedirectionHelper(0, read)
        outp = RedirectionHelper(1, write)

        # TODO: close inp in child process
        with outp:
            self.first.execute(builtins)
        outp.close()

        with inp:
            self.second.execute(builtins)

    def wait(self):
        self.first.wait()
        self.second.wait()

class CommandNode(Node):
    '''
    A node that contains a single shell command.

    Args:
        command: The name of the executable to run (will be looked up in PATH).
        args: The arguments to be passed to the executable.
    '''

    def __init__(self, command, args):
        self.command = command

        self.args = args
        self.args.insert(0, command)

        self.pid = None

    def execute(self, builtins):
        if self.command in builtins:
            builtins[self.command](*self.args)
        else:
            pid = os.fork()
            if pid == 0:
                # child process
                os.execv(self.full_command, self.args)
            else:
                # parent process
                self.pid = pid

    def wait(self):
        if self.pid:
            os.waitpid(self.pid, 0)

    @property
    def full_command(self):
        if os.path.exists(self.command):
            return self.command

        path = os.environ['PATH'].split(':')
        for di in path:
            cmd = os.path.join(di, self.command)
            if os.path.exists(cmd):
                return cmd

        raise FileNotFoundError('command not found')

class RedirectionsNode(Node):
    '''
    A node that performs a number of IO redirections.

    Args:
        base: The base node to operate on.
        redirections: The redirections to apply.
    '''

    def __init__(self, base, redirections):
        self.base = base
        self.redirections = redirections

    def execute(self, builtins):
        with self.redirections:
            self.base.execute(builtins)

    def wait(self):
        self.base.wait()

class RedirectionHelper:
    '''
    Helps perform a single file redirection.

    Args:
        fd: The file descriptor to modify.
        newfd: The new file descriptor.
    '''

    def __init__(self, fd, newfd):
        self.fd = fd
        self.backup = os.dup(fd)

        try:
            filename, mode = newfd
            self.newfd = os.open(filename, mode)
        except TypeError:
            self.newfd = newfd

    def close(self):
        os.close(self.newfd)

    def __enter__(self):
        os.dup2(self.newfd, self.fd)

    def __exit__(self, type, value, traceback):
        os.dup2(self.backup, self.fd)

class RedirectionsHelper:
    '''
    Helps perform multiple file redirections.

    Args:
        redirections: A list of redirections.
    '''

    def __init__(self, redirections):
        self.redirections = redirections

        self.stack = None

    def __enter__(self):
        if len(self.redirections) > 0:
            self.stack = contextlib.ExitStack()
            for redirection in self.redirections:
                self.stack.enter_context(redirection)

    def __exit__(self, type, value, traceback):
        if self.stack:
            self.stack.close()
            self.stack = None

if __name__ == ""__main__"":
    main()
/n/n/n",1
20,20,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://example.com', response['location'])

    def test_next_param_outside_site(self):
        """"""Test that next parameter cannot be used as an open redirector.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""http://www.mozilla.org/""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertNotEqual('http://www.mozilla.org/', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/napps/users/views.py/n/nimport logging

from django import http
from django.conf import settings
from django.contrib import auth
from django.contrib.auth import views as auth_views
from django.contrib.auth import forms as auth_forms
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext as _
from django.shortcuts import render_to_response, get_object_or_404
from django.template import RequestContext
from django.template.loader import render_to_string

from django_openid_auth import views as openid_views

from users import forms
from users.models import UserProfile
from users.decorators import anonymous_only, login_required
from links.models import Link
from projects.models import Project
from drumbeat import messages
from activity.models import Activity

log = logging.getLogger(__name__)


def render_openid_failure(request, message, status, template_name):
    if request.method == 'POST':
        form = forms.OpenIDForm(request.POST)
    else:
        form = forms.OpenIDForm()
    response = render_to_string(template_name, {
        'message': message,
        'form': form,
    }, context_instance=RequestContext(request))
    return http.HttpResponse(response, status=status)


def render_openid_registration_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/register_openid.html')


def render_openid_login_failure(request, message, status=403):
    return render_openid_failure(
        request, message, status, 'users/login_openid.html')


def _clean_next_url(request):
    """"""Taken from zamboni. Prevent us from redirecting outside of drumbeat.""""""
    gets = request.GET.copy()
    url = gets['next']
    if url and '://' in url:
        url = None
    gets['next'] = url
    request.GET = gets
    return request


@anonymous_only
def login(request):
    """"""Log the user in. Lifted most of this code from zamboni.""""""

    if 'next' in request.GET:
        request = _clean_next_url(request)
        request.session['next'] = request.GET['next']

    logout(request)

    r = auth_views.login(request, template_name='users/signin.html',
                         authentication_form=forms.AuthenticationForm)

    if isinstance(r, http.HttpResponseRedirect):
        # Succsesful log in according to django.  Now we do our checks.  I do
        # the checks here instead of the form's clean() because I want to use
        # the messages framework and it's not available in the request there
        user = request.user.get_profile()

        if user.confirmation_code:
            logout(request)
            log.info(u'Attempt to log in with unconfirmed account (%s)' % user)
            msg1 = _(('A link to activate your user account was sent by email '
                      'to your address {0}. You have to click it before you '
                      'can log in.').format(user.email))
            url = request.build_absolute_uri(
                reverse('users_confirm_resend',
                        kwargs=dict(username=user.username)))
            msg2 = _(('If you did not receive the confirmation email, make '
                      'sure your email service did not mark it as ""junk '
                      'mail"" or ""spam"". If you need to, you can have us '
                      '<a href=""%s"">resend the confirmation message</a> '
                      'to your email address mentioned above.') % url)
            messages.error(request, msg1)
            messages.info(request, msg2, safe=True)
            return render_to_response('users/signin.html', {
                'form': auth_forms.AuthenticationForm(),
            }, context_instance=RequestContext(request))

        if request.POST.get('remember_me', None):
            request.session.set_expiry(settings.SESSION_COOKIE_AGE)
            log.debug(u'User signed in with remember_me option')

        next_param = request.session.get('next', None)
        if next_param:
            del request.session['next']
            return http.HttpResponseRedirect(next_param)

    elif request.method == 'POST':
        messages.error(request, _('Incorrect email or password.'))
        data = request.POST.copy()
        del data['password']
        return render_to_response('users/signin.html', {
            'form': auth_forms.AuthenticationForm(initial=data),
        }, context_instance=RequestContext(request))

    return r


@anonymous_only
def login_openid(request):
    if request.method == 'POST':
        return openid_views.login_begin(
            request,
            template_name='users/login_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_login_openid_complete')
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/login_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def login_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', False)
    return openid_views.login_complete(
        request, render_failure=render_openid_login_failure)


@login_required(profile_required=False)
def logout(request):
    """"""Destroy user session.""""""
    auth.logout(request)
    return http.HttpResponseRedirect(reverse('dashboard_index'))


@anonymous_only
def register(request):
    """"""Present user registration form and handle registrations.""""""
    if request.method == 'POST':
        form = forms.RegisterForm(data=request.POST)

        if form.is_valid():
            user = form.save(commit=False)
            user.set_password(form.cleaned_data['password'])
            user.generate_confirmation_code()
            user.save()
            user.create_django_user()

            log.info(u""Registered new account for user (%s)"", user)

            messages.success(request, _('Congratulations! Your user account '
                                        'was successfully created.'))
            path = reverse('users_confirm_registration', kwargs={
                'username': user.username,
                'token': user.confirmation_code,
            })
            url = request.build_absolute_uri(path)
            user.email_confirmation_code(url)
            msg = _('Thanks! We have sent an email to {0} with '
                    'instructions for completing your '
                    'registration.').format(user.email)
            messages.info(request, msg)

            return http.HttpResponseRedirect(reverse('dashboard_index'))
        else:
            messages.error(request, _('There are errors in this form. Please '
                                      'correct them and resubmit.'))
    else:
        form = forms.RegisterForm()
    return render_to_response('users/register.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid(request):
    if request.method == 'POST':
        r = openid_views.login_begin(
            request,
            template_name='users/register_openid.html',
            form_class=forms.OpenIDForm,
            login_complete_view='users_register_openid_complete')
        return r
    else:
        form = forms.OpenIDForm()
    return render_to_response('users/register_openid.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@anonymous_only
def register_openid_complete(request):
    setattr(settings, 'OPENID_CREATE_USERS', True)
    return openid_views.login_complete(
        request, render_failure=render_openid_registration_failure)


def user_list(request):
    """"""Display a list of users on the site. Featured, new and active.""""""
    featured = UserProfile.objects.filter(featured=True)
    new = UserProfile.objects.all().order_by('-created_on')[:4]
    popular = UserProfile.objects.get_popular(limit=8)
    return render_to_response('users/user_list.html', {
        'featured': featured,
        'new': new,
        'popular': popular,
    }, context_instance=RequestContext(request))


@anonymous_only
def confirm_registration(request, token, username):
    """"""Confirm a users registration.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code != token:
        messages.error(
            request,
           _('Hmm, that doesn\'t look like the correct confirmation code'))
        log.info('Account confirmation failed for %s' % (profile,))
        return http.HttpResponseRedirect(reverse('users_login'))
    profile.confirmation_code = ''
    profile.save()
    messages.success(request, 'Success! You have verified your account. '
                     'You may now sign in.')
    return http.HttpResponseRedirect(reverse('users_login'))


@anonymous_only
def confirm_resend(request, username):
    """"""Resend a confirmation code.""""""
    profile = get_object_or_404(UserProfile, username=username)
    if profile.confirmation_code:
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        msg = _('A confirmation code has been sent to the email address '
                'associated with your account.')
        messages.info(request, msg)
    return http.HttpResponseRedirect(reverse('users_login'))


def profile_view(request, username):
    profile = get_object_or_404(UserProfile, username=username)
    following = profile.following()
    projects = profile.following(model=Project)
    followers = profile.followers()
    links = Link.objects.select_related('subscription').filter(user=profile)
    activities = Activity.objects.select_related(
        'actor', 'status', 'project').filter(
        actor=profile).order_by('-created_on')[0:25]
    return render_to_response('users/profile.html', {
        'profile': profile,
        'following': following,
        'followers': followers,
        'projects': projects,
        'skills': profile.tags.filter(category='skill'),
        'interests': profile.tags.filter(category='interest'),
        'links': links,
        'activities': activities,
    }, context_instance=RequestContext(request))


@login_required(profile_required=False)
def profile_create(request):
    if request.method != 'POST':
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    try:
        request.user.get_profile()
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    except UserProfile.DoesNotExist:
        pass
    form = forms.CreateProfileForm(request.POST)
    if form.is_valid():
        profile = form.save(commit=False)
        profile.user = request.user
        profile.confirmation_code = profile.generate_confirmation_code()
        profile.save()
        path = reverse('users_confirm_registration', kwargs={
            'username': profile.username,
            'token': profile.confirmation_code,
        })
        url = request.build_absolute_uri(path)
        profile.email_confirmation_code(url)
        auth.logout(request)
        msg = _('Thanks! We have sent an email to {0} with '
                'instructions for completing your '
                'registration.').format(profile.email)
        messages.info(request, msg)
        return http.HttpResponseRedirect(reverse('dashboard_index'))
    return render_to_response('dashboard/setup_profile.html', {
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileEditForm(request.POST, request.FILES,
                                     instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': profile.username,
            }))
        else:
            messages.error(request, _('There were problems updating your '
                                      'profile. Please correct the problems '
                                      'and submit again.'))
    else:
        form = forms.ProfileEditForm(instance=profile)

    return render_to_response('users/profile_edit_main.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_image(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileImageForm(request.POST, request.FILES,
                                      instance=profile)
        if form.is_valid():
            messages.success(request, _('Profile image updated'))
            form.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_edit_image'))
        else:
            messages.error(request, _('There was an error uploading '
                                      'your image.'))
    else:
        form = forms.ProfileImageForm(instance=profile)
    return render_to_response('users/profile_edit_image.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links(request):
    profile = get_object_or_404(UserProfile, user=request.user)
    if request.method == 'POST':
        form = forms.ProfileLinksForm(request.POST)
        if form.is_valid():
            messages.success(request, _('Profile link added.'))
            link = form.save(commit=False)
            log.debug(""User instance: %s"" % (profile.user,))
            link.user = profile
            link.save()
            return http.HttpResponseRedirect(
                reverse('users_profile_view', kwargs={
                    'username': request.user.get_profile().username,
                }),
            )
        else:
            messages.error(request, _('There was an error saving '
                                      'your link.'))
    else:
        form = forms.ProfileLinksForm()
    links = Link.objects.select_related('subscription').filter(user=profile)
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
        'links': links,
    }, context_instance=RequestContext(request))


@login_required
def profile_edit_links_delete(request, link):
    profile = get_object_or_404(UserProfile, user=request.user)
    link = get_object_or_404(Link, pk=link)
    if link.user != profile:
        return http.HttpResponseForbidden()
    link.delete()
    messages.success(request, _('The link was deleted.'))
    form = forms.ProfileLinksForm()
    return render_to_response('users/profile_edit_links.html', {
        'profile': profile,
        'form': form,
    }, context_instance=RequestContext(request))


def check_username(request):
    username = request.GET.get('username', None)
    if not username:
        return http.HttpResponse(status=404)
    try:
        UserProfile.objects.get(username=username)
        return http.HttpResponse()
    except UserProfile.DoesNotExist:
        return http.HttpResponse(status=404)
/n/n/n",0
21,21,91fedc0b7a23ad5c7a92101e45bac7100ab67b49,"/apps/users/tests.py/n/nfrom django.test import Client

from drumbeat.utils import get_partition_id
from users.models import UserProfile

from test_utils import TestCase


class TestLogins(TestCase):

    test_username = 'testuser'
    test_password = 'testpassword'
    test_email = 'test@mozillafoundation.org'

    def setUp(self):
        self.locale = 'en-US'
        self.client = Client()
        self.user = UserProfile(username=self.test_username,
                                email=self.test_email)
        self.user.set_password(self.test_password)
        self.user.save()
        self.user.create_django_user()

    def test_authenticated_redirects(self):
        """"""Test that authenticated users are redirected in specific views.""""""
        self.client.login(username=self.test_username,
                          password=self.test_password)
        paths = ('login/', 'register/',
                 'confirm/123456/username/',
                 'confirm/resend/username/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            print response
            self.assertRedirects(response, '/', status_code=302,
                                 target_status_code=301)
        self.client.logout()

    def test_unauthenticated_redirects(self):
        """"""Test that anonymous users are redirected for specific views.""""""
        paths = ('logout/', 'profile/edit/', 'profile/edit/image/')
        for path in paths:
            full = ""/%s/%s"" % (self.locale, path)
            response = self.client.get(full)
            expected = ""/%s/"" % (self.locale,)
            self.assertRedirects(response, expected, status_code=302,
                                 target_status_code=200)

    def test_login_post(self):
        """"""Test logging in.""""""
        path = ""/%s/login/"" % (self.locale,)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertRedirects(response, '/', status_code=302,
                             target_status_code=301)
        # TODO - Improve this so it doesn't take so many redirects to get a 200
        response2 = self.client.get(response[""location""])
        response3 = self.client.get(response2[""location""])
        response4 = self.client.get(response3[""location""])
        self.assertContains(response4, 'id=""dashboard""')
        self.client.logout()

        response5 = self.client.post(path, {
            'username': 'nonexistant',
            'password': 'password',
        })
        self.assertContains(response5, 'id=""id_username""')

    def test_login_next_param(self):
        """"""Test that user is redirected properly after logging in.""""""
        path = ""/%s/login/?next=/%s/profile/edit/"" % (self.locale, self.locale)
        response = self.client.post(path, {
            'username': self.test_username,
            'password': self.test_password,
        })
        self.assertEqual(
            ""http://testserver/%s/profile/edit/"" % (self.locale,),
            response[""location""],
        )

    def test_login_next_param_header_injection(self):
        """"""Test that we can't inject headers into response with next param.""""""
        path = ""/%s/login/"" % (self.locale,)
        next_param = ""foo\r\nLocation: http://example.com""
        response = self.client.post(path + ""?next=%s"" % (next_param), {
            'username': self.test_username,
            'password': self.test_password,
        })
        # we expect the header to be urlencoded before being sent.
        self.assertTrue('login/foo%0D%0ALocation' in response['location'])
        self.assertNotEqual('http://example.com', response['location'])

    def test_registration_opt_in(self):
        """"""Test account registration.""""""
        path = ""/%s/register/"" % (self.locale,)
        params = {
            'display_name': 'Joe User',
            'username': 'joeuser',
            'password': 'abcdefghijklmno1',
            'password_confirm': 'abcdefghijklmno1',
            'email': 'joe@mozilla.com',
        }
        response = self.client.post(path, params)
        self.assertContains(response, 'You must agree to the licensing terms')
        params['policy_optin'] = 'on'
        response = self.client.post(path, params)
        self.assertEqual(response.status_code, 302)

    def test_profile_image_directories(self):
        """"""Test that we partition image directories properly.""""""
        for i in range(1, 1001):
            p_id = get_partition_id(i)
            self.assertEqual(1, p_id)
        for i in range(1001, 2001):
            p_id = get_partition_id(i)
            self.assertEqual(2, p_id)
        for i in range(10001, 11001):
            p_id = get_partition_id(i)
            self.assertEqual(11, p_id)
        self.assertEqual(12, get_partition_id(11002))
/n/n/n",1
60,60,ef527f84abf0cee7ac6ad832828ff92311440ee4,"python/ray/experimental/state.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from collections import defaultdict
import json
import redis
import sys
import time

import ray
from ray.function_manager import FunctionDescriptor
import ray.gcs_utils
import ray.ray_constants as ray_constants
from ray.utils import (decode, binary_to_object_id, binary_to_hex,
                       hex_to_binary)


def parse_client_table(redis_client):
    """"""Read the client table.

    Args:
        redis_client: A client to the primary Redis shard.

    Returns:
        A list of information about the nodes in the cluster.
    """"""
    NIL_CLIENT_ID = ray.ObjectID.nil().binary()
    message = redis_client.execute_command(""RAY.TABLE_LOOKUP"",
                                           ray.gcs_utils.TablePrefix.CLIENT,
                                           """", NIL_CLIENT_ID)

    # Handle the case where no clients are returned. This should only
    # occur potentially immediately after the cluster is started.
    if message is None:
        return []

    node_info = {}
    gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(message, 0)

    ordered_client_ids = []

    # Since GCS entries are append-only, we override so that
    # only the latest entries are kept.
    for i in range(gcs_entry.EntriesLength()):
        client = (ray.gcs_utils.ClientTableData.GetRootAsClientTableData(
            gcs_entry.Entries(i), 0))

        resources = {
            decode(client.ResourcesTotalLabel(i)):
            client.ResourcesTotalCapacity(i)
            for i in range(client.ResourcesTotalLabelLength())
        }
        client_id = ray.utils.binary_to_hex(client.ClientId())

        # If this client is being removed, then it must
        # have previously been inserted, and
        # it cannot have previously been removed.
        if not client.IsInsertion():
            assert client_id in node_info, ""Client removed not found!""
            assert node_info[client_id][""IsInsertion""], (
                ""Unexpected duplicate removal of client."")
        else:
            ordered_client_ids.append(client_id)

        node_info[client_id] = {
            ""ClientID"": client_id,
            ""IsInsertion"": client.IsInsertion(),
            ""NodeManagerAddress"": decode(
                client.NodeManagerAddress(), allow_none=True),
            ""NodeManagerPort"": client.NodeManagerPort(),
            ""ObjectManagerPort"": client.ObjectManagerPort(),
            ""ObjectStoreSocketName"": decode(
                client.ObjectStoreSocketName(), allow_none=True),
            ""RayletSocketName"": decode(
                client.RayletSocketName(), allow_none=True),
            ""Resources"": resources
        }
    # NOTE: We return the list comprehension below instead of simply doing
    # 'list(node_info.values())' in order to have the nodes appear in the order
    # that they joined the cluster. Python dictionaries do not preserve
    # insertion order. We could use an OrderedDict, but then we'd have to be
    # sure to only insert a given node a single time (clients that die appear
    # twice in the GCS log).
    return [node_info[client_id] for client_id in ordered_client_ids]


class GlobalState(object):
    """"""A class used to interface with the Ray control state.

    # TODO(zongheng): In the future move this to use Ray's redis module in the
    # backend to cut down on # of request RPCs.

    Attributes:
        redis_client: The Redis client used to query the primary redis server.
        redis_clients: Redis clients for each of the Redis shards.
    """"""

    def __init__(self):
        """"""Create a GlobalState object.""""""
        # The redis server storing metadata, such as function table, client
        # table, log files, event logs, workers/actions info.
        self.redis_client = None
        # Clients for the redis shards, storing the object table & task table.
        self.redis_clients = None

    def _check_connected(self):
        """"""Check that the object has been initialized before it is used.

        Raises:
            Exception: An exception is raised if ray.init() has not been called
                yet.
        """"""
        if self.redis_client is None:
            raise Exception(""The ray.global_state API cannot be used before ""
                            ""ray.init has been called."")

        if self.redis_clients is None:
            raise Exception(""The ray.global_state API cannot be used before ""
                            ""ray.init has been called."")

    def _initialize_global_state(self,
                                 redis_ip_address,
                                 redis_port,
                                 redis_password=None,
                                 timeout=20):
        """"""Initialize the GlobalState object by connecting to Redis.

        It's possible that certain keys in Redis may not have been fully
        populated yet. In this case, we will retry this method until they have
        been populated or we exceed a timeout.

        Args:
            redis_ip_address: The IP address of the node that the Redis server
                lives on.
            redis_port: The port that the Redis server is listening on.
            redis_password: The password of the redis server.
        """"""
        self.redis_client = redis.StrictRedis(
            host=redis_ip_address, port=redis_port, password=redis_password)

        start_time = time.time()

        num_redis_shards = None
        ip_address_ports = []

        while time.time() - start_time < timeout:
            # Attempt to get the number of Redis shards.
            num_redis_shards = self.redis_client.get(""NumRedisShards"")
            if num_redis_shards is None:
                print(""Waiting longer for NumRedisShards to be populated."")
                time.sleep(1)
                continue
            num_redis_shards = int(num_redis_shards)
            if num_redis_shards < 1:
                raise Exception(""Expected at least one Redis shard, found ""
                                ""{}."".format(num_redis_shards))

            # Attempt to get all of the Redis shards.
            ip_address_ports = self.redis_client.lrange(
                ""RedisShards"", start=0, end=-1)
            if len(ip_address_ports) != num_redis_shards:
                print(""Waiting longer for RedisShards to be populated."")
                time.sleep(1)
                continue

            # If we got here then we successfully got all of the information.
            break

        # Check to see if we timed out.
        if time.time() - start_time >= timeout:
            raise Exception(""Timed out while attempting to initialize the ""
                            ""global state. num_redis_shards = {}, ""
                            ""ip_address_ports = {}"".format(
                                num_redis_shards, ip_address_ports))

        # Get the rest of the information.
        self.redis_clients = []
        for ip_address_port in ip_address_ports:
            shard_address, shard_port = ip_address_port.split(b"":"")
            self.redis_clients.append(
                redis.StrictRedis(
                    host=shard_address,
                    port=shard_port,
                    password=redis_password))

    def _execute_command(self, key, *args):
        """"""Execute a Redis command on the appropriate Redis shard based on key.

        Args:
            key: The object ID or the task ID that the query is about.
            args: The command to run.

        Returns:
            The value returned by the Redis command.
        """"""
        client = self.redis_clients[key.redis_shard_hash() % len(
            self.redis_clients)]
        return client.execute_command(*args)

    def _keys(self, pattern):
        """"""Execute the KEYS command on all Redis shards.

        Args:
            pattern: The KEYS pattern to query.

        Returns:
            The concatenated list of results from all shards.
        """"""
        result = []
        for client in self.redis_clients:
            result.extend(list(client.scan_iter(match=pattern)))
        return result

    def _object_table(self, object_id):
        """"""Fetch and parse the object table information for a single object ID.

        Args:
            object_id: An object ID to get information about.

        Returns:
            A dictionary with information about the object ID in question.
        """"""
        # Allow the argument to be either an ObjectID or a hex string.
        if not isinstance(object_id, ray.ObjectID):
            object_id = ray.ObjectID(hex_to_binary(object_id))

        # Return information about a single object ID.
        message = self._execute_command(object_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.OBJECT, """",
                                        object_id.binary())
        if message is None:
            return {}
        gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        assert gcs_entry.EntriesLength() > 0

        entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(
            gcs_entry.Entries(0), 0)

        object_info = {
            ""DataSize"": entry.ObjectSize(),
            ""Manager"": entry.Manager(),
            ""IsEviction"": [entry.IsEviction()],
        }

        for i in range(1, gcs_entry.EntriesLength()):
            entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(
                gcs_entry.Entries(i), 0)
            object_info[""IsEviction""].append(entry.IsEviction())

        return object_info

    def object_table(self, object_id=None):
        """"""Fetch and parse the object table info for one or more object IDs.

        Args:
            object_id: An object ID to fetch information about. If this is
                None, then the entire object table is fetched.

        Returns:
            Information from the object table.
        """"""
        self._check_connected()
        if object_id is not None:
            # Return information about a single object ID.
            return self._object_table(object_id)
        else:
            # Return the entire object table.
            object_keys = self._keys(ray.gcs_utils.TablePrefix_OBJECT_string +
                                     ""*"")
            object_ids_binary = {
                key[len(ray.gcs_utils.TablePrefix_OBJECT_string):]
                for key in object_keys
            }

            results = {}
            for object_id_binary in object_ids_binary:
                results[binary_to_object_id(object_id_binary)] = (
                    self._object_table(binary_to_object_id(object_id_binary)))
            return results

    def _task_table(self, task_id):
        """"""Fetch and parse the task table information for a single task ID.

        Args:
            task_id: A task ID to get information about.

        Returns:
            A dictionary with information about the task ID in question.
        """"""
        assert isinstance(task_id, ray.TaskID)
        message = self._execute_command(task_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.RAYLET_TASK,
                                        """", task_id.binary())
        if message is None:
            return {}
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        assert gcs_entries.EntriesLength() == 1

        task_table_message = ray.gcs_utils.Task.GetRootAsTask(
            gcs_entries.Entries(0), 0)

        execution_spec = task_table_message.TaskExecutionSpec()
        task_spec = task_table_message.TaskSpecification()
        task = ray._raylet.Task.from_string(task_spec)
        function_descriptor_list = task.function_descriptor_list()
        function_descriptor = FunctionDescriptor.from_bytes_list(
            function_descriptor_list)
        task_spec_info = {
            ""DriverID"": task.driver_id().hex(),
            ""TaskID"": task.task_id().hex(),
            ""ParentTaskID"": task.parent_task_id().hex(),
            ""ParentCounter"": task.parent_counter(),
            ""ActorID"": (task.actor_id().hex()),
            ""ActorCreationID"": task.actor_creation_id().hex(),
            ""ActorCreationDummyObjectID"": (
                task.actor_creation_dummy_object_id().hex()),
            ""ActorCounter"": task.actor_counter(),
            ""Args"": task.arguments(),
            ""ReturnObjectIDs"": task.returns(),
            ""RequiredResources"": task.required_resources(),
            ""FunctionID"": function_descriptor.function_id.hex(),
            ""FunctionHash"": binary_to_hex(function_descriptor.function_hash),
            ""ModuleName"": function_descriptor.module_name,
            ""ClassName"": function_descriptor.class_name,
            ""FunctionName"": function_descriptor.function_name,
        }

        return {
            ""ExecutionSpec"": {
                ""Dependencies"": [
                    execution_spec.Dependencies(i)
                    for i in range(execution_spec.DependenciesLength())
                ],
                ""LastTimestamp"": execution_spec.LastTimestamp(),
                ""NumForwards"": execution_spec.NumForwards()
            },
            ""TaskSpec"": task_spec_info
        }

    def task_table(self, task_id=None):
        """"""Fetch and parse the task table information for one or more task IDs.

        Args:
            task_id: A hex string of the task ID to fetch information about. If
                this is None, then the task object table is fetched.

        Returns:
            Information from the task table.
        """"""
        self._check_connected()
        if task_id is not None:
            task_id = ray.TaskID(hex_to_binary(task_id))
            return self._task_table(task_id)
        else:
            task_table_keys = self._keys(
                ray.gcs_utils.TablePrefix_RAYLET_TASK_string + ""*"")
            task_ids_binary = [
                key[len(ray.gcs_utils.TablePrefix_RAYLET_TASK_string):]
                for key in task_table_keys
            ]

            results = {}
            for task_id_binary in task_ids_binary:
                results[binary_to_hex(task_id_binary)] = self._task_table(
                    ray.TaskID(task_id_binary))
            return results

    def function_table(self, function_id=None):
        """"""Fetch and parse the function table.

        Returns:
            A dictionary that maps function IDs to information about the
                function.
        """"""
        self._check_connected()
        function_table_keys = self.redis_client.keys(
            ray.gcs_utils.FUNCTION_PREFIX + ""*"")
        results = {}
        for key in function_table_keys:
            info = self.redis_client.hgetall(key)
            function_info_parsed = {
                ""DriverID"": binary_to_hex(info[b""driver_id""]),
                ""Module"": decode(info[b""module""]),
                ""Name"": decode(info[b""name""])
            }
            results[binary_to_hex(info[b""function_id""])] = function_info_parsed
        return results

    def client_table(self):
        """"""Fetch and parse the Redis DB client table.

        Returns:
            Information about the Ray clients in the cluster.
        """"""
        self._check_connected()

        return parse_client_table(self.redis_client)

    def _profile_table(self, batch_id):
        """"""Get the profile events for a given batch of profile events.

        Args:
            batch_id: An identifier for a batch of profile events.

        Returns:
            A list of the profile events for the specified batch.
        """"""
        # TODO(rkn): This method should support limiting the number of log
        # events and should also support returning a window of events.
        message = self._execute_command(batch_id, ""RAY.TABLE_LOOKUP"",
                                        ray.gcs_utils.TablePrefix.PROFILE, """",
                                        batch_id.binary())

        if message is None:
            return []

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)

        profile_events = []
        for i in range(gcs_entries.EntriesLength()):
            profile_table_message = (
                ray.gcs_utils.ProfileTableData.GetRootAsProfileTableData(
                    gcs_entries.Entries(i), 0))

            component_type = decode(profile_table_message.ComponentType())
            component_id = binary_to_hex(profile_table_message.ComponentId())
            node_ip_address = decode(
                profile_table_message.NodeIpAddress(), allow_none=True)

            for j in range(profile_table_message.ProfileEventsLength()):
                profile_event_message = profile_table_message.ProfileEvents(j)

                profile_event = {
                    ""event_type"": decode(profile_event_message.EventType()),
                    ""component_id"": component_id,
                    ""node_ip_address"": node_ip_address,
                    ""component_type"": component_type,
                    ""start_time"": profile_event_message.StartTime(),
                    ""end_time"": profile_event_message.EndTime(),
                    ""extra_data"": json.loads(
                        decode(profile_event_message.ExtraData())),
                }

                profile_events.append(profile_event)

        return profile_events

    def profile_table(self):
        profile_table_keys = self._keys(
            ray.gcs_utils.TablePrefix_PROFILE_string + ""*"")
        batch_identifiers_binary = [
            key[len(ray.gcs_utils.TablePrefix_PROFILE_string):]
            for key in profile_table_keys
        ]

        result = defaultdict(list)
        for batch_id in batch_identifiers_binary:
            profile_data = self._profile_table(binary_to_object_id(batch_id))
            # Note that if keys are being evicted from Redis, then it is
            # possible that the batch will be evicted before we get it.
            if len(profile_data) > 0:
                component_id = profile_data[0][""component_id""]
                result[component_id].extend(profile_data)

        return dict(result)

    def _seconds_to_microseconds(self, time_in_seconds):
        """"""A helper function for converting seconds to microseconds.""""""
        time_in_microseconds = 10**6 * time_in_seconds
        return time_in_microseconds

    # Colors are specified at
    # https://github.com/catapult-project/catapult/blob/master/tracing/tracing/base/color_scheme.html.  # noqa: E501
    _default_color_mapping = defaultdict(
        lambda: ""generic_work"", {
            ""worker_idle"": ""cq_build_abandoned"",
            ""task"": ""rail_response"",
            ""task:deserialize_arguments"": ""rail_load"",
            ""task:execute"": ""rail_animation"",
            ""task:store_outputs"": ""rail_idle"",
            ""wait_for_function"": ""detailed_memory_dump"",
            ""ray.get"": ""good"",
            ""ray.put"": ""terrible"",
            ""ray.wait"": ""vsync_highlight_color"",
            ""submit_task"": ""background_memory_dump"",
            ""fetch_and_run_function"": ""detailed_memory_dump"",
            ""register_remote_function"": ""detailed_memory_dump"",
        })

    # These colors are for use in Chrome tracing.
    _chrome_tracing_colors = [
        ""thread_state_uninterruptible"",
        ""thread_state_iowait"",
        ""thread_state_running"",
        ""thread_state_runnable"",
        ""thread_state_sleeping"",
        ""thread_state_unknown"",
        ""background_memory_dump"",
        ""light_memory_dump"",
        ""detailed_memory_dump"",
        ""vsync_highlight_color"",
        ""generic_work"",
        ""good"",
        ""bad"",
        ""terrible"",
        # ""black"",
        # ""grey"",
        # ""white"",
        ""yellow"",
        ""olive"",
        ""rail_response"",
        ""rail_animation"",
        ""rail_idle"",
        ""rail_load"",
        ""startup"",
        ""heap_dump_stack_frame"",
        ""heap_dump_object_type"",
        ""heap_dump_child_node_arrow"",
        ""cq_build_running"",
        ""cq_build_passed"",
        ""cq_build_failed"",
        ""cq_build_abandoned"",
        ""cq_build_attempt_runnig"",
        ""cq_build_attempt_passed"",
        ""cq_build_attempt_failed"",
    ]

    def chrome_tracing_dump(self, filename=None):
        """"""Return a list of profiling events that can viewed as a timeline.

        To view this information as a timeline, simply dump it as a json file
        by passing in ""filename"" or using using json.dump, and then load go to
        chrome://tracing in the Chrome web browser and load the dumped file.
        Make sure to enable ""Flow events"" in the ""View Options"" menu.

        Args:
            filename: If a filename is provided, the timeline is dumped to that
                file.

        Returns:
            If filename is not provided, this returns a list of profiling
                events. Each profile event is a dictionary.
        """"""
        # TODO(rkn): Support including the task specification data in the
        # timeline.
        # TODO(rkn): This should support viewing just a window of time or a
        # limited number of events.

        profile_table = self.profile_table()
        all_events = []

        for component_id_hex, component_events in profile_table.items():
            # Only consider workers and drivers.
            component_type = component_events[0][""component_type""]
            if component_type not in [""worker"", ""driver""]:
                continue

            for event in component_events:
                new_event = {
                    # The category of the event.
                    ""cat"": event[""event_type""],
                    # The string displayed on the event.
                    ""name"": event[""event_type""],
                    # The identifier for the group of rows that the event
                    # appears in.
                    ""pid"": event[""node_ip_address""],
                    # The identifier for the row that the event appears in.
                    ""tid"": event[""component_type""] + "":"" +
                    event[""component_id""],
                    # The start time in microseconds.
                    ""ts"": self._seconds_to_microseconds(event[""start_time""]),
                    # The duration in microseconds.
                    ""dur"": self._seconds_to_microseconds(event[""end_time""] -
                                                         event[""start_time""]),
                    # What is this?
                    ""ph"": ""X"",
                    # This is the name of the color to display the box in.
                    ""cname"": self._default_color_mapping[event[""event_type""]],
                    # The extra user-defined data.
                    ""args"": event[""extra_data""],
                }

                # Modify the json with the additional user-defined extra data.
                # This can be used to add fields or override existing fields.
                if ""cname"" in event[""extra_data""]:
                    new_event[""cname""] = event[""extra_data""][""cname""]
                if ""name"" in event[""extra_data""]:
                    new_event[""name""] = event[""extra_data""][""name""]

                all_events.append(new_event)

        if filename is not None:
            with open(filename, ""w"") as outfile:
                json.dump(all_events, outfile)
        else:
            return all_events

    def chrome_tracing_object_transfer_dump(self, filename=None):
        """"""Return a list of transfer events that can viewed as a timeline.

        To view this information as a timeline, simply dump it as a json file
        by passing in ""filename"" or using using json.dump, and then load go to
        chrome://tracing in the Chrome web browser and load the dumped file.
        Make sure to enable ""Flow events"" in the ""View Options"" menu.

        Args:
            filename: If a filename is provided, the timeline is dumped to that
                file.

        Returns:
            If filename is not provided, this returns a list of profiling
                events. Each profile event is a dictionary.
        """"""
        client_id_to_address = {}
        for client_info in ray.global_state.client_table():
            client_id_to_address[client_info[""ClientID""]] = ""{}:{}"".format(
                client_info[""NodeManagerAddress""],
                client_info[""ObjectManagerPort""])

        all_events = []

        for key, items in self.profile_table().items():
            # Only consider object manager events.
            if items[0][""component_type""] != ""object_manager"":
                continue

            for event in items:
                if event[""event_type""] == ""transfer_send"":
                    object_id, remote_client_id, _, _ = event[""extra_data""]

                elif event[""event_type""] == ""transfer_receive"":
                    object_id, remote_client_id, _, _ = event[""extra_data""]

                elif event[""event_type""] == ""receive_pull_request"":
                    object_id, remote_client_id = event[""extra_data""]

                else:
                    assert False, ""This should be unreachable.""

                # Choose a color by reading the first couple of hex digits of
                # the object ID as an integer and turning that into a color.
                object_id_int = int(object_id[:2], 16)
                color = self._chrome_tracing_colors[object_id_int % len(
                    self._chrome_tracing_colors)]

                new_event = {
                    # The category of the event.
                    ""cat"": event[""event_type""],
                    # The string displayed on the event.
                    ""name"": event[""event_type""],
                    # The identifier for the group of rows that the event
                    # appears in.
                    ""pid"": client_id_to_address[key],
                    # The identifier for the row that the event appears in.
                    ""tid"": client_id_to_address[remote_client_id],
                    # The start time in microseconds.
                    ""ts"": self._seconds_to_microseconds(event[""start_time""]),
                    # The duration in microseconds.
                    ""dur"": self._seconds_to_microseconds(event[""end_time""] -
                                                         event[""start_time""]),
                    # What is this?
                    ""ph"": ""X"",
                    # This is the name of the color to display the box in.
                    ""cname"": color,
                    # The extra user-defined data.
                    ""args"": event[""extra_data""],
                }
                all_events.append(new_event)

                # Add another box with a color indicating whether it was a send
                # or a receive event.
                if event[""event_type""] == ""transfer_send"":
                    additional_event = new_event.copy()
                    additional_event[""cname""] = ""black""
                    all_events.append(additional_event)
                elif event[""event_type""] == ""transfer_receive"":
                    additional_event = new_event.copy()
                    additional_event[""cname""] = ""grey""
                    all_events.append(additional_event)
                else:
                    pass

        if filename is not None:
            with open(filename, ""w"") as outfile:
                json.dump(all_events, outfile)
        else:
            return all_events

    def workers(self):
        """"""Get a dictionary mapping worker ID to worker information.""""""
        worker_keys = self.redis_client.keys(""Worker*"")
        workers_data = {}

        for worker_key in worker_keys:
            worker_info = self.redis_client.hgetall(worker_key)
            worker_id = binary_to_hex(worker_key[len(""Workers:""):])

            workers_data[worker_id] = {
                ""node_ip_address"": decode(worker_info[b""node_ip_address""]),
                ""plasma_store_socket"": decode(
                    worker_info[b""plasma_store_socket""])
            }
            if b""stderr_file"" in worker_info:
                workers_data[worker_id][""stderr_file""] = decode(
                    worker_info[b""stderr_file""])
            if b""stdout_file"" in worker_info:
                workers_data[worker_id][""stdout_file""] = decode(
                    worker_info[b""stdout_file""])
        return workers_data

    def actors(self):
        actor_keys = self.redis_client.keys(""Actor:*"")
        actor_info = {}
        for key in actor_keys:
            info = self.redis_client.hgetall(key)
            actor_id = key[len(""Actor:""):]
            assert len(actor_id) == ray_constants.ID_SIZE
            actor_info[binary_to_hex(actor_id)] = {
                ""class_id"": binary_to_hex(info[b""class_id""]),
                ""driver_id"": binary_to_hex(info[b""driver_id""]),
                ""local_scheduler_id"": binary_to_hex(
                    info[b""local_scheduler_id""]),
                ""num_gpus"": int(info[b""num_gpus""]),
                ""removed"": decode(info[b""removed""]) == ""True""
            }
        return actor_info

    def _job_length(self):
        event_log_sets = self.redis_client.keys(""event_log*"")
        overall_smallest = sys.maxsize
        overall_largest = 0
        num_tasks = 0
        for event_log_set in event_log_sets:
            fwd_range = self.redis_client.zrange(
                event_log_set, start=0, end=0, withscores=True)
            overall_smallest = min(overall_smallest, fwd_range[0][1])

            rev_range = self.redis_client.zrevrange(
                event_log_set, start=0, end=0, withscores=True)
            overall_largest = max(overall_largest, rev_range[0][1])

            num_tasks += self.redis_client.zcount(
                event_log_set, min=0, max=time.time())
        if num_tasks == 0:
            return 0, 0, 0
        return overall_smallest, overall_largest, num_tasks

    def cluster_resources(self):
        """"""Get the current total cluster resources.

        Note that this information can grow stale as nodes are added to or
        removed from the cluster.

        Returns:
            A dictionary mapping resource name to the total quantity of that
                resource in the cluster.
        """"""
        resources = defaultdict(int)
        clients = self.client_table()
        for client in clients:
            # Only count resources from live clients.
            if client[""IsInsertion""]:
                for key, value in client[""Resources""].items():
                    resources[key] += value

        return dict(resources)

    def _live_client_ids(self):
        """"""Returns a set of client IDs corresponding to clients still alive.""""""
        return {
            client[""ClientID""]
            for client in self.client_table() if client[""IsInsertion""]
        }

    def available_resources(self):
        """"""Get the current available cluster resources.

        This is different from `cluster_resources` in that this will return
        idle (available) resources rather than total resources.

        Note that this information can grow stale as tasks start and finish.

        Returns:
            A dictionary mapping resource name to the total quantity of that
                resource in the cluster.
        """"""
        available_resources_by_id = {}

        subscribe_clients = [
            redis_client.pubsub(ignore_subscribe_messages=True)
            for redis_client in self.redis_clients
        ]
        for subscribe_client in subscribe_clients:
            subscribe_client.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL)

        client_ids = self._live_client_ids()

        while set(available_resources_by_id.keys()) != client_ids:
            for subscribe_client in subscribe_clients:
                # Parse client message
                raw_message = subscribe_client.get_message()
                if (raw_message is None or raw_message[""channel""] !=
                        ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL):
                    continue
                data = raw_message[""data""]
                gcs_entries = (
                    ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
                        data, 0))
                heartbeat_data = gcs_entries.Entries(0)
                message = (ray.gcs_utils.HeartbeatTableData.
                           GetRootAsHeartbeatTableData(heartbeat_data, 0))
                # Calculate available resources for this client
                num_resources = message.ResourcesAvailableLabelLength()
                dynamic_resources = {}
                for i in range(num_resources):
                    resource_id = decode(message.ResourcesAvailableLabel(i))
                    dynamic_resources[resource_id] = (
                        message.ResourcesAvailableCapacity(i))

                # Update available resources for this client
                client_id = ray.utils.binary_to_hex(message.ClientId())
                available_resources_by_id[client_id] = dynamic_resources

            # Update clients in cluster
            client_ids = self._live_client_ids()

            # Remove disconnected clients
            for client_id in available_resources_by_id.keys():
                if client_id not in client_ids:
                    del available_resources_by_id[client_id]

        # Calculate total available resources
        total_available_resources = defaultdict(int)
        for available_resources in available_resources_by_id.values():
            for resource_id, num_available in available_resources.items():
                total_available_resources[resource_id] += num_available

        # Close the pubsub clients to avoid leaking file descriptors.
        for subscribe_client in subscribe_clients:
            subscribe_client.close()

        return dict(total_available_resources)

    def _error_messages(self, job_id):
        """"""Get the error messages for a specific job.

        Args:
            job_id: The ID of the job to get the errors for.

        Returns:
            A list of the error messages for this job.
        """"""
        assert isinstance(job_id, ray.DriverID)
        message = self.redis_client.execute_command(
            ""RAY.TABLE_LOOKUP"", ray.gcs_utils.TablePrefix.ERROR_INFO, """",
            job_id.binary())

        # If there are no errors, return early.
        if message is None:
            return []

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            message, 0)
        error_messages = []
        for i in range(gcs_entries.EntriesLength()):
            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(
                gcs_entries.Entries(i), 0)
            assert job_id.binary() == error_data.JobId()
            error_message = {
                ""type"": decode(error_data.Type()),
                ""message"": decode(error_data.ErrorMessage()),
                ""timestamp"": error_data.Timestamp(),
            }
            error_messages.append(error_message)
        return error_messages

    def error_messages(self, job_id=None):
        """"""Get the error messages for all jobs or a specific job.

        Args:
            job_id: The specific job to get the errors for. If this is None,
                then this method retrieves the errors for all jobs.

        Returns:
            A dictionary mapping job ID to a list of the error messages for
                that job.
        """"""
        if job_id is not None:
            assert isinstance(job_id, ray.DriverID)
            return self._error_messages(job_id)

        error_table_keys = self.redis_client.keys(
            ray.gcs_utils.TablePrefix_ERROR_INFO_string + ""*"")
        job_ids = [
            key[len(ray.gcs_utils.TablePrefix_ERROR_INFO_string):]
            for key in error_table_keys
        ]

        return {
            binary_to_hex(job_id): self._error_messages(ray.DriverID(job_id))
            for job_id in job_ids
        }
/n/n/npython/ray/gcs_utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import flatbuffers
import ray.core.generated.ErrorTableData

from ray.core.generated.ClientTableData import ClientTableData
from ray.core.generated.DriverTableData import DriverTableData
from ray.core.generated.ErrorTableData import ErrorTableData
from ray.core.generated.GcsTableEntry import GcsTableEntry
from ray.core.generated.HeartbeatBatchTableData import HeartbeatBatchTableData
from ray.core.generated.HeartbeatTableData import HeartbeatTableData
from ray.core.generated.Language import Language
from ray.core.generated.ObjectTableData import ObjectTableData
from ray.core.generated.ProfileTableData import ProfileTableData
from ray.core.generated.TablePrefix import TablePrefix
from ray.core.generated.TablePubsub import TablePubsub

from ray.core.generated.ray.protocol.Task import Task

__all__ = [
    ""GcsTableEntry"", ""ClientTableData"", ""ErrorTableData"", ""HeartbeatTableData"",
    ""HeartbeatBatchTableData"", ""DriverTableData"", ""ProfileTableData"",
    ""ObjectTableData"", ""Task"", ""TablePrefix"", ""TablePubsub"", ""Language"",
    ""construct_error_message""
]

FUNCTION_PREFIX = ""RemoteFunction:""
LOG_FILE_CHANNEL = ""RAY_LOG_CHANNEL""

# xray heartbeats
XRAY_HEARTBEAT_CHANNEL = str(TablePubsub.HEARTBEAT).encode(""ascii"")
XRAY_HEARTBEAT_BATCH_CHANNEL = str(TablePubsub.HEARTBEAT_BATCH).encode(""ascii"")

# xray driver updates
XRAY_DRIVER_CHANNEL = str(TablePubsub.DRIVER).encode(""ascii"")

# These prefixes must be kept up-to-date with the TablePrefix enum in gcs.fbs.
# TODO(rkn): We should use scoped enums, in which case we should be able to
# just access the flatbuffer generated values.
TablePrefix_RAYLET_TASK_string = ""RAYLET_TASK""
TablePrefix_OBJECT_string = ""OBJECT""
TablePrefix_ERROR_INFO_string = ""ERROR_INFO""
TablePrefix_PROFILE_string = ""PROFILE""


def construct_error_message(driver_id, error_type, message, timestamp):
    """"""Construct a serialized ErrorTableData object.

    Args:
        driver_id: The ID of the driver that the error should go to. If this is
            nil, then the error will go to all drivers.
        error_type: The type of the error.
        message: The error message.
        timestamp: The time of the error.

    Returns:
        The serialized object.
    """"""
    builder = flatbuffers.Builder(0)
    driver_offset = builder.CreateString(driver_id.binary())
    error_type_offset = builder.CreateString(error_type)
    message_offset = builder.CreateString(message)

    ray.core.generated.ErrorTableData.ErrorTableDataStart(builder)
    ray.core.generated.ErrorTableData.ErrorTableDataAddJobId(
        builder, driver_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddType(
        builder, error_type_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddErrorMessage(
        builder, message_offset)
    ray.core.generated.ErrorTableData.ErrorTableDataAddTimestamp(
        builder, timestamp)
    error_data_offset = ray.core.generated.ErrorTableData.ErrorTableDataEnd(
        builder)
    builder.Finish(error_data_offset)

    return bytes(builder.Output())
/n/n/npython/ray/import_thread.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import traceback

import ray
from ray import ray_constants
from ray import cloudpickle as pickle
from ray import profiling
from ray import utils


class ImportThread(object):
    """"""A thread used to import exports from the driver or other workers.

    Note: The driver also has an import thread, which is used only to import
    custom class definitions from calls to register_custom_serializer that
    happen under the hood on workers.

    Attributes:
        worker: the worker object in this process.
        mode: worker mode
        redis_client: the redis client used to query exports.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    def __init__(self, worker, mode, threads_stopped):
        self.worker = worker
        self.mode = mode
        self.redis_client = worker.redis_client
        self.threads_stopped = threads_stopped

    def start(self):
        """"""Start the import thread.""""""
        self.t = threading.Thread(target=self._run, name=""ray_import_thread"")
        # Making the thread a daemon causes it to exit
        # when the main thread exits.
        self.t.daemon = True
        self.t.start()

    def join_import_thread(self):
        """"""Wait for the thread to exit.""""""
        self.t.join()

    def _run(self):
        import_pubsub_client = self.redis_client.pubsub()
        # Exports that are published after the call to
        # import_pubsub_client.subscribe and before the call to
        # import_pubsub_client.listen will still be processed in the loop.
        import_pubsub_client.subscribe(""__keyspace@0__:Exports"")
        # Keep track of the number of imports that we've imported.
        num_imported = 0

        try:
            # Get the exports that occurred before the call to subscribe.
            with self.worker.lock:
                export_keys = self.redis_client.lrange(""Exports"", 0, -1)
                for key in export_keys:
                    num_imported += 1
                    self._process_key(key)

            while True:
                # Exit if we received a signal that we should stop.
                if self.threads_stopped.is_set():
                    return

                msg = import_pubsub_client.get_message()
                if msg is None:
                    self.threads_stopped.wait(timeout=0.01)
                    continue

                with self.worker.lock:
                    if msg[""type""] == ""subscribe"":
                        continue
                    assert msg[""data""] == b""rpush""
                    num_imports = self.redis_client.llen(""Exports"")
                    assert num_imports >= num_imported
                    for i in range(num_imported, num_imports):
                        num_imported += 1
                        key = self.redis_client.lindex(""Exports"", i)
                        self._process_key(key)
        finally:
            # Close the pubsub client to avoid leaking file descriptors.
            import_pubsub_client.close()

    def _process_key(self, key):
        """"""Process the given export key from redis.""""""
        # Handle the driver case first.
        if self.mode != ray.WORKER_MODE:
            if key.startswith(b""FunctionsToRun""):
                with profiling.profile(
                        ""fetch_and_run_function"", worker=self.worker):
                    self.fetch_and_execute_function_to_run(key)
            # Return because FunctionsToRun are the only things that
            # the driver should import.
            return

        if key.startswith(b""RemoteFunction""):
            with profiling.profile(
                    ""register_remote_function"", worker=self.worker):
                (self.worker.function_actor_manager.
                 fetch_and_register_remote_function(key))
        elif key.startswith(b""FunctionsToRun""):
            with profiling.profile(
                    ""fetch_and_run_function"", worker=self.worker):
                self.fetch_and_execute_function_to_run(key)
        elif key.startswith(b""ActorClass""):
            # Keep track of the fact that this actor class has been
            # exported so that we know it is safe to turn this worker
            # into an actor of that class.
            self.worker.function_actor_manager.imported_actor_classes.add(key)
        # TODO(rkn): We may need to bring back the case of
        # fetching actor classes here.
        else:
            raise Exception(""This code should be unreachable."")

    def fetch_and_execute_function_to_run(self, key):
        """"""Run on arbitrary function on the worker.""""""
        (driver_id, serialized_function,
         run_on_other_drivers) = self.redis_client.hmget(
             key, [""driver_id"", ""function"", ""run_on_other_drivers""])

        if (utils.decode(run_on_other_drivers) == ""False""
                and self.worker.mode == ray.SCRIPT_MODE
                and driver_id != self.worker.task_driver_id.binary()):
            return

        try:
            # Deserialize the function.
            function = pickle.loads(serialized_function)
            # Run the function.
            function({""worker"": self.worker})
        except Exception:
            # If an exception was thrown when the function was run, we record
            # the traceback and notify the scheduler of the failure.
            traceback_str = traceback.format_exc()
            # Log the error message.
            utils.push_error_to_driver(
                self.worker,
                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,
                traceback_str,
                driver_id=ray.DriverID(driver_id))
/n/n/npython/ray/log_monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import errno
import logging
import os
import traceback

import colorama

import ray.ray_constants as ray_constants
import ray.utils

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)


class LogFileInfo(object):
    def __init__(self,
                 filename=None,
                 size_when_last_opened=None,
                 file_position=None,
                 file_handle=None):
        assert (filename is not None and size_when_last_opened is not None
                and file_position is not None)
        self.filename = filename
        self.size_when_last_opened = size_when_last_opened
        self.file_position = file_position
        self.file_handle = file_handle
        self.worker_pid = None


class LogMonitor(object):
    """"""A monitor process for monitoring Ray log files.

    This class mantains a list of open files and a list of closed log files. We
    can't simply leave all files open because we'll run out of file
    descriptors.

    The ""run"" method of this class will cycle between doing several things:
    1. First, it will check if any new files have appeared in the log
       directory. If so, they will be added to the list of closed files.
    2. Then, if we are unable to open any new files, we will close all of the
       files.
    3. Then, we will open as many closed files as we can that may have new
       lines (judged by an increase in file size since the last time the file
       was opened).
    4. Then we will loop through the open files and see if there are any new
       lines in the file. If so, we will publish them to Redis.

    Attributes:
        host (str): The hostname of this machine. Used to improve the log
            messages published to Redis.
        logs_dir (str): The directory that the log files are in.
        redis_client: A client used to communicate with the Redis server.
        log_filenames (set): This is the set of filenames of all files in
            open_file_infos and closed_file_infos.
        open_file_infos (list[LogFileInfo]): Info for all of the open files.
        closed_file_infos (list[LogFileInfo]): Info for all of the closed
            files.
        can_open_more_files (bool): True if we can still open more files and
            false otherwise.
    """"""

    def __init__(self, logs_dir, redis_address, redis_password=None):
        """"""Initialize the log monitor object.""""""
        self.host = os.uname()[1]
        self.logs_dir = logs_dir
        self.redis_client = ray.services.create_redis_client(
            redis_address, password=redis_password)
        self.log_filenames = set()
        self.open_file_infos = []
        self.closed_file_infos = []
        self.can_open_more_files = True

    def close_all_files(self):
        """"""Close all open files (so that we can open more).""""""
        while len(self.open_file_infos) > 0:
            file_info = self.open_file_infos.pop(0)
            file_info.file_handle.close()
            file_info.file_handle = None
            self.closed_file_infos.append(file_info)
        self.can_open_more_files = True

    def update_log_filenames(self):
        """"""Update the list of log files to monitor.""""""
        log_filenames = os.listdir(self.logs_dir)

        for log_filename in log_filenames:
            full_path = os.path.join(self.logs_dir, log_filename)
            if full_path not in self.log_filenames:
                self.log_filenames.add(full_path)
                self.closed_file_infos.append(
                    LogFileInfo(
                        filename=full_path,
                        size_when_last_opened=0,
                        file_position=0,
                        file_handle=None))
                logger.info(""Beginning to track file {}"".format(log_filename))

    def open_closed_files(self):
        """"""Open some closed files if they may have new lines.

        Opening more files may require us to close some of the already open
        files.
        """"""
        if not self.can_open_more_files:
            # If we can't open any more files. Close all of the files.
            self.close_all_files()

        files_with_no_updates = []
        while len(self.closed_file_infos) > 0:
            if (len(self.open_file_infos) >=
                    ray_constants.LOG_MONITOR_MAX_OPEN_FILES):
                self.can_open_more_files = False
                break

            file_info = self.closed_file_infos.pop(0)
            assert file_info.file_handle is None
            # Get the file size to see if it has gotten bigger since we last
            # opened it.
            try:
                file_size = os.path.getsize(file_info.filename)
            except (IOError, OSError) as e:
                # Catch ""file not found"" errors.
                if e.errno == errno.ENOENT:
                    logger.warning(""Warning: The file {} was not ""
                                   ""found."".format(file_info.filename))
                    self.log_filenames.remove(file_info.filename)
                    continue
                raise e

            # If some new lines have been added to this file, try to reopen the
            # file.
            if file_size > file_info.size_when_last_opened:
                try:
                    f = open(file_info.filename, ""r"")
                except (IOError, OSError) as e:
                    if e.errno == errno.ENOENT:
                        logger.warning(""Warning: The file {} was not ""
                                       ""found."".format(file_info.filename))
                        self.log_filenames.remove(file_info.filename)
                        continue
                    else:
                        raise e

                f.seek(file_info.file_position)
                file_info.filesize_when_last_opened = file_size
                file_info.file_handle = f
                self.open_file_infos.append(file_info)
            else:
                files_with_no_updates.append(file_info)

        # Add the files with no changes back to the list of closed files.
        self.closed_file_infos += files_with_no_updates

    def check_log_files_and_publish_updates(self):
        """"""Get any changes to the log files and push updates to Redis.""""""
        for file_info in self.open_file_infos:
            assert not file_info.file_handle.closed

            lines_to_publish = []
            max_num_lines_to_read = 100
            for _ in range(max_num_lines_to_read):
                next_line = file_info.file_handle.readline()
                if next_line == """":
                    break
                if next_line[-1] == ""\n"":
                    next_line = next_line[:-1]
                lines_to_publish.append(next_line)

            # Publish the lines if this is a worker process.
            filename = file_info.filename.split(""/"")[-1]
            is_worker = (filename.startswith(""worker"")
                         and (filename.endswith(""out"")
                              or filename.endswith(""err"")))
            output_type = ""stdout"" if filename.endswith(""out"") else ""stderr""

            if is_worker and file_info.file_position == 0:
                if (len(lines_to_publish) > 0 and
                        lines_to_publish[0].startswith(""Ray worker pid: "")):
                    file_info.worker_pid = int(
                        lines_to_publish[0].split("" "")[-1])
                    lines_to_publish = lines_to_publish[1:]

            # Record the current position in the file.
            file_info.file_position = file_info.file_handle.tell()

            if len(lines_to_publish) > 0 and is_worker:
                lines_to_publish.insert(
                    0, ""{}{}{} (pid={}, host={})"".format(
                        colorama.Fore.CYAN, ""worker ({})"".format(output_type),
                        colorama.Fore.RESET, file_info.worker_pid, self.host))

                self.redis_client.publish(ray.gcs_utils.LOG_FILE_CHANNEL,
                                          ""\n"".join(lines_to_publish))

    def run(self):
        """"""Run the log monitor.

        This will query Redis once every second to check if there are new log
        files to monitor. It will also store those log files in Redis.
        """"""
        while True:
            self.update_log_filenames()
            self.open_closed_files()
            self.check_log_files_and_publish_updates()


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""log monitor to connect ""
                     ""to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""The address to use for Redis."")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    parser.add_argument(
        ""--logs-dir"",
        required=True,
        type=str,
        help=""Specify the path of the temporary directory used by Ray ""
        ""processes."")
    args = parser.parse_args()
    ray.utils.setup_logger(args.logging_level, args.logging_format)

    log_monitor = LogMonitor(
        args.logs_dir, args.redis_address, redis_password=args.redis_password)

    try:
        log_monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = ray.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = (""The log monitor on node {} failed with the following ""
                   ""error:\n{}"".format(os.uname()[1], traceback_str))
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.LOG_MONITOR_DIED_ERROR, message)
        raise e
/n/n/npython/ray/monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import time
import traceback

import redis

import ray
from ray.autoscaler.autoscaler import LoadMetrics, StandardAutoscaler
import ray.cloudpickle as pickle
import ray.gcs_utils
import ray.utils
import ray.ray_constants as ray_constants
from ray.services import get_ip_address, get_port
from ray.utils import (binary_to_hex, binary_to_object_id, hex_to_binary,
                       setup_logger)

logger = logging.getLogger(__name__)


class Monitor(object):
    """"""A monitor for Ray processes.

    The monitor is in charge of cleaning up the tables in the global state
    after processes have died. The monitor is currently not responsible for
    detecting component failures.

    Attributes:
        redis: A connection to the Redis server.
        subscribe_client: A pubsub client for the Redis server. This is used to
            receive notifications about failed components.
    """"""

    def __init__(self, redis_address, autoscaling_config, redis_password=None):
        # Initialize the Redis clients.
        self.state = ray.experimental.state.GlobalState()
        redis_ip_address = get_ip_address(args.redis_address)
        redis_port = get_port(args.redis_address)
        self.state._initialize_global_state(
            redis_ip_address, redis_port, redis_password=redis_password)
        self.redis = ray.services.create_redis_client(
            redis_address, password=redis_password)
        # Setup subscriptions to the primary Redis server and the Redis shards.
        self.primary_subscribe_client = self.redis.pubsub(
            ignore_subscribe_messages=True)
        # Keep a mapping from local scheduler client ID to IP address to use
        # for updating the load metrics.
        self.local_scheduler_id_to_ip_map = {}
        self.load_metrics = LoadMetrics()
        if autoscaling_config:
            self.autoscaler = StandardAutoscaler(autoscaling_config,
                                                 self.load_metrics)
        else:
            self.autoscaler = None

        # Experimental feature: GCS flushing.
        self.issue_gcs_flushes = ""RAY_USE_NEW_GCS"" in os.environ
        self.gcs_flush_policy = None
        if self.issue_gcs_flushes:
            # Data is stored under the first data shard, so we issue flushes to
            # that redis server.
            addr_port = self.redis.lrange(""RedisShards"", 0, -1)
            if len(addr_port) > 1:
                logger.warning(
                    ""Monitor: ""
                    ""TODO: if launching > 1 redis shard, flushing needs to ""
                    ""touch shards in parallel."")
                self.issue_gcs_flushes = False
            else:
                addr_port = addr_port[0].split(b"":"")
                self.redis_shard = redis.StrictRedis(
                    host=addr_port[0],
                    port=addr_port[1],
                    password=redis_password)
                try:
                    self.redis_shard.execute_command(""HEAD.FLUSH 0"")
                except redis.exceptions.ResponseError as e:
                    logger.info(
                        ""Monitor: ""
                        ""Turning off flushing due to exception: {}"".format(
                            str(e)))
                    self.issue_gcs_flushes = False

    def __del__(self):
        """"""Destruct the monitor object.""""""
        # We close the pubsub client to avoid leaking file descriptors.
        self.primary_subscribe_client.close()

    def subscribe(self, channel):
        """"""Subscribe to the given channel on the primary Redis shard.

        Args:
            channel (str): The channel to subscribe to.

        Raises:
            Exception: An exception is raised if the subscription fails.
        """"""
        self.primary_subscribe_client.subscribe(channel)

    def xray_heartbeat_batch_handler(self, unused_channel, data):
        """"""Handle an xray heartbeat batch message from Redis.""""""

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        heartbeat_data = gcs_entries.Entries(0)

        message = (ray.gcs_utils.HeartbeatBatchTableData.
                   GetRootAsHeartbeatBatchTableData(heartbeat_data, 0))

        for j in range(message.BatchLength()):
            heartbeat_message = message.Batch(j)

            num_resources = heartbeat_message.ResourcesAvailableLabelLength()
            static_resources = {}
            dynamic_resources = {}
            for i in range(num_resources):
                dyn = heartbeat_message.ResourcesAvailableLabel(i)
                static = heartbeat_message.ResourcesTotalLabel(i)
                dynamic_resources[dyn] = (
                    heartbeat_message.ResourcesAvailableCapacity(i))
                static_resources[static] = (
                    heartbeat_message.ResourcesTotalCapacity(i))

            # Update the load metrics for this local scheduler.
            client_id = ray.utils.binary_to_hex(heartbeat_message.ClientId())
            ip = self.local_scheduler_id_to_ip_map.get(client_id)
            if ip:
                self.load_metrics.update(ip, static_resources,
                                         dynamic_resources)
            else:
                logger.warning(
                    ""Monitor: ""
                    ""could not find ip for client {}"".format(client_id))

    def _xray_clean_up_entries_for_driver(self, driver_id):
        """"""Remove this driver's object/task entries from redis.

        Removes control-state entries of all tasks and task return
        objects belonging to the driver.

        Args:
            driver_id: The driver id.
        """"""

        xray_task_table_prefix = (
            ray.gcs_utils.TablePrefix_RAYLET_TASK_string.encode(""ascii""))
        xray_object_table_prefix = (
            ray.gcs_utils.TablePrefix_OBJECT_string.encode(""ascii""))

        task_table_objects = self.state.task_table()
        driver_id_hex = binary_to_hex(driver_id)
        driver_task_id_bins = set()
        for task_id_hex, task_info in task_table_objects.items():
            task_table_object = task_info[""TaskSpec""]
            task_driver_id_hex = task_table_object[""DriverID""]
            if driver_id_hex != task_driver_id_hex:
                # Ignore tasks that aren't from this driver.
                continue
            driver_task_id_bins.add(hex_to_binary(task_id_hex))

        # Get objects associated with the driver.
        object_table_objects = self.state.object_table()
        driver_object_id_bins = set()
        for object_id, _ in object_table_objects.items():
            task_id_bin = ray._raylet.compute_task_id(object_id).binary()
            if task_id_bin in driver_task_id_bins:
                driver_object_id_bins.add(object_id.binary())

        def to_shard_index(id_bin):
            return binary_to_object_id(id_bin).redis_shard_hash() % len(
                self.state.redis_clients)

        # Form the redis keys to delete.
        sharded_keys = [[] for _ in range(len(self.state.redis_clients))]
        for task_id_bin in driver_task_id_bins:
            sharded_keys[to_shard_index(task_id_bin)].append(
                xray_task_table_prefix + task_id_bin)
        for object_id_bin in driver_object_id_bins:
            sharded_keys[to_shard_index(object_id_bin)].append(
                xray_object_table_prefix + object_id_bin)

        # Remove with best effort.
        for shard_index in range(len(sharded_keys)):
            keys = sharded_keys[shard_index]
            if len(keys) == 0:
                continue
            redis = self.state.redis_clients[shard_index]
            num_deleted = redis.delete(*keys)
            logger.info(""Monitor: ""
                        ""Removed {} dead redis entries of the ""
                        ""driver from redis shard {}."".format(
                            num_deleted, shard_index))
            if num_deleted != len(keys):
                logger.warning(""Monitor: ""
                               ""Failed to remove {} relevant redis ""
                               ""entries from redis shard {}."".format(
                                   len(keys) - num_deleted, shard_index))

    def xray_driver_removed_handler(self, unused_channel, data):
        """"""Handle a notification that a driver has been removed.

        Args:
            unused_channel: The message channel.
            data: The message data.
        """"""
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        driver_data = gcs_entries.Entries(0)
        message = ray.gcs_utils.DriverTableData.GetRootAsDriverTableData(
            driver_data, 0)
        driver_id = message.DriverId()
        logger.info(""Monitor: ""
                    ""XRay Driver {} has been removed."".format(
                        binary_to_hex(driver_id)))
        self._xray_clean_up_entries_for_driver(driver_id)

    def process_messages(self, max_messages=10000):
        """"""Process all messages ready in the subscription channels.

        This reads messages from the subscription channels and calls the
        appropriate handlers until there are no messages left.

        Args:
            max_messages: The maximum number of messages to process before
                returning.
        """"""
        subscribe_clients = [self.primary_subscribe_client]
        for subscribe_client in subscribe_clients:
            for _ in range(max_messages):
                message = subscribe_client.get_message()
                if message is None:
                    # Continue on to the next subscribe client.
                    break

                # Parse the message.
                channel = message[""channel""]
                data = message[""data""]

                # Determine the appropriate message handler.
                if channel == ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL:
                    # Similar functionality as local scheduler info channel
                    message_handler = self.xray_heartbeat_batch_handler
                elif channel == ray.gcs_utils.XRAY_DRIVER_CHANNEL:
                    # Handles driver death.
                    message_handler = self.xray_driver_removed_handler
                else:
                    raise Exception(""This code should be unreachable."")

                # Call the handler.
                message_handler(channel, data)

    def update_local_scheduler_map(self):
        local_schedulers = self.state.client_table()
        self.local_scheduler_id_to_ip_map = {}
        for local_scheduler_info in local_schedulers:
            client_id = local_scheduler_info.get(""DBClientID"") or \
                local_scheduler_info[""ClientID""]
            ip_address = (
                local_scheduler_info.get(""AuxAddress"")
                or local_scheduler_info[""NodeManagerAddress""]).split("":"")[0]
            self.local_scheduler_id_to_ip_map[client_id] = ip_address

    def _maybe_flush_gcs(self):
        """"""Experimental: issue a flush request to the GCS.

        The purpose of this feature is to control GCS memory usage.

        To activate this feature, Ray must be compiled with the flag
        RAY_USE_NEW_GCS set, and Ray must be started at run time with the flag
        as well.
        """"""
        if not self.issue_gcs_flushes:
            return
        if self.gcs_flush_policy is None:
            serialized = self.redis.get(""gcs_flushing_policy"")
            if serialized is None:
                # Client has not set any policy; by default flushing is off.
                return
            self.gcs_flush_policy = pickle.loads(serialized)

        if not self.gcs_flush_policy.should_flush(self.redis_shard):
            return

        max_entries_to_flush = self.gcs_flush_policy.num_entries_to_flush()
        num_flushed = self.redis_shard.execute_command(
            ""HEAD.FLUSH {}"".format(max_entries_to_flush))
        logger.info(""Monitor: num_flushed {}"".format(num_flushed))

        # This flushes event log and log files.
        ray.experimental.flush_redis_unsafe(self.redis)

        self.gcs_flush_policy.record_flush()

    def run(self):
        """"""Run the monitor.

        This function loops forever, checking for messages about dead database
        clients and cleaning up state accordingly.
        """"""
        # Initialize the subscription channel.
        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)
        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)

        # TODO(rkn): If there were any dead clients at startup, we should clean
        # up the associated state in the state tables.

        # Handle messages from the subscription channels.
        while True:
            # Update the mapping from local scheduler client ID to IP address.
            # This is only used to update the load metrics for the autoscaler.
            self.update_local_scheduler_map()

            # Process autoscaling actions
            if self.autoscaler:
                self.autoscaler.update()

            self._maybe_flush_gcs()

            # Process a round of messages.
            self.process_messages()

            # Wait for a heartbeat interval before processing the next round of
            # messages.
            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)

        # TODO(rkn): This infinite loop should be inside of a try/except block,
        # and if an exception is thrown we should push an error message to all
        # drivers.


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""monitor to connect to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""the address to use for Redis"")
    parser.add_argument(
        ""--autoscaling-config"",
        required=False,
        type=str,
        help=""the path to the autoscaling config file"")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    setup_logger(args.logging_level, args.logging_format)

    if args.autoscaling_config:
        autoscaling_config = os.path.expanduser(args.autoscaling_config)
    else:
        autoscaling_config = None

    monitor = Monitor(
        args.redis_address,
        autoscaling_config,
        redis_password=args.redis_password)

    try:
        monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = ray.services.create_redis_client(
            args.redis_address, password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = ""The monitor failed with the following error:\n{}"".format(
            traceback_str)
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)
        raise e
/n/n/npython/ray/node.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import atexit
import json
import os
import logging
import signal
import threading
import time

import ray
import ray.ray_constants as ray_constants
from ray.tempfile_services import (
    get_logs_dir_path, get_object_store_socket_name, get_raylet_socket_name,
    new_log_monitor_log_file, new_monitor_log_file,
    new_raylet_monitor_log_file, new_plasma_store_log_file,
    new_raylet_log_file, new_webui_log_file, set_temp_root,
    try_to_create_directory)

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray configures it by default automatically
# using logging.basicConfig in its entry/init points.
logger = logging.getLogger(__name__)


class Node(object):
    """"""An encapsulation of the Ray processes on a single node.

    This class is responsible for starting Ray processes and killing them.

    Attributes:
        all_processes (dict): A mapping from process type (str) to a list of
            ProcessInfo objects. All lists have length one except for the Redis
            server list, which has multiple.
    """"""

    def __init__(self, ray_params, head=False, shutdown_at_exit=True):
        """"""Start a node.

        Args:
            ray_params (ray.params.RayParams): The parameters to use to
                configure the node.
            head (bool): True if this is the head node, which means it will
                start additional processes like the Redis servers, monitor
                processes, and web UI.
            shutdown_at_exit (bool): If true, a handler will be registered to
                shutdown the processes started here when the Python interpreter
                exits.
        """"""
        self.all_processes = {}

        ray_params.update_if_absent(
            node_ip_address=ray.services.get_node_ip_address(),
            include_log_monitor=True,
            resources={},
            include_webui=False,
            worker_path=os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                ""workers/default_worker.py""))

        if head:
            ray_params.update_if_absent(num_redis_shards=1, include_webui=True)
        else:
            redis_client = ray.services.create_redis_client(
                ray_params.redis_address, ray_params.redis_password)
            ray_params.include_java = (
                ray.services.include_java_from_redis(redis_client))

        self._ray_params = ray_params
        self._config = (json.loads(ray_params._internal_config)
                        if ray_params._internal_config else None)
        self._node_ip_address = ray_params.node_ip_address
        self._redis_address = ray_params.redis_address
        self._plasma_store_socket_name = None
        self._raylet_socket_name = None
        self._webui_url = None

        self.start_ray_processes()

        if shutdown_at_exit:
            atexit.register(lambda: self.kill_all_processes(
                check_alive=False, allow_graceful=True))

    @property
    def node_ip_address(self):
        """"""Get the cluster Redis address.""""""
        return self._node_ip_address

    @property
    def redis_address(self):
        """"""Get the cluster Redis address.""""""
        return self._redis_address

    @property
    def plasma_store_socket_name(self):
        """"""Get the node's plasma store socket name.""""""
        return self._plasma_store_socket_name

    @property
    def webui_url(self):
        """"""Get the cluster's web UI url.""""""
        return self._webui_url

    @property
    def raylet_socket_name(self):
        """"""Get the node's raylet socket name.""""""
        return self._raylet_socket_name

    def prepare_socket_file(self, socket_path):
        """"""Prepare the socket file for raylet and plasma.

        This method helps to prepare a socket file.
        1. Make the directory if the directory does not exist.
        2. If the socket file exists, raise exception.

        Args:
            socket_path (string): the socket file to prepare.
        """"""
        if not os.path.exists(socket_path):
            path = os.path.dirname(socket_path)
            if not os.path.isdir(path):
                try_to_create_directory(path)
        else:
            raise Exception(""Socket file {} exists!"".format(socket_path))

    def start_redis(self):
        """"""Start the Redis servers.""""""
        assert self._redis_address is None
        (self._redis_address, redis_shards,
         process_infos) = ray.services.start_redis(
             self._node_ip_address,
             port=self._ray_params.redis_port,
             redis_shard_ports=self._ray_params.redis_shard_ports,
             num_redis_shards=self._ray_params.num_redis_shards,
             redis_max_clients=self._ray_params.redis_max_clients,
             redirect_output=self._ray_params.redirect_output,
             redirect_worker_output=self._ray_params.redirect_worker_output,
             password=self._ray_params.redis_password,
             redis_max_memory=self._ray_params.redis_max_memory)
        assert (
            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (
            process_infos)

    def start_log_monitor(self):
        """"""Start the log monitor.""""""
        stdout_file, stderr_file = new_log_monitor_log_file()
        process_info = ray.services.start_log_monitor(
            self.redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_LOG_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] = [
            process_info
        ]

    def start_ui(self):
        """"""Start the web UI.""""""
        stdout_file, stderr_file = new_webui_log_file()
        self._webui_url, process_info = ray.services.start_ui(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        assert ray_constants.PROCESS_TYPE_WEB_UI not in self.all_processes
        if process_info is not None:
            self.all_processes[ray_constants.PROCESS_TYPE_WEB_UI] = [
                process_info
            ]

    def start_plasma_store(self):
        """"""Start the plasma store.""""""
        assert self._plasma_store_socket_name is None
        # If the user specified a socket name, use it.
        self._plasma_store_socket_name = (
            self._ray_params.plasma_store_socket_name
            or get_object_store_socket_name())
        self.prepare_socket_file(self._plasma_store_socket_name)
        stdout_file, stderr_file = (new_plasma_store_log_file(
            self._ray_params.redirect_output))
        process_info = ray.services.start_plasma_store(
            self._node_ip_address,
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            object_store_memory=self._ray_params.object_store_memory,
            plasma_directory=self._ray_params.plasma_directory,
            huge_pages=self._ray_params.huge_pages,
            plasma_store_socket_name=self._plasma_store_socket_name)
        assert (
            ray_constants.PROCESS_TYPE_PLASMA_STORE not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_PLASMA_STORE] = [
            process_info
        ]

    def start_raylet(self, use_valgrind=False, use_profiler=False):
        """"""Start the raylet.

        Args:
            use_valgrind (bool): True if we should start the process in
                valgrind.
            use_profiler (bool): True if we should start the process in the
                valgrind profiler.
        """"""
        assert self._raylet_socket_name is None
        # If the user specified a socket name, use it.
        self._raylet_socket_name = (self._ray_params.raylet_socket_name
                                    or get_raylet_socket_name())
        self.prepare_socket_file(self._raylet_socket_name)
        stdout_file, stderr_file = new_raylet_log_file(
            redirect_output=self._ray_params.redirect_output)
        process_info = ray.services.start_raylet(
            self._redis_address,
            self._node_ip_address,
            self._raylet_socket_name,
            self._plasma_store_socket_name,
            self._ray_params.worker_path,
            self._ray_params.num_cpus,
            self._ray_params.num_gpus,
            self._ray_params.resources,
            self._ray_params.object_manager_port,
            self._ray_params.node_manager_port,
            self._ray_params.redis_password,
            use_valgrind=use_valgrind,
            use_profiler=use_profiler,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            config=self._config,
            include_java=self._ray_params.include_java,
            java_worker_options=self._ray_params.java_worker_options,
        )
        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]

    def start_worker(self):
        """"""Start a worker process.""""""
        raise NotImplementedError

    def start_monitor(self):
        """"""Start the monitor.""""""
        stdout_file, stderr_file = new_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_monitor(
            self._redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            autoscaling_config=self._ray_params.autoscaling_config,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_MONITOR] = [process_info]

    def start_raylet_monitor(self):
        """"""Start the raylet monitor.""""""
        stdout_file, stderr_file = new_raylet_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_raylet_monitor(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password,
            config=self._config)
        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in
                self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [
            process_info
        ]

    def start_ray_processes(self):
        """"""Start all of the processes on the node.""""""
        set_temp_root(self._ray_params.temp_dir)
        logger.info(
            ""Process STDOUT and STDERR is being redirected to {}."".format(
                get_logs_dir_path()))

        # If this is the head node, start the relevant head node processes.
        if self._redis_address is None:
            self.start_redis()
            self.start_monitor()
            self.start_raylet_monitor()

        self.start_plasma_store()
        self.start_raylet()

        if self._ray_params.include_log_monitor:
            self.start_log_monitor()
        if self._ray_params.include_webui:
            self.start_ui()

    def _kill_process_type(self,
                           process_type,
                           allow_graceful=False,
                           check_alive=True,
                           wait=False):
        """"""Kill a process of a given type.

        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all
        of the Redis servers.

        If the process was started in valgrind, then we will raise an exception
        if the process has a non-zero exit code.

        Args:
            process_type: The type of the process to kill.
            allow_graceful (bool): Send a SIGTERM first and give the process
                time to exit gracefully. If that doesn't work, then use
                SIGKILL. We usually want to do this outside of tests.
            check_alive (bool): If true, then we expect the process to be alive
                and will raise an exception if the process is already dead.
            wait (bool): If true, then this method will not return until the
                process in question has exited.

        Raises:
            This process raises an exception in the following cases:
                1. The process had already died and check_alive is true.
                2. The process had been started in valgrind and had a non-zero
                   exit code.
        """"""
        process_infos = self.all_processes[process_type]
        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:
            assert len(process_infos) == 1
        for process_info in process_infos:
            process = process_info.process
            # Handle the case where the process has already exited.
            if process.poll() is not None:
                if check_alive:
                    raise Exception(""Attempting to kill a process of type ""
                                    ""'{}', but this process is already dead.""
                                    .format(process_type))
                else:
                    continue

            if process_info.use_valgrind:
                process.terminate()
                process.wait()
                if process.returncode != 0:
                    message = (""Valgrind detected some errors in process of ""
                               ""type {}. Error code {}."".format(
                                   process_type, process.returncode))
                    if process_info.stdout_file is not None:
                        with open(process_info.stdout_file, ""r"") as f:
                            message += ""\nPROCESS STDOUT:\n"" + f.read()
                    if process_info.stderr_file is not None:
                        with open(process_info.stderr_file, ""r"") as f:
                            message += ""\nPROCESS STDERR:\n"" + f.read()
                    raise Exception(message)
                continue

            if process_info.use_valgrind_profiler:
                # Give process signal to write profiler data.
                os.kill(process.pid, signal.SIGINT)
                # Wait for profiling data to be written.
                time.sleep(0.1)

            if allow_graceful:
                # Allow the process one second to exit gracefully.
                process.terminate()
                timer = threading.Timer(1, lambda process: process.kill(),
                                        [process])
                try:
                    timer.start()
                    process.wait()
                finally:
                    timer.cancel()

                if process.poll() is not None:
                    continue

            # If the process did not exit within one second, force kill it.
            process.kill()
            # The reason we usually don't call process.wait() here is that
            # there's some chance we'd end up waiting a really long time.
            if wait:
                process.wait()

        del self.all_processes[process_type]

    def kill_redis(self, check_alive=True):
        """"""Kill the Redis servers.

        Args:
            check_alive (bool): Raise an exception if any of the processes
                were already dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)

    def kill_plasma_store(self, check_alive=True):
        """"""Kill the plasma store.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)

    def kill_raylet(self, check_alive=True):
        """"""Kill the raylet.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET, check_alive=check_alive)

    def kill_log_monitor(self, check_alive=True):
        """"""Kill the log monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_LOG_MONITOR, check_alive=check_alive)

    def kill_monitor(self, check_alive=True):
        """"""Kill the monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_MONITOR, check_alive=check_alive)

    def kill_raylet_monitor(self, check_alive=True):
        """"""Kill the raylet monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET_MONITOR, check_alive=check_alive)

    def kill_all_processes(self, check_alive=True, allow_graceful=False):
        """"""Kill all of the processes.

        Note that This is slower than necessary because it calls kill, wait,
        kill, wait, ... instead of kill, kill, ..., wait, wait, ...

        Args:
            check_alive (bool): Raise an exception if any of the processes were
                already dead.
        """"""
        # Kill the raylet first. This is important for suppressing errors at
        # shutdown because we give the raylet a chance to exit gracefully and
        # clean up its child worker processes. If we were to kill the plasma
        # store (or Redis) first, that could cause the raylet to exit
        # ungracefully, leading to more verbose output from the workers.
        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:
            self._kill_process_type(
                ray_constants.PROCESS_TYPE_RAYLET,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

        # We call ""list"" to copy the keys because we are modifying the
        # dictionary while iterating over it.
        for process_type in list(self.all_processes.keys()):
            self._kill_process_type(
                process_type,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

    def live_processes(self):
        """"""Return a list of the live processes.

        Returns:
            A list of the live processes.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is None:
                    result.append((process_type, process_info.process))
        return result

    def dead_processes(self):
        """"""Return a list of the dead processes.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            A list of the dead processes ignoring the ones that have been
                explicitly killed.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is not None:
                    result.append((process_type, process_info.process))
        return result

    def any_processes_alive(self):
        """"""Return true if any processes are still alive.

        Returns:
            True if any process is still alive.
        """"""
        return any(self.live_processes())

    def remaining_processes_alive(self):
        """"""Return true if all remaining processes are still alive.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            True if any process that wasn't explicitly killed is still alive.
        """"""
        return not any(self.dead_processes())
/n/n/npython/ray/parameter.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging

import ray.ray_constants as ray_constants


class RayParams(object):
    """"""A class used to store the parameters used by Ray.

    Attributes:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        redis_port (int): The port that the primary Redis shard should listen
            to. If None, then a random port will be chosen.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_cpus (int): Number of CPUs to configure the raylet with.
        num_gpus (int): Number of GPUs to configure the raylet with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries. This only applies to the
            sharded redis tables (task and object tables).
        object_manager_port int: The port to use for the object manager.
        node_manager_port: The port to use for the node manager.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        worker_path (str): The path of the source code that will be run by the
            worker.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        include_log_monitor (bool): If True, then start a log monitor to
            monitor the log files for all processes on this node and push their
            contents to Redis.
        autoscaling_config: path to autoscaling config file.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.
    """"""

    def __init__(self,
                 redis_address=None,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_store_memory=None,
                 redis_max_memory=None,
                 redis_port=None,
                 redis_shard_ports=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 node_ip_address=None,
                 object_id_seed=None,
                 num_workers=None,
                 local_mode=False,
                 driver_mode=None,
                 redirect_worker_output=True,
                 redirect_output=True,
                 num_redis_shards=None,
                 redis_max_clients=None,
                 redis_password=None,
                 plasma_directory=None,
                 worker_path=None,
                 huge_pages=False,
                 include_webui=None,
                 logging_level=logging.INFO,
                 logging_format=ray_constants.LOGGER_FORMAT,
                 plasma_store_socket_name=None,
                 raylet_socket_name=None,
                 temp_dir=None,
                 include_log_monitor=None,
                 autoscaling_config=None,
                 include_java=False,
                 java_worker_options=None,
                 _internal_config=None):
        self.object_id_seed = object_id_seed
        self.redis_address = redis_address
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.object_store_memory = object_store_memory
        self.redis_max_memory = redis_max_memory
        self.redis_port = redis_port
        self.redis_shard_ports = redis_shard_ports
        self.object_manager_port = object_manager_port
        self.node_manager_port = node_manager_port
        self.node_ip_address = node_ip_address
        self.num_workers = num_workers
        self.local_mode = local_mode
        self.driver_mode = driver_mode
        self.redirect_worker_output = redirect_worker_output
        self.redirect_output = redirect_output
        self.num_redis_shards = num_redis_shards
        self.redis_max_clients = redis_max_clients
        self.redis_password = redis_password
        self.plasma_directory = plasma_directory
        self.worker_path = worker_path
        self.huge_pages = huge_pages
        self.include_webui = include_webui
        self.plasma_store_socket_name = plasma_store_socket_name
        self.raylet_socket_name = raylet_socket_name
        self.temp_dir = temp_dir
        self.include_log_monitor = include_log_monitor
        self.autoscaling_config = autoscaling_config
        self.include_java = include_java
        self.java_worker_options = java_worker_options
        self._internal_config = _internal_config
        self._check_usage()

    def update(self, **kwargs):
        """"""Update the settings according to the keyword arguments.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update: %s"" % arg)

        self._check_usage()

    def update_if_absent(self, **kwargs):
        """"""Update the settings when the target fields are None.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                if getattr(self, arg) is None:
                    setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update_if_absent: %s"" % arg)

        self._check_usage()

    def _check_usage(self):
        if self.resources is not None:
            assert ""CPU"" not in self.resources, (
                ""'CPU' should not be included in the resource dictionary. Use ""
                ""num_cpus instead."")
            assert ""GPU"" not in self.resources, (
                ""'GPU' should not be included in the resource dictionary. Use ""
                ""num_gpus instead."")

        if self.num_workers is not None:
            raise ValueError(
                ""The 'num_workers' argument is deprecated. Please use ""
                ""'num_cpus' instead."")

        if self.include_java is None and self.java_worker_options is not None:
            raise ValueError(""Should not specify `java-worker-options` ""
                             ""without providing `include-java`."")
/n/n/npython/ray/profiling.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import time
import threading
import traceback

import ray

LOG_POINT = 0
LOG_SPAN_START = 1
LOG_SPAN_END = 2


class _NullLogSpan(object):
    """"""A log span context manager that does nothing""""""

    def __enter__(self):
        pass

    def __exit__(self, type, value, tb):
        pass


NULL_LOG_SPAN = _NullLogSpan()


def profile(event_type, extra_data=None, worker=None):
    """"""Profile a span of time so that it appears in the timeline visualization.

    Note that this only works in the raylet code path.

    This function can be used as follows (both on the driver or within a task).

    .. code-block:: python

        with ray.profile(""custom event"", extra_data={'key': 'value'}):
            # Do some computation here.

    Optionally, a dictionary can be passed as the ""extra_data"" argument, and
    it can have keys ""name"" and ""cname"" if you want to override the default
    timeline display text and box color. Other values will appear at the bottom
    of the chrome tracing GUI when you click on the box corresponding to this
    profile span.

    Args:
        event_type: A string describing the type of the event.
        extra_data: This must be a dictionary mapping strings to strings. This
            data will be added to the json objects that are used to populate
            the timeline, so if you want to set a particular color, you can
            simply set the ""cname"" attribute to an appropriate color.
            Similarly, if you set the ""name"" attribute, then that will set the
            text displayed on the box in the timeline.

    Returns:
        An object that can profile a span of time via a ""with"" statement.
    """"""
    if worker is None:
        worker = ray.worker.global_worker
    return RayLogSpanRaylet(worker.profiler, event_type, extra_data=extra_data)


class Profiler(object):
    """"""A class that holds the profiling states.

    Attributes:
        worker: the worker to profile.
        events: the buffer of events.
        lock: the lock to protect access of events.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    def __init__(self, worker, threads_stopped):
        self.worker = worker
        self.events = []
        self.lock = threading.Lock()
        self.threads_stopped = threads_stopped

    def start_flush_thread(self):
        self.t = threading.Thread(
            target=self._periodically_flush_profile_events,
            name=""ray_push_profiling_information"")
        # Making the thread a daemon causes it to exit when the main thread
        # exits.
        self.t.daemon = True
        self.t.start()

    def join_flush_thread(self):
        """"""Wait for the flush thread to exit.""""""
        self.t.join()

    def _periodically_flush_profile_events(self):
        """"""Drivers run this as a thread to flush profile data in the
        background.""""""
        # Note(rkn): This is run on a background thread in the driver. It uses
        # the local scheduler client. This should be ok because it doesn't read
        # from the local scheduler client and we have the GIL here. However,
        # if either of those things changes, then we could run into issues.
        while True:
            # Sleep for 1 second. This will be interrupted if
            # self.threads_stopped is set.
            self.threads_stopped.wait(timeout=1)

            # Exit if we received a signal that we should stop.
            if self.threads_stopped.is_set():
                return

            self.flush_profile_data()

    def flush_profile_data(self):
        """"""Push the logged profiling data to the global control store.""""""
        with self.lock:
            events = self.events
            self.events = []

        if self.worker.mode == ray.WORKER_MODE:
            component_type = ""worker""
        else:
            component_type = ""driver""

        self.worker.raylet_client.push_profile_events(
            component_type, ray.UniqueID(self.worker.worker_id),
            self.worker.node_ip_address, events)

    def add_event(self, event):
        with self.lock:
            self.events.append(event)


class RayLogSpanRaylet(object):
    """"""An object used to enable logging a span of events with a with statement.

    Attributes:
        event_type (str): The type of the event being logged.
        extra_data: Additional information to log.
    """"""

    def __init__(self, profiler, event_type, extra_data=None):
        """"""Initialize a RayLogSpanRaylet object.""""""
        self.profiler = profiler
        self.event_type = event_type
        self.extra_data = extra_data if extra_data is not None else {}

    def set_attribute(self, key, value):
        """"""Add a key-value pair to the extra_data dict.

        This can be used to add attributes that are not available when
        ray.profile was called.

        Args:
            key: The attribute name.
            value: The attribute value.
        """"""
        if not isinstance(key, str) or not isinstance(value, str):
            raise ValueError(""The arguments 'key' and 'value' must both be ""
                             ""strings. Instead they are {} and {}."".format(
                                 key, value))
        self.extra_data[key] = value

    def __enter__(self):
        """"""Log the beginning of a span event.

        Returns:
            The object itself is returned so that if the block is opened using
                ""with ray.profile(...) as prof:"", we can call
                ""prof.set_attribute"" inside the block.
        """"""
        self.start_time = time.time()
        return self

    def __exit__(self, type, value, tb):
        """"""Log the end of a span event. Log any exception that occurred.""""""
        for key, value in self.extra_data.items():
            if not isinstance(key, str) or not isinstance(value, str):
                raise ValueError(""The extra_data argument must be a ""
                                 ""dictionary mapping strings to strings. ""
                                 ""Instead it is {}."".format(self.extra_data))

        if type is not None:
            extra_data = json.dumps({
                ""type"": str(type),
                ""value"": str(value),
                ""traceback"": str(traceback.format_exc()),
            })
        else:
            extra_data = json.dumps(self.extra_data)

        event = {
            ""event_type"": self.event_type,
            ""start_time"": self.start_time,
            ""end_time"": time.time(),
            ""extra_data"": extra_data,
        }

        self.profiler.add_event(event)
/n/n/npython/ray/ray_constants.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function
""""""Ray constants used in the Python code.""""""

import os


def env_integer(key, default):
    if key in os.environ:
        return int(os.environ[key])
    return default


ID_SIZE = 20

# The default maximum number of bytes to allocate to the object store unless
# overridden by the user.
DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES = 20 * 10**9
# The smallest cap on the memory used by the object store that we allow.
OBJECT_STORE_MINIMUM_MEMORY_BYTES = 10**7
# The default maximum number of bytes that the non-primary Redis shards are
# allowed to use unless overridden by the user.
DEFAULT_REDIS_MAX_MEMORY_BYTES = 10**10
# The smallest cap on the memory used by Redis that we allow.
REDIS_MINIMUM_MEMORY_BYTES = 10**7

# If a remote function or actor (or some other export) has serialized size
# greater than this quantity, print an warning.
PICKLE_OBJECT_WARNING_SIZE = 10**7

# The maximum resource quantity that is allowed. TODO(rkn): This could be
# relaxed, but the current implementation of the node manager will be slower
# for large resource quantities due to bookkeeping of specific resource IDs.
MAX_RESOURCE_QUANTITY = 512

# Different types of Ray errors that can be pushed to the driver.
# TODO(rkn): These should be defined in flatbuffers and must be synced with
# the existing C++ definitions.
WAIT_FOR_CLASS_PUSH_ERROR = ""wait_for_class""
PICKLING_LARGE_OBJECT_PUSH_ERROR = ""pickling_large_object""
WAIT_FOR_FUNCTION_PUSH_ERROR = ""wait_for_function""
TASK_PUSH_ERROR = ""task""
REGISTER_REMOTE_FUNCTION_PUSH_ERROR = ""register_remote_function""
FUNCTION_TO_RUN_PUSH_ERROR = ""function_to_run""
VERSION_MISMATCH_PUSH_ERROR = ""version_mismatch""
CHECKPOINT_PUSH_ERROR = ""checkpoint""
REGISTER_ACTOR_PUSH_ERROR = ""register_actor""
WORKER_CRASH_PUSH_ERROR = ""worker_crash""
WORKER_DIED_PUSH_ERROR = ""worker_died""
WORKER_POOL_LARGE_ERROR = ""worker_pool_large""
PUT_RECONSTRUCTION_PUSH_ERROR = ""put_reconstruction""
INFEASIBLE_TASK_ERROR = ""infeasible_task""
REMOVED_NODE_ERROR = ""node_removed""
MONITOR_DIED_ERROR = ""monitor_died""
LOG_MONITOR_DIED_ERROR = ""log_monitor_died""

# Abort autoscaling if more than this number of errors are encountered. This
# is a safety feature to prevent e.g. runaway node launches.
AUTOSCALER_MAX_NUM_FAILURES = env_integer(""AUTOSCALER_MAX_NUM_FAILURES"", 5)

# The maximum number of nodes to launch in a single request.
# Multiple requests may be made for this batch size, up to
# the limit of AUTOSCALER_MAX_CONCURRENT_LAUNCHES.
AUTOSCALER_MAX_LAUNCH_BATCH = env_integer(""AUTOSCALER_MAX_LAUNCH_BATCH"", 5)

# Max number of nodes to launch at a time.
AUTOSCALER_MAX_CONCURRENT_LAUNCHES = env_integer(
    ""AUTOSCALER_MAX_CONCURRENT_LAUNCHES"", 10)

# Interval at which to perform autoscaling updates.
AUTOSCALER_UPDATE_INTERVAL_S = env_integer(""AUTOSCALER_UPDATE_INTERVAL_S"", 5)

# The autoscaler will attempt to restart Ray on nodes it hasn't heard from
# in more than this interval.
AUTOSCALER_HEARTBEAT_TIMEOUT_S = env_integer(""AUTOSCALER_HEARTBEAT_TIMEOUT_S"",
                                             30)

# Max number of retries to AWS (default is 5, time increases exponentially)
BOTO_MAX_RETRIES = env_integer(""BOTO_MAX_RETRIES"", 12)

LOGGER_FORMAT = (
    ""%(asctime)s\t%(levelname)s %(filename)s:%(lineno)s -- %(message)s"")
LOGGER_FORMAT_HELP = ""The logging format. default='{}'"".format(LOGGER_FORMAT)
LOGGER_LEVEL = ""info""
LOGGER_LEVEL_CHOICES = ['debug', 'info', 'warning', 'error', 'critical']
LOGGER_LEVEL_HELP = (""The logging level threshold, choices=['debug', 'info',""
                     "" 'warning', 'error', 'critical'], default='info'"")

# A constant indicating that an actor doesn't need reconstructions.
NO_RECONSTRUCTION = 0
# A constant indicating that an actor should be reconstructed infinite times.
INFINITE_RECONSTRUCTION = 2**30

# Constants used to define the different process types.
PROCESS_TYPE_MONITOR = ""monitor""
PROCESS_TYPE_RAYLET_MONITOR = ""raylet_monitor""
PROCESS_TYPE_LOG_MONITOR = ""log_monitor""
PROCESS_TYPE_WORKER = ""worker""
PROCESS_TYPE_RAYLET = ""raylet""
PROCESS_TYPE_PLASMA_STORE = ""plasma_store""
PROCESS_TYPE_REDIS_SERVER = ""redis_server""
PROCESS_TYPE_WEB_UI = ""web_ui""

LOG_MONITOR_MAX_OPEN_FILES = 200
/n/n/npython/ray/services.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import collections
import json
import logging
import multiprocessing
import os
import random
import resource
import socket
import subprocess
import sys
import time
import redis

import pyarrow
# Ray modules
import ray
import ray.ray_constants as ray_constants

from ray.tempfile_services import (
    get_gdb_init_path,
    get_ipython_notebook_path,
    get_logs_dir_path,
    get_temp_root,
    new_redis_log_file,
)

# True if processes are run in the valgrind profiler.
RUN_RAYLET_PROFILER = False
RUN_PLASMA_STORE_PROFILER = False

# Location of the redis server and module.
RAY_HOME = os.path.join(os.path.dirname(__file__), ""../.."")
REDIS_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/thirdparty/redis/src/redis-server"")
REDIS_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/gcs/redis_module/libray_redis_module.so"")

# Location of the credis server and modules.
# credis will be enabled if the environment variable RAY_USE_NEW_GCS is set.
CREDIS_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/redis/src/redis-server"")
CREDIS_MASTER_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/build/src/libmaster.so"")
CREDIS_MEMBER_MODULE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/credis/build/src/libmember.so"")

# Location of the plasma object store executable.
PLASMA_STORE_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/plasma/plasma_store_server"")

# Location of the raylet executables.
RAYLET_MONITOR_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)),
    ""core/src/ray/raylet/raylet_monitor"")
RAYLET_EXECUTABLE = os.path.join(
    os.path.abspath(os.path.dirname(__file__)), ""core/src/ray/raylet/raylet"")

DEFAULT_JAVA_WORKER_OPTIONS = ""-classpath {}"".format(
    os.path.join(
        os.path.abspath(os.path.dirname(__file__)), ""../../../build/java/*""))

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)

ProcessInfo = collections.namedtuple(""ProcessInfo"", [
    ""process"", ""stdout_file"", ""stderr_file"", ""use_valgrind"", ""use_gdb"",
    ""use_valgrind_profiler"", ""use_perftools_profiler"", ""use_tmux""
])


def address(ip_address, port):
    return ip_address + "":"" + str(port)


def get_ip_address(address):
    assert type(address) == str, ""Address must be a string""
    ip_address = address.split("":"")[0]
    return ip_address


def get_port(address):
    try:
        port = int(address.split("":"")[1])
    except Exception:
        raise Exception(""Unable to parse port from address {}"".format(address))
    return port


def new_port():
    return random.randint(10000, 65535)


def include_java_from_redis(redis_client):
    """"""This is used for query include_java bool from redis.

    Args:
        redis_client (StrictRedis): The redis client to GCS.

    Returns:
        True if this cluster backend enables Java worker.
    """"""
    return redis_client.get(""INCLUDE_JAVA"") == b""1""


def remaining_processes_alive():
    """"""See if the remaining processes are alive or not.

    Note that this ignores processes that have been explicitly killed,
    e.g., via a command like node.kill_raylet().

    Returns:
        True if the remaining processes started by ray.init() are alive and
            False otherwise.

    Raises:
        Exception: An exception is raised if the processes were not started by
            ray.init().
    """"""
    if ray.worker._global_node is None:
        raise Exception(""This process is not in a position to determine ""
                        ""whether all processes are alive or not."")
    return ray.worker._global_node.remaining_processes_alive()


def address_to_ip(address):
    """"""Convert a hostname to a numerical IP addresses in an address.

    This should be a no-op if address already contains an actual numerical IP
    address.

    Args:
        address: This can be either a string containing a hostname (or an IP
            address) and a port or it can be just an IP address.

    Returns:
        The same address but with the hostname replaced by a numerical IP
            address.
    """"""
    address_parts = address.split("":"")
    ip_address = socket.gethostbyname(address_parts[0])
    # Make sure localhost isn't resolved to the loopback ip
    if ip_address == ""127.0.0.1"":
        ip_address = get_node_ip_address()
    return "":"".join([ip_address] + address_parts[1:])


def get_node_ip_address(address=""8.8.8.8:53""):
    """"""Determine the IP address of the local node.

    Args:
        address (str): The IP address and port of any known live service on the
            network you care about.

    Returns:
        The IP address of the current node.
    """"""
    ip_address, port = address.split("":"")
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        # This command will raise an exception if there is no internet
        # connection.
        s.connect((ip_address, int(port)))
        node_ip_address = s.getsockname()[0]
    except Exception as e:
        node_ip_address = ""127.0.0.1""
        # [Errno 101] Network is unreachable
        if e.errno == 101:
            try:
                # try get node ip address from host name
                host_name = socket.getfqdn(socket.gethostname())
                node_ip_address = socket.gethostbyname(host_name)
            except Exception:
                pass

    return node_ip_address


def create_redis_client(redis_address, password=None):
    """"""Create a Redis client.

    Args:
        The IP address, port, and password of the Redis server.

    Returns:
        A Redis client.
    """"""
    redis_ip_address, redis_port = redis_address.split("":"")
    # For this command to work, some other client (on the same machine
    # as Redis) must have run ""CONFIG SET protected-mode no"".
    return redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=password)


def start_ray_process(command,
                      process_type,
                      env_updates=None,
                      cwd=None,
                      use_valgrind=False,
                      use_gdb=False,
                      use_valgrind_profiler=False,
                      use_perftools_profiler=False,
                      use_tmux=False,
                      stdout_file=None,
                      stderr_file=None):
    """"""Start one of the Ray processes.

    TODO(rkn): We need to figure out how these commands interact. For example,
    it may only make sense to start a process in gdb if we also start it in
    tmux. Similarly, certain combinations probably don't make sense, like
    simultaneously running the process in valgrind and the profiler.

    Args:
        command (List[str]): The command to use to start the Ray process.
        process_type (str): The type of the process that is being started
            (e.g., ""raylet"").
        env_updates (dict): A dictionary of additional environment variables to
            run the command with (in addition to the caller's environment
            variables).
        cwd (str): The directory to run the process in.
        use_valgrind (bool): True if we should start the process in valgrind.
        use_gdb (bool): True if we should start the process in gdb.
        use_valgrind_profiler (bool): True if we should start the process in
            the valgrind profiler.
        use_perftools_profiler (bool): True if we should profile the process
            using perftools.
        use_tmux (bool): True if we should start the process in tmux.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        Information about the process that was started including a handle to
            the process that was started.
    """"""
    # Detect which flags are set through environment variables.
    valgrind_env_var = ""RAY_{}_VALGRIND"".format(process_type.upper())
    if os.environ.get(valgrind_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", valgrind_env_var)
        use_valgrind = True
    valgrind_profiler_env_var = ""RAY_{}_VALGRIND_PROFILER"".format(
        process_type.upper())
    if os.environ.get(valgrind_profiler_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."",
                    valgrind_profiler_env_var)
        use_valgrind_profiler = True
    perftools_profiler_env_var = ""RAY_{}_PERFTOOLS_PROFILER"".format(
        process_type.upper())
    if os.environ.get(perftools_profiler_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."",
                    perftools_profiler_env_var)
        use_perftools_profiler = True
    tmux_env_var = ""RAY_{}_TMUX"".format(process_type.upper())
    if os.environ.get(tmux_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", tmux_env_var)
        use_tmux = True
    gdb_env_var = ""RAY_{}_GDB"".format(process_type.upper())
    if os.environ.get(gdb_env_var) == ""1"":
        logger.info(""Detected environment variable '%s'."", gdb_env_var)
        use_gdb = True

    if sum(
        [use_gdb, use_valgrind, use_valgrind_profiler, use_perftools_profiler
         ]) > 1:
        raise ValueError(
            ""At most one of the 'use_gdb', 'use_valgrind', ""
            ""'use_valgrind_profiler', and 'use_perftools_profiler' flags can ""
            ""be used at a time."")
    if env_updates is None:
        env_updates = {}
    if not isinstance(env_updates, dict):
        raise ValueError(""The 'env_updates' argument must be a dictionary."")

    modified_env = os.environ.copy()
    modified_env.update(env_updates)

    if use_gdb:
        if not use_tmux:
            raise ValueError(
                ""If 'use_gdb' is true, then 'use_tmux' must be true as well."")
        gdb_init_path = get_gdb_init_path(process_type)
        ray_process_path = command[0]
        ray_process_args = command[1:]
        run_args = "" "".join([""'{}'"".format(arg) for arg in ray_process_args])
        with open(gdb_init_path, ""w"") as gdb_init_file:
            gdb_init_file.write(""run {}"".format(run_args))
        command = [""gdb"", ray_process_path, ""-x"", gdb_init_path]

    if use_valgrind:
        command = [
            ""valgrind"", ""--track-origins=yes"", ""--leak-check=full"",
            ""--show-leak-kinds=all"", ""--leak-check-heuristics=stdstring"",
            ""--error-exitcode=1""
        ] + command

    if use_valgrind_profiler:
        command = [""valgrind"", ""--tool=callgrind""] + command

    if use_perftools_profiler:
        modified_env[""LD_PRELOAD""] = os.environ[""PERFTOOLS_PATH""]
        modified_env[""CPUPROFILE""] = os.environ[""PERFTOOLS_LOGFILE""]

    if use_tmux:
        # The command has to be created exactly as below to ensure that it
        # works on all versions of tmux. (Tested with tmux 1.8-5, travis'
        # version, and tmux 2.1)
        command = [""tmux"", ""new-session"", ""-d"", ""{}"".format("" "".join(command))]

    process = subprocess.Popen(
        command,
        env=modified_env,
        cwd=cwd,
        stdout=stdout_file,
        stderr=stderr_file)

    return ProcessInfo(
        process=process,
        stdout_file=stdout_file.name if stdout_file is not None else None,
        stderr_file=stderr_file.name if stderr_file is not None else None,
        use_valgrind=use_valgrind,
        use_gdb=use_gdb,
        use_valgrind_profiler=use_valgrind_profiler,
        use_perftools_profiler=use_perftools_profiler,
        use_tmux=use_tmux)


def wait_for_redis_to_start(redis_ip_address,
                            redis_port,
                            password=None,
                            num_retries=5):
    """"""Wait for a Redis server to be available.

    This is accomplished by creating a Redis client and sending a random
    command to the server until the command gets through.

    Args:
        redis_ip_address (str): The IP address of the redis server.
        redis_port (int): The port of the redis server.
        password (str): The password of the redis server.
        num_retries (int): The number of times to try connecting with redis.
            The client will sleep for one second between attempts.

    Raises:
        Exception: An exception is raised if we could not connect with Redis.
    """"""
    redis_client = redis.StrictRedis(
        host=redis_ip_address, port=redis_port, password=password)
    # Wait for the Redis server to start.
    counter = 0
    while counter < num_retries:
        try:
            # Run some random command and see if it worked.
            logger.info(
                ""Waiting for redis server at {}:{} to respond..."".format(
                    redis_ip_address, redis_port))
            redis_client.client_list()
        except redis.ConnectionError:
            # Wait a little bit.
            time.sleep(1)
            logger.info(""Failed to connect to the redis server, retrying."")
            counter += 1
        else:
            break
    if counter == num_retries:
        raise Exception(""Unable to connect to Redis. If the Redis instance is ""
                        ""on a different machine, check that your firewall is ""
                        ""configured properly."")


def _autodetect_num_gpus():
    """"""Attempt to detect the number of GPUs on this machine.

    TODO(rkn): This currently assumes Nvidia GPUs and Linux.

    Returns:
        The number of GPUs if any were detected, otherwise 0.
    """"""
    proc_gpus_path = ""/proc/driver/nvidia/gpus""
    if os.path.isdir(proc_gpus_path):
        return len(os.listdir(proc_gpus_path))
    return 0


def _compute_version_info():
    """"""Compute the versions of Python, pyarrow, and Ray.

    Returns:
        A tuple containing the version information.
    """"""
    ray_version = ray.__version__
    python_version = ""."".join(map(str, sys.version_info[:3]))
    pyarrow_version = pyarrow.__version__
    return ray_version, python_version, pyarrow_version


def _put_version_info_in_redis(redis_client):
    """"""Store version information in Redis.

    This will be used to detect if workers or drivers are started using
    different versions of Python, pyarrow, or Ray.

    Args:
        redis_client: A client for the primary Redis shard.
    """"""
    redis_client.set(""VERSION_INFO"", json.dumps(_compute_version_info()))


def check_version_info(redis_client):
    """"""Check if various version info of this process is correct.

    This will be used to detect if workers or drivers are started using
    different versions of Python, pyarrow, or Ray. If the version
    information is not present in Redis, then no check is done.

    Args:
        redis_client: A client for the primary Redis shard.

    Raises:
        Exception: An exception is raised if there is a version mismatch.
    """"""
    redis_reply = redis_client.get(""VERSION_INFO"")

    # Don't do the check if there is no version information in Redis. This
    # is to make it easier to do things like start the processes by hand.
    if redis_reply is None:
        return

    true_version_info = tuple(json.loads(ray.utils.decode(redis_reply)))
    version_info = _compute_version_info()
    if version_info != true_version_info:
        node_ip_address = ray.services.get_node_ip_address()
        error_message = (""Version mismatch: The cluster was started with:\n""
                         ""    Ray: "" + true_version_info[0] + ""\n""
                         ""    Python: "" + true_version_info[1] + ""\n""
                         ""    Pyarrow: "" + str(true_version_info[2]) + ""\n""
                         ""This process on node "" + node_ip_address +
                         "" was started with:"" + ""\n""
                         ""    Ray: "" + version_info[0] + ""\n""
                         ""    Python: "" + version_info[1] + ""\n""
                         ""    Pyarrow: "" + str(version_info[2]))
        if version_info[:2] != true_version_info[:2]:
            raise Exception(error_message)
        else:
            logger.warning(error_message)


def start_redis(node_ip_address,
                port=None,
                redis_shard_ports=None,
                num_redis_shards=1,
                redis_max_clients=None,
                redirect_output=False,
                redirect_worker_output=False,
                password=None,
                use_credis=None,
                redis_max_memory=None,
                include_java=False):
    """"""Start the Redis global state store.

    Args:
        node_ip_address: The IP address of the current node. This is only used
            for recording the log filenames in Redis.
        port (int): If provided, the primary Redis shard will be started on
            this port.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_redis_shards (int): If provided, the number of Redis shards to
            start, in addition to the primary one. The default value is one
            shard.
        redis_max_clients: If this is provided, Ray will attempt to configure
            Redis with this maxclients number.
        redirect_output (bool): True if output should be redirected to a file
            and false otherwise.
        redirect_worker_output (bool): True if worker output should be
            redirected to a file and false otherwise. Workers will have access
            to this value when they start up.
        password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        use_credis: If True, additionally load the chain-replicated libraries
            into the redis servers.  Defaults to None, which means its value is
            set by the presence of ""RAY_USE_NEW_GCS"" in os.environ.
        redis_max_memory: The max amount of memory (in bytes) to allow each
            redis shard to use. Once the limit is exceeded, redis will start
            LRU eviction of entries. This only applies to the sharded redis
            tables (task, object, and profile tables). By default, this is
            capped at 10GB but can be set higher.
        include_java (bool): If True, the raylet backend can also support
            Java worker.

    Returns:
        A tuple of the address for the primary Redis shard, a list of
            addresses for the remaining shards, and the processes that were
            started.
    """"""
    redis_stdout_file, redis_stderr_file = new_redis_log_file(redirect_output)

    if redis_shard_ports is None:
        redis_shard_ports = num_redis_shards * [None]
    elif len(redis_shard_ports) != num_redis_shards:
        raise Exception(""The number of Redis shard ports does not match the ""
                        ""number of Redis shards."")

    processes = []

    if use_credis is None:
        use_credis = (""RAY_USE_NEW_GCS"" in os.environ)
    if use_credis:
        if password is not None:
            # TODO(pschafhalter) remove this once credis supports
            # authenticating Redis ports
            raise Exception(""Setting the `redis_password` argument is not ""
                            ""supported in credis. To run Ray with ""
                            ""password-protected Redis ports, ensure that ""
                            ""the environment variable `RAY_USE_NEW_GCS=off`."")
        assert num_redis_shards == 1, (
            ""For now, RAY_USE_NEW_GCS supports 1 shard, and credis ""
            ""supports 1-node chain for that shard only."")

    if use_credis:
        redis_executable = CREDIS_EXECUTABLE
        # TODO(suquark): We need credis here because some symbols need to be
        # imported from credis dynamically through dlopen when Ray is built
        # with RAY_USE_NEW_GCS=on. We should remove them later for the primary
        # shard.
        # See src/ray/gcs/redis_module/ray_redis_module.cc
        redis_modules = [CREDIS_MASTER_MODULE, REDIS_MODULE]
    else:
        redis_executable = REDIS_EXECUTABLE
        redis_modules = [REDIS_MODULE]

    # Start the primary Redis shard.
    port, p = _start_redis_instance(
        redis_executable,
        modules=redis_modules,
        port=port,
        password=password,
        redis_max_clients=redis_max_clients,
        # Below we use None to indicate no limit on the memory of the
        # primary Redis shard.
        redis_max_memory=None,
        stdout_file=redis_stdout_file,
        stderr_file=redis_stderr_file)
    processes.append(p)
    redis_address = address(node_ip_address, port)

    # Register the number of Redis shards in the primary shard, so that clients
    # know how many redis shards to expect under RedisShards.
    primary_redis_client = redis.StrictRedis(
        host=node_ip_address, port=port, password=password)
    primary_redis_client.set(""NumRedisShards"", str(num_redis_shards))

    # Put the redirect_worker_output bool in the Redis shard so that workers
    # can access it and know whether or not to redirect their output.
    primary_redis_client.set(""RedirectOutput"", 1
                             if redirect_worker_output else 0)

    # put the include_java bool to primary redis-server, so that other nodes
    # can access it and know whether or not to enable cross-languages.
    primary_redis_client.set(""INCLUDE_JAVA"", 1 if include_java else 0)

    # Store version information in the primary Redis shard.
    _put_version_info_in_redis(primary_redis_client)

    # Cap the memory of the other redis shards if no limit is provided.
    redis_max_memory = (redis_max_memory if redis_max_memory is not None else
                        ray_constants.DEFAULT_REDIS_MAX_MEMORY_BYTES)
    if redis_max_memory < ray_constants.REDIS_MINIMUM_MEMORY_BYTES:
        raise ValueError(""Attempting to cap Redis memory usage at {} bytes, ""
                         ""but the minimum allowed is {} bytes."".format(
                             redis_max_memory,
                             ray_constants.REDIS_MINIMUM_MEMORY_BYTES))

    # Start other Redis shards. Each Redis shard logs to a separate file,
    # prefixed by ""redis-<shard number>"".
    redis_shards = []
    for i in range(num_redis_shards):
        redis_stdout_file, redis_stderr_file = new_redis_log_file(
            redirect_output, shard_number=i)

        if use_credis:
            redis_executable = CREDIS_EXECUTABLE
            # It is important to load the credis module BEFORE the ray module,
            # as the latter contains an extern declaration that the former
            # supplies.
            redis_modules = [CREDIS_MEMBER_MODULE, REDIS_MODULE]
        else:
            redis_executable = REDIS_EXECUTABLE
            redis_modules = [REDIS_MODULE]

        redis_shard_port, p = _start_redis_instance(
            redis_executable,
            modules=redis_modules,
            port=redis_shard_ports[i],
            password=password,
            redis_max_clients=redis_max_clients,
            redis_max_memory=redis_max_memory,
            stdout_file=redis_stdout_file,
            stderr_file=redis_stderr_file)
        processes.append(p)

        shard_address = address(node_ip_address, redis_shard_port)
        redis_shards.append(shard_address)
        # Store redis shard information in the primary redis shard.
        primary_redis_client.rpush(""RedisShards"", shard_address)

    if use_credis:
        # Configure the chain state. The way it is intended to work is
        # the following:
        #
        # PRIMARY_SHARD
        #
        # SHARD_1 (master replica) -> SHARD_1 (member replica)
        #                                        -> SHARD_1 (member replica)
        #
        # SHARD_2 (master replica) -> SHARD_2 (member replica)
        #                                        -> SHARD_2 (member replica)
        # ...
        #
        #
        # If we have credis members in future, their modules should be:
        # [CREDIS_MEMBER_MODULE, REDIS_MODULE], and they will be initialized by
        # execute_command(""MEMBER.CONNECT_TO_MASTER"", node_ip_address, port)
        #
        # Currently we have num_redis_shards == 1, so only one chain will be
        # created, and the chain only contains master.

        # TODO(suquark): Currently, this is not correct because we are
        # using the master replica as the primary shard. This should be
        # fixed later. I had tried to fix it but failed because of heartbeat
        # issues.
        primary_client = redis.StrictRedis(
            host=node_ip_address, port=port, password=password)
        shard_client = redis.StrictRedis(
            host=node_ip_address, port=redis_shard_port, password=password)
        primary_client.execute_command(""MASTER.ADD"", node_ip_address,
                                       redis_shard_port)
        shard_client.execute_command(""MEMBER.CONNECT_TO_MASTER"",
                                     node_ip_address, port)

    return redis_address, redis_shards, processes


def _start_redis_instance(executable,
                          modules,
                          port=None,
                          redis_max_clients=None,
                          num_retries=20,
                          stdout_file=None,
                          stderr_file=None,
                          password=None,
                          redis_max_memory=None):
    """"""Start a single Redis server.

    Notes:
        If ""port"" is not None, then we will only use this port and try
        only once. Otherwise, random ports will be used and the maximum
        retries count is ""num_retries"".

    Args:
        executable (str): Full path of the redis-server executable.
        modules (list of str): A list of pathnames, pointing to the redis
            module(s) that will be loaded in this redis server.
        port (int): If provided, start a Redis server with this port.
        redis_max_clients: If this is provided, Ray will attempt to configure
            Redis with this maxclients number.
        num_retries (int): The number of times to attempt to start Redis. If a
            port is provided, this defaults to 1.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries.

    Returns:
        A tuple of the port used by Redis and ProcessInfo for the process that
            was started. If a port is passed in, then the returned port value
            is the same.

    Raises:
        Exception: An exception is raised if Redis could not be started.
    """"""
    assert os.path.isfile(executable)
    for module in modules:
        assert os.path.isfile(module)
    counter = 0
    if port is not None:
        # If a port is specified, then try only once to connect.
        # This ensures that we will use the given port.
        num_retries = 1
    else:
        port = new_port()

    load_module_args = []
    for module in modules:
        load_module_args += [""--loadmodule"", module]

    while counter < num_retries:
        if counter > 0:
            logger.warning(""Redis failed to start, retrying now."")

        # Construct the command to start the Redis server.
        command = [executable]
        if password:
            command += [""--requirepass"", password]
        command += (
            [""--port"", str(port), ""--loglevel"", ""warning""] + load_module_args)
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_REDIS_SERVER,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        time.sleep(0.1)
        # Check if Redis successfully started (or at least if it the executable
        # did not exit within 0.1 seconds).
        if process_info.process.poll() is None:
            break
        port = new_port()
        counter += 1
    if counter == num_retries:
        raise Exception(""Couldn't start Redis. Check log files: {} {}"".format(
            stdout_file.name, stderr_file.name))

    # Create a Redis client just for configuring Redis.
    redis_client = redis.StrictRedis(
        host=""127.0.0.1"", port=port, password=password)
    # Wait for the Redis server to start.
    wait_for_redis_to_start(""127.0.0.1"", port, password=password)
    # Configure Redis to generate keyspace notifications. TODO(rkn): Change
    # this to only generate notifications for the export keys.
    redis_client.config_set(""notify-keyspace-events"", ""Kl"")

    # Configure Redis to not run in protected mode so that processes on other
    # hosts can connect to it. TODO(rkn): Do this in a more secure way.
    redis_client.config_set(""protected-mode"", ""no"")

    # Discard old task and object metadata.
    if redis_max_memory is not None:
        redis_client.config_set(""maxmemory"", str(redis_max_memory))
        redis_client.config_set(""maxmemory-policy"", ""allkeys-lru"")
        redis_client.config_set(""maxmemory-samples"", ""10"")
        logger.info(""Starting Redis shard with {} GB max memory."".format(
            round(redis_max_memory / 1e9, 2)))

    # If redis_max_clients is provided, attempt to raise the number of maximum
    # number of Redis clients.
    if redis_max_clients is not None:
        redis_client.config_set(""maxclients"", str(redis_max_clients))
    else:
        # If redis_max_clients is not provided, determine the current ulimit.
        # We will use this to attempt to raise the maximum number of Redis
        # clients.
        current_max_clients = int(
            redis_client.config_get(""maxclients"")[""maxclients""])
        # The below command should be the same as doing ulimit -n.
        ulimit_n = resource.getrlimit(resource.RLIMIT_NOFILE)[0]
        # The quantity redis_client_buffer appears to be the required buffer
        # between the maximum number of redis clients and ulimit -n. That is,
        # if ulimit -n returns 10000, then we can set maxclients to
        # 10000 - redis_client_buffer.
        redis_client_buffer = 32
        if current_max_clients < ulimit_n - redis_client_buffer:
            redis_client.config_set(""maxclients"",
                                    ulimit_n - redis_client_buffer)

    # Increase the hard and soft limits for the redis client pubsub buffer to
    # 128MB. This is a hack to make it less likely for pubsub messages to be
    # dropped and for pubsub connections to therefore be killed.
    cur_config = (redis_client.config_get(""client-output-buffer-limit"")[
        ""client-output-buffer-limit""])
    cur_config_list = cur_config.split()
    assert len(cur_config_list) == 12
    cur_config_list[8:] = [""pubsub"", ""134217728"", ""134217728"", ""60""]
    redis_client.config_set(""client-output-buffer-limit"",
                            "" "".join(cur_config_list))
    # Put a time stamp in Redis to indicate when it was started.
    redis_client.set(""redis_start_time"", time.time())
    return port, process_info


def start_log_monitor(redis_address,
                      stdout_file=None,
                      stderr_file=None,
                      redis_password=None):
    """"""Start a log monitor process.

    Args:
        redis_address (str): The address of the Redis instance.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    log_monitor_filepath = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""log_monitor.py"")
    command = [
        sys.executable, ""-u"", log_monitor_filepath,
        ""--redis-address={}"".format(redis_address), ""--logs-dir={}"".format(
            get_logs_dir_path())
    ]
    if redis_password:
        command += [""--redis-password"", redis_password]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_LOG_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_ui(redis_address, stdout_file=None, stderr_file=None):
    """"""Start a UI process.

    Args:
        redis_address: The address of the primary Redis shard.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        A tuple of the web UI url and ProcessInfo for the process that was
            started.
    """"""

    port = 8888
    while True:
        try:
            port_test_socket = socket.socket()
            port_test_socket.bind((""127.0.0.1"", port))
            port_test_socket.close()
            break
        except socket.error:
            port += 1

    notebook_name = get_ipython_notebook_path()
    new_notebook_directory = os.path.dirname(notebook_name)
    # We generate the token used for authentication ourselves to avoid
    # querying the jupyter server.
    token = ray.utils.decode(binascii.hexlify(os.urandom(24)))
    # The --ip=0.0.0.0 flag is intended to enable connecting to a notebook
    # running within a docker container (from the outside).
    command = [
        ""jupyter"", ""notebook"", ""--no-browser"", ""--port={}"".format(port),
        ""--ip=0.0.0.0"", ""--NotebookApp.iopub_data_rate_limit=10000000000"",
        ""--NotebookApp.open_browser=False"",
        ""--NotebookApp.token={}"".format(token)
    ]
    # If the user is root, add the --allow-root flag.
    if os.geteuid() == 0:
        command.append(""--allow-root"")

    try:
        process_info = start_ray_process(
            command,
            ray_constants.PROCESS_TYPE_WEB_UI,
            env_updates={""REDIS_ADDRESS"": redis_address},
            cwd=new_notebook_directory,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
    except Exception:
        logger.warning(""Failed to start the UI, you may need to run ""
                       ""'pip install jupyter'."")
    else:
        webui_url = (""http://localhost:{}/notebooks/{}?token={}"".format(
            port, os.path.basename(notebook_name), token))
        print(""\n"" + ""="" * 70)
        print(""View the web UI at {}"".format(webui_url))
        print(""="" * 70 + ""\n"")
        return webui_url, process_info
    return None, None


def check_and_update_resources(num_cpus, num_gpus, resources):
    """"""Sanity check a resource dictionary and add sensible defaults.

    Args:
        num_cpus: The number of CPUs.
        num_gpus: The number of GPUs.
        resources: A dictionary mapping resource names to resource quantities.

    Returns:
        A new resource dictionary.
    """"""
    if resources is None:
        resources = {}
    resources = resources.copy()
    assert ""CPU"" not in resources
    assert ""GPU"" not in resources
    if num_cpus is not None:
        resources[""CPU""] = num_cpus
    if num_gpus is not None:
        resources[""GPU""] = num_gpus

    if ""CPU"" not in resources:
        # By default, use the number of hardware execution threads for the
        # number of cores.
        resources[""CPU""] = multiprocessing.cpu_count()

    # See if CUDA_VISIBLE_DEVICES has already been set.
    gpu_ids = ray.utils.get_cuda_visible_devices()

    # Check that the number of GPUs that the local scheduler wants doesn't
    # excede the amount allowed by CUDA_VISIBLE_DEVICES.
    if (""GPU"" in resources and gpu_ids is not None
            and resources[""GPU""] > len(gpu_ids)):
        raise Exception(""Attempting to start local scheduler with {} GPUs, ""
                        ""but CUDA_VISIBLE_DEVICES contains {}."".format(
                            resources[""GPU""], gpu_ids))

    if ""GPU"" not in resources:
        # Try to automatically detect the number of GPUs.
        resources[""GPU""] = _autodetect_num_gpus()
        # Don't use more GPUs than allowed by CUDA_VISIBLE_DEVICES.
        if gpu_ids is not None:
            resources[""GPU""] = min(resources[""GPU""], len(gpu_ids))

    # Check types.
    for _, resource_quantity in resources.items():
        assert (isinstance(resource_quantity, int)
                or isinstance(resource_quantity, float))
        if (isinstance(resource_quantity, float)
                and not resource_quantity.is_integer()):
            raise ValueError(""Resource quantities must all be whole numbers."")

        if resource_quantity > ray_constants.MAX_RESOURCE_QUANTITY:
            raise ValueError(""Resource quantities must be at most {}."".format(
                ray_constants.MAX_RESOURCE_QUANTITY))

    return resources


def start_raylet(redis_address,
                 node_ip_address,
                 raylet_name,
                 plasma_store_name,
                 worker_path,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 redis_password=None,
                 use_valgrind=False,
                 use_profiler=False,
                 stdout_file=None,
                 stderr_file=None,
                 config=None,
                 include_java=False,
                 java_worker_options=None):
    """"""Start a raylet, which is a combined local scheduler and object manager.

    Args:
        redis_address (str): The address of the primary Redis server.
        node_ip_address (str): The IP address of this node.
        raylet_name (str): The name of the raylet socket to create.
        plasma_store_name (str): The name of the plasma store socket to connect
             to.
        worker_path (str): The path of the Python file that new worker
            processes will execute.
        num_cpus: The CPUs allocated for this raylet.
        num_gpus: The GPUs allocated for this raylet.
        resources: The custom resources allocated for this raylet.
        object_manager_port: The port to use for the object manager. If this is
            None, then the object manager will choose its own port.
        node_manager_port: The port to use for the node manager. If this is
            None, then the node manager will choose its own port.
        redis_password: The password to use when connecting to Redis.
        use_valgrind (bool): True if the raylet should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the raylet should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        config (dict|None): Optional Raylet configuration that will
            override defaults in RayConfig.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
    Returns:
        ProcessInfo for the process that was started.
    """"""
    config = config or {}
    config_str = "","".join([""{},{}"".format(*kv) for kv in config.items()])

    if use_valgrind and use_profiler:
        raise Exception(""Cannot use valgrind and profiler at the same time."")

    num_initial_workers = (num_cpus if num_cpus is not None else
                           multiprocessing.cpu_count())

    static_resources = check_and_update_resources(num_cpus, num_gpus,
                                                  resources)

    # Limit the number of workers that can be started in parallel by the
    # raylet. However, make sure it is at least 1.
    maximum_startup_concurrency = max(
        1, min(multiprocessing.cpu_count(), static_resources[""CPU""]))

    # Format the resource argument in a form like 'CPU,1.0,GPU,0,Custom,3'.
    resource_argument = "","".join(
        [""{},{}"".format(*kv) for kv in static_resources.items()])

    gcs_ip_address, gcs_port = redis_address.split("":"")

    if include_java is True:
        java_worker_options = (java_worker_options
                               or DEFAULT_JAVA_WORKER_OPTIONS)
        java_worker_command = build_java_worker_command(
            java_worker_options, redis_address, plasma_store_name, raylet_name)
    else:
        java_worker_command = """"

    # Create the command that the Raylet will use to start workers.
    start_worker_command = (""{} {} ""
                            ""--node-ip-address={} ""
                            ""--object-store-name={} ""
                            ""--raylet-name={} ""
                            ""--redis-address={} ""
                            ""--temp-dir={}"".format(
                                sys.executable, worker_path, node_ip_address,
                                plasma_store_name, raylet_name, redis_address,
                                get_temp_root()))
    if redis_password:
        start_worker_command += "" --redis-password {}"".format(redis_password)

    # If the object manager port is None, then use 0 to cause the object
    # manager to choose its own port.
    if object_manager_port is None:
        object_manager_port = 0
    # If the node manager port is None, then use 0 to cause the node manager
    # to choose its own port.
    if node_manager_port is None:
        node_manager_port = 0

    command = [
        RAYLET_EXECUTABLE,
        raylet_name,
        plasma_store_name,
        str(object_manager_port),
        str(node_manager_port),
        node_ip_address,
        gcs_ip_address,
        gcs_port,
        str(num_initial_workers),
        str(maximum_startup_concurrency),
        resource_argument,
        config_str,
        start_worker_command,
        java_worker_command,
        redis_password or """",
        get_temp_root(),
    ]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET,
        use_valgrind=use_valgrind,
        use_gdb=False,
        use_valgrind_profiler=use_profiler,
        use_perftools_profiler=(""RAYLET_PERFTOOLS_PATH"" in os.environ),
        stdout_file=stdout_file,
        stderr_file=stderr_file)

    return process_info


def build_java_worker_command(java_worker_options, redis_address,
                              plasma_store_name, raylet_name):
    """"""This method assembles the command used to start a Java worker.

    Args:
        java_worker_options (str): The command options for Java worker.
        redis_address (str): Redis address of GCS.
        plasma_store_name (str): The name of the plasma store socket to connect
           to.
        raylet_name (str): The name of the raylet socket to create.

    Returns:
        The command string for starting Java worker.
    """"""
    assert java_worker_options is not None

    command = ""java {} "".format(java_worker_options)
    if redis_address is not None:
        command += ""-Dray.redis.address={} "".format(redis_address)

    if plasma_store_name is not None:
        command += (
            ""-Dray.object-store.socket-name={} "".format(plasma_store_name))

    if raylet_name is not None:
        command += ""-Dray.raylet.socket-name={} "".format(raylet_name)

    command += ""-Dray.home={} "".format(RAY_HOME)
    command += ""-Dray.log-dir={} "".format(get_logs_dir_path())
    command += ""org.ray.runtime.runner.worker.DefaultWorker""

    return command


def determine_plasma_store_config(object_store_memory=None,
                                  plasma_directory=None,
                                  huge_pages=False):
    """"""Figure out how to configure the plasma object store.

    This will determine which directory to use for the plasma store (e.g.,
    /tmp or /dev/shm) and how much memory to start the store with. On Linux,
    we will try to use /dev/shm unless the shared memory file system is too
    small, in which case we will fall back to /tmp. If any of the object store
    memory or plasma directory parameters are specified by the user, then those
    values will be preserved.

    Args:
        object_store_memory (int): The user-specified object store memory
            parameter.
        plasma_directory (str): The user-specified plasma directory parameter.
        huge_pages (bool): The user-specified huge pages parameter.

    Returns:
        A tuple of the object store memory to use and the plasma directory to
            use. If either of these values is specified by the user, then that
            value will be preserved.
    """"""
    system_memory = ray.utils.get_system_memory()

    # Choose a default object store size.
    if object_store_memory is None:
        object_store_memory = int(system_memory * 0.4)
        # Cap memory to avoid memory waste and perf issues on large nodes
        if (object_store_memory >
                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES):
            logger.warning(
                ""Warning: Capping object memory store to {}GB. "".format(
                    ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES // 1e9)
                + ""To increase this further, specify `object_store_memory` ""
                ""when calling ray.init() or ray start."")
            object_store_memory = (
                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES)

    # Determine which directory to use. By default, use /tmp on MacOS and
    # /dev/shm on Linux, unless the shared-memory file system is too small,
    # in which case we default to /tmp on Linux.
    if plasma_directory is None:
        if sys.platform == ""linux"" or sys.platform == ""linux2"":
            shm_avail = ray.utils.get_shared_memory_bytes()
            # Compare the requested memory size to the memory available in
            # /dev/shm.
            if shm_avail > object_store_memory:
                plasma_directory = ""/dev/shm""
            else:
                plasma_directory = ""/tmp""
                logger.warning(
                    ""WARNING: The object store is using /tmp instead of ""
                    ""/dev/shm because /dev/shm has only {} bytes available. ""
                    ""This may slow down performance! You may be able to free ""
                    ""up space by deleting files in /dev/shm or terminating ""
                    ""any running plasma_store_server processes. If you are ""
                    ""inside a Docker container, you may need to pass an ""
                    ""argument with the flag '--shm-size' to 'docker run'."".
                    format(shm_avail))
        else:
            plasma_directory = ""/tmp""

        # Do some sanity checks.
        if object_store_memory > system_memory:
            raise Exception(
                ""The requested object store memory size is greater ""
                ""than the total available memory."")
    else:
        plasma_directory = os.path.abspath(plasma_directory)
        logger.warning(""WARNING: object_store_memory is not verified when ""
                       ""plasma_directory is set."")

    if not os.path.isdir(plasma_directory):
        raise Exception(
            ""The file {} does not exist or is not a directory."".format(
                plasma_directory))

    return object_store_memory, plasma_directory


def _start_plasma_store(plasma_store_memory,
                        use_valgrind=False,
                        use_profiler=False,
                        stdout_file=None,
                        stderr_file=None,
                        plasma_directory=None,
                        huge_pages=False,
                        socket_name=None):
    """"""Start a plasma store process.

    Args:
        plasma_store_memory (int): The amount of memory in bytes to start the
            plasma store with.
        use_valgrind (bool): True if the plasma store should be started inside
            of valgrind. If this is True, use_profiler must be False.
        use_profiler (bool): True if the plasma store should be started inside
            a profiler. If this is True, use_valgrind must be False.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: a boolean flag indicating whether to start the
            Object Store with hugetlbfs support. Requires plasma_directory.
        socket_name (str): If provided, it will specify the socket
            name used by the plasma store.

    Return:
        A tuple of the name of the plasma store socket and ProcessInfo for the
            plasma store process.
    """"""
    if use_valgrind and use_profiler:
        raise Exception(""Cannot use valgrind and profiler at the same time."")

    if huge_pages and not (sys.platform == ""linux""
                           or sys.platform == ""linux2""):
        raise Exception(""The huge_pages argument is only supported on ""
                        ""Linux."")

    if huge_pages and plasma_directory is None:
        raise Exception(""If huge_pages is True, then the ""
                        ""plasma_directory argument must be provided."")

    if not isinstance(plasma_store_memory, int):
        raise Exception(""plasma_store_memory should be an integer."")

    command = [
        PLASMA_STORE_EXECUTABLE, ""-s"", socket_name, ""-m"",
        str(plasma_store_memory)
    ]
    if plasma_directory is not None:
        command += [""-d"", plasma_directory]
    if huge_pages:
        command += [""-h""]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_PLASMA_STORE,
        use_valgrind=use_valgrind,
        use_valgrind_profiler=use_profiler,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_plasma_store(node_ip_address,
                       redis_address,
                       stdout_file=None,
                       stderr_file=None,
                       object_store_memory=None,
                       plasma_directory=None,
                       huge_pages=False,
                       plasma_store_socket_name=None):
    """"""This method starts an object store process.

    Args:
        node_ip_address (str): The IP address of the node running the object
            store.
        redis_address (str): The address of the Redis instance to connect to.
        stdout_file: A file handle opened for writing to redirect stdout
            to. If no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr
            to. If no redirection should happen, then this should be None.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    object_store_memory, plasma_directory = determine_plasma_store_config(
        object_store_memory, plasma_directory, huge_pages)

    if object_store_memory < ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES:
        raise ValueError(""Attempting to cap object store memory usage at {} ""
                         ""bytes, but the minimum allowed is {} bytes."".format(
                             object_store_memory,
                             ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES))

    # Print the object store memory using two decimal places.
    object_store_memory_str = (object_store_memory / 10**7) / 10**2
    logger.info(""Starting the Plasma object store with {} GB memory ""
                ""using {}."".format(object_store_memory_str, plasma_directory))
    # Start the Plasma store.
    process_info = _start_plasma_store(
        object_store_memory,
        use_profiler=RUN_PLASMA_STORE_PROFILER,
        stdout_file=stdout_file,
        stderr_file=stderr_file,
        plasma_directory=plasma_directory,
        huge_pages=huge_pages,
        socket_name=plasma_store_socket_name)

    return process_info


def start_worker(node_ip_address,
                 object_store_name,
                 raylet_name,
                 redis_address,
                 worker_path,
                 stdout_file=None,
                 stderr_file=None):
    """"""This method starts a worker process.

    Args:
        node_ip_address (str): The IP address of the node that this worker is
            running on.
        object_store_name (str): The socket name of the object store.
        raylet_name (str): The socket name of the raylet server.
        redis_address (str): The address that the Redis server is listening on.
        worker_path (str): The path of the source code which the worker process
            will run.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    command = [
        sys.executable, ""-u"", worker_path,
        ""--node-ip-address="" + node_ip_address,
        ""--object-store-name="" + object_store_name,
        ""--raylet-name="" + raylet_name,
        ""--redis-address="" + str(redis_address),
        ""--temp-dir="" + get_temp_root()
    ]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_WORKER,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_monitor(redis_address,
                  node_ip_address,
                  stdout_file=None,
                  stderr_file=None,
                  autoscaling_config=None,
                  redis_password=None):
    """"""Run a process to monitor the other processes.

    Args:
        redis_address (str): The address that the Redis server is listening on.
        node_ip_address: The IP address of the node that this process will run
            on.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        autoscaling_config: path to autoscaling config file.
        redis_password (str): The password of the redis server.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    monitor_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), ""monitor.py"")
    command = [
        sys.executable, ""-u"", monitor_path,
        ""--redis-address="" + str(redis_address)
    ]
    if autoscaling_config:
        command.append(""--autoscaling-config="" + str(autoscaling_config))
    if redis_password:
        command.append(""--redis-password="" + redis_password)
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info


def start_raylet_monitor(redis_address,
                         stdout_file=None,
                         stderr_file=None,
                         redis_password=None,
                         config=None):
    """"""Run a process to monitor the other processes.

    Args:
        redis_address (str): The address that the Redis server is listening on.
        stdout_file: A file handle opened for writing to redirect stdout to. If
            no redirection should happen, then this should be None.
        stderr_file: A file handle opened for writing to redirect stderr to. If
            no redirection should happen, then this should be None.
        redis_password (str): The password of the redis server.
        config (dict|None): Optional configuration that will
            override defaults in RayConfig.

    Returns:
        ProcessInfo for the process that was started.
    """"""
    gcs_ip_address, gcs_port = redis_address.split("":"")
    redis_password = redis_password or """"
    config = config or {}
    config_str = "","".join([""{},{}"".format(*kv) for kv in config.items()])
    command = [RAYLET_MONITOR_EXECUTABLE, gcs_ip_address, gcs_port, config_str]
    if redis_password:
        command += [redis_password]
    process_info = start_ray_process(
        command,
        ray_constants.PROCESS_TYPE_RAYLET_MONITOR,
        stdout_file=stdout_file,
        stderr_file=stderr_file)
    return process_info
/n/n/npython/ray/utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import functools
import hashlib
import inspect
import logging
import numpy as np
import os
import subprocess
import sys
import threading
import time
import uuid

import ray.gcs_utils
import ray.ray_constants as ray_constants


def _random_string():
    id_hash = hashlib.sha1()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest()
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes


def format_error_message(exception_message, task_exception=False):
    """"""Improve the formatting of an exception thrown by a remote function.

    This method takes a traceback from an exception and makes it nicer by
    removing a few uninformative lines and adding some space to indent the
    remaining lines nicely.

    Args:
        exception_message (str): A message generated by traceback.format_exc().

    Returns:
        A string of the formatted exception message.
    """"""
    lines = exception_message.split(""\n"")
    if task_exception:
        # For errors that occur inside of tasks, remove lines 1 and 2 which are
        # always the same, they just contain information about the worker code.
        lines = lines[0:1] + lines[3:]
        pass
    return ""\n"".join(lines)


def push_error_to_driver(worker, error_type, message, driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Args:
        worker: The worker to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    worker.raylet_client.push_error(driver_id, error_type, message,
                                    time.time())


def push_error_to_driver_through_redis(redis_client,
                                       error_type,
                                       message,
                                       driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Normally the push_error_to_driver function should be used. However, in some
    instances, the local scheduler client is not available, e.g., because the
    error happens in Python before the driver or worker has connected to the
    backend processes.

    Args:
        redis_client: The redis client to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    # Do everything in Python and through the Python Redis client instead
    # of through the raylet.
    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,
                                                       message, time.time())
    redis_client.execute_command(""RAY.TABLE_APPEND"",
                                 ray.gcs_utils.TablePrefix.ERROR_INFO,
                                 ray.gcs_utils.TablePubsub.ERROR_INFO,
                                 driver_id.binary(), error_data)


def is_cython(obj):
    """"""Check if an object is a Cython function or method""""""

    # TODO(suo): We could split these into two functions, one for Cython
    # functions and another for Cython methods.
    # TODO(suo): There doesn't appear to be a Cython function 'type' we can
    # check against via isinstance. Please correct me if I'm wrong.
    def check_cython(x):
        return type(x).__name__ == ""cython_function_or_method""

    # Check if function or method, respectively
    return check_cython(obj) or \
        (hasattr(obj, ""__func__"") and check_cython(obj.__func__))


def is_function_or_method(obj):
    """"""Check if an object is a function or method.

    Args:
        obj: The Python object in question.

    Returns:
        True if the object is an function or method.
    """"""
    return inspect.isfunction(obj) or inspect.ismethod(obj) or is_cython(obj)


def is_class_method(f):
    """"""Returns whether the given method is a class_method.""""""
    return hasattr(f, ""__self__"") and f.__self__ is not None


def random_string():
    """"""Generate a random string to use as an ID.

    Note that users may seed numpy, which could cause this function to generate
    duplicate IDs. Therefore, we need to seed numpy ourselves, but we can't
    interfere with the state of the user's random number generator, so we
    extract the state of the random number generator and reset it after we are
    done.

    TODO(rkn): If we want to later guarantee that these are generated in a
    deterministic manner, then we will need to make some changes here.

    Returns:
        A random byte string of length ray_constants.ID_SIZE.
    """"""
    # Get the state of the numpy random number generator.
    numpy_state = np.random.get_state()
    # Try to use true randomness.
    np.random.seed(None)
    # Generate the random ID.
    random_id = np.random.bytes(ray_constants.ID_SIZE)
    # Reset the state of the numpy random number generator.
    np.random.set_state(numpy_state)
    return random_id


def decode(byte_str, allow_none=False):
    """"""Make this unicode in Python 3, otherwise leave it as bytes.

    Args:
        byte_str: The byte string to decode.
        allow_none: If true, then we will allow byte_str to be None in which
            case we will return an empty string. TODO(rkn): Remove this flag.
            This is only here to simplify upgrading to flatbuffers 1.10.0.

    Returns:
        A byte string in Python 2 and a unicode string in Python 3.
    """"""
    if byte_str is None and allow_none:
        return """"

    if not isinstance(byte_str, bytes):
        raise ValueError(
            ""The argument {} must be a bytes object."".format(byte_str))
    if sys.version_info >= (3, 0):
        return byte_str.decode(""ascii"")
    else:
        return byte_str


def binary_to_object_id(binary_object_id):
    return ray.ObjectID(binary_object_id)


def binary_to_hex(identifier):
    hex_identifier = binascii.hexlify(identifier)
    if sys.version_info >= (3, 0):
        hex_identifier = hex_identifier.decode()
    return hex_identifier


def hex_to_binary(hex_identifier):
    return binascii.unhexlify(hex_identifier)


def get_cuda_visible_devices():
    """"""Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable.

    Returns:
        if CUDA_VISIBLE_DEVICES is set, this returns a list of integers with
            the IDs of the GPUs. If it is not set, this returns None.
    """"""
    gpu_ids_str = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    if gpu_ids_str is None:
        return None

    if gpu_ids_str == """":
        return []

    return [int(i) for i in gpu_ids_str.split("","")]


def set_cuda_visible_devices(gpu_ids):
    """"""Set the CUDA_VISIBLE_DEVICES environment variable.

    Args:
        gpu_ids: This is a list of integers representing GPU IDs.
    """"""
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join([str(i) for i in gpu_ids])


def resources_from_resource_arguments(default_num_cpus, default_num_gpus,
                                      default_resources, runtime_num_cpus,
                                      runtime_num_gpus, runtime_resources):
    """"""Determine a task's resource requirements.

    Args:
        default_num_cpus: The default number of CPUs required by this function
            or actor method.
        default_num_gpus: The default number of GPUs required by this function
            or actor method.
        default_resources: The default custom resources required by this
            function or actor method.
        runtime_num_cpus: The number of CPUs requested when the task was
            invoked.
        runtime_num_gpus: The number of GPUs requested when the task was
            invoked.
        runtime_resources: The custom resources requested when the task was
            invoked.

    Returns:
        A dictionary of the resource requirements for the task.
    """"""
    if runtime_resources is not None:
        resources = runtime_resources.copy()
    elif default_resources is not None:
        resources = default_resources.copy()
    else:
        resources = {}

    if ""CPU"" in resources or ""GPU"" in resources:
        raise ValueError(""The resources dictionary must not ""
                         ""contain the key 'CPU' or 'GPU'"")

    assert default_num_cpus is not None
    resources[""CPU""] = (default_num_cpus
                        if runtime_num_cpus is None else runtime_num_cpus)

    if runtime_num_gpus is not None:
        resources[""GPU""] = runtime_num_gpus
    elif default_num_gpus is not None:
        resources[""GPU""] = default_num_gpus

    return resources


_default_handler = None


def setup_logger(logging_level, logging_format):
    """"""Setup default logging for ray.""""""
    logger = logging.getLogger(""ray"")
    if type(logging_level) is str:
        logging_level = logging.getLevelName(logging_level.upper())
    logger.setLevel(logging_level)
    global _default_handler
    if _default_handler is None:
        _default_handler = logging.StreamHandler()
        logger.addHandler(_default_handler)
    _default_handler.setFormatter(logging.Formatter(logging_format))
    logger.propagate = False


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5bd44f8afcecbfb0db479ce230c790fc2c56569a/psutil/tests/test_linux.py#L132-L138  # noqa: E501
def vmstat(stat):
    """"""Run vmstat and get a particular statistic.

    Args:
        stat: The statistic that we are interested in retrieving.

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output([""vmstat"", ""-s""])
    stat = stat.encode(""ascii"")
    for line in out.split(b""\n""):
        line = line.strip()
        if stat in line:
            return int(line.split(b"" "")[0])
    raise ValueError(""Can't find {} in 'vmstat' output."".format(stat))


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5e90b0a7f3fccb177445a186cc4fac62cfadb510/psutil/tests/test_osx.py#L29-L38  # noqa: E501
def sysctl(command):
    """"""Run a sysctl command and parse the output.

    Args:
        command: A sysctl command with an argument, for example,
            [""sysctl"", ""hw.memsize""].

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output(command)
    result = out.split(b"" "")[1]
    try:
        return int(result)
    except ValueError:
        return result


def get_system_memory():
    """"""Return the total amount of system memory in bytes.

    Returns:
        The total amount of system memory in bytes.
    """"""
    # Try to accurately figure out the memory limit if we are in a docker
    # container. Note that this file is not specific to Docker and its value is
    # often much larger than the actual amount of memory.
    docker_limit = None
    memory_limit_filename = ""/sys/fs/cgroup/memory/memory.limit_in_bytes""
    if os.path.exists(memory_limit_filename):
        with open(memory_limit_filename, ""r"") as f:
            docker_limit = int(f.read())

    # Use psutil if it is available.
    psutil_memory_in_bytes = None
    try:
        import psutil
        psutil_memory_in_bytes = psutil.virtual_memory().total
    except ImportError:
        pass

    if psutil_memory_in_bytes is not None:
        memory_in_bytes = psutil_memory_in_bytes
    elif sys.platform == ""linux"" or sys.platform == ""linux2"":
        # Handle Linux.
        bytes_in_kilobyte = 1024
        memory_in_bytes = vmstat(""total memory"") * bytes_in_kilobyte
    else:
        # Handle MacOS.
        memory_in_bytes = sysctl([""sysctl"", ""hw.memsize""])

    if docker_limit is not None:
        return min(docker_limit, memory_in_bytes)
    else:
        return memory_in_bytes


def get_shared_memory_bytes():
    """"""Get the size of the shared memory file system.

    Returns:
        The size of the shared memory file system in bytes.
    """"""
    # Make sure this is only called on Linux.
    assert sys.platform == ""linux"" or sys.platform == ""linux2""

    shm_fd = os.open(""/dev/shm"", os.O_RDONLY)
    try:
        shm_fs_stats = os.fstatvfs(shm_fd)
        # The value shm_fs_stats.f_bsize is the block size and the
        # value shm_fs_stats.f_bavail is the number of available
        # blocks.
        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail
    finally:
        os.close(shm_fd)

    return shm_avail


def check_oversized_pickle(pickled, name, obj_type, worker):
    """"""Send a warning message if the pickled object is too large.

    Args:
        pickled: the pickled object.
        name: name of the pickled object.
        obj_type: type of the pickled object, can be 'function',
            'remote function', 'actor', or 'object'.
        worker: the worker used to send warning message.
    """"""
    length = len(pickled)
    if length <= ray_constants.PICKLE_OBJECT_WARNING_SIZE:
        return
    warning_message = (
        ""Warning: The {} {} has size {} when pickled. ""
        ""It will be stored in Redis, which could cause memory issues. ""
        ""This may mean that its definition uses a large array or other object.""
    ).format(obj_type, name, length)
    push_error_to_driver(
        worker,
        ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,
        warning_message,
        driver_id=worker.task_driver_id)


class _ThreadSafeProxy(object):
    """"""This class is used to create a thread-safe proxy for a given object.
        Every method call will be guarded with a lock.

    Attributes:
        orig_obj (object): the original object.
        lock (threading.Lock): the lock object.
        _wrapper_cache (dict): a cache from original object's methods to
            the proxy methods.
    """"""

    def __init__(self, orig_obj, lock):
        self.orig_obj = orig_obj
        self.lock = lock
        self._wrapper_cache = {}

    def __getattr__(self, attr):
        orig_attr = getattr(self.orig_obj, attr)
        if not callable(orig_attr):
            # If the original attr is a field, just return it.
            return orig_attr
        else:
            # If the orginal attr is a method,
            # return a wrapper that guards the original method with a lock.
            wrapper = self._wrapper_cache.get(attr)
            if wrapper is None:

                @functools.wraps(orig_attr)
                def _wrapper(*args, **kwargs):
                    with self.lock:
                        return orig_attr(*args, **kwargs)

                self._wrapper_cache[attr] = _wrapper
                wrapper = _wrapper
            return wrapper


def thread_safe_client(client, lock=None):
    """"""Create a thread-safe proxy which locks every method call
    for the given client.

    Args:
        client: the client object to be guarded.
        lock: the lock object that will be used to lock client's methods.
            If None, a new lock will be used.

    Returns:
        A thread-safe proxy for the given client.
    """"""
    if lock is None:
        lock = threading.Lock()
    return _ThreadSafeProxy(client, lock)


def is_main_thread():
    return threading.current_thread().getName() == ""MainThread""
/n/n/npython/ray/worker.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from contextlib import contextmanager
import atexit
import colorama
import faulthandler
import hashlib
import inspect
import logging
import numpy as np
import os
import redis
import signal
from six.moves import queue
import sys
import threading
import time
import traceback

# Ray modules
import pyarrow
import pyarrow.plasma as plasma
import ray.cloudpickle as pickle
import ray.experimental.state as state
import ray.gcs_utils
import ray.memory_monitor as memory_monitor
import ray.node
import ray.remote_function
import ray.serialization as serialization
import ray.services as services
import ray.signature
import ray.tempfile_services as tempfile_services
import ray.ray_constants as ray_constants
from ray import import_thread
from ray import ObjectID, DriverID, ActorID, ActorHandleID, ClientID, TaskID
from ray import profiling
from ray.function_manager import (FunctionActorManager, FunctionDescriptor)
import ray.parameter
from ray.utils import (check_oversized_pickle, is_cython, random_string,
                       thread_safe_client, setup_logger)

SCRIPT_MODE = 0
WORKER_MODE = 1
LOCAL_MODE = 2
PYTHON_MODE = 3

ERROR_KEY_PREFIX = b""Error:""

# Default resource requirements for actors when no resource requirements are
# specified.
DEFAULT_ACTOR_METHOD_CPUS_SIMPLE_CASE = 1
DEFAULT_ACTOR_CREATION_CPUS_SIMPLE_CASE = 0
# Default resource requirements for actors when some resource requirements are
# specified.
DEFAULT_ACTOR_METHOD_CPUS_SPECIFIED_CASE = 0
DEFAULT_ACTOR_CREATION_CPUS_SPECIFIED_CASE = 1

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)

try:
    import setproctitle
except ImportError:
    setproctitle = None


class RayTaskError(Exception):
    """"""An object used internally to represent a task that threw an exception.

    If a task throws an exception during execution, a RayTaskError is stored in
    the object store for each of the task's outputs. When an object is
    retrieved from the object store, the Python method that retrieved it checks
    to see if the object is a RayTaskError and if it is then an exception is
    thrown propagating the error message.

    Currently, we either use the exception attribute or the traceback attribute
    but not both.

    Attributes:
        function_name (str): The name of the function that failed and produced
            the RayTaskError.
        traceback_str (str): The traceback from the exception.
    """"""

    def __init__(self, function_name, traceback_str):
        """"""Initialize a RayTaskError.""""""
        if setproctitle:
            self.proctitle = setproctitle.getproctitle()
        else:
            self.proctitle = ""ray_worker""
        self.pid = os.getpid()
        self.host = os.uname()[1]
        self.function_name = function_name
        self.traceback_str = traceback_str
        assert traceback_str is not None

    def __str__(self):
        """"""Format a RayTaskError as a string.""""""
        lines = self.traceback_str.split(""\n"")
        out = []
        in_worker = False
        for line in lines:
            if line.startswith(""Traceback ""):
                out.append(""{}{}{} (pid={}, host={})"".format(
                    colorama.Fore.CYAN, self.proctitle, colorama.Fore.RESET,
                    self.pid, self.host))
            elif in_worker:
                in_worker = False
            elif ""ray/worker.py"" in line or ""ray/function_manager.py"" in line:
                in_worker = True
            else:
                out.append(line)
        return ""\n"".join(out)


class Worker(object):
    """"""A class used to define the control flow of a worker process.

    Note:
        The methods in this class are considered unexposed to the user. The
        functions outside of this class are considered exposed.

    Attributes:
        connected (bool): True if Ray has been started and False otherwise.
        mode: The mode of the worker. One of SCRIPT_MODE, LOCAL_MODE, and
            WORKER_MODE.
        cached_functions_to_run (List): A list of functions to run on all of
            the workers that should be exported as soon as connect is called.
        profiler: the profiler used to aggregate profiling information.
    """"""

    def __init__(self):
        """"""Initialize a Worker object.""""""
        self.connected = False
        self.mode = None
        self.cached_functions_to_run = []
        self.actor_init_error = None
        self.make_actor = None
        self.actors = {}
        self.actor_task_counter = 0
        # The number of threads Plasma should use when putting an object in the
        # object store.
        self.memcopy_threads = 12
        # When the worker is constructed. Record the original value of the
        # CUDA_VISIBLE_DEVICES environment variable.
        self.original_gpu_ids = ray.utils.get_cuda_visible_devices()
        self.profiler = None
        self.memory_monitor = memory_monitor.MemoryMonitor()
        # A dictionary that maps from driver id to SerializationContext
        # TODO: clean up the SerializationContext once the job finished.
        self.serialization_context_map = {}
        self.function_actor_manager = FunctionActorManager(self)
        # Identity of the driver that this worker is processing.
        # It is a DriverID.
        self.task_driver_id = DriverID.nil()
        self._task_context = threading.local()
        # This event is checked regularly by all of the threads so that they
        # know when to exit.
        self.threads_stopped = threading.Event()

    @property
    def task_context(self):
        """"""A thread-local that contains the following attributes.

        current_task_id: For the main thread, this field is the ID of this
            worker's current running task; for other threads, this field is a
            fake random ID.
        task_index: The number of tasks that have been submitted from the
            current task.
        put_index: The number of objects that have been put from the current
            task.
        """"""
        if not hasattr(self._task_context, 'initialized'):
            # Initialize task_context for the current thread.
            if ray.utils.is_main_thread():
                # If this is running on the main thread, initialize it to
                # NIL. The actual value will set when the worker receives
                # a task from raylet backend.
                self._task_context.current_task_id = TaskID.nil()
            else:
                # If this is running on a separate thread, then the mapping
                # to the current task ID may not be correct. Generate a
                # random task ID so that the backend can differentiate
                # between different threads.
                self._task_context.current_task_id = TaskID(random_string())
                if getattr(self, '_multithreading_warned', False) is not True:
                    logger.warning(
                        ""Calling ray.get or ray.wait in a separate thread ""
                        ""may lead to deadlock if the main thread blocks on ""
                        ""this thread and there are not enough resources to ""
                        ""execute more tasks"")
                    self._multithreading_warned = True

            self._task_context.task_index = 0
            self._task_context.put_index = 1
            self._task_context.initialized = True
        return self._task_context

    @property
    def current_task_id(self):
        return self.task_context.current_task_id

    def mark_actor_init_failed(self, error):
        """"""Called to mark this actor as failed during initialization.""""""

        self.actor_init_error = error

    def reraise_actor_init_error(self):
        """"""Raises any previous actor initialization error.""""""

        if self.actor_init_error is not None:
            raise self.actor_init_error

    def get_serialization_context(self, driver_id):
        """"""Get the SerializationContext of the driver that this worker is processing.

        Args:
            driver_id: The ID of the driver that indicates which driver to get
                the serialization context for.

        Returns:
            The serialization context of the given driver.
        """"""
        if driver_id not in self.serialization_context_map:
            _initialize_serialization(driver_id)
        return self.serialization_context_map[driver_id]

    def check_connected(self):
        """"""Check if the worker is connected.

        Raises:
          Exception: An exception is raised if the worker is not connected.
        """"""
        if not self.connected:
            raise RayConnectionError(""Ray has not been started yet. You can ""
                                     ""start Ray with 'ray.init()'."")

    def set_mode(self, mode):
        """"""Set the mode of the worker.

        The mode SCRIPT_MODE should be used if this Worker is a driver that is
        being run as a Python script or interactively in a shell. It will print
        information about task failures.

        The mode WORKER_MODE should be used if this Worker is not a driver. It
        will not print information about tasks.

        The mode LOCAL_MODE should be used if this Worker is a driver and if
        you want to run the driver in a manner equivalent to serial Python for
        debugging purposes. It will not send remote function calls to the
        scheduler and will insead execute them in a blocking fashion.

        Args:
            mode: One of SCRIPT_MODE, WORKER_MODE, and LOCAL_MODE.
        """"""
        self.mode = mode

    def store_and_register(self, object_id, value, depth=100):
        """"""Store an object and attempt to register its class if needed.

        Args:
            object_id: The ID of the object to store.
            value: The value to put in the object store.
            depth: The maximum number of classes to recursively register.

        Raises:
            Exception: An exception is raised if the attempt to store the
                object fails. This can happen if there is already an object
                with the same ID in the object store or if the object store is
                full.
        """"""
        counter = 0
        while True:
            if counter == depth:
                raise Exception(""Ray exceeded the maximum number of classes ""
                                ""that it will recursively serialize when ""
                                ""attempting to serialize an object of ""
                                ""type {}."".format(type(value)))
            counter += 1
            try:
                self.plasma_client.put(
                    value,
                    object_id=pyarrow.plasma.ObjectID(object_id.binary()),
                    memcopy_threads=self.memcopy_threads,
                    serialization_context=self.get_serialization_context(
                        self.task_driver_id))
                break
            except pyarrow.SerializationCallbackError as e:
                try:
                    register_custom_serializer(
                        type(e.example_object), use_dict=True)
                    warning_message = (""WARNING: Serializing objects of type ""
                                       ""{} by expanding them as dictionaries ""
                                       ""of their fields. This behavior may ""
                                       ""be incorrect in some cases."".format(
                                           type(e.example_object)))
                    logger.debug(warning_message)
                except (serialization.RayNotDictionarySerializable,
                        serialization.CloudPickleError,
                        pickle.pickle.PicklingError, Exception):
                    # We also handle generic exceptions here because
                    # cloudpickle can fail with many different types of errors.
                    try:
                        register_custom_serializer(
                            type(e.example_object), use_pickle=True)
                        warning_message = (""WARNING: Falling back to ""
                                           ""serializing objects of type {} by ""
                                           ""using pickle. This may be ""
                                           ""inefficient."".format(
                                               type(e.example_object)))
                        logger.warning(warning_message)
                    except serialization.CloudPickleError:
                        register_custom_serializer(
                            type(e.example_object),
                            use_pickle=True,
                            local=True)
                        warning_message = (""WARNING: Pickling the class {} ""
                                           ""failed, so we are using pickle ""
                                           ""and only registering the class ""
                                           ""locally."".format(
                                               type(e.example_object)))
                        logger.warning(warning_message)

    def put_object(self, object_id, value):
        """"""Put value in the local object store with object id objectid.

        This assumes that the value for objectid has not yet been placed in the
        local object store.

        Args:
            object_id (object_id.ObjectID): The object ID of the value to be
                put.
            value: The value to put in the object store.

        Raises:
            Exception: An exception is raised if the attempt to store the
                object fails. This can happen if there is already an object
                with the same ID in the object store or if the object store is
                full.
        """"""
        # Make sure that the value is not an object ID.
        if isinstance(value, ObjectID):
            raise Exception(
                ""Calling 'put' on an ray.ObjectID is not allowed ""
                ""(similarly, returning an ray.ObjectID from a remote ""
                ""function is not allowed). If you really want to ""
                ""do this, you can wrap the ray.ObjectID in a list and ""
                ""call 'put' on it (or return it)."")

        # Serialize and put the object in the object store.
        try:
            self.store_and_register(object_id, value)
        except pyarrow.PlasmaObjectExists:
            # The object already exists in the object store, so there is no
            # need to add it again. TODO(rkn): We need to compare the hashes
            # and make sure that the objects are in fact the same. We also
            # should return an error code to the caller instead of printing a
            # message.
            logger.info(
                ""The object with ID {} already exists in the object store.""
                .format(object_id))
        except TypeError:
            # This error can happen because one of the members of the object
            # may not be serializable for cloudpickle. So we need these extra
            # fallbacks here to start from the beginning. Hopefully the object
            # could have a `__reduce__` method.
            register_custom_serializer(type(value), use_pickle=True)
            warning_message = (""WARNING: Serializing the class {} failed, ""
                               ""so are are falling back to cloudpickle.""
                               .format(type(value)))
            logger.warning(warning_message)
            self.store_and_register(object_id, value)

    def retrieve_and_deserialize(self, object_ids, timeout, error_timeout=10):
        start_time = time.time()
        # Only send the warning once.
        warning_sent = False
        while True:
            try:
                # We divide very large get requests into smaller get requests
                # so that a single get request doesn't block the store for a
                # long time, if the store is blocked, it can block the manager
                # as well as a consequence.
                results = []
                for i in range(0, len(object_ids),
                               ray._config.worker_get_request_size()):
                    results += self.plasma_client.get(
                        object_ids[i:(
                            i + ray._config.worker_get_request_size())],
                        timeout,
                        self.get_serialization_context(self.task_driver_id))
                return results
            except pyarrow.lib.ArrowInvalid:
                # TODO(ekl): the local scheduler could include relevant
                # metadata in the task kill case for a better error message
                invalid_error = RayTaskError(
                    ""<unknown>"",
                    ""Invalid return value: likely worker died or was killed ""
                    ""while executing the task; check previous logs or dmesg ""
                    ""for errors."")
                return [invalid_error] * len(object_ids)
            except pyarrow.DeserializationCallbackError:
                # Wait a little bit for the import thread to import the class.
                # If we currently have the worker lock, we need to release it
                # so that the import thread can acquire it.
                if self.mode == WORKER_MODE:
                    self.lock.release()
                time.sleep(0.01)
                if self.mode == WORKER_MODE:
                    self.lock.acquire()

                if time.time() - start_time > error_timeout:
                    warning_message = (""This worker or driver is waiting to ""
                                       ""receive a class definition so that it ""
                                       ""can deserialize an object from the ""
                                       ""object store. This may be fine, or it ""
                                       ""may be a bug."")
                    if not warning_sent:
                        ray.utils.push_error_to_driver(
                            self,
                            ray_constants.WAIT_FOR_CLASS_PUSH_ERROR,
                            warning_message,
                            driver_id=self.task_driver_id)
                    warning_sent = True

    def get_object(self, object_ids):
        """"""Get the value or values in the object store associated with the IDs.

        Return the values from the local object store for object_ids. This will
        block until all the values for object_ids have been written to the
        local object store.

        Args:
            object_ids (List[object_id.ObjectID]): A list of the object IDs
                whose values should be retrieved.
        """"""
        # Make sure that the values are object IDs.
        for object_id in object_ids:
            if not isinstance(object_id, ObjectID):
                raise Exception(
                    ""Attempting to call `get` on the value {}, ""
                    ""which is not an ray.ObjectID."".format(object_id))
        # Do an initial fetch for remote objects. We divide the fetch into
        # smaller fetches so as to not block the manager for a prolonged period
        # of time in a single call.
        plain_object_ids = [
            plasma.ObjectID(object_id.binary()) for object_id in object_ids
        ]
        for i in range(0, len(object_ids),
                       ray._config.worker_fetch_request_size()):
            self.raylet_client.fetch_or_reconstruct(
                object_ids[i:(i + ray._config.worker_fetch_request_size())],
                True)

        # Get the objects. We initially try to get the objects immediately.
        final_results = self.retrieve_and_deserialize(plain_object_ids, 0)
        # Construct a dictionary mapping object IDs that we haven't gotten yet
        # to their original index in the object_ids argument.
        unready_ids = {
            plain_object_ids[i].binary(): i
            for (i, val) in enumerate(final_results)
            if val is plasma.ObjectNotAvailable
        }

        if len(unready_ids) > 0:
            # Try reconstructing any objects we haven't gotten yet. Try to
            # get them until at least get_timeout_milliseconds
            # milliseconds passes, then repeat.
            while len(unready_ids) > 0:
                object_ids_to_fetch = [
                    plasma.ObjectID(unready_id)
                    for unready_id in unready_ids.keys()
                ]
                ray_object_ids_to_fetch = [
                    ObjectID(unready_id) for unready_id in unready_ids.keys()
                ]
                fetch_request_size = ray._config.worker_fetch_request_size()
                for i in range(0, len(object_ids_to_fetch),
                               fetch_request_size):
                    self.raylet_client.fetch_or_reconstruct(
                        ray_object_ids_to_fetch[i:(i + fetch_request_size)],
                        False,
                        self.current_task_id,
                    )
                results = self.retrieve_and_deserialize(
                    object_ids_to_fetch,
                    max([
                        ray._config.get_timeout_milliseconds(),
                        int(0.01 * len(unready_ids)),
                    ]),
                )
                # Remove any entries for objects we received during this
                # iteration so we don't retrieve the same object twice.
                for i, val in enumerate(results):
                    if val is not plasma.ObjectNotAvailable:
                        object_id = object_ids_to_fetch[i].binary()
                        index = unready_ids[object_id]
                        final_results[index] = val
                        unready_ids.pop(object_id)

            # If there were objects that we weren't able to get locally,
            # let the local scheduler know that we're now unblocked.
            self.raylet_client.notify_unblocked(self.current_task_id)

        assert len(final_results) == len(object_ids)
        return final_results

    def submit_task(self,
                    function_descriptor,
                    args,
                    actor_id=None,
                    actor_handle_id=None,
                    actor_counter=0,
                    is_actor_checkpoint_method=False,
                    actor_creation_id=None,
                    actor_creation_dummy_object_id=None,
                    max_actor_reconstructions=0,
                    execution_dependencies=None,
                    new_actor_handles=None,
                    num_return_vals=None,
                    resources=None,
                    placement_resources=None,
                    driver_id=None):
        """"""Submit a remote task to the scheduler.

        Tell the scheduler to schedule the execution of the function with
        function_descriptor with arguments args. Retrieve object IDs for the
        outputs of the function from the scheduler and immediately return them.

        Args:
            function_descriptor: The function descriptor to execute.
            args: The arguments to pass into the function. Arguments can be
                object IDs or they can be values. If they are values, they must
                be serializable objects.
            actor_id: The ID of the actor that this task is for.
            actor_counter: The counter of the actor task.
            is_actor_checkpoint_method: True if this is an actor checkpoint
                task and false otherwise.
            actor_creation_id: The ID of the actor to create, if this is an
                actor creation task.
            actor_creation_dummy_object_id: If this task is an actor method,
                then this argument is the dummy object ID associated with the
                actor creation task for the corresponding actor.
            execution_dependencies: The execution dependencies for this task.
            num_return_vals: The number of return values this function should
                have.
            resources: The resource requirements for this task.
            placement_resources: The resources required for placing the task.
                If this is not provided or if it is an empty dictionary, then
                the placement resources will be equal to resources.
            driver_id: The ID of the relevant driver. This is almost always the
                driver ID of the driver that is currently running. However, in
                the exceptional case that an actor task is being dispatched to
                an actor created by a different driver, this should be the
                driver ID of the driver that created the actor.

        Returns:
            The return object IDs for this task.
        """"""
        with profiling.profile(""submit_task"", worker=self):
            if actor_id is None:
                assert actor_handle_id is None
                actor_id = ActorID.nil()
                actor_handle_id = ActorHandleID.nil()
            else:
                assert actor_handle_id is not None

            if actor_creation_id is None:
                actor_creation_id = ActorID.nil()

            if actor_creation_dummy_object_id is None:
                actor_creation_dummy_object_id = ObjectID.nil()

            # Put large or complex arguments that are passed by value in the
            # object store first.
            args_for_local_scheduler = []
            for arg in args:
                if isinstance(arg, ObjectID):
                    args_for_local_scheduler.append(arg)
                elif ray._raylet.check_simple_value(arg):
                    args_for_local_scheduler.append(arg)
                else:
                    args_for_local_scheduler.append(put(arg))

            # By default, there are no execution dependencies.
            if execution_dependencies is None:
                execution_dependencies = []

            if new_actor_handles is None:
                new_actor_handles = []

            if driver_id is None:
                driver_id = self.task_driver_id

            if resources is None:
                raise ValueError(""The resources dictionary is required."")
            for value in resources.values():
                assert (isinstance(value, int) or isinstance(value, float))
                if value < 0:
                    raise ValueError(
                        ""Resource quantities must be nonnegative."")
                if (value >= 1 and isinstance(value, float)
                        and not value.is_integer()):
                    raise ValueError(
                        ""Resource quantities must all be whole numbers."")

            if placement_resources is None:
                placement_resources = {}

            # Increment the worker's task index to track how many tasks
            # have been submitted by the current task so far.
            self.task_context.task_index += 1
            # The parent task must be set for the submitted task.
            assert not self.current_task_id.is_nil()
            # Current driver id must not be nil when submitting a task.
            # Because every task must belong to a driver.
            assert not self.task_driver_id.is_nil()
            # Submit the task to local scheduler.
            function_descriptor_list = (
                function_descriptor.get_function_descriptor_list())
            assert isinstance(driver_id, DriverID)
            task = ray._raylet.Task(
                driver_id,
                function_descriptor_list,
                args_for_local_scheduler,
                num_return_vals,
                self.current_task_id,
                self.task_context.task_index,
                actor_creation_id,
                actor_creation_dummy_object_id,
                max_actor_reconstructions,
                actor_id,
                actor_handle_id,
                actor_counter,
                new_actor_handles,
                execution_dependencies,
                resources,
                placement_resources,
            )
            self.raylet_client.submit_task(task)

            return task.returns()

    def run_function_on_all_workers(self, function,
                                    run_on_other_drivers=False):
        """"""Run arbitrary code on all of the workers.

        This function will first be run on the driver, and then it will be
        exported to all of the workers to be run. It will also be run on any
        new workers that register later. If ray.init has not been called yet,
        then cache the function and export it later.

        Args:
            function (Callable): The function to run on all of the workers. It
                takes only one argument, a worker info dict. If it returns
                anything, its return values will not be used.
            run_on_other_drivers: The boolean that indicates whether we want to
                run this function on other drivers. One case is we may need to
                share objects across drivers.
        """"""
        # If ray.init has not been called yet, then cache the function and
        # export it when connect is called. Otherwise, run the function on all
        # workers.
        if self.mode is None:
            self.cached_functions_to_run.append(function)
        else:
            # Attempt to pickle the function before we need it. This could
            # fail, and it is more convenient if the failure happens before we
            # actually run the function locally.
            pickled_function = pickle.dumps(function)

            function_to_run_id = hashlib.sha1(pickled_function).digest()
            key = b""FunctionsToRun:"" + function_to_run_id
            # First run the function on the driver.
            # We always run the task locally.
            function({""worker"": self})
            # Check if the function has already been put into redis.
            function_exported = self.redis_client.setnx(b""Lock:"" + key, 1)
            if not function_exported:
                # In this case, the function has already been exported, so
                # we don't need to export it again.
                return

            check_oversized_pickle(pickled_function, function.__name__,
                                   ""function"", self)

            # Run the function on all workers.
            self.redis_client.hmset(
                key, {
                    ""driver_id"": self.task_driver_id.binary(),
                    ""function_id"": function_to_run_id,
                    ""function"": pickled_function,
                    ""run_on_other_drivers"": str(run_on_other_drivers)
                })
            self.redis_client.rpush(""Exports"", key)
            # TODO(rkn): If the worker fails after it calls setnx and before it
            # successfully completes the hmset and rpush, then the program will
            # most likely hang. This could be fixed by making these three
            # operations into a transaction (or by implementing a custom
            # command that does all three things).

    def _get_arguments_for_execution(self, function_name, serialized_args):
        """"""Retrieve the arguments for the remote function.

        This retrieves the values for the arguments to the remote function that
        were passed in as object IDs. Arguments that were passed by value are
        not changed. This is called by the worker that is executing the remote
        function.

        Args:
            function_name (str): The name of the remote function whose
                arguments are being retrieved.
            serialized_args (List): The arguments to the function. These are
                either strings representing serialized objects passed by value
                or they are ray.ObjectIDs.

        Returns:
            The retrieved arguments in addition to the arguments that were
                passed by value.

        Raises:
            RayTaskError: This exception is raised if a task that
                created one of the arguments failed.
        """"""
        arguments = []
        for (i, arg) in enumerate(serialized_args):
            if isinstance(arg, ObjectID):
                # get the object from the local object store
                argument = self.get_object([arg])[0]
                if isinstance(argument, RayTaskError):
                    raise argument
            else:
                # pass the argument by value
                argument = arg

            arguments.append(argument)
        return arguments

    def _store_outputs_in_object_store(self, object_ids, outputs):
        """"""Store the outputs of a remote function in the local object store.

        This stores the values that were returned by a remote function in the
        local object store. If any of the return values are object IDs, then
        these object IDs are aliased with the object IDs that the scheduler
        assigned for the return values. This is called by the worker that
        executes the remote function.

        Note:
            The arguments object_ids and outputs should have the same length.

        Args:
            object_ids (List[ObjectID]): The object IDs that were assigned to
                the outputs of the remote function call.
            outputs (Tuple): The value returned by the remote function. If the
                remote function was supposed to only return one value, then its
                output was wrapped in a tuple with one element prior to being
                passed into this function.
        """"""
        for i in range(len(object_ids)):
            if isinstance(outputs[i], ray.actor.ActorHandle):
                raise Exception(""Returning an actor handle from a remote ""
                                ""function is not allowed)."")

            self.put_object(object_ids[i], outputs[i])

    def _process_task(self, task, function_execution_info):
        """"""Execute a task assigned to this worker.

        This method deserializes a task from the scheduler, and attempts to
        execute the task. If the task succeeds, the outputs are stored in the
        local object store. If the task throws an exception, RayTaskError
        objects are stored in the object store to represent the failed task
        (these will be retrieved by calls to get or by subsequent tasks that
        use the outputs of this task).
        """"""
        assert self.current_task_id.is_nil()
        assert self.task_context.task_index == 0
        assert self.task_context.put_index == 1
        if task.actor_id().is_nil():
            # If this worker is not an actor, check that `task_driver_id`
            # was reset when the worker finished the previous task.
            assert self.task_driver_id.is_nil()
            # Set the driver ID of the current running task. This is
            # needed so that if the task throws an exception, we propagate
            # the error message to the correct driver.
            self.task_driver_id = task.driver_id()
        else:
            # If this worker is an actor, task_driver_id wasn't reset.
            # Check that current task's driver ID equals the previous one.
            assert self.task_driver_id == task.driver_id()

        self.task_context.current_task_id = task.task_id()

        function_descriptor = FunctionDescriptor.from_bytes_list(
            task.function_descriptor_list())
        args = task.arguments()
        return_object_ids = task.returns()
        if (not task.actor_id().is_nil()
                or not task.actor_creation_id().is_nil()):
            dummy_return_id = return_object_ids.pop()
        function_executor = function_execution_info.function
        function_name = function_execution_info.function_name

        # Get task arguments from the object store.
        try:
            if function_name != ""__ray_terminate__"":
                self.reraise_actor_init_error()
            self.memory_monitor.raise_if_low_memory()
            with profiling.profile(""task:deserialize_arguments"", worker=self):
                arguments = self._get_arguments_for_execution(
                    function_name, args)
        except RayTaskError as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))
            return
        except Exception as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))
            return

        # Execute the task.
        try:
            with profiling.profile(""task:execute"", worker=self):
                if (task.actor_id().is_nil()
                        and task.actor_creation_id().is_nil()):
                    outputs = function_executor(*arguments)
                else:
                    if not task.actor_id().is_nil():
                        key = task.actor_id()
                    else:
                        key = task.actor_creation_id()
                    outputs = function_executor(dummy_return_id,
                                                self.actors[key], *arguments)
        except Exception as e:
            # Determine whether the exception occured during a task, not an
            # actor method.
            task_exception = task.actor_id().is_nil()
            traceback_str = ray.utils.format_error_message(
                traceback.format_exc(), task_exception=task_exception)
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e, traceback_str)
            return

        # Store the outputs in the local object store.
        try:
            with profiling.profile(""task:store_outputs"", worker=self):
                # If this is an actor task, then the last object ID returned by
                # the task is a dummy output, not returned by the function
                # itself. Decrement to get the correct number of return values.
                num_returns = len(return_object_ids)
                if num_returns == 1:
                    outputs = (outputs, )
                self._store_outputs_in_object_store(return_object_ids, outputs)
        except Exception as e:
            self._handle_process_task_failure(
                function_descriptor, return_object_ids, e,
                ray.utils.format_error_message(traceback.format_exc()))

    def _handle_process_task_failure(self, function_descriptor,
                                     return_object_ids, error, backtrace):
        function_name = function_descriptor.function_name
        failure_object = RayTaskError(function_name, backtrace)
        failure_objects = [
            failure_object for _ in range(len(return_object_ids))
        ]
        self._store_outputs_in_object_store(return_object_ids, failure_objects)
        # Log the error message.
        ray.utils.push_error_to_driver(
            self,
            ray_constants.TASK_PUSH_ERROR,
            str(failure_object),
            driver_id=self.task_driver_id)
        # Mark the actor init as failed
        if not self.actor_id.is_nil() and function_name == ""__init__"":
            self.mark_actor_init_failed(error)

    def _wait_for_and_process_task(self, task):
        """"""Wait for a task to be ready and process the task.

        Args:
            task: The task to execute.
        """"""
        function_descriptor = FunctionDescriptor.from_bytes_list(
            task.function_descriptor_list())
        driver_id = task.driver_id()

        # TODO(rkn): It would be preferable for actor creation tasks to share
        # more of the code path with regular task execution.
        if not task.actor_creation_id().is_nil():
            assert self.actor_id.is_nil()
            self.actor_id = task.actor_creation_id()
            self.function_actor_manager.load_actor(driver_id,
                                                   function_descriptor)

        execution_info = self.function_actor_manager.get_execution_info(
            driver_id, function_descriptor)

        # Execute the task.
        # TODO(rkn): Consider acquiring this lock with a timeout and pushing a
        # warning to the user if we are waiting too long to acquire the lock
        # because that may indicate that the system is hanging, and it'd be
        # good to know where the system is hanging.
        with self.lock:
            function_name = execution_info.function_name
            extra_data = {
                ""name"": function_name,
                ""task_id"": task.task_id().hex()
            }
            if task.actor_id().is_nil():
                if task.actor_creation_id().is_nil():
                    title = ""ray_worker:{}()"".format(function_name)
                    next_title = ""ray_worker""
                else:
                    actor = self.actors[task.actor_creation_id()]
                    title = ""ray_{}:{}()"".format(actor.__class__.__name__,
                                                 function_name)
                    next_title = ""ray_{}"".format(actor.__class__.__name__)
            else:
                actor = self.actors[task.actor_id()]
                title = ""ray_{}:{}()"".format(actor.__class__.__name__,
                                             function_name)
                next_title = ""ray_{}"".format(actor.__class__.__name__)
            with profiling.profile(""task"", extra_data=extra_data, worker=self):
                with _changeproctitle(title, next_title):
                    self._process_task(task, execution_info)
                # Reset the state fields so the next task can run.
                self.task_context.current_task_id = TaskID.nil()
                self.task_context.task_index = 0
                self.task_context.put_index = 1
                if self.actor_id.is_nil():
                    # Don't need to reset task_driver_id if the worker is an
                    # actor. Because the following tasks should all have the
                    # same driver id.
                    self.task_driver_id = DriverID.nil()

        # Increase the task execution counter.
        self.function_actor_manager.increase_task_counter(
            driver_id, function_descriptor)

        reached_max_executions = (self.function_actor_manager.get_task_counter(
            driver_id, function_descriptor) == execution_info.max_calls)
        if reached_max_executions:
            self.raylet_client.disconnect()
            sys.exit(0)

    def _get_next_task_from_local_scheduler(self):
        """"""Get the next task from the local scheduler.

        Returns:
            A task from the local scheduler.
        """"""
        with profiling.profile(""worker_idle"", worker=self):
            task = self.raylet_client.get_task()

        # Automatically restrict the GPUs available to this task.
        ray.utils.set_cuda_visible_devices(ray.get_gpu_ids())

        return task

    def main_loop(self):
        """"""The main loop a worker runs to receive and execute tasks.""""""

        def exit(signum, frame):
            shutdown(worker=self)
            sys.exit(0)

        signal.signal(signal.SIGTERM, exit)

        while True:
            task = self._get_next_task_from_local_scheduler()
            self._wait_for_and_process_task(task)


def get_gpu_ids():
    """"""Get the IDs of the GPUs that are available to the worker.

    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker
    started up, then the IDs returned by this method will be a subset of the
    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range
    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.

    Returns:
        A list of GPU IDs.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(""ray.get_gpu_ids() currently does not work in PYTHON ""
                        ""MODE."")

    all_resource_ids = global_worker.raylet_client.resource_ids()
    assigned_ids = [
        resource_id for resource_id, _ in all_resource_ids.get(""GPU"", [])
    ]
    # If the user had already set CUDA_VISIBLE_DEVICES, then respect that (in
    # the sense that only GPU IDs that appear in CUDA_VISIBLE_DEVICES should be
    # returned).
    if global_worker.original_gpu_ids is not None:
        assigned_ids = [
            global_worker.original_gpu_ids[gpu_id] for gpu_id in assigned_ids
        ]

    return assigned_ids


def get_resource_ids():
    """"""Get the IDs of the resources that are available to the worker.

    Returns:
        A dictionary mapping the name of a resource to a list of pairs, where
        each pair consists of the ID of a resource and the fraction of that
        resource reserved for this worker.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(
            ""ray.get_resource_ids() currently does not work in PYTHON ""
            ""MODE."")

    return global_worker.raylet_client.resource_ids()


def _webui_url_helper(client):
    """"""Parsing for getting the url of the web UI.

    Args:
        client: A redis client to use to query the primary Redis shard.

    Returns:
        The URL of the web UI as a string.
    """"""
    result = client.hmget(""webui"", ""url"")[0]
    return ray.utils.decode(result) if result is not None else result


def get_webui_url():
    """"""Get the URL to access the web UI.

    Note that the URL does not specify which node the web UI is on.

    Returns:
        The URL of the web UI as a string.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(""ray.get_webui_url() currently does not work in ""
                        ""PYTHON MODE."")
    return _webui_url_helper(global_worker.redis_client)


global_worker = Worker()
""""""Worker: The global Worker object for this worker process.

We use a global Worker object to ensure that there is a single worker object
per worker process.
""""""

global_state = state.GlobalState()

_global_node = None
""""""ray.node.Node: The global node object that is created by ray.init().""""""


class RayConnectionError(Exception):
    pass


def print_failed_task(task_status):
    """"""Print information about failed tasks.

    Args:
        task_status (Dict): A dictionary containing the name, operationid, and
            error message for a failed task.
    """"""
    logger.error(""""""
      Error: Task failed
        Function Name: {}
        Task ID: {}
        Error Message: \n{}
    """""".format(task_status[""function_name""], task_status[""operationid""],
               task_status[""error_message""]))


def error_info(worker=global_worker):
    """"""Return information about failed tasks.""""""
    worker.check_connected()
    return (global_state.error_messages(job_id=worker.task_driver_id) +
            global_state.error_messages(job_id=DriverID.nil()))


def _initialize_serialization(driver_id, worker=global_worker):
    """"""Initialize the serialization library.

    This defines a custom serializer for object IDs and also tells ray to
    serialize several exception classes that we define for error handling.
    """"""
    serialization_context = pyarrow.default_serialization_context()
    # Tell the serialization context to use the cloudpickle version that we
    # ship with Ray.
    serialization_context.set_pickle(pickle.dumps, pickle.loads)
    pyarrow.register_torch_serialization_handlers(serialization_context)

    for id_type in ray._ID_TYPES:
        serialization_context.register_type(
            id_type,
            ""{}.{}"".format(id_type.__module__, id_type.__name__),
            pickle=True)

    def actor_handle_serializer(obj):
        return obj._serialization_helper(True)

    def actor_handle_deserializer(serialized_obj):
        new_handle = ray.actor.ActorHandle.__new__(ray.actor.ActorHandle)
        new_handle._deserialization_helper(serialized_obj, True)
        return new_handle

    # We register this serializer on each worker instead of calling
    # register_custom_serializer from the driver so that isinstance still
    # works.
    serialization_context.register_type(
        ray.actor.ActorHandle,
        ""ray.ActorHandle"",
        pickle=False,
        custom_serializer=actor_handle_serializer,
        custom_deserializer=actor_handle_deserializer)

    worker.serialization_context_map[driver_id] = serialization_context

    register_custom_serializer(
        RayTaskError,
        use_dict=True,
        local=True,
        driver_id=driver_id,
        class_id=""ray.RayTaskError"")
    # Tell Ray to serialize lambdas with pickle.
    register_custom_serializer(
        type(lambda: 0),
        use_pickle=True,
        local=True,
        driver_id=driver_id,
        class_id=""lambda"")
    # Tell Ray to serialize types with pickle.
    register_custom_serializer(
        type(int),
        use_pickle=True,
        local=True,
        driver_id=driver_id,
        class_id=""type"")
    # Tell Ray to serialize FunctionSignatures as dictionaries. This is
    # used when passing around actor handles.
    register_custom_serializer(
        ray.signature.FunctionSignature,
        use_dict=True,
        local=True,
        driver_id=driver_id,
        class_id=""ray.signature.FunctionSignature"")


def get_address_info_from_redis_helper(redis_address,
                                       node_ip_address,
                                       redis_password=None):
    redis_ip_address, redis_port = redis_address.split("":"")
    # For this command to work, some other client (on the same machine as
    # Redis) must have run ""CONFIG SET protected-mode no"".
    redis_client = redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=redis_password)

    client_table = ray.experimental.state.parse_client_table(redis_client)
    if len(client_table) == 0:
        raise Exception(
            ""Redis has started but no raylets have registered yet."")

    relevant_client = None
    for client_info in client_table:
        client_node_ip_address = client_info[""NodeManagerAddress""]
        if (client_node_ip_address == node_ip_address or
            (client_node_ip_address == ""127.0.0.1""
             and redis_ip_address == ray.services.get_node_ip_address())):
            relevant_client = client_info
            break
    if relevant_client is None:
        raise Exception(
            ""Redis has started but no raylets have registered yet."")

    return {
        ""node_ip_address"": node_ip_address,
        ""redis_address"": redis_address,
        ""object_store_address"": relevant_client[""ObjectStoreSocketName""],
        ""raylet_socket_name"": relevant_client[""RayletSocketName""],
        # Web UI should be running.
        ""webui_url"": _webui_url_helper(redis_client)
    }


def get_address_info_from_redis(redis_address,
                                node_ip_address,
                                num_retries=5,
                                redis_password=None):
    counter = 0
    while True:
        try:
            return get_address_info_from_redis_helper(
                redis_address, node_ip_address, redis_password=redis_password)
        except Exception:
            if counter == num_retries:
                raise
            # Some of the information may not be in Redis yet, so wait a little
            # bit.
            logger.warning(
                ""Some processes that the driver needs to connect to have ""
                ""not registered with Redis, so retrying. Have you run ""
                ""'ray start' on this node?"")
            time.sleep(1)
        counter += 1


def init(redis_address=None,
         num_cpus=None,
         num_gpus=None,
         resources=None,
         object_store_memory=None,
         redis_max_memory=None,
         log_to_driver=True,
         node_ip_address=None,
         object_id_seed=None,
         num_workers=None,
         local_mode=False,
         driver_mode=None,
         redirect_worker_output=True,
         redirect_output=True,
         ignore_reinit_error=False,
         num_redis_shards=None,
         redis_max_clients=None,
         redis_password=None,
         plasma_directory=None,
         huge_pages=False,
         include_webui=True,
         driver_id=None,
         configure_logging=True,
         logging_level=logging.INFO,
         logging_format=ray_constants.LOGGER_FORMAT,
         plasma_store_socket_name=None,
         raylet_socket_name=None,
         temp_dir=None,
         _internal_config=None,
         use_raylet=None):
    """"""Connect to an existing Ray cluster or start one and connect to it.

    This method handles two cases. Either a Ray cluster already exists and we
    just attach this driver to it, or we start all of the processes associated
    with a Ray cluster and attach to the newly started cluster.

    To start Ray and all of the relevant processes, use this as follows:

    .. code-block:: python

        ray.init()

    To connect to an existing Ray cluster, use this as follows (substituting
    in the appropriate address):

    .. code-block:: python

        ray.init(redis_address=""123.45.67.89:6379"")

    Args:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        num_cpus (int): Number of cpus the user wishes all local schedulers to
            be configured with.
        num_gpus (int): Number of gpus the user wishes all local schedulers to
            be configured with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with. By default, this is capped at 20GB but can be
            set higher.
        redis_max_memory: The max amount of memory (in bytes) to allow each
            redis shard to use. Once the limit is exceeded, redis will start
            LRU eviction of entries. This only applies to the sharded redis
            tables (task, object, and profile tables). By default, this is
            capped at 10GB but can be set higher.
        log_to_driver (bool): If true, then output from all of the worker
            processes on all nodes will be directed to the driver.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        ignore_reinit_error: True if we should suppress errors from calling
            ray.init() a second time.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        driver_id: The ID of driver.
        configure_logging: True if allow the logging cofiguration here.
            Otherwise, the users may want to configure it by their own.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.

    Returns:
        Address information about the started processes.

    Raises:
        Exception: An exception is raised if an inappropriate combination of
            arguments is passed in.
    """"""

    if configure_logging:
        setup_logger(logging_level, logging_format)

    # Add the use_raylet option for backwards compatibility.
    if use_raylet is not None:
        if use_raylet:
            logger.warning(""WARNING: The use_raylet argument has been ""
                           ""deprecated. Please remove it."")
        else:
            raise DeprecationWarning(""The use_raylet argument is deprecated. ""
                                     ""Please remove it."")

    if driver_mode is not None:
        raise Exception(""The 'driver_mode' argument has been deprecated. ""
                        ""To run Ray in local mode, pass in local_mode=True."")
    if local_mode:
        driver_mode = LOCAL_MODE
    else:
        driver_mode = SCRIPT_MODE

    if setproctitle is None:
        logger.warning(
            ""WARNING: Not updating worker name since `setproctitle` is not ""
            ""installed. Install this with `pip install setproctitle` ""
            ""(or ray[debug]) to enable monitoring of worker processes."")

    if global_worker.connected:
        if ignore_reinit_error:
            logger.error(""Calling ray.init() again after it has already been ""
                         ""called."")
            return
        else:
            raise Exception(""Perhaps you called ray.init twice by accident? ""
                            ""This error can be suppressed by passing in ""
                            ""'ignore_reinit_error=True' or by calling ""
                            ""'ray.shutdown()' prior to 'ray.init()'."")

    # Convert hostnames to numerical IP address.
    if node_ip_address is not None:
        node_ip_address = services.address_to_ip(node_ip_address)
    if redis_address is not None:
        redis_address = services.address_to_ip(redis_address)

    address_info = {
        ""node_ip_address"": node_ip_address,
        ""redis_address"": redis_address
    }

    if driver_mode == LOCAL_MODE:
        # If starting Ray in LOCAL_MODE, don't start any other processes.
        pass
    elif redis_address is None:
        if node_ip_address is None:
            node_ip_address = ray.services.get_node_ip_address()
        if num_redis_shards is None:
            num_redis_shards = 1
        # In this case, we need to start a new cluster.
        ray_params = ray.parameter.RayParams(
            redis_address=redis_address,
            node_ip_address=node_ip_address,
            num_workers=num_workers,
            object_id_seed=object_id_seed,
            local_mode=local_mode,
            driver_mode=driver_mode,
            redirect_worker_output=redirect_worker_output,
            redirect_output=redirect_output,
            num_cpus=num_cpus,
            num_gpus=num_gpus,
            resources=resources,
            num_redis_shards=num_redis_shards,
            redis_max_clients=redis_max_clients,
            redis_password=redis_password,
            plasma_directory=plasma_directory,
            huge_pages=huge_pages,
            include_webui=include_webui,
            object_store_memory=object_store_memory,
            redis_max_memory=redis_max_memory,
            plasma_store_socket_name=plasma_store_socket_name,
            raylet_socket_name=raylet_socket_name,
            temp_dir=temp_dir,
            _internal_config=_internal_config,
        )
        # Start the Ray processes. We set shutdown_at_exit=False because we
        # shutdown the node in the ray.shutdown call that happens in the atexit
        # handler.
        global _global_node
        _global_node = ray.node.Node(
            head=True, shutdown_at_exit=False, ray_params=ray_params)
        address_info[""redis_address""] = _global_node.redis_address
        address_info[
            ""object_store_address""] = _global_node.plasma_store_socket_name
        address_info[""webui_url""] = _global_node.webui_url
        address_info[""raylet_socket_name""] = _global_node.raylet_socket_name
    else:
        # In this case, we are connecting to an existing cluster.
        if num_workers is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""num_workers must not be provided."")
        if num_cpus is not None or num_gpus is not None:
            raise Exception(""When connecting to an existing cluster, num_cpus ""
                            ""and num_gpus must not be provided."")
        if resources is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""resources must not be provided."")
        if num_redis_shards is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""num_redis_shards must not be provided."")
        if redis_max_clients is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""redis_max_clients must not be provided."")
        if object_store_memory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""object_store_memory must not be provided."")
        if redis_max_memory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""redis_max_memory must not be provided."")
        if plasma_directory is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""plasma_directory must not be provided."")
        if huge_pages:
            raise Exception(""When connecting to an existing cluster, ""
                            ""huge_pages must not be provided."")
        if temp_dir is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""temp_dir must not be provided."")
        if plasma_store_socket_name is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""plasma_store_socket_name must not be provided."")
        if raylet_socket_name is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""raylet_socket_name must not be provided."")
        if _internal_config is not None:
            raise Exception(""When connecting to an existing cluster, ""
                            ""_internal_config must not be provided."")

        # Get the node IP address if one is not provided.

        if node_ip_address is None:
            node_ip_address = services.get_node_ip_address(redis_address)
        # Get the address info of the processes to connect to from Redis.
        address_info = get_address_info_from_redis(
            redis_address, node_ip_address, redis_password=redis_password)

    if driver_mode == LOCAL_MODE:
        driver_address_info = {}
    else:
        driver_address_info = {
            ""node_ip_address"": node_ip_address,
            ""redis_address"": address_info[""redis_address""],
            ""store_socket_name"": address_info[""object_store_address""],
            ""webui_url"": address_info[""webui_url""],
            ""raylet_socket_name"": address_info[""raylet_socket_name""],
        }

    # We only pass `temp_dir` to a worker (WORKER_MODE).
    # It can't be a worker here.
    connect(
        driver_address_info,
        redis_password=redis_password,
        object_id_seed=object_id_seed,
        mode=driver_mode,
        log_to_driver=log_to_driver,
        worker=global_worker,
        driver_id=driver_id)

    for hook in _post_init_hooks:
        hook()

    return address_info


# Functions to run as callback after a successful ray init
_post_init_hooks = []


def cleanup(worker=global_worker):
    raise DeprecationWarning(
        ""The function ray.worker.cleanup() has been deprecated. Instead, ""
        ""please call ray.shutdown()."")


def shutdown(worker=global_worker):
    """"""Disconnect the worker, and terminate processes started by ray.init().

    This will automatically run at the end when a Python process that uses Ray
    exits. It is ok to run this twice in a row. The primary use case for this
    function is to cleanup state between tests.

    Note that this will clear any remote function definitions, actor
    definitions, and existing actors, so if you wish to use any previously
    defined remote functions or actors after calling ray.shutdown(), then you
    need to redefine them. If they were defined in an imported module, then you
    will need to reload the module.
    """"""
    disconnect(worker)

    # Shut down the Ray processes.
    global _global_node
    if _global_node is not None:
        _global_node.kill_all_processes(check_alive=False, allow_graceful=True)
        _global_node = None

    worker.set_mode(None)


atexit.register(shutdown)

# Define a custom excepthook so that if the driver exits with an exception, we
# can push that exception to Redis.
normal_excepthook = sys.excepthook


def custom_excepthook(type, value, tb):
    # If this is a driver, push the exception to redis.
    if global_worker.mode == SCRIPT_MODE:
        error_message = """".join(traceback.format_tb(tb))
        global_worker.redis_client.hmset(b""Drivers:"" + global_worker.worker_id,
                                         {""exception"": error_message})
    # Call the normal excepthook.
    normal_excepthook(type, value, tb)


sys.excepthook = custom_excepthook

# The last time we raised a TaskError in this process. We use this value to
# suppress redundant error messages pushed from the workers.
last_task_error_raise_time = 0

# The max amount of seconds to wait before printing out an uncaught error.
UNCAUGHT_ERROR_GRACE_PERIOD = 5


def print_logs(redis_client, threads_stopped):
    """"""Prints log messages from workers on all of the nodes.

    Args:
        redis_client: A client to the primary Redis shard.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""
    pubsub_client = redis_client.pubsub(ignore_subscribe_messages=True)
    pubsub_client.subscribe(ray.gcs_utils.LOG_FILE_CHANNEL)
    try:
        # Keep track of the number of consecutive log messages that have been
        # received with no break in between. If this number grows continually,
        # then the worker is probably not able to process the log messages as
        # rapidly as they are coming in.
        num_consecutive_messages_received = 0
        while True:
            # Exit if we received a signal that we should stop.
            if threads_stopped.is_set():
                return

            msg = pubsub_client.get_message()
            if msg is None:
                num_consecutive_messages_received = 0
                threads_stopped.wait(timeout=0.01)
                continue
            num_consecutive_messages_received += 1
            print(ray.utils.decode(msg[""data""]), file=sys.stderr)

            if (num_consecutive_messages_received % 100 == 0
                    and num_consecutive_messages_received > 0):
                logger.warning(
                    ""The driver may not be able to keep up with the ""
                    ""stdout/stderr of the workers. To avoid forwarding logs ""
                    ""to the driver, use 'ray.init(log_to_driver=False)'."")
    finally:
        # Close the pubsub client to avoid leaking file descriptors.
        pubsub_client.close()


def print_error_messages_raylet(task_error_queue, threads_stopped):
    """"""Prints message received in the given output queue.

    This checks periodically if any un-raised errors occured in the background.

    Args:
        task_error_queue (queue.Queue): A queue used to receive errors from the
            thread that listens to Redis.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""

    while True:
        # Exit if we received a signal that we should stop.
        if threads_stopped.is_set():
            return

        try:
            error, t = task_error_queue.get(block=False)
        except queue.Empty:
            threads_stopped.wait(timeout=0.01)
            continue
        # Delay errors a little bit of time to attempt to suppress redundant
        # messages originating from the worker.
        while t + UNCAUGHT_ERROR_GRACE_PERIOD > time.time():
            threads_stopped.wait(timeout=1)
        if t < last_task_error_raise_time + UNCAUGHT_ERROR_GRACE_PERIOD:
            logger.debug(""Suppressing error from worker: {}"".format(error))
        else:
            logger.error(
                ""Possible unhandled error from worker: {}"".format(error))


def listen_error_messages_raylet(worker, task_error_queue, threads_stopped):
    """"""Listen to error messages in the background on the driver.

    This runs in a separate thread on the driver and pushes (error, time)
    tuples to the output queue.

    Args:
        worker: The worker class that this thread belongs to.
        task_error_queue (queue.Queue): A queue used to communicate with the
            thread that prints the errors found by this thread.
        threads_stopped (threading.Event): A threading event used to signal to
            the thread that it should exit.
    """"""
    worker.error_message_pubsub_client = worker.redis_client.pubsub(
        ignore_subscribe_messages=True)
    # Exports that are published after the call to
    # error_message_pubsub_client.subscribe and before the call to
    # error_message_pubsub_client.listen will still be processed in the loop.

    # Really we should just subscribe to the errors for this specific job.
    # However, currently all errors seem to be published on the same channel.
    error_pubsub_channel = str(
        ray.gcs_utils.TablePubsub.ERROR_INFO).encode(""ascii"")
    worker.error_message_pubsub_client.subscribe(error_pubsub_channel)
    # worker.error_message_pubsub_client.psubscribe(""*"")

    try:
        # Get the exports that occurred before the call to subscribe.
        with worker.lock:
            error_messages = global_state.error_messages(worker.task_driver_id)
            for error_message in error_messages:
                logger.error(error_message)

        while True:
            # Exit if we received a signal that we should stop.
            if threads_stopped.is_set():
                return

            msg = worker.error_message_pubsub_client.get_message()
            if msg is None:
                threads_stopped.wait(timeout=0.01)
                continue
            gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
                msg[""data""], 0)
            assert gcs_entry.EntriesLength() == 1
            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(
                gcs_entry.Entries(0), 0)
            job_id = error_data.JobId()
            if job_id not in [
                    worker.task_driver_id.binary(),
                    DriverID.nil().binary()
            ]:
                continue

            error_message = ray.utils.decode(error_data.ErrorMessage())
            if (ray.utils.decode(
                    error_data.Type()) == ray_constants.TASK_PUSH_ERROR):
                # Delay it a bit to see if we can suppress it
                task_error_queue.put((error_message, time.time()))
            else:
                logger.error(error_message)
    finally:
        # Close the pubsub client to avoid leaking file descriptors.
        worker.error_message_pubsub_client.close()


def is_initialized():
    """"""Check if ray.init has been called yet.

    Returns:
        True if ray.init has already been called and false otherwise.
    """"""
    return ray.worker.global_worker.connected


def connect(info,
            redis_password=None,
            object_id_seed=None,
            mode=WORKER_MODE,
            log_to_driver=False,
            worker=global_worker,
            driver_id=None):
    """"""Connect this worker to the local scheduler, to Plasma, and to Redis.

    Args:
        info (dict): A dictionary with address of the Redis server and the
            sockets of the plasma store and raylet.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        mode: The mode of the worker. One of SCRIPT_MODE, WORKER_MODE, and
            LOCAL_MODE.
        log_to_driver (bool): If true, then output from all of the worker
            processes on all nodes will be directed to the driver.
        worker: The ray.Worker instance.
        driver_id: The ID of driver. If it's None, then we will generate one.
    """"""
    # Do some basic checking to make sure we didn't call ray.init twice.
    error_message = ""Perhaps you called ray.init twice by accident?""
    assert not worker.connected, error_message
    assert worker.cached_functions_to_run is not None, error_message

    # Enable nice stack traces on SIGSEGV etc.
    if not faulthandler.is_enabled():
        faulthandler.enable(all_threads=False)

    worker.profiler = profiling.Profiler(worker, worker.threads_stopped)

    # Initialize some fields.
    if mode is WORKER_MODE:
        worker.worker_id = random_string()
        if setproctitle:
            setproctitle.setproctitle(""ray_worker"")
    else:
        # This is the code path of driver mode.
        if driver_id is None:
            driver_id = DriverID(random_string())

        if not isinstance(driver_id, DriverID):
            raise Exception(""The type of given driver id must be DriverID."")

        worker.worker_id = driver_id.binary()

    # When tasks are executed on remote workers in the context of multiple
    # drivers, the task driver ID is used to keep track of which driver is
    # responsible for the task so that error messages will be propagated to
    # the correct driver.
    if mode != WORKER_MODE:
        worker.task_driver_id = DriverID(worker.worker_id)

    # All workers start out as non-actors. A worker can be turned into an actor
    # after it is created.
    worker.actor_id = ActorID.nil()
    worker.connected = True
    worker.set_mode(mode)

    # If running Ray in LOCAL_MODE, there is no need to create call
    # create_worker or to start the worker service.
    if mode == LOCAL_MODE:
        return
    # Set the node IP address.
    worker.node_ip_address = info[""node_ip_address""]
    worker.redis_address = info[""redis_address""]

    # Create a Redis client.
    redis_ip_address, redis_port = info[""redis_address""].split("":"")
    # The Redis client can safely be shared between threads. However, that is
    # not true of Redis pubsub clients. See the documentation at
    # https://github.com/andymccurdy/redis-py#thread-safety.
    worker.redis_client = redis.StrictRedis(
        host=redis_ip_address, port=int(redis_port), password=redis_password)

    # For driver's check that the version information matches the version
    # information that the Ray cluster was started with.
    try:
        ray.services.check_version_info(worker.redis_client)
    except Exception as e:
        if mode == SCRIPT_MODE:
            raise e
        elif mode == WORKER_MODE:
            traceback_str = traceback.format_exc()
            ray.utils.push_error_to_driver_through_redis(
                worker.redis_client,
                ray_constants.VERSION_MISMATCH_PUSH_ERROR,
                traceback_str,
                driver_id=None)

    worker.lock = threading.Lock()

    # Create an object for interfacing with the global state.
    global_state._initialize_global_state(
        redis_ip_address, int(redis_port), redis_password=redis_password)

    # Register the worker with Redis.
    if mode == SCRIPT_MODE:
        # The concept of a driver is the same as the concept of a ""job"".
        # Register the driver/job with Redis here.
        import __main__ as main
        driver_info = {
            ""node_ip_address"": worker.node_ip_address,
            ""driver_id"": worker.worker_id,
            ""start_time"": time.time(),
            ""plasma_store_socket"": info[""store_socket_name""],
            ""raylet_socket"": info.get(""raylet_socket_name""),
            ""name"": (main.__file__
                     if hasattr(main, ""__file__"") else ""INTERACTIVE MODE"")
        }
        worker.redis_client.hmset(b""Drivers:"" + worker.worker_id, driver_info)
        if (not worker.redis_client.exists(""webui"")
                and info[""webui_url""] is not None):
            worker.redis_client.hmset(""webui"", {""url"": info[""webui_url""]})
        is_worker = False
    elif mode == WORKER_MODE:
        # Check the RedirectOutput key in Redis and based on its value redirect
        # worker output and error to their own files.
        # This key is set in services.py when Redis is started.
        redirect_worker_output_val = worker.redis_client.get(""RedirectOutput"")
        if (redirect_worker_output_val is not None
                and int(redirect_worker_output_val) == 1):
            redirect_worker_output = 1
        else:
            redirect_worker_output = 0
        if redirect_worker_output:
            log_stdout_file, log_stderr_file = (
                tempfile_services.new_worker_redirected_log_file(
                    worker.worker_id))
            # Redirect stdout/stderr at the file descriptor level. If we simply
            # set sys.stdout and sys.stderr, then logging from C++ can fail to
            # be redirected.
            os.dup2(log_stdout_file.fileno(), sys.stdout.fileno())
            os.dup2(log_stderr_file.fileno(), sys.stderr.fileno())
            # This should always be the first message to appear in the worker's
            # stdout and stderr log files. The string ""Ray worker pid:"" is
            # parsed in the log monitor process.
            print(""Ray worker pid: {}"".format(os.getpid()))
            print(""Ray worker pid: {}"".format(os.getpid()), file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

        # Register the worker with Redis.
        worker_dict = {
            ""node_ip_address"": worker.node_ip_address,
            ""plasma_store_socket"": info[""store_socket_name""],
        }
        if redirect_worker_output:
            worker_dict[""stdout_file""] = os.path.abspath(log_stdout_file.name)
            worker_dict[""stderr_file""] = os.path.abspath(log_stderr_file.name)
        worker.redis_client.hmset(b""Workers:"" + worker.worker_id, worker_dict)
        is_worker = True
    else:
        raise Exception(""This code should be unreachable."")

    # Create an object store client.
    worker.plasma_client = thread_safe_client(
        plasma.connect(info[""store_socket_name""], None, 0, 300))

    raylet_socket = info[""raylet_socket_name""]

    # If this is a driver, set the current task ID, the task driver ID, and set
    # the task index to 0.
    if mode == SCRIPT_MODE:
        # If the user provided an object_id_seed, then set the current task ID
        # deterministically based on that seed (without altering the state of
        # the user's random number generator). Otherwise, set the current task
        # ID randomly to avoid object ID collisions.
        numpy_state = np.random.get_state()
        if object_id_seed is not None:
            np.random.seed(object_id_seed)
        else:
            # Try to use true randomness.
            np.random.seed(None)
        # Reset the state of the numpy random number generator.
        np.random.set_state(numpy_state)

        # Create an entry for the driver task in the task table. This task is
        # added immediately with status RUNNING. This allows us to push errors
        # related to this driver task back to the driver.  For example, if the
        # driver creates an object that is later evicted, we should notify the
        # user that we're unable to reconstruct the object, since we cannot
        # rerun the driver.
        nil_actor_counter = 0

        function_descriptor = FunctionDescriptor.for_driver_task()
        driver_task = ray._raylet.Task(
            worker.task_driver_id,
            function_descriptor.get_function_descriptor_list(),
            [],  # arguments.
            0,  # num_returns.
            TaskID(random_string()),  # parent_task_id.
            0,  # parent_counter.
            ActorID.nil(),  # actor_creation_id.
            ObjectID.nil(),  # actor_creation_dummy_object_id.
            0,  # max_actor_reconstructions.
            ActorID.nil(),  # actor_id.
            ActorHandleID.nil(),  # actor_handle_id.
            nil_actor_counter,  # actor_counter.
            [],  # new_actor_handles.
            [],  # execution_dependencies.
            {""CPU"": 0},  # resource_map.
            {},  # placement_resource_map.
        )

        # Add the driver task to the task table.
        global_state._execute_command(driver_task.task_id(), ""RAY.TABLE_ADD"",
                                      ray.gcs_utils.TablePrefix.RAYLET_TASK,
                                      ray.gcs_utils.TablePubsub.RAYLET_TASK,
                                      driver_task.task_id().binary(),
                                      driver_task._serialized_raylet_task())

        # Set the driver's current task ID to the task ID assigned to the
        # driver task.
        worker.task_context.current_task_id = driver_task.task_id()

    worker.raylet_client = ray._raylet.RayletClient(
        raylet_socket,
        ClientID(worker.worker_id),
        is_worker,
        DriverID(worker.current_task_id.binary()),
    )

    # Start the import thread
    worker.import_thread = import_thread.ImportThread(worker, mode,
                                                      worker.threads_stopped)
    worker.import_thread.start()

    # If this is a driver running in SCRIPT_MODE, start a thread to print error
    # messages asynchronously in the background. Ideally the scheduler would
    # push messages to the driver's worker service, but we ran into bugs when
    # trying to properly shutdown the driver's worker service, so we are
    # temporarily using this implementation which constantly queries the
    # scheduler for new error messages.
    if mode == SCRIPT_MODE:
        q = queue.Queue()
        worker.listener_thread = threading.Thread(
            target=listen_error_messages_raylet,
            name=""ray_listen_error_messages"",
            args=(worker, q, worker.threads_stopped))
        worker.printer_thread = threading.Thread(
            target=print_error_messages_raylet,
            name=""ray_print_error_messages"",
            args=(q, worker.threads_stopped))
        worker.listener_thread.daemon = True
        worker.listener_thread.start()
        worker.printer_thread.daemon = True
        worker.printer_thread.start()
        if log_to_driver:
            worker.logger_thread = threading.Thread(
                target=print_logs,
                name=""ray_print_logs"",
                args=(worker.redis_client, worker.threads_stopped))
            worker.logger_thread.daemon = True
            worker.logger_thread.start()

    # If we are using the raylet code path and we are not in local mode, start
    # a background thread to periodically flush profiling data to the GCS.
    if mode != LOCAL_MODE:
        worker.profiler.start_flush_thread()

    if mode == SCRIPT_MODE:
        # Add the directory containing the script that is running to the Python
        # paths of the workers. Also add the current directory. Note that this
        # assumes that the directory structures on the machines in the clusters
        # are the same.
        script_directory = os.path.abspath(os.path.dirname(sys.argv[0]))
        current_directory = os.path.abspath(os.path.curdir)
        worker.run_function_on_all_workers(
            lambda worker_info: sys.path.insert(1, script_directory))
        worker.run_function_on_all_workers(
            lambda worker_info: sys.path.insert(1, current_directory))
        # TODO(rkn): Here we first export functions to run, then remote
        # functions. The order matters. For example, one of the functions to
        # run may set the Python path, which is needed to import a module used
        # to define a remote function. We may want to change the order to
        # simply be the order in which the exports were defined on the driver.
        # In addition, we will need to retain the ability to decide what the
        # first few exports are (mostly to set the Python path). Additionally,
        # note that the first exports to be defined on the driver will be the
        # ones defined in separate modules that are imported by the driver.
        # Export cached functions_to_run.
        for function in worker.cached_functions_to_run:
            worker.run_function_on_all_workers(function)
        # Export cached remote functions and actors to the workers.
        worker.function_actor_manager.export_cached()
    worker.cached_functions_to_run = None


def disconnect(worker=global_worker):
    """"""Disconnect this worker from the scheduler and object store.""""""
    # Reset the list of cached remote functions and actors so that if more
    # remote functions or actors are defined and then connect is called again,
    # the remote functions will be exported. This is mostly relevant for the
    # tests.
    if worker.connected:
        # Shutdown all of the threads that we've started. TODO(rkn): This
        # should be handled cleanly in the worker object's destructor and not
        # in this disconnect method.
        worker.threads_stopped.set()
        if hasattr(worker, ""import_thread""):
            worker.import_thread.join_import_thread()
        if hasattr(worker, ""profiler"") and hasattr(worker.profiler, ""t""):
            worker.profiler.join_flush_thread()
        if hasattr(worker, ""listener_thread""):
            worker.listener_thread.join()
        if hasattr(worker, ""printer_thread""):
            worker.printer_thread.join()
        if hasattr(worker, ""logger_thread""):
            worker.logger_thread.join()
        worker.threads_stopped.clear()

    worker.connected = False
    worker.cached_functions_to_run = []
    worker.function_actor_manager.reset_cache()
    worker.serialization_context_map.clear()

    if hasattr(worker, ""raylet_client""):
        del worker.raylet_client
    if hasattr(worker, ""plasma_client""):
        worker.plasma_client.disconnect()


@contextmanager
def _changeproctitle(title, next_title):
    if setproctitle:
        setproctitle.setproctitle(title)
    yield
    if setproctitle:
        setproctitle.setproctitle(next_title)


def _try_to_compute_deterministic_class_id(cls, depth=5):
    """"""Attempt to produce a deterministic class ID for a given class.

    The goal here is for the class ID to be the same when this is run on
    different worker processes. Pickling, loading, and pickling again seems to
    produce more consistent results than simply pickling. This is a bit crazy
    and could cause problems, in which case we should revert it and figure out
    something better.

    Args:
        cls: The class to produce an ID for.
        depth: The number of times to repeatedly try to load and dump the
            string while trying to reach a fixed point.

    Returns:
        A class ID for this class. We attempt to make the class ID the same
            when this function is run on different workers, but that is not
            guaranteed.

    Raises:
        Exception: This could raise an exception if cloudpickle raises an
            exception.
    """"""
    # Pickling, loading, and pickling again seems to produce more consistent
    # results than simply pickling. This is a bit
    class_id = pickle.dumps(cls)
    for _ in range(depth):
        new_class_id = pickle.dumps(pickle.loads(class_id))
        if new_class_id == class_id:
            # We appear to have reached a fix point, so use this as the ID.
            return hashlib.sha1(new_class_id).digest()
        class_id = new_class_id

    # We have not reached a fixed point, so we may end up with a different
    # class ID for this custom class on each worker, which could lead to the
    # same class definition being exported many many times.
    logger.warning(
        ""WARNING: Could not produce a deterministic class ID for class ""
        ""{}"".format(cls))
    return hashlib.sha1(new_class_id).digest()


def register_custom_serializer(cls,
                               use_pickle=False,
                               use_dict=False,
                               serializer=None,
                               deserializer=None,
                               local=False,
                               driver_id=None,
                               class_id=None,
                               worker=global_worker):
    """"""Enable serialization and deserialization for a particular class.

    This method runs the register_class function defined below on every worker,
    which will enable ray to properly serialize and deserialize objects of
    this class.

    Args:
        cls (type): The class that ray should use this custom serializer for.
        use_pickle (bool): If true, then objects of this class will be
            serialized using pickle.
        use_dict: If true, then objects of this class be serialized turning
            their __dict__ fields into a dictionary. Must be False if
            use_pickle is true.
        serializer: The custom serializer to use. This should be provided if
            and only if use_pickle and use_dict are False.
        deserializer: The custom deserializer to use. This should be provided
            if and only if use_pickle and use_dict are False.
        local: True if the serializers should only be registered on the current
            worker. This should usually be False.
        driver_id: ID of the driver that we want to register the class for.
        class_id: ID of the class that we are registering. If this is not
            specified, we will calculate a new one inside the function.

    Raises:
        Exception: An exception is raised if pickle=False and the class cannot
            be efficiently serialized by Ray. This can also raise an exception
            if use_dict is true and cls is not pickleable.
    """"""
    assert (serializer is None) == (deserializer is None), (
        ""The serializer/deserializer arguments must both be provided or ""
        ""both not be provided."")
    use_custom_serializer = (serializer is not None)

    assert use_custom_serializer + use_pickle + use_dict == 1, (
        ""Exactly one of use_pickle, use_dict, or serializer/deserializer must ""
        ""be specified."")

    if use_dict:
        # Raise an exception if cls cannot be serialized efficiently by Ray.
        serialization.check_serializable(cls)

    if class_id is None:
        if not local:
            # In this case, the class ID will be used to deduplicate the class
            # across workers. Note that cloudpickle unfortunately does not
            # produce deterministic strings, so these IDs could be different
            # on different workers. We could use something weaker like
            # cls.__name__, however that would run the risk of having
            # collisions.
            # TODO(rkn): We should improve this.
            try:
                # Attempt to produce a class ID that will be the same on each
                # worker. However, determinism is not guaranteed, and the
                # result may be different on different workers.
                class_id = _try_to_compute_deterministic_class_id(cls)
            except Exception:
                raise serialization.CloudPickleError(""Failed to pickle class ""
                                                     ""'{}'"".format(cls))
        else:
            # In this case, the class ID only needs to be meaningful on this
            # worker and not across workers.
            class_id = random_string()

        # Make sure class_id is a string.
        class_id = ray.utils.binary_to_hex(class_id)

    if driver_id is None:
        driver_id = worker.task_driver_id
    assert isinstance(driver_id, DriverID)

    def register_class_for_serialization(worker_info):
        # TODO(rkn): We need to be more thoughtful about what to do if custom
        # serializers have already been registered for class_id. In some cases,
        # we may want to use the last user-defined serializers and ignore
        # subsequent calls to register_custom_serializer that were made by the
        # system.

        serialization_context = worker_info[
            ""worker""].get_serialization_context(driver_id)
        serialization_context.register_type(
            cls,
            class_id,
            pickle=use_pickle,
            custom_serializer=serializer,
            custom_deserializer=deserializer)

    if not local:
        worker.run_function_on_all_workers(register_class_for_serialization)
    else:
        # Since we are pickling objects of this class, we don't actually need
        # to ship the class definition.
        register_class_for_serialization({""worker"": worker})


def get(object_ids, worker=global_worker):
    """"""Get a remote object or a list of remote objects from the object store.

    This method blocks until the object corresponding to the object ID is
    available in the local object store. If this object is not in the local
    object store, it will be shipped from an object store that has it (once the
    object has been created). If object_ids is a list, then the objects
    corresponding to each object in the list will be returned.

    Args:
        object_ids: Object ID of the object to get or a list of object IDs to
            get.

    Returns:
        A Python object or a list of Python objects.

    Raises:
        Exception: An exception is raised if the task that created the object
            or that created one of the objects raised an exception.
    """"""
    worker.check_connected()
    with profiling.profile(""ray.get"", worker=worker):
        if worker.mode == LOCAL_MODE:
            # In LOCAL_MODE, ray.get is the identity operation (the input will
            # actually be a value not an objectid).
            return object_ids
        global last_task_error_raise_time
        if isinstance(object_ids, list):
            values = worker.get_object(object_ids)
            for i, value in enumerate(values):
                if isinstance(value, RayTaskError):
                    last_task_error_raise_time = time.time()
                    raise value
            return values
        else:
            value = worker.get_object([object_ids])[0]
            if isinstance(value, RayTaskError):
                # If the result is a RayTaskError, then the task that created
                # this object failed, and we should propagate the error message
                # here.
                last_task_error_raise_time = time.time()
                raise value
            return value


def put(value, worker=global_worker):
    """"""Store an object in the object store.

    Args:
        value: The Python object to be stored.

    Returns:
        The object ID assigned to this value.
    """"""
    worker.check_connected()
    with profiling.profile(""ray.put"", worker=worker):
        if worker.mode == LOCAL_MODE:
            # In LOCAL_MODE, ray.put is the identity operation.
            return value
        object_id = ray._raylet.compute_put_id(
            worker.current_task_id,
            worker.task_context.put_index,
        )
        worker.put_object(object_id, value)
        worker.task_context.put_index += 1
        return object_id


def wait(object_ids, num_returns=1, timeout=None, worker=global_worker):
    """"""Return a list of IDs that are ready and a list of IDs that are not.

    .. warning::

        The **timeout** argument used to be in **milliseconds** (up through
        ``ray==0.6.1``) and now it is in **seconds**.

    If timeout is set, the function returns either when the requested number of
    IDs are ready or when the timeout is reached, whichever occurs first. If it
    is not set, the function simply waits until that number of objects is ready
    and returns that exact number of object IDs.

    This method returns two lists. The first list consists of object IDs that
    correspond to objects that are available in the object store. The second
    list corresponds to the rest of the object IDs (which may or may not be
    ready).

    Ordering of the input list of object IDs is preserved. That is, if A
    precedes B in the input list, and both are in the ready list, then A will
    precede B in the ready list. This also holds true if A and B are both in
    the remaining list.

    Args:
        object_ids (List[ObjectID]): List of object IDs for objects that may or
            may not be ready. Note that these IDs must be unique.
        num_returns (int): The number of object IDs that should be returned.
        timeout (float): The maximum amount of time in seconds to wait before
            returning.

    Returns:
        A list of object IDs that are ready and a list of the remaining object
        IDs.
    """"""

    if isinstance(object_ids, ObjectID):
        raise TypeError(
            ""wait() expected a list of ray.ObjectID, got a single ray.ObjectID""
        )

    if not isinstance(object_ids, list):
        raise TypeError(
            ""wait() expected a list of ray.ObjectID, got {}"".format(
                type(object_ids)))

    if isinstance(timeout, int) and timeout != 0:
        logger.warning(""The 'timeout' argument now requires seconds instead ""
                       ""of milliseconds. This message can be suppressed by ""
                       ""passing in a float."")

    if timeout is not None and timeout < 0:
        raise ValueError(""The 'timeout' argument must be nonnegative. ""
                         ""Received {}"".format(timeout))

    if worker.mode != LOCAL_MODE:
        for object_id in object_ids:
            if not isinstance(object_id, ObjectID):
                raise TypeError(""wait() expected a list of ray.ObjectID, ""
                                ""got list containing {}"".format(
                                    type(object_id)))

    worker.check_connected()
    # TODO(swang): Check main thread.
    with profiling.profile(""ray.wait"", worker=worker):
        # When Ray is run in LOCAL_MODE, all functions are run immediately,
        # so all objects in object_id are ready.
        if worker.mode == LOCAL_MODE:
            return object_ids[:num_returns], object_ids[num_returns:]

        # TODO(rkn): This is a temporary workaround for
        # https://github.com/ray-project/ray/issues/997. However, it should be
        # fixed in Arrow instead of here.
        if len(object_ids) == 0:
            return [], []

        if len(object_ids) != len(set(object_ids)):
            raise Exception(""Wait requires a list of unique object IDs."")
        if num_returns <= 0:
            raise Exception(
                ""Invalid number of objects to return %d."" % num_returns)
        if num_returns > len(object_ids):
            raise Exception(""num_returns cannot be greater than the number ""
                            ""of objects provided to ray.wait."")

        timeout = timeout if timeout is not None else 10**6
        timeout_milliseconds = int(timeout * 1000)
        ready_ids, remaining_ids = worker.raylet_client.wait(
            object_ids,
            num_returns,
            timeout_milliseconds,
            False,
            worker.current_task_id,
        )
        return ready_ids, remaining_ids


def _mode(worker=global_worker):
    """"""This is a wrapper around worker.mode.

    We use this wrapper so that in the remote decorator, we can call _mode()
    instead of worker.mode. The difference is that when we attempt to serialize
    remote functions, we don't attempt to serialize the worker object, which
    cannot be serialized.
    """"""
    return worker.mode


def get_global_worker():
    return global_worker


def make_decorator(num_return_vals=None,
                   num_cpus=None,
                   num_gpus=None,
                   resources=None,
                   max_calls=None,
                   checkpoint_interval=None,
                   max_reconstructions=None,
                   worker=None):
    def decorator(function_or_class):
        if (inspect.isfunction(function_or_class)
                or is_cython(function_or_class)):
            # Set the remote function default resources.
            if checkpoint_interval is not None:
                raise Exception(""The keyword 'checkpoint_interval' is not ""
                                ""allowed for remote functions."")
            if max_reconstructions is not None:
                raise Exception(""The keyword 'max_reconstructions' is not ""
                                ""allowed for remote functions."")

            return ray.remote_function.RemoteFunction(
                function_or_class, num_cpus, num_gpus, resources,
                num_return_vals, max_calls)

        if inspect.isclass(function_or_class):
            if num_return_vals is not None:
                raise Exception(""The keyword 'num_return_vals' is not allowed ""
                                ""for actors."")
            if max_calls is not None:
                raise Exception(""The keyword 'max_calls' is not allowed for ""
                                ""actors."")

            # Set the actor default resources.
            if num_cpus is None and num_gpus is None and resources is None:
                # In the default case, actors acquire no resources for
                # their lifetime, and actor methods will require 1 CPU.
                cpus_to_use = DEFAULT_ACTOR_CREATION_CPUS_SIMPLE_CASE
                actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SIMPLE_CASE
            else:
                # If any resources are specified, then all resources are
                # acquired for the actor's lifetime and no resources are
                # associated with methods.
                cpus_to_use = (DEFAULT_ACTOR_CREATION_CPUS_SPECIFIED_CASE
                               if num_cpus is None else num_cpus)
                actor_method_cpus = DEFAULT_ACTOR_METHOD_CPUS_SPECIFIED_CASE

            return worker.make_actor(function_or_class, cpus_to_use, num_gpus,
                                     resources, actor_method_cpus,
                                     checkpoint_interval, max_reconstructions)

        raise Exception(""The @ray.remote decorator must be applied to ""
                        ""either a function or to a class."")

    return decorator


def remote(*args, **kwargs):
    """"""Define a remote function or an actor class.

    This can be used with no arguments to define a remote function or actor as
    follows:

    .. code-block:: python

        @ray.remote
        def f():
            return 1

        @ray.remote
        class Foo(object):
            def method(self):
                return 1

    It can also be used with specific keyword arguments:

    * **num_return_vals:** This is only for *remote functions*. It specifies
      the number of object IDs returned by the remote function invocation.
    * **num_cpus:** The quantity of CPU cores to reserve for this task or for
      the lifetime of the actor.
    * **num_gpus:** The quantity of GPUs to reserve for this task or for the
      lifetime of the actor.
    * **resources:** The quantity of various custom resources to reserve for
      this task or for the lifetime of the actor. This is a dictionary mapping
      strings (resource names) to numbers.
    * **max_calls:** Only for *remote functions*. This specifies the maximum
      number of times that a given worker can execute the given remote function
      before it must exit (this can be used to address memory leaks in
      third-party libraries or to reclaim resources that cannot easily be
      released, e.g., GPU memory that was acquired by TensorFlow). By
      default this is infinite.
    * **max_reconstructions**: Only for *actors*. This specifies the maximum
      number of times that the actor should be reconstructed when it dies
      unexpectedly. The minimum valid value is 0 (default), which indicates
      that the actor doesn't need to be reconstructed. And the maximum valid
      value is ray.ray_constants.INFINITE_RECONSTRUCTIONS.

    This can be done as follows:

    .. code-block:: python

        @ray.remote(num_gpus=1, max_calls=1, num_return_vals=2)
        def f():
            return 1, 2

        @ray.remote(num_cpus=2, resources={""CustomResource"": 1})
        class Foo(object):
            def method(self):
                return 1
    """"""
    worker = get_global_worker()

    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        # This is the case where the decorator is just @ray.remote.
        return make_decorator(worker=worker)(args[0])

    # Parse the keyword arguments from the decorator.
    error_string = (""The @ray.remote decorator must be applied either ""
                    ""with no arguments and no parentheses, for example ""
                    ""'@ray.remote', or it must be applied using some of ""
                    ""the arguments 'num_return_vals', 'num_cpus', 'num_gpus', ""
                    ""'resources', 'max_calls', 'checkpoint_interval',""
                    ""or 'max_reconstructions', like ""
                    ""'@ray.remote(num_return_vals=2, ""
                    ""resources={\""CustomResource\"": 1})'."")
    assert len(args) == 0 and len(kwargs) > 0, error_string
    for key in kwargs:
        assert key in [
            ""num_return_vals"", ""num_cpus"", ""num_gpus"", ""resources"",
            ""max_calls"", ""checkpoint_interval"", ""max_reconstructions""
        ], error_string

    num_cpus = kwargs[""num_cpus""] if ""num_cpus"" in kwargs else None
    num_gpus = kwargs[""num_gpus""] if ""num_gpus"" in kwargs else None
    resources = kwargs.get(""resources"")
    if not isinstance(resources, dict) and resources is not None:
        raise Exception(""The 'resources' keyword argument must be a ""
                        ""dictionary, but received type {}."".format(
                            type(resources)))
    if resources is not None:
        assert ""CPU"" not in resources, ""Use the 'num_cpus' argument.""
        assert ""GPU"" not in resources, ""Use the 'num_gpus' argument.""

    # Handle other arguments.
    num_return_vals = kwargs.get(""num_return_vals"")
    max_calls = kwargs.get(""max_calls"")
    checkpoint_interval = kwargs.get(""checkpoint_interval"")
    max_reconstructions = kwargs.get(""max_reconstructions"")

    return make_decorator(
        num_return_vals=num_return_vals,
        num_cpus=num_cpus,
        num_gpus=num_gpus,
        resources=resources,
        max_calls=max_calls,
        checkpoint_interval=checkpoint_interval,
        max_reconstructions=max_reconstructions,
        worker=worker)
/n/n/ntest/runtest.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import logging
import os
import random
import re
import setproctitle
import shutil
import string
import subprocess
import sys
import tempfile
import threading
import time
from collections import defaultdict, namedtuple, OrderedDict
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import pickle
import pytest

import ray
import ray.test.cluster_utils
import ray.test.test_utils
from ray.utils import _random_string

logger = logging.getLogger(__name__)


def assert_equal(obj1, obj2):
    module_numpy = (type(obj1).__module__ == np.__name__
                    or type(obj2).__module__ == np.__name__)
    if module_numpy:
        empty_shape = ((hasattr(obj1, ""shape"") and obj1.shape == ())
                       or (hasattr(obj2, ""shape"") and obj2.shape == ()))
        if empty_shape:
            # This is a special case because currently np.testing.assert_equal
            # fails because we do not properly handle different numerical
            # types.
            assert obj1 == obj2, (""Objects {} and {} are ""
                                  ""different."".format(obj1, obj2))
        else:
            np.testing.assert_equal(obj1, obj2)
    elif hasattr(obj1, ""__dict__"") and hasattr(obj2, ""__dict__""):
        special_keys = [""_pytype_""]
        assert (set(list(obj1.__dict__.keys()) + special_keys) == set(
            list(obj2.__dict__.keys()) + special_keys)), (""Objects {} ""
                                                          ""and {} are ""
                                                          ""different."".format(
                                                              obj1, obj2))
        for key in obj1.__dict__.keys():
            if key not in special_keys:
                assert_equal(obj1.__dict__[key], obj2.__dict__[key])
    elif type(obj1) is dict or type(obj2) is dict:
        assert_equal(obj1.keys(), obj2.keys())
        for key in obj1.keys():
            assert_equal(obj1[key], obj2[key])
    elif type(obj1) is list or type(obj2) is list:
        assert len(obj1) == len(obj2), (""Objects {} and {} are lists with ""
                                        ""different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    elif type(obj1) is tuple or type(obj2) is tuple:
        assert len(obj1) == len(obj2), (""Objects {} and {} are tuples with ""
                                        ""different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    elif (ray.serialization.is_named_tuple(type(obj1))
          or ray.serialization.is_named_tuple(type(obj2))):
        assert len(obj1) == len(obj2), (""Objects {} and {} are named tuples ""
                                        ""with different lengths."".format(
                                            obj1, obj2))
        for i in range(len(obj1)):
            assert_equal(obj1[i], obj2[i])
    else:
        assert obj1 == obj2, ""Objects {} and {} are different."".format(
            obj1, obj2)


if sys.version_info >= (3, 0):
    long_extras = [0, np.array([[""hi"", u""hi""], [1.3, 1]])]
else:

    long_extras = [
        long(0),  # noqa: E501,F821
        np.array([
            [""hi"", u""hi""],
            [1.3, long(1)]  # noqa: E501,F821
        ])
    ]

PRIMITIVE_OBJECTS = [
    0, 0.0, 0.9, 1 << 62, 1 << 100, 1 << 999, [1 << 100, [1 << 100]], ""a"",
    string.printable, ""\u262F"", u""hello world"", u""\xff\xfe\x9c\x001\x000\x00"",
    None, True, False, [], (), {},
    np.int8(3),
    np.int32(4),
    np.int64(5),
    np.uint8(3),
    np.uint32(4),
    np.uint64(5),
    np.float32(1.9),
    np.float64(1.9),
    np.zeros([100, 100]),
    np.random.normal(size=[100, 100]),
    np.array([""hi"", 3]),
    np.array([""hi"", 3], dtype=object)
] + long_extras

COMPLEX_OBJECTS = [
    [[[[[[[[[[[[]]]]]]]]]]]],
    {""obj{}"".format(i): np.random.normal(size=[100, 100])
     for i in range(10)},
    # {(): {(): {(): {(): {(): {(): {(): {(): {(): {(): {
    #      (): {(): {}}}}}}}}}}}}},
    (
        (((((((((), ), ), ), ), ), ), ), ), ),
    {
        ""a"": {
            ""b"": {
                ""c"": {
                    ""d"": {}
                }
            }
        }
    }
]


class Foo(object):
    def __init__(self, value=0):
        self.value = value

    def __hash__(self):
        return hash(self.value)

    def __eq__(self, other):
        return other.value == self.value


class Bar(object):
    def __init__(self):
        for i, val in enumerate(PRIMITIVE_OBJECTS + COMPLEX_OBJECTS):
            setattr(self, ""field{}"".format(i), val)


class Baz(object):
    def __init__(self):
        self.foo = Foo()
        self.bar = Bar()

    def method(self, arg):
        pass


class Qux(object):
    def __init__(self):
        self.objs = [Foo(), Bar(), Baz()]


class SubQux(Qux):
    def __init__(self):
        Qux.__init__(self)


class CustomError(Exception):
    pass


Point = namedtuple(""Point"", [""x"", ""y""])
NamedTupleExample = namedtuple(""Example"",
                               ""field1, field2, field3, field4, field5"")

CUSTOM_OBJECTS = [
    Exception(""Test object.""),
    CustomError(),
    Point(11, y=22),
    Foo(),
    Bar(),
    Baz(),  # Qux(), SubQux(),
    NamedTupleExample(1, 1.0, ""hi"", np.zeros([3, 5]), [1, 2, 3])
]

BASE_OBJECTS = PRIMITIVE_OBJECTS + COMPLEX_OBJECTS + CUSTOM_OBJECTS

LIST_OBJECTS = [[obj] for obj in BASE_OBJECTS]
TUPLE_OBJECTS = [(obj, ) for obj in BASE_OBJECTS]
# The check that type(obj).__module__ != ""numpy"" should be unnecessary, but
# otherwise this seems to fail on Mac OS X on Travis.
DICT_OBJECTS = (
    [{
        obj: obj
    } for obj in PRIMITIVE_OBJECTS
     if (obj.__hash__ is not None and type(obj).__module__ != ""numpy"")] + [{
         0: obj
     } for obj in BASE_OBJECTS] + [{
         Foo(123): Foo(456)
     }])

RAY_TEST_OBJECTS = BASE_OBJECTS + LIST_OBJECTS + TUPLE_OBJECTS + DICT_OBJECTS


@pytest.fixture
def ray_start():
    # Start the Ray processes.
    ray.init(num_cpus=1)
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()


@pytest.fixture
def shutdown_only():
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()


def test_passing_arguments_by_value(ray_start):
    @ray.remote
    def f(x):
        return x

    # Check that we can pass arguments by value to remote functions and
    # that they are uncorrupted.
    for obj in RAY_TEST_OBJECTS:
        assert_equal(obj, ray.get(f.remote(obj)))


def test_ray_recursive_objects(ray_start):
    class ClassA(object):
        pass

    # Make a list that contains itself.
    lst = []
    lst.append(lst)
    # Make an object that contains itself as a field.
    a1 = ClassA()
    a1.field = a1
    # Make two objects that contain each other as fields.
    a2 = ClassA()
    a3 = ClassA()
    a2.field = a3
    a3.field = a2
    # Make a dictionary that contains itself.
    d1 = {}
    d1[""key""] = d1
    # Create a list of recursive objects.
    recursive_objects = [lst, a1, a2, a3, d1]

    # Check that exceptions are thrown when we serialize the recursive
    # objects.
    for obj in recursive_objects:
        with pytest.raises(Exception):
            ray.put(obj)


def test_passing_arguments_by_value_out_of_the_box(ray_start):
    @ray.remote
    def f(x):
        return x

    # Test passing lambdas.

    def temp():
        return 1

    assert ray.get(f.remote(temp))() == 1
    assert ray.get(f.remote(lambda x: x + 1))(3) == 4

    # Test sets.
    assert ray.get(f.remote(set())) == set()
    s = {1, (1, 2, ""hi"")}
    assert ray.get(f.remote(s)) == s

    # Test types.
    assert ray.get(f.remote(int)) == int
    assert ray.get(f.remote(float)) == float
    assert ray.get(f.remote(str)) == str

    class Foo(object):
        def __init__(self):
            pass

    # Make sure that we can put and get a custom type. Note that the result
    # won't be ""equal"" to Foo.
    ray.get(ray.put(Foo))


def test_putting_object_that_closes_over_object_id(ray_start):
    # This test is here to prevent a regression of
    # https://github.com/ray-project/ray/issues/1317.

    class Foo(object):
        def __init__(self):
            self.val = ray.put(0)

        def method(self):
            f

    f = Foo()
    ray.put(f)


def test_put_get(shutdown_only):
    ray.init(num_cpus=0)

    for i in range(100):
        value_before = i * 10**6
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = i * 10**6 * 1.0
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = ""h"" * i
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after

    for i in range(100):
        value_before = [1] * i
        objectid = ray.put(value_before)
        value_after = ray.get(objectid)
        assert value_before == value_after


def test_custom_serializers(shutdown_only):
    ray.init(num_cpus=1)

    class Foo(object):
        def __init__(self):
            self.x = 3

    def custom_serializer(obj):
        return 3, ""string1"", type(obj).__name__

    def custom_deserializer(serialized_obj):
        return serialized_obj, ""string2""

    ray.register_custom_serializer(
        Foo, serializer=custom_serializer, deserializer=custom_deserializer)

    assert ray.get(ray.put(Foo())) == ((3, ""string1"", Foo.__name__), ""string2"")

    class Bar(object):
        def __init__(self):
            self.x = 3

    ray.register_custom_serializer(
        Bar, serializer=custom_serializer, deserializer=custom_deserializer)

    @ray.remote
    def f():
        return Bar()

    assert ray.get(f.remote()) == ((3, ""string1"", Bar.__name__), ""string2"")


def test_serialization_final_fallback(ray_start):
    pytest.importorskip(""catboost"")
    # This test will only run when ""catboost"" is installed.
    from catboost import CatBoostClassifier

    model = CatBoostClassifier(
        iterations=2,
        depth=2,
        learning_rate=1,
        loss_function=""Logloss"",
        logging_level=""Verbose"")

    reconstructed_model = ray.get(ray.put(model))
    assert set(model.get_params().items()) == set(
        reconstructed_model.get_params().items())


def test_register_class(shutdown_only):
    ray.init(num_cpus=2)

    # Check that putting an object of a class that has not been registered
    # throws an exception.
    class TempClass(object):
        pass

    ray.get(ray.put(TempClass()))

    # Test subtypes of dictionaries.
    value_before = OrderedDict([(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    value_before = defaultdict(lambda: 0, [(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    value_before = defaultdict(lambda: [], [(""hello"", 1), (""world"", 2)])
    object_id = ray.put(value_before)
    assert value_before == ray.get(object_id)

    # Test passing custom classes into remote functions from the driver.
    @ray.remote
    def f(x):
        return x

    foo = ray.get(f.remote(Foo(7)))
    assert foo == Foo(7)

    regex = re.compile(r""\d+\.\d*"")
    new_regex = ray.get(f.remote(regex))
    # This seems to fail on the system Python 3 that comes with
    # Ubuntu, so it is commented out for now:
    # assert regex == new_regex
    # Instead, we do this:
    assert regex.pattern == new_regex.pattern

    # Test returning custom classes created on workers.
    @ray.remote
    def g():
        return SubQux(), Qux()

    subqux, qux = ray.get(g.remote())
    assert subqux.objs[2].foo.value == 0

    # Test exporting custom class definitions from one worker to another
    # when the worker is blocked in a get.
    class NewTempClass(object):
        def __init__(self, value):
            self.value = value

    @ray.remote
    def h1(x):
        return NewTempClass(x)

    @ray.remote
    def h2(x):
        return ray.get(h1.remote(x))

    assert ray.get(h2.remote(10)).value == 10

    # Test registering multiple classes with the same name.
    @ray.remote(num_return_vals=3)
    def j():
        class Class0(object):
            def method0(self):
                pass

        c0 = Class0()

        class Class0(object):
            def method1(self):
                pass

        c1 = Class0()

        class Class0(object):
            def method2(self):
                pass

        c2 = Class0()

        return c0, c1, c2

    results = []
    for _ in range(5):
        results += j.remote()
    for i in range(len(results) // 3):
        c0, c1, c2 = ray.get(results[(3 * i):(3 * (i + 1))])

        c0.method0()
        c1.method1()
        c2.method2()

        assert not hasattr(c0, ""method1"")
        assert not hasattr(c0, ""method2"")
        assert not hasattr(c1, ""method0"")
        assert not hasattr(c1, ""method2"")
        assert not hasattr(c2, ""method0"")
        assert not hasattr(c2, ""method1"")

    @ray.remote
    def k():
        class Class0(object):
            def method0(self):
                pass

        c0 = Class0()

        class Class0(object):
            def method1(self):
                pass

        c1 = Class0()

        class Class0(object):
            def method2(self):
                pass

        c2 = Class0()

        return c0, c1, c2

    results = ray.get([k.remote() for _ in range(5)])
    for c0, c1, c2 in results:
        c0.method0()
        c1.method1()
        c2.method2()

        assert not hasattr(c0, ""method1"")
        assert not hasattr(c0, ""method2"")
        assert not hasattr(c1, ""method0"")
        assert not hasattr(c1, ""method2"")
        assert not hasattr(c2, ""method0"")
        assert not hasattr(c2, ""method1"")


def test_keyword_args(shutdown_only):
    @ray.remote
    def keyword_fct1(a, b=""hello""):
        return ""{} {}"".format(a, b)

    @ray.remote
    def keyword_fct2(a=""hello"", b=""world""):
        return ""{} {}"".format(a, b)

    @ray.remote
    def keyword_fct3(a, b, c=""hello"", d=""world""):
        return ""{} {} {} {}"".format(a, b, c, d)

    ray.init(num_cpus=1)

    x = keyword_fct1.remote(1)
    assert ray.get(x) == ""1 hello""
    x = keyword_fct1.remote(1, ""hi"")
    assert ray.get(x) == ""1 hi""
    x = keyword_fct1.remote(1, b=""world"")
    assert ray.get(x) == ""1 world""
    x = keyword_fct1.remote(a=1, b=""world"")
    assert ray.get(x) == ""1 world""

    x = keyword_fct2.remote(a=""w"", b=""hi"")
    assert ray.get(x) == ""w hi""
    x = keyword_fct2.remote(b=""hi"", a=""w"")
    assert ray.get(x) == ""w hi""
    x = keyword_fct2.remote(a=""w"")
    assert ray.get(x) == ""w world""
    x = keyword_fct2.remote(b=""hi"")
    assert ray.get(x) == ""hello hi""
    x = keyword_fct2.remote(""w"")
    assert ray.get(x) == ""w world""
    x = keyword_fct2.remote(""w"", ""hi"")
    assert ray.get(x) == ""w hi""

    x = keyword_fct3.remote(0, 1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, b=1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(a=0, b=1, c=""w"", d=""hi"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, 1, d=""hi"", c=""w"")
    assert ray.get(x) == ""0 1 w hi""
    x = keyword_fct3.remote(0, 1, c=""w"")
    assert ray.get(x) == ""0 1 w world""
    x = keyword_fct3.remote(0, 1, d=""hi"")
    assert ray.get(x) == ""0 1 hello hi""
    x = keyword_fct3.remote(0, 1)
    assert ray.get(x) == ""0 1 hello world""
    x = keyword_fct3.remote(a=0, b=1)
    assert ray.get(x) == ""0 1 hello world""

    # Check that we cannot pass invalid keyword arguments to functions.
    @ray.remote
    def f1():
        return

    @ray.remote
    def f2(x, y=0, z=0):
        return

    # Make sure we get an exception if too many arguments are passed in.
    with pytest.raises(Exception):
        f1.remote(3)

    with pytest.raises(Exception):
        f1.remote(x=3)

    with pytest.raises(Exception):
        f2.remote(0, w=0)

    with pytest.raises(Exception):
        f2.remote(3, x=3)

    # Make sure we get an exception if too many arguments are passed in.
    with pytest.raises(Exception):
        f2.remote(1, 2, 3, 4)

    @ray.remote
    def f3(x):
        return x

    assert ray.get(f3.remote(4)) == 4


def test_variable_number_of_args(shutdown_only):
    @ray.remote
    def varargs_fct1(*a):
        return "" "".join(map(str, a))

    @ray.remote
    def varargs_fct2(a, *b):
        return "" "".join(map(str, b))

    try:

        @ray.remote
        def kwargs_throw_exception(**c):
            return ()

        kwargs_exception_thrown = False
    except Exception:
        kwargs_exception_thrown = True

    ray.init(num_cpus=1)

    x = varargs_fct1.remote(0, 1, 2)
    assert ray.get(x) == ""0 1 2""
    x = varargs_fct2.remote(0, 1, 2)
    assert ray.get(x) == ""1 2""

    assert kwargs_exception_thrown

    @ray.remote
    def f1(*args):
        return args

    @ray.remote
    def f2(x, y, *args):
        return x, y, args

    assert ray.get(f1.remote()) == ()
    assert ray.get(f1.remote(1)) == (1, )
    assert ray.get(f1.remote(1, 2, 3)) == (1, 2, 3)
    with pytest.raises(Exception):
        f2.remote()
    with pytest.raises(Exception):
        f2.remote(1)
    assert ray.get(f2.remote(1, 2)) == (1, 2, ())
    assert ray.get(f2.remote(1, 2, 3)) == (1, 2, (3, ))
    assert ray.get(f2.remote(1, 2, 3, 4)) == (1, 2, (3, 4))

    def testNoArgs(self):
        @ray.remote
        def no_op():
            pass

        self.init_ray()

        ray.get(no_op.remote())


def test_defining_remote_functions(shutdown_only):
    ray.init(num_cpus=3)

    # Test that we can define a remote function in the shell.
    @ray.remote
    def f(x):
        return x + 1

    assert ray.get(f.remote(0)) == 1

    # Test that we can redefine the remote function.
    @ray.remote
    def f(x):
        return x + 10

    while True:
        val = ray.get(f.remote(0))
        assert val in [1, 10]
        if val == 10:
            break
        else:
            logger.info(""Still using old definition of f, trying again."")

    # Test that we can close over plain old data.
    data = [
        np.zeros([3, 5]), (1, 2, ""a""), [0.0, 1.0, 1 << 62], 1 << 60, {
            ""a"": np.zeros(3)
        }
    ]

    @ray.remote
    def g():
        return data

    ray.get(g.remote())

    # Test that we can close over modules.
    @ray.remote
    def h():
        return np.zeros([3, 5])

    assert_equal(ray.get(h.remote()), np.zeros([3, 5]))

    @ray.remote
    def j():
        return time.time()

    ray.get(j.remote())

    # Test that we can define remote functions that call other remote
    # functions.
    @ray.remote
    def k(x):
        return x + 1

    @ray.remote
    def k2(x):
        return ray.get(k.remote(x))

    @ray.remote
    def m(x):
        return ray.get(k2.remote(x))

    assert ray.get(k.remote(1)) == 2
    assert ray.get(k2.remote(1)) == 2
    assert ray.get(m.remote(1)) == 2

    def test_submit_api(shutdown_only):
        ray.init(num_cpus=1, num_gpus=1, resources={""Custom"": 1})

        @ray.remote
        def f(n):
            return list(range(n))

        @ray.remote
        def g():
            return ray.get_gpu_ids()

        assert f._remote([0], num_return_vals=0) is None
        id1 = f._remote(args=[1], num_return_vals=1)
        assert ray.get(id1) == [0]
        id1, id2 = f._remote(args=[2], num_return_vals=2)
        assert ray.get([id1, id2]) == [0, 1]
        id1, id2, id3 = f._remote(args=[3], num_return_vals=3)
        assert ray.get([id1, id2, id3]) == [0, 1, 2]
        assert ray.get(
            g._remote(
                args=[], num_cpus=1, num_gpus=1,
                resources={""Custom"": 1})) == [0]
        infeasible_id = g._remote(args=[], resources={""NonexistentCustom"": 1})
        ready_ids, remaining_ids = ray.wait([infeasible_id], timeout=0.05)
        assert len(ready_ids) == 0
        assert len(remaining_ids) == 1

        @ray.remote
        class Actor(object):
            def __init__(self, x, y=0):
                self.x = x
                self.y = y

            def method(self, a, b=0):
                return self.x, self.y, a, b

            def gpu_ids(self):
                return ray.get_gpu_ids()

        a = Actor._remote(
            args=[0], kwargs={""y"": 1}, num_gpus=1, resources={""Custom"": 1})

        id1, id2, id3, id4 = a.method._remote(
            args=[""test""], kwargs={""b"": 2}, num_return_vals=4)
        assert ray.get([id1, id2, id3, id4]) == [0, 1, ""test"", 2]


def test_get_multiple(shutdown_only):
    ray.init(num_cpus=1)
    object_ids = [ray.put(i) for i in range(10)]
    assert ray.get(object_ids) == list(range(10))

    # Get a random choice of object IDs with duplicates.
    indices = list(np.random.choice(range(10), 5))
    indices += indices
    results = ray.get([object_ids[i] for i in indices])
    assert results == indices


def test_get_multiple_experimental(shutdown_only):
    ray.init(num_cpus=1)
    object_ids = [ray.put(i) for i in range(10)]

    object_ids_tuple = tuple(object_ids)
    assert ray.experimental.get(object_ids_tuple) == list(range(10))

    object_ids_nparray = np.array(object_ids)
    assert ray.experimental.get(object_ids_nparray) == list(range(10))


def test_get_dict(shutdown_only):
    ray.init(num_cpus=1)
    d = {str(i): ray.put(i) for i in range(5)}
    for i in range(5, 10):
        d[str(i)] = i
    result = ray.experimental.get(d)
    expected = {str(i): i for i in range(10)}
    assert result == expected


def test_wait(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    objectids = [f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    ready_ids, remaining_ids = ray.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3
    ready_ids, remaining_ids = ray.wait(objectids, num_returns=4)
    assert set(ready_ids) == set(objectids)
    assert remaining_ids == []

    objectids = [f.remote(0.5), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    start_time = time.time()
    ready_ids, remaining_ids = ray.wait(objectids, timeout=1.75, num_returns=4)
    assert time.time() - start_time < 2
    assert len(ready_ids) == 3
    assert len(remaining_ids) == 1
    ray.wait(objectids)
    objectids = [f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5)]
    start_time = time.time()
    ready_ids, remaining_ids = ray.wait(objectids, timeout=5.0)
    assert time.time() - start_time < 5
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3

    # Verify that calling wait with duplicate object IDs throws an
    # exception.
    x = ray.put(1)
    with pytest.raises(Exception):
        ray.wait([x, x])

    # Make sure it is possible to call wait with an empty list.
    ready_ids, remaining_ids = ray.wait([])
    assert ready_ids == []
    assert remaining_ids == []

    # Test semantics of num_returns with no timeout.
    oids = [ray.put(i) for i in range(10)]
    (found, rest) = ray.wait(oids, num_returns=2)
    assert len(found) == 2
    assert len(rest) == 8

    # Verify that incorrect usage raises a TypeError.
    x = ray.put(1)
    with pytest.raises(TypeError):
        ray.wait(x)
    with pytest.raises(TypeError):
        ray.wait(1)
    with pytest.raises(TypeError):
        ray.wait([1])


def test_wait_iterables(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    objectids = (f.remote(1.0), f.remote(0.5), f.remote(0.5), f.remote(0.5))
    ready_ids, remaining_ids = ray.experimental.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3

    objectids = np.array(
        [f.remote(1.0),
         f.remote(0.5),
         f.remote(0.5),
         f.remote(0.5)])
    ready_ids, remaining_ids = ray.experimental.wait(objectids)
    assert len(ready_ids) == 1
    assert len(remaining_ids) == 3


def test_multiple_waits_and_gets(shutdown_only):
    # It is important to use three workers here, so that the three tasks
    # launched in this experiment can run at the same time.
    ray.init(num_cpus=3)

    @ray.remote
    def f(delay):
        time.sleep(delay)
        return 1

    @ray.remote
    def g(l):
        # The argument l should be a list containing one object ID.
        ray.wait([l[0]])

    @ray.remote
    def h(l):
        # The argument l should be a list containing one object ID.
        ray.get(l[0])

    # Make sure that multiple wait requests involving the same object ID
    # all return.
    x = f.remote(1)
    ray.get([g.remote([x]), g.remote([x])])

    # Make sure that multiple get requests involving the same object ID all
    # return.
    x = f.remote(1)
    ray.get([h.remote([x]), h.remote([x])])


def test_caching_functions_to_run(shutdown_only):
    # Test that we export functions to run on all workers before the driver
    # is connected.
    def f(worker_info):
        sys.path.append(1)

    ray.worker.global_worker.run_function_on_all_workers(f)

    def f(worker_info):
        sys.path.append(2)

    ray.worker.global_worker.run_function_on_all_workers(f)

    def g(worker_info):
        sys.path.append(3)

    ray.worker.global_worker.run_function_on_all_workers(g)

    def f(worker_info):
        sys.path.append(4)

    ray.worker.global_worker.run_function_on_all_workers(f)

    ray.init(num_cpus=1)

    @ray.remote
    def get_state():
        time.sleep(1)
        return sys.path[-4], sys.path[-3], sys.path[-2], sys.path[-1]

    res1 = get_state.remote()
    res2 = get_state.remote()
    assert ray.get(res1) == (1, 2, 3, 4)
    assert ray.get(res2) == (1, 2, 3, 4)

    # Clean up the path on the workers.
    def f(worker_info):
        sys.path.pop()
        sys.path.pop()
        sys.path.pop()
        sys.path.pop()

    ray.worker.global_worker.run_function_on_all_workers(f)


def test_running_function_on_all_workers(shutdown_only):
    ray.init(num_cpus=1)

    def f(worker_info):
        sys.path.append(""fake_directory"")

    ray.worker.global_worker.run_function_on_all_workers(f)

    @ray.remote
    def get_path1():
        return sys.path

    assert ""fake_directory"" == ray.get(get_path1.remote())[-1]

    def f(worker_info):
        sys.path.pop(-1)

    ray.worker.global_worker.run_function_on_all_workers(f)

    # Create a second remote function to guarantee that when we call
    # get_path2.remote(), the second function to run will have been run on
    # the worker.
    @ray.remote
    def get_path2():
        return sys.path

    assert ""fake_directory"" not in ray.get(get_path2.remote())


def test_profiling_api(shutdown_only):
    ray.init(num_cpus=2)

    @ray.remote
    def f():
        with ray.profile(
                ""custom_event"",
                extra_data={""name"": ""custom name""}) as ray_prof:
            ray_prof.set_attribute(""key"", ""value"")

    ray.put(1)
    object_id = f.remote()
    ray.wait([object_id])
    ray.get(object_id)

    # Wait until all of the profiling information appears in the profile
    # table.
    timeout_seconds = 20
    start_time = time.time()
    while True:
        if time.time() - start_time > timeout_seconds:
            raise Exception(""Timed out while waiting for information in ""
                            ""profile table."")
        profile_data = ray.global_state.chrome_tracing_dump()
        event_types = {event[""cat""] for event in profile_data}
        expected_types = [
            ""worker_idle"",
            ""task"",
            ""task:deserialize_arguments"",
            ""task:execute"",
            ""task:store_outputs"",
            ""wait_for_function"",
            ""ray.get"",
            ""ray.put"",
            ""ray.wait"",
            ""submit_task"",
            ""fetch_and_run_function"",
            ""register_remote_function"",
            ""custom_event"",  # This is the custom one from ray.profile.
        ]

        if all(expected_type in event_types
               for expected_type in expected_types):
            break


@pytest.fixture()
def ray_start_cluster():
    cluster = ray.test.cluster_utils.Cluster()
    yield cluster

    # The code after the yield will run as teardown code.
    ray.shutdown()
    cluster.shutdown()


def test_object_transfer_dump(ray_start_cluster):
    cluster = ray_start_cluster

    num_nodes = 3
    # Set the inline object size to 0 to force all objects to be written to
    # plasma.
    config = json.dumps({""inline_object_max_size_bytes"": 0})
    for i in range(num_nodes):
        cluster.add_node(
            resources={str(i): 1},
            object_store_memory=10**9,
            _internal_config=config)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f(x):
        return

    # These objects will live on different nodes.
    object_ids = [
        f._remote(args=[1], resources={str(i): 1}) for i in range(num_nodes)
    ]

    # Broadcast each object from each machine to each other machine.
    for object_id in object_ids:
        ray.get([
            f._remote(args=[object_id], resources={str(i): 1})
            for i in range(num_nodes)
        ])

    # The profiling information only flushes once every second.
    time.sleep(1.1)

    transfer_dump = ray.global_state.chrome_tracing_object_transfer_dump()
    # Make sure the transfer dump can be serialized with JSON.
    json.loads(json.dumps(transfer_dump))
    assert len(transfer_dump) >= num_nodes**2
    assert len({
        event[""pid""]
        for event in transfer_dump if event[""name""] == ""transfer_receive""
    }) == num_nodes
    assert len({
        event[""pid""]
        for event in transfer_dump if event[""name""] == ""transfer_send""
    }) == num_nodes


def test_identical_function_names(shutdown_only):
    # Define a bunch of remote functions and make sure that we don't
    # accidentally call an older version.
    ray.init(num_cpus=1)

    num_calls = 200

    @ray.remote
    def f():
        return 1

    results1 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 2

    results2 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 3

    results3 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 4

    results4 = [f.remote() for _ in range(num_calls)]

    @ray.remote
    def f():
        return 5

    results5 = [f.remote() for _ in range(num_calls)]

    assert ray.get(results1) == num_calls * [1]
    assert ray.get(results2) == num_calls * [2]
    assert ray.get(results3) == num_calls * [3]
    assert ray.get(results4) == num_calls * [4]
    assert ray.get(results5) == num_calls * [5]

    @ray.remote
    def g():
        return 1

    @ray.remote  # noqa: F811
    def g():
        return 2

    @ray.remote  # noqa: F811
    def g():
        return 3

    @ray.remote  # noqa: F811
    def g():
        return 4

    @ray.remote  # noqa: F811
    def g():
        return 5

    result_values = ray.get([g.remote() for _ in range(num_calls)])
    assert result_values == num_calls * [5]


def test_illegal_api_calls(shutdown_only):
    ray.init(num_cpus=1)

    # Verify that we cannot call put on an ObjectID.
    x = ray.put(1)
    with pytest.raises(Exception):
        ray.put(x)
    # Verify that we cannot call get on a regular value.
    with pytest.raises(Exception):
        ray.get(3)


def test_multithreading(shutdown_only):
    # This test requires at least 2 CPUs to finish since the worker does not
    # relase resources when joining the threads.
    ray.init(num_cpus=2)

    def run_test_in_multi_threads(test_case, num_threads=20, num_repeats=50):
        """"""A helper function that runs test cases in multiple threads.""""""

        def wrapper():
            for _ in range(num_repeats):
                test_case()
                time.sleep(random.randint(0, 10) / 1000.0)
            return ""ok""

        executor = ThreadPoolExecutor(max_workers=num_threads)
        futures = [executor.submit(wrapper) for _ in range(num_threads)]
        for future in futures:
            assert future.result() == ""ok""

    @ray.remote
    def echo(value, delay_ms=0):
        if delay_ms > 0:
            time.sleep(delay_ms / 1000.0)
        return value

    @ray.remote
    class Echo(object):
        def echo(self, value):
            return value

    def test_api_in_multi_threads():
        """"""Test using Ray api in multiple threads.""""""

        # Test calling remote functions in multiple threads.
        def test_remote_call():
            value = random.randint(0, 1000000)
            result = ray.get(echo.remote(value))
            assert value == result

        run_test_in_multi_threads(test_remote_call)

        # Test multiple threads calling one actor.
        actor = Echo.remote()

        def test_call_actor():
            value = random.randint(0, 1000000)
            result = ray.get(actor.echo.remote(value))
            assert value == result

        run_test_in_multi_threads(test_call_actor)

        # Test put and get.
        def test_put_and_get():
            value = random.randint(0, 1000000)
            result = ray.get(ray.put(value))
            assert value == result

        run_test_in_multi_threads(test_put_and_get)

        # Test multiple threads waiting for objects.
        num_wait_objects = 10
        objects = [
            echo.remote(i, delay_ms=10) for i in range(num_wait_objects)
        ]

        def test_wait():
            ready, _ = ray.wait(
                objects,
                num_returns=len(objects),
                timeout=1000.0,
            )
            assert len(ready) == num_wait_objects
            assert ray.get(ready) == list(range(num_wait_objects))

        run_test_in_multi_threads(test_wait, num_repeats=1)

    # Run tests in a driver.
    test_api_in_multi_threads()

    # Run tests in a worker.
    @ray.remote
    def run_tests_in_worker():
        test_api_in_multi_threads()
        return ""ok""

    assert ray.get(run_tests_in_worker.remote()) == ""ok""

    # Test actor that runs background threads.
    @ray.remote
    class MultithreadedActor(object):
        def __init__(self):
            self.lock = threading.Lock()
            self.thread_results = []

        def background_thread(self, wait_objects):
            try:
                # Test wait
                ready, _ = ray.wait(
                    wait_objects,
                    num_returns=len(wait_objects),
                    timeout=1000.0,
                )
                assert len(ready) == len(wait_objects)
                for _ in range(50):
                    num = 20
                    # Test remote call
                    results = [echo.remote(i) for i in range(num)]
                    assert ray.get(results) == list(range(num))
                    # Test put and get
                    objects = [ray.put(i) for i in range(num)]
                    assert ray.get(objects) == list(range(num))
                    time.sleep(random.randint(0, 10) / 1000.0)
            except Exception as e:
                with self.lock:
                    self.thread_results.append(e)
            else:
                with self.lock:
                    self.thread_results.append(""ok"")

        def spawn(self):
            wait_objects = [echo.remote(i, delay_ms=10) for i in range(20)]
            self.threads = [
                threading.Thread(
                    target=self.background_thread, args=(wait_objects, ))
                for _ in range(20)
            ]
            [thread.start() for thread in self.threads]

        def join(self):
            [thread.join() for thread in self.threads]
            assert self.thread_results == [""ok""] * len(self.threads)
            return ""ok""

    actor = MultithreadedActor.remote()
    actor.spawn.remote()
    ray.get(actor.join.remote()) == ""ok""


def test_free_objects_multi_node(ray_start_cluster):
    # This test will do following:
    # 1. Create 3 raylets that each hold an actor.
    # 2. Each actor creates an object which is the deletion target.
    # 3. Invoke 64 methods on each actor to flush plasma client.
    # 4. After flushing, the plasma client releases the targets.
    # 5. Check that the deletion targets have been deleted.
    # Caution: if remote functions are used instead of actor methods,
    # one raylet may create more than one worker to execute the
    # tasks, so the flushing operations may be executed in different
    # workers and the plasma client holding the deletion target
    # may not be flushed.
    cluster = ray_start_cluster
    config = json.dumps({""object_manager_repeated_push_delay_ms"": 1000})
    for i in range(3):
        cluster.add_node(
            num_cpus=1,
            resources={""Custom{}"".format(i): 1},
            _internal_config=config)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote(resources={""Custom0"": 1})
    class ActorOnNode0(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""Custom1"": 1})
    class ActorOnNode1(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""Custom2"": 1})
    class ActorOnNode2(object):
        def get(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    def create(actors):
        a = actors[0].get.remote()
        b = actors[1].get.remote()
        c = actors[2].get.remote()
        (l1, l2) = ray.wait([a, b, c], num_returns=3)
        assert len(l1) == 3
        assert len(l2) == 0
        return (a, b, c)

    def flush(actors):
        # Flush the Release History.
        # Current Plasma Client Cache will maintain 64-item list.
        # If the number changed, this will fail.
        logger.info(""Start Flush!"")
        for i in range(64):
            ray.get([actor.get.remote() for actor in actors])
        logger.info(""Flush finished!"")

    def run_one_test(actors, local_only):
        (a, b, c) = create(actors)
        # The three objects should be generated on different object stores.
        assert ray.get(a) != ray.get(b)
        assert ray.get(a) != ray.get(c)
        assert ray.get(c) != ray.get(b)
        ray.internal.free([a, b, c], local_only=local_only)
        flush(actors)
        return (a, b, c)

    actors = [
        ActorOnNode0.remote(),
        ActorOnNode1.remote(),
        ActorOnNode2.remote()
    ]
    # Case 1: run this local_only=False. All 3 objects will be deleted.
    (a, b, c) = run_one_test(actors, False)
    (l1, l2) = ray.wait([a, b, c], timeout=0.01, num_returns=1)
    # All the objects are deleted.
    assert len(l1) == 0
    assert len(l2) == 3
    # Case 2: run this local_only=True. Only 1 object will be deleted.
    (a, b, c) = run_one_test(actors, True)
    (l1, l2) = ray.wait([a, b, c], timeout=0.01, num_returns=3)
    # One object is deleted and 2 objects are not.
    assert len(l1) == 2
    assert len(l2) == 1
    # The deleted object will have the same store with the driver.
    local_return = ray.worker.global_worker.plasma_client.store_socket_name
    for object_id in l1:
        assert ray.get(object_id) != local_return


def test_local_mode(shutdown_only):
    @ray.remote
    def local_mode_f():
        return np.array([0, 0])

    @ray.remote
    def local_mode_g(x):
        x[0] = 1
        return x

    ray.init(local_mode=True)

    @ray.remote
    def f():
        return np.ones([3, 4, 5])

    xref = f.remote()
    # Remote functions should return by value.
    assert_equal(xref, np.ones([3, 4, 5]))
    # Check that ray.get is the identity.
    assert_equal(xref, ray.get(xref))
    y = np.random.normal(size=[11, 12])
    # Check that ray.put is the identity.
    assert_equal(y, ray.put(y))

    # Make sure objects are immutable, this example is why we need to copy
    # arguments before passing them into remote functions in python mode
    aref = local_mode_f.remote()
    assert_equal(aref, np.array([0, 0]))
    bref = local_mode_g.remote(aref)
    # Make sure local_mode_g does not mutate aref.
    assert_equal(aref, np.array([0, 0]))
    assert_equal(bref, np.array([1, 0]))

    # wait should return the first num_returns values passed in as the
    # first list and the remaining values as the second list
    num_returns = 5
    object_ids = [ray.put(i) for i in range(20)]
    ready, remaining = ray.wait(
        object_ids, num_returns=num_returns, timeout=None)
    assert_equal(ready, object_ids[:num_returns])
    assert_equal(remaining, object_ids[num_returns:])

    # Test actors in LOCAL_MODE.

    @ray.remote
    class LocalModeTestClass(object):
        def __init__(self, array):
            self.array = array

        def set_array(self, array):
            self.array = array

        def get_array(self):
            return self.array

        def modify_and_set_array(self, array):
            array[0] = -1
            self.array = array

    test_actor = LocalModeTestClass.remote(np.arange(10))
    # Remote actor functions should return by value
    assert_equal(test_actor.get_array.remote(), np.arange(10))

    test_array = np.arange(10)
    # Remote actor functions should not mutate arguments
    test_actor.modify_and_set_array.remote(test_array)
    assert_equal(test_array, np.arange(10))
    # Remote actor functions should keep state
    test_array[0] = -1
    assert_equal(test_array, test_actor.get_array.remote())

    # Check that actor handles work in Python mode.

    @ray.remote
    def use_actor_handle(handle):
        array = np.ones(10)
        handle.set_array.remote(array)
        assert np.alltrue(array == ray.get(handle.get_array.remote()))

    ray.get(use_actor_handle.remote(test_actor))


def test_resource_constraints(shutdown_only):
    num_workers = 20
    ray.init(num_cpus=10, num_gpus=2)

    @ray.remote(num_cpus=0)
    def get_worker_id():
        time.sleep(0.1)
        return os.getpid()

    # Attempt to wait for all of the workers to start up.
    while True:
        if len(
                set(
                    ray.get([
                        get_worker_id.remote() for _ in range(num_workers)
                    ]))) == num_workers:
            break

    time_buffer = 0.3

    # At most 10 copies of this can run at once.
    @ray.remote(num_cpus=1)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(10)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(11)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    @ray.remote(num_cpus=3)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(3)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(4)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    @ray.remote(num_gpus=1)
    def f(n):
        time.sleep(n)

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(2)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(3)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([f.remote(0.5) for _ in range(4)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1


def test_multi_resource_constraints(shutdown_only):
    num_workers = 20
    ray.init(num_cpus=10, num_gpus=10)

    @ray.remote(num_cpus=0)
    def get_worker_id():
        time.sleep(0.1)
        return os.getpid()

    # Attempt to wait for all of the workers to start up.
    while True:
        if len(
                set(
                    ray.get([
                        get_worker_id.remote() for _ in range(num_workers)
                    ]))) == num_workers:
            break

    @ray.remote(num_cpus=1, num_gpus=9)
    def f(n):
        time.sleep(n)

    @ray.remote(num_cpus=9, num_gpus=1)
    def g(n):
        time.sleep(n)

    time_buffer = 0.3

    start_time = time.time()
    ray.get([f.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 0.5 + time_buffer
    assert duration > 0.5

    start_time = time.time()
    ray.get([f.remote(0.5), f.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([g.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1

    start_time = time.time()
    ray.get([f.remote(0.5), f.remote(0.5), g.remote(0.5), g.remote(0.5)])
    duration = time.time() - start_time
    assert duration < 1 + time_buffer
    assert duration > 1


def test_gpu_ids(shutdown_only):
    num_gpus = 10
    ray.init(num_cpus=10, num_gpus=num_gpus)

    @ray.remote(num_gpus=0)
    def f0():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 0
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=1)
    def f1():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 1
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=2)
    def f2():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 2
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=3)
    def f3():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 3
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=4)
    def f4():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 4
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    @ray.remote(num_gpus=5)
    def f5():
        time.sleep(0.1)
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 5
        assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
            [str(i) for i in gpu_ids]))
        for gpu_id in gpu_ids:
            assert gpu_id in range(num_gpus)
        return gpu_ids

    # Wait for all workers to start up.
    @ray.remote
    def f():
        time.sleep(0.1)
        return os.getpid()

    start_time = time.time()
    while True:
        if len(set(ray.get([f.remote() for _ in range(10)]))) == 10:
            break
        if time.time() > start_time + 10:
            raise Exception(""Timed out while waiting for workers to start ""
                            ""up."")

    list_of_ids = ray.get([f0.remote() for _ in range(10)])
    assert list_of_ids == 10 * [[]]

    list_of_ids = ray.get([f1.remote() for _ in range(10)])
    set_of_ids = {tuple(gpu_ids) for gpu_ids in list_of_ids}
    assert set_of_ids == {(i, ) for i in range(10)}

    list_of_ids = ray.get([f2.remote(), f4.remote(), f4.remote()])
    all_ids = [gpu_id for gpu_ids in list_of_ids for gpu_id in gpu_ids]
    assert set(all_ids) == set(range(10))

    remaining = [f5.remote() for _ in range(20)]
    for _ in range(10):
        t1 = time.time()
        ready, remaining = ray.wait(remaining, num_returns=2)
        t2 = time.time()
        # There are only 10 GPUs, and each task uses 2 GPUs, so there
        # should only be 2 tasks scheduled at a given time, so if we wait
        # for 2 tasks to finish, then it should take at least 0.1 seconds
        # for each pair of tasks to finish.
        assert t2 - t1 > 0.09
        list_of_ids = ray.get(ready)
        all_ids = [gpu_id for gpu_ids in list_of_ids for gpu_id in gpu_ids]
        # Commenting out the below assert because it seems to fail a lot.
        # assert set(all_ids) == set(range(10))

    # Test that actors have CUDA_VISIBLE_DEVICES set properly.

    @ray.remote
    class Actor0(object):
        def __init__(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 0
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            # Set self.x to make sure that we got here.
            self.x = 1

        def test(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 0
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            return self.x

    @ray.remote(num_gpus=1)
    class Actor1(object):
        def __init__(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            # Set self.x to make sure that we got here.
            self.x = 1

        def test(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            assert (os.environ[""CUDA_VISIBLE_DEVICES""] == "","".join(
                [str(i) for i in gpu_ids]))
            return self.x

    a0 = Actor0.remote()
    ray.get(a0.test.remote())

    a1 = Actor1.remote()
    ray.get(a1.test.remote())


def test_zero_cpus(shutdown_only):
    ray.init(num_cpus=0)

    @ray.remote(num_cpus=0)
    def f():
        return 1

    # The task should be able to execute.
    ray.get(f.remote())


def test_zero_cpus_actor(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=0)
    cluster.add_node(num_cpus=2)
    ray.init(redis_address=cluster.redis_address)

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote
    class Foo(object):
        def method(self):
            return ray.worker.global_worker.plasma_client.store_socket_name

    # Make sure tasks and actors run on the remote local scheduler.
    a = Foo.remote()
    assert ray.get(a.method.remote()) != local_plasma


def test_fractional_resources(shutdown_only):
    ray.init(num_cpus=6, num_gpus=3, resources={""Custom"": 1})

    @ray.remote(num_gpus=0.5)
    class Foo1(object):
        def method(self):
            gpu_ids = ray.get_gpu_ids()
            assert len(gpu_ids) == 1
            return gpu_ids[0]

    foos = [Foo1.remote() for _ in range(6)]
    gpu_ids = ray.get([f.method.remote() for f in foos])
    for i in range(3):
        assert gpu_ids.count(i) == 2
    del foos

    @ray.remote
    class Foo2(object):
        def method(self):
            pass

    # Create an actor that requires 0.7 of the custom resource.
    f1 = Foo2._remote([], {}, resources={""Custom"": 0.7})
    ray.get(f1.method.remote())
    # Make sure that we cannot create an actor that requires 0.7 of the
    # custom resource. TODO(rkn): Re-enable this once ray.wait is
    # implemented.
    f2 = Foo2._remote([], {}, resources={""Custom"": 0.7})
    ready, _ = ray.wait([f2.method.remote()], timeout=0.5)
    assert len(ready) == 0
    # Make sure we can start an actor that requries only 0.3 of the custom
    # resource.
    f3 = Foo2._remote([], {}, resources={""Custom"": 0.3})
    ray.get(f3.method.remote())

    del f1, f3

    # Make sure that we get exceptions if we submit tasks that require a
    # fractional number of resources greater than 1.

    @ray.remote(num_cpus=1.5)
    def test():
        pass

    with pytest.raises(ValueError):
        test.remote()

    with pytest.raises(ValueError):
        Foo2._remote([], {}, resources={""Custom"": 1.5})


def test_multiple_local_schedulers(ray_start_cluster):
    # This test will define a bunch of tasks that can only be assigned to
    # specific local schedulers, and we will check that they are assigned
    # to the correct local schedulers.
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=11, num_gpus=0)
    cluster.add_node(num_cpus=5, num_gpus=5)
    cluster.add_node(num_cpus=10, num_gpus=1)
    ray.init(redis_address=cluster.redis_address)
    cluster.wait_for_nodes()

    # Define a bunch of remote functions that all return the socket name of
    # the plasma store. Since there is a one-to-one correspondence between
    # plasma stores and local schedulers (at least right now), this can be
    # used to identify which local scheduler the task was assigned to.

    # This must be run on the zeroth local scheduler.
    @ray.remote(num_cpus=11)
    def run_on_0():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the first local scheduler.
    @ray.remote(num_gpus=2)
    def run_on_1():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the second local scheduler.
    @ray.remote(num_cpus=6, num_gpus=1)
    def run_on_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This can be run anywhere.
    @ray.remote(num_cpus=0, num_gpus=0)
    def run_on_0_1_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the first or second local scheduler.
    @ray.remote(num_gpus=1)
    def run_on_1_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This must be run on the zeroth or second local scheduler.
    @ray.remote(num_cpus=8)
    def run_on_0_2():
        return ray.worker.global_worker.plasma_client.store_socket_name

    def run_lots_of_tasks():
        names = []
        results = []
        for i in range(100):
            index = np.random.randint(6)
            if index == 0:
                names.append(""run_on_0"")
                results.append(run_on_0.remote())
            elif index == 1:
                names.append(""run_on_1"")
                results.append(run_on_1.remote())
            elif index == 2:
                names.append(""run_on_2"")
                results.append(run_on_2.remote())
            elif index == 3:
                names.append(""run_on_0_1_2"")
                results.append(run_on_0_1_2.remote())
            elif index == 4:
                names.append(""run_on_1_2"")
                results.append(run_on_1_2.remote())
            elif index == 5:
                names.append(""run_on_0_2"")
                results.append(run_on_0_2.remote())
        return names, results

    client_table = ray.global_state.client_table()
    store_names = []
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 0
    ]
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 5
    ]
    store_names += [
        client[""ObjectStoreSocketName""] for client in client_table
        if client[""Resources""][""GPU""] == 1
    ]
    assert len(store_names) == 3

    def validate_names_and_results(names, results):
        for name, result in zip(names, ray.get(results)):
            if name == ""run_on_0"":
                assert result in [store_names[0]]
            elif name == ""run_on_1"":
                assert result in [store_names[1]]
            elif name == ""run_on_2"":
                assert result in [store_names[2]]
            elif name == ""run_on_0_1_2"":
                assert (result in [
                    store_names[0], store_names[1], store_names[2]
                ])
            elif name == ""run_on_1_2"":
                assert result in [store_names[1], store_names[2]]
            elif name == ""run_on_0_2"":
                assert result in [store_names[0], store_names[2]]
            else:
                raise Exception(""This should be unreachable."")
            assert set(ray.get(results)) == set(store_names)

    names, results = run_lots_of_tasks()
    validate_names_and_results(names, results)

    # Make sure the same thing works when this is nested inside of a task.

    @ray.remote
    def run_nested1():
        names, results = run_lots_of_tasks()
        return names, results

    @ray.remote
    def run_nested2():
        names, results = ray.get(run_nested1.remote())
        return names, results

    names, results = ray.get(run_nested2.remote())
    validate_names_and_results(names, results)


def test_custom_resources(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(num_cpus=3, resources={""CustomResource"": 0})
    cluster.add_node(num_cpus=3, resources={""CustomResource"": 1})
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource"": 1})
    def g():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource"": 1})
    def h():
        ray.get([f.remote() for _ in range(5)])
        return ray.worker.global_worker.plasma_client.store_socket_name

    # The f tasks should be scheduled on both local schedulers.
    assert len(set(ray.get([f.remote() for _ in range(50)]))) == 2

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    # The g tasks should be scheduled only on the second local scheduler.
    local_scheduler_ids = set(ray.get([g.remote() for _ in range(50)]))
    assert len(local_scheduler_ids) == 1
    assert list(local_scheduler_ids)[0] != local_plasma

    # Make sure that resource bookkeeping works when a task that uses a
    # custom resources gets blocked.
    ray.get([h.remote() for _ in range(5)])


def test_two_custom_resources(ray_start_cluster):
    cluster = ray_start_cluster
    cluster.add_node(
        num_cpus=3, resources={
            ""CustomResource1"": 1,
            ""CustomResource2"": 2
        })
    cluster.add_node(
        num_cpus=3, resources={
            ""CustomResource1"": 3,
            ""CustomResource2"": 4
        })
    ray.init(redis_address=cluster.redis_address)

    @ray.remote(resources={""CustomResource1"": 1})
    def f():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource2"": 1})
    def g():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource1"": 1, ""CustomResource2"": 3})
    def h():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource1"": 4})
    def j():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    @ray.remote(resources={""CustomResource3"": 1})
    def k():
        time.sleep(0.001)
        return ray.worker.global_worker.plasma_client.store_socket_name

    # The f and g tasks should be scheduled on both local schedulers.
    assert len(set(ray.get([f.remote() for _ in range(50)]))) == 2
    assert len(set(ray.get([g.remote() for _ in range(50)]))) == 2

    local_plasma = ray.worker.global_worker.plasma_client.store_socket_name

    # The h tasks should be scheduled only on the second local scheduler.
    local_scheduler_ids = set(ray.get([h.remote() for _ in range(50)]))
    assert len(local_scheduler_ids) == 1
    assert list(local_scheduler_ids)[0] != local_plasma

    # Make sure that tasks with unsatisfied custom resource requirements do
    # not get scheduled.
    ready_ids, remaining_ids = ray.wait([j.remote(), k.remote()], timeout=0.5)
    assert ready_ids == []


def test_many_custom_resources(shutdown_only):
    num_custom_resources = 10000
    total_resources = {
        str(i): np.random.randint(1, 7)
        for i in range(num_custom_resources)
    }
    ray.init(num_cpus=5, resources=total_resources)

    def f():
        return 1

    remote_functions = []
    for _ in range(20):
        num_resources = np.random.randint(0, num_custom_resources + 1)
        permuted_resources = np.random.permutation(
            num_custom_resources)[:num_resources]
        random_resources = {
            str(i): total_resources[str(i)]
            for i in permuted_resources
        }
        remote_function = ray.remote(resources=random_resources)(f)
        remote_functions.append(remote_function)

    remote_functions.append(ray.remote(f))
    remote_functions.append(ray.remote(resources=total_resources)(f))

    results = []
    for remote_function in remote_functions:
        results.append(remote_function.remote())
        results.append(remote_function.remote())
        results.append(remote_function.remote())

    ray.get(results)


@pytest.fixture
def save_gpu_ids_shutdown_only():
    # Record the curent value of this environment variable so that we can
    # reset it after the test.
    original_gpu_ids = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    yield None

    # The code after the yield will run as teardown code.
    ray.shutdown()
    # Reset the environment variable.
    if original_gpu_ids is not None:
        os.environ[""CUDA_VISIBLE_DEVICES""] = original_gpu_ids
    else:
        del os.environ[""CUDA_VISIBLE_DEVICES""]


def test_specific_gpus(save_gpu_ids_shutdown_only):
    allowed_gpu_ids = [4, 5, 6]
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join(
        [str(i) for i in allowed_gpu_ids])
    ray.init(num_gpus=3)

    @ray.remote(num_gpus=1)
    def f():
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 1
        assert gpu_ids[0] in allowed_gpu_ids

    @ray.remote(num_gpus=2)
    def g():
        gpu_ids = ray.get_gpu_ids()
        assert len(gpu_ids) == 2
        assert gpu_ids[0] in allowed_gpu_ids
        assert gpu_ids[1] in allowed_gpu_ids

    ray.get([f.remote() for _ in range(100)])
    ray.get([g.remote() for _ in range(100)])


def test_blocking_tasks(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote
    def f(i, j):
        return (i, j)

    @ray.remote
    def g(i):
        # Each instance of g submits and blocks on the result of another
        # remote task.
        object_ids = [f.remote(i, j) for j in range(2)]
        return ray.get(object_ids)

    @ray.remote
    def h(i):
        # Each instance of g submits and blocks on the result of another
        # remote task using ray.wait.
        object_ids = [f.remote(i, j) for j in range(2)]
        return ray.wait(object_ids, num_returns=len(object_ids))

    ray.get([h.remote(i) for i in range(4)])

    @ray.remote
    def _sleep(i):
        time.sleep(0.01)
        return (i)

    @ray.remote
    def sleep():
        # Each instance of sleep submits and blocks on the result of
        # another remote task, which takes some time to execute.
        ray.get([_sleep.remote(i) for i in range(10)])

    ray.get(sleep.remote())


def test_max_call_tasks(shutdown_only):
    ray.init(num_cpus=1)

    @ray.remote(max_calls=1)
    def f():
        return os.getpid()

    pid = ray.get(f.remote())
    ray.test.test_utils.wait_for_pid_to_exit(pid)

    @ray.remote(max_calls=2)
    def f():
        return os.getpid()

    pid1 = ray.get(f.remote())
    pid2 = ray.get(f.remote())
    assert pid1 == pid2
    ray.test.test_utils.wait_for_pid_to_exit(pid1)


def attempt_to_load_balance(remote_function,
                            args,
                            total_tasks,
                            num_nodes,
                            minimum_count,
                            num_attempts=100):
    attempts = 0
    while attempts < num_attempts:
        locations = ray.get(
            [remote_function.remote(*args) for _ in range(total_tasks)])
        names = set(locations)
        counts = [locations.count(name) for name in names]
        logger.info(""Counts are {}."".format(counts))
        if (len(names) == num_nodes
                and all(count >= minimum_count for count in counts)):
            break
        attempts += 1
    assert attempts < num_attempts


def test_load_balancing(ray_start_cluster):
    # This test ensures that tasks are being assigned to all local
    # schedulers in a roughly equal manner.
    cluster = ray_start_cluster
    num_nodes = 3
    num_cpus = 7
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=num_cpus)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f():
        time.sleep(0.01)
        return ray.worker.global_worker.plasma_client.store_socket_name

    attempt_to_load_balance(f, [], 100, num_nodes, 10)
    attempt_to_load_balance(f, [], 1000, num_nodes, 100)


def test_load_balancing_with_dependencies(ray_start_cluster):
    # This test ensures that tasks are being assigned to all local
    # schedulers in a roughly equal manner even when the tasks have
    # dependencies.
    cluster = ray_start_cluster
    num_nodes = 3
    for _ in range(num_nodes):
        cluster.add_node(num_cpus=1)
    ray.init(redis_address=cluster.redis_address)

    @ray.remote
    def f(x):
        time.sleep(0.010)
        return ray.worker.global_worker.plasma_client.store_socket_name

    # This object will be local to one of the local schedulers. Make sure
    # this doesn't prevent tasks from being scheduled on other local
    # schedulers.
    x = ray.put(np.zeros(1000000))

    attempt_to_load_balance(f, [x], 100, num_nodes, 25)


def wait_for_num_tasks(num_tasks, timeout=10):
    start_time = time.time()
    while time.time() - start_time < timeout:
        if len(ray.global_state.task_table()) >= num_tasks:
            return
        time.sleep(0.1)
    raise Exception(""Timed out while waiting for global state."")


def wait_for_num_objects(num_objects, timeout=10):
    start_time = time.time()
    while time.time() - start_time < timeout:
        if len(ray.global_state.object_table()) >= num_objects:
            return
        time.sleep(0.1)
    raise Exception(""Timed out while waiting for global state."")


@pytest.mark.skipif(
    os.environ.get(""RAY_USE_NEW_GCS"") == ""on"",
    reason=""New GCS API doesn't have a Python API yet."")
def test_global_state_api(shutdown_only):
    with pytest.raises(Exception):
        ray.global_state.object_table()

    with pytest.raises(Exception):
        ray.global_state.task_table()

    with pytest.raises(Exception):
        ray.global_state.client_table()

    with pytest.raises(Exception):
        ray.global_state.function_table()

    ray.init(num_cpus=5, num_gpus=3, resources={""CustomResource"": 1})

    resources = {""CPU"": 5, ""GPU"": 3, ""CustomResource"": 1}
    assert ray.global_state.cluster_resources() == resources

    assert ray.global_state.object_table() == {}

    driver_id = ray.experimental.state.binary_to_hex(
        ray.worker.global_worker.worker_id)
    driver_task_id = ray.worker.global_worker.current_task_id.hex()

    # One task is put in the task table which corresponds to this driver.
    wait_for_num_tasks(1)
    task_table = ray.global_state.task_table()
    assert len(task_table) == 1
    assert driver_task_id == list(task_table.keys())[0]
    task_spec = task_table[driver_task_id][""TaskSpec""]
    nil_id_hex = ray.ObjectID.nil().hex()

    assert task_spec[""TaskID""] == driver_task_id
    assert task_spec[""ActorID""] == nil_id_hex
    assert task_spec[""Args""] == []
    assert task_spec[""DriverID""] == driver_id
    assert task_spec[""FunctionID""] == nil_id_hex
    assert task_spec[""ReturnObjectIDs""] == []

    client_table = ray.global_state.client_table()
    node_ip_address = ray.worker.global_worker.node_ip_address

    assert len(client_table) == 1
    assert client_table[0][""NodeManagerAddress""] == node_ip_address

    @ray.remote
    def f(*xs):
        return 1

    x_id = ray.put(1)
    result_id = f.remote(1, ""hi"", x_id)

    # Wait for one additional task to complete.
    wait_for_num_tasks(1 + 1)
    task_table = ray.global_state.task_table()
    assert len(task_table) == 1 + 1
    task_id_set = set(task_table.keys())
    task_id_set.remove(driver_task_id)
    task_id = list(task_id_set)[0]

    function_table = ray.global_state.function_table()
    task_spec = task_table[task_id][""TaskSpec""]
    assert task_spec[""ActorID""] == nil_id_hex
    assert task_spec[""Args""] == [1, ""hi"", x_id]
    assert task_spec[""DriverID""] == driver_id
    assert task_spec[""ReturnObjectIDs""] == [result_id]
    function_table_entry = function_table[task_spec[""FunctionID""]]
    assert function_table_entry[""Name""] == ""runtest.f""
    assert function_table_entry[""DriverID""] == driver_id
    assert function_table_entry[""Module""] == ""runtest""

    assert task_table[task_id] == ray.global_state.task_table(task_id)

    # Wait for two objects, one for the x_id and one for result_id.
    wait_for_num_objects(2)

    def wait_for_object_table():
        timeout = 10
        start_time = time.time()
        while time.time() - start_time < timeout:
            object_table = ray.global_state.object_table()
            tables_ready = (object_table[x_id][""ManagerIDs""] is not None and
                            object_table[result_id][""ManagerIDs""] is not None)
            if tables_ready:
                return
            time.sleep(0.1)
        raise Exception(""Timed out while waiting for object table to ""
                        ""update."")

    object_table = ray.global_state.object_table()
    assert len(object_table) == 2

    assert object_table[x_id][""IsEviction""][0] is False

    assert object_table[result_id][""IsEviction""][0] is False

    assert object_table[x_id] == ray.global_state.object_table(x_id)
    object_table_entry = ray.global_state.object_table(result_id)
    assert object_table[result_id] == object_table_entry


# TODO(rkn): Pytest actually has tools for capturing stdout and stderr, so we
# should use those, but they seem to conflict with Ray's use of faulthandler.
class CaptureOutputAndError(object):
    """"""Capture stdout and stderr of some span.

    This can be used as follows.

        captured = {}
        with CaptureOutputAndError(captured):
            # Do stuff.
        # Access captured[""out""] and captured[""err""].
    """"""

    def __init__(self, captured_output_and_error):
        if sys.version_info >= (3, 0):
            import io
            self.output_buffer = io.StringIO()
            self.error_buffer = io.StringIO()
        else:
            import cStringIO
            self.output_buffer = cStringIO.StringIO()
            self.error_buffer = cStringIO.StringIO()
        self.captured_output_and_error = captured_output_and_error

    def __enter__(self):
        sys.stdout.flush()
        sys.stderr.flush()
        self.old_stdout = sys.stdout
        self.old_stderr = sys.stderr
        sys.stdout = self.output_buffer
        sys.stderr = self.error_buffer

    def __exit__(self, exc_type, exc_value, traceback):
        sys.stdout.flush()
        sys.stderr.flush()
        sys.stdout = self.old_stdout
        sys.stderr = self.old_stderr
        self.captured_output_and_error[""out""] = self.output_buffer.getvalue()
        self.captured_output_and_error[""err""] = self.error_buffer.getvalue()


def test_logging_to_driver(shutdown_only):
    ray.init(num_cpus=1, log_to_driver=True)

    @ray.remote
    def f():
        for i in range(100):
            print(i)
            print(100 + i, file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

    captured = {}
    with CaptureOutputAndError(captured):
        ray.get(f.remote())
        time.sleep(1)

    output_lines = captured[""out""]
    assert len(output_lines) == 0
    error_lines = captured[""err""]
    for i in range(200):
        assert str(i) in error_lines


def test_not_logging_to_driver(shutdown_only):
    ray.init(num_cpus=1, log_to_driver=False)

    @ray.remote
    def f():
        for i in range(100):
            print(i)
            print(100 + i, file=sys.stderr)
            sys.stdout.flush()
            sys.stderr.flush()

    captured = {}
    with CaptureOutputAndError(captured):
        ray.get(f.remote())
        time.sleep(1)

    output_lines = captured[""out""]
    assert len(output_lines) == 0
    error_lines = captured[""err""]
    assert len(error_lines) == 0


@pytest.mark.skipif(
    os.environ.get(""RAY_USE_NEW_GCS"") == ""on"",
    reason=""New GCS API doesn't have a Python API yet."")
def test_workers(shutdown_only):
    num_workers = 3
    ray.init(redirect_worker_output=True, num_cpus=num_workers)

    @ray.remote
    def f():
        return id(ray.worker.global_worker), os.getpid()

    # Wait until all of the workers have started.
    worker_ids = set()
    while len(worker_ids) != num_workers:
        worker_ids = set(ray.get([f.remote() for _ in range(10)]))

    worker_info = ray.global_state.workers()
    assert len(worker_info) >= num_workers
    for worker_id, info in worker_info.items():
        assert ""node_ip_address"" in info
        assert ""plasma_store_socket"" in info
        assert ""stderr_file"" in info
        assert ""stdout_file"" in info


def test_specific_driver_id():
    dummy_driver_id = ray.DriverID(b""00112233445566778899"")
    ray.init(driver_id=dummy_driver_id)

    @ray.remote
    def f():
        return ray.worker.global_worker.task_driver_id.binary()

    assert_equal(dummy_driver_id.binary(), ray.worker.global_worker.worker_id)

    task_driver_id = ray.get(f.remote())
    assert_equal(dummy_driver_id.binary(), task_driver_id)

    ray.shutdown()


def test_object_id_properties():
    id_bytes = b""00112233445566778899""
    object_id = ray.ObjectID(id_bytes)
    assert object_id.binary() == id_bytes
    object_id = ray.ObjectID.nil()
    assert object_id.is_nil()
    with pytest.raises(ValueError, match=r"".*needs to have length 20.*""):
        ray.ObjectID(id_bytes + b""1234"")
    with pytest.raises(ValueError, match=r"".*needs to have length 20.*""):
        ray.ObjectID(b""0123456789"")
    object_id = ray.ObjectID(_random_string())
    assert not object_id.is_nil()
    assert object_id.binary() != id_bytes
    id_dumps = pickle.dumps(object_id)
    id_from_dumps = pickle.loads(id_dumps)
    assert id_from_dumps == object_id


@pytest.fixture
def shutdown_only_with_initialization_check():
    yield None
    # The code after the yield will run as teardown code.
    ray.shutdown()
    assert not ray.is_initialized()


def test_initialized(shutdown_only_with_initialization_check):
    assert not ray.is_initialized()
    ray.init(num_cpus=0)
    assert ray.is_initialized()


def test_initialized_local_mode(shutdown_only_with_initialization_check):
    assert not ray.is_initialized()
    ray.init(num_cpus=0, local_mode=True)
    assert ray.is_initialized()


def test_wait_reconstruction(shutdown_only):
    ray.init(num_cpus=1, object_store_memory=10**8)

    @ray.remote
    def f():
        return np.zeros(6 * 10**7, dtype=np.uint8)

    x_id = f.remote()
    ray.wait([x_id])
    ray.wait([f.remote()])
    assert not ray.worker.global_worker.plasma_client.contains(
        ray.pyarrow.plasma.ObjectID(x_id.binary()))
    ready_ids, _ = ray.wait([x_id])
    assert len(ready_ids) == 1


def test_inline_objects(shutdown_only):
    config = json.dumps({""initial_reconstruction_timeout_milliseconds"": 200})
    ray.init(num_cpus=1, object_store_memory=10**7, _internal_config=config)

    @ray.remote
    class Actor(object):
        def create_inline_object(self):
            return ""inline""

        def create_non_inline_object(self):
            return 10000 * [1]

        def get(self):
            return

    a = Actor.remote()
    # Count the number of objects that were successfully inlined.
    inlined = 0
    for _ in range(100):
        inline_object = a.create_inline_object.remote()
        ray.get(inline_object)
        plasma_id = ray.pyarrow.plasma.ObjectID(inline_object.binary())
        ray.worker.global_worker.plasma_client.delete([plasma_id])
        # Make sure we can still get an inlined object created by an actor even
        # after it has been evicted.
        try:
            value = ray.get(inline_object)
            assert value == ""inline""
            inlined += 1
        except ray.worker.RayTaskError:
            pass
    # Make sure some objects were inlined. Some of them may not get inlined
    # because we evict the object soon after creating it.
    assert inlined > 0

    # Non-inlined objects are not able to be recreated after eviction.
    for _ in range(10):
        non_inline_object = a.create_non_inline_object.remote()
        ray.get(non_inline_object)
        plasma_id = ray.pyarrow.plasma.ObjectID(non_inline_object.binary())
        # This while loop is necessary because sometimes the object is still
        # there immediately after plasma_client.delete.
        while ray.worker.global_worker.plasma_client.contains(plasma_id):
            ray.worker.global_worker.plasma_client.delete([plasma_id])
        # Objects created by an actor that were evicted and larger than the
        # maximum inline object size cannot be retrieved or reconstructed.
        with pytest.raises(ray.worker.RayTaskError):
            ray.get(non_inline_object) == 10000 * [1]


def test_ray_setproctitle(shutdown_only):
    ray.init(num_cpus=2)

    @ray.remote
    class UniqueName(object):
        def __init__(self):
            assert setproctitle.getproctitle() == ""ray_UniqueName:__init__()""

        def f(self):
            assert setproctitle.getproctitle() == ""ray_UniqueName:f()""

    @ray.remote
    def unique_1():
        assert setproctitle.getproctitle() == ""ray_worker:runtest.unique_1()""

    actor = UniqueName.remote()
    ray.get(actor.f.remote())
    ray.get(unique_1.remote())


def test_duplicate_error_messages(shutdown_only):
    ray.init(num_cpus=0)

    driver_id = ray.DriverID.nil()
    error_data = ray.gcs_utils.construct_error_message(driver_id, ""test"",
                                                       ""message"", 0)

    # Push the same message to the GCS twice (they are the same because we
    # do not include a timestamp).

    r = ray.worker.global_worker.redis_client

    r.execute_command(""RAY.TABLE_APPEND"", ray.gcs_utils.TablePrefix.ERROR_INFO,
                      ray.gcs_utils.TablePubsub.ERROR_INFO, driver_id.binary(),
                      error_data)

    # Before https://github.com/ray-project/ray/pull/3316 this would
    # give an error
    r.execute_command(""RAY.TABLE_APPEND"", ray.gcs_utils.TablePrefix.ERROR_INFO,
                      ray.gcs_utils.TablePubsub.ERROR_INFO, driver_id.binary(),
                      error_data)


@pytest.mark.skipif(
    os.getenv(""TRAVIS"") is None,
    reason=""This test should only be run on Travis."")
def test_ray_stack(shutdown_only):
    ray.init(num_cpus=2)

    def unique_name_1():
        time.sleep(1000)

    @ray.remote
    def unique_name_2():
        time.sleep(1000)

    @ray.remote
    def unique_name_3():
        unique_name_1()

    unique_name_2.remote()
    unique_name_3.remote()

    success = False
    start_time = time.time()
    while time.time() - start_time < 30:
        # Attempt to parse the ""ray stack"" call.
        output = ray.utils.decode(subprocess.check_output([""ray"", ""stack""]))
        if (""unique_name_1"" in output and ""unique_name_2"" in output
                and ""unique_name_3"" in output):
            success = True
            break

    if not success:
        raise Exception(""Failed to find necessary information with ""
                        ""'ray stack'"")


def test_pandas_parquet_serialization():
    # Only test this if pandas is installed
    pytest.importorskip(""pandas"")

    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq

    tempdir = tempfile.mkdtemp()
    filename = os.path.join(tempdir, ""parquet-test"")
    pd.DataFrame({""col1"": [0, 1], ""col2"": [0, 1]}).to_parquet(filename)
    with open(os.path.join(tempdir, ""parquet-compression""), ""wb"") as f:
        table = pa.Table.from_arrays([pa.array([1, 2, 3])], [""hello""])
        pq.write_table(table, f, compression=""lz4"")
    # Clean up
    shutil.rmtree(tempdir)


def test_socket_dir_not_existing(shutdown_only):
    random_name = ray.ObjectID(_random_string()).hex()
    temp_raylet_socket_dir = ""/tmp/ray/tests/{}"".format(random_name)
    temp_raylet_socket_name = os.path.join(temp_raylet_socket_dir,
                                           ""raylet_socket"")
    ray.init(num_cpus=1, raylet_socket_name=temp_raylet_socket_name)
/n/n/ntest/tempfile_test.py/n/nimport os
import shutil
import time
import pytest
import ray
import ray.tempfile_services as tempfile_services


def test_conn_cluster():
    # plasma_store_socket_name
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"",
            plasma_store_socket_name=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""plasma_store_socket_name must not be provided."")

    # raylet_socket_name
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"",
            raylet_socket_name=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""raylet_socket_name must not be provided."")

    # temp_dir
    with pytest.raises(Exception) as exc_info:
        ray.init(
            redis_address=""127.0.0.1:6379"", temp_dir=""/tmp/this_should_fail"")
    assert exc_info.value.args[0] == (
        ""When connecting to an existing cluster, ""
        ""temp_dir must not be provided."")


def test_tempdir():
    ray.init(temp_dir=""/tmp/i_am_a_temp_dir"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_dir""), ""Specified temp dir not found.""
    ray.shutdown()
    shutil.rmtree(""/tmp/i_am_a_temp_dir"", ignore_errors=True)


def test_raylet_socket_name():
    ray.init(raylet_socket_name=""/tmp/i_am_a_temp_socket"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_socket""), ""Specified socket path not found.""
    ray.shutdown()
    try:
        os.remove(""/tmp/i_am_a_temp_socket"")
    except Exception:
        pass


def test_temp_plasma_store_socket():
    ray.init(plasma_store_socket_name=""/tmp/i_am_a_temp_socket"")
    assert os.path.exists(
        ""/tmp/i_am_a_temp_socket""), ""Specified socket path not found.""
    ray.shutdown()
    try:
        os.remove(""/tmp/i_am_a_temp_socket"")
    except Exception:
        pass


def test_raylet_tempfiles():
    ray.init(redirect_worker_output=False)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files == {
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    }  # with raylet logs
    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()

    ray.init(redirect_worker_output=True, num_cpus=0)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files == {
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    }  # with raylet logs
    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()

    ray.init(redirect_worker_output=True, num_cpus=2)
    top_levels = set(os.listdir(tempfile_services.get_temp_root()))
    assert top_levels == {""ray_ui.ipynb"", ""sockets"", ""logs""}
    time.sleep(3)  # wait workers to start
    log_files = set(os.listdir(tempfile_services.get_logs_dir_path()))
    assert log_files.issuperset({
        ""log_monitor.out"", ""log_monitor.err"", ""plasma_store.out"",
        ""plasma_store.err"", ""webui.out"", ""webui.err"", ""monitor.out"",
        ""monitor.err"", ""raylet_monitor.out"", ""raylet_monitor.err"",
        ""redis-shard_0.out"", ""redis-shard_0.err"", ""redis.out"", ""redis.err"",
        ""raylet.out"", ""raylet.err""
    })  # with raylet logs

    # Check numbers of worker log file.
    assert sum(
        1 for filename in log_files if filename.startswith(""worker"")) == 4

    socket_files = set(os.listdir(tempfile_services.get_sockets_dir_path()))
    assert socket_files == {""plasma_store"", ""raylet""}
    ray.shutdown()
/n/n/n",0
61,61,ef527f84abf0cee7ac6ad832828ff92311440ee4,"/python/ray/import_thread.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import threading
import traceback

import redis

import ray
from ray import ray_constants
from ray import cloudpickle as pickle
from ray import profiling
from ray import utils


class ImportThread(object):
    """"""A thread used to import exports from the driver or other workers.

    Note:
    The driver also has an import thread, which is used only to
    import custom class definitions from calls to register_custom_serializer
    that happen under the hood on workers.

    Attributes:
        worker: the worker object in this process.
        mode: worker mode
        redis_client: the redis client used to query exports.
    """"""

    def __init__(self, worker, mode):
        self.worker = worker
        self.mode = mode
        self.redis_client = worker.redis_client

    def start(self):
        """"""Start the import thread.""""""
        t = threading.Thread(target=self._run, name=""ray_import_thread"")
        # Making the thread a daemon causes it to exit
        # when the main thread exits.
        t.daemon = True
        t.start()

    def _run(self):
        import_pubsub_client = self.redis_client.pubsub()
        # Exports that are published after the call to
        # import_pubsub_client.subscribe and before the call to
        # import_pubsub_client.listen will still be processed in the loop.
        import_pubsub_client.subscribe(""__keyspace@0__:Exports"")
        # Keep track of the number of imports that we've imported.
        num_imported = 0

        # Get the exports that occurred before the call to subscribe.
        with self.worker.lock:
            export_keys = self.redis_client.lrange(""Exports"", 0, -1)
            for key in export_keys:
                num_imported += 1
                self._process_key(key)
        try:
            for msg in import_pubsub_client.listen():
                with self.worker.lock:
                    if msg[""type""] == ""subscribe"":
                        continue
                    assert msg[""data""] == b""rpush""
                    num_imports = self.redis_client.llen(""Exports"")
                    assert num_imports >= num_imported
                    for i in range(num_imported, num_imports):
                        num_imported += 1
                        key = self.redis_client.lindex(""Exports"", i)
                        self._process_key(key)
        except redis.ConnectionError:
            # When Redis terminates the listen call will throw a
            # ConnectionError, which we catch here.
            pass

    def _process_key(self, key):
        """"""Process the given export key from redis.""""""
        # Handle the driver case first.
        if self.mode != ray.WORKER_MODE:
            if key.startswith(b""FunctionsToRun""):
                with profiling.profile(
                        ""fetch_and_run_function"", worker=self.worker):
                    self.fetch_and_execute_function_to_run(key)
            # Return because FunctionsToRun are the only things that
            # the driver should import.
            return

        if key.startswith(b""RemoteFunction""):
            with profiling.profile(
                    ""register_remote_function"", worker=self.worker):
                (self.worker.function_actor_manager.
                 fetch_and_register_remote_function(key))
        elif key.startswith(b""FunctionsToRun""):
            with profiling.profile(
                    ""fetch_and_run_function"", worker=self.worker):
                self.fetch_and_execute_function_to_run(key)
        elif key.startswith(b""ActorClass""):
            # Keep track of the fact that this actor class has been
            # exported so that we know it is safe to turn this worker
            # into an actor of that class.
            self.worker.function_actor_manager.imported_actor_classes.add(key)
        # TODO(rkn): We may need to bring back the case of
        # fetching actor classes here.
        else:
            raise Exception(""This code should be unreachable."")

    def fetch_and_execute_function_to_run(self, key):
        """"""Run on arbitrary function on the worker.""""""
        (driver_id, serialized_function,
         run_on_other_drivers) = self.redis_client.hmget(
             key, [""driver_id"", ""function"", ""run_on_other_drivers""])

        if (utils.decode(run_on_other_drivers) == ""False""
                and self.worker.mode == ray.SCRIPT_MODE
                and driver_id != self.worker.task_driver_id.binary()):
            return

        try:
            # Deserialize the function.
            function = pickle.loads(serialized_function)
            # Run the function.
            function({""worker"": self.worker})
        except Exception:
            # If an exception was thrown when the function was run, we record
            # the traceback and notify the scheduler of the failure.
            traceback_str = traceback.format_exc()
            # Log the error message.
            utils.push_error_to_driver(
                self.worker,
                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,
                traceback_str,
                driver_id=ray.DriverID(driver_id))
/n/n/n/python/ray/log_monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import redis
import time

import ray.ray_constants as ray_constants
from ray.services import get_ip_address
from ray.services import get_port
import ray.utils

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray provides a default configuration at
# entry/init points.
logger = logging.getLogger(__name__)


class LogMonitor(object):
    """"""A monitor process for monitoring Ray log files.

    Attributes:
        node_ip_address: The IP address of the node that the log monitor
            process is running on. This will be used to determine which log
            files to track.
        redis_client: A client used to communicate with the Redis server.
        log_files: A dictionary mapping the name of a log file to a list of
            strings representing its contents.
        log_file_handles: A dictionary mapping the name of a log file to a file
            handle for that file.
    """"""

    def __init__(self,
                 redis_ip_address,
                 redis_port,
                 node_ip_address,
                 redis_password=None):
        """"""Initialize the log monitor object.""""""
        self.node_ip_address = node_ip_address
        self.redis_client = redis.StrictRedis(
            host=redis_ip_address, port=redis_port, password=redis_password)
        self.log_files = {}
        self.log_file_handles = {}
        self.files_to_ignore = set()

    def update_log_filenames(self):
        """"""Get the most up-to-date list of log files to monitor from Redis.""""""
        num_current_log_files = len(self.log_files)
        new_log_filenames = self.redis_client.lrange(
            ""LOG_FILENAMES:{}"".format(self.node_ip_address),
            num_current_log_files, -1)
        for log_filename in new_log_filenames:
            logger.info(""Beginning to track file {}"".format(log_filename))
            assert log_filename not in self.log_files
            self.log_files[log_filename] = []

    def check_log_files_and_push_updates(self):
        """"""Get any changes to the log files and push updates to Redis.""""""
        for log_filename in self.log_files:
            if log_filename in self.log_file_handles:
                # Get any updates to the file.
                new_lines = []
                while True:
                    current_position = (
                        self.log_file_handles[log_filename].tell())
                    next_line = self.log_file_handles[log_filename].readline()
                    if next_line != """":
                        new_lines.append(next_line)
                    else:
                        self.log_file_handles[log_filename].seek(
                            current_position)
                        break

                # If there are any new lines, cache them and also push them to
                # Redis.
                if len(new_lines) > 0:
                    self.log_files[log_filename] += new_lines
                    redis_key = ""LOGFILE:{}:{}"".format(
                        self.node_ip_address, ray.utils.decode(log_filename))
                    self.redis_client.rpush(redis_key, *new_lines)

            # Pass if we already failed to open the log file.
            elif log_filename in self.files_to_ignore:
                pass

            # Try to open this file for the first time.
            else:
                try:
                    self.log_file_handles[log_filename] = open(
                        log_filename, ""r"")
                except IOError as e:
                    if e.errno == os.errno.EMFILE:
                        logger.warning(
                            ""Warning: Ignoring {} because there are too ""
                            ""many open files."".format(log_filename))
                    elif e.errno == os.errno.ENOENT:
                        logger.warning(""Warning: The file {} was not ""
                                       ""found."".format(log_filename))
                    else:
                        raise e

                    # Don't try to open this file any more.
                    self.files_to_ignore.add(log_filename)

    def run(self):
        """"""Run the log monitor.

        This will query Redis once every second to check if there are new log
        files to monitor. It will also store those log files in Redis.
        """"""
        while True:
            self.update_log_filenames()
            self.check_log_files_and_push_updates()
            time.sleep(1)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""log monitor to connect ""
                     ""to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""The address to use for Redis."")
    parser.add_argument(
        ""--node-ip-address"",
        required=True,
        type=str,
        help=""The IP address of the node this process is on."")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    ray.utils.setup_logger(args.logging_level, args.logging_format)

    redis_ip_address = get_ip_address(args.redis_address)
    redis_port = get_port(args.redis_address)

    log_monitor = LogMonitor(
        redis_ip_address,
        redis_port,
        args.node_ip_address,
        redis_password=args.redis_password)
    log_monitor.run()
/n/n/n/python/ray/monitor.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
import os
import time
import traceback

import redis

import ray
from ray.autoscaler.autoscaler import LoadMetrics, StandardAutoscaler
import ray.cloudpickle as pickle
import ray.gcs_utils
import ray.utils
import ray.ray_constants as ray_constants
from ray.services import get_ip_address, get_port
from ray.utils import (binary_to_hex, binary_to_object_id, hex_to_binary,
                       setup_logger)

logger = logging.getLogger(__name__)


class Monitor(object):
    """"""A monitor for Ray processes.

    The monitor is in charge of cleaning up the tables in the global state
    after processes have died. The monitor is currently not responsible for
    detecting component failures.

    Attributes:
        redis: A connection to the Redis server.
        subscribe_client: A pubsub client for the Redis server. This is used to
            receive notifications about failed components.
    """"""

    def __init__(self,
                 redis_address,
                 redis_port,
                 autoscaling_config,
                 redis_password=None):
        # Initialize the Redis clients.
        self.state = ray.experimental.state.GlobalState()
        self.state._initialize_global_state(
            redis_address, redis_port, redis_password=redis_password)
        self.redis = redis.StrictRedis(
            host=redis_address, port=redis_port, db=0, password=redis_password)
        # Setup subscriptions to the primary Redis server and the Redis shards.
        self.primary_subscribe_client = self.redis.pubsub(
            ignore_subscribe_messages=True)
        # Keep a mapping from local scheduler client ID to IP address to use
        # for updating the load metrics.
        self.local_scheduler_id_to_ip_map = {}
        self.load_metrics = LoadMetrics()
        if autoscaling_config:
            self.autoscaler = StandardAutoscaler(autoscaling_config,
                                                 self.load_metrics)
        else:
            self.autoscaler = None

        # Experimental feature: GCS flushing.
        self.issue_gcs_flushes = ""RAY_USE_NEW_GCS"" in os.environ
        self.gcs_flush_policy = None
        if self.issue_gcs_flushes:
            # Data is stored under the first data shard, so we issue flushes to
            # that redis server.
            addr_port = self.redis.lrange(""RedisShards"", 0, -1)
            if len(addr_port) > 1:
                logger.warning(
                    ""Monitor: ""
                    ""TODO: if launching > 1 redis shard, flushing needs to ""
                    ""touch shards in parallel."")
                self.issue_gcs_flushes = False
            else:
                addr_port = addr_port[0].split(b"":"")
                self.redis_shard = redis.StrictRedis(
                    host=addr_port[0],
                    port=addr_port[1],
                    password=redis_password)
                try:
                    self.redis_shard.execute_command(""HEAD.FLUSH 0"")
                except redis.exceptions.ResponseError as e:
                    logger.info(
                        ""Monitor: ""
                        ""Turning off flushing due to exception: {}"".format(
                            str(e)))
                    self.issue_gcs_flushes = False

    def subscribe(self, channel):
        """"""Subscribe to the given channel on the primary Redis shard.

        Args:
            channel (str): The channel to subscribe to.

        Raises:
            Exception: An exception is raised if the subscription fails.
        """"""
        self.primary_subscribe_client.subscribe(channel)

    def xray_heartbeat_batch_handler(self, unused_channel, data):
        """"""Handle an xray heartbeat batch message from Redis.""""""

        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        heartbeat_data = gcs_entries.Entries(0)

        message = (ray.gcs_utils.HeartbeatBatchTableData.
                   GetRootAsHeartbeatBatchTableData(heartbeat_data, 0))

        for j in range(message.BatchLength()):
            heartbeat_message = message.Batch(j)

            num_resources = heartbeat_message.ResourcesAvailableLabelLength()
            static_resources = {}
            dynamic_resources = {}
            for i in range(num_resources):
                dyn = heartbeat_message.ResourcesAvailableLabel(i)
                static = heartbeat_message.ResourcesTotalLabel(i)
                dynamic_resources[dyn] = (
                    heartbeat_message.ResourcesAvailableCapacity(i))
                static_resources[static] = (
                    heartbeat_message.ResourcesTotalCapacity(i))

            # Update the load metrics for this local scheduler.
            client_id = ray.utils.binary_to_hex(heartbeat_message.ClientId())
            ip = self.local_scheduler_id_to_ip_map.get(client_id)
            if ip:
                self.load_metrics.update(ip, static_resources,
                                         dynamic_resources)
            else:
                logger.warning(
                    ""Monitor: ""
                    ""could not find ip for client {}"".format(client_id))

    def _xray_clean_up_entries_for_driver(self, driver_id):
        """"""Remove this driver's object/task entries from redis.

        Removes control-state entries of all tasks and task return
        objects belonging to the driver.

        Args:
            driver_id: The driver id.
        """"""

        xray_task_table_prefix = (
            ray.gcs_utils.TablePrefix_RAYLET_TASK_string.encode(""ascii""))
        xray_object_table_prefix = (
            ray.gcs_utils.TablePrefix_OBJECT_string.encode(""ascii""))

        task_table_objects = self.state.task_table()
        driver_id_hex = binary_to_hex(driver_id)
        driver_task_id_bins = set()
        for task_id_hex, task_info in task_table_objects.items():
            task_table_object = task_info[""TaskSpec""]
            task_driver_id_hex = task_table_object[""DriverID""]
            if driver_id_hex != task_driver_id_hex:
                # Ignore tasks that aren't from this driver.
                continue
            driver_task_id_bins.add(hex_to_binary(task_id_hex))

        # Get objects associated with the driver.
        object_table_objects = self.state.object_table()
        driver_object_id_bins = set()
        for object_id, _ in object_table_objects.items():
            task_id_bin = ray._raylet.compute_task_id(object_id).binary()
            if task_id_bin in driver_task_id_bins:
                driver_object_id_bins.add(object_id.binary())

        def to_shard_index(id_bin):
            return binary_to_object_id(id_bin).redis_shard_hash() % len(
                self.state.redis_clients)

        # Form the redis keys to delete.
        sharded_keys = [[] for _ in range(len(self.state.redis_clients))]
        for task_id_bin in driver_task_id_bins:
            sharded_keys[to_shard_index(task_id_bin)].append(
                xray_task_table_prefix + task_id_bin)
        for object_id_bin in driver_object_id_bins:
            sharded_keys[to_shard_index(object_id_bin)].append(
                xray_object_table_prefix + object_id_bin)

        # Remove with best effort.
        for shard_index in range(len(sharded_keys)):
            keys = sharded_keys[shard_index]
            if len(keys) == 0:
                continue
            redis = self.state.redis_clients[shard_index]
            num_deleted = redis.delete(*keys)
            logger.info(""Monitor: ""
                        ""Removed {} dead redis entries of the ""
                        ""driver from redis shard {}."".format(
                            num_deleted, shard_index))
            if num_deleted != len(keys):
                logger.warning(""Monitor: ""
                               ""Failed to remove {} relevant redis ""
                               ""entries from redis shard {}."".format(
                                   len(keys) - num_deleted, shard_index))

    def xray_driver_removed_handler(self, unused_channel, data):
        """"""Handle a notification that a driver has been removed.

        Args:
            unused_channel: The message channel.
            data: The message data.
        """"""
        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(
            data, 0)
        driver_data = gcs_entries.Entries(0)
        message = ray.gcs_utils.DriverTableData.GetRootAsDriverTableData(
            driver_data, 0)
        driver_id = message.DriverId()
        logger.info(""Monitor: ""
                    ""XRay Driver {} has been removed."".format(
                        binary_to_hex(driver_id)))
        self._xray_clean_up_entries_for_driver(driver_id)

    def process_messages(self, max_messages=10000):
        """"""Process all messages ready in the subscription channels.

        This reads messages from the subscription channels and calls the
        appropriate handlers until there are no messages left.

        Args:
            max_messages: The maximum number of messages to process before
                returning.
        """"""
        subscribe_clients = [self.primary_subscribe_client]
        for subscribe_client in subscribe_clients:
            for _ in range(max_messages):
                message = subscribe_client.get_message()
                if message is None:
                    # Continue on to the next subscribe client.
                    break

                # Parse the message.
                channel = message[""channel""]
                data = message[""data""]

                # Determine the appropriate message handler.
                if channel == ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL:
                    # Similar functionality as local scheduler info channel
                    message_handler = self.xray_heartbeat_batch_handler
                elif channel == ray.gcs_utils.XRAY_DRIVER_CHANNEL:
                    # Handles driver death.
                    message_handler = self.xray_driver_removed_handler
                else:
                    raise Exception(""This code should be unreachable."")

                # Call the handler.
                message_handler(channel, data)

    def update_local_scheduler_map(self):
        local_schedulers = self.state.client_table()
        self.local_scheduler_id_to_ip_map = {}
        for local_scheduler_info in local_schedulers:
            client_id = local_scheduler_info.get(""DBClientID"") or \
                local_scheduler_info[""ClientID""]
            ip_address = (
                local_scheduler_info.get(""AuxAddress"")
                or local_scheduler_info[""NodeManagerAddress""]).split("":"")[0]
            self.local_scheduler_id_to_ip_map[client_id] = ip_address

    def _maybe_flush_gcs(self):
        """"""Experimental: issue a flush request to the GCS.

        The purpose of this feature is to control GCS memory usage.

        To activate this feature, Ray must be compiled with the flag
        RAY_USE_NEW_GCS set, and Ray must be started at run time with the flag
        as well.
        """"""
        if not self.issue_gcs_flushes:
            return
        if self.gcs_flush_policy is None:
            serialized = self.redis.get(""gcs_flushing_policy"")
            if serialized is None:
                # Client has not set any policy; by default flushing is off.
                return
            self.gcs_flush_policy = pickle.loads(serialized)

        if not self.gcs_flush_policy.should_flush(self.redis_shard):
            return

        max_entries_to_flush = self.gcs_flush_policy.num_entries_to_flush()
        num_flushed = self.redis_shard.execute_command(
            ""HEAD.FLUSH {}"".format(max_entries_to_flush))
        logger.info(""Monitor: num_flushed {}"".format(num_flushed))

        # This flushes event log and log files.
        ray.experimental.flush_redis_unsafe(self.redis)

        self.gcs_flush_policy.record_flush()

    def run(self):
        """"""Run the monitor.

        This function loops forever, checking for messages about dead database
        clients and cleaning up state accordingly.
        """"""
        # Initialize the subscription channel.
        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)
        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)

        # TODO(rkn): If there were any dead clients at startup, we should clean
        # up the associated state in the state tables.

        # Handle messages from the subscription channels.
        while True:
            # Update the mapping from local scheduler client ID to IP address.
            # This is only used to update the load metrics for the autoscaler.
            self.update_local_scheduler_map()

            # Process autoscaling actions
            if self.autoscaler:
                self.autoscaler.update()

            self._maybe_flush_gcs()

            # Process a round of messages.
            self.process_messages()

            # Wait for a heartbeat interval before processing the next round of
            # messages.
            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)

        # TODO(rkn): This infinite loop should be inside of a try/except block,
        # and if an exception is thrown we should push an error message to all
        # drivers.


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=(""Parse Redis server for the ""
                     ""monitor to connect to.""))
    parser.add_argument(
        ""--redis-address"",
        required=True,
        type=str,
        help=""the address to use for Redis"")
    parser.add_argument(
        ""--autoscaling-config"",
        required=False,
        type=str,
        help=""the path to the autoscaling config file"")
    parser.add_argument(
        ""--redis-password"",
        required=False,
        type=str,
        default=None,
        help=""the password to use for Redis"")
    parser.add_argument(
        ""--logging-level"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_LEVEL,
        choices=ray_constants.LOGGER_LEVEL_CHOICES,
        help=ray_constants.LOGGER_LEVEL_HELP)
    parser.add_argument(
        ""--logging-format"",
        required=False,
        type=str,
        default=ray_constants.LOGGER_FORMAT,
        help=ray_constants.LOGGER_FORMAT_HELP)
    args = parser.parse_args()
    setup_logger(args.logging_level, args.logging_format)

    redis_ip_address = get_ip_address(args.redis_address)
    redis_port = get_port(args.redis_address)

    if args.autoscaling_config:
        autoscaling_config = os.path.expanduser(args.autoscaling_config)
    else:
        autoscaling_config = None

    monitor = Monitor(
        redis_ip_address,
        redis_port,
        autoscaling_config,
        redis_password=args.redis_password)

    try:
        monitor.run()
    except Exception as e:
        # Something went wrong, so push an error to all drivers.
        redis_client = redis.StrictRedis(
            host=redis_ip_address,
            port=redis_port,
            password=args.redis_password)
        traceback_str = ray.utils.format_error_message(traceback.format_exc())
        message = ""The monitor failed with the following error:\n{}"".format(
            traceback_str)
        ray.utils.push_error_to_driver_through_redis(
            redis_client, ray_constants.MONITOR_DIED_ERROR, message)
        raise e
/n/n/n/python/ray/node.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import atexit
import json
import os
import logging
import signal
import threading
import time

import ray
import ray.ray_constants as ray_constants
from ray.tempfile_services import (
    get_logs_dir_path, get_object_store_socket_name, get_raylet_socket_name,
    new_log_monitor_log_file, new_monitor_log_file,
    new_raylet_monitor_log_file, new_plasma_store_log_file,
    new_raylet_log_file, new_webui_log_file, set_temp_root,
    try_to_create_directory)

# Logger for this module. It should be configured at the entry point
# into the program using Ray. Ray configures it by default automatically
# using logging.basicConfig in its entry/init points.
logger = logging.getLogger(__name__)


class Node(object):
    """"""An encapsulation of the Ray processes on a single node.

    This class is responsible for starting Ray processes and killing them.

    Attributes:
        all_processes (dict): A mapping from process type (str) to a list of
            ProcessInfo objects. All lists have length one except for the Redis
            server list, which has multiple.
    """"""

    def __init__(self, ray_params, head=False, shutdown_at_exit=True):
        """"""Start a node.

        Args:
            ray_params (ray.params.RayParams): The parameters to use to
                configure the node.
            head (bool): True if this is the head node, which means it will
                start additional processes like the Redis servers, monitor
                processes, and web UI.
            shutdown_at_exit (bool): If true, a handler will be registered to
                shutdown the processes started here when the Python interpreter
                exits.
        """"""
        self.all_processes = {}

        ray_params.update_if_absent(
            node_ip_address=ray.services.get_node_ip_address(),
            include_log_monitor=True,
            resources={},
            include_webui=False,
            worker_path=os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                ""workers/default_worker.py""))

        if head:
            ray_params.update_if_absent(num_redis_shards=1, include_webui=True)
        else:
            redis_client = ray.services.create_redis_client(
                ray_params.redis_address, ray_params.redis_password)
            ray_params.include_java = (
                ray.services.include_java_from_redis(redis_client))

        self._ray_params = ray_params
        self._config = (json.loads(ray_params._internal_config)
                        if ray_params._internal_config else None)
        self._node_ip_address = ray_params.node_ip_address
        self._redis_address = ray_params.redis_address
        self._plasma_store_socket_name = None
        self._raylet_socket_name = None
        self._webui_url = None

        self.start_ray_processes()

        if shutdown_at_exit:
            atexit.register(lambda: self.kill_all_processes(
                check_alive=False, allow_graceful=True))

    @property
    def node_ip_address(self):
        """"""Get the cluster Redis address.""""""
        return self._node_ip_address

    @property
    def redis_address(self):
        """"""Get the cluster Redis address.""""""
        return self._redis_address

    @property
    def plasma_store_socket_name(self):
        """"""Get the node's plasma store socket name.""""""
        return self._plasma_store_socket_name

    @property
    def webui_url(self):
        """"""Get the cluster's web UI url.""""""
        return self._webui_url

    @property
    def raylet_socket_name(self):
        """"""Get the node's raylet socket name.""""""
        return self._raylet_socket_name

    def prepare_socket_file(self, socket_path):
        """"""Prepare the socket file for raylet and plasma.

        This method helps to prepare a socket file.
        1. Make the directory if the directory does not exist.
        2. If the socket file exists, raise exception.

        Args:
            socket_path (string): the socket file to prepare.
        """"""
        if not os.path.exists(socket_path):
            path = os.path.dirname(socket_path)
            if not os.path.isdir(path):
                try_to_create_directory(path)
        else:
            raise Exception(""Socket file {} exists!"".format(socket_path))

    def start_redis(self):
        """"""Start the Redis servers.""""""
        assert self._redis_address is None
        (self._redis_address, redis_shards,
         process_infos) = ray.services.start_redis(
             self._node_ip_address,
             port=self._ray_params.redis_port,
             redis_shard_ports=self._ray_params.redis_shard_ports,
             num_redis_shards=self._ray_params.num_redis_shards,
             redis_max_clients=self._ray_params.redis_max_clients,
             redirect_output=self._ray_params.redirect_output,
             redirect_worker_output=self._ray_params.redirect_worker_output,
             password=self._ray_params.redis_password,
             redis_max_memory=self._ray_params.redis_max_memory)
        assert (
            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (
            process_infos)

    def start_log_monitor(self):
        """"""Start the log monitor.""""""
        stdout_file, stderr_file = new_log_monitor_log_file()
        process_info = ray.services.start_log_monitor(
            self.redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_LOG_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] = [
            process_info
        ]

    def start_ui(self):
        """"""Start the web UI.""""""
        stdout_file, stderr_file = new_webui_log_file()
        self._webui_url, process_info = ray.services.start_ui(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file)
        assert ray_constants.PROCESS_TYPE_WEB_UI not in self.all_processes
        if process_info is not None:
            self.all_processes[ray_constants.PROCESS_TYPE_WEB_UI] = [
                process_info
            ]

    def start_plasma_store(self):
        """"""Start the plasma store.""""""
        assert self._plasma_store_socket_name is None
        # If the user specified a socket name, use it.
        self._plasma_store_socket_name = (
            self._ray_params.plasma_store_socket_name
            or get_object_store_socket_name())
        self.prepare_socket_file(self._plasma_store_socket_name)
        stdout_file, stderr_file = (new_plasma_store_log_file(
            self._ray_params.redirect_output))
        process_info = ray.services.start_plasma_store(
            self._node_ip_address,
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            object_store_memory=self._ray_params.object_store_memory,
            plasma_directory=self._ray_params.plasma_directory,
            huge_pages=self._ray_params.huge_pages,
            plasma_store_socket_name=self._plasma_store_socket_name,
            redis_password=self._ray_params.redis_password)
        assert (
            ray_constants.PROCESS_TYPE_PLASMA_STORE not in self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_PLASMA_STORE] = [
            process_info
        ]

    def start_raylet(self, use_valgrind=False, use_profiler=False):
        """"""Start the raylet.

        Args:
            use_valgrind (bool): True if we should start the process in
                valgrind.
            use_profiler (bool): True if we should start the process in the
                valgrind profiler.
        """"""
        assert self._raylet_socket_name is None
        # If the user specified a socket name, use it.
        self._raylet_socket_name = (self._ray_params.raylet_socket_name
                                    or get_raylet_socket_name())
        self.prepare_socket_file(self._raylet_socket_name)
        stdout_file, stderr_file = new_raylet_log_file(
            redirect_output=self._ray_params.redirect_worker_output)
        process_info = ray.services.start_raylet(
            self._redis_address,
            self._node_ip_address,
            self._raylet_socket_name,
            self._plasma_store_socket_name,
            self._ray_params.worker_path,
            self._ray_params.num_cpus,
            self._ray_params.num_gpus,
            self._ray_params.resources,
            self._ray_params.object_manager_port,
            self._ray_params.node_manager_port,
            self._ray_params.redis_password,
            use_valgrind=use_valgrind,
            use_profiler=use_profiler,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            config=self._config,
            include_java=self._ray_params.include_java,
            java_worker_options=self._ray_params.java_worker_options,
        )
        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]

    def start_worker(self):
        """"""Start a worker process.""""""
        raise NotImplementedError

    def start_monitor(self):
        """"""Start the monitor.""""""
        stdout_file, stderr_file = new_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_monitor(
            self._redis_address,
            self._node_ip_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            autoscaling_config=self._ray_params.autoscaling_config,
            redis_password=self._ray_params.redis_password)
        assert ray_constants.PROCESS_TYPE_MONITOR not in self.all_processes
        self.all_processes[ray_constants.PROCESS_TYPE_MONITOR] = [process_info]

    def start_raylet_monitor(self):
        """"""Start the raylet monitor.""""""
        stdout_file, stderr_file = new_raylet_monitor_log_file(
            self._ray_params.redirect_output)
        process_info = ray.services.start_raylet_monitor(
            self._redis_address,
            stdout_file=stdout_file,
            stderr_file=stderr_file,
            redis_password=self._ray_params.redis_password,
            config=self._config)
        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in
                self.all_processes)
        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [
            process_info
        ]

    def start_ray_processes(self):
        """"""Start all of the processes on the node.""""""
        set_temp_root(self._ray_params.temp_dir)
        logger.info(
            ""Process STDOUT and STDERR is being redirected to {}."".format(
                get_logs_dir_path()))

        # If this is the head node, start the relevant head node processes.
        if self._redis_address is None:
            self.start_redis()
            self.start_monitor()
            self.start_raylet_monitor()

        self.start_plasma_store()
        self.start_raylet()

        if self._ray_params.include_log_monitor:
            self.start_log_monitor()
        if self._ray_params.include_webui:
            self.start_ui()

    def _kill_process_type(self,
                           process_type,
                           allow_graceful=False,
                           check_alive=True,
                           wait=False):
        """"""Kill a process of a given type.

        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all
        of the Redis servers.

        If the process was started in valgrind, then we will raise an exception
        if the process has a non-zero exit code.

        Args:
            process_type: The type of the process to kill.
            allow_graceful (bool): Send a SIGTERM first and give the process
                time to exit gracefully. If that doesn't work, then use
                SIGKILL. We usually want to do this outside of tests.
            check_alive (bool): If true, then we expect the process to be alive
                and will raise an exception if the process is already dead.
            wait (bool): If true, then this method will not return until the
                process in question has exited.

        Raises:
            This process raises an exception in the following cases:
                1. The process had already died and check_alive is true.
                2. The process had been started in valgrind and had a non-zero
                   exit code.
        """"""
        process_infos = self.all_processes[process_type]
        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:
            assert len(process_infos) == 1
        for process_info in process_infos:
            process = process_info.process
            # Handle the case where the process has already exited.
            if process.poll() is not None:
                if check_alive:
                    raise Exception(""Attempting to kill a process of type ""
                                    ""'{}', but this process is already dead.""
                                    .format(process_type))
                else:
                    continue

            if process_info.use_valgrind:
                process.terminate()
                process.wait()
                if process.returncode != 0:
                    message = (""Valgrind detected some errors in process of ""
                               ""type {}. Error code {}."".format(
                                   process_type, process.returncode))
                    if process_info.stdout_file is not None:
                        with open(process_info.stdout_file, ""r"") as f:
                            message += ""\nPROCESS STDOUT:\n"" + f.read()
                    if process_info.stderr_file is not None:
                        with open(process_info.stderr_file, ""r"") as f:
                            message += ""\nPROCESS STDERR:\n"" + f.read()
                    raise Exception(message)
                continue

            if process_info.use_valgrind_profiler:
                # Give process signal to write profiler data.
                os.kill(process.pid, signal.SIGINT)
                # Wait for profiling data to be written.
                time.sleep(0.1)

            if allow_graceful:
                # Allow the process one second to exit gracefully.
                process.terminate()
                timer = threading.Timer(1, lambda process: process.kill(),
                                        [process])
                try:
                    timer.start()
                    process.wait()
                finally:
                    timer.cancel()

                if process.poll() is not None:
                    continue

            # If the process did not exit within one second, force kill it.
            process.kill()
            # The reason we usually don't call process.wait() here is that
            # there's some chance we'd end up waiting a really long time.
            if wait:
                process.wait()

        del self.all_processes[process_type]

    def kill_redis(self, check_alive=True):
        """"""Kill the Redis servers.

        Args:
            check_alive (bool): Raise an exception if any of the processes
                were already dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)

    def kill_plasma_store(self, check_alive=True):
        """"""Kill the plasma store.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)

    def kill_raylet(self, check_alive=True):
        """"""Kill the raylet.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET, check_alive=check_alive)

    def kill_log_monitor(self, check_alive=True):
        """"""Kill the log monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_LOG_MONITOR, check_alive=check_alive)

    def kill_monitor(self, check_alive=True):
        """"""Kill the monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_MONITOR, check_alive=check_alive)

    def kill_raylet_monitor(self, check_alive=True):
        """"""Kill the raylet monitor.

        Args:
            check_alive (bool): Raise an exception if the process was already
                dead.
        """"""
        self._kill_process_type(
            ray_constants.PROCESS_TYPE_RAYLET_MONITOR, check_alive=check_alive)

    def kill_all_processes(self, check_alive=True, allow_graceful=False):
        """"""Kill all of the processes.

        Note that This is slower than necessary because it calls kill, wait,
        kill, wait, ... instead of kill, kill, ..., wait, wait, ...

        Args:
            check_alive (bool): Raise an exception if any of the processes were
                already dead.
        """"""
        # Kill the raylet first. This is important for suppressing errors at
        # shutdown because we give the raylet a chance to exit gracefully and
        # clean up its child worker processes. If we were to kill the plasma
        # store (or Redis) first, that could cause the raylet to exit
        # ungracefully, leading to more verbose output from the workers.
        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:
            self._kill_process_type(
                ray_constants.PROCESS_TYPE_RAYLET,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

        # We call ""list"" to copy the keys because we are modifying the
        # dictionary while iterating over it.
        for process_type in list(self.all_processes.keys()):
            self._kill_process_type(
                process_type,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

    def live_processes(self):
        """"""Return a list of the live processes.

        Returns:
            A list of the live processes.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is None:
                    result.append((process_type, process_info.process))
        return result

    def dead_processes(self):
        """"""Return a list of the dead processes.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            A list of the dead processes ignoring the ones that have been
                explicitly killed.
        """"""
        result = []
        for process_type, process_infos in self.all_processes.items():
            for process_info in process_infos:
                if process_info.process.poll() is not None:
                    result.append((process_type, process_info.process))
        return result

    def any_processes_alive(self):
        """"""Return true if any processes are still alive.

        Returns:
            True if any process is still alive.
        """"""
        return any(self.live_processes())

    def remaining_processes_alive(self):
        """"""Return true if all remaining processes are still alive.

        Note that this ignores processes that have been explicitly killed,
        e.g., via a command like node.kill_raylet().

        Returns:
            True if any process that wasn't explicitly killed is still alive.
        """"""
        return not any(self.dead_processes())
/n/n/n/python/ray/parameter.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging

import ray.ray_constants as ray_constants


class RayParams(object):
    """"""A class used to store the parameters used by Ray.

    Attributes:
        redis_address (str): The address of the Redis server to connect to. If
            this address is not provided, then this command will start Redis, a
            global scheduler, a local scheduler, a plasma store, a plasma
            manager, and some workers. It will also kill these processes when
            Python exits.
        redis_port (int): The port that the primary Redis shard should listen
            to. If None, then a random port will be chosen.
        redis_shard_ports: A list of the ports to use for the non-primary Redis
            shards.
        num_cpus (int): Number of CPUs to configure the raylet with.
        num_gpus (int): Number of GPUs to configure the raylet with.
        resources: A dictionary mapping the name of a resource to the quantity
            of that resource available.
        object_store_memory: The amount of memory (in bytes) to start the
            object store with.
        redis_max_memory: The max amount of memory (in bytes) to allow redis
            to use, or None for no limit. Once the limit is exceeded, redis
            will start LRU eviction of entries. This only applies to the
            sharded redis tables (task and object tables).
        object_manager_port int: The port to use for the object manager.
        node_manager_port: The port to use for the node manager.
        node_ip_address (str): The IP address of the node that we are on.
        object_id_seed (int): Used to seed the deterministic generation of
            object IDs. The same value can be used across multiple runs of the
            same job in order to generate the object IDs in a consistent
            manner. However, the same ID should not be used for different jobs.
        local_mode (bool): True if the code should be executed serially
            without Ray. This is useful for debugging.
        redirect_worker_output: True if the stdout and stderr of worker
            processes should be redirected to files.
        redirect_output (bool): True if stdout and stderr for non-worker
            processes should be redirected to files and false otherwise.
        num_redis_shards: The number of Redis shards to start in addition to
            the primary Redis shard.
        redis_max_clients: If provided, attempt to configure Redis with this
            maxclients number.
        redis_password (str): Prevents external clients without the password
            from connecting to Redis if provided.
        plasma_directory: A directory where the Plasma memory mapped files will
            be created.
        worker_path (str): The path of the source code that will be run by the
            worker.
        huge_pages: Boolean flag indicating whether to start the Object
            Store with hugetlbfs support. Requires plasma_directory.
        include_webui: Boolean flag indicating whether to start the web
            UI, which is a Jupyter notebook.
        logging_level: Logging level, default will be logging.INFO.
        logging_format: Logging format, default contains a timestamp,
            filename, line number, and message. See ray_constants.py.
        plasma_store_socket_name (str): If provided, it will specify the socket
            name used by the plasma store.
        raylet_socket_name (str): If provided, it will specify the socket path
            used by the raylet process.
        temp_dir (str): If provided, it will specify the root temporary
            directory for the Ray process.
        include_log_monitor (bool): If True, then start a log monitor to
            monitor the log files for all processes on this node and push their
            contents to Redis.
        autoscaling_config: path to autoscaling config file.
        include_java (bool): If True, the raylet backend can also support
            Java worker.
        java_worker_options (str): The command options for Java worker.
        _internal_config (str): JSON configuration for overriding
            RayConfig defaults. For testing purposes ONLY.
    """"""

    def __init__(self,
                 redis_address=None,
                 num_cpus=None,
                 num_gpus=None,
                 resources=None,
                 object_store_memory=None,
                 redis_max_memory=None,
                 redis_port=None,
                 redis_shard_ports=None,
                 object_manager_port=None,
                 node_manager_port=None,
                 node_ip_address=None,
                 object_id_seed=None,
                 num_workers=None,
                 local_mode=False,
                 driver_mode=None,
                 redirect_worker_output=False,
                 redirect_output=True,
                 num_redis_shards=None,
                 redis_max_clients=None,
                 redis_password=None,
                 plasma_directory=None,
                 worker_path=None,
                 huge_pages=False,
                 include_webui=None,
                 logging_level=logging.INFO,
                 logging_format=ray_constants.LOGGER_FORMAT,
                 plasma_store_socket_name=None,
                 raylet_socket_name=None,
                 temp_dir=None,
                 include_log_monitor=None,
                 autoscaling_config=None,
                 include_java=False,
                 java_worker_options=None,
                 _internal_config=None):
        self.object_id_seed = object_id_seed
        self.redis_address = redis_address
        self.num_cpus = num_cpus
        self.num_gpus = num_gpus
        self.resources = resources
        self.object_store_memory = object_store_memory
        self.redis_max_memory = redis_max_memory
        self.redis_port = redis_port
        self.redis_shard_ports = redis_shard_ports
        self.object_manager_port = object_manager_port
        self.node_manager_port = node_manager_port
        self.node_ip_address = node_ip_address
        self.num_workers = num_workers
        self.local_mode = local_mode
        self.driver_mode = driver_mode
        self.redirect_worker_output = redirect_worker_output
        self.redirect_output = redirect_output
        self.num_redis_shards = num_redis_shards
        self.redis_max_clients = redis_max_clients
        self.redis_password = redis_password
        self.plasma_directory = plasma_directory
        self.worker_path = worker_path
        self.huge_pages = huge_pages
        self.include_webui = include_webui
        self.plasma_store_socket_name = plasma_store_socket_name
        self.raylet_socket_name = raylet_socket_name
        self.temp_dir = temp_dir
        self.include_log_monitor = include_log_monitor
        self.autoscaling_config = autoscaling_config
        self.include_java = include_java
        self.java_worker_options = java_worker_options
        self._internal_config = _internal_config
        self._check_usage()

    def update(self, **kwargs):
        """"""Update the settings according to the keyword arguments.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update: %s"" % arg)

        self._check_usage()

    def update_if_absent(self, **kwargs):
        """"""Update the settings when the target fields are None.

        Args:
            kwargs: The keyword arguments to set corresponding fields.
        """"""
        for arg in kwargs:
            if hasattr(self, arg):
                if getattr(self, arg) is None:
                    setattr(self, arg, kwargs[arg])
            else:
                raise ValueError(""Invalid RayParams parameter in""
                                 "" update_if_absent: %s"" % arg)

        self._check_usage()

    def _check_usage(self):
        if self.resources is not None:
            assert ""CPU"" not in self.resources, (
                ""'CPU' should not be included in the resource dictionary. Use ""
                ""num_cpus instead."")
            assert ""GPU"" not in self.resources, (
                ""'GPU' should not be included in the resource dictionary. Use ""
                ""num_gpus instead."")

        if self.num_workers is not None:
            raise ValueError(
                ""The 'num_workers' argument is deprecated. Please use ""
                ""'num_cpus' instead."")

        if self.include_java is None and self.java_worker_options is not None:
            raise ValueError(""Should not specify `java-worker-options` ""
                             ""without providing `include-java`."")
/n/n/n/python/ray/profiling.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import json
import time
import threading
import traceback

import ray

LOG_POINT = 0
LOG_SPAN_START = 1
LOG_SPAN_END = 2


class _NullLogSpan(object):
    """"""A log span context manager that does nothing""""""

    def __enter__(self):
        pass

    def __exit__(self, type, value, tb):
        pass


NULL_LOG_SPAN = _NullLogSpan()


def profile(event_type, extra_data=None, worker=None):
    """"""Profile a span of time so that it appears in the timeline visualization.

    Note that this only works in the raylet code path.

    This function can be used as follows (both on the driver or within a task).

    .. code-block:: python

        with ray.profile(""custom event"", extra_data={'key': 'value'}):
            # Do some computation here.

    Optionally, a dictionary can be passed as the ""extra_data"" argument, and
    it can have keys ""name"" and ""cname"" if you want to override the default
    timeline display text and box color. Other values will appear at the bottom
    of the chrome tracing GUI when you click on the box corresponding to this
    profile span.

    Args:
        event_type: A string describing the type of the event.
        extra_data: This must be a dictionary mapping strings to strings. This
            data will be added to the json objects that are used to populate
            the timeline, so if you want to set a particular color, you can
            simply set the ""cname"" attribute to an appropriate color.
            Similarly, if you set the ""name"" attribute, then that will set the
            text displayed on the box in the timeline.

    Returns:
        An object that can profile a span of time via a ""with"" statement.
    """"""
    if worker is None:
        worker = ray.worker.global_worker
    return RayLogSpanRaylet(worker.profiler, event_type, extra_data=extra_data)


class Profiler(object):
    """"""A class that holds the profiling states.

    Attributes:
        worker: the worker to profile.
        events: the buffer of events.
        lock: the lock to protect access of events.
    """"""

    def __init__(self, worker):
        self.worker = worker
        self.events = []
        self.lock = threading.Lock()

    def start_flush_thread(self):
        t = threading.Thread(
            target=self._periodically_flush_profile_events,
            name=""ray_push_profiling_information"")
        # Making the thread a daemon causes it to exit when the main thread
        # exits.
        t.daemon = True
        t.start()

    def _periodically_flush_profile_events(self):
        """"""Drivers run this as a thread to flush profile data in the
        background.""""""
        # Note(rkn): This is run on a background thread in the driver. It uses
        # the local scheduler client. This should be ok because it doesn't read
        # from the local scheduler client and we have the GIL here. However,
        # if either of those things changes, then we could run into issues.
        try:
            while True:
                time.sleep(1)
                self.flush_profile_data()
        except AttributeError:
            # TODO(suquark): It is a bad idea to ignore ""AttributeError"".
            # It has caused some very unexpected behaviors when implementing
            # new features (related to AttributeError).

            # This is to suppress errors that occur at shutdown.
            pass

    def flush_profile_data(self):
        """"""Push the logged profiling data to the global control store.

        By default, profiling information for a given task won't appear in the
        timeline until after the task has completed. For very long-running
        tasks, we may want profiling information to appear more quickly.
        In such cases, this function can be called. Note that as an
        alternative, we could start a thread in the background on workers that
        calls this automatically.
        """"""
        with self.lock:
            events = self.events
            self.events = []

        if self.worker.mode == ray.WORKER_MODE:
            component_type = ""worker""
        else:
            component_type = ""driver""

        self.worker.raylet_client.push_profile_events(
            component_type, ray.UniqueID(self.worker.worker_id),
            self.worker.node_ip_address, events)

    def add_event(self, event):
        with self.lock:
            self.events.append(event)


class RayLogSpanRaylet(object):
    """"""An object used to enable logging a span of events with a with statement.

    Attributes:
        event_type (str): The type of the event being logged.
        extra_data: Additional information to log.
    """"""

    def __init__(self, profiler, event_type, extra_data=None):
        """"""Initialize a RayLogSpanRaylet object.""""""
        self.profiler = profiler
        self.event_type = event_type
        self.extra_data = extra_data if extra_data is not None else {}

    def set_attribute(self, key, value):
        """"""Add a key-value pair to the extra_data dict.

        This can be used to add attributes that are not available when
        ray.profile was called.

        Args:
            key: The attribute name.
            value: The attribute value.
        """"""
        if not isinstance(key, str) or not isinstance(value, str):
            raise ValueError(""The arguments 'key' and 'value' must both be ""
                             ""strings. Instead they are {} and {}."".format(
                                 key, value))
        self.extra_data[key] = value

    def __enter__(self):
        """"""Log the beginning of a span event.

        Returns:
            The object itself is returned so that if the block is opened using
                ""with ray.profile(...) as prof:"", we can call
                ""prof.set_attribute"" inside the block.
        """"""
        self.start_time = time.time()
        return self

    def __exit__(self, type, value, tb):
        """"""Log the end of a span event. Log any exception that occurred.""""""
        for key, value in self.extra_data.items():
            if not isinstance(key, str) or not isinstance(value, str):
                raise ValueError(""The extra_data argument must be a ""
                                 ""dictionary mapping strings to strings. ""
                                 ""Instead it is {}."".format(self.extra_data))

        if type is not None:
            extra_data = json.dumps({
                ""type"": str(type),
                ""value"": str(value),
                ""traceback"": str(traceback.format_exc()),
            })
        else:
            extra_data = json.dumps(self.extra_data)

        event = {
            ""event_type"": self.event_type,
            ""start_time"": self.start_time,
            ""end_time"": time.time(),
            ""extra_data"": extra_data,
        }

        self.profiler.add_event(event)
/n/n/n/python/ray/utils.py/n/nfrom __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import binascii
import functools
import hashlib
import inspect
import logging
import numpy as np
import os
import subprocess
import sys
import threading
import time
import uuid

import ray.gcs_utils
import ray.ray_constants as ray_constants


def _random_string():
    id_hash = hashlib.sha1()
    id_hash.update(uuid.uuid4().bytes)
    id_bytes = id_hash.digest()
    assert len(id_bytes) == ray_constants.ID_SIZE
    return id_bytes


def format_error_message(exception_message, task_exception=False):
    """"""Improve the formatting of an exception thrown by a remote function.

    This method takes a traceback from an exception and makes it nicer by
    removing a few uninformative lines and adding some space to indent the
    remaining lines nicely.

    Args:
        exception_message (str): A message generated by traceback.format_exc().

    Returns:
        A string of the formatted exception message.
    """"""
    lines = exception_message.split(""\n"")
    if task_exception:
        # For errors that occur inside of tasks, remove lines 1 and 2 which are
        # always the same, they just contain information about the worker code.
        lines = lines[0:1] + lines[3:]
        pass
    return ""\n"".join(lines)


def push_error_to_driver(worker, error_type, message, driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Args:
        worker: The worker to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    worker.raylet_client.push_error(driver_id, error_type, message,
                                    time.time())


def push_error_to_driver_through_redis(redis_client,
                                       error_type,
                                       message,
                                       driver_id=None):
    """"""Push an error message to the driver to be printed in the background.

    Normally the push_error_to_driver function should be used. However, in some
    instances, the local scheduler client is not available, e.g., because the
    error happens in Python before the driver or worker has connected to the
    backend processes.

    Args:
        redis_client: The redis client to use.
        error_type (str): The type of the error.
        message (str): The message that will be printed in the background
            on the driver.
        driver_id: The ID of the driver to push the error message to. If this
            is None, then the message will be pushed to all drivers.
    """"""
    if driver_id is None:
        driver_id = ray.DriverID.nil()
    # Do everything in Python and through the Python Redis client instead
    # of through the raylet.
    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,
                                                       message, time.time())
    redis_client.execute_command(""RAY.TABLE_APPEND"",
                                 ray.gcs_utils.TablePrefix.ERROR_INFO,
                                 ray.gcs_utils.TablePubsub.ERROR_INFO,
                                 driver_id.binary(), error_data)


def is_cython(obj):
    """"""Check if an object is a Cython function or method""""""

    # TODO(suo): We could split these into two functions, one for Cython
    # functions and another for Cython methods.
    # TODO(suo): There doesn't appear to be a Cython function 'type' we can
    # check against via isinstance. Please correct me if I'm wrong.
    def check_cython(x):
        return type(x).__name__ == ""cython_function_or_method""

    # Check if function or method, respectively
    return check_cython(obj) or \
        (hasattr(obj, ""__func__"") and check_cython(obj.__func__))


def is_function_or_method(obj):
    """"""Check if an object is a function or method.

    Args:
        obj: The Python object in question.

    Returns:
        True if the object is an function or method.
    """"""
    return inspect.isfunction(obj) or inspect.ismethod(obj) or is_cython(obj)


def is_class_method(f):
    """"""Returns whether the given method is a class_method.""""""
    return hasattr(f, ""__self__"") and f.__self__ is not None


def random_string():
    """"""Generate a random string to use as an ID.

    Note that users may seed numpy, which could cause this function to generate
    duplicate IDs. Therefore, we need to seed numpy ourselves, but we can't
    interfere with the state of the user's random number generator, so we
    extract the state of the random number generator and reset it after we are
    done.

    TODO(rkn): If we want to later guarantee that these are generated in a
    deterministic manner, then we will need to make some changes here.

    Returns:
        A random byte string of length ray_constants.ID_SIZE.
    """"""
    # Get the state of the numpy random number generator.
    numpy_state = np.random.get_state()
    # Try to use true randomness.
    np.random.seed(None)
    # Generate the random ID.
    random_id = np.random.bytes(ray_constants.ID_SIZE)
    # Reset the state of the numpy random number generator.
    np.random.set_state(numpy_state)
    return random_id


def decode(byte_str, allow_none=False):
    """"""Make this unicode in Python 3, otherwise leave it as bytes.

    Args:
        byte_str: The byte string to decode.
        allow_none: If true, then we will allow byte_str to be None in which
            case we will return an empty string. TODO(rkn): Remove this flag.
            This is only here to simplify upgrading to flatbuffers 1.10.0.

    Returns:
        A byte string in Python 2 and a unicode string in Python 3.
    """"""
    if byte_str is None and allow_none:
        return """"

    if not isinstance(byte_str, bytes):
        raise ValueError(
            ""The argument {} must be a bytes object."".format(byte_str))
    if sys.version_info >= (3, 0):
        return byte_str.decode(""ascii"")
    else:
        return byte_str


def binary_to_object_id(binary_object_id):
    return ray.ObjectID(binary_object_id)


def binary_to_hex(identifier):
    hex_identifier = binascii.hexlify(identifier)
    if sys.version_info >= (3, 0):
        hex_identifier = hex_identifier.decode()
    return hex_identifier


def hex_to_binary(hex_identifier):
    return binascii.unhexlify(hex_identifier)


def get_cuda_visible_devices():
    """"""Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable.

    Returns:
        if CUDA_VISIBLE_DEVICES is set, this returns a list of integers with
            the IDs of the GPUs. If it is not set, this returns None.
    """"""
    gpu_ids_str = os.environ.get(""CUDA_VISIBLE_DEVICES"", None)

    if gpu_ids_str is None:
        return None

    if gpu_ids_str == """":
        return []

    return [int(i) for i in gpu_ids_str.split("","")]


def set_cuda_visible_devices(gpu_ids):
    """"""Set the CUDA_VISIBLE_DEVICES environment variable.

    Args:
        gpu_ids: This is a list of integers representing GPU IDs.
    """"""
    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join([str(i) for i in gpu_ids])


def resources_from_resource_arguments(default_num_cpus, default_num_gpus,
                                      default_resources, runtime_num_cpus,
                                      runtime_num_gpus, runtime_resources):
    """"""Determine a task's resource requirements.

    Args:
        default_num_cpus: The default number of CPUs required by this function
            or actor method.
        default_num_gpus: The default number of GPUs required by this function
            or actor method.
        default_resources: The default custom resources required by this
            function or actor method.
        runtime_num_cpus: The number of CPUs requested when the task was
            invoked.
        runtime_num_gpus: The number of GPUs requested when the task was
            invoked.
        runtime_resources: The custom resources requested when the task was
            invoked.

    Returns:
        A dictionary of the resource requirements for the task.
    """"""
    if runtime_resources is not None:
        resources = runtime_resources.copy()
    elif default_resources is not None:
        resources = default_resources.copy()
    else:
        resources = {}

    if ""CPU"" in resources or ""GPU"" in resources:
        raise ValueError(""The resources dictionary must not ""
                         ""contain the key 'CPU' or 'GPU'"")

    assert default_num_cpus is not None
    resources[""CPU""] = (default_num_cpus
                        if runtime_num_cpus is None else runtime_num_cpus)

    if runtime_num_gpus is not None:
        resources[""GPU""] = runtime_num_gpus
    elif default_num_gpus is not None:
        resources[""GPU""] = default_num_gpus

    return resources


_default_handler = None


def setup_logger(logging_level, logging_format):
    """"""Setup default logging for ray.""""""
    logger = logging.getLogger(""ray"")
    if type(logging_level) is str:
        logging_level = logging.getLevelName(logging_level.upper())
    logger.setLevel(logging_level)
    global _default_handler
    _default_handler = logging.StreamHandler()
    _default_handler.setFormatter(logging.Formatter(logging_format))
    logger.addHandler(_default_handler)
    logger.propagate = False


def try_update_handler(new_stream):
    global _default_handler
    logger = logging.getLogger(""ray"")
    if _default_handler:
        new_handler = logging.StreamHandler(stream=new_stream)
        new_handler.setFormatter(_default_handler.formatter)
        _default_handler.close()
        _default_handler = new_handler
        logger.addHandler(_default_handler)


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5bd44f8afcecbfb0db479ce230c790fc2c56569a/psutil/tests/test_linux.py#L132-L138  # noqa: E501
def vmstat(stat):
    """"""Run vmstat and get a particular statistic.

    Args:
        stat: The statistic that we are interested in retrieving.

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output([""vmstat"", ""-s""])
    stat = stat.encode(""ascii"")
    for line in out.split(b""\n""):
        line = line.strip()
        if stat in line:
            return int(line.split(b"" "")[0])
    raise ValueError(""Can't find {} in 'vmstat' output."".format(stat))


# This function is copied and modified from
# https://github.com/giampaolo/psutil/blob/5e90b0a7f3fccb177445a186cc4fac62cfadb510/psutil/tests/test_osx.py#L29-L38  # noqa: E501
def sysctl(command):
    """"""Run a sysctl command and parse the output.

    Args:
        command: A sysctl command with an argument, for example,
            [""sysctl"", ""hw.memsize""].

    Returns:
        The parsed output.
    """"""
    out = subprocess.check_output(command)
    result = out.split(b"" "")[1]
    try:
        return int(result)
    except ValueError:
        return result


def get_system_memory():
    """"""Return the total amount of system memory in bytes.

    Returns:
        The total amount of system memory in bytes.
    """"""
    # Try to accurately figure out the memory limit if we are in a docker
    # container. Note that this file is not specific to Docker and its value is
    # often much larger than the actual amount of memory.
    docker_limit = None
    memory_limit_filename = ""/sys/fs/cgroup/memory/memory.limit_in_bytes""
    if os.path.exists(memory_limit_filename):
        with open(memory_limit_filename, ""r"") as f:
            docker_limit = int(f.read())

    # Use psutil if it is available.
    psutil_memory_in_bytes = None
    try:
        import psutil
        psutil_memory_in_bytes = psutil.virtual_memory().total
    except ImportError:
        pass

    if psutil_memory_in_bytes is not None:
        memory_in_bytes = psutil_memory_in_bytes
    elif sys.platform == ""linux"" or sys.platform == ""linux2"":
        # Handle Linux.
        bytes_in_kilobyte = 1024
        memory_in_bytes = vmstat(""total memory"") * bytes_in_kilobyte
    else:
        # Handle MacOS.
        memory_in_bytes = sysctl([""sysctl"", ""hw.memsize""])

    if docker_limit is not None:
        return min(docker_limit, memory_in_bytes)
    else:
        return memory_in_bytes


def get_shared_memory_bytes():
    """"""Get the size of the shared memory file system.

    Returns:
        The size of the shared memory file system in bytes.
    """"""
    # Make sure this is only called on Linux.
    assert sys.platform == ""linux"" or sys.platform == ""linux2""

    shm_fd = os.open(""/dev/shm"", os.O_RDONLY)
    try:
        shm_fs_stats = os.fstatvfs(shm_fd)
        # The value shm_fs_stats.f_bsize is the block size and the
        # value shm_fs_stats.f_bavail is the number of available
        # blocks.
        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail
    finally:
        os.close(shm_fd)

    return shm_avail


def check_oversized_pickle(pickled, name, obj_type, worker):
    """"""Send a warning message if the pickled object is too large.

    Args:
        pickled: the pickled object.
        name: name of the pickled object.
        obj_type: type of the pickled object, can be 'function',
            'remote function', 'actor', or 'object'.
        worker: the worker used to send warning message.
    """"""
    length = len(pickled)
    if length <= ray_constants.PICKLE_OBJECT_WARNING_SIZE:
        return
    warning_message = (
        ""Warning: The {} {} has size {} when pickled. ""
        ""It will be stored in Redis, which could cause memory issues. ""
        ""This may mean that its definition uses a large array or other object.""
    ).format(obj_type, name, length)
    push_error_to_driver(
        worker,
        ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,
        warning_message,
        driver_id=worker.task_driver_id)


class _ThreadSafeProxy(object):
    """"""This class is used to create a thread-safe proxy for a given object.
        Every method call will be guarded with a lock.

    Attributes:
        orig_obj (object): the original object.
        lock (threading.Lock): the lock object.
        _wrapper_cache (dict): a cache from original object's methods to
            the proxy methods.
    """"""

    def __init__(self, orig_obj, lock):
        self.orig_obj = orig_obj
        self.lock = lock
        self._wrapper_cache = {}

    def __getattr__(self, attr):
        orig_attr = getattr(self.orig_obj, attr)
        if not callable(orig_attr):
            # If the original attr is a field, just return it.
            return orig_attr
        else:
            # If the orginal attr is a method,
            # return a wrapper that guards the original method with a lock.
            wrapper = self._wrapper_cache.get(attr)
            if wrapper is None:

                @functools.wraps(orig_attr)
                def _wrapper(*args, **kwargs):
                    with self.lock:
                        return orig_attr(*args, **kwargs)

                self._wrapper_cache[attr] = _wrapper
                wrapper = _wrapper
            return wrapper


def thread_safe_client(client, lock=None):
    """"""Create a thread-safe proxy which locks every method call
    for the given client.

    Args:
        client: the client object to be guarded.
        lock: the lock object that will be used to lock client's methods.
            If None, a new lock will be used.

    Returns:
        A thread-safe proxy for the given client.
    """"""
    if lock is None:
        lock = threading.Lock()
    return _ThreadSafeProxy(client, lock)


def is_main_thread():
    return threading.current_thread().getName() == ""MainThread""
/n/n/n",1
