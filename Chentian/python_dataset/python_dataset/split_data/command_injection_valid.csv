,Unnamed: 0,id,code,label
6,6,ebdef37b7e5d2b95a01d34b211c61c61da67e46a,"salt/modules/disk.py/n/n# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    flags = ''
    allowed = ('a', 'B', 'h', 'H', 'i', 'k', 'l', 'P', 't', 'T', 'x', 'v')
    for flag in args:
        if flag in allowed:
            flags += flag
        else:
            break
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd += ' -{0}'.format(flags)
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn(""Problem parsing disk usage information"")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn(""Problem parsing inode usage information"")
            ret = {}
    return ret
/n/n/n",0
7,7,ebdef37b7e5d2b95a01d34b211c61c61da67e46a,"/salt/modules/disk.py/n/n# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn(""Problem parsing disk usage information"")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn(""Problem parsing inode usage information"")
            ret = {}
    return ret
/n/n/n",1
134,134,35708a0b975f89357e75f8f74999900da9fe9894,"lib/contrib/multipartpost.py/n/n#!/usr/bin/env python

""""""
$Id$

02/2006 Will Holcomb <wholcomb@gmail.com>

Reference: http://odin.himinbi.org/MultipartPostHandler.py

This library is free software; you can redistribute it and/or
modify it under the terms of the GNU Lesser General Public
License as published by the Free Software Foundation; either
version 2.1 of the License, or (at your option) any later version.

This library is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public
License along with this library; if not, write to the Free Software
Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import mimetools
import mimetypes
import os
import stat
import sys
import urllib
import urllib2

from lib.core.exception import sqlmapDataException


class Callable:
    def __init__(self, anycallable):
        self.__call__ = anycallable


# Controls how sequences are uncoded. If true, elements may be given
# multiple values by assigning a sequence.
doseq = 1


class MultipartPostHandler(urllib2.BaseHandler):
    handler_order = urllib2.HTTPHandler.handler_order - 10 # needs to run first

    def http_request(self, request):
        data = request.get_data()
        if data is not None and type(data) != str:
            v_files = []
            v_vars = []
            try:
                 for(key, value) in data.items():
                     if type(value) == file:
                         v_files.append((key, value))
                     else:
                         v_vars.append((key, value))
            except TypeError:
                systype, value, traceback = sys.exc_info()
                raise sqlmapDataException, ""not a valid non-string sequence or mapping object"", traceback

            if len(v_files) == 0:
                data = urllib.urlencode(v_vars, doseq)
            else:
                boundary, data = self.multipart_encode(v_vars, v_files)
                contenttype = 'multipart/form-data; boundary=%s' % boundary
                #if (request.has_header('Content-Type') and request.get_header('Content-Type').find('multipart/form-data') != 0):
                #    print ""Replacing %s with %s"" % (request.get_header('content-type'), 'multipart/form-data')
                request.add_unredirected_header('Content-Type', contenttype)

            request.add_data(data)
        return request


    def multipart_encode(vars, files, boundary = None, buffer = None):
        if boundary is None:
            boundary = mimetools.choose_boundary()
        if buffer is None:
            buffer = ''
        for(key, value) in vars:
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name=""%s""' % key
            buffer += '\r\n\r\n' + value + '\r\n'
        for(key, fd) in files:
            file_size = os.fstat(fd.fileno())[stat.ST_SIZE]
            filename = fd.name.split('/')[-1]
            contenttype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
            buffer += '--%s\r\n' % boundary
            buffer += 'Content-Disposition: form-data; name=""%s""; filename=""%s""\r\n' % (key, filename)
            buffer += 'Content-Type: %s\r\n' % contenttype
            # buffer += 'Content-Length: %s\r\n' % file_size
            fd.seek(0)
            buffer += '\r\n' + fd.read() + '\r\n'
        buffer += '--%s--\r\n\r\n' % boundary
        return boundary, buffer
    multipart_encode = Callable(multipart_encode)

    https_request = http_request

/n/n/nlib/controller/checks.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """"""
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """"""

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = """"
        postfix = """"

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = ""testing custom injection ""
        infoMsg += ""on %s parameter '%s'"" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%d=%d %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%d=%d %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""confirming custom injection ""
                infoMsg += ""on %s parameter '%s'"" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%s %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                    infoMsg += ""custom injectable ""
                    logger.info(infoMsg)

                    return ""custom""

    infoMsg  = ""testing unescaped numeric injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s%s AND %s%d=%d"" % (value, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s%s AND %s%d=%d"" % (value, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming unescaped numeric injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s%s AND %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""unescaped numeric injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""numeric""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""unescaped numeric injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing single quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s'='%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s'='%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming single quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s'%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""single quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""stringsingle""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""single quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing LIKE single quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s' LIKE '%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s' LIKE '%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming LIKE single quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s'%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""LIKE single quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""likesingle""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""LIKE single quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing double quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\""=\""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\""=\""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming double quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s\""%s AND %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""double quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""stringdouble""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""double quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing LIKE double quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\"" LIKE \""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\"" LIKE \""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming LIKE double quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s\""%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""LIKE double quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""likedouble""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""LIKE double quoted string injectable""
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """"""
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """"""

    infoMsg = ""testing if %s parameter '%s' is dynamic"" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = ""confirming that %s parameter '%s' is dynamic"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""'%s"" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, ""\""%s"" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """"""
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """"""

    infoMsg = ""testing if the url is stable, wait a few seconds""
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(1)

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)

    condition = firstPage == secondPage

    if condition == False:
        warnMsg  = ""url is not stable, sqlmap will base the page ""
        warnMsg += ""comparison on a sequence matcher, if no dynamic nor ""
        warnMsg += ""injectable parameters are detected, refer to user's ""
        warnMsg += ""manual paragraph 'Page comparison' and provide a ""
        warnMsg += ""string or regular expression to match on""
        logger.warn(warnMsg)

    if condition == True:
        logMsg = ""url is stable""
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key(""String"") and
                  kb.resumedQueries[conf.url][""String""][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = ""testing if the provided string is within the ""
    infoMsg += ""target URL page content""
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = ""you provided '%s' as the string to "" % conf.string
        errMsg += ""match, but such a string is not within the target ""
        errMsg += ""URL page content, please provide another string.""
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key(""Regular expression"") and
                  kb.resumedQueries[conf.url][""Regular expression""][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = ""testing if the provided regular expression matches within ""
    infoMsg += ""the target URL page content""
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = ""you provided '%s' as the regular expression to "" % conf.regexp
        errMsg += ""match, but such a regular expression does not have any ""
        errMsg += ""match within the target URL page content, please provide ""
        errMsg += ""another regular expression.""
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = ""testing connection to the target url""
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += "", skipping to next url""
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True
/n/n/nlib/parse/cmdline.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """"""
    This function parses the command line parameters and arguments
    """"""

    usage = ""%s [options]"" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option(""-v"", dest=""verbose"", type=""int"",
                          help=""Verbosity level: 0-5 (default 1)"")

        # Target options
        target = OptionGroup(parser, ""Target"", ""At least one of these ""
                             ""options has to be specified to set the source ""
                             ""to get target urls from."")

        target.add_option(""-u"", ""--url"", dest=""url"", help=""Target url"")

        target.add_option(""-l"", dest=""list"", help=""Parse targets from Burp ""
                          ""or WebScarab logs"")

        target.add_option(""-g"", dest=""googleDork"",
                          help=""Process Google dork results as target urls"")

        target.add_option(""-c"", dest=""configFile"",
                          help=""Load options from a configuration INI file"")


        # Request options
        request = OptionGroup(parser, ""Request"", ""These options can be used ""
                              ""to specify how to connect to the target url."")

        request.add_option(""--method"", dest=""method"", default=""GET"",
                           help=""HTTP method, GET or POST (default: GET)"")

        request.add_option(""--data"", dest=""data"",
                           help=""Data string to be sent through POST"")

        request.add_option(""--cookie"", dest=""cookie"",
                           help=""HTTP Cookie header"")

        request.add_option(""--referer"", dest=""referer"",
                           help=""HTTP Referer header"")

        request.add_option(""--user-agent"", dest=""agent"",
                           help=""HTTP User-Agent header"")

        request.add_option(""-a"", dest=""userAgentsFile"",
                           help=""Load a random HTTP User-Agent ""
                                ""header from file"")

        request.add_option(""--headers"", dest=""headers"",
                           help=""Extra HTTP headers '\\n' separated"")

        request.add_option(""--auth-type"", dest=""aType"",
                           help=""HTTP Authentication type, value: ""
                                ""Basic or Digest"")

        request.add_option(""--auth-cred"", dest=""aCred"",
                           help=""HTTP Authentication credentials, value: ""
                                ""name:password"")

        request.add_option(""--proxy"", dest=""proxy"",
                           help=""Use a HTTP proxy to connect to the target url"")

        request.add_option(""--threads"", dest=""threads"", type=""int"",
                           help=""Maximum number of concurrent HTTP ""
                                ""requests (default 1)"")

        request.add_option(""--delay"", dest=""delay"", type=""float"",
                           help=""Delay in seconds between each HTTP request"")

        request.add_option(""--timeout"", dest=""timeout"", type=""float"",
                           help=""Seconds to wait before timeout connection ""
                                ""(default 30)"")


        # Injection options
        injection = OptionGroup(parser, ""Injection"", ""These options can be ""
                                ""used to specify which parameters to test ""
                                ""for, provide custom injection payloads and ""
                                ""how to parse and compare HTTP responses ""
                                ""page content when using the blind SQL ""
                                ""injection technique."")

        injection.add_option(""-p"", dest=""testParameter"",
                             help=""Testable parameter(s)"")

        injection.add_option(""--dbms"", dest=""dbms"",
                             help=""Force back-end DBMS to this value"")

        injection.add_option(""--prefix"", dest=""prefix"",
                             help=""Injection payload prefix string"")

        injection.add_option(""--postfix"", dest=""postfix"",
                             help=""Injection payload postfix string"")

        injection.add_option(""--string"", dest=""string"",
                             help=""String to match in page when the ""
                                  ""query is valid"")

        injection.add_option(""--regexp"", dest=""regexp"",
                             help=""Regexp to match in page when the ""
                                  ""query is valid"")

        injection.add_option(""--excl-str"", dest=""eString"",
                             help=""String to be excluded before calculating ""
                                  ""page hash"")

        injection.add_option(""--excl-reg"", dest=""eRegexp"",
                             help=""Regexp matches to be excluded before ""
                                  ""calculating page hash"")


        # Techniques options
        techniques = OptionGroup(parser, ""Techniques"", ""These options can ""
                                 ""be used to test for specific SQL injection ""
                                 ""technique or to use one of them to exploit ""
                                 ""the affected parameter(s) rather than using ""
                                 ""the default blind SQL injection technique."")

        techniques.add_option(""--stacked-test"", dest=""stackedTest"",
                              action=""store_true"",
                              help=""Test for stacked queries (multiple ""
                                   ""statements) support"")

        techniques.add_option(""--time-test"", dest=""timeTest"",
                              action=""store_true"",
                              help=""Test for Time based blind SQL injection"")

        techniques.add_option(""--union-test"", dest=""unionTest"",
                              action=""store_true"",
                              help=""Test for UNION query (inband) SQL injection"")

        techniques.add_option(""--union-use"", dest=""unionUse"",
                              action=""store_true"",
                              help=""Use the UNION query (inband) SQL injection ""
                                   ""to retrieve the queries output. No ""
                                   ""need to go blind"")


        # Fingerprint options
        fingerprint = OptionGroup(parser, ""Fingerprint"")

        fingerprint.add_option(""-f"", ""--fingerprint"", dest=""extensiveFp"",
                               action=""store_true"",
                               help=""Perform an extensive DBMS version fingerprint"")


        # Enumeration options
        enumeration = OptionGroup(parser, ""Enumeration"", ""These options can ""
                                  ""be used to enumerate the back-end database ""
                                  ""management system information, structure ""
                                  ""and data contained in the tables. Moreover ""
                                  ""you can run your own SQL statements."")

        enumeration.add_option(""-b"", ""--banner"", dest=""getBanner"",
                               action=""store_true"", help=""Retrieve DBMS banner"")

        enumeration.add_option(""--current-user"", dest=""getCurrentUser"",
                               action=""store_true"",
                               help=""Retrieve DBMS current user"")

        enumeration.add_option(""--current-db"", dest=""getCurrentDb"",
                               action=""store_true"",
                               help=""Retrieve DBMS current database"")

        enumeration.add_option(""--is-dba"", dest=""isDba"",
                               action=""store_true"",
                               help=""Detect if the DBMS current user is DBA"")

        enumeration.add_option(""--users"", dest=""getUsers"", action=""store_true"",
                               help=""Enumerate DBMS users"")

        enumeration.add_option(""--passwords"", dest=""getPasswordHashes"",
                               action=""store_true"",
                               help=""Enumerate DBMS users password hashes (opt: -U)"")

        enumeration.add_option(""--privileges"", dest=""getPrivileges"",
                               action=""store_true"",
                               help=""Enumerate DBMS users privileges (opt: -U)"")

        enumeration.add_option(""--dbs"", dest=""getDbs"", action=""store_true"",
                               help=""Enumerate DBMS databases"")

        enumeration.add_option(""--tables"", dest=""getTables"", action=""store_true"",
                               help=""Enumerate DBMS database tables (opt: -D)"")

        enumeration.add_option(""--columns"", dest=""getColumns"", action=""store_true"",
                               help=""Enumerate DBMS database table columns ""
                                    ""(req:-T opt:-D)"")

        enumeration.add_option(""--dump"", dest=""dumpTable"", action=""store_true"",
                               help=""Dump DBMS database table entries ""
                                    ""(req: -T, opt: -D, -C, --start, --stop)"")

        enumeration.add_option(""--dump-all"", dest=""dumpAll"", action=""store_true"",
                               help=""Dump all DBMS databases tables entries"")

        enumeration.add_option(""-D"", dest=""db"",
                               help=""DBMS database to enumerate"")

        enumeration.add_option(""-T"", dest=""tbl"",
                               help=""DBMS database table to enumerate"")

        enumeration.add_option(""-C"", dest=""col"",
                               help=""DBMS database table column to enumerate"")

        enumeration.add_option(""-U"", dest=""user"",
                               help=""DBMS user to enumerate"")

        enumeration.add_option(""--exclude-sysdbs"", dest=""excludeSysDbs"",
                               action=""store_true"",
                               help=""Exclude DBMS system databases when ""
                                    ""enumerating tables"")

        enumeration.add_option(""--start"", dest=""limitStart"", type=""int"",
                               help=""First table entry to dump"")

        enumeration.add_option(""--stop"", dest=""limitStop"", type=""int"",
                               help=""Last table entry to dump"")

        enumeration.add_option(""--sql-query"", dest=""query"",
                               help=""SQL statement to be executed"")

        enumeration.add_option(""--sql-shell"", dest=""sqlShell"",
                               action=""store_true"",
                               help=""Prompt for an interactive SQL shell"")


        # File system options
        filesystem = OptionGroup(parser, ""File system access"", ""These options ""
                                 ""can be used to access the back-end database ""
                                 ""management system file system taking ""
                                 ""advantage of native DBMS functions or ""
                                 ""specific DBMS design weaknesses."")

        filesystem.add_option(""--read-file"", dest=""rFile"",
                              help=""Read a specific OS file content (only on MySQL)"")

        filesystem.add_option(""--write-file"", dest=""wFile"",
                              help=""Write to a specific OS file (not yet available)"")


        # Takeover options
        takeover = OptionGroup(parser, ""Operating system access"", ""This ""
                               ""option can be used to access the back-end ""
                               ""database management system operating ""
                               ""system taking advantage of specific DBMS ""
                               ""design weaknesses."")

        takeover.add_option(""--os-shell"", dest=""osShell"", action=""store_true"",
                            help=""Prompt for an interactive OS shell ""
                                 ""(only on PHP/MySQL environment with a ""
                                 ""writable directory within the web ""
                                 ""server document root for the moment)"")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, ""Miscellaneous"")

        miscellaneous.add_option(""--eta"", dest=""eta"", action=""store_true"",
                                 help=""Retrieve each query output length and ""
                                      ""calculate the estimated time of arrival ""
                                      ""in real time"")

        miscellaneous.add_option(""--update"", dest=""updateAll"", action=""store_true"",
                                help=""Update sqlmap to the latest stable version"")

        miscellaneous.add_option(""-s"", dest=""sessionFile"",
                                 help=""Save and resume all data retrieved ""
                                      ""on a session file"")

        miscellaneous.add_option(""--save"", dest=""saveCmdline"", action=""store_true"",
                                 help=""Save options on a configuration INI file"")

        miscellaneous.add_option(""--batch"", dest=""batch"", action=""store_true"",
                                 help=""Never ask for user input, use the default behaviour"")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = ""missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), ""
            errMsg += ""-h for help""
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = ""parsing command line""
    logger.debug(debugMsg)
/n/n/nlib/request/comparison.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 3)

    elif round(conf.seqMatcher.ratio(), 3) >= MATCH_RATIO:
        return True

    else:
        return False
/n/n/nlib/techniques/inband/union/test.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """"""
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """"""

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == ""Oracle"" and query.endswith("" FROM DUAL""):
            query = query[:-len("" FROM DUAL"")]

        if count:
            query += "", NULL""

        if kb.dbms == ""Oracle"":
            query += "" FROM DUAL""

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload, getSeqMatcher=True)

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count > 3:
            for ratio, element in resultDict.items():
                if element[0] == 1 and ratio > 0.5:
                    if kb.injPlace == ""GET"":
                        value = ""%s?%s"" % (conf.url, element[1])
                    elif kb.injPlace == ""POST"":
                        value  = ""URL:\t'%s'"" % conf.url
                        value += ""\nPOST:\t'%s'\n"" % element[1]
                    elif kb.injPlace == ""Cookie"":
                        value  = ""URL:\t'%s'"" % conf.url
                        value += ""\nCookie:\t'%s'\n"" % element[1]
                    elif kb.injPlace == ""User-Agent"":
                        value  = ""URL:\t\t'%s'"" % conf.url
                        value += ""\nUser-Agent:\t'%s'\n"" % element[1]

                    return value

    return None


def unionTest():
    """"""
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """"""

    logMsg  = ""testing inband sql injection on parameter ""
    logMsg += ""'%s'"" % kb.injParameter
    logger.info(logMsg)

    value = """"

    query = agent.prefixQuery("" UNION ALL SELECT NULL"")

    for comment in (queries[kb.dbms].comment, """"):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count(""NULL""))

            break

    if kb.unionCount:
        logMsg  = ""the target url could be affected by an ""
        logMsg += ""inband sql injection vulnerability""
        logger.info(logMsg)
    else:
        warnMsg  = ""the target url is not affected by an ""
        warnMsg += ""inband sql injection vulnerability""
        logger.warn(warnMsg)

    return value
/n/n/n",0
135,135,35708a0b975f89357e75f8f74999900da9fe9894,"/lib/controller/checks.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """"""
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """"""

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = """"
        postfix = """"

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = ""testing custom injection ""
        infoMsg += ""on %s parameter '%s'"" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%d=%d %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%d=%d %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""confirming custom injection ""
                infoMsg += ""on %s parameter '%s'"" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, ""%s%s%s AND %s%s %s"" % (value, prefix, "")"" * parenthesis, ""("" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                    infoMsg += ""custom injectable ""
                    logger.info(infoMsg)

                    return ""custom""

    infoMsg  = ""testing unescaped numeric injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s%s AND %s%d=%d"" % (value, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s%s AND %s%d=%d"" % (value, "")"" * parenthesis, ""("" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming unescaped numeric injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s%s AND %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""unescaped numeric injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""numeric""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""unescaped numeric injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing single quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s'='%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s'='%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming single quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s'%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""single quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""stringsingle""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""single quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing LIKE single quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s' LIKE '%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s'%s AND %s'%s' LIKE '%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming LIKE single quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s'%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""LIKE single quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""likesingle""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""LIKE single quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing double quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\""=\""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\""=\""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming double quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s\""%s AND %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""double quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""stringdouble""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""double quoted string injectable""
    logger.info(infoMsg)

    infoMsg  = ""testing LIKE double quoted string injection ""
    infoMsg += ""on %s parameter '%s'"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\"" LIKE \""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, ""%s\""%s AND %s\""%s\"" LIKE \""%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = ""confirming LIKE double quoted string injection ""
            infoMsg += ""on %s parameter '%s'"" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, ""%s\""%s and %s%s"" % (value, "")"" * parenthesis, ""("" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = ""%s parameter '%s' is "" % (place, parameter)
                infoMsg += ""LIKE double quoted string injectable ""
                infoMsg += ""with %d parenthesis"" % parenthesis
                logger.info(infoMsg)

                return ""likedouble""

    infoMsg  = ""%s parameter '%s' is not "" % (place, parameter)
    infoMsg += ""LIKE double quoted string injectable""
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """"""
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """"""

    infoMsg = ""testing if %s parameter '%s' is dynamic"" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = ""confirming that %s parameter '%s' is dynamic"" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, ""'%s"" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, ""\""%s"" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """"""
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """"""

    infoMsg = ""testing if the url is stable, wait a few seconds""
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)

    thirdPage, thirdHeaders = Request.queryPage(content=True)

    condition  = firstPage == secondPage
    condition &= secondPage == thirdPage

    if condition == False:
        warnMsg  = ""url is not stable, sqlmap will base the page ""
        warnMsg += ""comparison on a sequence matcher, if no dynamic nor ""
        warnMsg += ""injectable parameters are detected, refer to user's ""
        warnMsg += ""manual paragraph 'Page comparison' and provide a ""
        warnMsg += ""string or regular expression to match on""
        logger.warn(warnMsg)

    if condition == True:
        logMsg = ""url is stable""
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key(""String"") and
                  kb.resumedQueries[conf.url][""String""][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = ""testing if the provided string is within the ""
    infoMsg += ""target URL page content""
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = ""you provided '%s' as the string to "" % conf.string
        errMsg += ""match, but such a string is not within the target ""
        errMsg += ""URL page content, please provide another string.""
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key(""Regular expression"") and
                  kb.resumedQueries[conf.url][""Regular expression""][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = ""testing if the provided regular expression matches within ""
    infoMsg += ""the target URL page content""
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = ""you provided '%s' as the regular expression to "" % conf.regexp
        errMsg += ""match, but such a regular expression does not have any ""
        errMsg += ""match within the target URL page content, please provide ""
        errMsg += ""another regular expression.""
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = ""testing connection to the target url""
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += "", skipping to next url""
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True
/n/n/n/lib/parse/cmdline.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """"""
    This function parses the command line parameters and arguments
    """"""

    usage = ""%s [options]"" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option(""-v"", dest=""verbose"", type=""int"",
                          help=""Verbosity level: 0-5 (default 1)"")

        # Target options
        target = OptionGroup(parser, ""Target"", ""At least one of these ""
                             ""options has to be specified to set the source ""
                             ""to get target urls from."")

        target.add_option(""-u"", ""--url"", dest=""url"", help=""Target url"")

        target.add_option(""-l"", dest=""list"", help=""Parse targets from Burp ""
                          ""or WebScarab logs"")

        target.add_option(""-g"", dest=""googleDork"",
                          help=""Process Google dork results as target urls"")

        target.add_option(""-c"", dest=""configFile"",
                          help=""Load options from a configuration INI file"")


        # Request options
        request = OptionGroup(parser, ""Request"", ""These options can be used ""
                              ""to specify how to connect to the target url."")

        request.add_option(""--method"", dest=""method"", default=""GET"",
                           help=""HTTP method, GET or POST (default: GET)"")

        request.add_option(""--data"", dest=""data"",
                           help=""Data string to be sent through POST"")

        request.add_option(""--cookie"", dest=""cookie"",
                           help=""HTTP Cookie header"")

        request.add_option(""--referer"", dest=""referer"",
                           help=""HTTP Referer header"")

        request.add_option(""--user-agent"", dest=""agent"",
                           help=""HTTP User-Agent header"")

        request.add_option(""-a"", dest=""userAgentsFile"",
                           help=""Load a random HTTP User-Agent ""
                                ""header from file"")

        request.add_option(""--headers"", dest=""headers"",
                           help=""Extra HTTP headers '\\n' separated"")

        request.add_option(""--auth-type"", dest=""aType"",
                           help=""HTTP Authentication type, value: ""
                                ""Basic or Digest"")

        request.add_option(""--auth-cred"", dest=""aCred"",
                           help=""HTTP Authentication credentials, value: ""
                                ""name:password"")

        request.add_option(""--proxy"", dest=""proxy"",
                           help=""Use a HTTP proxy to connect to the target url"")

        request.add_option(""--threads"", dest=""threads"", type=""int"",
                           help=""Maximum number of concurrent HTTP ""
                                ""requests (default 1)"")

        request.add_option(""--delay"", dest=""delay"", type=""float"",
                           help=""Delay in seconds between each HTTP request"")

        request.add_option(""--timeout"", dest=""timeout"", type=""float"",
                           help=""Seconds to wait before timeout connection ""
                                ""(default 30)"")


        # Injection options
        injection = OptionGroup(parser, ""Injection"", ""These options can be ""
                                ""used to specify which parameters to test ""
                                ""for, provide custom injection payloads and ""
                                ""how to parse and compare HTTP responses ""
                                ""page content when using the blind SQL ""
                                ""injection technique."")

        injection.add_option(""-p"", dest=""testParameter"",
                             help=""Testable parameter(s)"")

        injection.add_option(""--dbms"", dest=""dbms"",
                             help=""Force back-end DBMS to this value"")

        injection.add_option(""--prefix"", dest=""prefix"",
                             help=""Injection payload prefix string"")

        injection.add_option(""--postfix"", dest=""postfix"",
                             help=""Injection payload postfix string"")

        injection.add_option(""--string"", dest=""string"",
                             help=""String to match in page when the ""
                                  ""query is valid"")

        injection.add_option(""--regexp"", dest=""regexp"",
                             help=""Regexp to match in page when the ""
                                  ""query is valid"")

        injection.add_option(""--excl-str"", dest=""eString"",
                             help=""String to be excluded before calculating ""
                                  ""page hash"")

        injection.add_option(""--excl-reg"", dest=""eRegexp"",
                             help=""Regexp matches to be excluded before ""
                                  ""calculating page hash"")


        # Techniques options
        techniques = OptionGroup(parser, ""Techniques"", ""These options can ""
                                 ""be used to test for specific SQL injection ""
                                 ""technique or to use one of them to exploit ""
                                 ""the affected parameter(s) rather than using ""
                                 ""the default blind SQL injection technique."")

        techniques.add_option(""--stacked-test"", dest=""stackedTest"",
                              action=""store_true"",
                              help=""Test for stacked queries (multiple ""
                                   ""statements) support"")

        techniques.add_option(""--time-test"", dest=""timeTest"",
                              action=""store_true"",
                              help=""Test for Time based blind SQL injection"")

        techniques.add_option(""--union-test"", dest=""unionTest"",
                              action=""store_true"",
                              help=""Test for UNION query (inband) SQL injection"")

        techniques.add_option(""--union-use"", dest=""unionUse"",
                              action=""store_true"",
                              help=""Use the UNION query (inband) SQL injection ""
                                   ""to retrieve the queries output. No ""
                                   ""need to go blind"")


        # Fingerprint options
        fingerprint = OptionGroup(parser, ""Fingerprint"")

        fingerprint.add_option(""-f"", ""--fingerprint"", dest=""extensiveFp"",
                               action=""store_true"",
                               help=""Perform an extensive DBMS version fingerprint"")


        # Enumeration options
        enumeration = OptionGroup(parser, ""Enumeration"", ""These options can ""
                                  ""be used to enumerate the back-end database ""
                                  ""management system information, structure ""
                                  ""and data contained in the tables. Moreover ""
                                  ""you can run your own SQL SELECT queries."")

        enumeration.add_option(""-b"", ""--banner"", dest=""getBanner"",
                               action=""store_true"", help=""Retrieve DBMS banner"")

        enumeration.add_option(""--current-user"", dest=""getCurrentUser"",
                               action=""store_true"",
                               help=""Retrieve DBMS current user"")

        enumeration.add_option(""--current-db"", dest=""getCurrentDb"",
                               action=""store_true"",
                               help=""Retrieve DBMS current database"")

        enumeration.add_option(""--is-dba"", dest=""isDba"",
                               action=""store_true"",
                               help=""Detect if the DBMS current user is DBA"")

        enumeration.add_option(""--users"", dest=""getUsers"", action=""store_true"",
                               help=""Enumerate DBMS users"")

        enumeration.add_option(""--passwords"", dest=""getPasswordHashes"",
                               action=""store_true"",
                               help=""Enumerate DBMS users password hashes (opt: -U)"")

        enumeration.add_option(""--privileges"", dest=""getPrivileges"",
                               action=""store_true"",
                               help=""Enumerate DBMS users privileges (opt: -U)"")

        enumeration.add_option(""--dbs"", dest=""getDbs"", action=""store_true"",
                               help=""Enumerate DBMS databases"")

        enumeration.add_option(""--tables"", dest=""getTables"", action=""store_true"",
                               help=""Enumerate DBMS database tables (opt: -D)"")

        enumeration.add_option(""--columns"", dest=""getColumns"", action=""store_true"",
                               help=""Enumerate DBMS database table columns ""
                                    ""(req:-T opt:-D)"")

        enumeration.add_option(""--dump"", dest=""dumpTable"", action=""store_true"",
                               help=""Dump DBMS database table entries ""
                                    ""(req: -T, opt: -D, -C, --start, --stop)"")

        enumeration.add_option(""--dump-all"", dest=""dumpAll"", action=""store_true"",
                               help=""Dump all DBMS databases tables entries"")

        enumeration.add_option(""-D"", dest=""db"",
                               help=""DBMS database to enumerate"")

        enumeration.add_option(""-T"", dest=""tbl"",
                               help=""DBMS database table to enumerate"")

        enumeration.add_option(""-C"", dest=""col"",
                               help=""DBMS database table column to enumerate"")

        enumeration.add_option(""-U"", dest=""user"",
                               help=""DBMS user to enumerate"")

        enumeration.add_option(""--exclude-sysdbs"", dest=""excludeSysDbs"",
                               action=""store_true"",
                               help=""Exclude DBMS system databases when ""
                                    ""enumerating tables"")

        enumeration.add_option(""--start"", dest=""limitStart"", type=""int"",
                               help=""First table entry to dump"")

        enumeration.add_option(""--stop"", dest=""limitStop"", type=""int"",
                               help=""Last table entry to dump"")

        enumeration.add_option(""--sql-query"", dest=""query"",
                               help=""SQL SELECT query to be executed"")

        enumeration.add_option(""--sql-shell"", dest=""sqlShell"",
                               action=""store_true"",
                               help=""Prompt for an interactive SQL shell"")


        # File system options
        filesystem = OptionGroup(parser, ""File system access"", ""These options ""
                                 ""can be used to access the back-end database ""
                                 ""management system file system taking ""
                                 ""advantage of native DBMS functions or ""
                                 ""specific DBMS design weaknesses."")

        filesystem.add_option(""--read-file"", dest=""rFile"",
                              help=""Read a specific OS file content (only on MySQL)"")

        filesystem.add_option(""--write-file"", dest=""wFile"",
                              help=""Write to a specific OS file (not yet available)"")


        # Takeover options
        takeover = OptionGroup(parser, ""Operating system access"", ""This ""
                               ""option can be used to access the back-end ""
                               ""database management system operating ""
                               ""system taking advantage of specific DBMS ""
                               ""design weaknesses."")

        takeover.add_option(""--os-shell"", dest=""osShell"", action=""store_true"",
                            help=""Prompt for an interactive OS shell ""
                                 ""(only on PHP/MySQL environment with a ""
                                 ""writable directory within the web ""
                                 ""server document root for the moment)"")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, ""Miscellaneous"")

        miscellaneous.add_option(""--eta"", dest=""eta"", action=""store_true"",
                                 help=""Retrieve each query output length and ""
                                      ""calculate the estimated time of arrival ""
                                      ""in real time"")

        miscellaneous.add_option(""--update"", dest=""updateAll"", action=""store_true"",
                                help=""Update sqlmap to the latest stable version"")

        miscellaneous.add_option(""-s"", dest=""sessionFile"",
                                 help=""Save and resume all data retrieved ""
                                      ""on a session file"")

        miscellaneous.add_option(""--save"", dest=""saveCmdline"", action=""store_true"",
                                 help=""Save options on a configuration INI file"")

        miscellaneous.add_option(""--batch"", dest=""batch"", action=""store_true"",
                                 help=""Never ask for user input, use the default behaviour"")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = ""missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), ""
            errMsg += ""-h for help""
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = ""parsing command line""
    logger.debug(debugMsg)
/n/n/n/lib/request/comparison.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:
        return True

    else:
        return False
/n/n/n/lib/techniques/inband/union/test.py/n/n#!/usr/bin/env python

""""""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
""""""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """"""
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """"""

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == ""Oracle"" and query.endswith("" FROM DUAL""):
            query = query[:-len("" FROM DUAL"")]

        if count:
            query += "", NULL""

        if kb.dbms == ""Oracle"":
            query += "" FROM DUAL""

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:
            for element in resultDict.values():
                if element[0] == 1:
                    if kb.injPlace == ""GET"":
                        value = ""%s?%s"" % (conf.url, payload)
                    elif kb.injPlace == ""POST"":
                        value  = ""URL:\t'%s'"" % conf.url
                        value += ""\nPOST:\t'%s'\n"" % payload
                    elif kb.injPlace == ""Cookie"":
                        value  = ""URL:\t'%s'"" % conf.url
                        value += ""\nCookie:\t'%s'\n"" % payload
                    elif kb.injPlace == ""User-Agent"":
                        value  = ""URL:\t\t'%s'"" % conf.url
                        value += ""\nUser-Agent:\t'%s'\n"" % payload

                    return value

    return None


def unionTest():
    """"""
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """"""

    logMsg  = ""testing inband sql injection on parameter ""
    logMsg += ""'%s'"" % kb.injParameter
    logger.info(logMsg)

    value = """"

    query = agent.prefixQuery("" UNION ALL SELECT NULL"")

    for comment in (queries[kb.dbms].comment, """"):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count(""NULL""))

            break

    if kb.unionCount:
        logMsg  = ""the target url could be affected by an ""
        logMsg += ""inband sql injection vulnerability""
        logger.info(logMsg)
    else:
        warnMsg  = ""the target url is not affected by an ""
        warnMsg += ""inband sql injection vulnerability""
        logger.warn(warnMsg)

    return value
/n/n/n",1
18,18,e332ec93e9c90f1cbee676b022bf2c5d5b7b1239,"utils/autoban.py/n/n#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import sys
import socket
import argparse
import subprocess


def inet_pton(str_ip):
    try:
        return socket.inet_pton(socket.AF_INET, str_ip)
    except socket.error:
        return None

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' not in line:
            continue
        ip_str = line.split()[-1].rsplit(':', 1)[0]
        ip = inet_pton(ip_str)
        if ip is None:
            continue
        if ip not in ips:
            ips[ip] = 1
            sys.stdout.flush()
        else:
            ips[ip] += 1
        if ip not in banned and ips[ip] >= config.count:
            banned.add(ip)
            print('ban ip %s' % ip_str)
            cmd = ['iptables', '-A', 'INPUT', '-s', ip_str, '-j', 'DROP',
                   '-m', 'comment', '--comment', 'autoban']
            print(' '.join(cmd), file=sys.stderr)
            sys.stderr.flush()
            subprocess.call(cmd)
/n/n/n",0
19,19,e332ec93e9c90f1cbee676b022bf2c5d5b7b1239,"/utils/autoban.py/n/n#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:
            ip = line.split()[-1].split(':')[-2]
            if ip not in ips:
                ips[ip] = 1
                print(ip)
                sys.stdout.flush()
            else:
                ips[ip] += 1
            if ip not in banned and ips[ip] >= config.count:
                banned.add(ip)
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip
                print(cmd, file=sys.stderr)
                sys.stderr.flush()
                os.system(cmd)
/n/n/n",1
20,20,41aa6af4a8e18e3607f5cae1add446fa52350ec8,"common/src/stack/command/stack/commands/add/storage/controller/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#
# @rocks@

import stack.commands
from stack.exception import CommandError, ParamRequired, ParamType, ParamValue, ParamError


class Command(stack.commands.OSArgumentProcessor, stack.commands.HostArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	""""""
	Add a storage controller configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='int' name='adapter' optional='1'>
	Adapter address.
	</param>

	<param type='int' name='enclosure' optional='1'>
	Enclosure address.
	</param>

	<param type='int' name='slot'>
	Slot address(es). This can be a comma-separated list meaning all disks
	in the specified slots will be associated with the same array
	</param>

	<param type='int' name='raidlevel'>
	RAID level. Raid 0, 1, 5, 6 and 10 are currently supported.
	</param>

	<param type='int' name='hotspare' optional='1'>
	Slot address(es) of the hotspares associated with this array id. This
	can be a comma-separated list (like the 'slot' parameter). If the
	'arrayid' is 'global', then the specified slots are global hotspares.
	</param>

	<param type='string' name='arrayid'>
	The 'arrayid' is used to determine which disks are grouped as part
	of the same array. For example, all the disks with arrayid of '1' will
	be part of the same array. Arrayids must be integers starting at 1
	or greater. If the arrayid is 'global', then 'hotspare' must
	have at least one slot definition (this is how one specifies a global
	hotspare).
	In addition, the arrays will be created in arrayid order, that is,
	the array with arrayid equal to 1 will be created first, arrayid
	equal to 2 will be created second, etc.
	</param>

	<example cmd='add storage controller backend-0-0 slot=1 raidlevel=0 arrayid=1'>
	The disk in slot 1 on backend-0-0 should be a RAID 0 disk.
	</example>

	<example cmd='add storage controller backend-0-0 slot=2,3,4,5,6 raidlevel=6 hotspare=7,8 arrayid=2'>
	The disks in slots 2-6 on backend-0-0 should be a RAID 6 with two
	hotspares associated with the array in slots 7 and 8.
	</example>
	""""""

	def checkIt(self, name, scope, tableid, adapter, enclosure, slot):
		self.db.execute(""""""select scope, tableid, adapter, enclosure,
			slot from storage_controller where
			scope = '%s' and tableid = %s and adapter = %s and
			enclosure = %s and slot = %s"""""" % (scope, tableid,
			adapter, enclosure, slot))

		row = self.db.fetchone()

		if row:
			label = [ 'scope', 'name' ]
			value = [ scope, name ]

			if adapter > -1:
				label.append('adapter')
				value.append('%s' % adapter)
			if enclosure > -1:
				label.append('enclosure')
				value.append('%s' % enclosure)

			label.append('slot')
			value.append('%s' % slot)

			raise CommandError(self, 'disk specification %s %s already exists in the database' % ('/'.join(label), '/'.join(value)))


	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:
				hosts = []
		else:
			raise CommandError(self, 'must supply zero or one argument')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'
		if not scope:
			raise CommandError(self, 'argument ""%s"" must be a valid os, appliance name or host name' % args[0])

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		adapter, enclosure, slot, hotspare, raidlevel, arrayid, options, force = self.fillParams([
			('adapter', None),
			('enclosure', None),
			('slot', None),
			('hotspare', None),
			('raidlevel', None),
			('arrayid', None, True),
			('options', ''),
			('force', 'n')
			])

		if not hotspare and not slot:
			raise ParamRequired(self, [ 'slot', 'hotspare' ])
		if arrayid != 'global' and not raidlevel:
			raise ParamRequired(self, 'raidlevel')

		if adapter:
			try:
				adapter = int(adapter)
			except:
				raise ParamType(self, 'adapter', 'integer')
			if adapter < 0:
				raise ParamValue(self, 'adapter', '>= 0')
		else:
			adapter = -1

		if enclosure:
			try:
				enclosure = int(enclosure)
			except:
				raise ParamType(self, 'enclosure', 'integer')
			if enclosure < 0:
				raise ParamValue(self, 'enclosure', '>= 0')
		else:
			enclosure = -1

		slots = []
		if slot:
			for s in slot.split(','):
				if s == '*':
					#
					# represent '*' in the database as '-1'
					#
					s = -1
				else:
					try:
						s = int(s)
					except:
						raise ParamType(self, 'slot', 'integer')
					if s < 0:
						raise ParamValue(self, 'slot', '>= 0')
					if s in slots:
						raise ParamError(self, 'slot', ' ""%s"" is listed twice' % s)
				slots.append(s)

		hotspares = []
		if hotspare:
			for h in hotspare.split(','):
				try:
					h = int(h)
				except:	
					raise ParamType(self, 'hotspare', 'integer')
				if h < 0:
					raise ParamValue(self, 'hostspare', '>= 0')
				if h in hotspares:
					raise ParamError(self, 'hostspare', ' ""%s"" is listed twice' % h)
				hotspares.append(h)

		if arrayid in [ 'global', '*' ]:
			pass
		else:
			try:
				arrayid = int(arrayid)
			except:
				raise ParamType(self, 'arrayid', 'integer')
			if arrayid < 1:
				raise ParamValue(self, 'arrayid', '>= 0')

		if arrayid == 'global' and len(hotspares) == 0:
			raise ParamError(self, 'arrayid', 'is ""global"" with no hotspares. Please supply at least one hotspare')

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute(""""""select id from appliances where
				name = %s """""", name)
			tableid, = self.db.fetchone()

		elif scope == 'os':
			self.db.execute(""""""select id from oses where
				name = %s """""", name)
			tableid, = self.db.fetchone()

		elif scope == 'host':
			self.db.execute(""""""select id from nodes where
				name = %s """""", name)
			tableid, = self.db.fetchone()

		#
		# make sure the specification doesn't already exist
		#
		force = self.str2bool(force)
		for slot in slots:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					slot)
		for hotspare in hotspares:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					hotspare)

		if arrayid == 'global':
			arrayid = -1
		elif arrayid == '*':
			arrayid = -2

		#
		# now add the specifications to the database
		#
		for slot in slots:
			self.db.execute(""""""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values (%s, %s, %s, %s,
				%s, %s, %s, %s) """""",(scope, tableid, adapter,
				enclosure, slot, raidlevel, arrayid, options))

		for hotspare in hotspares:
			raidlevel = -1
			if arrayid == 'global':
				arrayid = -1

			self.db.execute(""""""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values (%s, %s, %s, %s,
				%s, %s, %s, %s) """""",(scope, tableid, adapter,
				enclosure, hotspare, raidlevel, arrayid, options))

/n/n/ncommon/src/stack/command/stack/commands/add/storage/partition/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import CommandError, ArgRequired, ArgValue, ParamRequired, ParamType, ParamValue


class Command(stack.commands.OSArgumentProcessor, stack.commands.HostArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	""""""
	Add a partition configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='string' name='device' optional='0'>
	Disk device on which we are creating partitions
	</param>

	<param type='string' name='mountpoint' optional='1'>
	Mountpoint to create
	</param>

	<param type='int' name='size' optional='0'>
	Size of the partition.
	</param>

	<param type='string' name='type' optional='1'>
	Type of partition E.g: ext4, ext3, xfs, raid, etc.
	</param>

	<param type='string' name='options' optional='0'>
	Options that need to be supplied while adding partitions.
	</param>

	<param type='int' name='partid' optional='1'>
	The relative partition id for this partition. Partitions will be
	created in ascending partition id order.
	</param>
	
	<example cmd='add storage partition backend-0-0 device=sda mountpoint=/var
		size=50 type=ext4'>
	Creates a ext4 partition on device sda with mountpoints /var.
	</example>

	<example cmd='add storage partition backend-0-2 device=sdc mountpoint=pv.01
		 size=0 type=lvm'>
	Creates a physical volume named pv.01 for lvm.
	</example>

	<example cmd='add storage partition backend-0-2 mountpoint=volgrp01 device=pv.01 pv.02 pv.03
		size=32768 type=volgroup'>
	Creates a volume group from 3 physical volumes i.e. pv.01, pv.02, pv.03. All these 3
	physical volumes need to be created with the previous example. PV's need to be space
	separated.
	</example>
	<example cmd='add storage partition backend-0-2 device=volgrp01 mountpoint=/banktools
		size=8192 type=xfs options=--name=banktools'>
	Created an xfs lvm partition of size 8192 on volgrp01. volgrp01 needs to be created
	with the previous example.
	</example>
	""""""

	#
	# Checks if partition config already exists in DB for a device and 
	# a mount point.
	#
	def checkIt(self, device, scope, tableid, mountpt):
		self.db.execute(""""""select Scope, TableID, Mountpoint,
			device, Size, FsType from storage_partition where
			Scope=%s and TableID=%s and device= %s
			and Mountpoint=%s"""""",(scope, tableid, device, mountpt))

		row = self.db.fetchone()

		if row:
			raise CommandError(self, """"""partition specification for device %s,
				mount point %s already exists in the 
				database"""""" % (device, mountpt))

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:
				hosts = []
		else:
			raise ArgRequired(self, 'scope')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ArgValue(self, 'scope', 'valid os, appliance name or host name')

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		device, size, fstype, mountpt, options, partid = \
			self.fillParams([
				('device', None, True),
				('size', None), 
				('type', None), 
				('mountpoint', None),
				('options', None),
				('partid', None),
				])

		if not device:
			raise ParamRequired(self, 'device')
		#if size is blank then the sql command will crash
		if not size:
			raise ParamRequired(self, 'size')

		# Validate size
		if size:
			try:
				s = int(size)
			except:
				#
				# If mountpoint is 'swap' then allow
				# 'hibernate', 'recommended' as sizes.
				#
				if mountpt == 'swap' and \
					size not in ['recommended', 'hibernation']:
						raise ParamType(self, 'size', 'integer')
				else:
					raise ParamType(self, 'size', 'integer')
			if s < 0:
				raise ParamValue(self, 'size', '>= 0')

		# Validate partid
		if partid:
			try:
				p = int(partid)
			except ValueError:
				raise ParamValue(self, 'partid', 'an integer')

			if p < 1:
				raise ParamValue(self, 'partid', '>= 0')

			partid = p

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute(""""""select id from appliances where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()


		elif scope == 'os':
			self.db.execute(""""""select id from oses where name = %s """""", name)
			tableid, = self.db.fetchone()


		elif scope == 'host':
			self.db.execute(""""""select id from nodes where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()

		#
		# make sure the specification for mountpt doesn't already exist
		#
		if mountpt:
			self.checkIt(device, scope, tableid, mountpt)

		if not options:
			options = """"
		
		#
		# now add the specifications to the database
		#


		sqlvars = ""Scope, TableID, device, Mountpoint, Size, FsType, Options""
		sqldata = ""'%s', %s, '%s', '%s', %s, '%s', '%s'"" % \
			(scope, tableid, device, mountpt, size, fstype, options)

		if partid:
			sqlvars += "", PartID""
			sqldata += "", %s"" % partid


		self.db.execute(""""""insert into storage_partition
			(%s) values (%s) """""" % (sqlvars, sqldata))

/n/n/ncommon/src/stack/command/stack/commands/list/storage/controller/__init__.py/n/n#
# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	""""""
	List the storage controller configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage controller
	configuration will be output.
	</arg>

	<example cmd='list storage controller backend-0-0'>
	List host-specific storage controller configuration for backend-0-0.
	</example>

	<example cmd='list storage controller backend'>
	List appliance-specific storage controller configuration for all
	backend appliances.
	</example>

	<example cmd='list storage controller'>
	List global storage controller configuration for all hosts.
	</example>

	""""""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')

		query = None
		if scope == 'global':
			query = """"""select adapter, enclosure, slot, raidlevel,
				arrayid, options from storage_controller 
				where scope = 'global'
				order by enclosure, adapter, slot""""""
		elif scope == 'os':

			query = """"""select adapter, enclosure, slot, raidlevel,
                                arrayid, options from storage_controller where
                                scope = ""os"" and tableid = (select id from oses
                                where name = '%s') order by enclosure, adapter, slot"""""" % args[0]
		elif scope == 'appliance':
			query = """"""select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = ""appliance"" and tableid = (select
				id from appliances
				where name = '%s')
				order by enclosure, adapter, slot"""""" % args[0]
		elif scope == 'host':
			query = """"""select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = ""host"" and tableid = (select
				id from nodes where name = '%s')
				order by enclosure, adapter, slot"""""" % args[0]

		if not query:
			return

		name = None
		if scope == 'global':
			name = 'global'
		elif scope in [ 'appliance', 'host', 'os']:
			name = args[0]

		self.beginOutput()

		self.db.execute(query)

		i = 0
		for row in self.db.fetchall():
			adapter, enclosure, slot, raidlevel, arrayid, options = row

			if i > 0:
				name = None
			if adapter == -1:
				adapter = None
			if enclosure == -1:
				enclosure = None
			if slot == -1:
				slot = '*'
			if raidlevel == '-1':
				raidlevel = 'hotspare'
			if arrayid == -1:
				arrayid = 'global'
			elif arrayid == -2:
				arrayid = '*'
			# Remove leading and trailing double quotes
			options = options.strip(""\"""")

			self.addOutput(name, [ enclosure, adapter, slot,
				raidlevel, arrayid, options ])

			i += 1

		self.endOutput(header=['scope', 'enclosure', 'adapter', 'slot', 
			'raidlevel', 'arrayid', 'options' ], trimOwner=False)

/n/n/ncommon/src/stack/command/stack/commands/list/storage/partition/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	""""""
	List the storage partition configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage partition
	configuration will be output.
	</arg>

	<param type=""bool"" name=""globalOnly"" optional=""0"" default=""n"">
	Flag that specifies if only the 'global' partition entries should
	be displayed.
	</param>

	<example cmd='list storage partition backend-0-0'>
	List host-specific storage partition configuration for backend-0-0.
	</example>

	<example cmd='list storage partition backend'>
	List appliance-specific storage partition configuration for all
	backend appliances.
	</example>

	<example cmd='list storage partition'>
	List all storage partition configurations in the database.
	</example>

	<example cmd='list storage partition globalOnly=y'>
	Lists only global storage partition configuration i.e. configuration
	not associated with a specific host or appliance type.
	</example>
	""""""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		globalOnly, = self.fillParams([('globalOnly', 'n')])
		globalOnlyFlag = self.str2bool(globalOnly)

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'
		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')
		query = None
		if scope == 'global':
			if globalOnlyFlag:
				query = """"""select scope, device, mountpoint, size, fstype, options, partid 
					from storage_partition
					where scope = 'global'
					order by device,partid,fstype, size""""""
			else:
				query = """"""(select scope, device, mountpoint, size, fstype, options, partid
					from storage_partition where scope = 'global') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					nodes as a on p.tableid=a.id where p.scope='host') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					appliances as a on p.tableid=a.id where
					p.scope='appliance') order by scope,device,partid,size,fstype""""""




		elif scope == 'os':
			query = """"""select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope = ""os"" and tableid = (select id
				from oses where name = '%s') order by device,partid,fstype,size"""""" % args[0]
		elif scope == 'appliance':
			query = """"""select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope = ""appliance""
				and tableid = (select id from appliances
				where name = '%s') order by device,partid,fstype, size"""""" % args[0]
		elif scope == 'host':
			query = """"""select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope=""host"" and
				tableid = (select id from nodes
				where name = '%s') order by device,partid,fstype, size"""""" % args[0]

		if not query:
			return

		self.beginOutput()

		self.db.execute(query)
		i = 0
		for row in self.db.fetchall():
			name, device, mountpoint, size, fstype, options, partid = row
			if size == -1:
				size = ""recommended""
			elif size == -2:
				size = ""hibernation""
			if name == ""host"" or name == ""appliance"" or name == ""os"":
				name = args[0]	

			if mountpoint == 'None':
				mountpoint = None

			if fstype == 'None':
				fstype = None

			if partid == 0:
				partid = None

			self.addOutput(name, [device, partid, mountpoint,
				size, fstype, options])

			i += 1

		self.endOutput(header=['scope', 'device', 'partid', 'mountpoint', 'size', 'fstype', 'options'], trimOwner=False)

/n/n/ntest-framework/test-suites/integration/tests/add/test_add_storage_controller.py/n/nimport pytest


class TestAddStorageController:

	""""""
	Test that we can successfully add an os level storage controller
	""""""

	def test_add_storage_controller(self, host):
		#this should work and have no errors
		results = host.run('stack add storage controller redhat adapter=1 arrayid=2 enclosure=3 raidlevel=4 slot=5')
		assert results.rc == 0

		#make sure that we are able to list the entry we just added
		results = host.run('stack list storage controller redhat')
		assert 'redhat' in str(results.stdout)

		#this should not work because 'blah' is not a valid scope
		results = host.run('stack add storage controller blah adapter=1 arrayid=2 enclosure=3 raidlevel=4 slot=5')
		assert results.rc != 0
/n/n/ntest-framework/test-suites/integration/tests/add/test_add_storage_partition.py/n/nimport pytest


class TestAddStorageController:

	""""""
	Test that we can successfully add an os level storage partition
	""""""

	def test_add_storage_partition(self, host):

		#this should work and have no errors
		results = host.run('stack add storage partition redhat device=test0 size=1 partid=1')
		assert results.rc == 0

		#make sure that we are able to list the entry we just added
		results = host.run('stack list storage partition redhat')
		assert 'redhat' in str(results.stdout)

		#this should not work because the size parameter is required
		results = host.run('stack add storage partition redhat  device=test0')
		assert results.rc != 0

		#this should not work because 'blah' is not a valid scope
		results = host.run('stack add storage partition blah  device=test0 size=1 partid=1')
		assert results.rc != 0
/n/n/n",0
21,21,41aa6af4a8e18e3607f5cae1add446fa52350ec8,"/common/src/stack/command/stack/commands/add/storage/controller/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#
# @rocks@

import stack.commands
from stack.exception import CommandError, ParamRequired, ParamType, ParamValue, ParamError


class Command(stack.commands.HostArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	""""""
	Add a storage controller configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='int' name='adapter' optional='1'>
	Adapter address.
	</param>

	<param type='int' name='enclosure' optional='1'>
	Enclosure address.
	</param>

	<param type='int' name='slot'>
	Slot address(es). This can be a comma-separated list meaning all disks
	in the specified slots will be associated with the same array
	</param>

	<param type='int' name='raidlevel'>
	RAID level. Raid 0, 1, 5, 6 and 10 are currently supported.
	</param>

	<param type='int' name='hotspare' optional='1'>
	Slot address(es) of the hotspares associated with this array id. This
	can be a comma-separated list (like the 'slot' parameter). If the
	'arrayid' is 'global', then the specified slots are global hotspares.
	</param>

	<param type='string' name='arrayid'>
	The 'arrayid' is used to determine which disks are grouped as part
	of the same array. For example, all the disks with arrayid of '1' will
	be part of the same array. Arrayids must be integers starting at 1
	or greater. If the arrayid is 'global', then 'hotspare' must
	have at least one slot definition (this is how one specifies a global
	hotspare).
	In addition, the arrays will be created in arrayid order, that is,
	the array with arrayid equal to 1 will be created first, arrayid
	equal to 2 will be created second, etc.
	</param>

	<example cmd='add storage controller backend-0-0 slot=1 raidlevel=0 arrayid=1'>
	The disk in slot 1 on backend-0-0 should be a RAID 0 disk.
	</example>

	<example cmd='add storage controller backend-0-0 slot=2,3,4,5,6 raidlevel=6 hotspare=7,8 arrayid=2'>
	The disks in slots 2-6 on backend-0-0 should be a RAID 6 with two
	hotspares associated with the array in slots 7 and 8.
	</example>
	""""""

	def checkIt(self, name, scope, tableid, adapter, enclosure, slot):
		self.db.execute(""""""select scope, tableid, adapter, enclosure,
			slot from storage_controller where
			scope = '%s' and tableid = %s and adapter = %s and
			enclosure = %s and slot = %s"""""" % (scope, tableid,
			adapter, enclosure, slot))

		row = self.db.fetchone()

		if row:
			label = [ 'scope', 'name' ]
			value = [ scope, name ]

			if adapter > -1:
				label.append('adapter')
				value.append('%s' % adapter)
			if enclosure > -1:
				label.append('enclosure')
				value.append('%s' % enclosure)

			label.append('slot')
			value.append('%s' % slot)

			raise CommandError(self, 'disk specification %s %s already exists in the database' % ('/'.join(label), '/'.join(value)))


	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:
				hosts = []
		else:
			raise CommandError(self, 'must supply zero or one argument')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise CommandError(self, 'argument ""%s"" must be a valid os, appliance name or host name' % args[0])

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		adapter, enclosure, slot, hotspare, raidlevel, arrayid, options, force = self.fillParams([
			('adapter', None),
			('enclosure', None),
			('slot', None),
			('hotspare', None),
			('raidlevel', None),
			('arrayid', None, True),
			('options', ''),
			('force', 'n')
			])

		if not hotspare and not slot:
			raise ParamRequired(self, [ 'slot', 'hotspare' ])
		if arrayid != 'global' and not raidlevel:
			raise ParamRequired(self, 'raidlevel')

		if adapter:
			try:
				adapter = int(adapter)
			except:
				raise ParamType(self, 'adapter', 'integer')
			if adapter < 0:
				raise ParamValue(self, 'adapter', '>= 0')
		else:
			adapter = -1

		if enclosure:
			try:
				enclosure = int(enclosure)
			except:
				raise ParamType(self, 'enclosure', 'integer')
			if enclosure < 0:
				raise ParamValue(self, 'enclosure', '>= 0')
		else:
			enclosure = -1

		slots = []
		if slot:
			for s in slot.split(','):
				if s == '*':
					#
					# represent '*' in the database as '-1'
					#
					s = -1
				else:
					try:
						s = int(s)
					except:
						raise ParamType(self, 'slot', 'integer')
					if s < 0:
						raise ParamValue(self, 'slot', '>= 0')
					if s in slots:
						raise ParamError(self, 'slot', ' ""%s"" is listed twice' % s)
				slots.append(s)

		hotspares = []
		if hotspare:
			for h in hotspare.split(','):
				try:
					h = int(h)
				except:	
					raise ParamType(self, 'hotspare', 'integer')
				if h < 0:
					raise ParamValue(self, 'hostspare', '>= 0')
				if h in hotspares:
					raise ParamError(self, 'hostspare', ' ""%s"" is listed twice' % h)
				hotspares.append(h)

		if arrayid in [ 'global', '*' ]:
			pass
		else:
			try:
				arrayid = int(arrayid)
			except:
				raise ParamType(self, 'arrayid', 'integer')
			if arrayid < 1:
				raise ParamValue(self, 'arrayid', '>= 0')

		if arrayid == 'global' and len(hotspares) == 0:
			raise ParamError(self, 'arrayid', 'is ""global"" with no hotspares. Please supply at least one hotspare')

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute(""""""select id from appliances where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()
		elif scope == 'host':
			self.db.execute(""""""select id from nodes where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()

		#
		# make sure the specification doesn't already exist
		#
		force = self.str2bool(force)
		for slot in slots:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					slot)
		for hotspare in hotspares:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					hotspare)

		if arrayid == 'global':
			arrayid = -1
		elif arrayid == '*':
			arrayid = -2

		#
		# now add the specifications to the database
		#
		for slot in slots:
			self.db.execute(""""""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values ('%s', %s, %s, %s,
				%s, %s, %s, '%s') """""" % (scope, tableid, adapter,
				enclosure, slot, raidlevel, arrayid, options))

		for hotspare in hotspares:
			raidlevel = -1
			if arrayid == 'global':
				arrayid = -1

			self.db.execute(""""""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values ('%s', %s, %s, %s,
				%s, %s, %s, '%s') """""" % (scope, tableid, adapter,
				enclosure, hotspare, raidlevel, arrayid, options))

/n/n/n/common/src/stack/command/stack/commands/add/storage/partition/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import CommandError, ArgRequired, ArgValue, ParamRequired, ParamType, ParamValue


class Command(stack.commands.HostArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	""""""
	Add a partition configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='string' name='device' optional='0'>
	Disk device on which we are creating partitions
	</param>

	<param type='string' name='mountpoint' optional='1'>
	Mountpoint to create
	</param>

	<param type='int' name='size' optional='1'>
	Size of the partition.
	</param>

	<param type='string' name='type' optional='1'>
	Type of partition E.g: ext4, ext3, xfs, raid, etc.
	</param>

	<param type='string' name='options' optional='0'>
	Options that need to be supplied while adding partitions.
	</param>

	<param type='string' name='partid' optional='1'>
	The relative partition id for this partition. Partitions will be
	created in ascending partition id order.
	</param>
	
	<example cmd='add storage partition backend-0-0 device=sda mountpoint=/var
		size=50 type=ext4'>
	Creates a ext4 partition on device sda with mountpoints /var.
	</example>

	<example cmd='add storage partition backend-0-2 device=sdc mountpoint=pv.01
		 size=0 type=lvm'>
	Creates a physical volume named pv.01 for lvm.
	</example>

	<example cmd='add storage partition backend-0-2 mountpoint=volgrp01 device=pv.01 pv.02 pv.03
		size=32768 type=volgroup'>
	Creates a volume group from 3 physical volumes i.e. pv.01, pv.02, pv.03. All these 3
	physical volumes need to be created with the previous example. PV's need to be space
	separated.
	</example>
	<example cmd='add storage partition backend-0-2 device=volgrp01 mountpoint=/banktools
		size=8192 type=xfs options=--name=banktools'>
	Created an xfs lvm partition of size 8192 on volgrp01. volgrp01 needs to be created
	with the previous example.
	</example>
	""""""

	#
	# Checks if partition config already exists in DB for a device and 
	# a mount point.
	#
	def checkIt(self, device, scope, tableid, mountpt):
		self.db.execute(""""""select Scope, TableID, Mountpoint,
			device, Size, FsType from storage_partition where
			Scope='%s' and TableID=%s and device= '%s'
			and Mountpoint='%s'"""""" % (scope, tableid, device, mountpt))

		row = self.db.fetchone()

		if row:
			raise CommandError(self, """"""partition specification for device %s,
				mount point %s already exists in the 
				database"""""" % (device, mountpt))

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:
				hosts = []
		else:
			raise ArgRequired(self, 'scope')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ArgValue(self, 'scope', 'valid os, appliance name or host name')

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		device, size, fstype, mountpt, options, partid = \
			self.fillParams([
				('device', None, True),
				('size', None), 
				('type', None), 
				('mountpoint', None),
				('options', None),
				('partid', None),
				])

		if not device:
			raise ParamRequired(self, 'device')

		# Validate size
		if size:
			try:
				s = int(size)
			except:
				#
				# If mountpoint is 'swap' then allow
				# 'hibernate', 'recommended' as sizes.
				#
				if mountpt == 'swap' and \
					size not in ['recommended', 'hibernation']:
						raise ParamType(self, 'size', 'integer')
			if s < 0:
				raise ParamValue(self, 'size', '>= 0')

		# Validate partid
		if partid:
			try:
				p = int(partid)
			except:
				partid = None

			if p < 1:
				raise ParamValue(self, 'partid', '>= 0')

			partid = p

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute(""""""select id from appliances where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()
		elif scope == 'host':
			self.db.execute(""""""select id from nodes where
				name = '%s' """""" % name)
			tableid, = self.db.fetchone()

		#
		# make sure the specification for mountpt doesn't already exist
		#
		if mountpt:
			self.checkIt(device, scope, tableid, mountpt)

		if not options:
			options = """"
		
		#
		# now add the specifications to the database
		#
		sqlvars = ""Scope, TableID, device, Mountpoint, Size, FsType, Options""
		sqldata = ""'%s', %s, '%s', '%s', %s, '%s', '%s'"" % \
			(scope, tableid, device, mountpt, size, fstype, options)

		if partid:
			sqlvars += "", PartID""
			sqldata += "", %s"" % partid

		self.db.execute(""""""insert into storage_partition
			(%s) values (%s) """""" % (sqlvars, sqldata))
/n/n/n/common/src/stack/command/stack/commands/list/storage/controller/__init__.py/n/n#
# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	""""""
	List the storage controller configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage controller
	configuration will be output.
	</arg>

	<example cmd='list storage controller backend-0-0'>
	List host-specific storage controller configuration for backend-0-0.
	</example>

	<example cmd='list storage controller backend'>
	List appliance-specific storage controller configuration for all
	backend appliances.
	</example>

	<example cmd='list storage controller'>
	List global storage controller configuration for all hosts.
	</example>

	""""""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')

		query = None
		if scope == 'global':
			query = """"""select adapter, enclosure, slot, raidlevel,
				arrayid, options from storage_controller 
				where scope = 'global'
				order by enclosure, adapter, slot""""""
		elif scope == 'os':
			#
			# not currently supported
			#
			return
		elif scope == 'appliance':
			query = """"""select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = ""appliance"" and tableid = (select
				id from appliances
				where name = '%s')
				order by enclosure, adapter, slot"""""" % args[0]
		elif scope == 'host':
			query = """"""select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = ""host"" and tableid = (select
				id from nodes where name = '%s')
				order by enclosure, adapter, slot"""""" % args[0]

		if not query:
			return

		name = None
		if scope == 'global':
			name = 'global'
		elif scope in [ 'appliance', 'host']:
			name = args[0]

		self.beginOutput()

		self.db.execute(query)

		i = 0
		for row in self.db.fetchall():
			adapter, enclosure, slot, raidlevel, arrayid, options = row

			if i > 0:
				name = None
			if adapter == -1:
				adapter = None
			if enclosure == -1:
				enclosure = None
			if slot == -1:
				slot = '*'
			if raidlevel == '-1':
				raidlevel = 'hotspare'
			if arrayid == -1:
				arrayid = 'global'
			elif arrayid == -2:
				arrayid = '*'
			# Remove leading and trailing double quotes
			options = options.strip(""\"""")

			self.addOutput(name, [ enclosure, adapter, slot,
				raidlevel, arrayid, options ])

			i += 1

		self.endOutput(header=['scope', 'enclosure', 'adapter', 'slot', 
			'raidlevel', 'arrayid', 'options' ], trimOwner=False)

/n/n/n/common/src/stack/command/stack/commands/list/storage/partition/__init__.py/n/n# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	""""""
	List the storage partition configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage partition
	configuration will be output.
	</arg>

	<param type=""bool"" name=""globalOnly"" optional=""0"" default=""n"">
	Flag that specifies if only the 'global' partition entries should
	be displayed.
	</param>

	<example cmd='list storage partition backend-0-0'>
	List host-specific storage partition configuration for backend-0-0.
	</example>

	<example cmd='list storage partition backend'>
	List appliance-specific storage partition configuration for all
	backend appliances.
	</example>

	<example cmd='list storage partition'>
	List all storage partition configurations in the database.
	</example>

	<example cmd='list storage partition globalOnly=y'>
	Lists only global storage partition configuration i.e. configuration
	not associated with a specific host or appliance type.
	</example>
	""""""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		globalOnly, = self.fillParams([('globalOnly', 'n')])
		globalOnlyFlag = self.str2bool(globalOnly)

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')
		query = None
		if scope == 'global':
			if globalOnlyFlag:
				query = """"""select scope, device, mountpoint, size, fstype, options, partid 
					from storage_partition
					where scope = 'global'
					order by device,partid,fstype, size""""""
			else:
				query = """"""(select scope, device, mountpoint, size, fstype, options, partid
					from storage_partition where scope = 'global') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					nodes as a on p.tableid=a.id where p.scope='host') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					appliances as a on p.tableid=a.id where
					p.scope='appliance') order by scope,device,partid,size,fstype""""""
		elif scope == 'os':
			#
			# not currently supported
			#
			return
		elif scope == 'appliance':
			query = """"""select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope = ""appliance""
				and tableid = (select id from appliances
				where name = '%s') order by device,partid,fstype, size"""""" % args[0]
		elif scope == 'host':
			query = """"""select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope=""host"" and
				tableid = (select id from nodes
				where name = '%s') order by device,partid,fstype, size"""""" % args[0]

		if not query:
			return

		self.beginOutput()

		self.db.execute(query)

		i = 0
		for row in self.db.fetchall():
			name, device, mountpoint, size, fstype, options, partid = row
			if size == -1:
				size = ""recommended""
			elif size == -2:
				size = ""hibernation""

			if name == ""host"" or name == ""appliance"":
				name = args[0]	

			if mountpoint == 'None':
				mountpoint = None

			if fstype == 'None':
				fstype = None

			if partid == 0:
				partid = None

			self.addOutput(name, [device, partid, mountpoint,
				size, fstype, options])

			i += 1

		self.endOutput(header=['scope', 'device', 'partid', 'mountpoint', 'size', 'fstype', 'options'], trimOwner=False)
/n/n/n",1
82,82,c55589b131828f3a595903f6796cb2d0babb772f,"cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Unit tests for OpenStack Cinder volume drivers
""""""
import ast
import mox
import shutil
import tempfile

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver
from cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver

LOG = logging.getLogger(__name__)

HP3PAR_DOMAIN = 'OpenStack',
HP3PAR_CPG = 'OpenStackCPG',
HP3PAR_CPG_SNAP = 'OpenStackCPGSnap'
CLI_CR = '\r\n'


class FakeHP3ParClient(object):

    api_url = None
    debug = False

    volumes = []
    hosts = []
    vluns = []
    cpgs = [
        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 8192},
         'SAUsage': {'rawTotalMiB': 24576,
                     'rawUsedMiB': 768,
                     'totalMiB': 8192,
                     'usedMiB': 256},
         'SDGrowth': {'LDLayout': {'RAIDType': 4,
                      'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 32768},
         'SDUsage': {'rawTotalMiB': 49152,
                     'rawUsedMiB': 1023,
                     'totalMiB': 36864,
                     'usedMiB': 768},
         'UsrUsage': {'rawTotalMiB': 57344,
                      'rawUsedMiB': 43349,
                      'totalMiB': 43008,
                      'usedMiB': 32512},
         'additionalStates': [],
         'degradedStates': [],
         'domain': HP3PAR_DOMAIN,
         'failedStates': [],
         'id': 5,
         'name': HP3PAR_CPG,
         'numFPVVs': 2,
         'numTPVVs': 0,
         'state': 1,
         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]

    def __init__(self, api_url):
        self.api_url = api_url
        self.volumes = []
        self.hosts = []
        self.vluns = []

    def debug_rest(self, flag):
        self.debug = flag

    def login(self, username, password, optional=None):
        return None

    def logout(self):
        return None

    def getVolumes(self):
        return self.volumes

    def getVolume(self, name):
        if self.volumes:
            for volume in self.volumes:
                if volume['name'] == name:
                    return volume

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VOLUME '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def createVolume(self, name, cpgName, sizeMiB, optional=None):
        new_vol = {'additionalStates': [],
                   'adminSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 384,
                                  'reservedMiB': 128,
                                  'usedMiB': 128},
                   'baseId': 115,
                   'comment': optional['comment'],
                   'copyType': 1,
                   'creationTime8601': '2012-10-22T16:37:57-07:00',
                   'creationTimeSec': 1350949077,
                   'degradedStates': [],
                   'domain': HP3PAR_DOMAIN,
                   'failedStates': [],
                   'id': 115,
                   'name': name,
                   'policies': {'caching': True,
                                'oneHost': False,
                                'staleSS': True,
                                'system': False,
                                'zeroDetect': False},
                   'provisioningType': 1,
                   'readOnly': False,
                   'sizeMiB': sizeMiB,
                   'snapCPG': optional['snapCPG'],
                   'snapshotSpace': {'freeMiB': 0,
                                     'rawReservedMiB': 683,
                                     'reservedMiB': 512,
                                     'usedMiB': 512},
                   'ssSpcAllocLimitPct': 0,
                   'ssSpcAllocWarningPct': 0,
                   'state': 1,
                   'userCPG': cpgName,
                   'userSpace': {'freeMiB': 0,
                                 'rawReservedMiB': 41984,
                                 'reservedMiB': 31488,
                                 'usedMiB': 31488},
                   'usrSpcAllocLimitPct': 0,
                   'usrSpcAllocWarningPct': 0,
                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',
                   'wwn': '50002AC00073383D'}
        self.volumes.append(new_vol)
        return None

    def deleteVolume(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createSnapshot(self, name, copyOfName, optional=None):
        new_snap = {'additionalStates': [],
                    'adminSpace': {'freeMiB': 0,
                                   'rawReservedMiB': 0,
                                   'reservedMiB': 0,
                                   'usedMiB': 0},
                    'baseId': 342,
                    'comment': optional['comment'],
                    'copyOf': copyOfName,
                    'copyType': 3,
                    'creationTime8601': '2012-11-09T15:13:28-08:00',
                    'creationTimeSec': 1352502808,
                    'degradedStates': [],
                    'domain': HP3PAR_DOMAIN,
                    'expirationTime8601': '2012-11-09T17:13:28-08:00',
                    'expirationTimeSec': 1352510008,
                    'failedStates': [],
                    'id': 343,
                    'name': name,
                    'parentId': 342,
                    'policies': {'caching': True,
                                 'oneHost': False,
                                 'staleSS': True,
                                 'system': False,
                                 'zeroDetect': False},
                    'provisioningType': 3,
                    'readOnly': True,
                    'retentionTime8601': '2012-11-09T16:13:27-08:00',
                    'retentionTimeSec': 1352506407,
                    'sizeMiB': 256,
                    'snapCPG': HP3PAR_CPG_SNAP,
                    'snapshotSpace': {'freeMiB': 0,
                                      'rawReservedMiB': 0,
                                      'reservedMiB': 0,
                                      'usedMiB': 0},
                    'ssSpcAllocLimitPct': 0,
                    'ssSpcAllocWarningPct': 0,
                    'state': 1,
                    'userCPG': HP3PAR_CPG,
                    'userSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 0,
                                  'reservedMiB': 0,
                                  'usedMiB': 0},
                    'usrSpcAllocLimitPct': 0,
                    'usrSpcAllocWarningPct': 0,
                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',
                    'wwn': '50002AC00157383D'}
        self.volumes.append(new_snap)
        return None

    def deleteSnapshot(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createCPG(self, name, optional=None):
        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 8192},
               'SAUsage': {'rawTotalMiB': 24576,
                           'rawUsedMiB': 768,
                           'totalMiB': 8192,
                           'usedMiB': 256},
               'SDGrowth': {'LDLayout': {'RAIDType': 4,
                            'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 32768},
               'SDUsage': {'rawTotalMiB': 49152,
                           'rawUsedMiB': 1023,
                           'totalMiB': 36864,
                           'usedMiB': 768},
               'UsrUsage': {'rawTotalMiB': 57344,
                            'rawUsedMiB': 43349,
                            'totalMiB': 43008,
                            'usedMiB': 32512},
               'additionalStates': [],
               'degradedStates': [],
               'domain': HP3PAR_DOMAIN,
               'failedStates': [],
               'id': 1,
               'name': name,
               'numFPVVs': 2,
               'numTPVVs': 0,
               'state': 1,
               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}

        new_cpg = cpg.copy()
        new_cpg.update(optional)
        self.cpgs.append(new_cpg)

    def getCPGs(self):
        return self.cpgs

    def getCPG(self, name):
        if self.cpgs:
            for cpg in self.cpgs:
                if cpg['name'] == name:
                    return cpg

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""CPG '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def deleteCPG(self, name):
        cpg = self.getCPG(name)
        self.cpgs.remove(cpg)

    def createVLUN(self, volumeName, lun, hostname=None,
                   portPos=None, noVcn=None,
                   overrideLowerPriority=None):

        vlun = {'active': False,
                'failedPathInterval': 0,
                'failedPathPol': 1,
                'hostname': hostname,
                'lun': lun,
                'multipathing': 1,
                'portPos': portPos,
                'type': 4,
                'volumeName': volumeName,
                'volumeWWN': '50002AC00077383D'}
        self.vluns.append(vlun)
        return None

    def deleteVLUN(self, name, lunID, hostname=None, port=None):
        vlun = self.getVLUN(name)
        self.vluns.remove(vlun)

    def getVLUNs(self):
        return self.vluns

    def getVLUN(self, volumeName):
        for vlun in self.vluns:
            if vlun['volumeName'] == volumeName:
                return vlun

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VLUN '%s' was not found"" % volumeName}
        raise hpexceptions.HTTPNotFound(msg)


class HP3PARBaseDriver():

    VOLUME_ID = ""d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    CLONE_ID = ""d03338a9-9115-48a3-8dfc-000000000000""
    VOLUME_NAME = ""volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    SNAPSHOT_ID = ""2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    SNAPSHOT_NAME = ""snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    VOLUME_3PAR_NAME = ""osv-0DM4qZEVSKON-DXN-NwVpw""
    SNAPSHOT_3PAR_NAME = ""oss-L4I73ONuTci9Fd4ceij-MQ""
    FAKE_HOST = ""fakehost""
    USER_ID = '2689d9a913974c008b1d859013f23607'
    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'
    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'
    FAKE_DESC = 'test description name'
    FAKE_FC_PORTS = ['0987654321234', '123456789000987']
    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}
    VVS_NAME = ""myvvs""
    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',
                                    'iqn': ('iqn.2000-05.com.3pardata:'
                                            '21810002ac00383d'),
                                    'ip_port': '3262'}}

    volume = {'name': VOLUME_NAME,
              'id': VOLUME_ID,
              'display_name': 'Foo Volume',
              'size': 2,
              'host': FAKE_HOST,
              'volume_type': None,
              'volume_type_id': None}

    volume_qos = {'name': VOLUME_NAME,
                  'id': VOLUME_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': FAKE_HOST,
                  'volume_type': None,
                  'volume_type_id': 'gold'}

    snapshot = {'name': SNAPSHOT_NAME,
                'id': SNAPSHOT_ID,
                'user_id': USER_ID,
                'project_id': PROJECT_ID,
                'volume_id': VOLUME_ID_SNAP,
                'volume_name': VOLUME_NAME,
                'status': 'creating',
                'progress': '0%',
                'volume_size': 2,
                'display_name': 'fakesnap',
                'display_description': FAKE_DESC}

    connector = {'ip': '10.0.0.2',
                 'initiator': 'iqn.1993-08.org.debian:01:222',
                 'wwpns': [""123456789012345"", ""123456789054321""],
                 'wwnns': [""223456789012345"", ""223456789054321""],
                 'host': 'fakehost'}

    volume_type = {'name': 'gold',
                   'deleted': False,
                   'updated_at': None,
                   'extra_specs': {'qos:maxBWS': '50',
                                   'qos:maxIOPS': '1000'},
                   'deleted_at': None,
                   'id': 'gold'}

    def setup_configuration(self):
        configuration = mox.MockObject(conf.Configuration)
        configuration.hp3par_debug = False
        configuration.hp3par_username = 'testUser'
        configuration.hp3par_password = 'testPassword'
        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'
        configuration.hp3par_domain = HP3PAR_DOMAIN
        configuration.hp3par_cpg = HP3PAR_CPG
        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP
        configuration.iscsi_ip_address = '1.1.1.2'
        configuration.iscsi_port = '1234'
        configuration.san_ip = '2.2.2.2'
        configuration.san_login = 'test'
        configuration.san_password = 'test'
        configuration.hp3par_snapshot_expiration = """"
        configuration.hp3par_snapshot_retention = """"
        configuration.hp3par_iscsi_ips = []
        return configuration

    def setup_fakes(self):
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_3par_host"",
                       self.fake_get_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_delete_3par_host"",
                       self.fake_delete_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_3par_vlun"",
                       self.fake_create_3par_vlun)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                       self.fake_get_ports)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)

    def clear_mox(self):
        self.mox.ResetAll()
        self.stubs.UnsetAll()

    def fake_create_client(self):
        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)

    def fake_get_cpg(self, volume, allowSnap=False):
        return HP3PAR_CPG

    def fake_set_connections(self):
        return

    def fake_get_domain(self, cpg):
        return HP3PAR_DOMAIN

    def fake_extend_volume(self, volume, new_size):
        vol = self.driver.common.client.getVolume(volume['name'])
        old_size = vol['sizeMiB']
        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}
        self.driver.common.client.deleteVolume(volume['name'])
        self.driver.common.client.createVolume(vol['name'],
                                               vol['userCPG'],
                                               new_size, option)

    def fake_get_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            return self._hosts[hostname]

    def fake_delete_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            del self._hosts[hostname]

    def fake_create_3par_vlun(self, volume, hostname):
        self.driver.common.client.createVLUN(volume, 19, hostname)

    def fake_get_ports(self):
        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}

    def fake_get_volume_type(self, type_id):
        return self.volume_type

    def fake_get_qos_by_volume_type(self, volume_type):
        return self.QOS

    def fake_add_volume_to_volume_set(self, volume, volume_name,
                                      cpg, vvs_name, qos):
        return volume

    def fake_copy_volume(self, src_name, dest_name, cpg=None,
                         snap_cpg=None, tpvv=True):
        pass

    def fake_get_volume_stats(self, vol_name):
        return ""normal""

    def fake_get_volume_settings_from_type(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': self.VVS_NAME,
                'qos': self.QOS,
                'tpvv': True,
                'volume_type': self.volume_type}

    def fake_get_volume_settings_from_type_noqos(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': None,
                'qos': None,
                'tpvv': True,
                'volume_type': None}

    def test_create_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type_noqos)
        self.driver.create_volume(self.volume)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)

    def test_create_volume_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume(self.volume_qos)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)

        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))

    def test_delete_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.driver.delete_volume(self.volume)
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.VOLUME_ID)

    def test_create_cloned_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_copy_volume"",
                       self.fake_copy_volume)
        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,
                  'id': HP3PARBaseDriver.CLONE_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': HP3PARBaseDriver.FAKE_HOST,
                  'source_volid': HP3PARBaseDriver.VOLUME_ID}
        src_vref = {}
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertTrue(model_update is not None)

    def test_create_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_snapshot(self.snapshot)

        # check to see if the snapshot was created
        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)

    def test_delete_snapshot(self):
        self.flags(lock_path=self.tempdir)

        self.driver.create_snapshot(self.snapshot)
        #make sure it exists first
        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)
        self.driver.delete_snapshot(self.snapshot)

        # the snapshot should be deleted now
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.SNAPSHOT_3PAR_NAME)

    def test_create_volume_from_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)

        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_create_volume_from_snapshot_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_volume_type"",
                       self.fake_get_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_get_qos_by_volume_type"",
                       self.fake_get_qos_by_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)
        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_terminate_connection(self):
        self.flags(lock_path=self.tempdir)
        #setup the connections
        self.driver.initialize_connection(self.volume, self.connector)
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)
        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)
        self.driver.terminate_connection(self.volume, self.connector,
                                         force=True)
        # vlun should be gone.
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVLUN,
                          self.VOLUME_3PAR_NAME)

    def test_extend_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.UnsetAll()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""extend_volume"",
                       self.fake_extend_volume)
        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}
        self.driver.common.client.createVolume(self.volume['name'],
                                               HP3PAR_CPG,
                                               self.volume['size'],
                                               option)
        old_size = self.volume['size']
        volume = self.driver.common.client.getVolume(self.volume['name'])
        self.driver.extend_volume(volume, str(old_size + 1))
        vol = self.driver.common.client.getVolume(self.volume['name'])
        self.assertEqual(vol['sizeMiB'], str(old_size + 1))


class TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARFCDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARFCDriver, self).setup_fakes()
        self.stubs.Set(hpfcdriver.HP3PARFCDriver,
                       ""_create_3par_fibrechan_host"",
                       self.fake_create_3par_fibrechan_host)

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        super(TestHP3PARFCDriver, self).tearDown()

    def setup_driver(self, configuration):
        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_fibrechan_host(self, hostname, wwn,
                                        domain, persona_id):
        host = {'FCPaths': [{'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 1,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[0]},
                            {'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 0,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[1]}],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'fibre_channel'}
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'fibre_channel')

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        create_host_ret = pack(CLI_CR +
                               'already used by host fakehost.foo (19)')
        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])

        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',
                           '123456789054321']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)


class TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):

    TARGET_IQN = ""iqn.2000-05.com.3pardata:21810002ac00383d""

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARISCSIDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARISCSIDriver, self).setup_fakes()

        self.stubs.Set(hpdriver.HP3PARISCSIDriver, ""_create_3par_iscsi_host"",
                       self.fake_create_3par_iscsi_host)

        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_iqn': self.TARGET_IQN,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'iscsi'}

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        self._hosts = {}
        super(TestHP3PARISCSIDriver, self).tearDown()

    def setup_driver(self, configuration, set_up_fakes=True):
        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)

        if set_up_fakes:
            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                           self.fake_get_ports)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,
                                    domain, persona_id):
        host = {'FCPaths': [],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [{'driverVersion': None,
                                'firmwareVersion': None,
                                'hostSpeed': 0,
                                'ipAddr': '10.10.221.59',
                                'model': None,
                                'name': iscsi_iqn,
                                'portPos': {'cardPort': 1, 'node': 1,
                                            'slot': 8},
                                'vendor': None}],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'iscsi')
        self.assertEqual(result['data']['target_iqn'],
                         self.properties['data']['target_iqn'])
        self.assertEqual(result['data']['target_portal'],
                         self.properties['data']['target_portal'])
        self.assertEqual(result['data']['target_discovered'],
                         self.properties['data']['target_discovered'])

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                           ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        in_use_ret = pack('\r\nalready used by host fakehost.foo ')
        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])

        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',
                           'iqn.1993-08.org.debian:01:222']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_get_ports(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),
                                                    ''])
        self.mox.ReplayAll()

        ports = self.driver.common.get_ports()
        self.assertEqual(ports['FC'][0], '20210002AC00383D')
        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')

    def test_get_iscsi_ip_active(self):
        self.flags(lock_path=self.tempdir)

        #record set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])

        self.mox.ReplayAll()

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.253')

    def test_get_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        #record
        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        show_vlun_ret = 'no vluns listed\r\n'
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])
        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.iscsi_ip_address = '10.10.10.10'
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.252')

    def test_invalid_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']
        config.iscsi_ip_address = '10.10.10.10'
        self.mox.ReplayAll()

        # no valid ip addr should be configured.
        self.assertRaises(exception.InvalidInput,
                          self.setup_driver,
                          config,
                          set_up_fakes=False)

    def test_get_least_used_nsp(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()
        # in use count                           11       12
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])
        self.assertEqual(nsp, '0:2:1')

        # in use count                            11       10
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])
        self.assertEqual(nsp, '1:2:1')

        # in use count                            0       10
        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])
        self.assertEqual(nsp, '1:1:1')


def pack(arg):
    header = '\r\n\r\n\r\n\r\n\r\n'
    footer = '\r\n\r\n\r\n'
    return header + arg + footer

FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

FC_SHOWHOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

NO_FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_NO_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_PORT_IDS_RET = (
    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\r\n'
    '0:2:1,28210002AC00383D,20210002AC00383D\r\n'
    '0:2:2,2FF70002AC00383D,20220002AC00383D\r\n'
    '0:2:3,2FF70002AC00383D,20230002AC00383D\r\n'
    '0:2:4,2FF70002AC00383D,20240002AC00383D\r\n'
    '0:5:1,2FF70002AC00383D,20510002AC00383D\r\n'
    '0:5:2,2FF70002AC00383D,20520002AC00383D\r\n'
    '0:5:3,2FF70002AC00383D,20530002AC00383D\r\n'
    '0:5:4,2FF70202AC00383D,20540202AC00383D\r\n'
    '0:6:4,2FF70002AC00383D,20640002AC00383D\r\n'
    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:2:1,29210002AC00383D,21210002AC00383D\r\n'
    '1:2:2,2FF70002AC00383D,21220002AC00383D\r\n'
    '-----------------------------------------------------------------\r\n')

VOLUME_STATE_RET = (
    'Id,Name,Prov,Type,State,-Detailed_State-\r\n'
    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'
    'normal\r\n'
    '-----------------------------------------------------------------\r\n')

PORT_RET = (
    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'
    'Label,Partner,FailoverState\r\n'
    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'
    '-,1:2:1,none\r\n'
    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'
    '-,1:5:4,active\r\n'
    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'
    '-,-,-\r\n'
    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'
    '-,-,-\r\n')

ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\r\n')

ISCSI_3PAR_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'
    '10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

SHOW_PORT_ISCSI = (
    'N:S:P,IPAddr,---------------iSCSI_Name----------------\r\n'
    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\r\n'
    '-------------------------------------------------------------\r\n')

SHOW_VLUN = (
    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'
    'Status,ID\r\n'
    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'
    'active,0\r\n'
    '------------------------------------------------------------------------'
    '--------------\r\n')

SHOW_VLUN_NONE = (
    'Port\r\n0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n'
    '1:8:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n'
    '0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n1:2:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:8:1\r\n-----')

READY_ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '-------------------------------------------------------------------'
    '----------------------\r\n')
/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver common utilities for HP 3PAR Storage array

The 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

The drivers uses both the REST service and the SSH
command line to correctly operate.  Since the
ssh credentials and the REST credentials can be different
we need to have settings for both.

The drivers requires the use of the san_ip, san_login,
san_password settings for ssh connections into the 3PAR
array.   It also requires the setting of
hp3par_api_url, hp3par_username, hp3par_password
for credentials to talk to the REST service on the 3PAR
array.
""""""

import ast
import base64
import json
import paramiko
import pprint
from random import randint
import re
import time
import uuid

from eventlet import greenthread
from hp3parclient import client
from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)

hp3par_opts = [
    cfg.StrOpt('hp3par_api_url',
               default='',
               help=""3PAR WSAPI Server Url like ""
                    ""https://<3par ip>:8080/api/v1""),
    cfg.StrOpt('hp3par_username',
               default='',
               help=""3PAR Super user username""),
    cfg.StrOpt('hp3par_password',
               default='',
               help=""3PAR Super user password"",
               secret=True),
    #TODO(kmartin): Remove hp3par_domain during I release.
    cfg.StrOpt('hp3par_domain',
               default=None,
               help=""This option is DEPRECATED and no longer used. ""
                    ""The 3par domain name to use.""),
    cfg.StrOpt('hp3par_cpg',
               default=""OpenStack"",
               help=""The CPG to use for volume creation""),
    cfg.StrOpt('hp3par_cpg_snap',
               default="""",
               help=""The CPG to use for Snapshots for volumes. ""
                    ""If empty hp3par_cpg will be used""),
    cfg.StrOpt('hp3par_snapshot_retention',
               default="""",
               help=""The time in hours to retain a snapshot.  ""
                    ""You can't delete it before this expires.""),
    cfg.StrOpt('hp3par_snapshot_expiration',
               default="""",
               help=""The time in hours when a snapshot expires ""
                    "" and is deleted.  This must be larger than expiration""),
    cfg.BoolOpt('hp3par_debug',
                default=False,
                help=""Enable HTTP debugging to 3PAR""),
    cfg.ListOpt('hp3par_iscsi_ips',
                default=[],
                help=""List of target iSCSI addresses to use."")
]


CONF = cfg.CONF
CONF.register_opts(hp3par_opts)


class HP3PARCommon(object):

    stats = {}

    # Valid values for volume type extra specs
    # The first value in the list is the default value
    valid_prov_values = ['thin', 'full']
    valid_persona_values = ['1 - Generic',
                            '2 - Generic-ALUA',
                            '6 - Generic-legacy',
                            '7 - HPUX-legacy',
                            '8 - AIX-legacy',
                            '9 - EGENERA',
                            '10 - ONTAP-legacy',
                            '11 - VMware',
                            '12 - OpenVMS']
    hp_qos_keys = ['maxIOPS', 'maxBWS']
    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']

    def __init__(self, config):
        self.sshpool = None
        self.config = config
        self.hosts_naming_dict = dict()
        self.client = None
        if CONF.hp3par_domain is not None:
            LOG.deprecated(_(""hp3par_domain has been deprecated and ""
                             ""is no longer used. The domain is automatically ""
                             ""looked up based on the CPG.""))

    def check_flags(self, options, required_flags):
        for flag in required_flags:
            if not getattr(options, flag, None):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

    def _create_client(self):
        return client.HP3ParClient(self.config.hp3par_api_url)

    def client_login(self):
        try:
            LOG.debug(""Connecting to 3PAR"")
            self.client.login(self.config.hp3par_username,
                              self.config.hp3par_password)
        except hpexceptions.HTTPUnauthorized as ex:
            LOG.warning(""Failed to connect to 3PAR (%s) because %s"" %
                       (self.config.hp3par_api_url, str(ex)))
            msg = _(""Login to 3PAR array invalid"")
            raise exception.InvalidInput(reason=msg)

    def client_logout(self):
        self.client.logout()
        LOG.debug(""Disconnect from 3PAR"")

    def do_setup(self, context):
        self.client = self._create_client()
        if self.config.hp3par_debug:
            self.client.debug_rest(True)

        self.client_login()

        try:
            # make sure the default CPG exists
            self.validate_cpg(self.config.hp3par_cpg)
            self._set_connections()
        finally:
            self.client_logout()

    def validate_cpg(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound as ex:
            err = (_(""CPG (%s) doesn't exist on array"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

    def _set_connections(self):
        """"""Set the number of concurrent connections.

        The 3PAR WS API server has a limit of concurrent connections.
        This is setting the number to the highest allowed, 15 connections.
        """"""
        self._cli_run(['setwsapi', '-sru', 'high'])

    def get_domain(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound:
            err = (_(""Failed to get domain because CPG (%s) doesn't ""
                     ""exist on array."") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        domain = cpg['domain']
        if not domain:
            err = (_(""CPG (%s) must be in a domain"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)
        return domain

    def extend_volume(self, volume, new_size):
        volume_name = self._get_3par_vol_name(volume['id'])
        old_size = volume.size
        growth_size = int(new_size) - old_size
        LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
                  (volume_name, old_size, new_size, growth_size))
        try:
            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error extending volume %s"") % volume)

    def _get_3par_vol_name(self, volume_id):
        """"""Get converted 3PAR volume name.

        Converts the openstack volume id from
        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2
        to
        osv-7P.DD5jLTPWF7tcwnMF80g

        We convert the 128 bits of the uuid into a 24character long
        base64 encoded string to ensure we don't exceed the maximum
        allowed 31 character name limit on 3Par

        We strip the padding '=' and replace + with .
        and / with -
        """"""
        volume_name = self._encode_name(volume_id)
        return ""osv-%s"" % volume_name

    def _get_3par_snap_name(self, snapshot_id):
        snapshot_name = self._encode_name(snapshot_id)
        return ""oss-%s"" % snapshot_name

    def _get_3par_vvs_name(self, volume_id):
        vvs_name = self._encode_name(volume_id)
        return ""vvs-%s"" % vvs_name

    def _encode_name(self, name):
        uuid_str = name.replace(""-"", """")
        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)
        vol_encoded = base64.b64encode(vol_uuid.bytes)

        # 3par doesn't allow +, nor /
        vol_encoded = vol_encoded.replace('+', '.')
        vol_encoded = vol_encoded.replace('/', '-')
        # strip off the == as 3par doesn't like those.
        vol_encoded = vol_encoded.replace('=', '')
        return vol_encoded

    def _capacity_from_size(self, vol_size):

        # because 3PAR volume sizes are in
        # Mebibytes, Gigibytes, not Megabytes.
        MB = 1000L
        MiB = 1.048576

        if int(vol_size) == 0:
            capacity = MB  # default: 1GB
        else:
            capacity = vol_size * MB

        capacity = int(round(capacity / MiB))
        return capacity

    def _cli_run(self, cmd):
        """"""Runs a CLI command over SSH, without doing any result parsing.""""""
        LOG.debug(""SSH CMD = %s "" % cmd)

        (stdout, stderr) = self._run_ssh(cmd, False)

        # we have to strip out the input and exit lines
        tmp = stdout.split(""\r\n"")
        out = tmp[5:len(tmp) - 2]
        return out

    def _ssh_execute(self, ssh, cmd, check_exit_code=True):
        """"""We have to do this in order to get CSV output from the CLI command.

        We first have to issue a command to tell the CLI that we want the
        output to be formatted in CSV, then we issue the real command.
        """"""
        LOG.debug(_('Running cmd (SSH): %s'), cmd)

        channel = ssh.invoke_shell()
        stdin_stream = channel.makefile('wb')
        stdout_stream = channel.makefile('rb')
        stderr_stream = channel.makefile('rb')

        stdin_stream.write('''setclienv csvtable 1
%s
exit
''' % cmd)

        # stdin.write('process_input would go here')
        # stdin.flush()

        # NOTE(justinsb): This seems suspicious...
        # ...other SSH clients have buffering issues with this approach
        stdout = stdout_stream.read()
        stderr = stderr_stream.read()
        stdin_stream.close()
        stdout_stream.close()
        stderr_stream.close()

        exit_status = channel.recv_exit_status()

        # exit_status == -1 if no exit code was returned
        if exit_status != -1:
            LOG.debug(_('Result was %s') % exit_status)
            if check_exit_code and exit_status != 0:
                raise exception.ProcessExecutionError(exit_code=exit_status,
                                                      stdout=stdout,
                                                      stderr=stderr,
                                                      cmd=cmd)
        channel.close()
        return (stdout, stderr)

    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            self.sshpool = utils.SSHPool(self.config.san_ip,
                                         self.config.san_ssh_port,
                                         self.config.ssh_conn_timeout,
                                         self.config.san_login,
                                         password=self.config.san_password,
                                         privatekey=
                                         self.config.san_private_key,
                                         min_size=
                                         self.config.ssh_min_pool_conn,
                                         max_size=
                                         self.config.ssh_max_pool_conn)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return self._ssh_execute(ssh, command,
                                                 check_exit_code=check_exit)
                    except Exception as e:
                        LOG.error(e)
                        greenthread.sleep(randint(20, 500) / 100.0)
                msg = (_(""SSH Command failed after '%(total_attempts)r' ""
                         ""attempts : '%(command)s'"") %
                       {'total_attempts': total_attempts, 'command': command})
                raise paramiko.SSHException(msg)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running ssh command: %s"") % command)

    def _delete_3par_host(self, hostname):
        self._cli_run(['removehost', hostname])

    def _create_3par_vlun(self, volume, hostname):
        out = self._cli_run(['createvlun', volume, 'auto', hostname])
        if out and len(out) > 1:
            if ""must be in the same domain"" in out[0]:
                err = out[0].strip()
                err = err + "" "" + out[1].strip()
                raise exception.Invalid3PARDomain(err=err)

    def _safe_hostname(self, hostname):
        """"""We have to use a safe hostname length for 3PAR host names.""""""
        try:
            index = hostname.index('.')
        except ValueError:
            # couldn't find it
            index = len(hostname)

        # we'll just chop this off for now.
        if index > 23:
            index = 23

        return hostname[:index]

    def _get_3par_host(self, hostname):
        out = self._cli_run(['showhost', '-verbose', hostname])
        LOG.debug(""OUTPUT = \n%s"" % (pprint.pformat(out)))
        host = {'id': None, 'name': None,
                'domain': None,
                'descriptors': {},
                'iSCSIPaths': [],
                'FCPaths': []}

        if out:
            err = out[0]
            if err == 'no hosts listed':
                msg = {'code': 'NON_EXISTENT_HOST',
                       'desc': ""HOST '%s' was not found"" % hostname}
                raise hpexceptions.HTTPNotFound(msg)

            # start parsing the lines after the header line
            for line in out[1:]:
                if line == '':
                    break
                tmp = line.split(',')
                paths = {}

                LOG.debug(""line = %s"" % (pprint.pformat(tmp)))
                host['id'] = tmp[0]
                host['name'] = tmp[1]

                portPos = tmp[4]
                LOG.debug(""portPos = %s"" % (pprint.pformat(portPos)))
                if portPos == '---':
                    portPos = None
                else:
                    port = portPos.split(':')
                    portPos = {'node': int(port[0]), 'slot': int(port[1]),
                               'cardPort': int(port[2])}

                paths['portPos'] = portPos

                # If FC entry
                if tmp[5] == 'n/a':
                    paths['wwn'] = tmp[3]
                    host['FCPaths'].append(paths)
                # else iSCSI entry
                else:
                    paths['name'] = tmp[3]
                    paths['ipAddr'] = tmp[5]
                    host['iSCSIPaths'].append(paths)

            # find the offset to the description stuff
            offset = 0
            for line in out:
                if line[:15] == '---------- Host':
                    break
                else:
                    offset += 1

            info = out[offset + 2]
            tmp = info.split(':')
            host['domain'] = tmp[1]

            info = out[offset + 4]
            tmp = info.split(':')
            host['descriptors']['location'] = tmp[1]

            info = out[offset + 5]
            tmp = info.split(':')
            host['descriptors']['ipAddr'] = tmp[1]

            info = out[offset + 6]
            tmp = info.split(':')
            host['descriptors']['os'] = tmp[1]

            info = out[offset + 7]
            tmp = info.split(':')
            host['descriptors']['model'] = tmp[1]

            info = out[offset + 8]
            tmp = info.split(':')
            host['descriptors']['contact'] = tmp[1]

            info = out[offset + 9]
            tmp = info.split(':')
            host['descriptors']['comment'] = tmp[1]

        return host

    def get_ports(self):
        # First get the active FC ports
        out = self._cli_run(['showport'])

        # strip out header
        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,
        # Protocol,Label,Partner,FailoverState
        out = out[1:len(out) - 2]

        ports = {'FC': [], 'iSCSI': {}}
        for line in out:
            tmp = line.split(',')

            if tmp:
                if tmp[1] == 'target' and tmp[2] == 'ready':
                    if tmp[6] == 'FC':
                        ports['FC'].append(tmp[4])

        # now get the active iSCSI ports
        out = self._cli_run(['showport', '-iscsi'])

        # strip out header
        # N:S:P,State,IPAddr,Netmask,Gateway,
        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port
        out = out[1:len(out) - 2]
        for line in out:
            tmp = line.split(',')

            if tmp and len(tmp) > 2:
                if tmp[1] == 'ready':
                    ports['iSCSI'][tmp[2]] = {}

        # now get the nsp and iqn
        result = self._cli_run(['showport', '-iscsiname'])
        if result:
            # first line is header
            # nsp, ip,iqn
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 2:
                    if info[1] in ports['iSCSI']:
                        nsp = info[0]
                        ip_addr = info[1]
                        iqn = info[2]
                        ports['iSCSI'][ip_addr] = {'nsp': nsp,
                                                   'iqn': iqn
                                                   }

        LOG.debug(""PORTS = %s"" % pprint.pformat(ports))
        return ports

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_volume_stats()

        return self.stats

    def _update_volume_stats(self):
        # const to convert MiB to GB
        const = 0.0009765625

        # storage_protocol and volume_backend_name are
        # set in the child classes
        stats = {'driver_version': '1.0',
                 'free_capacity_gb': 'unknown',
                 'reserved_percentage': 0,
                 'storage_protocol': None,
                 'total_capacity_gb': 'unknown',
                 'QoS_support': True,
                 'vendor_name': 'Hewlett-Packard',
                 'volume_backend_name': None}

        try:
            cpg = self.client.getCPG(self.config.hp3par_cpg)
            if 'limitMiB' not in cpg['SDGrowth']:
                total_capacity = 'infinite'
                free_capacity = 'infinite'
            else:
                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)
                free_capacity = int((cpg['SDGrowth']['limitMiB'] -
                                    cpg['UsrUsage']['usedMiB']) * const)

            stats['total_capacity_gb'] = total_capacity
            stats['free_capacity_gb'] = free_capacity
        except hpexceptions.HTTPNotFound:
            err = (_(""CPG (%s) doesn't exist on array"")
                   % self.config.hp3par_cpg)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        self.stats = stats

    def create_vlun(self, volume, host):
        """"""Create a VLUN.

        In order to export a volume on a 3PAR box, we have to create a VLUN.
        """"""
        volume_name = self._get_3par_vol_name(volume['id'])
        self._create_3par_vlun(volume_name, host['name'])
        return self.client.getVLUN(volume_name)

    def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self.client.getVLUN(volume_name)
        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        self._delete_3par_host(hostname)

    def _get_volume_type(self, type_id):
        ctxt = context.get_admin_context()
        return volume_types.get_volume_type(ctxt, type_id)

    def _get_key_value(self, hp3par_keys, key, default=None):
        if hp3par_keys is not None and key in hp3par_keys:
            return hp3par_keys[key]
        else:
            return default

    def _get_qos_value(self, qos, key, default=None):
        if key in qos:
            return qos[key]
        else:
            return default

    def _get_qos_by_volume_type(self, volume_type):
        qos = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if 'qos:' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp_qos_keys:
                qos[key] = int(value)
        return qos

    def _get_keys_by_volume_type(self, volume_type):
        hp3par_keys = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if ':' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp3par_valid_keys:
                hp3par_keys[key] = value
        return hp3par_keys

    def _set_qos_rule(self, qos, vvs_name):
        max_io = self._get_qos_value(qos, 'maxIOPS')
        max_bw = self._get_qos_value(qos, 'maxBWS')
        cli_qos_string = """"
        if max_io is not None:
            cli_qos_string += ('-io %s ' % max_io)
        if max_bw is not None:
            cli_qos_string += ('-bw %sM ' % max_bw)
        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])

    def _add_volume_to_volume_set(self, volume, volume_name,
                                  cpg, vvs_name, qos):
        if vvs_name is not None:
            # Admin has set a volume set name to add the volume to
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])
        else:
            vvs_name = self._get_3par_vvs_name(volume['id'])
            domain = self.get_domain(cpg)
            self._cli_run(['createvvset', '-domain', domain, vvs_name])
            self._set_qos_rule(qos, vvs_name)
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])

    def _remove_volume_set(self, vvs_name):
        # Must first clear the QoS rules before removing the volume set
        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])
        self._cli_run(['removevvset', '-f', vvs_name])

    def _remove_volume_from_volume_set(self, volume_name, vvs_name):
        self._cli_run(['removevvset', '-f', vvs_name, volume_name])

    def get_cpg(self, volume, allowSnap=False):
        volume_name = self._get_3par_vol_name(volume['id'])
        vol = self.client.getVolume(volume_name)
        if 'userCPG' in vol:
            return vol['userCPG']
        elif allowSnap:
            return vol['snapCPG']
        return None

    def _get_3par_vol_comment(self, volume_name):
        vol = self.client.getVolume(volume_name)
        if 'comment' in vol:
            return vol['comment']
        return None

    def get_persona_type(self, volume, hp3par_keys=None):
        default_persona = self.valid_persona_values[0]
        type_id = volume.get('volume_type_id', None)
        volume_type = None
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            if hp3par_keys is None:
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
        persona_value = self._get_key_value(hp3par_keys, 'persona',
                                            default_persona)
        if persona_value not in self.valid_persona_values:
            err = _(""Must specify a valid persona %(valid)s, ""
                    ""value '%(persona)s' is invalid."") % \
                   ({'valid': self.valid_persona_values,
                     'persona': persona_value})
            raise exception.InvalidInput(reason=err)
        # persona is set by the id so remove the text and return the id
        # i.e for persona '1 - Generic' returns 1
        persona_id = persona_value.split(' ')
        return persona_id[0]

    def get_volume_settings_from_type(self, volume):
        cpg = None
        snap_cpg = None
        volume_type = None
        vvs_name = None
        hp3par_keys = {}
        qos = {}
        type_id = volume.get('volume_type_id', None)
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            hp3par_keys = self._get_keys_by_volume_type(volume_type)
            vvs_name = self._get_key_value(hp3par_keys, 'vvs')
            if vvs_name is None:
                qos = self._get_qos_by_volume_type(volume_type)

        cpg = self._get_key_value(hp3par_keys, 'cpg',
                                  self.config.hp3par_cpg)
        if cpg is not self.config.hp3par_cpg:
            # The cpg was specified in a volume type extra spec so it
            # needs to be validiated that it's in the correct domain.
            self.validate_cpg(cpg)
            # Also, look to see if the snap_cpg was specified in volume
            # type extra spec, if not use the extra spec cpg as the
            # default.
            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)
        else:
            # default snap_cpg to hp3par_cpg_snap if it's not specified
            # in the volume type extra specs.
            snap_cpg = self.config.hp3par_cpg_snap
            # if it's still not set or empty then set it to the cpg
            # specified in the cinder.conf file.
            if not self.config.hp3par_cpg_snap:
                snap_cpg = cpg

        # if provisioning is not set use thin
        default_prov = self.valid_prov_values[0]
        prov_value = self._get_key_value(hp3par_keys, 'provisioning',
                                         default_prov)
        # check for valid provisioning type
        if prov_value not in self.valid_prov_values:
            err = _(""Must specify a valid provisioning type %(valid)s, ""
                    ""value '%(prov)s' is invalid."") % \
                   ({'valid': self.valid_prov_values,
                     'prov': prov_value})
            raise exception.InvalidInput(reason=err)

        tpvv = True
        if prov_value == ""full"":
            tpvv = False

        # check for valid persona even if we don't use it until
        # attach time, this will give the end user notice that the
        # persona type is invalid at volume creation time
        self.get_persona_type(volume, hp3par_keys)

        return {'cpg': cpg, 'snap_cpg': snap_cpg,
                'vvs_name': vvs_name, 'qos': qos,
                'tpvv': tpvv, 'volume_type': volume_type}

    def create_volume(self, volume):
        LOG.debug(""CREATE VOLUME (%s : %s %s)"" %
                  (volume['display_name'], volume['name'],
                   self._get_3par_vol_name(volume['id'])))
        try:
            comments = {'volume_id': volume['id'],
                        'name': volume['name'],
                        'type': 'OpenStack'}

            name = volume.get('display_name', None)
            if name:
                comments['display_name'] = name

            # get the options supported by volume types
            type_info = self.get_volume_settings_from_type(volume)
            volume_type = type_info['volume_type']
            vvs_name = type_info['vvs_name']
            qos = type_info['qos']
            cpg = type_info['cpg']
            snap_cpg = type_info['snap_cpg']
            tpvv = type_info['tpvv']

            type_id = volume.get('volume_type_id', None)
            if type_id is not None:
                comments['volume_type_name'] = volume_type.get('name')
                comments['volume_type_id'] = type_id
                if vvs_name is not None:
                    comments['vvs'] = vvs_name
                else:
                    comments['qos'] = qos

            extras = {'comment': json.dumps(comments),
                      'snapCPG': snap_cpg,
                      'tpvv': tpvv}

            capacity = self._capacity_from_size(volume['size'])
            volume_name = self._get_3par_vol_name(volume['id'])
            self.client.createVolume(volume_name, cpg, capacity, extras)
            if qos or vvs_name is not None:
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPConflict:
            raise exception.Duplicate(_(""Volume (%s) already exists on array"")
                                      % volume_name)
        except hpexceptions.HTTPBadRequest as ex:
            LOG.error(str(ex))
            raise exception.Invalid(ex.get_description())
        except exception.InvalidInput as ex:
            LOG.error(str(ex))
            raise ex
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,
                     tpvv=True):
        # Virtual volume sets are not supported with the -online option
        cmd = ['createvvcopy', '-p', src_name, '-online']
        if snap_cpg:
            cmd.extend(['-snp_cpg', snap_cpg])
        if tpvv:
            cmd.append('-tpvv')
        if cpg:
            cmd.append(cpg)
        cmd.append(dest_name)
        LOG.debug('Creating clone of a volume with %s' % cmd)
        self._cli_run(cmd)

    def get_next_word(self, s, search_string):
        """"""Return the next word.

        Search 's' for 'search_string', if found return the word preceding
        'search_string' from 's'.
        """"""
        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)
        return word.groups()[0].strip(' ')

    def _get_3par_vol_comment_value(self, vol_comment, key):
        comment_dict = dict(ast.literal_eval(vol_comment))
        if key in comment_dict:
            return comment_dict[key]
        return None

    def create_cloned_volume(self, volume, src_vref):
        try:
            orig_name = self._get_3par_vol_name(volume['source_volid'])
            vol_name = self._get_3par_vol_name(volume['id'])

            type_info = self.get_volume_settings_from_type(volume)

            # make the 3PAR copy the contents.
            # can't delete the original until the copy is done.
            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],
                              snap_cpg=type_info['snap_cpg'],
                              tpvv=type_info['tpvv'])
            return None
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def _get_vvset_from_3par(self, volume_name):
        """"""Get Virtual Volume Set from 3PAR.

        The only way to do this currently is to try and delete the volume
        to get the error message.

        NOTE(walter-boring): don't call this unless you know the volume is
        already in a vvset!
        """"""
        cmd = ['removevv', '-f', volume_name]
        LOG.debug(""Issuing remove command to find vvset name %s"" % cmd)
        out = self._cli_run(cmd)
        vvset_name = None
        if out and len(out) > 1:
            if out[1].startswith(""Attempt to delete ""):
                words = out[1].split("" "")
                vvset_name = words[len(words) - 1]

        return vvset_name

    def delete_volume(self, volume):
        try:
            volume_name = self._get_3par_vol_name(volume['id'])
            # Try and delete the volume, it might fail here because
            # the volume is part of a volume set which will have the
            # volume set name in the error.
            try:
                self.client.deleteVolume(volume_name)
            except hpexceptions.HTTPConflict as ex:
                if ex.get_code() == 34:
                    # This is a special case which means the
                    # volume is part of a volume set.
                    vvset_name = self._get_vvset_from_3par(volume_name)
                    LOG.debug(""Returned vvset_name = %s"" % vvset_name)
                    if vvset_name is not None and \
                       vvset_name.startswith('vvs-'):
                        # We have a single volume per volume set, so
                        # remove the volume set.
                        self._remove_volume_set(
                            self._get_3par_vvs_name(volume['id']))
                    elif vvset_name is not None:
                        # We have a pre-defined volume set just remove the
                        # volume and leave the volume set.
                        self._remove_volume_from_volume_set(volume_name,
                                                            vvset_name)
                    self.client.deleteVolume(volume_name)
                else:
                    raise ex

        except hpexceptions.HTTPNotFound as ex:
            # We'll let this act as if it worked
            # it helps clean up the cinder entries.
            LOG.error(str(ex))
        except hpexceptions.HTTPForbidden as ex:
            LOG.error(str(ex))
            raise exception.NotAuthorized(ex.get_description())
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        LOG.debug(""Create Volume from Snapshot\n%s\n%s"" %
                  (pprint.pformat(volume['display_name']),
                   pprint.pformat(snapshot['display_name'])))

        if snapshot['volume_size'] != volume['size']:
            err = ""You cannot change size of the volume.  It must ""
            ""be the same as the snapshot.""
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            volume_name = self._get_3par_vol_name(volume['id'])

            extra = {'volume_id': volume['id'],
                     'snapshot_id': snapshot['id']}

            volume_type = None
            type_id = volume.get('volume_type_id', None)
            vvs_name = None
            qos = {}
            hp3par_keys = {}
            if type_id is not None:
                volume_type = self._get_volume_type(type_id)
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
                vvs_name = self._get_key_value(hp3par_keys, 'vvs')
                if vvs_name is None:
                    qos = self._get_qos_by_volume_type(volume_type)

            name = volume.get('display_name', None)
            if name:
                extra['display_name'] = name

            description = volume.get('display_description', None)
            if description:
                extra['description'] = description

            optional = {'comment': json.dumps(extra),
                        'readOnly': False}

            self.client.createSnapshot(volume_name, snap_name, optional)
            if qos or vvs_name is not None:
                cpg = self._get_key_value(hp3par_keys, 'cpg',
                                          self.config.hp3par_cpg)
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def create_snapshot(self, snapshot):
        LOG.debug(""Create Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            vol_name = self._get_3par_vol_name(snapshot['volume_id'])

            extra = {'volume_name': snapshot['volume_name']}
            vol_id = snapshot.get('volume_id', None)
            if vol_id:
                extra['volume_id'] = vol_id

            try:
                extra['display_name'] = snapshot['display_name']
            except AttributeError:
                pass

            try:
                extra['description'] = snapshot['display_description']
            except AttributeError:
                pass

            optional = {'comment': json.dumps(extra),
                        'readOnly': True}
            if self.config.hp3par_snapshot_expiration:
                optional['expirationHours'] = (
                    self.config.hp3par_snapshot_expiration)

            if self.config.hp3par_snapshot_retention:
                optional['retentionHours'] = (
                    self.config.hp3par_snapshot_retention)

            self.client.createSnapshot(snap_name, vol_name, optional)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()

    def delete_snapshot(self, snapshot):
        LOG.debug(""Delete Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            self.client.deleteVolume(snap_name)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound as ex:
            LOG.error(str(ex))

    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):
        out = self._cli_run(['showhost', '-d'])
        # wwns_iqn may be a list of strings or a single
        # string. So, if necessary, create a list to loop.
        if not isinstance(wwns_iqn, list):
            wwn_iqn_list = [wwns_iqn]
        else:
            wwn_iqn_list = wwns_iqn

        for wwn_iqn in wwn_iqn_list:
            for showhost in out:
                if (wwn_iqn.upper() in showhost.upper()):
                    return showhost.split(',')[1]

    def terminate_connection(self, volume, hostname, wwn_iqn):
        """"""Driver entry point to unattach a volume from an instance.""""""
        try:
            # does 3par know this host by a different name?
            if hostname in self.hosts_naming_dict:
                hostname = self.hosts_naming_dict.get(hostname)
            self.delete_vlun(volume, hostname)
            return
        except hpexceptions.HTTPNotFound as e:
            if 'host does not exist' in e.get_description():
                # use the wwn to see if we can find the hostname
                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)
                # no 3par host, re-throw
                if (hostname is None):
                    raise
            else:
            # not a 'host does not exist' HTTPNotFound exception, re-throw
                raise

        #try again with name retrieved from 3par
        self.delete_vlun(volume, hostname)

    def parse_create_host_error(self, hostname, out):
        search_str = ""already used by host ""
        if search_str in out[1]:
            #host exists, return name used by 3par
            hostname_3par = self.get_next_word(out[1], search_str)
            self.hosts_naming_dict[hostname] = hostname_3par
            return hostname_3par
/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        command = ['createhost', '-persona', persona_id, '-domain', domain,
                   hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwns):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-add', hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',
               domain, hostname, iscsi_iqn]
        out = self.common._cli_run(cmd)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]
        self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cmd_list = [verb]
        for k, v in cliq_args.items():
            cmd_list.append(""%s=%s"" % (k, v))

        return self._run_ssh(cmd_list, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",0
83,83,c55589b131828f3a595903f6796cb2d0babb772f,"/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'
                                   % (persona_id, domain,
                                      hostname, "" "".join(wwn)), None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwn):
        # when using -add, you can not send the persona or domain options
        out = self.common._cli_run('createhost -add %s %s'
                                   % (hostname, "" "".join(wwn)), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \
              (persona_id, domain, hostname, iscsi_iqn)
        out = self.common._cli_run(cmd, None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'
                             % (hostname, iscsi_iqn), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cliq_arg_strings = []
        for k, v in cliq_args.items():
            cliq_arg_strings.append("" %s=%s"" % (k, v))
        cmd = verb + ''.join(cliq_arg_strings)

        return self._run_ssh(cmd, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",1
88,88,adc94c745e4d9f792fd9c9791c7b4cd8790d0d2f,"coalib/misc/Shell.py/n/nfrom contextlib import contextmanager
import shlex
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """"""
    Runs a single command in shell and provides stdout, stderr and stdin
    streams.

    This function creates a context manager that sets up the process (using
    `subprocess.Popen()`), returns to caller, closes streams and waits for
    process to exit on leaving.

    Shell execution is disabled by default (so no shell expansion takes place).
    If you want to turn shell execution on, you can pass `shell=True` like you
    would do for `subprocess.Popen()`.

    The process is opened in `universal_newlines` mode by default.

    :param command: The command to run on shell. This parameter can either
                    be a sequence of arguments that are directly passed to
                    the process or a string. A string gets splitted beforehand
                    using `shlex.split()`.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `stdout`,
                    `stderr`, `stdin` and `universal_newlines`, a `TypeError`
                    is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """"""
    if isinstance(command, str):
        command = shlex.split(command)

    process = Popen(command,
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """"""
    Runs a single command in shell and returns the read stdout and stderr data.

    This function waits for the process (created using `subprocess.Popen()`) to
    exit.

    Shell execution is disabled by default (so no shell expansion takes place).
    If you want to turn shell execution on, you can pass `shell=True` like you
    would do for `subprocess.Popen()`.

    See also `run_interactive_shell_command()`.

    :param command: The command to run on shell. This parameter can either
                    be a sequence of arguments that are directly passed to
                    the process or a string. A string gets splitted beforehand
                    using `shlex.split()`.
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `stdout`,
                    `stderr`, `stdin` and `universal_newlines`, a `TypeError`
                    is raised then).
    :return:        A tuple with `(stdoutstring, stderrstring)`.
    """"""
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """"""
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either ""powershell"" if Windows
             Powershell is detected, ""cmd"" if command prompt is been
             detected or ""sh"" if it's neither of these.
    """"""
    out_hostname, _ = run_shell_command([""echo"", ""$host.name""], shell=True)
    if out_hostname.strip() == ""ConsoleHost"":
        return ""powershell""
    out_0, _ = run_shell_command([""echo"", ""$0""], shell=True)
    if out_0.strip() == """" and out_0.strip() == """":
        return ""cmd""
    return ""sh""


def prepare_string_argument(string, shell=get_shell_type()):
    """"""
    Prepares a string argument for being passed as a parameter on shell.

    On `sh` this function effectively encloses the given string
    with quotes (either '' or """", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not ""sh"" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """"""
    if shell == ""sh"":
        return '""' + escape(string, '""') + '""'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """"""
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are ""sh"", ""powershell"", and ""cmd"" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """"""
    if shell == ""cmd"":
        # If a quote ("") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '""' + escape(path, '""', '^') + '""'
    elif shell == ""sh"":
        return escape(path, "" "")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path
/n/n/ncoalib/tests/bearlib/abstractions/LintTest.py/n/nimport os
import platform
import unittest

from coalib.bearlib.abstractions.Lint import Lint
from coalib.misc.ContextManagers import prepare_file
from coalib.misc.Shell import escape_path_argument
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY
from coalib.results.SourceRange import SourceRange
from coalib.settings.Section import Section


class LintTest(unittest.TestCase):

    def setUp(self):
        section = Section(""some_name"")
        self.uut = Lint(section, None)

    def test_invalid_output(self):
        out = list(self.uut.process_output(
            [""1.0|0: Info message\n"",
             ""2.2|1: Normal message\n"",
             ""3.4|2: Major message\n""],
            ""a/file.py"",
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 3)
        self.assertEqual(out[0].origin, ""Lint"")

        self.assertEqual(out[0].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 1, 0))
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].message, ""Info message"")

        self.assertEqual(out[1].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 2, 2))
        self.assertEqual(out[1].severity, RESULT_SEVERITY.NORMAL)
        self.assertEqual(out[1].message, ""Normal message"")

        self.assertEqual(out[2].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 3, 4))
        self.assertEqual(out[2].severity, RESULT_SEVERITY.MAJOR)
        self.assertEqual(out[2].message, ""Major message"")

    def test_custom_regex(self):
        self.uut.output_regex = (r'(?P<origin>\w+)\|'
                                 r'(?P<line>\d+)\.(?P<column>\d+)\|'
                                 r'(?P<end_line>\d+)\.(?P<end_column>\d+)\|'
                                 r'(?P<severity>\w+): (?P<message>.*)')
        self.uut.severity_map = {""I"": RESULT_SEVERITY.INFO}
        out = list(self.uut.process_output(
            [""info_msg|1.0|2.3|I: Info message\n""],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0].affected_code[0].start.line, 1)
        self.assertEqual(out[0].affected_code[0].start.column, 0)
        self.assertEqual(out[0].affected_code[0].end.line, 2)
        self.assertEqual(out[0].affected_code[0].end.column, 3)
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].origin, 'Lint (info_msg)')

    def test_valid_output(self):
        out = list(self.uut.process_output(
            [""Random line that shouldn't be captured\n"",
             ""*************\n""],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 0)

    def test_stdin_input(self):
        with prepare_file([""abcd"", ""efgh""], None) as (lines, filename):
            # Use more which is a command that can take stdin and show it.
            # This is available in windows and unix.
            if platform.system() == ""Windows"":
                # Windows maps `more.com` to `more` only in shell, but `Lint`
                # doesn't use it.
                self.uut.executable = ""more.com""
            else:
                self.uut.executable = ""more""
            self.uut.use_stdin = True
            self.uut.use_stderr = False
            self.uut.process_output = lambda output, filename, file: output

            out = self.uut.lint(file=lines)
            # Some implementations of `more` add an extra newline at the end.
            self.assertTrue((""abcd\n"", ""efgh\n"") == out or
                            (""abcd\n"", ""efgh\n"", ""\n"") == out)

    def test_stderr_output(self):
        self.uut.executable = ""echo""
        self.uut.arguments = ""hello""
        self.uut.use_stdin = False
        self.uut.use_stderr = True
        self.uut.process_output = lambda output, filename, file: output
        out = self.uut.lint(""unused_filename"")
        self.assertEqual((), out)  # stderr is used

        self.uut.use_stderr = False
        out = self.uut.lint(""unused_filename"")
        self.assertEqual(('hello\n',), out)  # stdout is used

        def assert_warn(line):
            assert line == ""hello""
        old_warn = self.uut.warn
        self.uut.warn = assert_warn
        self.uut._print_errors([""hello"", ""\n""])
        self.uut.warn = old_warn

    def test_gives_corrected(self):
        self.uut.gives_corrected = True
        out = tuple(self.uut.process_output([""a"", ""b""], ""filename"", [""a"", ""b""]))
        self.assertEqual((), out)
        out = tuple(self.uut.process_output([""a"", ""b""], ""filename"", [""a""]))
        self.assertEqual(len(out), 1)

    def test_missing_binary(self):
        old_binary = Lint.executable
        invalid_binary = ""invalid_binary_which_doesnt_exist""
        Lint.executable = invalid_binary

        self.assertEqual(Lint.check_prerequisites(),
                         ""'{}' is not installed."".format(invalid_binary))

        # ""echo"" is existent on nearly all platforms.
        Lint.executable = ""echo""
        self.assertTrue(Lint.check_prerequisites())

        del Lint.executable
        self.assertTrue(Lint.check_prerequisites())

        Lint.executable = old_binary

    def test_config_file_generator(self):
        self.uut.executable = ""echo""
        self.uut.arguments = ""-c {config_file}""

        self.assertEqual(
            self.uut._create_command(config_file=""configfile"").strip(),
            ""echo -c "" + escape_path_argument(""configfile""))

    def test_config_file_generator(self):
        self.uut.executable = ""echo""
        self.uut.config_file = lambda: [""config line1""]
        config_filename = self.uut.generate_config_file()
        self.assertTrue(os.path.isfile(config_filename))
        os.remove(config_filename)

        # To complete coverage of closing the config file and check if any
        # errors are thrown there.
        self.uut.lint(""filename"")
/n/n/ncoalib/tests/misc/ShellTest.py/n/nimport os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = ""sh""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/usr/a-dir/"", _type),
            ""/home/usr/a-dir/"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"",
                                 _type),
            ""/home/us\\ r/a-file\\ with\\ spaces.bla"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-dir with spaces/x/"",
                                 _type),
            ""/home/us\\ r/a-dir\\ with\\ spaces/x/"")
        self.assertEqual(
            escape_path_argument(
                ""relative something/with cherries and/pickles.delicious"",
                _type),
            ""relative\\ something/with\\ cherries\\ and/pickles.delicious"")

    def test_escape_path_argument_cmd(self):
        _type = ""cmd""
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\has-a-weird-shell.txt"", _type),
            ""\""C:\\Windows\\has-a-weird-shell.txt\"""")
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\lolrofl\\dirs\\"", _type),
            ""\""C:\\Windows\\lolrofl\\dirs\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\fi le.exe"", _type),
            ""\""X:\\Users\\Maito Gai\\fi le.exe\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Mai to Gai\\director y\\"",
                                 _type),
            ""\""X:\\Users\\Mai to Gai\\director y\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\\""seven-gates\"".y"",
                                 _type),
            ""\""X:\\Users\\Maito Gai\\^\""seven-gates^\"".y\"""")
        self.assertEqual(
            escape_path_argument(""System32\\my-custom relative tool\\"",
                                 _type),
            ""\""System32\\my-custom relative tool\\\"""")
        self.assertEqual(
            escape_path_argument(""System32\\illegal\"" name \""\"".curd"", _type),
            ""\""System32\\illegal^\"" name ^\""^\"".curd\"""")

    def test_escape_path_argument_unsupported(self):
        _type = ""INVALID""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"", _type),
            ""/home/us r/a-file with spaces.bla"")
        self.assertEqual(
            escape_path_argument(""|home|us r|a*dir with spaces|x|"", _type),
            ""|home|us r|a*dir with spaces|x|"")
        self.assertEqual(
            escape_path_argument(""system|a|b|c?d"", _type),
            ""system|a|b|c?d"")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return (sys.executable,
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             ""run_shell_command_testfiles"",
                             scriptname))

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_interactive_program.py"")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), ""test_program X\n"")
            self.assertEqual(p.stdout.readline(), ""Type in a number:\n"")
            p.stdin.write(""33\n"")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), ""33\n"")
            self.assertEqual(p.stdout.readline(), ""Exiting program.\n"")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"", stdout=None):
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_program.py"")

        stdout, stderr = run_shell_command(command)

        expected = (""test_program Z\n""
                    ""non-interactive mode.\n""
                    ""Exiting...\n"")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, """")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_input_program.py"")

        stdout, stderr = run_shell_command(command, ""1  4  10  22"")

        self.assertEqual(stdout, ""37\n"")
        self.assertEqual(stderr, """")

        stdout, stderr = run_shell_command(command, ""1 p 5"")

        self.assertEqual(stdout, """")
        self.assertEqual(stderr, ""INVALID INPUT\n"")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", weird_parameter2=""abc"")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = (""normal_string"",
                             ""string with spaces"",
                             'string with quotes""a',
                             ""string with s-quotes'b"",
                             ""bsn \n A"",
                             ""unrecognized \\q escape"")

    def test_prepare_string_argument_sh(self):
        expected_results = ('""normal_string""',
                            '""string with spaces""',
                            '""string with quotes\\""a""',
                            '""string with s-quotes\'b""',
                            '""bsn \n A""',
                            '""unrecognized \\q escape""')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, ""sh""),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, ""WeIrD_O/S""),
                             string)
/n/n/n",0
89,89,adc94c745e4d9f792fd9c9791c7b4cd8790d0d2f,"/coalib/misc/Shell.py/n/nfrom contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """"""
    Runs a command in shell and provides stdout, stderr and stdin streams.

    This function creates a context manager that sets up the process, returns
    to caller, closes streams and waits for process to exit on leaving.

    The process is opened in `universal_newlines` mode.

    :param command: The command to run on shell.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a
                    `TypeError` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """"""
    process = Popen(command,
                    shell=True,
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """"""
    Runs a command in shell and returns the read stdout and stderr data.

    This function waits for the process to exit.

    :param command: The command to run on shell.
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a
                    `TypeError` is raised then).
    :return:        A tuple with `(stdoutstring, stderrstring)`.
    """"""
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """"""
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either ""powershell"" if Windows
             Powershell is detected, ""cmd"" if command prompt is been
             detected or ""sh"" if it's neither of these.
    """"""
    out_hostname, _ = run_shell_command([""echo"", ""$host.name""])
    if out_hostname.strip() == ""ConsoleHost"":
        return ""powershell""
    out_0, _ = run_shell_command([""echo"", ""$0""])
    if out_0.strip() == """" and out_0.strip() == """":
        return ""cmd""
    return ""sh""


def prepare_string_argument(string, shell=get_shell_type()):
    """"""
    Prepares a string argument for being passed as a parameter on shell.

    On `sh` this function effectively encloses the given string
    with quotes (either '' or """", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not ""sh"" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """"""
    if shell == ""sh"":
        return '""' + escape(string, '""') + '""'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """"""
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are ""sh"", ""powershell"", and ""cmd"" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """"""
    if shell == ""cmd"":
        # If a quote ("") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '""' + escape(path, '""', '^') + '""'
    elif shell == ""sh"":
        return escape(path, "" "")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path
/n/n/n/coalib/tests/bearlib/abstractions/LintTest.py/n/nimport os
import unittest

from coalib.bearlib.abstractions.Lint import Lint
from coalib.misc.ContextManagers import prepare_file
from coalib.misc.Shell import escape_path_argument
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY
from coalib.results.SourceRange import SourceRange
from coalib.settings.Section import Section


class LintTest(unittest.TestCase):

    def setUp(self):
        section = Section(""some_name"")
        self.uut = Lint(section, None)

    def test_invalid_output(self):
        out = list(self.uut.process_output(
            [""1.0|0: Info message\n"",
             ""2.2|1: Normal message\n"",
             ""3.4|2: Major message\n""],
            ""a/file.py"",
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 3)
        self.assertEqual(out[0].origin, ""Lint"")

        self.assertEqual(out[0].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 1, 0))
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].message, ""Info message"")

        self.assertEqual(out[1].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 2, 2))
        self.assertEqual(out[1].severity, RESULT_SEVERITY.NORMAL)
        self.assertEqual(out[1].message, ""Normal message"")

        self.assertEqual(out[2].affected_code[0],
                         SourceRange.from_values(""a/file.py"", 3, 4))
        self.assertEqual(out[2].severity, RESULT_SEVERITY.MAJOR)
        self.assertEqual(out[2].message, ""Major message"")

    def test_custom_regex(self):
        self.uut.output_regex = (r'(?P<origin>\w+)\|'
                                 r'(?P<line>\d+)\.(?P<column>\d+)\|'
                                 r'(?P<end_line>\d+)\.(?P<end_column>\d+)\|'
                                 r'(?P<severity>\w+): (?P<message>.*)')
        self.uut.severity_map = {""I"": RESULT_SEVERITY.INFO}
        out = list(self.uut.process_output(
            [""info_msg|1.0|2.3|I: Info message\n""],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0].affected_code[0].start.line, 1)
        self.assertEqual(out[0].affected_code[0].start.column, 0)
        self.assertEqual(out[0].affected_code[0].end.line, 2)
        self.assertEqual(out[0].affected_code[0].end.column, 3)
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].origin, 'Lint (info_msg)')

    def test_valid_output(self):
        out = list(self.uut.process_output(
            [""Random line that shouldn't be captured\n"",
             ""*************\n""],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 0)

    def test_stdin_input(self):
        with prepare_file([""abcd"", ""efgh""], None) as (lines, filename):
            # Use more which is a command that can take stdin and show it.
            # This is available in windows and unix.
            self.uut.executable = ""more""
            self.uut.use_stdin = True
            self.uut.use_stderr = False
            self.uut.process_output = lambda output, filename, file: output

            out = self.uut.lint(file=lines)
            # Some implementations of `more` add an extra newline at the end.
            self.assertTrue((""abcd\n"", ""efgh\n"") == out or
                            (""abcd\n"", ""efgh\n"", ""\n"") == out)

    def test_stderr_output(self):
        self.uut.executable = ""echo""
        self.uut.arguments = ""hello""
        self.uut.use_stdin = False
        self.uut.use_stderr = True
        self.uut.process_output = lambda output, filename, file: output
        out = self.uut.lint(""unused_filename"")
        self.assertEqual((), out)  # stderr is used

        self.uut.use_stderr = False
        out = self.uut.lint(""unused_filename"")
        self.assertEqual(('hello\n',), out)  # stdout is used

        def assert_warn(line):
            assert line == ""hello""
        old_warn = self.uut.warn
        self.uut.warn = assert_warn
        self.uut._print_errors([""hello"", ""\n""])
        self.uut.warn = old_warn

    def test_gives_corrected(self):
        self.uut.gives_corrected = True
        out = tuple(self.uut.process_output([""a"", ""b""], ""filename"", [""a"", ""b""]))
        self.assertEqual((), out)
        out = tuple(self.uut.process_output([""a"", ""b""], ""filename"", [""a""]))
        self.assertEqual(len(out), 1)

    def test_missing_binary(self):
        old_binary = Lint.executable
        invalid_binary = ""invalid_binary_which_doesnt_exist""
        Lint.executable = invalid_binary

        self.assertEqual(Lint.check_prerequisites(),
                         ""'{}' is not installed."".format(invalid_binary))

        # ""echo"" is existent on nearly all platforms.
        Lint.executable = ""echo""
        self.assertTrue(Lint.check_prerequisites())

        del Lint.executable
        self.assertTrue(Lint.check_prerequisites())

        Lint.executable = old_binary

    def test_config_file_generator(self):
        self.uut.executable = ""echo""
        self.uut.arguments = ""-c {config_file}""

        self.assertEqual(
            self.uut._create_command(config_file=""configfile"").strip(),
            ""echo -c "" + escape_path_argument(""configfile""))

    def test_config_file_generator(self):
        self.uut.executable = ""echo""
        self.uut.config_file = lambda: [""config line1""]
        config_filename = self.uut.generate_config_file()
        self.assertTrue(os.path.isfile(config_filename))
        os.remove(config_filename)

        # To complete coverage of closing the config file and check if any
        # errors are thrown there.
        self.uut.lint(""filename"")
/n/n/n/coalib/tests/misc/ShellTest.py/n/nimport os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = ""sh""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/usr/a-dir/"", _type),
            ""/home/usr/a-dir/"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"",
                                 _type),
            ""/home/us\\ r/a-file\\ with\\ spaces.bla"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-dir with spaces/x/"",
                                 _type),
            ""/home/us\\ r/a-dir\\ with\\ spaces/x/"")
        self.assertEqual(
            escape_path_argument(
                ""relative something/with cherries and/pickles.delicious"",
                _type),
            ""relative\\ something/with\\ cherries\\ and/pickles.delicious"")

    def test_escape_path_argument_cmd(self):
        _type = ""cmd""
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\has-a-weird-shell.txt"", _type),
            ""\""C:\\Windows\\has-a-weird-shell.txt\"""")
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\lolrofl\\dirs\\"", _type),
            ""\""C:\\Windows\\lolrofl\\dirs\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\fi le.exe"", _type),
            ""\""X:\\Users\\Maito Gai\\fi le.exe\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Mai to Gai\\director y\\"",
                                 _type),
            ""\""X:\\Users\\Mai to Gai\\director y\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\\""seven-gates\"".y"",
                                 _type),
            ""\""X:\\Users\\Maito Gai\\^\""seven-gates^\"".y\"""")
        self.assertEqual(
            escape_path_argument(""System32\\my-custom relative tool\\"",
                                 _type),
            ""\""System32\\my-custom relative tool\\\"""")
        self.assertEqual(
            escape_path_argument(""System32\\illegal\"" name \""\"".curd"", _type),
            ""\""System32\\illegal^\"" name ^\""^\"".curd\"""")

    def test_escape_path_argument_unsupported(self):
        _type = ""INVALID""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"", _type),
            ""/home/us r/a-file with spaces.bla"")
        self.assertEqual(
            escape_path_argument(""|home|us r|a*dir with spaces|x|"", _type),
            ""|home|us r|a*dir with spaces|x|"")
        self.assertEqual(
            escape_path_argument(""system|a|b|c?d"", _type),
            ""system|a|b|c?d"")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return "" "".join(
            escape_path_argument(s) for s in (
                sys.executable,
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             ""run_shell_command_testfiles"",
                             scriptname)))

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_interactive_program.py"")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), ""test_program X\n"")
            self.assertEqual(p.stdout.readline(), ""Type in a number:\n"")
            p.stdin.write(""33\n"")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), ""33\n"")
            self.assertEqual(p.stdout.readline(), ""Exiting program.\n"")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"", shell=False):
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_program.py"")

        stdout, stderr = run_shell_command(command)

        expected = (""test_program Z\n""
                    ""non-interactive mode.\n""
                    ""Exiting...\n"")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, """")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_input_program.py"")

        stdout, stderr = run_shell_command(command, ""1  4  10  22"")

        self.assertEqual(stdout, ""37\n"")
        self.assertEqual(stderr, """")

        stdout, stderr = run_shell_command(command, ""1 p 5"")

        self.assertEqual(stdout, """")
        self.assertEqual(stderr, ""INVALID INPUT\n"")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", weird_parameter2=""abc"")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = (""normal_string"",
                             ""string with spaces"",
                             'string with quotes""a',
                             ""string with s-quotes'b"",
                             ""bsn \n A"",
                             ""unrecognized \\q escape"")

    def test_prepare_string_argument_sh(self):
        expected_results = ('""normal_string""',
                            '""string with spaces""',
                            '""string with quotes\\""a""',
                            '""string with s-quotes\'b""',
                            '""bsn \n A""',
                            '""unrecognized \\q escape""')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, ""sh""),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, ""WeIrD_O/S""),
                             string)
/n/n/n",1
154,154,29873f8e87b6e3a6135a69380685e40cfbf7ae9b,"admin.py/n/nimport logging

from django.contrib import admin
from django.contrib import messages

from checkcve.forms import CheckCVEForm, CheckCVEChangeForm
from checkcve.models import Checkcve, Software, WhiteList, Cve
from checkcve.utils import create_check_cve_task

logger = logging.getLogger(__name__)


class CheckCVEAdmin(admin.ModelAdmin):
    form = CheckCVEForm

    def check_cve(self, request, obj):
        errors = list()
        test = True
        for probe in obj:
            try:
                probe.check_cve()
            except Exception as e:  # pragma: no cover
                test = False
                logger.exception('Error in check_cve ' + str(self.actions))
                errors.append(str(e))
        if test:
            messages.add_message(request, messages.SUCCESS, ""Check CVE OK"")
        else:  # pragma: no cover
            messages.add_message(request, messages.ERROR, ""Check CVE failed ! "" + str(errors))

    actions = [check_cve]

    def save_model(self, request, obj, form, change):
        create_check_cve_task(obj)
        super().save_model(request, obj, form, change)

    def get_form(self, request, obj=None, **kwargs):
        """"""A ModelAdmin that uses a different form class when adding an object.""""""
        if obj is None:
            return super(CheckCVEAdmin, self).get_form(request, obj, **kwargs)
        else:
            return CheckCVEChangeForm


admin.site.register(Checkcve, CheckCVEAdmin)
admin.site.register(Software)
admin.site.register(WhiteList)
admin.site.register(Cve)
/n/n/nmodels.py/n/nimport logging

import select2.fields
from django.conf import settings
from django.contrib.postgres.fields import ArrayField
from django.db import models
from django.db.models import Q
from django.utils import timezone

from checkcve.utils import convert_to_cpe, CVESearch
from core.models import Probe, OsSupported
from core.notifications import send_notification
from core.ssh import execute
from core.modelsmixins import CommonMixin
from checkcve.modelsmixins import NameMixin

logger = logging.getLogger(__name__)


class Cve(NameMixin, CommonMixin, models.Model):
    name = models.CharField(max_length=100, unique=True, null=False, blank=False)

    def __str__(self):
        return self.name


class WhiteList(NameMixin, CommonMixin, models.Model):
    """"""
    The white list of CVE (Software not vulnerable).
    """"""
    name = models.CharField(max_length=100, unique=True, null=False, blank=False)
    cves = select2.fields.ManyToManyField(Cve,
                                          blank=True,
                                          ajax=True,
                                          search_field=lambda q: Q(name__icontains=q),
                                          sort_field='name',
                                          js_options={'quiet_millis': 200}
                                          )

    def __str__(self):
        return self.name

    def check_if_exists(self, cve_name):
        test = False
        for cve in self.cves.all():
            if cve.name == cve_name:
                test = True
                break
        return test


class Software(CommonMixin, models.Model):
    """"""
    The software to check the common vulnerabilities and exposures.
    """"""
    INSTALED_CHOICES = (
        ('apt', 'apt'),
        ('brew', 'brew'),
    )
    name = models.CharField(max_length=100, null=False, blank=False)
    os = models.ForeignKey(OsSupported, on_delete=models.CASCADE)
    command = models.CharField(max_length=700, null=True, blank=True, editable=False)
    cpe = models.CharField(max_length=100, null=False, blank=False)
    instaled_by = models.CharField(max_length=255, choices=INSTALED_CHOICES, null=False, blank=False)

    class Meta:
        unique_together = ('name', 'os', 'instaled_by')

    def __str__(self):
        return self.name + "" - "" + self.os.name + "" - "" + self.instaled_by

    def get_version(self, probe):
        software_by_os = Software.objects.get(name=self.name, os=probe.server.os)
        command = {'get_version': software_by_os.command}
        if self.instaled_by == 'apt':
            command = {'get_version': ""apt-cache policy "" + str(
                self.name) + "" | sed -n '2 p' | grep -Po '\d{1,2}\.\d{1,2}\.{0,1}\d{0,2}' | sed -n '1 p'""}
        elif self.instaled_by == 'brew':
            command = {'get_version': ""brew list "" + str(self.name) + "" --versions | cut -d ' ' -f 2""}
        try:
            output = execute(probe.server, command)
        except Exception as e:  # pragma: no cover
            logger.exception('Error during get version')
            return str(e)
        logger.info(""output : "" + str(output))
        return output['get_version']


class Checkcve(Probe):
    """"""
    The software to check the common vulnerabilities and exposures.
    """"""
    softwares = models.ManyToManyField(Software, blank=True)
    whitelist = models.ForeignKey(WhiteList, on_delete=models.CASCADE)
    vulnerability_found = models.BooleanField(default=False, editable=False)
    vulnerabilities = ArrayField(models.CharField(max_length=100, blank=True), editable=False, blank=True, null=True,
                                 default=list())

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.type = self.__class__.__name__

    def __str__(self):
        return self.name + ""  "" + self.description

    def check_cve(self):
        cpe_list = list()
        vulnerabilities_list = list()
        list_new_cve = """"
        nbr = 0
        for software in self.softwares.all():
            version = software.get_version(self)
            logger.info("" software get version : "" + str(version))
            cpe_list.append(convert_to_cpe(software.cpe, version))
        for cpe in cpe_list:
            logger.info(""cpe: "" + str(cpe))
            new = False
            list_new_cve_rows = """"
            cve = CVESearch()
            cves_json = cve.cvefor(cpe)
            for i, val in enumerate(cves_json):
                if not self.whitelist.check_if_exists(cves_json[i]['id']):
                    vulnerabilities_list.append(cves_json[i]['id'])
                    new = True
                    nbr += 1
                    logger.info(""CVE not in Whitelist : "" + str(val))
                    if self.server.os.name == 'debian':
                        title = ""<h4><a href='https://security-tracker.debian.org/tracker/"" + cves_json[i][
                            'id'] + ""'>"" + cves_json[i]['id'] + "" :</a></h4>""
                    else:  # pragma: no cover
                        title = ""<h4><a href='https://www.cvedetails.com/cve/"" + cves_json[i]['id'] + ""'>"" + \
                                cves_json[i]['id'] + "" :</a></h4>""
                    infos = ""<b>Infos server :</b> "" + str(self.server) + ""<br/><br/>""
                    list_new_cve_rows = list_new_cve_rows + title + infos + cves_json[i]['summary'] + ""<br/>""
            if new:
                list_new_cve += ""<h2>"" + cpe + ""</h2><br/>"" + list_new_cve_rows
        self.rules_updated_date = timezone.now()
        self.save()
        if list_new_cve:
            self.vulnerability_found = True
            self.vulnerabilities = vulnerabilities_list
            self.save()
            if hasattr(settings, 'HOST'):  # pragma: no cover
                link = ""<br/><a href='https://"" + settings.HOST + ""'>Edit in ProbeManager</a>""
                list_new_cve = list_new_cve + link
            send_notification('%s new CVE' % nbr, list_new_cve, html=True)
            return list_new_cve
        else:
            self.vulnerability_found = False
            self.save()
            return ""No CVE found ! for : "" + str(cpe_list)
/n/n/n",0
155,155,29873f8e87b6e3a6135a69380685e40cfbf7ae9b,"/admin.py/n/nimport logging

from django.contrib import admin
from django.contrib import messages

from checkcve.forms import CheckCVEForm, CheckCVEChangeForm
from checkcve.models import Checkcve, Software, WhiteList, Cve
from checkcve.utils import create_check_cve_task

logger = logging.getLogger(__name__)


class CheckCVEAdmin(admin.ModelAdmin):
    form = CheckCVEForm

    def check_cve(self, request, obj):
        errors = list()
        test = True
        for probe in obj:
            try:
                probe.check_cve()
            except Exception as e:  # pragma: no cover
                test = False
                logger.exception('Error in check_cve ' + str(self.actions))
                errors.append(str(e))
        if test:
            messages.add_message(request, messages.SUCCESS, ""Check CVE OK"")
        else:  # pragma: no cover
            messages.add_message(request, messages.ERROR, ""Check CVE failed ! "" + str(errors))

    actions = [check_cve]

    def save_model(self, request, obj, form, change):
        create_check_cve_task(obj)
        super().save_model(request, obj, form, change)

    def get_form(self, request, obj=None, **kwargs):
        """"""A ModelAdmin that uses a different form class when adding an object.""""""
        if obj is None:
            return super(CheckCVEAdmin, self).get_form(request, obj, **kwargs)
        else:
            return CheckCVEChangeForm


class SoftwareAdmin(admin.ModelAdmin):
    class Media:
        js = (
            'checkcve/js/mask-command-field.js',
        )


admin.site.register(Checkcve, CheckCVEAdmin)
admin.site.register(Software, SoftwareAdmin)
admin.site.register(WhiteList)
admin.site.register(Cve)
/n/n/n",1
156,156,5f3c4a9a14598f4f706bfccb007ead2ec2816931,"modules/qdb.py/n/nfrom module import Module
from response.response import Response

import hashlib
import requests
import json
from datetime import date

class QDB2(Module):
	def __init__(self):
		self.modname = ""Fearnode QDB""
		self.qdb_api_post = ""http://qdb.vortigaunt.net/api/send/%s""
		self.qdb_login = ""http://qdb.vortigaunt.net/login/%s""
		self.qdb_api_read = ""http://qdb.vortigaunt.net/api/read/%s""
		self.qdb_secret = ""[redacted]""
		self.quote_users = {}
	
	def parse(self, msg, cmd, user, arg):
		if cmd == "".read"":
			response = Response()
			
			result = requests.get(self.qdb_api_read % arg[0])
			try:
				result = json.loads(result.content)
			
				if result['results'].has_key('success'):
					if result['results']['success'] == 1:
						quote = result['results']['data']['text'].split(""\n"")
						for line in quote:
							if ""\r"" in line or ""\n"" in line:
								for subline in line.split(""\r\n"")
									response.add_action(self.send_message(subline))
							else:
								response.add_action(self.send_message(line))
					else:
						problem = {'hidden_quote': 'The quote is hidden.', 'no_such_quote': 'No such quote exists.'}[result['results']['error']]
						response.add_action(self.send_message(""Error: "" + problem))
			except:
				response.add_action(self.send_message('wodim, arregla el qdb.'))
				
			return self.multiple_response(response.generate_response())
		if cmd == "".password"":
			m = hashlib.md5()
			m.update(date.today().strftime(""%d/%m/%Y"") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			return self.send_message(self.qdb_login % password)
		if msg[:5] == "".send"":
			if user[:2] == ""**"":
				user = user[2:].strip()
			else:
				return self.ignore()
				
			if user not in self.quote_users:
					return self.send_raw_message(""PRIVMSG "" + user + "" :Pega un quote antes."")
				
			quote = """"
			m = hashlib.md5()
			m.update(date.today().strftime(""%d/%m/%Y"") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			if(msg[:13] == "".send_private""):
				comment = msg[13:]
				private = 1
			else:
				comment = msg[5:]
				private = 0
				
			for line in self.quote_users[user]:
				quote = quote + ""\n"" + line
			
			if quote == """":
				return self.ignore()
			
			payload = {'nick': user + ' (bot)', 'text': quote, 'comment': comment, 'hidden': private}
			r = requests.post(self.qdb_api_post % password, data=payload)
			r = json.loads(r.content)
			
			self.quote_users[user] = []
			return self.send_message(r['results']['url'] + "" "" + comment + "" ("" + user + "")"")
		elif cmd == "".cancel"" and user[:2] == ""**"":
			user = user[2:]
			
			self.quote_users[user] = []
			return self.send_raw_message(""PRIVMSG "" + user + "" :Hecho."")
		elif user[:2] == ""**"":
			user = user[2:]
			
			if user in self.quote_users:
				if self.quote_users[user] == []:
					send = True
				else:
					send = False
				
				self.quote_users[user].append(msg)
				
				if send:
					return self.send_raw_message(""PRIVMSG "" + user + "" :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar."")
				else:
					return self.accept()
			else:
				self.quote_users[user] = [msg]
				return self.send_raw_message(""PRIVMSG "" + user + "" :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar."")

		else:
			return self.ignore()
/n/n/n",0
157,157,5f3c4a9a14598f4f706bfccb007ead2ec2816931,"/modules/qdb.py/n/nfrom module import Module
from response.response import Response

import hashlib
import requests
import json
from datetime import date

class QDB2(Module):
	def __init__(self):
		self.modname = ""Fearnode QDB""
		self.qdb_api_post = ""http://qdb.vortigaunt.net/api/send/%s""
		self.qdb_login = ""http://qdb.vortigaunt.net/login/%s""
		self.qdb_api_read = ""http://qdb.vortigaunt.net/api/read/%s""
		self.qdb_secret = ""[redacted]""
		self.quote_users = {}
	
	def parse(self, msg, cmd, user, arg):
		if cmd == "".read"":
			response = Response()
			
			result = requests.get(self.qdb_api_read % arg[0])
			try:
				result = json.loads(result.content)
			
				if result['results'].has_key('success'):
					if result['results']['success'] == 1:
						quote = result['results']['data']['text'].split(""\n"")
						for line in quote:
							response.add_action(self.send_message(line))
					else:
						problem = {'hidden_quote': 'The quote is hidden.', 'no_such_quote': 'No such quote exists.'}[result['results']['error']]
						response.add_action(self.send_message(""Error: "" + problem))
			except:
				response.add_action(self.send_message('wodim, arregla el qdb.'))
				
			return self.multiple_response(response.generate_response())
		if cmd == "".password"":
			m = hashlib.md5()
			m.update(date.today().strftime(""%d/%m/%Y"") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			return self.send_message(self.qdb_login % password)
		if msg[:5] == "".send"":
			if user[:2] == ""**"":
				user = user[2:].strip()
			else:
				return self.ignore()
				
			if user not in self.quote_users:
					return self.send_raw_message(""PRIVMSG "" + user + "" :Pega un quote antes."")
				
			quote = """"
			m = hashlib.md5()
			m.update(date.today().strftime(""%d/%m/%Y"") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			if(msg[:13] == "".send_private""):
				comment = msg[13:]
				private = 1
			else:
				comment = msg[5:]
				private = 0
				
			for line in self.quote_users[user]:
				quote = quote + ""\n"" + line
			
			if quote == """":
				return self.ignore()
			
			payload = {'nick': user + ' (bot)', 'text': quote, 'comment': comment, 'hidden': private}
			r = requests.post(self.qdb_api_post % password, data=payload)
			r = json.loads(r.content)
			
			self.quote_users[user] = []
			return self.send_message(r['results']['url'] + "" "" + comment + "" ("" + user + "")"")
		elif cmd == "".cancel"" and user[:2] == ""**"":
			user = user[2:]
			
			self.quote_users[user] = []
			return self.send_raw_message(""PRIVMSG "" + user + "" :Hecho."")
		elif user[:2] == ""**"":
			user = user[2:]
			
			if user in self.quote_users:
				if self.quote_users[user] == []:
					send = True
				else:
					send = False
				
				self.quote_users[user].append(msg)
				
				if send:
					return self.send_raw_message(""PRIVMSG "" + user + "" :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar."")
				else:
					return self.accept()
			else:
				self.quote_users[user] = [msg]
				return self.send_raw_message(""PRIVMSG "" + user + "" :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar."")

		else:
			return self.ignore()
/n/n/n",1
98,98,801092d1f01896f6c627cea35d1beeb82a533f5a,"coalib/bearlib/abstractions/Lint.py/n/nimport os
import re
import shutil
from subprocess import check_call, CalledProcessError, DEVNULL
import tempfile

from coalib.bears.Bear import Bear
from coalib.misc.Decorators import enforce_signature
from coalib.misc.Shell import escape_path_argument, run_shell_command
from coalib.results.Diff import Diff
from coalib.results.Result import Result
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY


class Lint(Bear):

    """"""
    Deals with the creation of linting bears.

    For the tutorial see:
    http://coala.readthedocs.org/en/latest/Users/Tutorials/Linter_Bears.html

    :param executable:                  The executable to run the linter.
    :param prerequisite_command:        The command to run as a prerequisite
                                        and is of type ``list``.
    :param prerequisites_fail_msg:      The message to be displayed if the
                                        prerequisite fails.
    :param arguments:                   The arguments to supply to the linter,
                                        such that the file name to be analyzed
                                        can be appended to the end. Note that
                                        we use ``.format()`` on the arguments -
                                        so, ``{abc}`` needs to be given as
                                        ``{{abc}}``. Currently, the following
                                        will be replaced:

                                         - ``{filename}`` - The filename passed
                                           to ``lint()``
                                         - ``{config_file}`` - The config file
                                           created using ``config_file()``

    :param output_regex:    The regex which will match the output of the linter
                            to get results. This is not used if
                            ``gives_corrected`` is set. This regex should give
                            out the following variables:

                             - line - The line where the issue starts.
                             - column - The column where the issue starts.
                             - end_line - The line where the issue ends.
                             - end_column - The column where the issue ends.
                             - severity - The severity of the issue.
                             - message - The message of the result.
                             - origin - The origin of the issue.

    :param diff_severity:   The severity to use for all results if
                            ``gives_corrected`` is set.
    :param diff_message:    The message to use for all results if
                            ``gives_corrected`` is set.
    :param use_stderr:      Uses stderr as the output stream is it's True.
    :param use_stdin:       Sends file as stdin instead of giving the file name.
    :param gives_corrected: True if the executable gives the corrected file
                            or just the issues.
    :param severity_map:    A dict where the keys are the possible severity
                            values the Linter gives out and the values are the
                            severity of the coala Result to set it to. If it is
                            not a dict, it is ignored.
    """"""
    executable = None
    prerequisite_command = None
    prerequisite_fail_msg = 'Unknown failure.'
    arguments = """"
    output_regex = re.compile(r'(?P<line>\d+)\.(?P<column>\d+)\|'
                              r'(?P<severity>\d+): (?P<message>.*)')
    diff_message = 'No result message was set'
    diff_severity = RESULT_SEVERITY.NORMAL
    use_stderr = False
    use_stdin = False
    gives_corrected = False
    severity_map = None

    def lint(self, filename=None, file=None):
        """"""
        Takes a file and lints it using the linter variables defined apriori.

        :param filename:  The name of the file to execute.
        :param file:      The contents of the file as a list of strings.
        """"""
        assert ((self.use_stdin and file is not None) or
                (not self.use_stdin and filename is not None))

        config_file = self.generate_config_file()
        self.command = self._create_command(filename=filename,
                                            config_file=config_file)

        stdin_input = """".join(file) if self.use_stdin else None
        stdout_output, stderr_output = run_shell_command(self.command,
                                                         stdin=stdin_input,
                                                         shell=True)
        self.stdout_output = tuple(stdout_output.splitlines(keepends=True))
        self.stderr_output = tuple(stderr_output.splitlines(keepends=True))
        results_output = (self.stderr_output if self.use_stderr
                          else self.stdout_output)
        results = self.process_output(results_output, filename, file)
        if not self.use_stderr:
            self._print_errors(self.stderr_output)

        if config_file:
            os.remove(config_file)

        return results

    def process_output(self, output, filename, file):
        """"""
        Take the output (from stdout or stderr) and use it to create Results.
        If the class variable ``gives_corrected`` is set to True, the
        ``_process_corrected()`` is called. If it is False,
        ``_process_issues()`` is called.

        :param output:   The output to be used to obtain Results from. The
                         output is either stdout or stderr depending on the
                         class variable ``use_stderr``.
        :param filename: The name of the file whose output is being processed.
        :param file:     The contents of the file whose output is being
                         processed.
        :return:         Generator which gives Results produced based on this
                         output.
        """"""
        if self.gives_corrected:
            return self._process_corrected(output, filename, file)
        else:
            return self._process_issues(output, filename)

    def _process_corrected(self, output, filename, file):
        """"""
        Process the output and use it to create Results by creating diffs.
        The diffs are created by comparing the output and the original file.

        :param output:   The corrected file contents.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on the
                         diffs created by comparing the original and corrected
                         contents.
        """"""
        for diff in self.__yield_diffs(file, output):
            yield Result(self,
                         self.diff_message,
                         affected_code=(diff.range(filename),),
                         diffs={filename: diff},
                         severity=self.diff_severity)

    def _process_issues(self, output, filename):
        """"""
        Process the output using the regex provided in ``output_regex`` and
        use it to create Results by using named captured groups from the regex.

        :param output:   The output to be parsed by regex.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on regex
                         matches using the ``output_regex`` provided and the
                         ``output`` parameter.
        """"""
        regex = self.output_regex
        if isinstance(regex, str):
            regex = regex % {""file_name"": filename}

        # Note: We join ``output`` because the regex may want to capture
        #       multiple lines also.
        for match in re.finditer(regex, """".join(output)):
            yield self.match_to_result(match, filename)

    def _get_groupdict(self, match):
        """"""
        Convert a regex match's groups into a dictionary with data to be used
        to create a Result. This is used internally in ``match_to_result``.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The dictionary containing the information:
                         - line - The line where the result starts.
                         - column - The column where the result starts.
                         - end_line - The line where the result ends.
                         - end_column - The column where the result ends.
                         - severity - The severity of the result.
                         - message - The message of the result.
                         - origin - The origin of the result.
        """"""
        groups = match.groupdict()
        if (
                isinstance(self.severity_map, dict) and
                ""severity"" in groups and
                groups[""severity""] in self.severity_map):
            groups[""severity""] = self.severity_map[groups[""severity""]]
        return groups

    def _create_command(self, **kwargs):
        command = self.executable + ' ' + self.arguments
        for key in (""filename"", ""config_file""):
            kwargs[key] = escape_path_argument(kwargs.get(key, """") or """")
        return command.format(**kwargs)

    def _print_errors(self, errors):
        for line in filter(lambda error: bool(error.strip()), errors):
            self.warn(line)

    @staticmethod
    def __yield_diffs(file, new_file):
        if tuple(new_file) != tuple(file):
            wholediff = Diff.from_string_arrays(file, new_file)

            for diff in wholediff.split_diff():
                yield diff

    def match_to_result(self, match, filename):
        """"""
        Convert a regex match's groups into a coala Result object.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The Result object.
        """"""
        groups = self._get_groupdict(match)

        # Pre process the groups
        for variable in (""line"", ""column"", ""end_line"", ""end_column""):
            if variable in groups and groups[variable]:
                groups[variable] = int(groups[variable])

        if ""origin"" in groups:
            groups['origin'] = ""{} ({})"".format(str(self.__class__.__name__),
                                                str(groups[""origin""]))

        return Result.from_values(
            origin=groups.get(""origin"", self),
            message=groups.get(""message"", """"),
            file=filename,
            severity=int(groups.get(""severity"", RESULT_SEVERITY.NORMAL)),
            line=groups.get(""line"", None),
            column=groups.get(""column"", None),
            end_line=groups.get(""end_line"", None),
            end_column=groups.get(""end_column"", None))

    @classmethod
    def check_prerequisites(cls):
        """"""
        Checks for prerequisites required by the Linter Bear.

        It uses the class variables:
        -  ``executable`` - Checks that it is available in the PATH using
        ``shutil.which``.
        -  ``prerequisite_command`` - Checks that when this command is run,
        the exitcode is 0. If it is not zero, ``prerequisite_fail_msg``
        is gives as the failure message.

        If either of them is set to ``None`` that check is ignored.

        :return: True is all checks are valid, else False.
        """"""
        return cls._check_executable_command(
            executable=cls.executable,
            command=cls.prerequisite_command,
            fail_msg=cls.prerequisite_fail_msg)

    @classmethod
    @enforce_signature
    def _check_executable_command(cls, executable,
                                  command: (list, tuple, None), fail_msg):
        """"""
        Checks whether the required executable is found and the
        required command succesfully executes.

        The function is intended be used with classes having an
        executable, prerequisite_command and prerequisite_fail_msg.

        :param executable:   The executable to check for.
        :param command:      The command to check as a prerequisite.
        :param fail_msg:     The fail message to display when the
                             command doesn't return an exitcode of zero.

        :return: True if command successfully executes, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of the error.
        """"""
        if cls._check_executable(executable):
            if command is None:
                return True  # when there are no prerequisites
            try:
                check_call(command, stdout=DEVNULL, stderr=DEVNULL)
                return True
            except (OSError, CalledProcessError):
                return fail_msg
        else:
            return repr(executable) + "" is not installed.""

    @staticmethod
    def _check_executable(executable):
        """"""
        Checks whether the needed executable is present in the system.

        :param executable: The executable to check for.

        :return: True if binary is present, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of what's missing.
        """"""
        if executable is None:
            return True
        return shutil.which(executable) is not None

    def generate_config_file(self):
        """"""
        Generates a temporary config file.
        Note: The user of the function is responsible for deleting the
        tempfile when done with it.

        :return: The file name of the tempfile created.
        """"""
        config_lines = self.config_file()
        config_file = """"
        if config_lines is not None:
            for i, line in enumerate(config_lines):
                config_lines[i] = line if line.endswith(""\n"") else line + ""\n""
            config_fd, config_file = tempfile.mkstemp()
            os.close(config_fd)
            with open(config_file, 'w') as conf_file:
                conf_file.writelines(config_lines)
        return config_file

    @staticmethod
    def config_file():
        """"""
        Returns a configuation file from the section given to the bear.
        The section is available in ``self.section``. To add the config
        file's name generated by this function to the arguments,
        use ``{config_file}``.

        :return: A list of lines of the config file to be used or None.
        """"""
        return None
/n/n/ncoalib/misc/Shell.py/n/nfrom contextlib import contextmanager
import shlex
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """"""
    Runs a single command in shell and provides stdout, stderr and stdin
    streams.

    This function creates a context manager that sets up the process (using
    ``subprocess.Popen()``), returns to caller, closes streams and waits for
    process to exit on leaving.

    Shell execution is disabled by default (so no shell expansion takes place).
    If you want to turn shell execution on, you can pass ``shell=True`` like
    you would do for ``subprocess.Popen()``.

    The process is opened in ``universal_newlines`` mode by default.

    :param command: The command to run on shell. This parameter can either
                    be a sequence of arguments that are directly passed to
                    the process or a string. A string gets splitted beforehand
                    using ``shlex.split()``. If providing ``shell=True`` as a
                    keyword-argument, no ``shlex.split()`` is performed and the
                    command string goes directly to ``subprocess.Popen()``.
    :param kwargs:  Additional keyword arguments to pass to
                    ``subprocess.Popen`` that is used to spawn the process
                    (except ``stdout``, ``stderr``, ``stdin`` and
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """"""
    if not kwargs.get(""shell"", False) and isinstance(command, str):
        command = shlex.split(command)

    process = Popen(command,
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """"""
    Runs a single command in shell and returns the read stdout and stderr data.

    This function waits for the process (created using ``subprocess.Popen()``)
    to exit.

    Shell execution is disabled by default (so no shell expansion takes place).
    If you want to turn shell execution on, you can pass ``shell=True`` like
    you would do for ``subprocess.Popen()``.

    See also ``run_interactive_shell_command()``.

    :param command: The command to run on shell. This parameter can either
                    be a sequence of arguments that are directly passed to
                    the process or a string. A string gets splitted beforehand
                    using ``shlex.split()``.
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to
                    ``subprocess.Popen`` that are used to spawn the process
                    (except ``stdout``, ``stderr``, ``stdin`` and
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A tuple with ``(stdoutstring, stderrstring)``.
    """"""
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """"""
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either ""powershell"" if Windows
             Powershell is detected, ""cmd"" if command prompt is been
             detected or ""sh"" if it's neither of these.
    """"""
    out = run_shell_command(""echo $host.name"", shell=True)[0]
    if out.strip() == ""ConsoleHost"":
        return ""powershell""
    out = run_shell_command(""echo $0"", shell=True)[0]
    if out.strip() == ""$0"":
        return ""cmd""
    return ""sh""


def prepare_string_argument(string, shell=get_shell_type()):
    """"""
    Prepares a string argument for being passed as a parameter on shell.

    On ``sh`` this function effectively encloses the given string
    with quotes (either '' or """", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not ""sh"" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """"""
    if shell == ""sh"":
        return '""' + escape(string, '""') + '""'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """"""
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are ""sh"", ""powershell"", and ""cmd"" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """"""
    if shell == ""cmd"":
        # If a quote ("") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '""' + escape(path, '""', '^') + '""'
    elif shell == ""sh"":
        return escape(path, "" "")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path
/n/n/ntests/misc/ShellTest.py/n/nimport os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = ""sh""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/usr/a-dir/"", _type),
            ""/home/usr/a-dir/"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"",
                                 _type),
            ""/home/us\\ r/a-file\\ with\\ spaces.bla"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-dir with spaces/x/"",
                                 _type),
            ""/home/us\\ r/a-dir\\ with\\ spaces/x/"")
        self.assertEqual(
            escape_path_argument(
                ""relative something/with cherries and/pickles.delicious"",
                _type),
            ""relative\\ something/with\\ cherries\\ and/pickles.delicious"")

    def test_escape_path_argument_cmd(self):
        _type = ""cmd""
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\has-a-weird-shell.txt"", _type),
            ""\""C:\\Windows\\has-a-weird-shell.txt\"""")
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\lolrofl\\dirs\\"", _type),
            ""\""C:\\Windows\\lolrofl\\dirs\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\fi le.exe"", _type),
            ""\""X:\\Users\\Maito Gai\\fi le.exe\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Mai to Gai\\director y\\"",
                                 _type),
            ""\""X:\\Users\\Mai to Gai\\director y\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\\""seven-gates\"".y"",
                                 _type),
            ""\""X:\\Users\\Maito Gai\\^\""seven-gates^\"".y\"""")
        self.assertEqual(
            escape_path_argument(""System32\\my-custom relative tool\\"",
                                 _type),
            ""\""System32\\my-custom relative tool\\\"""")
        self.assertEqual(
            escape_path_argument(""System32\\illegal\"" name \""\"".curd"", _type),
            ""\""System32\\illegal^\"" name ^\""^\"".curd\"""")

    def test_escape_path_argument_unsupported(self):
        _type = ""INVALID""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"", _type),
            ""/home/us r/a-file with spaces.bla"")
        self.assertEqual(
            escape_path_argument(""|home|us r|a*dir with spaces|x|"", _type),
            ""|home|us r|a*dir with spaces|x|"")
        self.assertEqual(
            escape_path_argument(""system|a|b|c?d"", _type),
            ""system|a|b|c?d"")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return (sys.executable,
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             ""run_shell_command_testfiles"",
                             scriptname))

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_interactive_program.py"")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), ""test_program X\n"")
            self.assertEqual(p.stdout.readline(), ""Type in a number:\n"")
            p.stdin.write(""33\n"")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), ""33\n"")
            self.assertEqual(p.stdout.readline(), ""Exiting program.\n"")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"", stdout=None):
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_program.py"")

        stdout, stderr = run_shell_command(command)

        expected = (""test_program Z\n""
                    ""non-interactive mode.\n""
                    ""Exiting...\n"")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, """")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_input_program.py"")

        stdout, stderr = run_shell_command(command, ""1  4  10  22"")

        self.assertEqual(stdout, ""37\n"")
        self.assertEqual(stderr, """")

        stdout, stderr = run_shell_command(command, ""1 p 5"")

        self.assertEqual(stdout, """")
        self.assertEqual(stderr, ""INVALID INPUT\n"")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", weird_parameter2=""abc"")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = (""normal_string"",
                             ""string with spaces"",
                             'string with quotes""a',
                             ""string with s-quotes'b"",
                             ""bsn \n A"",
                             ""unrecognized \\q escape"")

    def test_prepare_string_argument_sh(self):
        expected_results = ('""normal_string""',
                            '""string with spaces""',
                            '""string with quotes\\""a""',
                            '""string with s-quotes\'b""',
                            '""bsn \n A""',
                            '""unrecognized \\q escape""')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, ""sh""),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, ""WeIrD_O/S""),
                             string)
/n/n/n",0
99,99,801092d1f01896f6c627cea35d1beeb82a533f5a,"/coalib/bearlib/abstractions/Lint.py/n/nimport os
import re
import shutil
from subprocess import check_call, CalledProcessError, DEVNULL
import tempfile

from coalib.bears.Bear import Bear
from coalib.misc.Decorators import enforce_signature
from coalib.misc.Shell import escape_path_argument, run_shell_command
from coalib.results.Diff import Diff
from coalib.results.Result import Result
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY


class Lint(Bear):

    """"""
    Deals with the creation of linting bears.

    For the tutorial see:
    http://coala.readthedocs.org/en/latest/Users/Tutorials/Linter_Bears.html

    :param executable:                  The executable to run the linter.
    :param prerequisite_command:        The command to run as a prerequisite
                                        and is of type ``list``.
    :param prerequisites_fail_msg:      The message to be displayed if the
                                        prerequisite fails.
    :param arguments:                   The arguments to supply to the linter,
                                        such that the file name to be analyzed
                                        can be appended to the end. Note that
                                        we use ``.format()`` on the arguments -
                                        so, ``{abc}`` needs to be given as
                                        ``{{abc}}``. Currently, the following
                                        will be replaced:

                                         - ``{filename}`` - The filename passed
                                           to ``lint()``
                                         - ``{config_file}`` - The config file
                                           created using ``config_file()``

    :param output_regex:    The regex which will match the output of the linter
                            to get results. This is not used if
                            ``gives_corrected`` is set. This regex should give
                            out the following variables:

                             - line - The line where the issue starts.
                             - column - The column where the issue starts.
                             - end_line - The line where the issue ends.
                             - end_column - The column where the issue ends.
                             - severity - The severity of the issue.
                             - message - The message of the result.
                             - origin - The origin of the issue.

    :param diff_severity:   The severity to use for all results if
                            ``gives_corrected`` is set.
    :param diff_message:    The message to use for all results if
                            ``gives_corrected`` is set.
    :param use_stderr:      Uses stderr as the output stream is it's True.
    :param use_stdin:       Sends file as stdin instead of giving the file name.
    :param gives_corrected: True if the executable gives the corrected file
                            or just the issues.
    :param severity_map:    A dict where the keys are the possible severity
                            values the Linter gives out and the values are the
                            severity of the coala Result to set it to. If it is
                            not a dict, it is ignored.
    """"""
    executable = None
    prerequisite_command = None
    prerequisite_fail_msg = 'Unknown failure.'
    arguments = """"
    output_regex = re.compile(r'(?P<line>\d+)\.(?P<column>\d+)\|'
                              r'(?P<severity>\d+): (?P<message>.*)')
    diff_message = 'No result message was set'
    diff_severity = RESULT_SEVERITY.NORMAL
    use_stderr = False
    use_stdin = False
    gives_corrected = False
    severity_map = None

    def lint(self, filename=None, file=None):
        """"""
        Takes a file and lints it using the linter variables defined apriori.

        :param filename:  The name of the file to execute.
        :param file:      The contents of the file as a list of strings.
        """"""
        assert ((self.use_stdin and file is not None) or
                (not self.use_stdin and filename is not None))

        config_file = self.generate_config_file()
        self.command = self._create_command(filename=filename,
                                            config_file=config_file)

        stdin_input = """".join(file) if self.use_stdin else None
        stdout_output, stderr_output = run_shell_command(self.command,
                                                         stdin=stdin_input)
        self.stdout_output = tuple(stdout_output.splitlines(keepends=True))
        self.stderr_output = tuple(stderr_output.splitlines(keepends=True))
        results_output = (self.stderr_output if self.use_stderr
                          else self.stdout_output)
        results = self.process_output(results_output, filename, file)
        if not self.use_stderr:
            self._print_errors(self.stderr_output)

        if config_file:
            os.remove(config_file)

        return results

    def process_output(self, output, filename, file):
        """"""
        Take the output (from stdout or stderr) and use it to create Results.
        If the class variable ``gives_corrected`` is set to True, the
        ``_process_corrected()`` is called. If it is False,
        ``_process_issues()`` is called.

        :param output:   The output to be used to obtain Results from. The
                         output is either stdout or stderr depending on the
                         class variable ``use_stderr``.
        :param filename: The name of the file whose output is being processed.
        :param file:     The contents of the file whose output is being
                         processed.
        :return:         Generator which gives Results produced based on this
                         output.
        """"""
        if self.gives_corrected:
            return self._process_corrected(output, filename, file)
        else:
            return self._process_issues(output, filename)

    def _process_corrected(self, output, filename, file):
        """"""
        Process the output and use it to create Results by creating diffs.
        The diffs are created by comparing the output and the original file.

        :param output:   The corrected file contents.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on the
                         diffs created by comparing the original and corrected
                         contents.
        """"""
        for diff in self.__yield_diffs(file, output):
            yield Result(self,
                         self.diff_message,
                         affected_code=(diff.range(filename),),
                         diffs={filename: diff},
                         severity=self.diff_severity)

    def _process_issues(self, output, filename):
        """"""
        Process the output using the regex provided in ``output_regex`` and
        use it to create Results by using named captured groups from the regex.

        :param output:   The output to be parsed by regex.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on regex
                         matches using the ``output_regex`` provided and the
                         ``output`` parameter.
        """"""
        regex = self.output_regex
        if isinstance(regex, str):
            regex = regex % {""file_name"": filename}

        # Note: We join ``output`` because the regex may want to capture
        #       multiple lines also.
        for match in re.finditer(regex, """".join(output)):
            yield self.match_to_result(match, filename)

    def _get_groupdict(self, match):
        """"""
        Convert a regex match's groups into a dictionary with data to be used
        to create a Result. This is used internally in ``match_to_result``.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The dictionary containing the information:
                         - line - The line where the result starts.
                         - column - The column where the result starts.
                         - end_line - The line where the result ends.
                         - end_column - The column where the result ends.
                         - severity - The severity of the result.
                         - message - The message of the result.
                         - origin - The origin of the result.
        """"""
        groups = match.groupdict()
        if (
                isinstance(self.severity_map, dict) and
                ""severity"" in groups and
                groups[""severity""] in self.severity_map):
            groups[""severity""] = self.severity_map[groups[""severity""]]
        return groups

    def _create_command(self, **kwargs):
        command = self.executable + ' ' + self.arguments
        for key in (""filename"", ""config_file""):
            kwargs[key] = escape_path_argument(kwargs.get(key, """") or """")
        return command.format(**kwargs)

    def _print_errors(self, errors):
        for line in filter(lambda error: bool(error.strip()), errors):
            self.warn(line)

    @staticmethod
    def __yield_diffs(file, new_file):
        if tuple(new_file) != tuple(file):
            wholediff = Diff.from_string_arrays(file, new_file)

            for diff in wholediff.split_diff():
                yield diff

    def match_to_result(self, match, filename):
        """"""
        Convert a regex match's groups into a coala Result object.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The Result object.
        """"""
        groups = self._get_groupdict(match)

        # Pre process the groups
        for variable in (""line"", ""column"", ""end_line"", ""end_column""):
            if variable in groups and groups[variable]:
                groups[variable] = int(groups[variable])

        if ""origin"" in groups:
            groups['origin'] = ""{} ({})"".format(str(self.__class__.__name__),
                                                str(groups[""origin""]))

        return Result.from_values(
            origin=groups.get(""origin"", self),
            message=groups.get(""message"", """"),
            file=filename,
            severity=int(groups.get(""severity"", RESULT_SEVERITY.NORMAL)),
            line=groups.get(""line"", None),
            column=groups.get(""column"", None),
            end_line=groups.get(""end_line"", None),
            end_column=groups.get(""end_column"", None))

    @classmethod
    def check_prerequisites(cls):
        """"""
        Checks for prerequisites required by the Linter Bear.

        It uses the class variables:
        -  ``executable`` - Checks that it is available in the PATH using
        ``shutil.which``.
        -  ``prerequisite_command`` - Checks that when this command is run,
        the exitcode is 0. If it is not zero, ``prerequisite_fail_msg``
        is gives as the failure message.

        If either of them is set to ``None`` that check is ignored.

        :return: True is all checks are valid, else False.
        """"""
        return cls._check_executable_command(
            executable=cls.executable,
            command=cls.prerequisite_command,
            fail_msg=cls.prerequisite_fail_msg)

    @classmethod
    @enforce_signature
    def _check_executable_command(cls, executable,
                                  command: (list, tuple, None), fail_msg):
        """"""
        Checks whether the required executable is found and the
        required command succesfully executes.

        The function is intended be used with classes having an
        executable, prerequisite_command and prerequisite_fail_msg.

        :param executable:   The executable to check for.
        :param command:      The command to check as a prerequisite.
        :param fail_msg:     The fail message to display when the
                             command doesn't return an exitcode of zero.

        :return: True if command successfully executes, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of the error.
        """"""
        if cls._check_executable(executable):
            if command is None:
                return True  # when there are no prerequisites
            try:
                check_call(command, stdout=DEVNULL, stderr=DEVNULL)
                return True
            except (OSError, CalledProcessError):
                return fail_msg
        else:
            return repr(executable) + "" is not installed.""

    @staticmethod
    def _check_executable(executable):
        """"""
        Checks whether the needed executable is present in the system.

        :param executable: The executable to check for.

        :return: True if binary is present, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of what's missing.
        """"""
        if executable is None:
            return True
        return shutil.which(executable) is not None

    def generate_config_file(self):
        """"""
        Generates a temporary config file.
        Note: The user of the function is responsible for deleting the
        tempfile when done with it.

        :return: The file name of the tempfile created.
        """"""
        config_lines = self.config_file()
        config_file = """"
        if config_lines is not None:
            for i, line in enumerate(config_lines):
                config_lines[i] = line if line.endswith(""\n"") else line + ""\n""
            config_fd, config_file = tempfile.mkstemp()
            os.close(config_fd)
            with open(config_file, 'w') as conf_file:
                conf_file.writelines(config_lines)
        return config_file

    @staticmethod
    def config_file():
        """"""
        Returns a configuation file from the section given to the bear.
        The section is available in ``self.section``. To add the config
        file's name generated by this function to the arguments,
        use ``{config_file}``.

        :return: A list of lines of the config file to be used or None.
        """"""
        return None
/n/n/n/coalib/misc/Shell.py/n/nfrom contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """"""
    Runs a command in shell and provides stdout, stderr and stdin streams.

    This function creates a context manager that sets up the process, returns
    to caller, closes streams and waits for process to exit on leaving.

    The process is opened in ``universal_newlines`` mode.

    :param command: The command to run on shell.
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``
                    that is used to spawn the process (except ``shell``,
                    ``stdout``, ``stderr``, ``stdin`` and
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """"""
    process = Popen(command,
                    shell=True,
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """"""
    Runs a command in shell and returns the read stdout and stderr data.

    This function waits for the process to exit.

    :param command: The command to run on shell.
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``
                    that is used to spawn the process (except ``shell``,
                    ``stdout``, ``stderr``, ``stdin`` and
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A tuple with ``(stdoutstring, stderrstring)``.
    """"""
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """"""
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either ""powershell"" if Windows
             Powershell is detected, ""cmd"" if command prompt is been
             detected or ""sh"" if it's neither of these.
    """"""
    out = run_shell_command(""echo $host.name"")[0]
    if out.strip() == ""ConsoleHost"":
        return ""powershell""
    out = run_shell_command(""echo $0"")[0]
    if out.strip() == ""$0"":
        return ""cmd""
    return ""sh""


def prepare_string_argument(string, shell=get_shell_type()):
    """"""
    Prepares a string argument for being passed as a parameter on shell.

    On ``sh`` this function effectively encloses the given string
    with quotes (either '' or """", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not ""sh"" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """"""
    if shell == ""sh"":
        return '""' + escape(string, '""') + '""'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """"""
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are ""sh"", ""powershell"", and ""cmd"" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """"""
    if shell == ""cmd"":
        # If a quote ("") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '""' + escape(path, '""', '^') + '""'
    elif shell == ""sh"":
        return escape(path, "" "")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path
/n/n/n/tests/misc/ShellTest.py/n/nimport os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = ""sh""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/usr/a-dir/"", _type),
            ""/home/usr/a-dir/"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"",
                                 _type),
            ""/home/us\\ r/a-file\\ with\\ spaces.bla"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-dir with spaces/x/"",
                                 _type),
            ""/home/us\\ r/a-dir\\ with\\ spaces/x/"")
        self.assertEqual(
            escape_path_argument(
                ""relative something/with cherries and/pickles.delicious"",
                _type),
            ""relative\\ something/with\\ cherries\\ and/pickles.delicious"")

    def test_escape_path_argument_cmd(self):
        _type = ""cmd""
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\has-a-weird-shell.txt"", _type),
            ""\""C:\\Windows\\has-a-weird-shell.txt\"""")
        self.assertEqual(
            escape_path_argument(""C:\\Windows\\lolrofl\\dirs\\"", _type),
            ""\""C:\\Windows\\lolrofl\\dirs\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\fi le.exe"", _type),
            ""\""X:\\Users\\Maito Gai\\fi le.exe\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Mai to Gai\\director y\\"",
                                 _type),
            ""\""X:\\Users\\Mai to Gai\\director y\\\"""")
        self.assertEqual(
            escape_path_argument(""X:\\Users\\Maito Gai\\\""seven-gates\"".y"",
                                 _type),
            ""\""X:\\Users\\Maito Gai\\^\""seven-gates^\"".y\"""")
        self.assertEqual(
            escape_path_argument(""System32\\my-custom relative tool\\"",
                                 _type),
            ""\""System32\\my-custom relative tool\\\"""")
        self.assertEqual(
            escape_path_argument(""System32\\illegal\"" name \""\"".curd"", _type),
            ""\""System32\\illegal^\"" name ^\""^\"".curd\"""")

    def test_escape_path_argument_unsupported(self):
        _type = ""INVALID""
        self.assertEqual(
            escape_path_argument(""/home/usr/a-file"", _type),
            ""/home/usr/a-file"")
        self.assertEqual(
            escape_path_argument(""/home/us r/a-file with spaces.bla"", _type),
            ""/home/us r/a-file with spaces.bla"")
        self.assertEqual(
            escape_path_argument(""|home|us r|a*dir with spaces|x|"", _type),
            ""|home|us r|a*dir with spaces|x|"")
        self.assertEqual(
            escape_path_argument(""system|a|b|c?d"", _type),
            ""system|a|b|c?d"")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return "" "".join(
            escape_path_argument(s) for s in (
                sys.executable,
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             ""run_shell_command_testfiles"",
                             scriptname)))

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_interactive_program.py"")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), ""test_program X\n"")
            self.assertEqual(p.stdout.readline(), ""Type in a number:\n"")
            p.stdin.write(""33\n"")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), ""33\n"")
            self.assertEqual(p.stdout.readline(), ""Exiting program.\n"")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command(""some_command"", shell=False):
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_program.py"")

        stdout, stderr = run_shell_command(command)

        expected = (""test_program Z\n""
                    ""non-interactive mode.\n""
                    ""Exiting...\n"")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, """")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            ""test_input_program.py"")

        stdout, stderr = run_shell_command(command, ""1  4  10  22"")

        self.assertEqual(stdout, ""37\n"")
        self.assertEqual(stderr, """")

        stdout, stderr = run_shell_command(command, ""1 p 5"")

        self.assertEqual(stdout, """")
        self.assertEqual(stderr, ""INVALID INPUT\n"")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", weird_parameter2=""abc"")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command(""super-cool-command"", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = (""normal_string"",
                             ""string with spaces"",
                             'string with quotes""a',
                             ""string with s-quotes'b"",
                             ""bsn \n A"",
                             ""unrecognized \\q escape"")

    def test_prepare_string_argument_sh(self):
        expected_results = ('""normal_string""',
                            '""string with spaces""',
                            '""string with quotes\\""a""',
                            '""string with s-quotes\'b""',
                            '""bsn \n A""',
                            '""unrecognized \\q escape""')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, ""sh""),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, ""WeIrD_O/S""),
                             string)
/n/n/n",1
38,38,f4ac975d776092dfe881ec63fd398c1ab851365b,"curtin/block/__init__.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

from contextlib import contextmanager
import errno
import itertools
import os
import stat
import sys
import tempfile

from curtin import util
from curtin.block import lvm
from curtin.log import LOG
from curtin.udev import udevadm_settle


def get_dev_name_entry(devname):
    """"""
    convert device name to path in /dev
    """"""
    bname = devname.split('/dev/')[-1]
    return (bname, ""/dev/"" + bname)


def is_valid_device(devname):
    """"""
    check if device is a valid device
    """"""
    devent = get_dev_name_entry(devname)[1]
    return is_block_device(devent)


def is_block_device(path):
    """"""
    check if path is a block device
    """"""
    try:
        return stat.S_ISBLK(os.stat(path).st_mode)
    except OSError as e:
        if not util.is_file_not_found_exc(e):
            raise
    return False


def dev_short(devname):
    """"""
    get short form of device name
    """"""
    devname = os.path.normpath(devname)
    if os.path.sep in devname:
        return os.path.basename(devname)
    return devname


def dev_path(devname):
    """"""
    convert device name to path in /dev
    """"""
    if devname.startswith('/dev/'):
        return devname
    else:
        return '/dev/' + devname


def path_to_kname(path):
    """"""
    converts a path in /dev or a path in /sys/block to the device kname,
    taking special devices and unusual naming schemes into account
    """"""
    # if path given is a link, get real path
    # only do this if given a path though, if kname is already specified then
    # this would cause a failure where the function should still be able to run
    if os.path.sep in path:
        path = os.path.realpath(path)
    # using basename here ensures that the function will work given a path in
    # /dev, a kname, or a path in /sys/block as an arg
    dev_kname = os.path.basename(path)
    # cciss devices need to have 'cciss!' prepended
    if path.startswith('/dev/cciss'):
        dev_kname = 'cciss!' + dev_kname
    return dev_kname


def kname_to_path(kname):
    """"""
    converts a kname to a path in /dev, taking special devices and unusual
    naming schemes into account
    """"""
    # if given something that is already a dev path, return it
    if os.path.exists(kname) and is_valid_device(kname):
        path = kname
        return os.path.realpath(path)
    # adding '/dev' to path is not sufficient to handle cciss devices and
    # possibly other special devices which have not been encountered yet
    path = os.path.realpath(os.sep.join(['/dev'] + kname.split('!')))
    # make sure path we get is correct
    if not (os.path.exists(path) and is_valid_device(path)):
        raise OSError('could not get path to dev from kname: {}'.format(kname))
    return path


def partition_kname(disk_kname, partition_number):
    """"""
    Add number to disk_kname prepending a 'p' if needed
    """"""
    for dev_type in ['nvme', 'mmcblk', 'cciss', 'mpath', 'dm', 'md']:
        if disk_kname.startswith(dev_type):
            partition_number = ""p%s"" % partition_number
            break
    return ""%s%s"" % (disk_kname, partition_number)


def sysfs_to_devpath(sysfs_path):
    """"""
    convert a path in /sys/class/block to a path in /dev
    """"""
    path = kname_to_path(path_to_kname(sysfs_path))
    if not is_block_device(path):
        raise ValueError('could not find blockdev for sys path: {}'
                         .format(sysfs_path))
    return path


def sys_block_path(devname, add=None, strict=True):
    """"""
    get path to device in /sys/class/block
    """"""
    toks = ['/sys/class/block']
    # insert parent dev if devname is partition
    devname = os.path.normpath(devname)
    (parent, partnum) = get_blockdev_for_partition(devname, strict=strict)
    if partnum:
        toks.append(path_to_kname(parent))

    toks.append(path_to_kname(devname))

    if add is not None:
        toks.append(add)
    path = os.sep.join(toks)

    if strict and not os.path.exists(path):
        err = OSError(
            ""devname '{}' did not have existing syspath '{}'"".format(
                devname, path))
        err.errno = errno.ENOENT
        raise err

    return os.path.normpath(path)


def get_holders(device):
    """"""
    Look up any block device holders, return list of knames
    """"""
    # block.sys_block_path works when given a /sys or /dev path
    sysfs_path = sys_block_path(device)
    # get holders
    holders = os.listdir(os.path.join(sysfs_path, 'holders'))
    LOG.debug(""devname '%s' had holders: %s"", device, holders)
    return holders


def get_device_slave_knames(device):
    """"""
    Find the underlying knames of a given device by walking sysfs
    recursively.

    Returns a list of knames
    """"""
    slave_knames = []
    slaves_dir_path = os.path.join(sys_block_path(device), 'slaves')

    # if we find a 'slaves' dir, recurse and check
    # the underlying devices
    if os.path.exists(slaves_dir_path):
        slaves = os.listdir(slaves_dir_path)
        if len(slaves) > 0:
            for slave_kname in slaves:
                slave_knames.extend(get_device_slave_knames(slave_kname))
        else:
            slave_knames.append(path_to_kname(device))

        return slave_knames
    else:
        # if a device has no 'slaves' attribute then
        # we've found the underlying device, return
        # the kname of the device
        return [path_to_kname(device)]


def _lsblock_pairs_to_dict(lines):
    """"""
    parse lsblock output and convert to dict
    """"""
    ret = {}
    for line in lines.splitlines():
        toks = util.shlex_split(line)
        cur = {}
        for tok in toks:
            k, v = tok.split(""="", 1)
            cur[k] = v
        # use KNAME, as NAME may include spaces and other info,
        # for example, lvm decices may show 'dm0 lvm1'
        cur['device_path'] = get_dev_name_entry(cur['KNAME'])[1]
        ret[cur['KNAME']] = cur
    return ret


def _lsblock(args=None):
    """"""
    get lsblock data as dict
    """"""
    # lsblk  --help | sed -n '/Available/,/^$/p' |
    #     sed -e 1d -e '$d' -e 's,^[ ]\+,,' -e 's, .*,,' | sort
    keys = ['ALIGNMENT', 'DISC-ALN', 'DISC-GRAN', 'DISC-MAX', 'DISC-ZERO',
            'FSTYPE', 'GROUP', 'KNAME', 'LABEL', 'LOG-SEC', 'MAJ:MIN',
            'MIN-IO', 'MODE', 'MODEL', 'MOUNTPOINT', 'NAME', 'OPT-IO', 'OWNER',
            'PHY-SEC', 'RM', 'RO', 'ROTA', 'RQ-SIZE', 'SCHED', 'SIZE', 'STATE',
            'TYPE', 'UUID']
    if args is None:
        args = []
    args = [x.replace('!', '/') for x in args]

    # in order to avoid a very odd error with '-o' and all output fields above
    # we just drop one.  doesn't really matter which one.
    keys.remove('SCHED')
    basecmd = ['lsblk', '--noheadings', '--bytes', '--pairs',
               '--output=' + ','.join(keys)]
    (out, _err) = util.subp(basecmd + list(args), capture=True)
    out = out.replace('!', '/')
    return _lsblock_pairs_to_dict(out)


def get_unused_blockdev_info():
    """"""
    return a list of unused block devices.
    These are devices that do not have anything mounted on them.
    """"""

    # get a list of top level block devices, then iterate over it to get
    # devices dependent on those.  If the lsblk call for that specific
    # call has nothing 'MOUNTED"", then this is an unused block device
    bdinfo = _lsblock(['--nodeps'])
    unused = {}
    for devname, data in bdinfo.items():
        cur = _lsblock([data['device_path']])
        mountpoints = [x for x in cur if cur[x].get('MOUNTPOINT')]
        if len(mountpoints) == 0:
            unused[devname] = data
    return unused


def get_devices_for_mp(mountpoint):
    """"""
    return a list of devices (full paths) used by the provided mountpoint
    """"""
    bdinfo = _lsblock()
    found = set()
    for devname, data in bdinfo.items():
        if data['MOUNTPOINT'] == mountpoint:
            found.add(data['device_path'])

    if found:
        return list(found)

    # for some reason, on some systems, lsblk does not list mountpoint
    # for devices that are mounted.  This happens on /dev/vdc1 during a run
    # using tools/launch.
    mountpoint = [os.path.realpath(dev)
                  for (dev, mp, vfs, opts, freq, passno) in
                  get_proc_mounts() if mp == mountpoint]

    return mountpoint


def get_installable_blockdevs(include_removable=False, min_size=1024**3):
    """"""
    find blockdevs suitable for installation
    """"""
    good = []
    unused = get_unused_blockdev_info()
    for devname, data in unused.items():
        if not include_removable and data.get('RM') == ""1"":
            continue
        if data.get('RO') != ""0"" or data.get('TYPE') != ""disk"":
            continue
        if min_size is not None and int(data.get('SIZE', '0')) < min_size:
            continue
        good.append(devname)
    return good


def get_blockdev_for_partition(devpath, strict=True):
    """"""
    find the parent device for a partition.
    returns a tuple of the parent block device and the partition number
    if device is not a partition, None will be returned for partition number
    """"""
    # normalize path
    rpath = os.path.realpath(devpath)

    # convert an entry in /dev/ to parent disk and partition number
    # if devpath is a block device and not a partition, return (devpath, None)
    base = '/sys/class/block'

    # input of /dev/vdb, /dev/disk/by-label/foo, /sys/block/foo,
    # /sys/block/class/foo, or just foo
    syspath = os.path.join(base, path_to_kname(devpath))

    # don't need to try out multiple sysfs paths as path_to_kname handles cciss
    if strict and not os.path.exists(syspath):
        raise OSError(""%s had no syspath (%s)"" % (devpath, syspath))

    ptpath = os.path.join(syspath, ""partition"")
    if not os.path.exists(ptpath):
        return (rpath, None)

    ptnum = util.load_file(ptpath).rstrip()

    # for a partition, real syspath is something like:
    # /sys/devices/pci0000:00/0000:00:04.0/virtio1/block/vda/vda1
    rsyspath = os.path.realpath(syspath)
    disksyspath = os.path.dirname(rsyspath)

    diskmajmin = util.load_file(os.path.join(disksyspath, ""dev"")).rstrip()
    diskdevpath = os.path.realpath(""/dev/block/%s"" % diskmajmin)

    # diskdevpath has something like 253:0
    # and udev has put links in /dev/block/253:0 to the device name in /dev/
    return (diskdevpath, ptnum)


def get_sysfs_partitions(device):
    """"""
    get a list of sysfs paths for partitions under a block device
    accepts input as a device kname, sysfs path, or dev path
    returns empty list if no partitions available
    """"""
    sysfs_path = sys_block_path(device)
    return [sys_block_path(kname) for kname in os.listdir(sysfs_path)
            if os.path.exists(os.path.join(sysfs_path, kname, 'partition'))]


def get_pardevs_on_blockdevs(devs):
    """"""
    return a dict of partitions with their info that are on provided devs
    """"""
    if devs is None:
        devs = []
    devs = [get_dev_name_entry(d)[1] for d in devs]
    found = _lsblock(devs)
    ret = {}
    for short in found:
        if found[short]['device_path'] not in devs:
            ret[short] = found[short]
    return ret


def stop_all_unused_multipath_devices():
    """"""
    Stop all unused multipath devices.
    """"""
    multipath = util.which('multipath')

    # Command multipath is not available only when multipath-tools package
    # is not installed. Nothing needs to be done in this case because system
    # doesn't create multipath devices without this package installed and we
    # have nothing to stop.
    if not multipath:
        return

    # Command multipath -F flushes all unused multipath device maps
    cmd = [multipath, '-F']
    try:
        # unless multipath cleared *everything* it will exit with 1
        util.subp(cmd, rcs=[0, 1])
    except util.ProcessExecutionError as e:
        LOG.warn(""Failed to stop multipath devices: %s"", e)


def rescan_block_devices():
    """"""
    run 'blockdev --rereadpt' for all block devices not currently mounted
    """"""
    unused = get_unused_blockdev_info()
    devices = []
    for devname, data in unused.items():
        if data.get('RM') == ""1"":
            continue
        if data.get('RO') != ""0"" or data.get('TYPE') != ""disk"":
            continue
        devices.append(data['device_path'])

    if not devices:
        LOG.debug(""no devices found to rescan"")
        return

    cmd = ['blockdev', '--rereadpt'] + devices
    try:
        util.subp(cmd, capture=True)
    except util.ProcessExecutionError as e:
        # FIXME: its less than ideal to swallow this error, but until
        # we fix LP: #1489521 we kind of need to.
        LOG.warn(""Error rescanning devices, possibly known issue LP: #1489521"")
        # Reformatting the exception output so as to not trigger
        # vmtest scanning for Unexepected errors in install logfile
        LOG.warn(""cmd: %s\nstdout:%s\nstderr:%s\nexit_code:%s"", e.cmd,
                 e.stdout, e.stderr, e.exit_code)

    udevadm_settle()

    return


def blkid(devs=None, cache=True):
    """"""
    get data about block devices from blkid and convert to dict
    """"""
    if devs is None:
        devs = []

    # 14.04 blkid reads undocumented /dev/.blkid.tab
    # man pages mention /run/blkid.tab and /etc/blkid.tab
    if not cache:
        cfiles = (""/run/blkid/blkid.tab"", ""/dev/.blkid.tab"", ""/etc/blkid.tab"")
        for cachefile in cfiles:
            if os.path.exists(cachefile):
                os.unlink(cachefile)

    cmd = ['blkid', '-o', 'full']
    cmd.extend(devs)
    # blkid output is <device_path>: KEY=VALUE
    # where KEY is TYPE, UUID, PARTUUID, LABEL
    out, err = util.subp(cmd, capture=True)
    data = {}
    for line in out.splitlines():
        curdev, curdata = line.split("":"", 1)
        data[curdev] = dict(tok.split('=', 1)
                            for tok in util.shlex_split(curdata))
    return data


def detect_multipath(target_mountpoint):
    """"""
    Detect if the operating system has been installed to a multipath device.
    """"""
    # The obvious way to detect multipath is to use multipath utility which is
    # provided by the multipath-tools package. Unfortunately, multipath-tools
    # package is not available in all ephemeral images hence we can't use it.
    # Another reasonable way to detect multipath is to look for two (or more)
    # devices with the same World Wide Name (WWN) which can be fetched using
    # scsi_id utility. This way doesn't work as well because WWNs are not
    # unique in some cases which leads to false positives which may prevent
    # system from booting (see LP: #1463046 for details).
    # Taking into account all the issues mentioned above, curent implementation
    # detects multipath by looking for a filesystem with the same UUID
    # as the target device. It relies on the fact that all alternative routes
    # to the same disk observe identical partition information including UUID.
    # There are some issues with this approach as well though. We won't detect
    # multipath disk if it doesn't any filesystems.  Good news is that
    # target disk will always have a filesystem because curtin creates them
    # while installing the system.
    rescan_block_devices()
    binfo = blkid(cache=False)
    LOG.debug(""detect_multipath found blkid info: %s"", binfo)
    # get_devices_for_mp may return multiple devices by design. It is not yet
    # implemented but it should return multiple devices when installer creates
    # separate disk partitions for / and /boot. We need to do UUID-based
    # multipath detection against each of target devices.
    target_devs = get_devices_for_mp(target_mountpoint)
    LOG.debug(""target_devs: %s"" % target_devs)
    for devpath, data in binfo.items():
        # We need to figure out UUID of the target device first
        if devpath not in target_devs:
            continue
        # This entry contains information about one of target devices
        target_uuid = data.get('UUID')
        # UUID-based multipath detection won't work if target partition
        # doesn't have UUID assigned
        if not target_uuid:
            LOG.warn(""Target partition %s doesn't have UUID assigned"",
                     devpath)
            continue
        LOG.debug(""%s: %s"" % (devpath, data.get('UUID', """")))
        # Iterating over available devices to see if any other device
        # has the same UUID as the target device. If such device exists
        # we probably installed the system to the multipath device.
        for other_devpath, other_data in binfo.items():
            if ((other_data.get('UUID') == target_uuid) and
                    (other_devpath != devpath)):
                return True
    # No other devices have the same UUID as the target devices.
    # We probably installed the system to the non-multipath device.
    return False


def get_scsi_wwid(device, replace_whitespace=False):
    """"""
    Issue a call to scsi_id utility to get WWID of the device.
    """"""
    cmd = ['/lib/udev/scsi_id', '--whitelisted', '--device=%s' % device]
    if replace_whitespace:
        cmd.append('--replace-whitespace')
    try:
        (out, err) = util.subp(cmd, capture=True)
        LOG.debug(""scsi_id output raw:\n%s\nerror:\n%s"", out, err)
        scsi_wwid = out.rstrip('\n')
        return scsi_wwid
    except util.ProcessExecutionError as e:
        LOG.warn(""Failed to get WWID: %s"", e)
        return None


def get_multipath_wwids():
    """"""
    Get WWIDs of all multipath devices available in the system.
    """"""
    multipath_devices = set()
    multipath_wwids = set()
    devuuids = [(d, i['UUID']) for d, i in blkid().items() if 'UUID' in i]
    # Looking for two disks which contain filesystems with the same UUID.
    for (dev1, uuid1), (dev2, uuid2) in itertools.combinations(devuuids, 2):
        if uuid1 == uuid2:
            multipath_devices.add(get_blockdev_for_partition(dev1)[0])
    for device in multipath_devices:
        wwid = get_scsi_wwid(device)
        # Function get_scsi_wwid() may return None in case of errors or
        # WWID field may be empty for some buggy disk. We don't want to
        # propagate both of these value further to avoid generation of
        # incorrect /etc/multipath/bindings file.
        if wwid:
            multipath_wwids.add(wwid)
    return multipath_wwids


def get_root_device(dev, paths=None):
    """"""
    Get root partition for specified device, based on presence of any
    paths in the provided paths list:
    """"""
    if paths is None:
        paths = [""curtin""]
    LOG.debug('Searching for filesystem on %s containing one of: %s',
              dev, paths)
    partitions = get_pardevs_on_blockdevs(dev)
    target = None
    tmp_mount = tempfile.mkdtemp()
    for i in partitions:
        dev_path = partitions[i]['device_path']
        mp = None
        try:
            util.do_mount(dev_path, tmp_mount)
            mp = tmp_mount
            for path in paths:
                fullpath = os.path.join(tmp_mount, path)
                if os.path.isdir(fullpath):
                    target = dev_path
                    LOG.debug(""Found path '%s' on device '%s'"",
                              path, dev_path)
                    break
        except Exception:
            pass
        finally:
            if mp:
                util.do_umount(mp)

    os.rmdir(tmp_mount)
    if target is None:
        raise ValueError(
            ""Did not find any filesystem on %s that contained one of %s"" %
            (dev, paths))
    return target


def get_blockdev_sector_size(devpath):
    """"""
    Get the logical and physical sector size of device at devpath
    Returns a tuple of integer values (logical, physical).
    """"""
    info = _lsblock([devpath])
    LOG.debug('get_blockdev_sector_size: info:\n%s' % util.json_dumps(info))
    # (LP: 1598310) The call to _lsblock() may return multiple results.
    # If it does, then search for a result with the correct device path.
    # If no such device is found among the results, then fall back to previous
    # behavior, which was taking the first of the results
    assert len(info) > 0
    for (k, v) in info.items():
        if v.get('device_path') == devpath:
            parent = k
            break
    else:
        parent = list(info.keys())[0]

    return (int(info[parent]['LOG-SEC']), int(info[parent]['PHY-SEC']))


def get_volume_uuid(path):
    """"""
    Get uuid of disk with given path. This address uniquely identifies
    the device and remains consistant across reboots
    """"""
    (out, _err) = util.subp([""blkid"", ""-o"", ""export"", path], capture=True)
    for line in out.splitlines():
        if ""UUID"" in line:
            return line.split('=')[-1]
    return ''


def get_mountpoints():
    """"""
    Returns a list of all mountpoints where filesystems are currently mounted.
    """"""
    info = _lsblock()
    proc_mounts = [mp for (dev, mp, vfs, opts, freq, passno) in
                   get_proc_mounts()]
    lsblock_mounts = list(i.get(""MOUNTPOINT"") for name, i in info.items() if
                          i.get(""MOUNTPOINT"") is not None and
                          i.get(""MOUNTPOINT"") != """")

    return list(set(proc_mounts + lsblock_mounts))


def get_proc_mounts():
    """"""
    Returns a list of tuples for each entry in /proc/mounts
    """"""
    mounts = []
    with open(""/proc/mounts"", ""r"") as fp:
        for line in fp:
            try:
                (dev, mp, vfs, opts, freq, passno) = \
                    line.strip().split(None, 5)
                mounts.append((dev, mp, vfs, opts, freq, passno))
            except ValueError:
                continue
    return mounts


def get_dev_disk_byid():
    """"""
    Construct a dictionary mapping devname to disk/by-id paths

    :returns: Dictionary populated by examining /dev/disk/by-id/*

    {
     '/dev/sda': '/dev/disk/by-id/virtio-aaaa',
     '/dev/sda1': '/dev/disk/by-id/virtio-aaaa-part1',
    }
    """"""

    prefix = '/dev/disk/by-id'
    return {
        os.path.realpath(byid): byid
        for byid in [os.path.join(prefix, path) for path in os.listdir(prefix)]
    }


def disk_to_byid_path(kname):
    """"""""
    Return a /dev/disk/by-id path to kname if present.
    """"""

    mapping = get_dev_disk_byid()
    return mapping.get(dev_path(kname))


def lookup_disk(serial):
    """"""
    Search for a disk by its serial number using /dev/disk/by-id/
    """"""
    # Get all volumes in /dev/disk/by-id/ containing the serial string. The
    # string specified can be either in the short or long serial format
    # hack, some serials have spaces, udev usually converts ' ' -> '_'
    serial_udev = serial.replace(' ', '_')
    LOG.info('Processing serial %s via udev to %s', serial, serial_udev)

    disks = list(filter(lambda x: serial_udev in x,
                        os.listdir(""/dev/disk/by-id/"")))
    if not disks or len(disks) < 1:
        raise ValueError(""no disk with serial '%s' found"" % serial_udev)

    # Sort by length and take the shortest path name, as the longer path names
    # will be the partitions on the disk. Then use os.path.realpath to
    # determine the path to the block device in /dev/
    disks.sort(key=lambda x: len(x))
    path = os.path.realpath(""/dev/disk/by-id/%s"" % disks[0])

    if not os.path.exists(path):
        raise ValueError(""path '%s' to block device for disk with serial '%s' \
            does not exist"" % (path, serial_udev))
    return path


def sysfs_partition_data(blockdev=None, sysfs_path=None):
    # given block device or sysfs_path, return a list of tuples
    # of (kernel_name, number, offset, size)
    if blockdev:
        blockdev = os.path.normpath(blockdev)
        sysfs_path = sys_block_path(blockdev)
    elif sysfs_path:
        # use normpath to ensure that paths with trailing slash work
        sysfs_path = os.path.normpath(sysfs_path)
        blockdev = os.path.join('/dev', os.path.basename(sysfs_path))
    else:
        raise ValueError(""Blockdev and sysfs_path cannot both be None"")

    # queue property is only on parent devices, ie, we can't read
    # /sys/class/block/vda/vda1/queue/* as queue is only on the
    # parent device
    sysfs_prefix = sysfs_path
    (parent, partnum) = get_blockdev_for_partition(blockdev)
    if partnum:
        sysfs_prefix = sys_block_path(parent)
        partnum = int(partnum)

    block_size = int(util.load_file(os.path.join(
        sysfs_prefix, 'queue/logical_block_size')))
    unit = block_size

    ptdata = []
    for part_sysfs in get_sysfs_partitions(sysfs_prefix):
        data = {}
        for sfile in ('partition', 'start', 'size'):
            dfile = os.path.join(part_sysfs, sfile)
            if not os.path.isfile(dfile):
                continue
            data[sfile] = int(util.load_file(dfile))
        if partnum is None or data['partition'] == partnum:
            ptdata.append((path_to_kname(part_sysfs), data['partition'],
                           data['start'] * unit, data['size'] * unit,))

    return ptdata


def get_part_table_type(device):
    """"""
    check the type of partition table present on the specified device
    returns None if no ptable was present or device could not be read
    """"""
    # it is neccessary to look for the gpt signature first, then the dos
    # signature, because a gpt formatted disk usually has a valid mbr to
    # protect the disk from being modified by older partitioning tools
    return ('gpt' if check_efi_signature(device) else
            'dos' if check_dos_signature(device) else None)


def check_dos_signature(device):
    """"""
    check if there is a dos partition table signature present on device
    """"""
    # the last 2 bytes of a dos partition table have the signature with the
    # value 0xAA55. the dos partition table is always 0x200 bytes long, even if
    # the underlying disk uses a larger logical block size, so the start of
    # this signature must be at 0x1fe
    # https://en.wikipedia.org/wiki/Master_boot_record#Sector_layout
    return (is_block_device(device) and util.file_size(device) >= 0x200 and
            (util.load_file(device, decode=False, read_len=2, offset=0x1fe) ==
             b'\x55\xAA'))


def check_efi_signature(device):
    """"""
    check if there is a gpt partition table signature present on device
    """"""
    # the gpt partition table header is always on lba 1, regardless of the
    # logical block size used by the underlying disk. therefore, a static
    # offset cannot be used, the offset to the start of the table header is
    # always the sector size of the disk
    # the start of the gpt partition table header shoult have the signaure
    # 'EFI PART'.
    # https://en.wikipedia.org/wiki/GUID_Partition_Table
    sector_size = get_blockdev_sector_size(device)[0]
    return (is_block_device(device) and
            util.file_size(device) >= 2 * sector_size and
            (util.load_file(device, decode=False, read_len=8,
                            offset=sector_size) == b'EFI PART'))


def is_extended_partition(device):
    """"""
    check if the specified device path is a dos extended partition
    """"""
    # an extended partition must be on a dos disk, must be a partition, must be
    # within the first 4 partitions and will have a valid dos signature,
    # because the format of the extended partition matches that of a real mbr
    (parent_dev, part_number) = get_blockdev_for_partition(device)
    return (get_part_table_type(parent_dev) in ['dos', 'msdos'] and
            part_number is not None and int(part_number) <= 4 and
            check_dos_signature(device))


def is_zfs_member(device):
    """"""
    check if the specified device path is a zfs member
    """"""
    info = _lsblock()
    kname = path_to_kname(device)
    if kname in info and info[kname].get('FSTYPE') == 'zfs_member':
        return True

    return False


@contextmanager
def exclusive_open(path, exclusive=True):
    """"""
    Obtain an exclusive file-handle to the file/device specified unless
    caller specifics exclusive=False.
    """"""
    mode = 'rb+'
    fd = None
    if not os.path.exists(path):
        raise ValueError(""No such file at path: %s"" % path)

    flags = os.O_RDWR
    if exclusive:
        flags += os.O_EXCL
    try:
        fd = os.open(path, flags)
        try:
            fd_needs_closing = True
            with os.fdopen(fd, mode) as fo:
                yield fo
            fd_needs_closing = False
        except OSError:
            LOG.exception(""Failed to create file-object from fd"")
            raise
        finally:
            # python2 leaves fd open if there os.fdopen fails
            if fd_needs_closing and sys.version_info.major == 2:
                os.close(fd)
    except OSError:
        LOG.error(""Failed to exclusively open path: %s"", path)
        holders = get_holders(path)
        LOG.error('Device holders with exclusive access: %s', holders)
        mount_points = util.list_device_mounts(path)
        LOG.error('Device mounts: %s', mount_points)
        fusers = util.fuser_mount(path)
        LOG.error('Possible users of %s:\n%s', path, fusers)
        raise


def wipe_file(path, reader=None, buflen=4 * 1024 * 1024):
    """"""
    wipe the existing file at path.
    if reader is provided, it will be called as a 'reader(buflen)'
    to provide data for each write.  Otherwise, zeros are used.
    writes will be done in size of buflen.
    """"""
    if reader:
        readfunc = reader
    else:
        buf = buflen * b'\0'

        def readfunc(size):
            return buf

    size = util.file_size(path)
    LOG.debug(""%s is %s bytes. wiping with buflen=%s"",
              path, size, buflen)

    with exclusive_open(path) as fp:
        while True:
            pbuf = readfunc(buflen)
            pos = fp.tell()
            if len(pbuf) != buflen and len(pbuf) + pos < size:
                raise ValueError(
                    ""short read on reader got %d expected %d after %d"" %
                    (len(pbuf), buflen, pos))

            if pos + buflen >= size:
                fp.write(pbuf[0:size-pos])
                break
            else:
                fp.write(pbuf)


def quick_zero(path, partitions=True):
    """"""
    zero 1M at front, 1M at end, and 1M at front
    if this is a block device and partitions is true, then
    zero 1M at front and end of each partition.
    """"""
    buflen = 1024
    count = 1024
    zero_size = buflen * count
    offsets = [0, -zero_size]
    is_block = is_block_device(path)
    if not (is_block or os.path.isfile(path)):
        raise ValueError(""%s: not an existing file or block device"", path)

    pt_names = []
    if partitions and is_block:
        ptdata = sysfs_partition_data(path)
        for kname, ptnum, start, size in ptdata:
            pt_names.append((dev_path(kname), kname, ptnum))
        pt_names.reverse()

    for (pt, kname, ptnum) in pt_names:
        LOG.debug('Wiping path: dev:%s kname:%s partnum:%s',
                  pt, kname, ptnum)
        quick_zero(pt, partitions=False)

    LOG.debug(""wiping 1M on %s at offsets %s"", path, offsets)
    return zero_file_at_offsets(path, offsets, buflen=buflen, count=count)


def zero_file_at_offsets(path, offsets, buflen=1024, count=1024, strict=False,
                         exclusive=True):
    """"""
    write zeros to file at specified offsets
    """"""
    bmsg = ""{path} (size={size}): ""
    m_short = bmsg + ""{tot} bytes from {offset} > size.""
    m_badoff = bmsg + ""invalid offset {offset}.""
    if not strict:
        m_short += "" Shortened to {wsize} bytes.""
        m_badoff += "" Skipping.""

    buf = b'\0' * buflen
    tot = buflen * count
    msg_vals = {'path': path, 'tot': buflen * count}

    # allow caller to control if we require exclusive open
    with exclusive_open(path, exclusive=exclusive) as fp:
        # get the size by seeking to end.
        fp.seek(0, 2)
        size = fp.tell()
        msg_vals['size'] = size

        for offset in offsets:
            if offset < 0:
                pos = size + offset
            else:
                pos = offset
            msg_vals['offset'] = offset
            msg_vals['pos'] = pos
            if pos > size or pos < 0:
                if strict:
                    raise ValueError(m_badoff.format(**msg_vals))
                else:
                    LOG.debug(m_badoff.format(**msg_vals))
                    continue

            msg_vals['wsize'] = size - pos
            if pos + tot > size:
                if strict:
                    raise ValueError(m_short.format(**msg_vals))
                else:
                    LOG.debug(m_short.format(**msg_vals))
            fp.seek(pos)
            for i in range(count):
                pos = fp.tell()
                if pos + buflen > size:
                    fp.write(buf[0:size-pos])
                else:
                    fp.write(buf)


def wipe_volume(path, mode=""superblock""):
    """"""wipe a volume/block device

    :param path: a path to a block device
    :param mode: how to wipe it.
       pvremove: wipe a lvm physical volume
       zero: write zeros to the entire volume
       random: write random data (/dev/urandom) to the entire volume
       superblock: zero the beginning and the end of the volume
       superblock-recursive: zero the beginning of the volume, the end of the
                    volume and beginning and end of any partitions that are
                    known to be on this device.
    """"""
    if mode == ""pvremove"":
        # We need to use --force --force in case it's already in a volgroup and
        # pvremove doesn't want to remove it

        # If pvremove is run and there is no label on the system,
        # then it exits with 5. That is also okay, because we might be
        # wiping something that is already blank
        util.subp(['pvremove', '--force', '--force', '--yes', path],
                  rcs=[0, 5], capture=True)
        lvm.lvm_scan()
    elif mode == ""zero"":
        wipe_file(path)
    elif mode == ""random"":
        with open(""/dev/urandom"", ""rb"") as reader:
            wipe_file(path, reader=reader.read)
    elif mode == ""superblock"":
        quick_zero(path, partitions=False)
    elif mode == ""superblock-recursive"":
        quick_zero(path, partitions=True)
    else:
        raise ValueError(""wipe mode %s not supported"" % mode)


def storage_config_required_packages(storage_config, mapping):
    """"""Read storage configuration dictionary and determine
       which packages are required for the supplied configuration
       to function.  Return a list of packaged to install.
    """"""

    if not storage_config or not isinstance(storage_config, dict):
        raise ValueError('Invalid storage configuration.  '
                         'Must be a dict:\n %s' % storage_config)

    if not mapping or not isinstance(mapping, dict):
        raise ValueError('Invalid storage mapping.  Must be a dict')

    if 'storage' in storage_config:
        storage_config = storage_config.get('storage')

    needed_packages = []

    # get reqs by device operation type
    dev_configs = set(operation['type']
                      for operation in storage_config['config'])

    for dev_type in dev_configs:
        if dev_type in mapping:
            needed_packages.extend(mapping[dev_type])

    # for any format operations, check the fstype and
    # determine if we need any mkfs tools as well.
    format_configs = set([operation['fstype']
                         for operation in storage_config['config']
                         if operation['type'] == 'format'])
    for format_type in format_configs:
        if format_type in mapping:
            needed_packages.extend(mapping[format_type])

    return needed_packages


def detect_required_packages_mapping():
    """"""Return a dictionary providing a versioned configuration which maps
       storage configuration elements to the packages which are required
       for functionality.

       The mapping key is either a config type value, or an fstype value.

    """"""
    version = 1
    mapping = {
        version: {
            'handler': storage_config_required_packages,
            'mapping': {
                'bcache': ['bcache-tools'],
                'btrfs': ['btrfs-tools'],
                'ext2': ['e2fsprogs'],
                'ext3': ['e2fsprogs'],
                'ext4': ['e2fsprogs'],
                'lvm_partition': ['lvm2'],
                'lvm_volgroup': ['lvm2'],
                'raid': ['mdadm'],
                'xfs': ['xfsprogs'],
                'zfs': ['zfsutils-linux', 'zfs-initramfs'],
                'zpool': ['zfsutils-linux', 'zfs-initramfs'],
            },
        },
    }
    return mapping

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/block/clear_holders.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

""""""
This module provides a mechanism for shutting down virtual storage layers on
top of a block device, making it possible to reuse the block device without
having to reboot the system
""""""

import errno
import os
import time

from curtin import (block, udev, util)
from curtin.block import lvm
from curtin.block import mdadm
from curtin.block import zfs
from curtin.log import LOG

# poll frequenty, but wait up to 60 seconds total
MDADM_RELEASE_RETRIES = [0.4] * 150


def _define_handlers_registry():
    """"""
    returns instantiated dev_types
    """"""
    return {
        'partition': {'shutdown': wipe_superblock,
                      'ident': identify_partition},
        'lvm': {'shutdown': shutdown_lvm, 'ident': identify_lvm},
        'crypt': {'shutdown': shutdown_crypt, 'ident': identify_crypt},
        'raid': {'shutdown': shutdown_mdadm, 'ident': identify_mdadm},
        'bcache': {'shutdown': shutdown_bcache, 'ident': identify_bcache},
        'disk': {'ident': lambda x: False, 'shutdown': wipe_superblock},
    }


def get_dmsetup_uuid(device):
    """"""
    get the dm uuid for a specified dmsetup device
    """"""
    blockdev = block.sysfs_to_devpath(device)
    (out, _) = util.subp(['dmsetup', 'info', blockdev, '-C', '-o', 'uuid',
                          '--noheadings'], capture=True)
    return out.strip()


def get_bcache_using_dev(device, strict=True):
    """"""
    Get the /sys/fs/bcache/ path of the bcache cache device bound to
    specified device
    """"""
    # FIXME: when block.bcache is written this should be moved there
    sysfs_path = block.sys_block_path(device)
    path = os.path.realpath(os.path.join(sysfs_path, 'bcache', 'cache'))
    if strict and not os.path.exists(path):
        err = OSError(
            ""device '{}' did not have existing syspath '{}'"".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def get_bcache_sys_path(device, strict=True):
    """"""
    Get the /sys/class/block/<device>/bcache path
    """"""
    sysfs_path = block.sys_block_path(device, strict=strict)
    path = os.path.join(sysfs_path, 'bcache')
    if strict and not os.path.exists(path):
        err = OSError(
            ""device '{}' did not have existing syspath '{}'"".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def maybe_stop_bcache_device(device):
    """"""Attempt to stop the provided device_path or raise unexpected errors.""""""
    bcache_stop = os.path.join(device, 'stop')
    try:
        util.write_file(bcache_stop, '1', mode=None)
    except (IOError, OSError) as e:
        # Note: if we get any exceptions in the above exception classes
        # it is a result of attempting to write ""1"" into the sysfs path
        # The range of errors changes depending on when we race with
        # the kernel asynchronously removing the sysfs path. Therefore
        # we log the exception errno we got, but do not re-raise as
        # the calling process is watching whether the same sysfs path
        # is being removed;  if it fails to go away then we'll have
        # a log of the exceptions to debug.
        LOG.debug('Error writing to bcache stop file %s, device removed: %s',
                  bcache_stop, e)


def shutdown_bcache(device):
    """"""
    Shut down bcache for specified bcache device

    1. Stop the cacheset that `device` is connected to
    2. Stop the 'device'
    """"""
    if not device.startswith('/sys/class/block'):
        raise ValueError('Invalid Device (%s): '
                         'Device path must start with /sys/class/block/',
                         device)

    # bcache device removal should be fast but in an extreme
    # case, might require the cache device to flush large
    # amounts of data to a backing device.  The strategy here
    # is to wait for approximately 30 seconds but to check
    # frequently since curtin cannot proceed until devices
    # cleared.
    removal_retries = [0.2] * 150  # 30 seconds total
    bcache_shutdown_message = ('shutdown_bcache running on {} has determined '
                               'that the device has already been shut down '
                               'during handling of another bcache dev. '
                               'skipping'.format(device))

    if not os.path.exists(device):
        LOG.info(bcache_shutdown_message)
        return

    # get slaves [vdb1, vdc], allow for slaves to not have bcache dir
    slave_paths = [get_bcache_sys_path(k, strict=False) for k in
                   os.listdir(os.path.join(device, 'slaves'))]

    # stop cacheset if it exists
    bcache_cache_sysfs = get_bcache_using_dev(device, strict=False)
    if not os.path.exists(bcache_cache_sysfs):
        LOG.info('bcache cacheset already removed: %s',
                 os.path.basename(bcache_cache_sysfs))
    else:
        LOG.info('stopping bcache cacheset at: %s', bcache_cache_sysfs)
        maybe_stop_bcache_device(bcache_cache_sysfs)
        try:
            util.wait_for_removal(bcache_cache_sysfs, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache cacheset %s', bcache_cache_sysfs)
            raise

        # let kernel settle before the next remove
        udev.udevadm_settle()

    # after stopping cache set, we may need to stop the device
    # both the dev and sysfs entry should be gone.

    # we know the bcacheN device is really gone when we've removed:
    #  /sys/class/block/{bcacheN}
    #  /sys/class/block/slaveN1/bcache
    #  /sys/class/block/slaveN2/bcache
    bcache_block_sysfs = get_bcache_sys_path(device, strict=False)
    to_check = [device] + slave_paths
    found_devs = [os.path.exists(p) for p in to_check]
    LOG.debug('os.path.exists on blockdevs:\n%s',
              list(zip(to_check, found_devs)))
    if not any(found_devs):
        LOG.info('bcache backing device already removed: %s (%s)',
                 bcache_block_sysfs, device)
        LOG.debug('bcache slave paths checked: %s', slave_paths)
        return
    else:
        LOG.info('stopping bcache backing device at: %s', bcache_block_sysfs)
        maybe_stop_bcache_device(bcache_block_sysfs)
        try:
            # wait for them all to go away
            for dev in [device, bcache_block_sysfs] + slave_paths:
                util.wait_for_removal(dev, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache backing device %s',
                     bcache_block_sysfs)
            raise

    return


def shutdown_lvm(device):
    """"""
    Shutdown specified lvm device.
    """"""
    device = block.sys_block_path(device)
    # lvm devices have a dm directory that containes a file 'name' containing
    # '{volume group}-{logical volume}'. The volume can be freed using lvremove
    name_file = os.path.join(device, 'dm', 'name')
    (vg_name, lv_name) = lvm.split_lvm_name(util.load_file(name_file))
    # use two --force flags here in case the volume group that this lv is
    # attached two has been damaged
    LOG.debug('running lvremove on %s/%s', vg_name, lv_name)
    util.subp(['lvremove', '--force', '--force',
               '{}/{}'.format(vg_name, lv_name)], rcs=[0, 5])
    # if that was the last lvol in the volgroup, get rid of volgroup
    if len(lvm.get_lvols_in_volgroup(vg_name)) == 0:
        util.subp(['vgremove', '--force', '--force', vg_name], rcs=[0, 5])
    # refresh lvmetad
    lvm.lvm_scan()


def shutdown_crypt(device):
    """"""
    Shutdown specified cryptsetup device
    """"""
    blockdev = block.sysfs_to_devpath(device)
    util.subp(['cryptsetup', 'remove', blockdev], capture=True)


def shutdown_mdadm(device):
    """"""
    Shutdown specified mdadm device.
    """"""
    blockdev = block.sysfs_to_devpath(device)
    LOG.debug('using mdadm.mdadm_stop on dev: %s', blockdev)
    mdadm.mdadm_stop(blockdev)

    # mdadm stop operation is asynchronous so we must wait for the kernel to
    # release resources. For more details see  LP: #1682456
    try:
        for wait in MDADM_RELEASE_RETRIES:
            if mdadm.md_present(block.path_to_kname(blockdev)):
                time.sleep(wait)
            else:
                LOG.debug('%s has been removed', blockdev)
                break

        if mdadm.md_present(block.path_to_kname(blockdev)):
            raise OSError('Timeout exceeded for removal of %s', blockdev)

    except OSError:
        LOG.critical('Failed to stop mdadm device %s', device)
        if os.path.exists('/proc/mdstat'):
            LOG.critical(""/proc/mdstat:\n%s"", util.load_file('/proc/mdstat'))
        raise


def wipe_superblock(device):
    """"""
    Wrapper for block.wipe_volume compatible with shutdown function interface
    """"""
    blockdev = block.sysfs_to_devpath(device)
    # when operating on a disk that used to have a dos part table with an
    # extended partition, attempting to wipe the extended partition will fail
    if block.is_extended_partition(blockdev):
        LOG.info(""extended partitions do not need wiping, so skipping: '%s'"",
                 blockdev)
    else:
        # release zfs member by exporting the pool
        if block.is_zfs_member(blockdev):
            poolname = zfs.device_to_poolname(blockdev)
            zfs.zpool_export(poolname)

        # some volumes will be claimed by the bcache layer but do not surface
        # an actual /dev/bcacheN device which owns the parts (backing, cache)
        # The result is that some volumes cannot be wiped while bcache claims
        # the device.  Resolve this by stopping bcache layer on those volumes
        # if present.
        for bcache_path in ['bcache', 'bcache/set']:
            stop_path = os.path.join(device, bcache_path)
            if os.path.exists(stop_path):
                LOG.debug('Attempting to release bcache layer from device: %s',
                          device)
                maybe_stop_bcache_device(stop_path)
                continue

        retries = [1, 3, 5, 7]
        LOG.info('wiping superblock on %s', blockdev)
        for attempt, wait in enumerate(retries):
            LOG.debug('wiping %s attempt %s/%s',
                      blockdev, attempt + 1, len(retries))
            try:
                block.wipe_volume(blockdev, mode='superblock')
                LOG.debug('successfully wiped device %s on attempt %s/%s',
                          blockdev, attempt + 1, len(retries))
                return
            except OSError:
                if attempt + 1 >= len(retries):
                    raise
                else:
                    LOG.debug(""wiping device '%s' failed on attempt""
                              "" %s/%s.  sleeping %ss before retry"",
                              blockdev, attempt + 1, len(retries), wait)
                    time.sleep(wait)


def identify_lvm(device):
    """"""
    determine if specified device is a lvm device
    """"""
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('LVM'))


def identify_crypt(device):
    """"""
    determine if specified device is dm-crypt device
    """"""
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('CRYPT'))


def identify_mdadm(device):
    """"""
    determine if specified device is a mdadm device
    """"""
    return block.path_to_kname(device).startswith('md')


def identify_bcache(device):
    """"""
    determine if specified device is a bcache device
    """"""
    return block.path_to_kname(device).startswith('bcache')


def identify_partition(device):
    """"""
    determine if specified device is a partition
    """"""
    path = os.path.join(block.sys_block_path(device), 'partition')
    return os.path.exists(path)


def get_holders(device):
    """"""
    Look up any block device holders, return list of knames
    """"""
    # block.sys_block_path works when given a /sys or /dev path
    sysfs_path = block.sys_block_path(device)
    # get holders
    holders = os.listdir(os.path.join(sysfs_path, 'holders'))
    LOG.debug(""devname '%s' had holders: %s"", device, holders)
    return holders


def gen_holders_tree(device):
    """"""
    generate a tree representing the current storage hirearchy above 'device'
    """"""
    device = block.sys_block_path(device)
    dev_name = block.path_to_kname(device)
    # the holders for a device should consist of the devices in the holders/
    # dir in sysfs and any partitions on the device. this ensures that a
    # storage tree starting from a disk will include all devices holding the
    # disk's partitions
    holder_paths = ([block.sys_block_path(h) for h in get_holders(device)] +
                    block.get_sysfs_partitions(device))
    # the DEV_TYPE registry contains a function under the key 'ident' for each
    # device type entry that returns true if the device passed to it is of the
    # correct type. there should never be a situation in which multiple
    # identify functions return true. therefore, it will always work to take
    # the device type with the first identify function that returns true as the
    # device type for the current device. in the event that no identify
    # functions return true, the device will be treated as a disk
    # (DEFAULT_DEV_TYPE). the identify function for disk never returns true.
    # the next() builtin in python will not raise a StopIteration exception if
    # there is a default value defined
    dev_type = next((k for k, v in DEV_TYPES.items() if v['ident'](device)),
                    DEFAULT_DEV_TYPE)
    return {
        'device': device, 'dev_type': dev_type, 'name': dev_name,
        'holders': [gen_holders_tree(h) for h in holder_paths],
    }


def plan_shutdown_holder_trees(holders_trees):
    """"""
    plan best order to shut down holders in, taking into account high level
    storage layers that may have many devices below them

    returns a sorted list of descriptions of storage config entries including
    their path in /sys/block and their dev type

    can accept either a single storage tree or a list of storage trees assumed
    to start at an equal place in storage hirearchy (i.e. a list of trees
    starting from disk)
    """"""
    # holds a temporary registry of holders to allow cross references
    # key = device sysfs path, value = {} of priority level, shutdown function
    reg = {}

    # normalize to list of trees
    if not isinstance(holders_trees, (list, tuple)):
        holders_trees = [holders_trees]

    def flatten_holders_tree(tree, level=0):
        """"""
        add entries from holders tree to registry with level key corresponding
        to how many layers from raw disks the current device is at
        """"""
        device = tree['device']

        # always go with highest level if current device has been
        # encountered already. since the device and everything above it is
        # re-added to the registry it ensures that any increase of level
        # required here will propagate down the tree
        # this handles a scenario like mdadm + bcache, where the backing
        # device for bcache is a 3nd level item like mdadm, but the cache
        # device is 1st level (disk) or second level (partition), ensuring
        # that the bcache item is always considered higher level than
        # anything else regardless of whether it was added to the tree via
        # the cache device or backing device first
        if device in reg:
            level = max(reg[device]['level'], level)

        reg[device] = {'level': level, 'device': device,
                       'dev_type': tree['dev_type']}

        # handle holders above this level
        for holder in tree['holders']:
            flatten_holders_tree(holder, level=level + 1)

    # flatten the holders tree into the registry
    for holders_tree in holders_trees:
        flatten_holders_tree(holders_tree)

    # return list of entry dicts with highest level first
    return [reg[k] for k in sorted(reg, key=lambda x: reg[x]['level'] * -1)]


def format_holders_tree(holders_tree):
    """"""
    draw a nice dirgram of the holders tree
    """"""
    # spacer styles based on output of 'tree --charset=ascii'
    spacers = (('`-- ', ' ' * 4), ('|-- ', '|' + ' ' * 3))

    def format_tree(tree):
        """"""
        format entry and any subentries
        """"""
        result = [tree['name']]
        holders = tree['holders']
        for (holder_no, holder) in enumerate(holders):
            spacer_style = spacers[min(len(holders) - (holder_no + 1), 1)]
            subtree_lines = format_tree(holder)
            for (line_no, line) in enumerate(subtree_lines):
                result.append(spacer_style[min(line_no, 1)] + line)
        return result

    return '\n'.join(format_tree(holders_tree))


def get_holder_types(tree):
    """"""
    get flattened list of types of holders in holders tree and the devices
    they correspond to
    """"""
    types = {(tree['dev_type'], tree['device'])}
    for holder in tree['holders']:
        types.update(get_holder_types(holder))
    return types


def assert_clear(base_paths):
    """"""
    Check if all paths in base_paths are clear to use
    """"""
    valid = ('disk', 'partition')
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]
    base_paths = [block.sys_block_path(path) for path in base_paths]
    for holders_tree in [gen_holders_tree(p) for p in base_paths]:
        if any(holder_type not in valid and path not in base_paths
               for (holder_type, path) in get_holder_types(holders_tree)):
            raise OSError('Storage not clear, remaining:\n{}'
                          .format(format_holders_tree(holders_tree)))


def clear_holders(base_paths, try_preserve=False):
    """"""
    Clear all storage layers depending on the devices specified in 'base_paths'
    A single device or list of devices can be specified.
    Device paths can be specified either as paths in /dev or /sys/block
    Will throw OSError if any holders could not be shut down
    """"""
    # handle single path
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]

    # get current holders and plan how to shut them down
    holder_trees = [gen_holders_tree(path) for path in base_paths]
    LOG.info('Current device storage tree:\n%s',
             '\n'.join(format_holders_tree(tree) for tree in holder_trees))
    ordered_devs = plan_shutdown_holder_trees(holder_trees)

    # run shutdown functions
    for dev_info in ordered_devs:
        dev_type = DEV_TYPES.get(dev_info['dev_type'])
        shutdown_function = dev_type.get('shutdown')
        if not shutdown_function:
            continue
        if try_preserve and shutdown_function in DATA_DESTROYING_HANDLERS:
            LOG.info('shutdown function for holder type: %s is destructive. '
                     'attempting to preserve data, so not skipping' %
                     dev_info['dev_type'])
            continue
        LOG.info(""shutdown running on holder type: '%s' syspath: '%s'"",
                 dev_info['dev_type'], dev_info['device'])
        shutdown_function(dev_info['device'])
        udev.udevadm_settle()


def start_clear_holders_deps():
    """"""
    prepare system for clear holders to be able to scan old devices
    """"""
    # a mdadm scan has to be started in case there is a md device that needs to
    # be detected. if the scan fails, it is either because there are no mdadm
    # devices on the system, or because there is a mdadm device in a damaged
    # state that could not be started. due to the nature of mdadm tools, it is
    # difficult to know which is the case. if any errors did occur, then ignore
    # them, since no action needs to be taken if there were no mdadm devices on
    # the system, and in the case where there is some mdadm metadata on a disk,
    # but there was not enough to start the array, the call to wipe_volume on
    # all disks and partitions should be sufficient to remove the mdadm
    # metadata
    mdadm.mdadm_assemble(scan=True, ignore_errors=True)
    # the bcache module needs to be present to properly detect bcache devs
    # on some systems (precise without hwe kernel) it may not be possible to
    # lad the bcache module bcause it is not present in the kernel. if this
    # happens then there is no need to halt installation, as the bcache devices
    # will never appear and will never prevent the disk from being reformatted
    util.load_kernel_module('bcache')
    # the zfs module is needed to find and export devices which may be in-use
    # and need to be cleared.
    util.load_kernel_module('zfs')


# anything that is not identified can assumed to be a 'disk' or similar
DEFAULT_DEV_TYPE = 'disk'
# handlers that should not be run if an attempt is being made to preserve data
DATA_DESTROYING_HANDLERS = [wipe_superblock]
# types of devices that could be encountered by clear holders and functions to
# identify them and shut them down
DEV_TYPES = _define_handlers_registry()

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/block/zfs.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

""""""
Wrap calls to the zfsutils-linux package (zpool, zfs) for creating zpools
and volumes.""""""

import os

from curtin import util
from . import blkid

ZPOOL_DEFAULT_PROPERTIES = {
    'ashift': 12,
}

ZFS_DEFAULT_PROPERTIES = {
    'atime': 'off',
    'canmount': 'off',
    'compression': 'lz4',
    'normalization': 'formD',
}


def _join_flags(optflag, params):
    """"""
    Insert optflag for each param in params and return combined list.

    :param optflag: String of the optional flag, like '-o'
    :param params: dictionary of parameter names and values
    :returns: List of strings
    :raises: ValueError: if params are of incorrect type

    Example:
        optflag='-o', params={'foo': 1, 'bar': 2} =>
            ['-o', 'foo=1', '-o', 'bar=2']
    """"""

    if not isinstance(optflag, str) or not optflag:
        raise ValueError(""Invalid optflag: %s"", optflag)

    if not isinstance(params, dict):
        raise ValueError(""Invalid params: %s"", params)

    # zfs flags and params require string booleans ('on', 'off')
    # yaml implicity converts those and others to booleans, we
    # revert that here
    def _b2s(value):
        if not isinstance(value, bool):
            return value
        if value:
            return 'on'
        return 'off'

    return [] if not params else (
        [param for opt in zip([optflag] * len(params),
                              [""%s=%s"" % (k, _b2s(v))
                               for (k, v) in params.items()])
         for param in opt])


def _join_pool_volume(poolname, volume):
    """"""
    Combine poolname and volume.
    """"""
    if not poolname or not volume:
        raise ValueError('Invalid pool (%s) or volume (%s)', poolname, volume)

    return os.path.normpath(""%s/%s"" % (poolname, volume))


def zpool_create(poolname, vdevs, mountpoint=None, altroot=None,
                 pool_properties=None, zfs_properties=None):
    """"""
    Create a zpool called <poolname> comprised of devices specified in <vdevs>.

    :param poolname: String used to name the pool.
    :param vdevs: An iterable of strings of block devices paths which *should*
                  start with '/dev/disk/by-id/' to follow best practices.
    :param pool_properties: A dictionary of key, value pairs to be passed
                            to `zpool create` with the `-o` flag as properties
                            of the zpool.  If value is None, then
                            ZPOOL_DEFAULT_PROPERTIES will be used.
    :param zfs_properties: A dictionary of key, value pairs to be passed
                           to `zpool create` with the `-O` flag as properties
                           of the filesystems created under the pool.  If the
                           value is None, then ZFS_DEFAULT_PROPERTIES will be
                           used.
    :returns: None on success.
    :raises: ValueError: raises exceptions on missing/badd input
    :raises: ProcessExecutionError: raised on unhandled exceptions from
                                    invoking `zpool create`.
    """"""
    if not isinstance(poolname, util.string_types) or not poolname:
        raise ValueError(""Invalid poolname: %s"", poolname)

    if isinstance(vdevs, util.string_types) or isinstance(vdevs, dict):
        raise TypeError(""Invalid vdevs: expected list-like iterable"")
    else:
        try:
            vdevs = list(vdevs)
        except TypeError:
            raise TypeError(""vdevs must be iterable, not: %s"" % str(vdevs))

    if not pool_properties:
        pool_properties = ZPOOL_DEFAULT_PROPERTIES
    if not zfs_properties:
        zfs_properties = ZFS_DEFAULT_PROPERTIES

    options = _join_flags('-o', pool_properties)
    options.extend(_join_flags('-O', zfs_properties))

    if mountpoint:
        options.extend(_join_flags('-O', {'mountpoint': mountpoint}))

    if altroot:
        options.extend(['-R', altroot])

    cmd = [""zpool"", ""create""] + options + [poolname] + vdevs
    util.subp(cmd, capture=True)


def zfs_create(poolname, volume, zfs_properties=None):
    """"""
    Create a filesystem dataset within the specified zpool.

    :param poolname: String used to specify the pool in which to create the
                     filesystem.
    :param volume: String used as the name of the filesystem.
    :param zfs_properties: A dict of properties to be passed
                           to `zfs create` with the `-o` flag as properties
                           of the filesystems created under the pool. If
                           value is None then no properties will be set on
                           the filesystem.
    :returns: None
    :raises: ValueError: raises exceptions on missing/bad input.
    :raises: ProcessExecutionError: raised on unhandled exceptions from
                                    invoking `zfs create`.
    """"""
    if not isinstance(poolname, util.string_types) or not poolname:
        raise ValueError(""Invalid poolname: %s"", poolname)

    if not isinstance(volume, util.string_types) or not volume:
        raise ValueError(""Invalid volume: %s"", volume)

    if not zfs_properties:
        zfs_properties = {}

    if not isinstance(zfs_properties, dict):
        raise ValueError(""Invalid zfs_properties: %s"", zfs_properties)

    options = _join_flags('-o', zfs_properties)

    cmd = [""zfs"", ""create""] + options + [_join_pool_volume(poolname, volume)]
    util.subp(cmd, capture=True)

    # mount volume if it canmount=noauto
    if zfs_properties.get('canmount') == 'noauto':
        zfs_mount(poolname, volume)


def zfs_mount(poolname, volume):
    """"""
    Mount zfs pool/volume

    :param poolname: String used to specify the pool in which to create the
                     filesystem.
    :param volume: String used as the name of the filesystem.
    :returns: None
    :raises: ValueError: raises exceptions on missing/bad input.
    :raises: ProcessExecutionError: raised on unhandled exceptions from
                                    invoking `zfs mount`.
    """"""

    if not isinstance(poolname, util.string_types) or not poolname:
        raise ValueError(""Invalid poolname: %s"", poolname)

    if not isinstance(volume, util.string_types) or not volume:
        raise ValueError(""Invalid volume: %s"", volume)

    cmd = ['zfs', 'mount', _join_pool_volume(poolname, volume)]
    util.subp(cmd, capture=True)


def zpool_list():
    """"""
    Return a list of zfs pool names

    :returns: List of strings
    """"""

    # -H drops the header, -o specifies an attribute to fetch
    out, _err = util.subp(['zpool', 'list', '-H', '-o', 'name'], capture=True)

    return out.splitlines()


def zpool_export(poolname):
    """"""
    Export specified zpool

    :param poolname: String used to specify the pool to export.
    :returns: None
    """"""

    if not isinstance(poolname, util.string_types) or not poolname:
        raise ValueError(""Invalid poolname: %s"", poolname)

    util.subp(['zpool', 'export', poolname])


def device_to_poolname(devname):
    """"""
    Use blkid information to map a devname to a zpool poolname
    stored in in 'LABEL' if devname is a zfs_member and LABEL
    is set.

    :param devname: A block device name
    :returns: String

    Example blkid output on a zfs vdev:
        {'/dev/vdb1': {'LABEL': 'rpool',
                       'PARTUUID': '52dff41a-49be-44b3-a36a-1b499e570e69',
                       'TYPE': 'zfs_member',
                       'UUID': '12590398935543668673',
                       'UUID_SUB': '7809435738165038086'}}

    device_to_poolname('/dev/vdb1') would return 'rpool'
    """"""
    if not isinstance(devname, util.string_types) or not devname:
        raise ValueError(""device_to_poolname: invalid devname: '%s'"" % devname)

    blkid_info = blkid(devs=[devname])
    if not blkid_info or devname not in blkid_info:
        return

    vdev = blkid_info.get(devname)
    vdev_type = vdev.get('TYPE')
    label = vdev.get('LABEL')
    if vdev_type == 'zfs_member' and label:
        return label

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/commands/block_meta.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

from collections import OrderedDict
from curtin import (block, config, util)
from curtin.block import (mdadm, mkfs, clear_holders, lvm, iscsi, zfs)
from curtin.log import LOG
from curtin.reporter import events

from . import populate_one_subcmd
from curtin.udev import compose_udev_equality, udevadm_settle, udevadm_trigger

import glob
import os
import platform
import string
import sys
import tempfile
import time

SIMPLE = 'simple'
SIMPLE_BOOT = 'simple-boot'
CUSTOM = 'custom'
BCACHE_REGISTRATION_RETRY = [0.2] * 60

CMD_ARGUMENTS = (
    ((('-D', '--devices'),
      {'help': 'which devices to operate on', 'action': 'append',
       'metavar': 'DEVICE', 'default': None, }),
     ('--fstype', {'help': 'root partition filesystem type',
                   'choices': ['ext4', 'ext3'], 'default': 'ext4'}),
     (('-t', '--target'),
      {'help': 'chroot to target. default is env[TARGET_MOUNT_POINT]',
       'action': 'store', 'metavar': 'TARGET',
       'default': os.environ.get('TARGET_MOUNT_POINT')}),
     ('--boot-fstype', {'help': 'boot partition filesystem type',
                        'choices': ['ext4', 'ext3'], 'default': None}),
     ('--umount', {'help': 'unmount any mounted filesystems before exit',
                   'action': 'store_true', 'default': False}),
     ('mode', {'help': 'meta-mode to use',
               'choices': [CUSTOM, SIMPLE, SIMPLE_BOOT]}),
     )
)


def block_meta(args):
    # main entry point for the block-meta command.
    state = util.load_command_environment()
    cfg = config.load_command_config(args, state)
    dd_images = util.get_dd_images(cfg.get('sources', {}))
    if ((args.mode == CUSTOM or cfg.get(""storage"") is not None) and
            len(dd_images) == 0):
        meta_custom(args)
    elif args.mode in (SIMPLE, SIMPLE_BOOT) or len(dd_images) > 0:
        meta_simple(args)
    else:
        raise NotImplementedError(""mode=%s is not implemented"" % args.mode)


def logtime(msg, func, *args, **kwargs):
    with util.LogTimer(LOG.debug, msg):
        return func(*args, **kwargs)


def write_image_to_disk(source, dev):
    """"""
    Write disk image to block device
    """"""
    LOG.info('writing image to disk %s, %s', source, dev)
    extractor = {
        'dd-tgz': '|tar -xOzf -',
        'dd-txz': '|tar -xOJf -',
        'dd-tbz': '|tar -xOjf -',
        'dd-tar': '|smtar -xOf -',
        'dd-bz2': '|bzcat',
        'dd-gz': '|zcat',
        'dd-xz': '|xzcat',
        'dd-raw': ''
    }
    (devname, devnode) = block.get_dev_name_entry(dev)
    util.subp(args=['sh', '-c',
                    ('wget ""$1"" --progress=dot:mega -O - ' +
                     extractor[source['type']] + '| dd bs=4M of=""$2""'),
                    '--', source['uri'], devnode])
    util.subp(['partprobe', devnode])
    udevadm_settle()
    paths = [""curtin"", ""system-data/var/lib/snapd""]
    return block.get_root_device([devname], paths=paths)


def get_bootpt_cfg(cfg, enabled=False, fstype=None, root_fstype=None):
    # 'cfg' looks like:
    #   enabled: boolean
    #   fstype: filesystem type (default to 'fstype')
    #   label:  filesystem label (default to 'boot')
    # parm enable can enable, but not disable
    # parm fstype overrides cfg['fstype']
    def_boot = (platform.machine() in ('aarch64') and
                not util.is_uefi_bootable())
    ret = {'enabled': def_boot, 'fstype': None, 'label': 'boot'}
    ret.update(cfg)
    if enabled:
        ret['enabled'] = True

    if ret['enabled'] and not ret['fstype']:
        if root_fstype:
            ret['fstype'] = root_fstype
        if fstype:
            ret['fstype'] = fstype
    return ret


def get_partition_format_type(cfg, machine=None, uefi_bootable=None):
    if machine is None:
        machine = platform.machine()
    if uefi_bootable is None:
        uefi_bootable = util.is_uefi_bootable()

    cfgval = cfg.get('format', None)
    if cfgval:
        return cfgval

    if uefi_bootable:
        return 'uefi'

    if machine in ['aarch64']:
        return 'gpt'
    elif machine.startswith('ppc64'):
        return 'prep'

    return ""mbr""


def devsync(devpath):
    LOG.debug('devsync for %s', devpath)
    util.subp(['partprobe', devpath], rcs=[0, 1])
    udevadm_settle()
    for x in range(0, 10):
        if os.path.exists(devpath):
            LOG.debug('devsync happy - path %s now exists', devpath)
            return
        else:
            LOG.debug('Waiting on device path: %s', devpath)
            time.sleep(1)
    raise OSError('Failed to find device at path: %s', devpath)


def determine_partition_number(partition_id, storage_config):
    vol = storage_config.get(partition_id)
    partnumber = vol.get('number')
    if vol.get('flag') == ""logical"":
        if not partnumber:
            LOG.warn('partition \'number\' key not set in config:\n%s',
                     util.json_dumps(vol))
            partnumber = 5
            for key, item in storage_config.items():
                if item.get('type') == ""partition"" and \
                        item.get('device') == vol.get('device') and\
                        item.get('flag') == ""logical"":
                    if item.get('id') == vol.get('id'):
                        break
                    else:
                        partnumber += 1
    else:
        if not partnumber:
            LOG.warn('partition \'number\' key not set in config:\n%s',
                     util.json_dumps(vol))
            partnumber = 1
            for key, item in storage_config.items():
                if item.get('type') == ""partition"" and \
                        item.get('device') == vol.get('device'):
                    if item.get('id') == vol.get('id'):
                        break
                    else:
                        partnumber += 1
    return partnumber


def sanitize_dname(dname):
    """"""
    dnames should be sanitized before writing rule files, in case maas has
    emitted a dname with a special character

    only letters, numbers and '-' and '_' are permitted, as this will be
    used for a device path. spaces are also not permitted
    """"""
    valid = string.digits + string.ascii_letters + '-_'
    return ''.join(c if c in valid else '-' for c in dname)


def make_dname(volume, storage_config):
    state = util.load_command_environment()
    rules_dir = os.path.join(state['scratch'], ""rules.d"")
    vol = storage_config.get(volume)
    path = get_path_to_storage_volume(volume, storage_config)
    ptuuid = None
    dname = vol.get('name')
    if vol.get('type') in [""partition"", ""disk""]:
        (out, _err) = util.subp([""blkid"", ""-o"", ""export"", path], capture=True,
                                rcs=[0, 2], retries=[1, 1, 1])
        for line in out.splitlines():
            if ""PTUUID"" in line or ""PARTUUID"" in line:
                ptuuid = line.split('=')[-1]
                break
    # we may not always be able to find a uniq identifier on devices with names
    if not ptuuid and vol.get('type') in [""disk"", ""partition""]:
        LOG.warning(""Can't find a uuid for volume: {}. Skipping dname."".format(
            volume))
        return

    rule = [
        compose_udev_equality(""SUBSYSTEM"", ""block""),
        compose_udev_equality(""ACTION"", ""add|change""),
        ]
    if vol.get('type') == ""disk"":
        rule.append(compose_udev_equality('ENV{DEVTYPE}', ""disk""))
        rule.append(compose_udev_equality('ENV{ID_PART_TABLE_UUID}', ptuuid))
    elif vol.get('type') == ""partition"":
        rule.append(compose_udev_equality('ENV{DEVTYPE}', ""partition""))
        dname = storage_config.get(vol.get('device')).get('name') + \
            ""-part%s"" % determine_partition_number(volume, storage_config)
        rule.append(compose_udev_equality('ENV{ID_PART_ENTRY_UUID}', ptuuid))
    elif vol.get('type') == ""raid"":
        md_data = mdadm.mdadm_query_detail(path)
        md_uuid = md_data.get('MD_UUID')
        rule.append(compose_udev_equality(""ENV{MD_UUID}"", md_uuid))
    elif vol.get('type') == ""bcache"":
        rule.append(compose_udev_equality(""ENV{DEVNAME}"", path))
    elif vol.get('type') == ""lvm_partition"":
        volgroup_name = storage_config.get(vol.get('volgroup')).get('name')
        dname = ""%s-%s"" % (volgroup_name, dname)
        rule.append(compose_udev_equality(""ENV{DM_NAME}"", dname))
    else:
        raise ValueError('cannot make dname for device with type: {}'
                         .format(vol.get('type')))

    # note: this sanitization is done here instead of for all name attributes
    #       at the beginning of storage configuration, as some devices, such as
    #       lvm devices may use the name attribute and may permit special chars
    sanitized = sanitize_dname(dname)
    if sanitized != dname:
        LOG.warning(
            ""dname modified to remove invalid chars. old: '{}' new: '{}'""
            .format(dname, sanitized))

    rule.append(""SYMLINK+=\""disk/by-dname/%s\"""" % sanitized)
    LOG.debug(""Writing dname udev rule '{}'"".format(str(rule)))
    util.ensure_dir(rules_dir)
    rule_file = os.path.join(rules_dir, '{}.rules'.format(sanitized))
    util.write_file(rule_file, ', '.join(rule))


def get_poolname(info, storage_config):
    """""" Resolve pool name from zfs info """"""

    LOG.debug('get_poolname for volume {}'.format(info))
    if info.get('type') == 'zfs':
        pool_id = info.get('pool')
        poolname = get_poolname(storage_config.get(pool_id), storage_config)
    elif info.get('type') == 'zpool':
        poolname = info.get('pool')
    else:
        msg = 'volume is not type zfs or zpool: %s' % info
        LOG.error(msg)
        raise ValueError(msg)

    return poolname


def get_path_to_storage_volume(volume, storage_config):
    # Get path to block device for volume. Volume param should refer to id of
    # volume in storage config

    LOG.debug('get_path_to_storage_volume for volume {}'.format(volume))
    devsync_vol = None
    vol = storage_config.get(volume)
    if not vol:
        raise ValueError(""volume with id '%s' not found"" % volume)

    # Find path to block device
    if vol.get('type') == ""partition"":
        partnumber = determine_partition_number(vol.get('id'), storage_config)
        disk_block_path = get_path_to_storage_volume(vol.get('device'),
                                                     storage_config)
        disk_kname = block.path_to_kname(disk_block_path)
        partition_kname = block.partition_kname(disk_kname, partnumber)
        volume_path = block.kname_to_path(partition_kname)
        devsync_vol = os.path.join(disk_block_path)

    elif vol.get('type') == ""disk"":
        # Get path to block device for disk. Device_id param should refer
        # to id of device in storage config
        if vol.get('serial'):
            volume_path = block.lookup_disk(vol.get('serial'))
        elif vol.get('path'):
            if vol.get('path').startswith('iscsi:'):
                i = iscsi.ensure_disk_connected(vol.get('path'))
                volume_path = os.path.realpath(i.devdisk_path)
            else:
                # resolve any symlinks to the dev_kname so
                # sys/class/block access is valid.  ie, there are no
                # udev generated values in sysfs
                volume_path = os.path.realpath(vol.get('path'))
        elif vol.get('wwn'):
            by_wwn = '/dev/disk/by-id/wwn-%s' % vol.get('wwn')
            volume_path = os.path.realpath(by_wwn)
        else:
            raise ValueError(""serial, wwn or path to block dev must be \
                specified to identify disk"")

    elif vol.get('type') == ""lvm_partition"":
        # For lvm partitions, a directory in /dev/ should be present with the
        # name of the volgroup the partition belongs to. We can simply append
        # the id of the lvm partition to the path of that directory
        volgroup = storage_config.get(vol.get('volgroup'))
        if not volgroup:
            raise ValueError(""lvm volume group '%s' could not be found""
                             % vol.get('volgroup'))
        volume_path = os.path.join(""/dev/"", volgroup.get('name'),
                                   vol.get('name'))

    elif vol.get('type') == ""dm_crypt"":
        # For dm_crypted partitions, unencrypted block device is at
        # /dev/mapper/<dm_name>
        dm_name = vol.get('dm_name')
        if not dm_name:
            dm_name = vol.get('id')
        volume_path = os.path.join(""/dev"", ""mapper"", dm_name)

    elif vol.get('type') == ""raid"":
        # For raid partitions, block device is at /dev/mdX
        name = vol.get('name')
        volume_path = os.path.join(""/dev"", name)

    elif vol.get('type') == ""bcache"":
        # For bcache setups, the only reliable way to determine the name of the
        # block device is to look in all /sys/block/bcacheX/ dirs and see what
        # block devs are in the slaves dir there. Then, those blockdevs can be
        # checked against the kname of the devs in the config for the desired
        # bcache device. This is not very elegant though
        backing_device_path = get_path_to_storage_volume(
            vol.get('backing_device'), storage_config)
        backing_device_kname = block.path_to_kname(backing_device_path)
        sys_path = list(filter(lambda x: backing_device_kname in x,
                               glob.glob(""/sys/block/bcache*/slaves/*"")))[0]
        while ""bcache"" not in os.path.split(sys_path)[-1]:
            sys_path = os.path.split(sys_path)[0]
        bcache_kname = block.path_to_kname(sys_path)
        volume_path = block.kname_to_path(bcache_kname)
        LOG.debug('got bcache volume path {}'.format(volume_path))

    else:
        raise NotImplementedError(""cannot determine the path to storage \
            volume '%s' with type '%s'"" % (volume, vol.get('type')))

    # sync devices
    if not devsync_vol:
        devsync_vol = volume_path
    devsync(devsync_vol)

    LOG.debug('return volume path {}'.format(volume_path))
    return volume_path


def disk_handler(info, storage_config):
    _dos_names = ['dos', 'msdos']
    ptable = info.get('ptable')
    disk = get_path_to_storage_volume(info.get('id'), storage_config)

    if config.value_as_boolean(info.get('preserve')):
        # Handle preserve flag, verifying if ptable specified in config
        if config.value_as_boolean(ptable):
            current_ptable = block.get_part_table_type(disk)
            if not ((ptable in _dos_names and current_ptable in _dos_names) or
                    (ptable == 'gpt' and current_ptable == 'gpt')):
                raise ValueError(
                    ""disk '%s' does not have correct partition table or ""
                    ""cannot be read, but preserve is set to true. ""
                    ""cannot continue installation."" % info.get('id'))
        LOG.info(""disk '%s' marked to be preserved, so keeping partition ""
                 ""table"" % disk)
    else:
        # wipe the disk and create the partition table if instructed to do so
        if config.value_as_boolean(info.get('wipe')):
            block.wipe_volume(disk, mode=info.get('wipe'))
        if config.value_as_boolean(ptable):
            LOG.info(""labeling device: '%s' with '%s' partition table"", disk,
                     ptable)
            if ptable == ""gpt"":
                # Wipe both MBR and GPT that may be present on the disk.
                # N.B.: wipe_volume wipes 1M at front and end of the disk.
                # This could destroy disk data in filesystems that lived
                # there.
                block.wipe_volume(disk, mode='superblock')
            elif ptable in _dos_names:
                util.subp([""parted"", disk, ""--script"", ""mklabel"", ""msdos""])
            else:
                raise ValueError('invalid partition table type: %s', ptable)
        holders = clear_holders.get_holders(disk)
        if len(holders) > 0:
            LOG.info('Detected block holders on disk %s: %s', disk, holders)
            clear_holders.clear_holders(disk)
            clear_holders.assert_clear(disk)

    # Make the name if needed
    if info.get('name'):
        make_dname(info.get('id'), storage_config)


def getnumberoflogicaldisks(device, storage_config):
    logicaldisks = 0
    for key, item in storage_config.items():
        if item.get('device') == device and item.get('flag') == ""logical"":
            logicaldisks = logicaldisks + 1
    return logicaldisks


def find_previous_partition(disk_id, part_id, storage_config):
    last_partnum = None
    for item_id, command in storage_config.items():
        if item_id == part_id:
            break

        # skip anything not on this disk, not a 'partition' or 'extended'
        if command['type'] != 'partition' or command['device'] != disk_id:
            continue
        if command.get('flag') == ""extended"":
            continue

        last_partnum = determine_partition_number(item_id, storage_config)

    return last_partnum


def partition_handler(info, storage_config):
    device = info.get('device')
    size = info.get('size')
    flag = info.get('flag')
    disk_ptable = storage_config.get(device).get('ptable')
    partition_type = None
    if not device:
        raise ValueError(""device must be set for partition to be created"")
    if not size:
        raise ValueError(""size must be specified for partition to be created"")

    disk = get_path_to_storage_volume(device, storage_config)
    partnumber = determine_partition_number(info.get('id'), storage_config)
    disk_kname = block.path_to_kname(disk)
    disk_sysfs_path = block.sys_block_path(disk)
    # consider the disks logical sector size when calculating sectors
    try:
        lbs_path = os.path.join(disk_sysfs_path, 'queue', 'logical_block_size')
        with open(lbs_path, 'r') as f:
            logical_block_size_bytes = int(f.readline())
    except Exception:
        logical_block_size_bytes = 512
    LOG.debug(
        ""{} logical_block_size_bytes: {}"".format(disk_kname,
                                                 logical_block_size_bytes))

    if partnumber > 1:
        if partnumber == 5 and disk_ptable == ""msdos"":
            for key, item in storage_config.items():
                if item.get('type') == ""partition"" and \
                        item.get('device') == device and \
                        item.get('flag') == ""extended"":
                    extended_part_no = determine_partition_number(
                        key, storage_config)
                    break
            pnum = extended_part_no
        else:
            pnum = find_previous_partition(device, info['id'], storage_config)

        LOG.debug(""previous partition number for '%s' found to be '%s'"",
                  info.get('id'), pnum)
        partition_kname = block.partition_kname(disk_kname, pnum)
        previous_partition = os.path.join(disk_sysfs_path, partition_kname)
        LOG.debug(""previous partition: {}"".format(previous_partition))
        # XXX: sys/block/X/{size,start} is *ALWAYS* in 512b value
        previous_size = util.load_file(os.path.join(previous_partition,
                                                    ""size""))
        previous_size_sectors = (int(previous_size) * 512 /
                                 logical_block_size_bytes)
        previous_start = util.load_file(os.path.join(previous_partition,
                                                     ""start""))
        previous_start_sectors = (int(previous_start) * 512 /
                                  logical_block_size_bytes)
        LOG.debug(""previous partition.size_sectors: {}"".format(
                  previous_size_sectors))
        LOG.debug(""previous partition.start_sectors: {}"".format(
                  previous_start_sectors))

    # Align to 1M at the beginning of the disk and at logical partitions
    alignment_offset = int((1 << 20) / logical_block_size_bytes)
    if partnumber == 1:
        # start of disk
        offset_sectors = alignment_offset
    else:
        # further partitions
        if disk_ptable == ""gpt"" or flag != ""logical"":
            # msdos primary and any gpt part start after former partition end
            offset_sectors = previous_start_sectors + previous_size_sectors
        else:
            # msdos extended/logical partitions
            if flag == ""logical"":
                if partnumber == 5:
                    # First logical partition
                    # start at extended partition start + alignment_offset
                    offset_sectors = (previous_start_sectors +
                                      alignment_offset)
                else:
                    # Further logical partitions
                    # start at former logical partition end + alignment_offset
                    offset_sectors = (previous_start_sectors +
                                      previous_size_sectors +
                                      alignment_offset)

    length_bytes = util.human2bytes(size)
    # start sector is part of the sectors that define the partitions size
    # so length has to be ""size in sectors - 1""
    length_sectors = int(length_bytes / logical_block_size_bytes) - 1
    # logical partitions can't share their start sector with the extended
    # partition and logical partitions can't go head-to-head, so we have to
    # realign and for that increase size as required
    if info.get('flag') == ""extended"":
        logdisks = getnumberoflogicaldisks(device, storage_config)
        length_sectors = length_sectors + (logdisks * alignment_offset)

    # Handle preserve flag
    if config.value_as_boolean(info.get('preserve')):
        return
    elif config.value_as_boolean(storage_config.get(device).get('preserve')):
        raise NotImplementedError(""Partition '%s' is not marked to be \
            preserved, but device '%s' is. At this time, preserving devices \
            but not also the partitions on the devices is not supported, \
            because of the possibility of damaging partitions intended to be \
            preserved."" % (info.get('id'), device))

    # Set flag
    # 'sgdisk --list-types'
    sgdisk_flags = {""boot"": 'ef00',
                    ""lvm"": '8e00',
                    ""raid"": 'fd00',
                    ""bios_grub"": 'ef02',
                    ""prep"": '4100',
                    ""swap"": '8200',
                    ""home"": '8302',
                    ""linux"": '8300'}

    LOG.info(""adding partition '%s' to disk '%s' (ptable: '%s')"",
             info.get('id'), device, disk_ptable)
    LOG.debug(""partnum: %s offset_sectors: %s length_sectors: %s"",
              partnumber, offset_sectors, length_sectors)

    # Wipe the partition if told to do so, do not wipe dos extended partitions
    # as this may damage the extended partition table
    if config.value_as_boolean(info.get('wipe')):
        LOG.info(""Preparing partition location on disk %s"", disk)
        if info.get('flag') == ""extended"":
            LOG.warn(""extended partitions do not need wiping, so skipping: ""
                     ""'%s'"" % info.get('id'))
        else:
            # wipe the start of the new partition first by zeroing 1M at the
            # length of the previous partition
            wipe_offset = int(offset_sectors * logical_block_size_bytes)
            LOG.debug('Wiping 1M on %s at offset %s', disk, wipe_offset)
            # We don't require exclusive access as we're wiping data at an
            # offset and the current holder maybe part of the current storage
            # configuration.
            block.zero_file_at_offsets(disk, [wipe_offset], exclusive=False)

    if disk_ptable == ""msdos"":
        if flag in [""extended"", ""logical"", ""primary""]:
            partition_type = flag
        else:
            partition_type = ""primary""
        cmd = [""parted"", disk, ""--script"", ""mkpart"", partition_type,
               ""%ss"" % offset_sectors, ""%ss"" % str(offset_sectors +
                                                   length_sectors)]
        util.subp(cmd, capture=True)
    elif disk_ptable == ""gpt"":
        if flag and flag in sgdisk_flags:
            typecode = sgdisk_flags[flag]
        else:
            typecode = sgdisk_flags['linux']
        cmd = [""sgdisk"", ""--new"", ""%s:%s:%s"" % (partnumber, offset_sectors,
               length_sectors + offset_sectors),
               ""--typecode=%s:%s"" % (partnumber, typecode), disk]
        util.subp(cmd, capture=True)
    else:
        raise ValueError(""parent partition has invalid partition table"")

    # Make the name if needed
    if storage_config.get(device).get('name') and partition_type != 'extended':
        make_dname(info.get('id'), storage_config)


def format_handler(info, storage_config):
    volume = info.get('volume')
    if not volume:
        raise ValueError(""volume must be specified for partition '%s'"" %
                         info.get('id'))

    # Get path to volume
    volume_path = get_path_to_storage_volume(volume, storage_config)

    # Handle preserve flag
    if config.value_as_boolean(info.get('preserve')):
        # Volume marked to be preserved, not formatting
        return

    # Make filesystem using block library
    LOG.debug(""mkfs {} info: {}"".format(volume_path, info))
    mkfs.mkfs_from_config(volume_path, info)

    device_type = storage_config.get(volume).get('type')
    LOG.debug('Formated device type: %s', device_type)
    if device_type == 'bcache':
        # other devs have a udev watch on them. Not bcache (LP: #1680597).
        LOG.debug('Detected bcache device format, calling udevadm trigger to '
                  'generate by-uuid symlinks on ""%s""', volume_path)
        udevadm_trigger([volume_path])


def mount_handler(info, storage_config):
    """""" Handle storage config type: mount

    info = {
        'id': 'rootfs_mount',
        'type': 'mount',
        'path': '/',
        'options': 'defaults,errors=remount-ro',
        'device': 'rootfs',
    }

    Mount specified device under target at 'path' and generate
    fstab entry.
    """"""
    state = util.load_command_environment()
    path = info.get('path')
    filesystem = storage_config.get(info.get('device'))
    mount_options = info.get('options')
    # handle unset, or empty('') strings
    if not mount_options:
        mount_options = 'defaults'

    if not path and filesystem.get('fstype') != ""swap"":
        raise ValueError(""path to mountpoint must be specified"")
    volume = storage_config.get(filesystem.get('volume'))

    # Get path to volume
    volume_path = get_path_to_storage_volume(filesystem.get('volume'),
                                             storage_config)

    if filesystem.get('fstype') != ""swap"":
        # Figure out what point should be
        while len(path) > 0 and path[0] == ""/"":
            path = path[1:]
        mount_point = os.path.sep.join([state['target'], path])
        mount_point = os.path.normpath(mount_point)

        options = mount_options.split("","")
        # If the volume_path's kname is backed by iSCSI or (in the case of
        # LVM/DM) if any of its slaves are backed by iSCSI, then we need to
        # append _netdev to the fstab line
        if iscsi.volpath_is_iscsi(volume_path):
            LOG.debug(""Marking volume_path:%s as '_netdev'"", volume_path)
            options.append(""_netdev"")

        # Create mount point if does not exist
        util.ensure_dir(mount_point)

        # Mount volume, with options
        try:
            opts = ['-o', ','.join(options)]
            util.subp(['mount', volume_path, mount_point] + opts, capture=True)
        except util.ProcessExecutionError as e:
            LOG.exception(e)
            msg = ('Mount failed: %s @ %s with options %s' % (volume_path,
                                                              mount_point,
                                                              "","".join(opts)))
            LOG.error(msg)
            raise RuntimeError(msg)

        # set path
        path = ""/%s"" % path

    else:
        path = ""none""
        options = [""sw""]

    # Add volume to fstab
    if state['fstab']:
        uuid = block.get_volume_uuid(volume_path)
        location = (""UUID=%s"" % uuid) if uuid else (
                    get_path_to_storage_volume(volume.get('id'),
                                               storage_config))

        fstype = filesystem.get('fstype')
        if fstype in [""fat"", ""fat12"", ""fat16"", ""fat32"", ""fat64""]:
            fstype = ""vfat""

        fstab_entry = ""%s %s %s %s 0 0\n"" % (location, path, fstype,
                                             "","".join(options))
        util.write_file(state['fstab'], fstab_entry, omode='a')
    else:
        LOG.info(""fstab not in environment, so not writing"")


def lvm_volgroup_handler(info, storage_config):
    devices = info.get('devices')
    device_paths = []
    name = info.get('name')
    if not devices:
        raise ValueError(""devices for volgroup '%s' must be specified"" %
                         info.get('id'))
    if not name:
        raise ValueError(""name for volgroups needs to be specified"")

    for device_id in devices:
        device = storage_config.get(device_id)
        if not device:
            raise ValueError(""device '%s' could not be found in storage config""
                             % device_id)
        device_paths.append(get_path_to_storage_volume(device_id,
                            storage_config))

    # Handle preserve flag
    if config.value_as_boolean(info.get('preserve')):
        # LVM will probably be offline, so start it
        util.subp([""vgchange"", ""-a"", ""y""])
        # Verify that volgroup exists and contains all specified devices
        if set(lvm.get_pvols_in_volgroup(name)) != set(device_paths):
            raise ValueError(""volgroup '%s' marked to be preserved, but does ""
                             ""not exist or does not contain the right ""
                             ""physical volumes"" % info.get('id'))
    else:
        # Create vgrcreate command and run
        # capture output to avoid printing it to log
        # Use zero to clear target devices of any metadata
        util.subp(['vgcreate', '--force', '--zero=y', '--yes',
                   name] + device_paths, capture=True)

    # refresh lvmetad
    lvm.lvm_scan()


def lvm_partition_handler(info, storage_config):
    volgroup = storage_config.get(info.get('volgroup')).get('name')
    name = info.get('name')
    if not volgroup:
        raise ValueError(""lvm volgroup for lvm partition must be specified"")
    if not name:
        raise ValueError(""lvm partition name must be specified"")
    if info.get('ptable'):
        raise ValueError(""Partition tables on top of lvm logical volumes is ""
                         ""not supported"")

    # Handle preserve flag
    if config.value_as_boolean(info.get('preserve')):
        if name not in lvm.get_lvols_in_volgroup(volgroup):
            raise ValueError(""lvm partition '%s' marked to be preserved, but ""
                             ""does not exist or does not mach storage ""
                             ""configuration"" % info.get('id'))
    elif storage_config.get(info.get('volgroup')).get('preserve'):
        raise NotImplementedError(
            ""Lvm Partition '%s' is not marked to be preserved, but volgroup ""
            ""'%s' is. At this time, preserving volgroups but not also the lvm ""
            ""partitions on the volgroup is not supported, because of the ""
            ""possibility of damaging lvm  partitions intended to be ""
            ""preserved."" % (info.get('id'), volgroup))
    else:
        # Use 'wipesignatures' (if available) and 'zero' to clear target lv
        # of any fs metadata
        cmd = [""lvcreate"", volgroup, ""--name"", name, ""--zero=y""]
        release = util.lsb_release()['codename']
        if release not in ['precise', 'trusty']:
            cmd.extend([""--wipesignatures=y""])

        if info.get('size'):
            cmd.extend([""--size"", info.get('size')])
        else:
            cmd.extend([""--extents"", ""100%FREE""])

        util.subp(cmd)

    # refresh lvmetad
    lvm.lvm_scan()

    make_dname(info.get('id'), storage_config)


def dm_crypt_handler(info, storage_config):
    state = util.load_command_environment()
    volume = info.get('volume')
    key = info.get('key')
    keysize = info.get('keysize')
    cipher = info.get('cipher')
    dm_name = info.get('dm_name')
    if not volume:
        raise ValueError(""volume for cryptsetup to operate on must be \
            specified"")
    if not key:
        raise ValueError(""encryption key must be specified"")
    if not dm_name:
        dm_name = info.get('id')

    volume_path = get_path_to_storage_volume(volume, storage_config)

    # TODO: this is insecure, find better way to do this
    tmp_keyfile = tempfile.mkstemp()[1]
    fp = open(tmp_keyfile, ""w"")
    fp.write(key)
    fp.close()

    cmd = [""cryptsetup""]
    if cipher:
        cmd.extend([""--cipher"", cipher])
    if keysize:
        cmd.extend([""--key-size"", keysize])
    cmd.extend([""luksFormat"", volume_path, tmp_keyfile])

    util.subp(cmd)

    cmd = [""cryptsetup"", ""open"", ""--type"", ""luks"", volume_path, dm_name,
           ""--key-file"", tmp_keyfile]

    util.subp(cmd)

    os.remove(tmp_keyfile)

    # A crypttab will be created in the same directory as the fstab in the
    # configuration. This will then be copied onto the system later
    if state['fstab']:
        crypt_tab_location = os.path.join(os.path.split(state['fstab'])[0],
                                          ""crypttab"")
        uuid = block.get_volume_uuid(volume_path)
        with open(crypt_tab_location, ""a"") as fp:
            fp.write(""%s UUID=%s none luks\n"" % (dm_name, uuid))
    else:
        LOG.info(""fstab configuration is not present in environment, so \
            cannot locate an appropriate directory to write crypttab in \
            so not writing crypttab"")


def raid_handler(info, storage_config):
    state = util.load_command_environment()
    devices = info.get('devices')
    raidlevel = info.get('raidlevel')
    spare_devices = info.get('spare_devices')
    md_devname = block.dev_path(info.get('name'))
    if not devices:
        raise ValueError(""devices for raid must be specified"")
    if raidlevel not in ['linear', 'raid0', 0, 'stripe', 'raid1', 1, 'mirror',
                         'raid4', 4, 'raid5', 5, 'raid6', 6, 'raid10', 10]:
        raise ValueError(""invalid raidlevel '%s'"" % raidlevel)
    if raidlevel in ['linear', 'raid0', 0, 'stripe']:
        if spare_devices:
            raise ValueError(""spareunsupported in raidlevel '%s'"" % raidlevel)

    LOG.debug('raid: cfg: {}'.format(util.json_dumps(info)))
    device_paths = list(get_path_to_storage_volume(dev, storage_config) for
                        dev in devices)
    LOG.debug('raid: device path mapping: {}'.format(
              zip(devices, device_paths)))

    spare_device_paths = []
    if spare_devices:
        spare_device_paths = list(get_path_to_storage_volume(dev,
                                  storage_config) for dev in spare_devices)
        LOG.debug('raid: spare device path mapping: {}'.format(
                  zip(spare_devices, spare_device_paths)))

    # Handle preserve flag
    if config.value_as_boolean(info.get('preserve')):
        # check if the array is already up, if not try to assemble
        if not mdadm.md_check(md_devname, raidlevel,
                              device_paths, spare_device_paths):
            LOG.info(""assembling preserved raid for ""
                     ""{}"".format(md_devname))

            mdadm.mdadm_assemble(md_devname, device_paths, spare_device_paths)

            # try again after attempting to assemble
            if not mdadm.md_check(md_devname, raidlevel,
                                  devices, spare_device_paths):
                raise ValueError(""Unable to confirm preserved raid array: ""
                                 "" {}"".format(md_devname))
        # raid is all OK
        return

    mdadm.mdadm_create(md_devname, raidlevel,
                       device_paths, spare_device_paths,
                       info.get('mdname', ''))

    # Make dname rule for this dev
    make_dname(info.get('id'), storage_config)

    # A mdadm.conf will be created in the same directory as the fstab in the
    # configuration. This will then be copied onto the installed system later.
    # The file must also be written onto the running system to enable it to run
    # mdadm --assemble and continue installation
    if state['fstab']:
        mdadm_location = os.path.join(os.path.split(state['fstab'])[0],
                                      ""mdadm.conf"")
        mdadm_scan_data = mdadm.mdadm_detail_scan()
        with open(mdadm_location, ""w"") as fp:
            fp.write(mdadm_scan_data)
    else:
        LOG.info(""fstab configuration is not present in the environment, so \
            cannot locate an appropriate directory to write mdadm.conf in, \
            so not writing mdadm.conf"")

    # If ptable is specified, call disk_handler on this mdadm device to create
    # the table
    if info.get('ptable'):
        disk_handler(info, storage_config)


def bcache_handler(info, storage_config):
    backing_device = get_path_to_storage_volume(info.get('backing_device'),
                                                storage_config)
    cache_device = get_path_to_storage_volume(info.get('cache_device'),
                                              storage_config)
    cache_mode = info.get('cache_mode', None)

    if not backing_device or not cache_device:
        raise ValueError(""backing device and cache device for bcache""
                         "" must be specified"")

    bcache_sysfs = ""/sys/fs/bcache""
    udevadm_settle(exists=bcache_sysfs)

    def register_bcache(bcache_device):
        LOG.debug('register_bcache: %s > /sys/fs/bcache/register',
                  bcache_device)
        with open(""/sys/fs/bcache/register"", ""w"") as fp:
            fp.write(bcache_device)

    def _validate_bcache(bcache_device, bcache_sys_path):
        """""" check if bcache is ready, dump info

        For cache devices, we expect to find a cacheN symlink
        which will point to the underlying cache device; Find
        this symlink, read it and compare bcache_device
        specified in the parameters.

        For backing devices, we expec to find a dev symlink
        pointing to the bcacheN device to which the backing
        device is enslaved.  From the dev symlink, we can
        read the bcacheN holders list, which should contain
        the backing device kname.

        In either case, if we fail to find the correct
        symlinks in sysfs, this method will raise
        an OSError indicating the missing attribute.
        """"""
        # cacheset
        # /sys/fs/bcache/<uuid>

        # cache device
        # /sys/class/block/<cdev>/bcache/set -> # .../fs/bcache/uuid

        # backing
        # /sys/class/block/<bdev>/bcache/cache -> # .../block/bcacheN
        # /sys/class/block/<bdev>/bcache/dev -> # .../block/bcacheN

        if bcache_sys_path.startswith('/sys/fs/bcache'):
            LOG.debug(""validating bcache caching device '%s' from sys_path""
                      "" '%s'"", bcache_device, bcache_sys_path)
            # we expect a cacheN symlink to point to bcache_device/bcache
            sys_path_links = [os.path.join(bcache_sys_path, l)
                              for l in os.listdir(bcache_sys_path)]
            cache_links = [l for l in sys_path_links
                           if os.path.islink(l) and (
                              os.path.basename(l).startswith('cache'))]

            if len(cache_links) == 0:
                msg = ('Failed to find any cache links in %s:%s' % (
                       bcache_sys_path, sys_path_links))
                raise OSError(msg)

            for link in cache_links:
                target = os.readlink(link)
                LOG.debug('Resolving symlink %s -> %s', link, target)
                # cacheN  -> ../../../devices/.../<bcache_device>/bcache
                # basename(dirname(readlink(link)))
                target_cache_device = os.path.basename(
                    os.path.dirname(target))
                if os.path.basename(bcache_device) == target_cache_device:
                    LOG.debug('Found match: bcache_device=%s target_device=%s',
                              bcache_device, target_cache_device)
                    return
                else:
                    msg = ('Cache symlink %s ' % target_cache_device +
                           'points to incorrect device: %s' % bcache_device)
                    raise OSError(msg)
        elif bcache_sys_path.startswith('/sys/class/block'):
            LOG.debug(""validating bcache backing device '%s' from sys_path""
                      "" '%s'"", bcache_device, bcache_sys_path)
            # we expect a 'dev' symlink to point to the bcacheN device
            bcache_dev = os.path.join(bcache_sys_path, 'dev')
            if os.path.islink(bcache_dev):
                bcache_dev_link = (
                    os.path.basename(os.readlink(bcache_dev)))
                LOG.debug('bcache device %s using bcache kname: %s',
                          bcache_sys_path, bcache_dev_link)

                bcache_slaves_path = os.path.join(bcache_dev, 'slaves')
                slaves = os.listdir(bcache_slaves_path)
                LOG.debug('bcache device %s has slaves: %s',
                          bcache_sys_path, slaves)
                if os.path.basename(bcache_device) in slaves:
                    LOG.debug('bcache device %s found in slaves',
                              os.path.basename(bcache_device))
                    return
                else:
                    msg = ('Failed to find bcache device %s' % bcache_device +
                           'in slaves list %s' % slaves)
                    raise OSError(msg)
            else:
                msg = 'didnt find ""dev"" attribute on: %s', bcache_dev
                return OSError(msg)

        else:
            LOG.debug(""Failed to validate bcache device '%s' from sys_path""
                      "" '%s'"", bcache_device, bcache_sys_path)
            msg = ('sysfs path %s does not appear to be a bcache device' %
                   bcache_sys_path)
            return ValueError(msg)

    def ensure_bcache_is_registered(bcache_device, expected, retry=None):
        """""" Test that bcache_device is found at an expected path and
            re-register the device if it's not ready.

            Retry the validation and registration as needed.
        """"""
        if not retry:
            retry = BCACHE_REGISTRATION_RETRY

        for attempt, wait in enumerate(retry):
            # find the actual bcache device name via sysfs using the
            # backing device's holders directory.
            LOG.debug('check just created bcache %s if it is registered,'
                      ' try=%s', bcache_device, attempt + 1)
            try:
                udevadm_settle()
                if os.path.exists(expected):
                    LOG.debug('Found bcache dev %s at expected path %s',
                              bcache_device, expected)
                    _validate_bcache(bcache_device, expected)
                else:
                    msg = 'bcache device path not found: %s' % expected
                    LOG.debug(msg)
                    raise ValueError(msg)

                # if bcache path exists and holders are > 0 we can return
                LOG.debug('bcache dev %s at path %s successfully registered'
                          ' on attempt %s/%s',  bcache_device, expected,
                          attempt + 1, len(retry))
                return

            except (OSError, IndexError, ValueError):
                # Some versions of bcache-tools will register the bcache device
                # as soon as we run make-bcache using udev rules, so wait for
                # udev to settle, then try to locate the dev, on older versions
                # we need to register it manually though
                LOG.debug('bcache device was not registered, registering %s '
                          'at /sys/fs/bcache/register', bcache_device)
                try:
                    register_bcache(bcache_device)
                except IOError:
                    # device creation is notoriously racy and this can trigger
                    # ""Invalid argument"" IOErrors if it got created in ""the
                    # meantime"" - just restart the function a few times to
                    # check it all again
                    pass

            LOG.debug(""bcache dev %s not ready, waiting %ss"",
                      bcache_device, wait)
            time.sleep(wait)

        # we've exhausted our retries
        LOG.warning('Repetitive error registering the bcache dev %s',
                    bcache_device)
        raise RuntimeError(""bcache device %s can't be registered"" %
                           bcache_device)

    if cache_device:
        # /sys/class/block/XXX/YYY/
        cache_device_sysfs = block.sys_block_path(cache_device)

        if os.path.exists(os.path.join(cache_device_sysfs, ""bcache"")):
            LOG.debug('caching device already exists at {}/bcache. Read '
                      'cset.uuid'.format(cache_device_sysfs))
            (out, err) = util.subp([""bcache-super-show"", cache_device],
                                   capture=True)
            LOG.debug('bcache-super-show=[{}]'.format(out))
            [cset_uuid] = [line.split()[-1] for line in out.split(""\n"")
                           if line.startswith('cset.uuid')]
        else:
            LOG.debug('caching device does not yet exist at {}/bcache. Make '
                      'cache and get uuid'.format(cache_device_sysfs))
            # make the cache device, extracting cacheset uuid
            (out, err) = util.subp([""make-bcache"", ""-C"", cache_device],
                                   capture=True)
            LOG.debug('out=[{}]'.format(out))
            [cset_uuid] = [line.split()[-1] for line in out.split(""\n"")
                           if line.startswith('Set UUID:')]

        target_sysfs_path = '/sys/fs/bcache/%s' % cset_uuid
        ensure_bcache_is_registered(cache_device, target_sysfs_path)

    if backing_device:
        backing_device_sysfs = block.sys_block_path(backing_device)
        target_sysfs_path = os.path.join(backing_device_sysfs, ""bcache"")
        if not os.path.exists(os.path.join(backing_device_sysfs, ""bcache"")):
            LOG.debug('Creating a backing device on %s', backing_device)
            util.subp([""make-bcache"", ""-B"", backing_device])
        ensure_bcache_is_registered(backing_device, target_sysfs_path)

        # via the holders we can identify which bcache device we just created
        # for a given backing device
        holders = clear_holders.get_holders(backing_device)
        if len(holders) != 1:
            err = ('Invalid number {} of holding devices:'
                   ' ""{}""'.format(len(holders), holders))
            LOG.error(err)
            raise ValueError(err)
        [bcache_dev] = holders
        LOG.debug('The just created bcache device is {}'.format(holders))

        if cache_device:
            # if we specify both then we need to attach backing to cache
            if cset_uuid:
                LOG.info(""Attaching backing device to cacheset: ""
                         ""{} -> {} cset.uuid: {}"".format(backing_device,
                                                         cache_device,
                                                         cset_uuid))
                attach = os.path.join(backing_device_sysfs,
                                      ""bcache"",
                                      ""attach"")
                with open(attach, ""w"") as fp:
                    fp.write(cset_uuid)
            else:
                msg = ""Invalid cset_uuid: {}"".format(cset_uuid)
                LOG.error(msg)
                raise ValueError(msg)

        if cache_mode:
            LOG.info(""Setting cache_mode on {} to {}"".format(bcache_dev,
                                                             cache_mode))
            cache_mode_file = \
                '/sys/block/{}/bcache/cache_mode'.format(bcache_dev)
            with open(cache_mode_file, ""w"") as fp:
                fp.write(cache_mode)
    else:
        # no backing device
        if cache_mode:
            raise ValueError(""cache mode specified which can only be set per \
                              backing devices, but none was specified"")

    if info.get('name'):
        # Make dname rule for this dev
        make_dname(info.get('id'), storage_config)

    if info.get('ptable'):
        raise ValueError(""Partition tables on top of lvm logical volumes is \
                         not supported"")
    LOG.debug('Finished bcache creation for backing {} or caching {}'
              .format(backing_device, cache_device))


def zpool_handler(info, storage_config):
    """"""
    Create a zpool based in storage_configuration
    """"""
    state = util.load_command_environment()

    # extract /dev/disk/by-id paths for each volume used
    vdevs = [get_path_to_storage_volume(v, storage_config)
             for v in info.get('vdevs', [])]
    poolname = info.get('pool')
    mountpoint = info.get('mountpoint')
    altroot = state['target']

    if not vdevs or not poolname:
        raise ValueError(""pool and vdevs for zpool must be specified"")

    # map storage volume to by-id path for persistent path
    vdevs_byid = []
    for vdev in vdevs:
        byid = block.disk_to_byid_path(vdev)
        if not byid:
            msg = 'Cannot find by-id path to zpool device ""%s""' % vdev
            LOG.error(msg)
            raise RuntimeError(msg)
        vdevs_byid.append(byid)

    LOG.info('Creating zpool %s with vdevs %s', poolname, vdevs_byid)
    zfs.zpool_create(poolname, vdevs_byid,
                     mountpoint=mountpoint, altroot=altroot)


def zfs_handler(info, storage_config):
    """"""
    Create a zfs filesystem
    """"""
    state = util.load_command_environment()
    poolname = get_poolname(info, storage_config)
    volume = info.get('volume')
    properties = info.get('properties', {})

    LOG.info('Creating zfs dataset %s/%s with properties %s',
             poolname, volume, properties)
    zfs.zfs_create(poolname, volume, zfs_properties=properties)

    mountpoint = properties.get('mountpoint')
    if mountpoint:
        if state['fstab']:
            fstab_entry = (
                ""# Use `zfs list` for current zfs mount info\n"" +
                ""# %s %s defaults 0 0\n"" % (poolname, mountpoint))
            util.write_file(state['fstab'], fstab_entry, omode='a')


def extract_storage_ordered_dict(config):
    storage_config = config.get('storage', {})
    if not storage_config:
        raise ValueError(""no 'storage' entry in config"")
    scfg = storage_config.get('config')
    if not scfg:
        raise ValueError(""invalid storage config data"")

    # Since storage config will often have to be searched for a value by its
    # id, and this can become very inefficient as storage_config grows, a dict
    # will be generated with the id of each component of the storage_config as
    # its index and the component of storage_config as its value
    return OrderedDict((d[""id""], d) for (i, d) in enumerate(scfg))


def meta_custom(args):
    """"""Does custom partitioning based on the layout provided in the config
    file. Section with the name storage contains information on which
    partitions on which disks to create. It also contains information about
    overlays (raid, lvm, bcache) which need to be setup.
    """"""

    command_handlers = {
        'disk': disk_handler,
        'partition': partition_handler,
        'format': format_handler,
        'mount': mount_handler,
        'lvm_volgroup': lvm_volgroup_handler,
        'lvm_partition': lvm_partition_handler,
        'dm_crypt': dm_crypt_handler,
        'raid': raid_handler,
        'bcache': bcache_handler,
        'zfs': zfs_handler,
        'zpool': zpool_handler,
    }

    state = util.load_command_environment()
    cfg = config.load_command_config(args, state)

    storage_config_dict = extract_storage_ordered_dict(cfg)

    # set up reportstack
    stack_prefix = state.get('report_stack_prefix', '')

    # shut down any already existing storage layers above any disks used in
    # config that have 'wipe' set
    with events.ReportEventStack(
            name=stack_prefix, reporting_enabled=True, level='INFO',
            description=""removing previous storage devices""):
        clear_holders.start_clear_holders_deps()
        disk_paths = [get_path_to_storage_volume(k, storage_config_dict)
                      for (k, v) in storage_config_dict.items()
                      if v.get('type') == 'disk' and
                      config.value_as_boolean(v.get('wipe')) and
                      not config.value_as_boolean(v.get('preserve'))]
        clear_holders.clear_holders(disk_paths)
        # if anything was not properly shut down, stop installation
        clear_holders.assert_clear(disk_paths)

    for item_id, command in storage_config_dict.items():
        handler = command_handlers.get(command['type'])
        if not handler:
            raise ValueError(""unknown command type '%s'"" % command['type'])
        with events.ReportEventStack(
                name=stack_prefix, reporting_enabled=True, level=""INFO"",
                description=""configuring %s: %s"" % (command['type'],
                                                    command['id'])):
            try:
                handler(command, storage_config_dict)
            except Exception as error:
                LOG.error(""An error occured handling '%s': %s - %s"" %
                          (item_id, type(error).__name__, error))
                raise

    if args.umount:
        util.do_umount(state['target'], recursive=True)
    return 0


def meta_simple(args):
    """"""Creates a root partition. If args.mode == SIMPLE_BOOT, it will also
    create a separate /boot partition.
    """"""
    state = util.load_command_environment()

    cfg = config.load_command_config(args, state)
    devpath = None
    if cfg.get(""storage"") is not None:
        for i in cfg[""storage""][""config""]:
            serial = i.get(""serial"")
            if serial is None:
                continue
            grub = i.get(""grub_device"")
            diskPath = block.lookup_disk(serial)
            if grub is True:
                devpath = diskPath
            if config.value_as_boolean(i.get('wipe')):
                block.wipe_volume(diskPath, mode=i.get('wipe'))

    if args.target is not None:
        state['target'] = args.target

    if state['target'] is None:
        sys.stderr.write(""Unable to find target.  ""
                         ""Use --target or set TARGET_MOUNT_POINT\n"")
        sys.exit(2)

    devices = args.devices
    if devices is None:
        devices = cfg.get('block-meta', {}).get('devices', [])

    bootpt = get_bootpt_cfg(
        cfg.get('block-meta', {}).get('boot-partition', {}),
        enabled=args.mode == SIMPLE_BOOT, fstype=args.boot_fstype,
        root_fstype=args.fstype)

    ptfmt = get_partition_format_type(cfg.get('block-meta', {}))

    # Remove duplicates but maintain ordering.
    devices = list(OrderedDict.fromkeys(devices))

    # Multipath devices might be automatically assembled if multipath-tools
    # package is available in the installation environment. We need to stop
    # all multipath devices to exclusively use one of paths as a target disk.
    block.stop_all_unused_multipath_devices()

    if len(devices) == 0 and devpath is None:
        devices = block.get_installable_blockdevs()
        LOG.warn(""'%s' mode, no devices given. unused list: %s"",
                 args.mode, devices)
        # Check if the list of installable block devices is still empty after
        # checking for block devices and filtering out the removable ones.  In
        # this case we may have a system which has its harddrives reported by
        # lsblk incorrectly. In this case we search for installable
        # blockdevices that are removable as a last resort before raising an
        # exception.
        if len(devices) == 0:
            devices = block.get_installable_blockdevs(include_removable=True)
            if len(devices) == 0:
                # Fail gracefully if no devices are found, still.
                raise Exception(""No valid target devices found that curtin ""
                                ""can install on."")
            else:
                LOG.warn(""No non-removable, installable devices found. List ""
                         ""populated with removable devices allowed: %s"",
                         devices)
    elif len(devices) == 0 and devpath:
        devices = [devpath]

    if len(devices) > 1:
        if args.devices is not None:
            LOG.warn(""'%s' mode but multiple devices given. ""
                     ""using first found"", args.mode)
        available = [f for f in devices
                     if block.is_valid_device(f)]
        target = sorted(available)[0]
        LOG.warn(""mode is '%s'. multiple devices given. using '%s' ""
                 ""(first available)"", args.mode, target)
    else:
        target = devices[0]

    if not block.is_valid_device(target):
        raise Exception(""target device '%s' is not a valid device"" % target)

    (devname, devnode) = block.get_dev_name_entry(target)

    LOG.info(""installing in '%s' mode to '%s'"", args.mode, devname)

    sources = cfg.get('sources', {})
    dd_images = util.get_dd_images(sources)

    if len(dd_images):
        # we have at least one dd-able image
        # we will only take the first one
        rootdev = write_image_to_disk(dd_images[0], devname)
        util.subp(['mount', rootdev, state['target']])
        return 0

    # helper partition will forcibly set up partition there
    ptcmd = ['partition', '--format=' + ptfmt]
    if bootpt['enabled']:
        ptcmd.append('--boot')
    ptcmd.append(devnode)

    if bootpt['enabled'] and ptfmt in (""uefi"", ""prep""):
        raise ValueError(""format=%s with boot partition not supported"" % ptfmt)

    bootdev_ptnum = None
    rootdev_ptnum = None
    bootdev = None
    if bootpt['enabled']:
        bootdev_ptnum = 1
        rootdev_ptnum = 2
    else:
        if ptfmt == ""prep"":
            rootdev_ptnum = 2
        else:
            rootdev_ptnum = 1

    logtime(""creating partition with: %s"" % ' '.join(ptcmd),
            util.subp, ptcmd)

    ptpre = """"
    if not os.path.exists(""%s%s"" % (devnode, rootdev_ptnum)):
        # perhaps the device is /dev/<blockname>p<ptnum>
        if os.path.exists(""%sp%s"" % (devnode, rootdev_ptnum)):
            ptpre = ""p""
        else:
            LOG.warn(""root device %s%s did not exist, expecting failure"",
                     devnode, rootdev_ptnum)

    if bootdev_ptnum:
        bootdev = ""%s%s%s"" % (devnode, ptpre, bootdev_ptnum)

    if ptfmt == ""uefi"":
        # assumed / required from the partitioner pt_uefi
        uefi_ptnum = ""15""
        uefi_label = ""uefi-boot""
        uefi_dev = ""%s%s%s"" % (devnode, ptpre, uefi_ptnum)

    rootdev = ""%s%s%s"" % (devnode, ptpre, rootdev_ptnum)

    LOG.debug(""rootdev=%s bootdev=%s fmt=%s bootpt=%s"",
              rootdev, bootdev, ptfmt, bootpt)

    # mkfs for root partition first and mount
    cmd = ['mkfs.%s' % args.fstype, '-q', '-L', 'cloudimg-rootfs', rootdev]
    logtime(' '.join(cmd), util.subp, cmd)
    util.subp(['mount', rootdev, state['target']])

    if bootpt['enabled']:
        # create 'boot' directory in state['target']
        boot_dir = os.path.join(state['target'], 'boot')
        util.subp(['mkdir', boot_dir])
        # mkfs for boot partition and mount
        cmd = ['mkfs.%s' % bootpt['fstype'],
               '-q', '-L', bootpt['label'], bootdev]
        logtime(' '.join(cmd), util.subp, cmd)
        util.subp(['mount', bootdev, boot_dir])

    if ptfmt == ""uefi"":
        uefi_dir = os.path.join(state['target'], 'boot', 'efi')
        util.ensure_dir(uefi_dir)
        util.subp(['mount', uefi_dev, uefi_dir])

    if state['fstab']:
        with open(state['fstab'], ""w"") as fp:
            if bootpt['enabled']:
                fp.write(""LABEL=%s /boot %s defaults 0 0\n"" %
                         (bootpt['label'], bootpt['fstype']))

            if ptfmt == ""uefi"":
                # label created in helpers/partition for uefi
                fp.write(""LABEL=%s /boot/efi vfat defaults 0 0\n"" %
                         uefi_label)

            fp.write(""LABEL=%s / %s defaults 0 0\n"" %
                     ('cloudimg-rootfs', args.fstype))
    else:
        LOG.info(""fstab not in environment, so not writing"")

    if args.umount:
        util.do_umount(state['target'], recursive=True)

    return 0


def POPULATE_SUBCMD(parser):
    populate_one_subcmd(parser, CMD_ARGUMENTS, block_meta)

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/commands/curthooks.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import copy
import glob
import os
import platform
import re
import sys
import shutil
import textwrap

from curtin import config
from curtin import block
from curtin import net
from curtin import futil
from curtin.log import LOG
from curtin import swap
from curtin import util
from curtin.reporter import events
from curtin.commands import apply_net, apt_config

from . import populate_one_subcmd

write_files = futil._legacy_write_files  # LP: #1731709

CMD_ARGUMENTS = (
    ((('-t', '--target'),
      {'help': 'operate on target. default is env[TARGET_MOUNT_POINT]',
       'action': 'store', 'metavar': 'TARGET', 'default': None}),
     (('-c', '--config'),
      {'help': 'operate on config. default is env[CONFIG]',
       'action': 'store', 'metavar': 'CONFIG', 'default': None}),
     )
)

KERNEL_MAPPING = {
    'precise': {
        '3.2.0': '',
        '3.5.0': '-lts-quantal',
        '3.8.0': '-lts-raring',
        '3.11.0': '-lts-saucy',
        '3.13.0': '-lts-trusty',
    },
    'trusty': {
        '3.13.0': '',
        '3.16.0': '-lts-utopic',
        '3.19.0': '-lts-vivid',
        '4.2.0': '-lts-wily',
        '4.4.0': '-lts-xenial',
    },
    'xenial': {
        '4.3.0': '',  # development release has 4.3, release will have 4.4
        '4.4.0': '',
    }
}

CLOUD_INIT_YUM_REPO_TEMPLATE = """"""
[group_cloud-init-el-stable]
name=Copr repo for el-stable owned by @cloud-init
baseurl=https://copr-be.cloud.fedoraproject.org/results/@cloud-init/el-stable/epel-%s-$basearch/
type=rpm-md
skip_if_unavailable=True
gpgcheck=1
gpgkey=https://copr-be.cloud.fedoraproject.org/results/@cloud-init/el-stable/pubkey.gpg
repo_gpgcheck=0
enabled=1
enabled_metadata=1
""""""


def do_apt_config(cfg, target):
    cfg = apt_config.translate_old_apt_features(cfg)
    apt_cfg = cfg.get(""apt"")
    if apt_cfg is not None:
        LOG.info(""curthooks handling apt to target %s with config %s"",
                 target, apt_cfg)
        apt_config.handle_apt(apt_cfg, target)
    else:
        LOG.info(""No apt config provided, skipping"")


def disable_overlayroot(cfg, target):
    # cloud images come with overlayroot, but installed systems need disabled
    disable = cfg.get('disable_overlayroot', True)
    local_conf = os.path.sep.join([target, 'etc/overlayroot.local.conf'])
    if disable and os.path.exists(local_conf):
        LOG.debug(""renaming %s to %s"", local_conf, local_conf + "".old"")
        shutil.move(local_conf, local_conf + "".old"")


def setup_zipl(cfg, target):
    if platform.machine() != 's390x':
        return

    # assuming that below gives the ""/"" rootfs
    target_dev = block.get_devices_for_mp(target)[0]

    root_arg = None
    # not mapped rootfs, use UUID
    if 'mapper' in target_dev:
        root_arg = target_dev
    else:
        uuid = block.get_volume_uuid(target_dev)
        if uuid:
            root_arg = ""UUID=%s"" % uuid

    if not root_arg:
        msg = ""Failed to identify root= for %s at %s."" % (target, target_dev)
        LOG.warn(msg)
        raise ValueError(msg)

    zipl_conf = """"""
# This has been modified by the MAAS curtin installer
[defaultboot]
default=ubuntu

[ubuntu]
target = /boot
image = /boot/vmlinuz
ramdisk = /boot/initrd.img
parameters = root=%s

"""""" % root_arg
    futil.write_files(
        files={""zipl_conf"": {""path"": ""/etc/zipl.conf"", ""content"": zipl_conf}},
        base_dir=target)


def run_zipl(cfg, target):
    if platform.machine() != 's390x':
        return
    with util.ChrootableTarget(target) as in_chroot:
        in_chroot.subp(['zipl'])


def get_flash_kernel_pkgs(arch=None, uefi=None):
    if arch is None:
        arch = util.get_architecture()
    if uefi is None:
        uefi = util.is_uefi_bootable()
    if uefi:
        return None
    if not arch.startswith('arm'):
        return None

    try:
        fk_packages, _ = util.subp(
            ['list-flash-kernel-packages'], capture=True)
        return fk_packages
    except util.ProcessExecutionError:
        # Ignore errors
        return None


def install_kernel(cfg, target):
    kernel_cfg = cfg.get('kernel', {'package': None,
                                    'fallback-package': ""linux-generic"",
                                    'mapping': {}})
    if kernel_cfg is not None:
        kernel_package = kernel_cfg.get('package')
        kernel_fallback = kernel_cfg.get('fallback-package')
    else:
        kernel_package = None
        kernel_fallback = None

    mapping = copy.deepcopy(KERNEL_MAPPING)
    config.merge_config(mapping, kernel_cfg.get('mapping', {}))

    # Machines using flash-kernel may need additional dependencies installed
    # before running. Run those checks in the ephemeral environment so the
    # target only has required packages installed.  See LP:1640519
    fk_packages = get_flash_kernel_pkgs()
    if fk_packages:
        util.install_packages(fk_packages.split(), target=target)

    if kernel_package:
        util.install_packages([kernel_package], target=target)
        return

    # uname[2] is kernel name (ie: 3.16.0-7-generic)
    # version gets X.Y.Z, flavor gets anything after second '-'.
    kernel = os.uname()[2]
    codename, _ = util.subp(['lsb_release', '--codename', '--short'],
                            capture=True, target=target)
    codename = codename.strip()
    version, abi, flavor = kernel.split('-', 2)

    try:
        map_suffix = mapping[codename][version]
    except KeyError:
        LOG.warn(""Couldn't detect kernel package to install for %s.""
                 % kernel)
        if kernel_fallback is not None:
            util.install_packages([kernel_fallback], target=target)
        return

    package = ""linux-{flavor}{map_suffix}"".format(
        flavor=flavor, map_suffix=map_suffix)

    if util.has_pkg_available(package, target):
        if util.has_pkg_installed(package, target):
            LOG.debug(""Kernel package '%s' already installed"", package)
        else:
            LOG.debug(""installing kernel package '%s'"", package)
            util.install_packages([package], target=target)
    else:
        if kernel_fallback is not None:
            LOG.info(""Kernel package '%s' not available.  ""
                     ""Installing fallback package '%s'."",
                     package, kernel_fallback)
            util.install_packages([kernel_fallback], target=target)
        else:
            LOG.warn(""Kernel package '%s' not available and no fallback.""
                     "" System may not boot."", package)


def uefi_remove_old_loaders(grubcfg, target):
    """"""Removes the old UEFI loaders from efibootmgr.""""""
    efi_output = util.get_efibootmgr(target)
    current_uefi_boot = efi_output.get('current', None)
    old_efi_entries = {
        entry: info
        for entry, info in efi_output['entries'].items()
        if re.match(r'^.*File\(\\EFI.*$', info['path'])
    }
    old_efi_entries.pop(current_uefi_boot, None)
    remove_old_loaders = grubcfg.get('remove_old_uefi_loaders', True)
    if old_efi_entries:
        if remove_old_loaders:
            with util.ChrootableTarget(target) as in_chroot:
                for entry, info in old_efi_entries.items():
                    LOG.debug(""removing old UEFI entry: %s"" % info['name'])
                    in_chroot.subp(
                        ['efibootmgr', '-B', '-b', entry], capture=True)
        else:
            LOG.debug(
                ""Skipped removing %d old UEFI entrie%s."",
                len(old_efi_entries),
                '' if len(old_efi_entries) == 1 else 's')
            for info in old_efi_entries.values():
                LOG.debug(
                    ""UEFI entry '%s' might no longer exist and ""
                    ""should be removed."", info['name'])


def uefi_reorder_loaders(grubcfg, target):
    """"""Reorders the UEFI BootOrder to place BootCurrent first.

    The specifically doesn't try to do to much. The order in which grub places
    a new EFI loader is up to grub. This only moves the BootCurrent to the
    front of the BootOrder.
    """"""
    if grubcfg.get('reorder_uefi', True):
        efi_output = util.get_efibootmgr(target)
        currently_booted = efi_output.get('current', None)
        boot_order = efi_output.get('order', [])
        if currently_booted:
            if currently_booted in boot_order:
                boot_order.remove(currently_booted)
            boot_order = [currently_booted] + boot_order
            new_boot_order = ','.join(boot_order)
            LOG.debug(
                ""Setting currently booted %s as the first ""
                ""UEFI loader."", currently_booted)
            LOG.debug(
                ""New UEFI boot order: %s"", new_boot_order)
            with util.ChrootableTarget(target) as in_chroot:
                in_chroot.subp(['efibootmgr', '-o', new_boot_order])
    else:
        LOG.debug(""Skipped reordering of UEFI boot methods."")
        LOG.debug(""Currently booted UEFI loader might no longer boot."")


def setup_grub(cfg, target):
    # target is the path to the mounted filesystem

    # FIXME: these methods need moving to curtin.block
    # and using them from there rather than commands.block_meta
    from curtin.commands.block_meta import (extract_storage_ordered_dict,
                                            get_path_to_storage_volume)

    grubcfg = cfg.get('grub', {})

    # copy legacy top level name
    if 'grub_install_devices' in cfg and 'install_devices' not in grubcfg:
        grubcfg['install_devices'] = cfg['grub_install_devices']

    LOG.debug(""setup grub on target %s"", target)
    # if there is storage config, look for devices tagged with 'grub_device'
    storage_cfg_odict = None
    try:
        storage_cfg_odict = extract_storage_ordered_dict(cfg)
    except ValueError as e:
        pass

    if storage_cfg_odict:
        storage_grub_devices = []
        for item_id, item in storage_cfg_odict.items():
            if not item.get('grub_device'):
                continue
            LOG.debug(""checking: %s"", item)
            storage_grub_devices.append(
                get_path_to_storage_volume(item_id, storage_cfg_odict))
        if len(storage_grub_devices) > 0:
            grubcfg['install_devices'] = storage_grub_devices

    LOG.debug(""install_devices: %s"", grubcfg.get('install_devices'))
    if 'install_devices' in grubcfg:
        instdevs = grubcfg.get('install_devices')
        if isinstance(instdevs, str):
            instdevs = [instdevs]
        if instdevs is None:
            LOG.debug(""grub installation disabled by config"")
    else:
        # If there were no install_devices found then we try to do the right
        # thing.  That right thing is basically installing on all block
        # devices that are mounted.  On powerpc, though it means finding PrEP
        # partitions.
        devs = block.get_devices_for_mp(target)
        blockdevs = set()
        for maybepart in devs:
            try:
                (blockdev, part) = block.get_blockdev_for_partition(maybepart)
                blockdevs.add(blockdev)
            except ValueError as e:
                # if there is no syspath for this device such as a lvm
                # or raid device, then a ValueError is raised here.
                LOG.debug(""failed to find block device for %s"", maybepart)

        if platform.machine().startswith(""ppc64""):
            # assume we want partitions that are 4100 (PReP). The snippet here
            # just prints the partition number partitions of that type.
            shnip = textwrap.dedent(""""""
                export LANG=C;
                for d in ""$@""; do
                    sgdisk ""$d"" --print |
                        awk ""\$6 == prep { print d \$1 }"" ""d=$d"" prep=4100
                done
                """""")
            try:
                out, err = util.subp(
                    ['sh', '-c', shnip, '--'] + list(blockdevs),
                    capture=True)
                instdevs = str(out).splitlines()
                if not instdevs:
                    LOG.warn(""No power grub target partitions found!"")
                    instdevs = None
            except util.ProcessExecutionError as e:
                LOG.warn(""Failed to find power grub partitions: %s"", e)
                instdevs = None
        else:
            instdevs = list(blockdevs)

    # UEFI requires grub-efi-{arch}. If a signed version of that package
    # exists then it will be installed.
    if util.is_uefi_bootable():
        arch = util.get_architecture()
        pkgs = ['grub-efi-%s' % arch]

        # Architecture might support a signed UEFI loader
        uefi_pkg_signed = 'grub-efi-%s-signed' % arch
        if util.has_pkg_available(uefi_pkg_signed):
            pkgs.append(uefi_pkg_signed)

        # AMD64 has shim-signed for SecureBoot support
        if arch == ""amd64"":
            pkgs.append(""shim-signed"")

        # Install the UEFI packages needed for the architecture
        util.install_packages(pkgs, target=target)

    env = os.environ.copy()

    replace_default = grubcfg.get('replace_linux_default', True)
    if str(replace_default).lower() in (""0"", ""false""):
        env['REPLACE_GRUB_LINUX_DEFAULT'] = ""0""
    else:
        env['REPLACE_GRUB_LINUX_DEFAULT'] = ""1""

    if instdevs:
        instdevs = [block.get_dev_name_entry(i)[1] for i in instdevs]
    else:
        instdevs = [""none""]

    if util.is_uefi_bootable() and grubcfg.get('update_nvram', True):
        uefi_remove_old_loaders(grubcfg, target)

    LOG.debug(""installing grub to %s [replace_default=%s]"",
              instdevs, replace_default)
    with util.ChrootableTarget(target):
        args = ['install-grub']
        if util.is_uefi_bootable():
            args.append(""--uefi"")
            if grubcfg.get('update_nvram', True):
                LOG.debug(""GRUB UEFI enabling NVRAM updates"")
                args.append(""--update-nvram"")
            else:
                LOG.debug(""NOT enabling UEFI nvram updates"")
                LOG.debug(""Target system may not boot"")
        args.append(target)

        # capture stdout and stderr joined.
        join_stdout_err = ['sh', '-c', 'exec ""$0"" ""$@"" 2>&1']
        out, _err = util.subp(
            join_stdout_err + args + instdevs, env=env, capture=True)
        LOG.debug(""%s\n%s\n"", args, out)

    if util.is_uefi_bootable() and grubcfg.get('update_nvram', True):
        uefi_reorder_loaders(grubcfg, target)


def update_initramfs(target=None, all_kernels=False):
    cmd = ['update-initramfs', '-u']
    if all_kernels:
        cmd.extend(['-k', 'all'])
    with util.ChrootableTarget(target) as in_chroot:
        in_chroot.subp(cmd)


def copy_fstab(fstab, target):
    if not fstab:
        LOG.warn(""fstab variable not in state, not copying fstab"")
        return

    shutil.copy(fstab, os.path.sep.join([target, 'etc/fstab']))


def copy_crypttab(crypttab, target):
    if not crypttab:
        LOG.warn(""crypttab config must be specified, not copying"")
        return

    shutil.copy(crypttab, os.path.sep.join([target, 'etc/crypttab']))


def copy_iscsi_conf(nodes_dir, target):
    if not nodes_dir:
        LOG.warn(""nodes directory must be specified, not copying"")
        return

    LOG.info(""copying iscsi nodes database into target"")
    shutil.copytree(nodes_dir, os.path.sep.join([target,
                    'etc/iscsi/nodes']))


def copy_mdadm_conf(mdadm_conf, target):
    if not mdadm_conf:
        LOG.warn(""mdadm config must be specified, not copying"")
        return

    LOG.info(""copying mdadm.conf into target"")
    shutil.copy(mdadm_conf, os.path.sep.join([target,
                'etc/mdadm/mdadm.conf']))


def apply_networking(target, state):
    netconf = state.get('network_config')
    interfaces = state.get('interfaces')

    def is_valid_src(infile):
        with open(infile, 'r') as fp:
            content = fp.read()
            if len(content.split('\n')) > 1:
                return True
        return False

    if is_valid_src(netconf):
        LOG.info(""applying network_config"")
        apply_net.apply_net(target, network_state=None, network_config=netconf)
    else:
        LOG.debug(""copying interfaces"")
        copy_interfaces(interfaces, target)


def copy_interfaces(interfaces, target):
    if not interfaces:
        LOG.warn(""no interfaces file to copy!"")
        return
    eni = os.path.sep.join([target, 'etc/network/interfaces'])
    shutil.copy(interfaces, eni)


def copy_dname_rules(rules_d, target):
    if not rules_d:
        LOG.warn(""no udev rules directory to copy"")
        return
    for rule in os.listdir(rules_d):
        target_file = os.path.join(
            target, ""etc/udev/rules.d"", ""%s.rules"" % rule)
        shutil.copy(os.path.join(rules_d, rule), target_file)


def restore_dist_interfaces(cfg, target):
    # cloud images have a link of /etc/network/interfaces into /run
    eni = os.path.sep.join([target, 'etc/network/interfaces'])
    if not cfg.get('restore_dist_interfaces', True):
        return

    rp = os.path.realpath(eni)
    if (os.path.exists(eni + "".dist"") and
            (rp.startswith(""/run"") or rp.startswith(target + ""/run""))):

        LOG.debug(""restoring dist interfaces, existing link pointed to /run"")
        shutil.move(eni, eni + "".old"")
        shutil.move(eni + "".dist"", eni)


def add_swap(cfg, target, fstab):
    # add swap file per cfg to filesystem root at target. update fstab.
    #
    # swap:
    #  filename: 'swap.img',
    #  size: None # (or 1G)
    #  maxsize: 2G
    if 'swap' in cfg and not cfg.get('swap'):
        LOG.debug(""disabling 'add_swap' due to config"")
        return

    swapcfg = cfg.get('swap', {})
    fname = swapcfg.get('filename', None)
    size = swapcfg.get('size', None)
    maxsize = swapcfg.get('maxsize', None)

    if size:
        size = util.human2bytes(str(size))
    if maxsize:
        maxsize = util.human2bytes(str(maxsize))

    swap.setup_swapfile(target=target, fstab=fstab, swapfile=fname, size=size,
                        maxsize=maxsize)


def detect_and_handle_multipath(cfg, target):
    DEFAULT_MULTIPATH_PACKAGES = ['multipath-tools-boot']
    mpcfg = cfg.get('multipath', {})
    mpmode = mpcfg.get('mode', 'auto')
    mppkgs = mpcfg.get('packages', DEFAULT_MULTIPATH_PACKAGES)
    mpbindings = mpcfg.get('overwrite_bindings', True)

    if isinstance(mppkgs, str):
        mppkgs = [mppkgs]

    if mpmode == 'disabled':
        return

    if mpmode == 'auto' and not block.detect_multipath(target):
        return

    LOG.info(""Detected multipath devices. Installing support via %s"", mppkgs)

    util.install_packages(mppkgs, target=target)
    replace_spaces = True
    try:
        # check in-target version
        pkg_ver = util.get_package_version('multipath-tools', target=target)
        LOG.debug(""get_package_version:\n%s"", pkg_ver)
        LOG.debug(""multipath version is %s (major=%s minor=%s micro=%s)"",
                  pkg_ver['semantic_version'], pkg_ver['major'],
                  pkg_ver['minor'], pkg_ver['micro'])
        # multipath-tools versions < 0.5.0 do _NOT_ want whitespace replaced
        # i.e. 0.4.X in Trusty.
        if pkg_ver['semantic_version'] < 500:
            replace_spaces = False
    except Exception as e:
        LOG.warn(""failed reading multipath-tools version, ""
                 ""assuming it wants no spaces in wwids: %s"", e)

    multipath_cfg_path = os.path.sep.join([target, '/etc/multipath.conf'])
    multipath_bind_path = os.path.sep.join([target, '/etc/multipath/bindings'])

    # We don't want to overwrite multipath.conf file provided by the image.
    if not os.path.isfile(multipath_cfg_path):
        # Without user_friendly_names option enabled system fails to boot
        # if any of the disks has spaces in its name. Package multipath-tools
        # has bug opened for this issue (LP: 1432062) but it was not fixed yet.
        multipath_cfg_content = '\n'.join(
            ['# This file was created by curtin while installing the system.',
             'defaults {',
             '	user_friendly_names yes',
             '}',
             ''])
        util.write_file(multipath_cfg_path, content=multipath_cfg_content)

    if mpbindings or not os.path.isfile(multipath_bind_path):
        # we do assume that get_devices_for_mp()[0] is /
        target_dev = block.get_devices_for_mp(target)[0]
        wwid = block.get_scsi_wwid(target_dev,
                                   replace_whitespace=replace_spaces)
        blockdev, partno = block.get_blockdev_for_partition(target_dev)

        mpname = ""mpath0""
        grub_dev = ""/dev/mapper/"" + mpname
        if partno is not None:
            grub_dev += ""-part%s"" % partno

        LOG.debug(""configuring multipath install for root=%s wwid=%s"",
                  grub_dev, wwid)

        multipath_bind_content = '\n'.join(
            ['# This file was created by curtin while installing the system.',
             ""%s %s"" % (mpname, wwid),
             '# End of content generated by curtin.',
             '# Everything below is maintained by multipath subsystem.',
             ''])
        util.write_file(multipath_bind_path, content=multipath_bind_content)

        grub_cfg = os.path.sep.join(
            [target, '/etc/default/grub.d/50-curtin-multipath.cfg'])
        msg = '\n'.join([
            '# Written by curtin for multipath device wwid ""%s""' % wwid,
            'GRUB_DEVICE=%s' % grub_dev,
            'GRUB_DISABLE_LINUX_UUID=true',
            ''])
        util.write_file(grub_cfg, content=msg)

    else:
        LOG.warn(""Not sure how this will boot"")

    # Initrams needs to be updated to include /etc/multipath.cfg
    # and /etc/multipath/bindings files.
    update_initramfs(target, all_kernels=True)


def detect_required_packages(cfg):
    """"""
    detect packages that will be required in-target by custom config items
    """"""

    mapping = {
        'storage': block.detect_required_packages_mapping(),
        'network': net.detect_required_packages_mapping(),
    }

    needed_packages = []
    for cfg_type, cfg_map in mapping.items():

        # skip missing or invalid config items, configs may
        # only have network or storage, not always both
        if not isinstance(cfg.get(cfg_type), dict):
            continue

        cfg_version = cfg[cfg_type].get('version')
        if not isinstance(cfg_version, int) or cfg_version not in cfg_map:
            msg = ('Supplied configuration version ""%s"", for config type'
                   '""%s"" is not present in the known mapping.' % (cfg_version,
                                                                  cfg_type))
            raise ValueError(msg)

        mapped_config = cfg_map[cfg_version]
        found_reqs = mapped_config['handler'](cfg, mapped_config['mapping'])
        needed_packages.extend(found_reqs)

    LOG.debug('Curtin config dependencies requires additional packages: %s',
              needed_packages)
    return needed_packages


def install_missing_packages(cfg, target):
    ''' describe which operation types will require specific packages

    'custom_config_key': {
         'pkg1': ['op_name_1', 'op_name_2', ...]
     }
    '''

    installed_packages = util.get_installed_packages(target)
    needed_packages = set([pkg for pkg in detect_required_packages(cfg)
                           if pkg not in installed_packages])

    arch_packages = {
        's390x': [('s390-tools', 'zipl')],
    }

    for pkg, cmd in arch_packages.get(platform.machine(), []):
        if not util.which(cmd, target=target):
            if pkg not in needed_packages:
                needed_packages.add(pkg)

    # Filter out ifupdown network packages on netplan enabled systems.
    if 'ifupdown' not in installed_packages and 'nplan' in installed_packages:
        drops = set(['bridge-utils', 'ifenslave', 'vlan'])
        if needed_packages.union(drops):
            LOG.debug(""Skipping install of %s.  Not needed on netplan system."",
                      needed_packages.union(drops))
            needed_packages = needed_packages.difference(drops)

    if needed_packages:
        to_add = list(sorted(needed_packages))
        state = util.load_command_environment()
        with events.ReportEventStack(
                name=state.get('report_stack_prefix'),
                reporting_enabled=True, level=""INFO"",
                description=""Installing packages on target system: "" +
                str(to_add)):
            util.install_packages(to_add, target=target)


def system_upgrade(cfg, target):
    """"""run system-upgrade (apt-get dist-upgrade) or other in target.

    config:
      system_upgrade:
        enabled: False

    """"""
    mycfg = {'system_upgrade': {'enabled': False}}
    config.merge_config(mycfg, cfg)
    mycfg = mycfg.get('system_upgrade')
    if not isinstance(mycfg, dict):
        LOG.debug(""system_upgrade disabled by config. entry not a dict."")
        return

    if not config.value_as_boolean(mycfg.get('enabled', True)):
        LOG.debug(""system_upgrade disabled by config."")
        return

    util.system_upgrade(target=target)


def handle_cloudconfig(cfg, base_dir=None):
    """"""write cloud-init configuration files into base_dir.

    cloudconfig format is a dictionary of keys and values of content

    cloudconfig:
      cfg-datasource:
        content:
         |
         #cloud-cfg
         datasource_list: [ MAAS ]
      cfg-maas:
        content:
         |
         #cloud-cfg
         reporting:
           maas: { consumer_key: 8cW9kadrWZcZvx8uWP,
                   endpoint: 'http://XXX',
                   token_key: jD57DB9VJYmDePCRkq,
                   token_secret: mGFFMk6YFLA3h34QHCv22FjENV8hJkRX,
                   type: webhook}
    """"""
    # check that cfg is dict
    if not isinstance(cfg, dict):
        raise ValueError(""cloudconfig configuration is not in dict format"")

    # for each item in the dict
    #   generate a path based on item key
    #   if path is already in the item, LOG warning, and use generated path
    for cfgname, cfgvalue in cfg.items():
        cfgpath = ""50-cloudconfig-%s.cfg"" % cfgname
        if 'path' in cfgvalue:
            LOG.warning(""cloudconfig ignoring 'path' key in config"")
        cfgvalue['path'] = cfgpath

    # re-use write_files format and adjust target to prepend
    LOG.debug('Calling write_files with cloudconfig @ %s', base_dir)
    LOG.debug('Injecting cloud-config:\n%s', cfg)
    futil.write_files(cfg, base_dir)


def ubuntu_core_curthooks(cfg, target=None):
    """""" Ubuntu-Core 16 images cannot execute standard curthooks
        Instead we copy in any cloud-init configuration to
        the 'LABEL=writable' partition mounted at target.
    """"""

    ubuntu_core_target = os.path.join(target, ""system-data"")
    cc_target = os.path.join(ubuntu_core_target, 'etc/cloud/cloud.cfg.d')

    cloudconfig = cfg.get('cloudconfig', None)
    if cloudconfig:
        # remove cloud-init.disabled, if found
        cloudinit_disable = os.path.join(ubuntu_core_target,
                                         'etc/cloud/cloud-init.disabled')
        if os.path.exists(cloudinit_disable):
            util.del_file(cloudinit_disable)

        handle_cloudconfig(cloudconfig, base_dir=cc_target)

    netconfig = cfg.get('network', None)
    if netconfig:
        LOG.info('Writing network configuration')
        ubuntu_core_netconfig = os.path.join(cc_target,
                                             ""50-curtin-networking.cfg"")
        util.write_file(ubuntu_core_netconfig,
                        content=config.dump_config({'network': netconfig}))


def rpm_get_dist_id(target):
    """"""Use rpm command to extract the '%rhel' distro macro which returns
       the major os version id (6, 7, 8).  This works for centos or rhel
    """"""
    with util.ChrootableTarget(target) as in_chroot:
        dist, _ = in_chroot.subp(['rpm', '-E', '%rhel'], capture=True)
    return dist.rstrip()


def centos_apply_network_config(netcfg, target=None):
    """""" CentOS images execute built-in curthooks which only supports
        simple networking configuration.  This hook enables advanced
        network configuration via config passthrough to the target.
    """"""

    def cloud_init_repo(version):
        if not version:
            raise ValueError('Missing required version parameter')

        return CLOUD_INIT_YUM_REPO_TEMPLATE % version

    if netcfg:
        LOG.info('Removing embedded network configuration (if present)')
        ifcfgs = glob.glob(util.target_path(target,
                                            'etc/sysconfig/network-scripts') +
                           '/ifcfg-*')
        # remove ifcfg-* (except ifcfg-lo)
        for ifcfg in ifcfgs:
            if os.path.basename(ifcfg) != ""ifcfg-lo"":
                util.del_file(ifcfg)

        LOG.info('Checking cloud-init in target [%s] for network '
                 'configuration passthrough support.', target)
        passthrough = net.netconfig_passthrough_available(target)
        LOG.debug('passthrough available via in-target: %s', passthrough)

        # if in-target cloud-init is not updated, upgrade via cloud-init repo
        if not passthrough:
            cloud_init_yum_repo = (
                util.target_path(target,
                                 'etc/yum.repos.d/curtin-cloud-init.repo'))
            # Inject cloud-init daily yum repo
            util.write_file(cloud_init_yum_repo,
                            content=cloud_init_repo(rpm_get_dist_id(target)))

            # we separate the installation of repository packages (epel,
            # cloud-init-el-release) as we need a new invocation of yum
            # to read the newly installed repo files.
            YUM_CMD = ['yum', '-y', '--noplugins', 'install']
            retries = [1] * 30
            with util.ChrootableTarget(target) as in_chroot:
                # ensure up-to-date ca-certificates to handle https mirror
                # connections
                in_chroot.subp(YUM_CMD + ['ca-certificates'], capture=True,
                               log_captured=True, retries=retries)
                in_chroot.subp(YUM_CMD + ['epel-release'], capture=True,
                               log_captured=True, retries=retries)
                in_chroot.subp(YUM_CMD + ['cloud-init-el-release'],
                               log_captured=True, capture=True,
                               retries=retries)
                in_chroot.subp(YUM_CMD + ['cloud-init'], capture=True,
                               log_captured=True, retries=retries)

            # remove cloud-init el-stable bootstrap repo config as the
            # cloud-init-el-release package points to the correct repo
            util.del_file(cloud_init_yum_repo)

            # install bridge-utils if needed
            with util.ChrootableTarget(target) as in_chroot:
                try:
                    in_chroot.subp(['rpm', '-q', 'bridge-utils'],
                                   capture=False, rcs=[0])
                except util.ProcessExecutionError:
                    LOG.debug('Image missing bridge-utils package, installing')
                    in_chroot.subp(YUM_CMD + ['bridge-utils'], capture=True,
                                   log_captured=True, retries=retries)

    LOG.info('Passing network configuration through to target')
    net.render_netconfig_passthrough(target, netconfig={'network': netcfg})


def target_is_ubuntu_core(target):
    """"""Check if Ubuntu-Core specific directory is present at target""""""
    if target:
        return os.path.exists(util.target_path(target,
                                               'system-data/var/lib/snapd'))
    return False


def target_is_centos(target):
    """"""Check if CentOS specific file is present at target""""""
    if target:
        return os.path.exists(util.target_path(target, 'etc/centos-release'))

    return False


def target_is_rhel(target):
    """"""Check if RHEL specific file is present at target""""""
    if target:
        return os.path.exists(util.target_path(target, 'etc/redhat-release'))

    return False


def curthooks(args):
    state = util.load_command_environment()

    if args.target is not None:
        target = args.target
    else:
        target = state['target']

    if target is None:
        sys.stderr.write(""Unable to find target.  ""
                         ""Use --target or set TARGET_MOUNT_POINT\n"")
        sys.exit(2)

    cfg = config.load_command_config(args, state)
    stack_prefix = state.get('report_stack_prefix', '')

    # if curtin-hooks hook exists in target we can defer to the in-target hooks
    if util.run_hook_if_exists(target, 'curtin-hooks'):
        # For vmtests to force execute centos_apply_network_config, uncomment
        # the value in examples/tests/centos_defaults.yaml
        if cfg.get('_ammend_centos_curthooks'):
            if cfg.get('cloudconfig'):
                handle_cloudconfig(
                    cfg['cloudconfig'],
                    base_dir=util.target_path(target, 'etc/cloud/cloud.cfg.d'))

            if target_is_centos(target) or target_is_rhel(target):
                LOG.info('Detected RHEL/CentOS image, running extra hooks')
                with events.ReportEventStack(
                        name=stack_prefix, reporting_enabled=True,
                        level=""INFO"",
                        description=""Configuring CentOS for first boot""):
                    centos_apply_network_config(cfg.get('network', {}), target)
        sys.exit(0)

    if target_is_ubuntu_core(target):
        LOG.info('Detected Ubuntu-Core image, running hooks')
        with events.ReportEventStack(
                name=stack_prefix, reporting_enabled=True, level=""INFO"",
                description=""Configuring Ubuntu-Core for first boot""):
            ubuntu_core_curthooks(cfg, target)
        sys.exit(0)

    with events.ReportEventStack(
            name=stack_prefix + '/writing-config',
            reporting_enabled=True, level=""INFO"",
            description=""configuring apt configuring apt""):
        do_apt_config(cfg, target)
        disable_overlayroot(cfg, target)

    # LP: #1742560 prevent zfs-dkms from being installed (Xenial)
    if util.lsb_release(target=target)['codename'] == 'xenial':
        util.apt_update(target=target)
        with util.ChrootableTarget(target) as in_chroot:
            in_chroot.subp(['apt-mark', 'hold', 'zfs-dkms'])

    # packages may be needed prior to installing kernel
    with events.ReportEventStack(
            name=stack_prefix + '/installing-missing-packages',
            reporting_enabled=True, level=""INFO"",
            description=""installing missing packages""):
        install_missing_packages(cfg, target)

    # If a /etc/iscsi/nodes/... file was created by block_meta then it
    # needs to be copied onto the target system
    nodes_location = os.path.join(os.path.split(state['fstab'])[0],
                                  ""nodes"")
    if os.path.exists(nodes_location):
        copy_iscsi_conf(nodes_location, target)
        # do we need to reconfigure open-iscsi?

    # If a mdadm.conf file was created by block_meta than it needs to be copied
    # onto the target system
    mdadm_location = os.path.join(os.path.split(state['fstab'])[0],
                                  ""mdadm.conf"")
    if os.path.exists(mdadm_location):
        copy_mdadm_conf(mdadm_location, target)
        # as per https://bugs.launchpad.net/ubuntu/+source/mdadm/+bug/964052
        # reconfigure mdadm
        util.subp(['dpkg-reconfigure', '--frontend=noninteractive', 'mdadm'],
                  data=None, target=target)

    # if target has zfs, set ZPOOL_VDEV_NAME_PATH=1 in env (LP: #1527727).
    if (util.which(""zfs"", target=target) and
            util.lsb_release(target=target)['codename'] == 'xenial'):
        etc_env = util.target_path(target, '/etc/environment')
        export_line = '# LP: #1527727\nexport ZPOOL_VDEV_NAME_PATH=""1""\n'
        util.write_file(etc_env, content=export_line, omode=""a"")

    with events.ReportEventStack(
            name=stack_prefix + '/installing-kernel',
            reporting_enabled=True, level=""INFO"",
            description=""installing kernel""):
        setup_zipl(cfg, target)
        install_kernel(cfg, target)
        run_zipl(cfg, target)
        restore_dist_interfaces(cfg, target)

    with events.ReportEventStack(
            name=stack_prefix + '/setting-up-swap',
            reporting_enabled=True, level=""INFO"",
            description=""setting up swap""):
        add_swap(cfg, target, state.get('fstab'))

    with events.ReportEventStack(
            name=stack_prefix + '/apply-networking-config',
            reporting_enabled=True, level=""INFO"",
            description=""apply networking config""):
        apply_networking(target, state)

    with events.ReportEventStack(
            name=stack_prefix + '/writing-etc-fstab',
            reporting_enabled=True, level=""INFO"",
            description=""writing etc/fstab""):
        copy_fstab(state.get('fstab'), target)

    with events.ReportEventStack(
            name=stack_prefix + '/configuring-multipath',
            reporting_enabled=True, level=""INFO"",
            description=""configuring multipath""):
        detect_and_handle_multipath(cfg, target)

    with events.ReportEventStack(
            name=stack_prefix + '/system-upgrade',
            reporting_enabled=True, level=""INFO"",
            description=""updating packages on target system""):
        system_upgrade(cfg, target)

    # If a crypttab file was created by block_meta than it needs to be copied
    # onto the target system, and update_initramfs() needs to be run, so that
    # the cryptsetup hooks are properly configured on the installed system and
    # it will be able to open encrypted volumes at boot.
    crypttab_location = os.path.join(os.path.split(state['fstab'])[0],
                                     ""crypttab"")
    if os.path.exists(crypttab_location):
        copy_crypttab(crypttab_location, target)
        update_initramfs(target)

    # If udev dname rules were created, copy them to target
    udev_rules_d = os.path.join(state['scratch'], ""rules.d"")
    if os.path.isdir(udev_rules_d):
        copy_dname_rules(udev_rules_d, target)

    # As a rule, ARMv7 systems don't use grub. This may change some
    # day, but for now, assume no. They do require the initramfs
    # to be updated, and this also triggers boot loader setup via
    # flash-kernel.
    machine = platform.machine()
    if (machine.startswith('armv7') or
            machine.startswith('s390x') or
            machine.startswith('aarch64') and not util.is_uefi_bootable()):
        update_initramfs(target)
    else:
        setup_grub(cfg, target)

    sys.exit(0)


def POPULATE_SUBCMD(parser):
    populate_one_subcmd(parser, CMD_ARGUMENTS, curthooks)

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/commands/install.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import argparse
import json
import os
import re
import shlex
import shutil
import subprocess
import sys
import tempfile

from curtin.block import iscsi
from curtin import config
from curtin import util
from curtin import version
from curtin.log import LOG
from curtin.reporter.legacy import load_reporter
from curtin.reporter import events
from . import populate_one_subcmd

INSTALL_LOG = ""/var/log/curtin/install.log""
SAVE_INSTALL_LOG = '/root/curtin-install.log'
SAVE_INSTALL_CONFIG = '/root/curtin-install-cfg.yaml'

INSTALL_START_MSG = (""curtin: Installation started. (%s)"" %
                     version.version_string())
INSTALL_PASS_MSG = ""curtin: Installation finished.""
INSTALL_FAIL_MSG = ""curtin: Installation failed with exception: {exception}""

STAGE_DESCRIPTIONS = {
    'early': 'preparing for installation',
    'partitioning': 'configuring storage',
    'network': 'configuring network',
    'extract': 'writing install sources to disk',
    'curthooks': 'configuring installed system',
    'hook': 'finalizing installation',
    'late': 'executing late commands',
}

CONFIG_BUILTIN = {
    'sources': {},
    'stages': ['early', 'partitioning', 'network', 'extract', 'curthooks',
               'hook', 'late'],
    'extract_commands': {'builtin': ['curtin', 'extract']},
    'hook_commands': {'builtin': ['curtin', 'hook']},
    'partitioning_commands': {
        'builtin': ['curtin', 'block-meta', 'simple']},
    'curthooks_commands': {'builtin': ['curtin', 'curthooks']},
    'late_commands': {'builtin': []},
    'network_commands': {'builtin': ['curtin', 'net-meta', 'auto']},
    'apply_net_commands': {'builtin': []},
    'install': {'log_file': INSTALL_LOG}
}


def clear_install_log(logfile):
    """"""Clear the installation log, so no previous installation is present.""""""
    util.ensure_dir(os.path.dirname(logfile))
    try:
        open(logfile, 'w').close()
    except Exception:
        pass


def copy_install_log(logfile, target, log_target_path):
    """"""Copy curtin install log file to target system""""""
    basemsg = 'Cannot copy curtin install log ""%s"" to target.' % logfile
    if not logfile:
        LOG.warn(basemsg)
        return
    if not os.path.isfile(logfile):
        LOG.warn(basemsg + ""  file does not exist."")
        return

    LOG.debug('Copying curtin install log from %s to target/%s',
              logfile, log_target_path)
    util.write_file(
        filename=util.target_path(target, log_target_path),
        content=util.load_file(logfile, decode=False),
        mode=0o400, omode=""wb"")


def writeline_and_stdout(logfile, message):
    writeline(logfile, message)
    out = sys.stdout
    msg = message + ""\n""
    if hasattr(out, 'buffer'):
        out = out.buffer  # pylint: disable=no-member
        msg = msg.encode()
    out.write(msg)
    out.flush()


def writeline(fname, output):
    """"""Write a line to a file.""""""
    if not output.endswith('\n'):
        output += '\n'
    try:
        with open(fname, 'a') as fp:
            fp.write(output)
    except IOError:
        pass


class WorkingDir(object):
    def __init__(self, config):
        top_d = tempfile.mkdtemp()
        state_d = os.path.join(top_d, 'state')
        target_d = config.get('install', {}).get('target')
        if not target_d:
            target_d = os.path.join(top_d, 'target')
        scratch_d = os.path.join(top_d, 'scratch')
        for p in (state_d, target_d, scratch_d):
            os.mkdir(p)

        netconf_f = os.path.join(state_d, 'network_config')
        netstate_f = os.path.join(state_d, 'network_state')
        interfaces_f = os.path.join(state_d, 'interfaces')
        config_f = os.path.join(state_d, 'config')
        fstab_f = os.path.join(state_d, 'fstab')

        with open(config_f, ""w"") as fp:
            json.dump(config, fp)

        # just touch these files to make sure they exist
        for f in (interfaces_f, config_f, fstab_f, netconf_f, netstate_f):
            with open(f, ""ab"") as fp:
                pass

        self.scratch = scratch_d
        self.target = target_d
        self.top = top_d
        self.interfaces = interfaces_f
        self.netconf = netconf_f
        self.netstate = netstate_f
        self.fstab = fstab_f
        self.config = config
        self.config_file = config_f

    def env(self):
        return ({'WORKING_DIR': self.scratch, 'OUTPUT_FSTAB': self.fstab,
                 'OUTPUT_INTERFACES': self.interfaces,
                 'OUTPUT_NETWORK_CONFIG': self.netconf,
                 'OUTPUT_NETWORK_STATE': self.netstate,
                 'TARGET_MOUNT_POINT': self.target,
                 'CONFIG': self.config_file})


class Stage(object):

    def __init__(self, name, commands, env, reportstack=None, logfile=None):
        self.name = name
        self.commands = commands
        self.env = env
        if logfile is None:
            logfile = INSTALL_LOG
        self.install_log = self._open_install_log(logfile)

        if hasattr(sys.stdout, 'buffer'):
            self.write_stdout = self._write_stdout3
        else:
            self.write_stdout = self._write_stdout2

        if reportstack is None:
            reportstack = events.ReportEventStack(
                name=""stage-%s"" % name, description=""basic stage %s"" % name,
                reporting_enabled=False)
        self.reportstack = reportstack

    def _open_install_log(self, logfile):
        """"""Open the install log.""""""
        if not logfile:
            return None
        try:
            return open(logfile, 'ab')
        except IOError:
            return None

    def _write_stdout3(self, data):
        sys.stdout.buffer.write(data)  # pylint: disable=no-member
        sys.stdout.flush()

    def _write_stdout2(self, data):
        sys.stdout.write(data)
        sys.stdout.flush()

    def write(self, data):
        """"""Write data to stdout and to the install_log.""""""
        self.write_stdout(data)
        if self.install_log is not None:
            self.install_log.write(data)
            self.install_log.flush()

    def run(self):
        for cmdname in sorted(self.commands.keys()):
            cmd = self.commands[cmdname]
            if not cmd:
                continue
            cur_res = events.ReportEventStack(
                name=cmdname, description=""running '%s'"" % ' '.join(cmd),
                parent=self.reportstack, level=""DEBUG"")

            env = self.env.copy()
            env['CURTIN_REPORTSTACK'] = cur_res.fullname

            # LP: #1527727: This must be in the environment on xenial installs
            # anywhere that grub might be invoked (apt or setup-grub).
            env['ZPOOL_VDEV_NAME_PATH'] = ""1""

            shell = not isinstance(cmd, list)
            with util.LogTimer(LOG.debug, cmdname):
                with cur_res:
                    try:
                        sp = subprocess.Popen(
                            cmd, stdout=subprocess.PIPE,
                            stderr=subprocess.STDOUT,
                            env=env, shell=shell)
                    except OSError as e:
                        LOG.warn(""%s command failed"", cmdname)
                        raise util.ProcessExecutionError(cmd=cmd, reason=e)

                    output = b""""
                    while True:
                        data = sp.stdout.read(1)
                        if not data and sp.poll() is not None:
                            break
                        self.write(data)
                        output += data

                    rc = sp.returncode
                    if rc != 0:
                        LOG.warn(""%s command failed"", cmdname)
                        raise util.ProcessExecutionError(
                            stdout=output, stderr="""",
                            exit_code=rc, cmd=cmd)


def apply_power_state(pstate):
    """"""
    power_state:
     delay: 5
     mode: poweroff
     message: Bye Bye
    """"""
    cmd = load_power_state(pstate)
    if not cmd:
        return

    LOG.info(""powering off with %s"", cmd)
    fid = os.fork()
    if fid == 0:
        try:
            util.subp(cmd)
            os._exit(0)
        except Exception as e:
            LOG.warn(""%s returned non-zero: %s"" % (cmd, e))
            os._exit(1)
    return


def load_power_state(pstate):
    """"""Returns a command to reboot the system if power_state should.""""""
    if pstate is None:
        return None

    if not isinstance(pstate, dict):
        raise TypeError(""power_state is not a dict."")

    opt_map = {'halt': '-H', 'poweroff': '-P', 'reboot': '-r'}

    mode = pstate.get(""mode"")
    if mode not in opt_map:
        raise TypeError(""power_state[mode] required, must be one of: %s."" %
                        ','.join(opt_map.keys()))

    delay = pstate.get(""delay"", ""5"")
    if delay == ""now"":
        delay = ""0""
    elif re.match(r""\+[0-9]+"", str(delay)):
        delay = ""%sm"" % delay[1:]
    else:
        delay = str(delay)

    args = [""shutdown"", opt_map[mode], ""now""]
    if pstate.get(""message""):
        args.append(pstate.get(""message""))

    shcmd = ('sleep ""$1"" && shift; '
             '[ -f /run/block-curtin-poweroff ] && exit 0; '
             'exec ""$@""')

    return (['sh', '-c', shcmd, 'curtin-poweroff', delay] + args)


def apply_kexec(kexec, target):
    """"""
    load kexec kernel from target dir, similar to /etc/init.d/kexec-load
    kexec:
     mode: on
    """"""
    grubcfg = ""boot/grub/grub.cfg""
    target_grubcfg = os.path.join(target, grubcfg)

    if kexec is None or kexec.get(""mode"") != ""on"":
        return False

    if not isinstance(kexec, dict):
        raise TypeError(""kexec is not a dict."")

    if not util.which('kexec'):
        util.install_packages('kexec-tools')

    if not os.path.isfile(target_grubcfg):
        raise ValueError(""%s does not exist in target"" % grubcfg)

    with open(target_grubcfg, ""r"") as fp:
        default = 0
        menu_lines = []

        # get the default grub boot entry number and menu entry line numbers
        for line_num, line in enumerate(fp, 1):
            if re.search(r""\bset default=\""[0-9]+\""\b"", "" %s "" % line):
                default = int(re.sub(r""[^0-9]"", '', line))
            if re.search(r""\bmenuentry\b"", "" %s "" % line):
                menu_lines.append(line_num)

        if not menu_lines:
            LOG.error(""grub config file does not have a menuentry\n"")
            return False

        # get the begin and end line numbers for default menuentry section,
        # using end of file if it's the last menuentry section
        begin = menu_lines[default]
        if begin != menu_lines[-1]:
            end = menu_lines[default + 1] - 1
        else:
            end = line_num

        fp.seek(0)
        lines = fp.readlines()
        kernel = append = initrd = """"

        for i in range(begin, end):
            if 'linux' in lines[i].split():
                split_line = shlex.split(lines[i])
                kernel = os.path.join(target, split_line[1])
                append = ""--append="" + ' '.join(split_line[2:])
            if 'initrd' in lines[i].split():
                split_line = shlex.split(lines[i])
                initrd = ""--initrd="" + os.path.join(target, split_line[1])

        if not kernel:
            LOG.error(""grub config file does not have a kernel\n"")
            return False

        LOG.debug(""kexec -l %s %s %s"" % (kernel, append, initrd))
        util.subp(args=['kexec', '-l', kernel, append, initrd])
        return True


def migrate_proxy_settings(cfg):
    """"""Move the legacy proxy setting 'http_proxy' into cfg['proxy'].""""""
    proxy = cfg.get('proxy', {})
    if not isinstance(proxy, dict):
        raise ValueError(""'proxy' in config is not a dictionary: %s"" % proxy)

    if 'http_proxy' in cfg:
        hp = cfg['http_proxy']
        if hp:
            if proxy.get('http_proxy', hp) != hp:
                LOG.warn(""legacy http_proxy setting (%s) differs from ""
                         ""proxy/http_proxy (%s), using %s"",
                         hp, proxy['http_proxy'], proxy['http_proxy'])
            else:
                LOG.debug(""legacy 'http_proxy' migrated to proxy/http_proxy"")
                proxy['http_proxy'] = hp
        del cfg['http_proxy']

    cfg['proxy'] = proxy


def cmd_install(args):
    cfg = CONFIG_BUILTIN.copy()
    config.merge_config(cfg, args.config)

    for source in args.source:
        src = util.sanitize_source(source)
        cfg['sources'][""%02d_cmdline"" % len(cfg['sources'])] = src

    LOG.info(INSTALL_START_MSG)
    LOG.debug('LANG=%s', os.environ.get('LANG'))
    LOG.debug(""merged config: %s"" % cfg)
    if not len(cfg.get('sources', [])):
        raise util.BadUsage(""no sources provided to install"")

    for i in cfg['sources']:
        # we default to tgz for old style sources config
        cfg['sources'][i] = util.sanitize_source(cfg['sources'][i])

    migrate_proxy_settings(cfg)
    for k in ('http_proxy', 'https_proxy', 'no_proxy'):
        if k in cfg['proxy']:
            os.environ[k] = cfg['proxy'][k]

    instcfg = cfg.get('install', {})
    logfile = instcfg.get('log_file')
    post_files = instcfg.get('post_files', [logfile])

    # Generate curtin configuration dump and add to write_files unless
    # installation config disables dump
    yaml_dump_file = instcfg.get('save_install_config', SAVE_INSTALL_CONFIG)
    if yaml_dump_file:
        write_files = cfg.get('write_files', {})
        write_files['curtin_install_cfg'] = {
            'path': yaml_dump_file,
            'permissions': '0400',
            'owner': 'root:root',
            'content': config.dump_config(cfg)
        }
        cfg['write_files'] = write_files

    # Load reporter
    clear_install_log(logfile)
    post_files = cfg.get('post_files', [logfile])
    legacy_reporter = load_reporter(cfg)
    legacy_reporter.files = post_files

    writeline_and_stdout(logfile, INSTALL_START_MSG)
    args.reportstack.post_files = post_files
    try:
        dd_images = util.get_dd_images(cfg.get('sources', {}))
        if len(dd_images) > 1:
            raise ValueError(""You may not use more then one disk image"")

        workingd = WorkingDir(cfg)
        LOG.debug(workingd.env())
        env = os.environ.copy()
        env.update(workingd.env())

        for name in cfg.get('stages'):
            desc = STAGE_DESCRIPTIONS.get(name, ""stage %s"" % name)
            reportstack = events.ReportEventStack(
                ""stage-%s"" % name, description=desc,
                parent=args.reportstack)
            env['CURTIN_REPORTSTACK'] = reportstack.fullname

            with reportstack:
                commands_name = '%s_commands' % name
                with util.LogTimer(LOG.debug, 'stage_%s' % name):
                    stage = Stage(name, cfg.get(commands_name, {}), env,
                                  reportstack=reportstack, logfile=logfile)
                    stage.run()

        if apply_kexec(cfg.get('kexec'), workingd.target):
            cfg['power_state'] = {'mode': 'reboot', 'delay': 'now',
                                  'message': ""'rebooting with kexec'""}

        writeline_and_stdout(logfile, INSTALL_PASS_MSG)
        legacy_reporter.report_success()
    except Exception as e:
        exp_msg = INSTALL_FAIL_MSG.format(exception=e)
        writeline(logfile, exp_msg)
        LOG.error(exp_msg)
        legacy_reporter.report_failure(exp_msg)
        raise e
    finally:
        log_target_path = instcfg.get('save_install_log', SAVE_INSTALL_LOG)
        if log_target_path:
            copy_install_log(logfile, workingd.target, log_target_path)

        if instcfg.get('unmount', """") == ""disabled"":
            LOG.info('Skipping unmount: config disabled target unmounting')
            return

        # unmount everything (including iscsi disks)
        util.do_umount(workingd.target, recursive=True)

        # The open-iscsi service in the ephemeral environment handles
        # disconnecting active sessions.  On Artful release the systemd
        # unit file has conditionals that are not met at boot time and
        # results in open-iscsi service not being started; This breaks
        # shutdown on Artful releases.
        # Additionally, in release < Artful, if the storage configuration
        # is layered, like RAID over iscsi volumes, then disconnecting iscsi
        # sessions before stopping the raid device hangs.
        # As it turns out, letting the open-iscsi service take down the
        # session last is the cleanest way to handle all releases regardless
        # of what may be layered on top of the iscsi disks.
        #
        # Check if storage configuration has iscsi volumes and if so ensure
        # iscsi service is active before exiting install
        if iscsi.get_iscsi_disks_from_config(cfg):
            iscsi.restart_iscsi_service()

        shutil.rmtree(workingd.top)

    apply_power_state(cfg.get('power_state'))

    sys.exit(0)


# we explicitly accept config on install for backwards compatibility
CMD_ARGUMENTS = (
    ((('-c', '--config'),
      {'help': 'read configuration from cfg', 'action': util.MergedCmdAppend,
       'metavar': 'FILE', 'type': argparse.FileType(""rb""),
       'dest': 'cfgopts', 'default': []}),
     ('--set', {'action': util.MergedCmdAppend,
                'help': ('define a config variable. key can be a ""/"" '
                         'delimited path (""early_commands/cmd1=a""). if '
                         'key starts with ""json:"" then val is loaded as '
                         'json (json:stages=""[\'early\']"")'),
                'metavar': 'key=val', 'dest': 'cfgopts'}),
     ('source', {'help': 'what to install', 'nargs': '*'}),
     )
)


def POPULATE_SUBCMD(parser):
    populate_one_subcmd(parser, CMD_ARGUMENTS, cmd_install)

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/deps/__init__.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import os
import sys

from curtin.util import (
    ProcessExecutionError,
    get_architecture,
    install_packages,
    is_uefi_bootable,
    lsb_release,
    subp,
    which,
)

REQUIRED_IMPORTS = [
    # import string to execute, python2 package, python3 package
    ('import yaml', 'python-yaml', 'python3-yaml'),
]

REQUIRED_EXECUTABLES = [
    # executable in PATH, package
    ('file', 'file'),
    ('lvcreate', 'lvm2'),
    ('mdadm', 'mdadm'),
    ('mkfs.vfat', 'dosfstools'),
    ('mkfs.btrfs', 'btrfs-tools'),
    ('mkfs.ext4', 'e2fsprogs'),
    ('mkfs.xfs', 'xfsprogs'),
    ('partprobe', 'parted'),
    ('sgdisk', 'gdisk'),
    ('udevadm', 'udev'),
    ('make-bcache', 'bcache-tools'),
    ('iscsiadm', 'open-iscsi'),
]

REQUIRED_KERNEL_MODULES = [
    # kmod name
]

if lsb_release()['codename'] == ""precise"":
    REQUIRED_IMPORTS.append(
        ('import oauth.oauth', 'python-oauth', None),)
else:
    REQUIRED_IMPORTS.append(
        ('import oauthlib.oauth1', 'python-oauthlib', 'python3-oauthlib'),)

# zfs is > trusty only
if not lsb_release()['codename'] in [""precise"", ""trusty""]:
    REQUIRED_EXECUTABLES.append(('zfs', 'zfsutils-linux'))
    REQUIRED_KERNEL_MODULES.append('zfs')

if not is_uefi_bootable() and 'arm' in get_architecture():
    REQUIRED_EXECUTABLES.append(('flash-kernel', 'flash-kernel'))


class MissingDeps(Exception):
    def __init__(self, message, deps):
        self.message = message
        if isinstance(deps, str) or deps is None:
            deps = [deps]
        self.deps = [d for d in deps if d is not None]
        self.fatal = None in deps

    def __str__(self):
        if self.fatal:
            if not len(self.deps):
                return self.message + "" Unresolvable.""
            return (self.message +
                    "" Unresolvable.  Partially resolvable with packages: %s"" %
                    ' '.join(self.deps))
        else:
            return self.message + "" Install packages: %s"" % ' '.join(self.deps)


def check_import(imports, py2pkgs, py3pkgs, message=None):
    import_group = imports
    if isinstance(import_group, str):
        import_group = [import_group]

    for istr in import_group:
        try:
            exec(istr)
            return
        except ImportError:
            pass

    if not message:
        if isinstance(imports, str):
            message = ""Failed '%s'."" % imports
        else:
            message = ""Unable to do any of %s."" % import_group

    if sys.version_info[0] == 2:
        pkgs = py2pkgs
    else:
        pkgs = py3pkgs

    raise MissingDeps(message, pkgs)


def check_executable(cmdname, pkg):
    if not which(cmdname):
        raise MissingDeps(""Missing program '%s'."" % cmdname, pkg)


def check_executables(executables=None):
    if executables is None:
        executables = REQUIRED_EXECUTABLES
    mdeps = []
    for exe, pkg in executables:
        try:
            check_executable(exe, pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def check_imports(imports=None):
    if imports is None:
        imports = REQUIRED_IMPORTS

    mdeps = []
    for import_str, py2pkg, py3pkg in imports:
        try:
            check_import(import_str, py2pkg, py3pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def check_kernel_modules(modules=None):
    if modules is None:
        modules = REQUIRED_KERNEL_MODULES

    # if we're missing any modules, install the full
    # linux-image package for this environment
    for kmod in modules:
        try:
            subp(['modinfo', '--filename', kmod], capture=True)
        except ProcessExecutionError:
            kernel_pkg = 'linux-image-%s' % os.uname()[2]
            return [MissingDeps('missing kernel module %s' % kmod, kernel_pkg)]

    return []


def find_missing_deps():
    return check_executables() + check_imports() + check_kernel_modules()


def install_deps(verbosity=False, dry_run=False, allow_daemons=True):
    errors = find_missing_deps()
    if len(errors) == 0:
        if verbosity:
            sys.stderr.write(""No missing dependencies\n"")
        return 0

    missing_pkgs = []
    for e in errors:
        missing_pkgs += e.deps

    deps_string = ' '.join(sorted(missing_pkgs))

    if dry_run:
        sys.stderr.write(""Missing dependencies: %s\n"" % deps_string)
        return 0

    if os.geteuid() != 0:
        sys.stderr.write(""Missing dependencies: %s\n"" % deps_string)
        sys.stderr.write(""Package installation is not possible as non-root.\n"")
        return 2

    if verbosity:
        sys.stderr.write(""Installing %s\n"" % deps_string)

    ret = 0
    try:
        install_packages(missing_pkgs, allow_daemons=allow_daemons,
                         aptopts=[""--no-install-recommends""])
    except ProcessExecutionError as e:
        sys.stderr.write(""%s\n"" % e)
        ret = e.exit_code

    return ret

# vi: ts=4 expandtab syntax=python
/n/n/ncurtin/util.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import argparse
import collections
from contextlib import contextmanager
import errno
import glob
import json
import os
import platform
import re
import shlex
import shutil
import socket
import subprocess
import stat
import sys
import tempfile
import time

# avoid the dependency to python3-six as used in cloud-init
try:
    from urlparse import urlparse
except ImportError:
    # python3
    # avoid triggering pylint, https://github.com/PyCQA/pylint/issues/769
    # pylint:disable=import-error,no-name-in-module
    from urllib.parse import urlparse

try:
    string_types = (basestring,)
except NameError:
    string_types = (str,)

try:
    numeric_types = (int, float, long)
except NameError:
    # python3 does not have a long type.
    numeric_types = (int, float)

from .log import LOG

_INSTALLED_HELPERS_PATH = 'usr/lib/curtin/helpers'
_INSTALLED_MAIN = 'usr/bin/curtin'

_LSB_RELEASE = {}
_USES_SYSTEMD = None
_HAS_UNSHARE_PID = None

_DNS_REDIRECT_IP = None

# matcher used in template rendering functions
BASIC_MATCHER = re.compile(r'\$\{([A-Za-z0-9_.]+)\}|\$([A-Za-z0-9_.]+)')


def _subp(args, data=None, rcs=None, env=None, capture=False,
          combine_capture=False, shell=False, logstring=False,
          decode=""replace"", target=None, cwd=None, log_captured=False,
          unshare_pid=None):
    if rcs is None:
        rcs = [0]
    devnull_fp = None

    tpath = target_path(target)
    chroot_args = [] if tpath == ""/"" else ['chroot', target]
    sh_args = ['sh', '-c'] if shell else []
    if isinstance(args, string_types):
        args = [args]

    try:
        unshare_args = _get_unshare_pid_args(unshare_pid, tpath)
    except RuntimeError as e:
        raise RuntimeError(""Unable to unshare pid (cmd=%s): %s"" % (args, e))

    args = unshare_args + chroot_args + sh_args + list(args)

    if not logstring:
        LOG.debug(
            ""Running command %s with allowed return codes %s (capture=%s)"",
            args, rcs, 'combine' if combine_capture else capture)
    else:
        LOG.debug((""Running hidden command to protect sensitive ""
                   ""input/output logstring: %s""), logstring)
    try:
        stdin = None
        stdout = None
        stderr = None
        if capture:
            stdout = subprocess.PIPE
            stderr = subprocess.PIPE
        if combine_capture:
            stdout = subprocess.PIPE
            stderr = subprocess.STDOUT
        if data is None:
            devnull_fp = open(os.devnull)
            stdin = devnull_fp
        else:
            stdin = subprocess.PIPE
        sp = subprocess.Popen(args, stdout=stdout,
                              stderr=stderr, stdin=stdin,
                              env=env, shell=False, cwd=cwd)
        # communicate in python2 returns str, python3 returns bytes
        (out, err) = sp.communicate(data)

        # Just ensure blank instead of none.
        if not out and capture:
            out = b''
        if not err and capture:
            err = b''
        if decode:
            def ldecode(data, m='utf-8'):
                if not isinstance(data, bytes):
                    return data
                return data.decode(m, errors=decode)

            out = ldecode(out)
            err = ldecode(err)
    except OSError as e:
        raise ProcessExecutionError(cmd=args, reason=e)
    finally:
        if devnull_fp:
            devnull_fp.close()

    if capture and log_captured:
        LOG.debug(""Command returned stdout=%s, stderr=%s"", out, err)

    rc = sp.returncode  # pylint: disable=E1101
    if rc not in rcs:
        raise ProcessExecutionError(stdout=out, stderr=err,
                                    exit_code=rc,
                                    cmd=args)
    return (out, err)


def _has_unshare_pid():
    global _HAS_UNSHARE_PID
    if _HAS_UNSHARE_PID is not None:
        return _HAS_UNSHARE_PID

    if not which('unshare'):
        _HAS_UNSHARE_PID = False
        return False
    out, err = subp([""unshare"", ""--help""], capture=True, decode=False,
                    unshare_pid=False)
    joined = b'\n'.join([out, err])
    _HAS_UNSHARE_PID = b'--fork' in joined and b'--pid' in joined
    return _HAS_UNSHARE_PID


def _get_unshare_pid_args(unshare_pid=None, target=None, euid=None):
    """"""Get args for calling unshare for a pid.

    If unshare_pid is False, return empty list.
    If unshare_pid is True, check if it is usable.  If not, raise exception.
    if unshare_pid is None, then unshare if
       * euid is 0
       * 'unshare' with '--fork' and '--pid' is available.
       * target != /
    """"""
    if unshare_pid is not None and not unshare_pid:
        # given a false-ish other than None means no.
        return []

    if euid is None:
        euid = os.geteuid()

    tpath = target_path(target)

    unshare_pid_in = unshare_pid
    if unshare_pid is None:
        unshare_pid = False
        if tpath != ""/"" and euid == 0:
            if _has_unshare_pid():
                unshare_pid = True

    if not unshare_pid:
        return []

    # either unshare was passed in as True, or None and turned to True.
    if euid != 0:
        raise RuntimeError(
            ""given unshare_pid=%s but euid (%s) != 0."" %
            (unshare_pid_in, euid))

    if not _has_unshare_pid():
        raise RuntimeError(
            ""given unshare_pid=%s but no unshare command."" % unshare_pid_in)

    return ['unshare', '--fork', '--pid', '--']


def subp(*args, **kwargs):
    """"""Run a subprocess.

    :param args: command to run in a list. [cmd, arg1, arg2...]
    :param data: input to the command, made available on its stdin.
    :param rcs:
        a list of allowed return codes.  If subprocess exits with a value not
        in this list, a ProcessExecutionError will be raised.  By default,
        data is returned as a string.  See 'decode' parameter.
    :param env: a dictionary for the command's environment.
    :param capture:
        boolean indicating if output should be captured.  If True, then stderr
        and stdout will be returned.  If False, they will not be redirected.
    :param combine_capture:
        boolean indicating if stderr should be redirected to stdout. When True,
        interleaved stderr and stdout will be returned as the first element of
        a tuple.
    :param log_captured:
        boolean indicating if output should be logged on capture.  If
        True, then stderr and stdout will be logged at DEBUG level.  If
        False, they will not be logged.
    :param shell: boolean indicating if this should be run with a shell.
    :param logstring:
        the command will be logged to DEBUG.  If it contains info that should
        not be logged, then logstring will be logged instead.
    :param decode:
        if False, no decoding will be done and returned stdout and stderr will
        be bytes.  Other allowed values are 'strict', 'ignore', and 'replace'.
        These values are passed through to bytes().decode() as the 'errors'
        parameter.  There is no support for decoding to other than utf-8.
    :param retries:
        a list of times to sleep in between retries.  After each failure
        subp will sleep for N seconds and then try again.  A value of [1, 3]
        means to run, sleep 1, run, sleep 3, run and then return exit code.
    :param target:
        run the command as 'chroot target <args>'
    :param unshare_pid:
        unshare the pid namespace.
        default value (None) is to unshare pid namespace if possible
        and target != /

    :return
        if not capturing, return is (None, None)
        if capturing, stdout and stderr are returned.
            if decode:
                python2 unicode or python3 string
            if not decode:
                python2 string or python3 bytes
    """"""
    retries = []
    if ""retries"" in kwargs:
        retries = kwargs.pop(""retries"")
        if not retries:
            # allow retries=None
            retries = []

    if args:
        cmd = args[0]
    if 'args' in kwargs:
        cmd = kwargs['args']

    # Retry with waits between the retried command.
    for num, wait in enumerate(retries):
        try:
            return _subp(*args, **kwargs)
        except ProcessExecutionError as e:
            LOG.debug(""try %s: command %s failed, rc: %s"", num,
                      cmd, e.exit_code)
            time.sleep(wait)
    # Final try without needing to wait or catch the error. If this
    # errors here then it will be raised to the caller.
    return _subp(*args, **kwargs)


def wait_for_removal(path, retries=[1, 3, 5, 7]):
    if not path:
        raise ValueError('wait_for_removal: missing path parameter')

    # Retry with waits between checking for existence
    LOG.debug('waiting for %s to be removed', path)
    for num, wait in enumerate(retries):
        if not os.path.exists(path):
            LOG.debug('%s has been removed', path)
            return
        LOG.debug('sleeping %s', wait)
        time.sleep(wait)

    # final check
    if not os.path.exists(path):
        LOG.debug('%s has been removed', path)
        return

    raise OSError('Timeout exceeded for removal of %s', path)


def load_command_environment(env=os.environ, strict=False):

    mapping = {'scratch': 'WORKING_DIR', 'fstab': 'OUTPUT_FSTAB',
               'interfaces': 'OUTPUT_INTERFACES', 'config': 'CONFIG',
               'target': 'TARGET_MOUNT_POINT',
               'network_state': 'OUTPUT_NETWORK_STATE',
               'network_config': 'OUTPUT_NETWORK_CONFIG',
               'report_stack_prefix': 'CURTIN_REPORTSTACK'}

    if strict:
        missing = [k for k in mapping.values() if k not in env]
        if len(missing):
            raise KeyError(""missing environment vars: %s"" % missing)

    return {k: env.get(v) for k, v in mapping.items()}


def is_kmod_loaded(module):
    """"""Test if kernel module 'module' is current loaded by checking sysfs""""""

    if not module:
        raise ValueError('is_kmod_loaded: invalid module: ""%s""', module)

    return os.path.isdir('/sys/module/%s' % module)


def load_kernel_module(module, check_loaded=True):
    """"""Install kernel module via modprobe.  Optionally check if it's already
       loaded .
    """"""

    if not module:
        raise ValueError('load_kernel_module: invalid module: ""%s""', module)

    if check_loaded:
        if is_kmod_loaded(module):
            LOG.debug('Skipping kernel module load, %s already loaded', module)
            return

    LOG.debug('Loading kernel module %s via modprobe', module)
    subp(['modprobe', '--use-blacklist', module])


class BadUsage(Exception):
    pass


class ProcessExecutionError(IOError):

    MESSAGE_TMPL = ('%(description)s\n'
                    'Command: %(cmd)s\n'
                    'Exit code: %(exit_code)s\n'
                    'Reason: %(reason)s\n'
                    'Stdout: %(stdout)s\n'
                    'Stderr: %(stderr)s')
    stdout_indent_level = 8

    def __init__(self, stdout=None, stderr=None,
                 exit_code=None, cmd=None,
                 description=None, reason=None):
        if not cmd:
            self.cmd = '-'
        else:
            self.cmd = cmd

        if not description:
            self.description = 'Unexpected error while running command.'
        else:
            self.description = description

        if not isinstance(exit_code, int):
            self.exit_code = '-'
        else:
            self.exit_code = exit_code

        if not stderr:
            self.stderr = ""''""
        else:
            self.stderr = self._indent_text(stderr)

        if not stdout:
            self.stdout = ""''""
        else:
            self.stdout = self._indent_text(stdout)

        if reason:
            self.reason = reason
        else:
            self.reason = '-'

        message = self.MESSAGE_TMPL % {
            'description': self.description,
            'cmd': self.cmd,
            'exit_code': self.exit_code,
            'stdout': self.stdout,
            'stderr': self.stderr,
            'reason': self.reason,
        }
        IOError.__init__(self, message)

    def _indent_text(self, text):
        if type(text) == bytes:
            text = text.decode()
        return text.replace('\n', '\n' + ' ' * self.stdout_indent_level)


class LogTimer(object):
    def __init__(self, logfunc, msg):
        self.logfunc = logfunc
        self.msg = msg

    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, etype, value, trace):
        self.logfunc(""%s took %0.3f seconds"" %
                     (self.msg, time.time() - self.start))


def is_mounted(target, src=None, opts=None):
    # return whether or not src is mounted on target
    mounts = """"
    with open(""/proc/mounts"", ""r"") as fp:
        mounts = fp.read()

    for line in mounts.splitlines():
        if line.split()[1] == os.path.abspath(target):
            return True
    return False


def list_device_mounts(device):
    # return mount entry if device is in /proc/mounts
    mounts = """"
    with open(""/proc/mounts"", ""r"") as fp:
        mounts = fp.read()

    dev_mounts = []
    for line in mounts.splitlines():
        if line.split()[0] == device:
            dev_mounts.append(line)
    return dev_mounts


def fuser_mount(path):
    """""" Execute fuser to determine open file handles from mountpoint path

        Use verbose mode and then combine stdout, stderr from fuser into
        a dictionary:

        {pid: ""fuser-details""}

        path may also be a kernel devpath (e.g. /dev/sda)

    """"""
    fuser_output = {}
    try:
        stdout, stderr = subp(['fuser', '--verbose', '--mount', path],
                              capture=True)
    except ProcessExecutionError as e:
        LOG.debug('fuser returned non-zero: %s', e.stderr)
        return None

    pidlist = stdout.split()

    """"""
    fuser writes a header in verbose mode, we'll ignore that but the
    order if the input is <mountpoint> <user> <pid*> <access> <command>

    note that <pid> is not present in stderr, it's only in stdout.  Also
    only the entry with pid=kernel entry will contain the mountpoint

    # Combined stdout and stderr look like:
    #                      USER        PID ACCESS COMMAND
    # /home:               root     kernel mount /
    #                      root          1 .rce. systemd
    #
    # This would return
    #
    {
        'kernel': ['/home', 'root', 'mount', '/'],
        '1': ['root', '1', '.rce.', 'systemd'],
    }
    """"""
    # Note that fuser only writes PIDS to stdout. Each PID value is
    # 'kernel' or an integer and indicates a process which has an open
    # file handle against the path specified path. All other output
    # is sent to stderr.  This code below will merge the two as needed.
    for (pid, status) in zip(pidlist, stderr.splitlines()[1:]):
        fuser_output[pid] = status.split()

    return fuser_output


@contextmanager
def chdir(dirname):
    curdir = os.getcwd()
    try:
        os.chdir(dirname)
        yield dirname
    finally:
        os.chdir(curdir)


def do_mount(src, target, opts=None):
    # mount src at target with opts and return True
    # if already mounted, return False
    if opts is None:
        opts = []
    if isinstance(opts, str):
        opts = [opts]

    if is_mounted(target, src, opts):
        return False

    ensure_dir(target)
    cmd = ['mount'] + opts + [src, target]
    subp(cmd)
    return True


def do_umount(mountpoint, recursive=False):
    # unmount mountpoint. if recursive, unmount all mounts under it.
    # return boolean indicating if mountpoint was previously mounted.
    mp = os.path.abspath(mountpoint)
    ret = False
    for line in reversed(load_file(""/proc/mounts"", decode=True).splitlines()):
        curmp = line.split()[1]
        if curmp == mp or (recursive and curmp.startswith(mp + os.path.sep)):
            subp(['umount', curmp])
        if curmp == mp:
            ret = True
    return ret


def ensure_dir(path, mode=None):
    try:
        os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise

    if mode is not None:
        os.chmod(path, mode)


def write_file(filename, content, mode=0o644, omode=""w""):
    """"""
    write 'content' to file at 'filename' using python open mode 'omode'.
    if mode is not set, then chmod file to mode. mode is 644 by default
    """"""
    ensure_dir(os.path.dirname(filename))
    with open(filename, omode) as fp:
        fp.write(content)
    if mode:
        os.chmod(filename, mode)


def load_file(path, read_len=None, offset=0, decode=True):
    with open(path, ""rb"") as fp:
        if offset:
            fp.seek(offset)
        contents = fp.read(read_len) if read_len else fp.read()

    if decode:
        return decode_binary(contents)
    else:
        return contents


def decode_binary(blob, encoding='utf-8', errors='replace'):
    # Converts a binary type into a text type using given encoding.
    return blob.decode(encoding, errors=errors)


def file_size(path):
    """"""get the size of a file""""""
    with open(path, 'rb') as fp:
        fp.seek(0, 2)
        return fp.tell()


def del_file(path):
    try:
        os.unlink(path)
        LOG.debug(""del_file: removed %s"", path)
    except OSError as e:
        LOG.exception(""del_file: %s did not exist."", path)
        if e.errno != errno.ENOENT:
            raise e


def disable_daemons_in_root(target):
    contents = ""\n"".join(
        ['#!/bin/sh',
         '# see invoke-rc.d for exit codes. 101 is ""do not run""',
         'while true; do',
         '   case ""$1"" in',
         '      -*) shift;;',
         '      makedev|x11-common) exit 0;;',
         '      *) exit 101;;',
         '   esac',
         'done',
         ''])

    fpath = target_path(target, ""/usr/sbin/policy-rc.d"")

    if os.path.isfile(fpath):
        return False

    write_file(fpath, mode=0o755, content=contents)
    return True


def undisable_daemons_in_root(target):
    try:
        os.unlink(target_path(target, ""/usr/sbin/policy-rc.d""))
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise
        return False
    return True


class ChrootableTarget(object):
    def __init__(self, target, allow_daemons=False, sys_resolvconf=True):
        if target is None:
            target = ""/""
        self.target = target_path(target)
        self.mounts = [""/dev"", ""/proc"", ""/sys""]
        self.umounts = []
        self.disabled_daemons = False
        self.allow_daemons = allow_daemons
        self.sys_resolvconf = sys_resolvconf
        self.rconf_d = None

    def __enter__(self):
        for p in self.mounts:
            tpath = target_path(self.target, p)
            if do_mount(p, tpath, opts='--bind'):
                self.umounts.append(tpath)

        if not self.allow_daemons:
            self.disabled_daemons = disable_daemons_in_root(self.target)

        rconf = target_path(self.target, ""/etc/resolv.conf"")
        target_etc = os.path.dirname(rconf)
        if self.target != ""/"" and os.path.isdir(target_etc):
            # never muck with resolv.conf on /
            rconf = os.path.join(target_etc, ""resolv.conf"")
            rtd = None
            try:
                rtd = tempfile.mkdtemp(dir=target_etc)
                tmp = os.path.join(rtd, ""resolv.conf"")
                os.rename(rconf, tmp)
                self.rconf_d = rtd
                shutil.copy(""/etc/resolv.conf"", rconf)
            except Exception:
                if rtd:
                    shutil.rmtree(rtd)
                    self.rconf_d = None
                raise

        return self

    def __exit__(self, etype, value, trace):
        if self.disabled_daemons:
            undisable_daemons_in_root(self.target)

        # if /dev is to be unmounted, udevadm settle (LP: #1462139)
        if target_path(self.target, ""/dev"") in self.umounts:
            subp(['udevadm', 'settle'])

        for p in reversed(self.umounts):
            do_umount(p)

        rconf = target_path(self.target, ""/etc/resolv.conf"")
        if self.sys_resolvconf and self.rconf_d:
            os.rename(os.path.join(self.rconf_d, ""resolv.conf""), rconf)
            shutil.rmtree(self.rconf_d)

    def subp(self, *args, **kwargs):
        kwargs['target'] = self.target
        return subp(*args, **kwargs)

    def path(self, path):
        return target_path(self.target, path)


def is_exe(fpath):
    # Return path of program for execution if found in path
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


def which(program, search=None, target=None):
    target = target_path(target)

    if os.path.sep in program:
        # if program had a '/' in it, then do not search PATH
        # 'which' does consider cwd here. (cd / && which bin/ls) = bin/ls
        # so effectively we set cwd to / (or target)
        if is_exe(target_path(target, program)):
            return program

    if search is None:
        paths = [p.strip('""') for p in
                 os.environ.get(""PATH"", """").split(os.pathsep)]
        if target == ""/"":
            search = paths
        else:
            search = [p for p in paths if p.startswith(""/"")]

    # normalize path input
    search = [os.path.abspath(p) for p in search]

    for path in search:
        ppath = os.path.sep.join((path, program))
        if is_exe(target_path(target, ppath)):
            return ppath

    return None


def _installed_file_path(path, check_file=None):
    # check the install root for the file 'path'.
    #  if 'check_file', then path is a directory that contains file.
    # return absolute path or None.
    inst_pre = ""/""
    if os.environ.get('SNAP'):
        inst_pre = os.path.abspath(os.environ['SNAP'])
    inst_path = os.path.join(inst_pre, path)
    if check_file:
        check_path = os.path.sep.join((inst_path, check_file))
    else:
        check_path = inst_path

    if os.path.isfile(check_path):
        return os.path.abspath(inst_path)
    return None


def get_paths(curtin_exe=None, lib=None, helpers=None):
    # return a dictionary with paths for 'curtin_exe', 'helpers' and 'lib'
    # that represent where 'curtin' executable lives, where the 'curtin' module
    # directory is (containing __init__.py) and where the 'helpers' directory.
    mydir = os.path.realpath(os.path.dirname(__file__))
    tld = os.path.realpath(mydir + os.path.sep + "".."")

    if curtin_exe is None:
        if os.path.isfile(os.path.join(tld, ""bin"", ""curtin"")):
            curtin_exe = os.path.join(tld, ""bin"", ""curtin"")

    if (curtin_exe is None and
            (os.path.basename(sys.argv[0]).startswith(""curtin"") and
             os.path.isfile(sys.argv[0]))):
        curtin_exe = os.path.realpath(sys.argv[0])

    if curtin_exe is None:
        found = which('curtin')
        if found:
            curtin_exe = found

    if curtin_exe is None:
        curtin_exe = _installed_file_path(_INSTALLED_MAIN)

    # ""common"" is a file in helpers
    cfile = ""common""
    if (helpers is None and
            os.path.isfile(os.path.join(tld, ""helpers"", cfile))):
        helpers = os.path.join(tld, ""helpers"")

    if helpers is None:
        helpers = _installed_file_path(_INSTALLED_HELPERS_PATH, cfile)

    return({'curtin_exe': curtin_exe, 'lib': mydir, 'helpers': helpers})


def get_architecture(target=None):
    out, _ = subp(['dpkg', '--print-architecture'], capture=True,
                  target=target)
    return out.strip()


def has_pkg_available(pkg, target=None):
    out, _ = subp(['apt-cache', 'pkgnames'], capture=True, target=target)
    for item in out.splitlines():
        if pkg == item.strip():
            return True
    return False


def get_installed_packages(target=None):
    (out, _) = subp(['dpkg-query', '--list'], target=target, capture=True)

    pkgs_inst = set()
    for line in out.splitlines():
        try:
            (state, pkg, other) = line.split(None, 2)
        except ValueError:
            continue
        if state.startswith(""hi"") or state.startswith(""ii""):
            pkgs_inst.add(re.sub("":.*"", """", pkg))

    return pkgs_inst


def has_pkg_installed(pkg, target=None):
    try:
        out, _ = subp(['dpkg-query', '--show', '--showformat',
                       '${db:Status-Abbrev}', pkg],
                      capture=True, target=target)
        return out.rstrip() == ""ii""
    except ProcessExecutionError:
        return False


def parse_dpkg_version(raw, name=None, semx=None):
    """"""Parse a dpkg version string into various parts and calcualate a
       numerical value of the version for use in comparing package versions

       returns a dictionary with the results
    """"""
    if semx is None:
        semx = (10000, 100, 1)

    upstream = raw.split('-')[0]
    toks = upstream.split(""."", 2)
    if len(toks) == 3:
        major, minor, micro = toks
    elif len(toks) == 2:
        major, minor, micro = (toks[0], toks[1], 0)
    elif len(toks) == 1:
        major, minor, micro = (toks[0], 0, 0)

    version = {
        'major': major,
        'minor': minor,
        'micro': micro,
        'raw': raw,
        'upstream': upstream,
    }
    if name:
        version['name'] = name

    if semx:
        try:
            version['semantic_version'] = int(
                int(major) * semx[0] + int(minor) * semx[1] +
                int(micro) * semx[2])
        except (ValueError, IndexError):
            version['semantic_version'] = None

    return version


def get_package_version(pkg, target=None, semx=None):
    """"""Use dpkg-query to extract package pkg's version string
       and parse the version string into a dictionary
    """"""
    try:
        out, _ = subp(['dpkg-query', '--show', '--showformat',
                       '${Version}', pkg], capture=True, target=target)
        raw = out.rstrip()
        return parse_dpkg_version(raw, name=pkg, semx=semx)
    except ProcessExecutionError:
        return None


def find_newer(src, files):
    mtime = os.stat(src).st_mtime
    return [f for f in files if
            os.path.exists(f) and os.stat(f).st_mtime > mtime]


def set_unexecutable(fname, strict=False):
    """"""set fname so it is not executable.

    if strict, raise an exception if the file does not exist.
    return the current mode, or None if no change is needed.
    """"""
    if not os.path.exists(fname):
        if strict:
            raise ValueError('%s: file does not exist' % fname)
        return None
    cur = stat.S_IMODE(os.lstat(fname).st_mode)
    target = cur & (~stat.S_IEXEC & ~stat.S_IXGRP & ~stat.S_IXOTH)
    if cur == target:
        return None
    os.chmod(fname, target)
    return cur


def apt_update(target=None, env=None, force=False, comment=None,
               retries=None):

    marker = ""tmp/curtin.aptupdate""
    if target is None:
        target = ""/""

    if env is None:
        env = os.environ.copy()

    if retries is None:
        # by default run apt-update up to 3 times to allow
        # for transient failures
        retries = (1, 2, 3)

    if comment is None:
        comment = ""no comment provided""

    if comment.endswith(""\n""):
        comment = comment[:-1]

    marker = target_path(target, marker)
    # if marker exists, check if there are files that would make it obsolete
    listfiles = [target_path(target, ""/etc/apt/sources.list"")]
    listfiles += glob.glob(
        target_path(target, ""etc/apt/sources.list.d/*.list""))

    if os.path.exists(marker) and not force:
        if len(find_newer(marker, listfiles)) == 0:
            return

    restore_perms = []

    abs_tmpdir = tempfile.mkdtemp(dir=target_path(target, ""/tmp""))
    try:
        abs_slist = abs_tmpdir + ""/sources.list""
        abs_slistd = abs_tmpdir + ""/sources.list.d""
        ch_tmpdir = ""/tmp/"" + os.path.basename(abs_tmpdir)
        ch_slist = ch_tmpdir + ""/sources.list""
        ch_slistd = ch_tmpdir + ""/sources.list.d""

        # this file gets executed on apt-get update sometimes. (LP: #1527710)
        motd_update = target_path(
            target, ""/usr/lib/update-notifier/update-motd-updates-available"")
        pmode = set_unexecutable(motd_update)
        if pmode is not None:
            restore_perms.append((motd_update, pmode),)

        # create tmpdir/sources.list with all lines other than deb-src
        # avoid apt complaining by using existing and empty dir for sourceparts
        os.mkdir(abs_slistd)
        with open(abs_slist, ""w"") as sfp:
            for sfile in listfiles:
                with open(sfile, ""r"") as fp:
                    contents = fp.read()
                for line in contents.splitlines():
                    line = line.lstrip()
                    if not line.startswith(""deb-src""):
                        sfp.write(line + ""\n"")

        update_cmd = [
            'apt-get', '--quiet',
            '--option=Acquire::Languages=none',
            '--option=Dir::Etc::sourcelist=%s' % ch_slist,
            '--option=Dir::Etc::sourceparts=%s' % ch_slistd,
            'update']

        # do not using 'run_apt_command' so we can use 'retries' to subp
        with ChrootableTarget(target, allow_daemons=True) as inchroot:
            inchroot.subp(update_cmd, env=env, retries=retries)
    finally:
        for fname, perms in restore_perms:
            os.chmod(fname, perms)
        if abs_tmpdir:
            shutil.rmtree(abs_tmpdir)

    with open(marker, ""w"") as fp:
        fp.write(comment + ""\n"")


def run_apt_command(mode, args=None, aptopts=None, env=None, target=None,
                    execute=True, allow_daemons=False):
    opts = ['--quiet', '--assume-yes',
            '--option=Dpkg::options::=--force-unsafe-io',
            '--option=Dpkg::Options::=--force-confold']

    if args is None:
        args = []

    if aptopts is None:
        aptopts = []

    if env is None:
        env = os.environ.copy()
        env['DEBIAN_FRONTEND'] = 'noninteractive'

    if which('eatmydata', target=target):
        emd = ['eatmydata']
    else:
        emd = []

    cmd = emd + ['apt-get'] + opts + aptopts + [mode] + args
    if not execute:
        return env, cmd

    apt_update(target, env=env, comment=' '.join(cmd))
    with ChrootableTarget(target, allow_daemons=allow_daemons) as inchroot:
        return inchroot.subp(cmd, env=env)


def system_upgrade(aptopts=None, target=None, env=None, allow_daemons=False):
    LOG.debug(""Upgrading system in %s"", target)
    for mode in ('dist-upgrade', 'autoremove'):
        ret = run_apt_command(
            mode, aptopts=aptopts, target=target,
            env=env, allow_daemons=allow_daemons)
    return ret


def install_packages(pkglist, aptopts=None, target=None, env=None,
                     allow_daemons=False):
    if isinstance(pkglist, str):
        pkglist = [pkglist]
    return run_apt_command(
        'install', args=pkglist,
        aptopts=aptopts, target=target, env=env, allow_daemons=allow_daemons)


def is_uefi_bootable():
    return os.path.exists('/sys/firmware/efi') is True


def get_efibootmgr(target):
    """"""Return mapping of EFI information.

    Calls `efibootmgr` inside the `target`.

    Example output:
        {
            'current': '0000',
            'timeout': '1 seconds',
            'order': ['0000', '0001'],
            'entries': {
                '0000': {
                    'name': 'ubuntu',
                    'path': (
                        'HD(1,GPT,0,0x8,0x1)/File(\\EFI\\ubuntu\\shimx64.efi)'),
                },
                '0001': {
                    'name': 'UEFI:Network Device',
                    'path': 'BBS(131,,0x0)',
                }
            }
        }
    """"""
    efikey_to_dict_key = {
        'BootCurrent': 'current',
        'Timeout': 'timeout',
        'BootOrder': 'order',
    }
    with ChrootableTarget(target) as in_chroot:
        stdout, _ = in_chroot.subp(['efibootmgr', '-v'], capture=True)
        output = {}
        for line in stdout.splitlines():
            split = line.split(':')
            if len(split) == 2:
                key = split[0].strip()
                output_key = efikey_to_dict_key.get(key, None)
                if output_key:
                    output[output_key] = split[1].strip()
                    if output_key == 'order':
                        output[output_key] = output[output_key].split(',')
        output['entries'] = {
            entry: {
                'name': name.strip(),
                'path': path.strip(),
            }
            for entry, name, path in re.findall(
                r""^Boot(?P<entry>[0-9a-fA-F]{4})\*?\s(?P<name>.+)\t""
                r""(?P<path>.*)$"",
                stdout, re.MULTILINE)
        }
        return output


def run_hook_if_exists(target, hook):
    """"""
    Look for ""hook"" in ""target"" and run it
    """"""
    target_hook = target_path(target, '/curtin/' + hook)
    if os.path.isfile(target_hook):
        LOG.debug(""running %s"" % target_hook)
        subp([target_hook])
        return True
    return False


def sanitize_source(source):
    """"""
    Check the install source for type information
    If no type information is present or it is an invalid
    type, we default to the standard tgz format
    """"""
    if type(source) is dict:
        # already sanitized?
        return source
    supported = ['tgz', 'dd-tgz', 'dd-tbz', 'dd-txz', 'dd-tar', 'dd-bz2',
                 'dd-gz', 'dd-xz', 'dd-raw']
    deftype = 'tgz'
    for i in supported:
        prefix = i + "":""
        if source.startswith(prefix):
            return {'type': i, 'uri': source[len(prefix):]}

    LOG.debug(""unknown type for url '%s', assuming type '%s'"", source, deftype)
    # default to tgz for unknown types
    return {'type': deftype, 'uri': source}


def get_dd_images(sources):
    """"""
    return all disk images in sources list
    """"""
    src = []
    if type(sources) is not dict:
        return src
    for i in sources:
        if type(sources[i]) is not dict:
            continue
        if sources[i]['type'].startswith('dd-'):
            src.append(sources[i])
    return src


def get_meminfo(meminfo=""/proc/meminfo"", raw=False):
    mpliers = {'kB': 2**10, 'mB': 2 ** 20, 'B': 1, 'gB': 2 ** 30}
    kmap = {'MemTotal:': 'total', 'MemFree:': 'free',
            'MemAvailable:': 'available'}
    ret = {}
    with open(meminfo, ""r"") as fp:
        for line in fp:
            try:
                key, value, unit = line.split()
            except ValueError:
                key, value = line.split()
                unit = 'B'
            if raw:
                ret[key] = int(value) * mpliers[unit]
            elif key in kmap:
                ret[kmap[key]] = int(value) * mpliers[unit]

    return ret


def get_fs_use_info(path):
    # return some filesystem usage info as tuple of (size_in_bytes, free_bytes)
    statvfs = os.statvfs(path)
    return (statvfs.f_frsize * statvfs.f_blocks,
            statvfs.f_frsize * statvfs.f_bfree)


def human2bytes(size):
    # convert human 'size' to integer
    size_in = size

    if isinstance(size, int):
        return size
    elif isinstance(size, float):
        if int(size) != size:
            raise ValueError(""'%s': resulted in non-integer (%s)"" %
                             (size_in, int(size)))
        return size
    elif not isinstance(size, str):
        raise TypeError(""cannot convert type %s ('%s')."" % (type(size), size))

    if size.endswith(""B""):
        size = size[:-1]

    mpliers = {'B': 1, 'K': 2 ** 10, 'M': 2 ** 20, 'G': 2 ** 30, 'T': 2 ** 40}

    num = size
    mplier = 'B'
    for m in mpliers:
        if size.endswith(m):
            mplier = m
            num = size[0:-len(m)]

    try:
        num = float(num)
    except ValueError:
        raise ValueError(""'%s' is not valid input."" % size_in)

    if num < 0:
        raise ValueError(""'%s': cannot be negative"" % size_in)

    val = num * mpliers[mplier]
    if int(val) != val:
        raise ValueError(""'%s': resulted in non-integer (%s)"" % (size_in, val))

    return val


def bytes2human(size):
    """"""convert size in bytes to human readable""""""
    if not isinstance(size, numeric_types):
        raise ValueError('size must be a numeric value, not %s', type(size))
    isize = int(size)
    if isize != size:
        raise ValueError('size ""%s"" is not a whole number.' % size)
    if isize < 0:
        raise ValueError('size ""%d"" < 0.' % isize)
    mpliers = {'B': 1, 'K': 2 ** 10, 'M': 2 ** 20, 'G': 2 ** 30, 'T': 2 ** 40}
    unit_order = sorted(mpliers, key=lambda x: -1 * mpliers[x])
    unit = next((u for u in unit_order if (isize / mpliers[u]) >= 1), 'B')
    return str(int(isize / mpliers[unit])) + unit


def import_module(import_str):
    """"""Import a module.""""""
    __import__(import_str)
    return sys.modules[import_str]


def try_import_module(import_str, default=None):
    """"""Try to import a module.""""""
    try:
        return import_module(import_str)
    except ImportError:
        return default


def is_file_not_found_exc(exc):
    return (isinstance(exc, (IOError, OSError)) and
            hasattr(exc, 'errno') and
            exc.errno in (errno.ENOENT, errno.EIO, errno.ENXIO))


def _lsb_release(target=None):
    fmap = {'Codename': 'codename', 'Description': 'description',
            'Distributor ID': 'id', 'Release': 'release'}

    data = {}
    try:
        out, _ = subp(['lsb_release', '--all'], capture=True, target=target)
        for line in out.splitlines():
            fname, _, val = line.partition("":"")
            if fname in fmap:
                data[fmap[fname]] = val.strip()
        missing = [k for k in fmap.values() if k not in data]
        if len(missing):
            LOG.warn(""Missing fields in lsb_release --all output: %s"",
                     ','.join(missing))

    except ProcessExecutionError as err:
        LOG.warn(""Unable to get lsb_release --all: %s"", err)
        data = {v: ""UNAVAILABLE"" for v in fmap.values()}

    return data


def lsb_release(target=None):
    if target_path(target) != ""/"":
        # do not use or update cache if target is provided
        return _lsb_release(target)

    global _LSB_RELEASE
    if not _LSB_RELEASE:
        data = _lsb_release()
        _LSB_RELEASE.update(data)
    return _LSB_RELEASE


class MergedCmdAppend(argparse.Action):
    """"""This appends to a list in order of appearence both the option string
       and the value""""""
    def __call__(self, parser, namespace, values, option_string=None):
        if getattr(namespace, self.dest, None) is None:
            setattr(namespace, self.dest, [])
        getattr(namespace, self.dest).append((option_string, values,))


def json_dumps(data):
    return json.dumps(data, indent=1, sort_keys=True, separators=(',', ': '))


def get_platform_arch():
    platform2arch = {
        'i586': 'i386',
        'i686': 'i386',
        'x86_64': 'amd64',
        'ppc64le': 'ppc64el',
        'aarch64': 'arm64',
    }
    return platform2arch.get(platform.machine(), platform.machine())


def basic_template_render(content, params):
    """"""This does simple replacement of bash variable like templates.

    It identifies patterns like ${a} or $a and can also identify patterns like
    ${a.b} or $a.b which will look for a key 'b' in the dictionary rooted
    by key 'a'.
    """"""

    def replacer(match):
        """""" replacer
            replacer used in regex match to replace content
        """"""
        # Only 1 of the 2 groups will actually have a valid entry.
        name = match.group(1)
        if name is None:
            name = match.group(2)
        if name is None:
            raise RuntimeError(""Match encountered but no valid group present"")
        path = collections.deque(name.split("".""))
        selected_params = params
        while len(path) > 1:
            key = path.popleft()
            if not isinstance(selected_params, dict):
                raise TypeError(""Can not traverse into""
                                "" non-dictionary '%s' of type %s while""
                                "" looking for subkey '%s'""
                                % (selected_params,
                                   selected_params.__class__.__name__,
                                   key))
            selected_params = selected_params[key]
        key = path.popleft()
        if not isinstance(selected_params, dict):
            raise TypeError(""Can not extract key '%s' from non-dictionary""
                            "" '%s' of type %s""
                            % (key, selected_params,
                               selected_params.__class__.__name__))
        return str(selected_params[key])

    return BASIC_MATCHER.sub(replacer, content)


def render_string(content, params):
    """""" render_string
        render a string following replacement rules as defined in
        basic_template_render returning the string
    """"""
    if not params:
        params = {}
    return basic_template_render(content, params)


def is_resolvable(name):
    """"""determine if a url is resolvable, return a boolean
    This also attempts to be resilent against dns redirection.

    Note, that normal nsswitch resolution is used here.  So in order
    to avoid any utilization of 'search' entries in /etc/resolv.conf
    we have to append '.'.

    The top level 'invalid' domain is invalid per RFC.  And example.com
    should also not exist.  The random entry will be resolved inside
    the search list.
    """"""
    global _DNS_REDIRECT_IP
    if _DNS_REDIRECT_IP is None:
        badips = set()
        badnames = (""does-not-exist.example.com."", ""example.invalid."")
        badresults = {}
        for iname in badnames:
            try:
                result = socket.getaddrinfo(iname, None, 0, 0,
                                            socket.SOCK_STREAM,
                                            socket.AI_CANONNAME)
                badresults[iname] = []
                for (_, _, _, cname, sockaddr) in result:
                    badresults[iname].append(""%s: %s"" % (cname, sockaddr[0]))
                    badips.add(sockaddr[0])
            except (socket.gaierror, socket.error):
                pass
        _DNS_REDIRECT_IP = badips
        if badresults:
            LOG.debug(""detected dns redirection: %s"", badresults)

    try:
        result = socket.getaddrinfo(name, None)
        # check first result's sockaddr field
        addr = result[0][4][0]
        if addr in _DNS_REDIRECT_IP:
            LOG.debug(""dns %s in _DNS_REDIRECT_IP"", name)
            return False
        LOG.debug(""dns %s resolved to '%s'"", name, result)
        return True
    except (socket.gaierror, socket.error):
        LOG.debug(""dns %s failed to resolve"", name)
        return False


def is_valid_ipv6_address(addr):
    try:
        socket.inet_pton(socket.AF_INET6, addr)
    except socket.error:
        return False
    return True


def is_resolvable_url(url):
    """"""determine if this url is resolvable (existing or ip).""""""
    return is_resolvable(urlparse(url).hostname)


def target_path(target, path=None):
    # return 'path' inside target, accepting target as None
    if target in (None, """"):
        target = ""/""
    elif not isinstance(target, string_types):
        raise ValueError(""Unexpected input for target: %s"" % target)
    else:
        target = os.path.abspath(target)
        # abspath(""//"") returns ""//"" specifically for 2 slashes.
        if target.startswith(""//""):
            target = target[1:]

    if not path:
        return target

    if not isinstance(path, string_types):
        raise ValueError(""Unexpected input for path: %s"" % path)

    # os.path.join(""/etc"", ""/foo"") returns ""/foo"". Chomp all leading /.
    while len(path) and path[0] == ""/"":
        path = path[1:]

    return os.path.join(target, path)


class RunInChroot(ChrootableTarget):
    """"""Backwards compatibility for RunInChroot (LP: #1617375).
    It needs to work like:
        with RunInChroot(""/target"") as in_chroot:
            in_chroot([""your"", ""chrooted"", ""command""])""""""
    __call__ = ChrootableTarget.subp


def shlex_split(str_in):
    # shlex.split takes a string
    # but in python2 if input here is a unicode, encode it to a string.
    # http://stackoverflow.com/questions/2365411/
    #     python-convert-unicode-to-ascii-without-errors
    if sys.version_info.major == 2:
        try:
            if isinstance(str_in, unicode):
                str_in = str_in.encode('utf-8')
        except NameError:
            pass

        return shlex.split(str_in)
    else:
        return shlex.split(str_in)


def load_shell_content(content, add_empty=False, empty_val=None):
    """"""Given shell like syntax (key=value\nkey2=value2\n) in content
       return the data in dictionary form.  If 'add_empty' is True
       then add entries in to the returned dictionary for 'VAR='
       variables.  Set their value to empty_val.""""""

    data = {}
    for line in shlex_split(content):
        key, value = line.split(""="", 1)
        if not value:
            value = empty_val
        if add_empty or value:
            data[key] = value

    return data


def uses_systemd():
    """""" Check if current enviroment uses systemd by testing if
        /run/systemd/system is a directory; only present if
        systemd is available on running system.
    """"""

    global _USES_SYSTEMD
    if _USES_SYSTEMD is None:
        _USES_SYSTEMD = os.path.isdir('/run/systemd/system')

    return _USES_SYSTEMD

# vi: ts=4 expandtab syntax=python
/n/n/ntests/unittests/test_block.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import functools
import os
import mock
import sys

from collections import OrderedDict

from .helpers import CiTestCase, simple_mocked_open
from curtin import util
from curtin import block


class TestBlock(CiTestCase):

    @mock.patch(""curtin.block.util"")
    def test_get_volume_uuid(self, mock_util):
        path = ""/dev/sda1""
        expected_call = [""blkid"", ""-o"", ""export"", path]
        mock_util.subp.return_value = (""""""
            UUID=182e8e23-5322-46c9-a1b8-cf2c6a88f9f7
            """""", """")

        uuid = block.get_volume_uuid(path)

        mock_util.subp.assert_called_with(expected_call, capture=True)
        self.assertEqual(uuid, ""182e8e23-5322-46c9-a1b8-cf2c6a88f9f7"")

    @mock.patch(""curtin.block.get_proc_mounts"")
    @mock.patch(""curtin.block._lsblock"")
    def test_get_mountpoints(self, mock_lsblk, mock_proc_mounts):
        mock_lsblk.return_value = {""sda1"": {""MOUNTPOINT"": None},
                                   ""sda2"": {""MOUNTPOINT"": """"},
                                   ""sda3"": {""MOUNTPOINT"": ""/mnt""}}
        mock_proc_mounts.return_value = [
            ('sysfs', '/sys', 'sysfs', 'sysfs_opts', '0', '0'),
        ]

        mountpoints = block.get_mountpoints()

        self.assertTrue(mock_lsblk.called)
        self.assertEqual(sorted(mountpoints),
                         sorted([""/mnt"", ""/sys""]))

    @mock.patch('curtin.block._lsblock')
    def test_get_blockdev_sector_size(self, mock_lsblk):
        mock_lsblk.return_value = {
            'sda':  {'LOG-SEC': '512', 'PHY-SEC': '4096',
                     'device_path': '/dev/sda'},
            'sda1': {'LOG-SEC': '512', 'PHY-SEC': '4096',
                     'device_path': '/dev/sda1'},
            'dm-0': {'LOG-SEC': '512', 'PHY-SEC': '512',
                     'device_path': '/dev/dm-0'},
        }
        for (devpath, expected) in [('/dev/sda', (512, 4096)),
                                    ('/dev/sda1', (512, 4096)),
                                    ('/dev/dm-0', (512, 512))]:
            res = block.get_blockdev_sector_size(devpath)
            mock_lsblk.assert_called_with([devpath])
            self.assertEqual(res, expected)

        # test that fallback works and gives right return
        mock_lsblk.return_value = OrderedDict()
        mock_lsblk.return_value.update({
            'vda': {'LOG-SEC': '4096', 'PHY-SEC': '4096',
                    'device_path': '/dev/vda'},
        })
        mock_lsblk.return_value.update({
            'vda1': {'LOG-SEC': '512', 'PHY-SEC': '512',
                     'device_path': '/dev/vda1'},
        })
        res = block.get_blockdev_sector_size('/dev/vda2')
        self.assertEqual(res, (4096, 4096))

    @mock.patch(""curtin.block.os.path.realpath"")
    @mock.patch(""curtin.block.os.path.exists"")
    @mock.patch(""curtin.block.os.listdir"")
    def test_lookup_disk(self, mock_os_listdir, mock_os_path_exists,
                         mock_os_path_realpath):
        serial = ""SERIAL123""
        mock_os_listdir.return_value = [""sda_%s-part1"" % serial,
                                        ""sda_%s"" % serial, ""other""]
        mock_os_path_exists.return_value = True
        mock_os_path_realpath.return_value = ""/dev/sda""

        path = block.lookup_disk(serial)

        mock_os_listdir.assert_called_with(""/dev/disk/by-id/"")
        mock_os_path_realpath.assert_called_with(""/dev/disk/by-id/sda_%s"" %
                                                 serial)
        self.assertTrue(mock_os_path_exists.called)
        self.assertEqual(path, ""/dev/sda"")

        with self.assertRaises(ValueError):
            mock_os_path_exists.return_value = False
            block.lookup_disk(serial)

        with self.assertRaises(ValueError):
            mock_os_path_exists.return_value = True
            mock_os_listdir.return_value = [""other""]
            block.lookup_disk(serial)

    @mock.patch(""curtin.block.get_dev_disk_byid"")
    def test_disk_to_byid_path(self, mock_byid):
        """""" disk_to_byid path returns a /dev/disk/by-id path """"""
        mapping = {
            '/dev/sda': '/dev/disk/by-id/scsi-abcdef',
        }
        mock_byid.return_value = mapping

        byid_path = block.disk_to_byid_path('/dev/sda')
        self.assertEqual(mapping['/dev/sda'], byid_path)

    @mock.patch(""curtin.block.get_dev_disk_byid"")
    def test_disk_to_byid_path_notfound(self, mock_byid):
        """""" disk_to_byid path returns None for not found devices """"""
        mapping = {
            '/dev/sda': '/dev/disk/by-id/scsi-abcdef',
        }
        mock_byid.return_value = mapping

        byid_path = block.disk_to_byid_path('/dev/sdb')
        self.assertEqual(mapping.get('/dev/sdb'), byid_path)


class TestSysBlockPath(CiTestCase):
    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_existing_valid_devname(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = True
        m_get_blk.return_value = ('foodevice', None)
        self.assertEqual('/sys/class/block/foodevice',
                         block.sys_block_path(""foodevice""))

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_existing_devpath_allowed(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = True
        m_get_blk.return_value = ('foodev', None)
        self.assertEqual('/sys/class/block/foodev',
                         block.sys_block_path(""/dev/foodev""))

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_add_works(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = True
        m_get_blk.return_value = ('foodev', None)
        self.assertEqual('/sys/class/block/foodev/md/b',
                         block.sys_block_path(""/dev/foodev"", ""md/b""))

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_add_works_leading_slash(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = True
        m_get_blk.return_value = ('foodev', None)
        self.assertEqual('/sys/class/block/foodev/md/b',
                         block.sys_block_path(""/dev/foodev"", ""/md/b""))

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_invalid_devname_raises(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = False
        with self.assertRaises(ValueError):
            block.sys_block_path(""foodevice"")

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    def test_invalid_with_add(self, m_get_blk):
        # test the device exists, but 'add' does not
        # path_exists returns true unless 'md/device' is in it
        #  so /sys/class/foodev/ exists, but not /sys/class/foodev/md/device
        add = ""md/device""

        def path_exists(path):
            return add not in path

        m_get_blk.return_value = (""foodev"", None)
        with mock.patch('os.path.exists', side_effect=path_exists):
            self.assertRaises(OSError, block.sys_block_path, ""foodev"", add)

    @mock.patch(""curtin.block.get_blockdev_for_partition"")
    @mock.patch(""os.path.exists"")
    def test_not_strict_does_not_care(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = False
        m_get_blk.return_value = ('foodev', None)
        self.assertEqual('/sys/class/block/foodev/md/b',
                         block.sys_block_path(""foodev"", ""/md/b"", strict=False))

    @mock.patch('curtin.block.get_blockdev_for_partition')
    @mock.patch('os.path.exists')
    def test_cciss_sysfs_path(self, m_os_path_exists, m_get_blk):
        m_os_path_exists.return_value = True
        m_get_blk.return_value = ('cciss!c0d0', None)
        self.assertEqual('/sys/class/block/cciss!c0d0',
                         block.sys_block_path('/dev/cciss/c0d0'))
        m_get_blk.return_value = ('cciss!c0d0', 1)
        self.assertEqual('/sys/class/block/cciss!c0d0/cciss!c0d0p1',
                         block.sys_block_path('/dev/cciss/c0d0p1'))


class TestWipeFile(CiTestCase):
    def __init__(self, *args, **kwargs):
        super(TestWipeFile, self).__init__(*args, **kwargs)

    def test_non_exist_raises_file_not_found(self):
        try:
            p = self.tmp_path(""enofile"")
            block.wipe_file(p)
            raise Exception(""%s did not raise exception"" % p)
        except Exception as e:
            if not util.is_file_not_found_exc(e):
                raise Exception(""exc was not file_not_found: %s"" % e)

    def test_non_exist_dir_raises_file_not_found(self):
        try:
            p = self.tmp_path(os.path.sep.join([""enodir"", ""file""]))
            block.wipe_file(p)
            raise Exception(""%s did not raise exception"" % p)
        except Exception as e:
            if not util.is_file_not_found_exc(e):
                raise Exception(""exc was not file_not_found: %s"" % e)

    def test_default_is_zero(self):
        flen = 1024
        myfile = self.tmp_path(""def_zero"")
        util.write_file(myfile, flen * b'\1', omode=""wb"")
        block.wipe_file(myfile)
        found = util.load_file(myfile, decode=False)
        self.assertEqual(found, flen * b'\0')

    def test_reader_used(self):
        flen = 17

        def reader(size):
            return size * b'\1'

        myfile = self.tmp_path(""reader_used"")
        # populate with nulls
        util.write_file(myfile, flen * b'\0', omode=""wb"")
        block.wipe_file(myfile, reader=reader, buflen=flen)
        found = util.load_file(myfile, decode=False)
        self.assertEqual(found, flen * b'\1')

    def test_reader_twice(self):
        flen = 37
        data = {'x': 20 * b'a' + 20 * b'b'}
        expected = data['x'][0:flen]

        def reader(size):
            buf = data['x'][0:size]
            data['x'] = data['x'][size:]
            return buf

        myfile = self.tmp_path(""reader_twice"")
        util.write_file(myfile, flen * b'\xff', omode=""wb"")
        block.wipe_file(myfile, reader=reader, buflen=20)
        found = util.load_file(myfile, decode=False)
        self.assertEqual(found, expected)

    def test_reader_fhandle(self):
        srcfile = self.tmp_path(""fhandle_src"")
        trgfile = self.tmp_path(""fhandle_trg"")
        data = '\n'.join([""this is source file."" for f in range(0, 10)] + [])
        util.write_file(srcfile, data)
        util.write_file(trgfile, 'a' * len(data))
        with open(srcfile, ""rb"") as fp:
            block.wipe_file(trgfile, reader=fp.read)
        found = util.load_file(trgfile)
        self.assertEqual(data, found)

    def test_exclusive_open_raise_missing(self):
        myfile = self.tmp_path(""no-such-file"")

        with self.assertRaises(ValueError):
            with block.exclusive_open(myfile) as fp:
                fp.close()

    @mock.patch('os.close')
    @mock.patch('os.fdopen')
    @mock.patch('os.open')
    def test_exclusive_open(self, mock_os_open, mock_os_fdopen, mock_os_close):
        flen = 1024
        myfile = self.tmp_path(""my_exclusive_file"")
        util.write_file(myfile, flen * b'\1', omode=""wb"")
        mock_fd = 3
        mock_os_open.return_value = mock_fd

        with block.exclusive_open(myfile) as fp:
            fp.close()

        mock_os_open.assert_called_with(myfile, os.O_RDWR | os.O_EXCL)
        mock_os_fdopen.assert_called_with(mock_fd, 'rb+')
        self.assertEqual([], mock_os_close.call_args_list)

    @mock.patch('curtin.util.fuser_mount')
    @mock.patch('os.close')
    @mock.patch('curtin.util.list_device_mounts')
    @mock.patch('curtin.block.get_holders')
    @mock.patch('os.open')
    def test_exclusive_open_non_exclusive_exception(self, mock_os_open,
                                                    mock_holders,
                                                    mock_list_mounts,
                                                    mock_os_close,
                                                    mock_util_fuser):
        flen = 1024
        myfile = self.tmp_path(""my_exclusive_file"")
        util.write_file(myfile, flen * b'\1', omode=""wb"")
        mock_os_open.side_effect = OSError(""NO_O_EXCL"")
        mock_holders.return_value = ['md1']
        mock_list_mounts.return_value = []
        mock_util_fuser.return_value = {}

        with self.assertRaises(OSError):
            with block.exclusive_open(myfile) as fp:
                fp.close()

        mock_os_open.assert_called_with(myfile, os.O_RDWR | os.O_EXCL)
        mock_holders.assert_called_with(myfile)
        mock_list_mounts.assert_called_with(myfile)
        self.assertEqual([], mock_os_close.call_args_list)

    @mock.patch('os.close')
    @mock.patch('os.fdopen')
    @mock.patch('os.open')
    def test_exclusive_open_fdopen_failure(self, mock_os_open,
                                           mock_os_fdopen, mock_os_close):
        flen = 1024
        myfile = self.tmp_path(""my_exclusive_file"")
        util.write_file(myfile, flen * b'\1', omode=""wb"")
        mock_fd = 3
        mock_os_open.return_value = mock_fd
        mock_os_fdopen.side_effect = OSError(""EBADF"")

        with self.assertRaises(OSError):
            with block.exclusive_open(myfile) as fp:
                fp.close()

        mock_os_open.assert_called_with(myfile, os.O_RDWR | os.O_EXCL)
        mock_os_fdopen.assert_called_with(mock_fd, 'rb+')
        if sys.version_info.major == 2:
            mock_os_close.assert_called_with(mock_fd)
        else:
            self.assertEqual([], mock_os_close.call_args_list)


class TestWipeVolume(CiTestCase):
    dev = '/dev/null'

    @mock.patch('curtin.block.lvm')
    @mock.patch('curtin.block.util')
    def test_wipe_pvremove(self, mock_util, mock_lvm):
        block.wipe_volume(self.dev, mode='pvremove')
        mock_util.subp.assert_called_with(
            ['pvremove', '--force', '--force', '--yes', self.dev], rcs=[0, 5],
            capture=True)
        self.assertTrue(mock_lvm.lvm_scan.called)

    @mock.patch('curtin.block.quick_zero')
    def test_wipe_superblock(self, mock_quick_zero):
        block.wipe_volume(self.dev, mode='superblock')
        mock_quick_zero.assert_called_with(self.dev, partitions=False)
        block.wipe_volume(self.dev, mode='superblock-recursive')
        mock_quick_zero.assert_called_with(self.dev, partitions=True)

    @mock.patch('curtin.block.wipe_file')
    def test_wipe_zero(self, mock_wipe_file):
        with simple_mocked_open():
            block.wipe_volume(self.dev, mode='zero')
            mock_wipe_file.assert_called_with(self.dev)

    @mock.patch('curtin.block.wipe_file')
    def test_wipe_random(self, mock_wipe_file):
        with simple_mocked_open() as mock_open:
            block.wipe_volume(self.dev, mode='random')
            mock_open.assert_called_with('/dev/urandom', 'rb')
            mock_wipe_file.assert_called_with(
                self.dev, reader=mock_open.return_value.__enter__().read)

    def test_bad_input(self):
        with self.assertRaises(ValueError):
            block.wipe_volume(self.dev, mode='invalidmode')


class TestBlockKnames(CiTestCase):
    """"""Tests for some of the kname functions in block""""""
    def test_determine_partition_kname(self):
        part_knames = [(('sda', 1), 'sda1'),
                       (('vda', 1), 'vda1'),
                       (('nvme0n1', 1), 'nvme0n1p1'),
                       (('mmcblk0', 1), 'mmcblk0p1'),
                       (('cciss!c0d0', 1), 'cciss!c0d0p1'),
                       (('dm-0', 1), 'dm-0p1'),
                       (('md0', 1), 'md0p1'),
                       (('mpath1', 2), 'mpath1p2')]
        for ((disk_kname, part_number), part_kname) in part_knames:
            self.assertEqual(block.partition_kname(disk_kname, part_number),
                             part_kname)

    @mock.patch('curtin.block.os.path.realpath')
    def test_path_to_kname(self, mock_os_realpath):
        mock_os_realpath.side_effect = lambda x: os.path.normpath(x)
        path_knames = [('/dev/sda', 'sda'),
                       ('/dev/sda1', 'sda1'),
                       ('/dev////dm-0/', 'dm-0'),
                       ('/dev/md0p1', 'md0p1'),
                       ('vdb', 'vdb'),
                       ('/dev/mmcblk0p1', 'mmcblk0p1'),
                       ('/dev/nvme0n0p1', 'nvme0n0p1'),
                       ('/sys/block/vdb', 'vdb'),
                       ('/sys/block/vdb/vdb2/', 'vdb2'),
                       ('/dev/cciss/c0d0', 'cciss!c0d0'),
                       ('/dev/cciss/c0d0p1/', 'cciss!c0d0p1'),
                       ('/sys/class/block/cciss!c0d0p1', 'cciss!c0d0p1'),
                       ('nvme0n1p4', 'nvme0n1p4')]
        for (path, expected_kname) in path_knames:
            self.assertEqual(block.path_to_kname(path), expected_kname)
            if os.path.sep in path:
                mock_os_realpath.assert_called_with(path)

    @mock.patch('curtin.block.os.path.exists')
    @mock.patch('curtin.block.os.path.realpath')
    @mock.patch('curtin.block.is_valid_device')
    def test_kname_to_path(self, mock_is_valid_device, mock_os_realpath,
                           mock_exists):
        kname_paths = [('sda', '/dev/sda'),
                       ('sda1', '/dev/sda1'),
                       ('/dev/sda', '/dev/sda'),
                       ('cciss!c0d0p1', '/dev/cciss/c0d0p1'),
                       ('/dev/cciss/c0d0', '/dev/cciss/c0d0'),
                       ('mmcblk0p1', '/dev/mmcblk0p1')]

        mock_exists.return_value = True
        mock_os_realpath.side_effect = lambda x: x.replace('!', '/')
        # first call to is_valid_device needs to return false for nonpaths
        mock_is_valid_device.side_effect = lambda x: x.startswith('/dev')
        for (kname, expected_path) in kname_paths:
            self.assertEqual(block.kname_to_path(kname), expected_path)
            mock_is_valid_device.assert_called_with(expected_path)

        # test failure
        mock_is_valid_device.return_value = False
        mock_is_valid_device.side_effect = None
        for (kname, expected_path) in kname_paths:
            with self.assertRaises(OSError):
                block.kname_to_path(kname)


class TestPartTableSignature(CiTestCase):
    blockdev = '/dev/null'
    dos_content = b'\x00' * 0x1fe + b'\x55\xAA' + b'\x00' * 0xf00
    gpt_content = b'\x00' * 0x200 + b'EFI PART' + b'\x00' * (0x200 - 8)
    gpt_content_4k = b'\x00' * 0x800 + b'EFI PART' + b'\x00' * (0x800 - 8)
    null_content = b'\x00' * 0xf00

    def _test_util_load_file(self, content, device, read_len, offset, decode):
        return (bytes if not decode else str)(content[offset:offset+read_len])

    @mock.patch('curtin.block.check_dos_signature')
    @mock.patch('curtin.block.check_efi_signature')
    def test_gpt_part_table_type(self, mock_check_efi, mock_check_dos):
        """"""test block.get_part_table_type logic""""""
        for (has_dos, has_efi, expected) in [(True, True, 'gpt'),
                                             (True, False, 'dos'),
                                             (False, False, None)]:
            mock_check_dos.return_value = has_dos
            mock_check_efi.return_value = has_efi
            self.assertEqual(
                block.get_part_table_type(self.blockdev), expected)

    @mock.patch('curtin.block.is_block_device')
    @mock.patch('curtin.block.util')
    def test_check_dos_signature(self, mock_util, mock_is_block_device):
        """"""test block.check_dos_signature""""""
        for (is_block, f_size, contents, expected) in [
                (True, 0x200, self.dos_content, True),
                (False, 0x200, self.dos_content, False),
                (True, 0, self.dos_content, False),
                (True, 0x400, self.dos_content, True),
                (True, 0x200, self.null_content, False)]:
            mock_util.load_file.side_effect = (
                functools.partial(self._test_util_load_file, contents))
            mock_util.file_size.return_value = f_size
            mock_is_block_device.return_value = is_block
            (self.assertTrue if expected else self.assertFalse)(
                block.check_dos_signature(self.blockdev))

    @mock.patch('curtin.block.is_block_device')
    @mock.patch('curtin.block.get_blockdev_sector_size')
    @mock.patch('curtin.block.util')
    def test_check_efi_signature(self, mock_util, mock_get_sector_size,
                                 mock_is_block_device):
        """"""test block.check_efi_signature""""""
        for (sector_size, gpt_dat) in zip(
                (0x200, 0x800), (self.gpt_content, self.gpt_content_4k)):
            mock_get_sector_size.return_value = (sector_size, sector_size)
            for (is_block, f_size, contents, expected) in [
                    (True, 2 * sector_size, gpt_dat, True),
                    (True, 1 * sector_size, gpt_dat, False),
                    (False, 2 * sector_size, gpt_dat, False),
                    (True, 0, gpt_dat, False),
                    (True, 2 * sector_size, self.dos_content, False),
                    (True, 2 * sector_size, self.null_content, False)]:
                mock_util.load_file.side_effect = (
                    functools.partial(self._test_util_load_file, contents))
                mock_util.file_size.return_value = f_size
                mock_is_block_device.return_value = is_block
                (self.assertTrue if expected else self.assertFalse)(
                    block.check_efi_signature(self.blockdev))


class TestNonAscii(CiTestCase):
    @mock.patch('curtin.block.util.subp')
    def test_lsblk(self, mock_subp):
        # lsblk can write non-ascii data, causing shlex to blow up
        out = (b'ALIGNMENT=""0"" DISC-ALN=""0"" DISC-GRAN=""512"" '
               b'DISC-MAX=""2147450880"" DISC-ZERO=""0"" FSTYPE="""" '
               b'GROUP=""root"" KNAME=""sda"" LABEL="""" LOG-SEC=""512"" '
               b'MAJ:MIN=""8:0"" MIN-IO=""512"" MODE=""\xc3\xb8---------"" '
               b'MODEL=""Samsung SSD 850 "" MOUNTPOINT="""" NAME=""sda"" '
               b'OPT-IO=""0"" OWNER=""root"" PHY-SEC=""512"" RM=""0"" RO=""0"" '
               b'ROTA=""0"" RQ-SIZE=""128"" SIZE=""500107862016"" '
               b'STATE=""running"" TYPE=""disk"" UUID=""""').decode('utf-8')
        err = b''.decode()
        mock_subp.return_value = (out, err)
        out = block._lsblock()

    @mock.patch('curtin.block.util.subp')
    def test_blkid(self, mock_subp):
        # we use shlex on blkid, so cover that it might output non-ascii
        out = (b'/dev/sda2: UUID=""19ac97d5-6973-4193-9a09-2e6bbfa38262"" '
               b'LABEL=""\xc3\xb8foo"" TYPE=""ext4""').decode('utf-8')
        err = b''.decode()
        mock_subp.return_value = (out, err)
        block.blkid()


class TestSlaveKnames(CiTestCase):

    def setUp(self):
        super(TestSlaveKnames, self).setUp()
        self.add_patch('curtin.block.get_blockdev_for_partition',
                       'm_blockdev_for_partition')
        self.add_patch('curtin.block.os.path.exists',
                       'm_os_path_exists')
        # trusty-p3 does not like autospec=True for os.listdir
        self.add_patch('curtin.block.os.listdir',
                       'm_os_listdir', autospec=False)

    def _prepare_mocks(self, device, cfg):
        """"""
        Construct the correct sequence of mocks
        give a mapping of device and slaves

        cfg = {
            'wark': ['foo', 'bar'],
            'foo': [],
            'bar': [],
        }
        device = 'wark', slaves = ['foo, 'bar']

        cfg = {
            'wark': ['foo', 'bar'],
            'foo': ['zip'],
            'bar': [],
            'zip': []
        }
        device = 'wark', slaves = ['zip', 'bar']
        """"""
        # kname side-effect mapping
        parts = [(k, None) for k in cfg.keys()]
        self.m_blockdev_for_partition.side_effect = iter(parts)

        # construct side effects to os.path.exists
        # and os.listdir based on mapping.
        dirs = []
        exists = []
        for (dev, slvs) in cfg.items():
            # sys_block_dev checks if dev exists
            exists.append(True)
            if slvs:
                # os.path.exists on slaves dir
                exists.append(True)
                # result of os.listdir
                dirs.append(slvs)
            else:
                # os.path.exists on slaves dir
                exists.append(False)

        self.m_os_path_exists.side_effect = iter(exists)
        self.m_os_listdir.side_effect = iter(dirs)

    def test_get_device_slave_knames(self):
        #
        # /sys/class/block/wark/slaves/foo -> ../../foo
        # /sys/class/block/foo #
        # should return 'bar'
        cfg = OrderedDict([
            ('wark', ['foo']),
            ('foo', []),
        ])
        device = ""/dev/wark""
        slaves = [""foo""]
        self._prepare_mocks(device, cfg)
        knames = block.get_device_slave_knames(device)
        self.assertEqual(slaves, knames)

    def test_get_device_slave_knames_stacked(self):
        #
        # /sys/class/block/wark/slaves/foo -> ../../foo
        # /sys/class/block/wark/slaves/bar -> ../../bar
        # /sys/class/block/foo
        # /sys/class/block/bar
        #
        # should return ['foo', 'bar']
        cfg = OrderedDict([
            ('wark', ['foo', 'bar']),
            ('foo', []),
            ('bar', []),
        ])
        device = 'wark'
        slaves = ['foo', 'bar']
        self._prepare_mocks(device, cfg)
        knames = block.get_device_slave_knames(device)
        self.assertEqual(slaves, knames)

    def test_get_device_slave_knames_double_stacked(self):
        # /sys/class/block/wark/slaves/foo -> ../../foo
        # /sys/class/block/wark/slaves/bar -> ../../bar
        # /sys/class/block/foo
        # /sys/class/block/bar/slaves/zip -> ../../zip
        # /sys/class/block/zip
        #
        # mapping of device:
        cfg = OrderedDict([
            ('wark', ['foo', 'bar']),
            ('foo', []),
            ('bar', ['zip']),
            ('zip', []),
        ])
        device = 'wark'
        slaves = ['foo', 'zip']
        self._prepare_mocks(device, cfg)
        knames = block.get_device_slave_knames(device)
        self.assertEqual(slaves, knames)

# vi: ts=4 expandtab syntax=python
/n/n/ntests/unittests/test_block_zfs.py/n/nfrom curtin.block import zfs
from .helpers import CiTestCase


class TestBlockZfsJoinFlags(CiTestCase):
    def test_zfs_join_flags_bad_optflags(self):
        """""" zfs._join_flags raises ValueError if invalid optflag paramter """"""
        with self.assertRaises(ValueError):
            zfs._join_flags(None, {'a': 1})

        with self.assertRaises(ValueError):
            zfs._join_flags(23, {'a': 1})

    def test_zfs_join_flags_count_optflag(self):
        """""" zfs._join_flags has correct number of optflags in output """"""
        oflag = '-o'
        params = {'a': 1}
        result = zfs._join_flags(oflag, params)
        self.assertEqual(result.count(oflag), len(params))

        params = {}
        result = zfs._join_flags(oflag, params)
        self.assertEqual(result.count(oflag), len(params))

    def test_zfs_join_flags_bad_params(self):
        """""" zfs._join_flags raises ValueError if invalid params """"""
        with self.assertRaises(ValueError):
            zfs._join_flags('-o', None)

        with self.assertRaises(ValueError):
            zfs._join_flags('-p', [1, 2, 3])

        with self.assertRaises(ValueError):
            zfs._join_flags('-p', 'foobar')

    def test_zfs_join_flags_empty_params_ok(self):
        """""" zfs._join_flags returns empty list with empty params """"""
        self.assertEqual([], zfs._join_flags('-o', {}))

    def test_zfs_join_flags_in_key_equal_value(self):
        """""" zfs._join_flags converts dict to key=value """"""
        oflag = '-o'
        params = {'a': 1}
        result = zfs._join_flags(oflag, params)
        self.assertEqual([oflag, ""a=1""], result)

    def test_zfs_join_flags_converts_booleans(self):
        """""" zfs._join_flags converts True -> on, False -> off """"""
        params = {'setfoo': False, 'setwark': True}
        result = zfs._join_flags('-o', params)
        self.assertEqual(sorted([""-o"", ""setfoo=off"", ""-o"", ""setwark=on""]),
                         sorted(result))


class TestBlockZfsJoinPoolVolume(CiTestCase):

    def test_zfs_join_pool_volume(self):
        """""" zfs._join_pool_volume combines poolname and volume """"""
        pool = 'mypool'
        volume = '/myvolume'
        self.assertEqual('mypool/myvolume',
                         zfs._join_pool_volume(pool, volume))

    def test_zfs_join_pool_volume_extra_slash(self):
        """""" zfs._join_pool_volume removes extra slashes """"""
        pool = 'wark'
        volume = '//myvol/fs//foobar'
        self.assertEqual('wark/myvol/fs/foobar',
                         zfs._join_pool_volume(pool, volume))

    def test_zfs_join_pool_volume_no_slash(self):
        """""" zfs._join_pool_volume handles no slash """"""
        pool = 'rpool'
        volume = 'ROOT'
        self.assertEqual('rpool/ROOT', zfs._join_pool_volume(pool, volume))

    def test_zfs_join_pool_volume_invalid_pool(self):
        """""" zfs._join_pool_volume raises ValueError on invalid pool """"""
        with self.assertRaises(ValueError):
            zfs._join_pool_volume(None, 'myvol')

    def test_zfs_join_pool_volume_invalid_volume(self):
        """""" zfs._join_pool_volume raises ValueError on invalid volume """"""
        with self.assertRaises(ValueError):
            zfs._join_pool_volume('rpool', None)

    def test_zfs_join_pool_volume_empty_params(self):
        """""" zfs._join_pool_volume raises ValueError on invalid volume """"""
        with self.assertRaises(ValueError):
            zfs._join_pool_volume('', '')


class TestBlockZfsZpoolCreate(CiTestCase):

    def setUp(self):
        super(TestBlockZfsZpoolCreate, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')

    def test_zpool_create_raises_value_errors(self):
        """""" zfs.zpool_create raises ValueError/TypeError for invalid inputs """"""

        # poolname
        for val in [None, '', {'a': 1}]:
            with self.assertRaises(ValueError):
                zfs.zpool_create(val, [])

        # vdevs
        for val in [None, '', {'a': 1}, 'mydev']:
            with self.assertRaises(TypeError):
                # All the assert methods (except assertRaises(),
                # assertRaisesRegexp()) accept a msg argument that,
                # if specified, is used as the error message on failure
                print('vdev value: %s' % val)
                zfs.zpool_create('mypool', val)

    def test_zpool_create_default_pool_properties(self):
        """""" zpool_create uses default pool properties if none provided """"""
        zfs.zpool_create('mypool', ['/dev/disk/by-id/virtio-abcfoo1'])
        zpool_params = [""%s=%s"" % (k, v) for k, v in
                        zfs.ZPOOL_DEFAULT_PROPERTIES.items()]
        args, _ = self.mock_subp.call_args
        self.assertTrue(set(zpool_params).issubset(set(args[0])))

    def test_zpool_create_pool_iterable(self):
        """""" zpool_create accepts vdev iterables besides list """"""
        zfs.zpool_create('mypool', ('/dev/virtio-disk1', '/dev/virtio-disk2'))
        args, _ = self.mock_subp.call_args
        self.assertIn(""/dev/virtio-disk1"", args[0])
        self.assertIn(""/dev/virtio-disk2"", args[0])

    def test_zpool_create_default_zfs_properties(self):
        """""" zpool_create uses default zfs properties if none provided """"""
        zfs.zpool_create('mypool', ['/dev/disk/by-id/virtio-abcfoo1'])
        zfs_params = [""%s=%s"" % (k, v) for k, v in
                      zfs.ZFS_DEFAULT_PROPERTIES.items()]
        args, _ = self.mock_subp.call_args
        self.assertTrue(set(zfs_params).issubset(set(args[0])))

    def test_zpool_create_use_passed_properties(self):
        """""" zpool_create uses provided properties """"""
        zpool_props = {'prop1': 'val1'}
        zfs_props = {'fsprop1': 'val2'}
        zfs.zpool_create('mypool', ['/dev/disk/by-id/virtio-abcfoo1'],
                         pool_properties=zpool_props, zfs_properties=zfs_props)
        all_props = zpool_props.copy()
        all_props.update(zfs_props)
        params = [""%s=%s"" % (k, v) for k, v in all_props.items()]
        args, _ = self.mock_subp.call_args
        self.assertTrue(set(params).issubset(set(args[0])))

    def test_zpool_create_set_mountpoint(self):
        """""" zpool_create uses mountpoint """"""
        mountpoint = '/srv'
        zfs.zpool_create('mypool', ['/dev/disk/by-id/virtio-abcfoo1'],
                         mountpoint=mountpoint)
        args, _ = self.mock_subp.call_args
        self.assertIn(""mountpoint=%s"" % mountpoint, args[0])

    def test_zpool_create_set_altroot(self):
        """""" zpool_create uses altroot """"""
        altroot = '/var/tmp/mytarget'
        zfs.zpool_create('mypool', ['/dev/disk/by-id/virtio-abcfoo1'],
                         altroot=altroot)
        args, _ = self.mock_subp.call_args
        self.assertIn('-R', args[0])
        self.assertIn(altroot, args[0])

    def test_zpool_create_zfsroot(self):
        """""" zpool_create sets up root command correctly """"""
        pool = 'rpool'
        mountpoint = '/'
        altroot = '/var/tmp/mytarget'
        vdev = '/dev/disk/by-id/virtio-abcfoo1'
        zfs.zpool_create('rpool', [vdev], mountpoint=mountpoint,
                         altroot=altroot)
        # the dictionary ordering is not guaranteed which means the
        # pairs of parameters may shift; this does not harm the function
        # of the call, but is harder to test; instead we will compare
        # the arg list sorted
        args, kwargs = self.mock_subp.call_args
        print(args[0])
        print(kwargs)
        expected_args = (
            ['zpool', 'create', '-o', 'ashift=12', '-O', 'normalization=formD',
             '-O', 'canmount=off', '-O', 'atime=off', '-O', 'compression=lz4',
             '-O', 'mountpoint=%s' % mountpoint, '-R', altroot, pool,
             vdev])
        expected_kwargs = {'capture': True}
        self.assertEqual(sorted(expected_args), sorted(args[0]))
        self.assertEqual(expected_kwargs, kwargs)


class TestBlockZfsZfsCreate(CiTestCase):

    def setUp(self):
        super(TestBlockZfsZfsCreate, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')

    def test_zfs_create_raises_value_errors(self):
        """""" zfs.zfs_create raises ValueError for invalid inputs """"""

        # poolname
        for val in [None, '', {'a': 1}]:
            with self.assertRaises(ValueError):
                zfs.zfs_create(val, [])

        # volume
        for val in [None, '', {'a': 1}]:
            with self.assertRaises(ValueError):
                zfs.zfs_create('pool1', val)

        # properties
        for val in [12, ['a', 1]]:
            with self.assertRaises(ValueError):
                zfs.zfs_create('pool1', 'vol1', zfs_properties=val)

    def test_zfs_create_sets_zfs_properties(self):
        """""" zfs.zfs_create uses zfs_properties parameters """"""
        zfs_props = {'fsprop1': 'val2'}
        zfs.zfs_create('mypool', 'myvol', zfs_properties=zfs_props)
        params = [""%s=%s"" % (k, v) for k, v in zfs_props.items()]
        args, _ = self.mock_subp.call_args
        self.assertTrue(set(params).issubset(set(args[0])))

    def test_zfs_create_no_options(self):
        """""" zfs.zfs_create passes no options by default """"""
        pool = 'rpool'
        volume = 'ROOT'
        zfs.zfs_create(pool, volume)
        self.mock_subp.assert_called_with(['zfs', 'create', 'rpool/ROOT'],
                                          capture=True)

    def test_zfs_create_calls_mount_if_canmount_is_noauto(self):
        """""" zfs.zfs_create calls zfs mount if canmount=noauto """"""
        pool = 'rpool'
        volume = 'ROOT'
        props = {'canmount': 'noauto'}
        zfs.zfs_create(pool, volume, zfs_properties=props)
        self.mock_subp.assert_called_with(['zfs', 'mount',
                                           ""%s/%s"" % (pool, volume)],
                                          capture=True)


class TestBlockZfsZfsMount(CiTestCase):

    def setUp(self):
        super(TestBlockZfsZfsMount, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')

    def test_zfs_mount_raises_value_errors(self):
        """""" zfs.zfs_mount raises ValueError for invalid inputs """"""

        # poolname
        for pool in [None, '', {'a': 1}, 'rpool']:
            for vol in [None, '', {'a': 1}, 'vol1']:
                if pool == ""rpool"" and vol == ""vol1"":
                    continue
                with self.assertRaises(ValueError):
                    zfs.zfs_mount(pool, vol)

    def test_zfs_mount(self):
        """""" zfs.zfs_mount calls zfs mount command with pool and volume """"""
        pool = 'rpool'
        volume = 'home'
        zfs.zfs_mount(pool, volume)
        self.mock_subp.assert_called_with(['zfs', 'mount',
                                           '%s/%s' % (pool, volume)],
                                          capture=True)


class TestBlockZfsZpoolList(CiTestCase):

    def setUp(self):
        super(TestBlockZfsZpoolList, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')

    def test_zpool_list(self):
        """"""zpool list output returns list of pools""""""
        pools = ['fake_pool', 'wark', 'nodata']
        stdout = ""\n"".join(pools)
        self.mock_subp.return_value = (stdout, """")

        found_pools = zfs.zpool_list()
        self.assertEqual(sorted(pools), sorted(found_pools))

    def test_zpool_list_empty(self):
        """"""zpool list returns empty list with no pools""""""
        pools = []
        self.mock_subp.return_value = ("""", """")
        found_pools = zfs.zpool_list()
        self.assertEqual(sorted(pools), sorted(found_pools))


class TestBlockZfsZpoolExport(CiTestCase):

    def setUp(self):
        super(TestBlockZfsZpoolExport, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')

    def test_zpool_export_no_poolname(self):
        """"""zpool_export raises ValueError on invalid poolname""""""
        # poolname
        for val in [None, '', {'a': 1}]:
            with self.assertRaises(ValueError):
                zfs.zpool_export(val)

    def test_zpool_export(self):
        """"""zpool export calls zpool export <poolname>""""""
        poolname = 'fake_pool'
        zfs.zpool_export(poolname)
        self.mock_subp.assert_called_with(['zpool', 'export', poolname])


class TestBlockZfsDeviceToPoolname(CiTestCase):

    def setUp(self):
        super(TestBlockZfsDeviceToPoolname, self).setUp()
        self.add_patch('curtin.block.zfs.util.subp', 'mock_subp')
        self.add_patch('curtin.block.zfs.blkid', 'mock_blkid')

    def test_device_to_poolname_invalid_devname(self):
        """"""device_to_poolname raises ValueError on invalid devname""""""
        # devname
        for val in [None, '', {'a': 1}]:
            with self.assertRaises(ValueError):
                zfs.device_to_poolname(val)

    def test_device_to_poolname_finds_poolname(self):
        """"""find_poolname extracts 'LABEL' from zfs_member device""""""
        devname = '/dev/wark'
        poolname = 'fake_pool'
        self.mock_blkid.return_value = {
            devname: {'LABEL': poolname,
                      'PARTUUID': '52dff41a-49be-44b3-a36a-1b499e570e69',
                      'TYPE': 'zfs_member',
                      'UUID': '12590398935543668673',
                      'UUID_SUB': '7809435738165038086'}}

        found_poolname = zfs.device_to_poolname(devname)
        self.assertEqual(poolname, found_poolname)
        self.mock_blkid.assert_called_with(devs=[devname])

    def test_device_to_poolname_no_match(self):
        """"""device_to_poolname returns None if devname not in blkid results""""""
        devname = '/dev/wark'
        self.mock_blkid.return_value = {'/dev/foobar': {}}
        found_poolname = zfs.device_to_poolname(devname)
        self.assertEqual(None, found_poolname)
        self.mock_blkid.assert_called_with(devs=[devname])

    def test_device_to_poolname_no_zfs_member(self):
        """"""device_to_poolname returns None when device is not zfs_member""""""
        devname = '/dev/wark'
        self.mock_blkid.return_value = {devname: {'TYPE': 'foobar'}}
        found_poolname = zfs.device_to_poolname(devname)
        self.assertEqual(None, found_poolname)
        self.mock_blkid.assert_called_with(devs=[devname])


# vi: ts=4 expandtab syntax=python
/n/n/ntests/unittests/test_clear_holders.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import errno
import mock
import os
import textwrap

from curtin.block import clear_holders
from .helpers import CiTestCase


class TestClearHolders(CiTestCase):
    test_blockdev = '/dev/null'
    test_syspath = '/sys/class/block/null'
    remove_retries = [0.2] * 150  # clear_holders defaults to 30 seconds
    example_holders_trees = [
        [{'device': '/sys/class/block/sda', 'name': 'sda', 'holders':
          [{'device': '/sys/class/block/sda/sda1', 'name': 'sda1',
            'holders': [], 'dev_type': 'partition'},
           {'device': '/sys/class/block/sda/sda2', 'name': 'sda2',
            'holders': [], 'dev_type': 'partition'},
           {'device': '/sys/class/block/sda/sda5', 'name': 'sda5', 'holders':
            [{'device': '/sys/class/block/dm-0', 'name': 'dm-0', 'holders':
              [{'device': '/sys/class/block/dm-1', 'name': 'dm-1',
                'holders': [], 'dev_type': 'lvm'},
               {'device': '/sys/class/block/dm-2', 'name': 'dm-2', 'holders':
                [{'device': '/sys/class/block/dm-3', 'name': 'dm-3',
                  'holders': [], 'dev_type': 'crypt'}],
                'dev_type': 'lvm'}],
              'dev_type': 'crypt'}],
            'dev_type': 'partition'}],
          'dev_type': 'disk'}],
        [{""device"": ""/sys/class/block/vdb"", 'name': 'vdb', ""holders"":
          [{""device"": ""/sys/class/block/vdb/vdb1"", 'name': 'vdb1',
            ""holders"": [], ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb2"", 'name': 'vdb2',
            ""holders"": [], ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb3"", 'name': 'vdb3', ""holders"":
            [{""device"": ""/sys/class/block/md0"", 'name': 'md0', ""holders"":
              [{""device"": ""/sys/class/block/bcache1"", 'name': 'bcache1',
                ""holders"": [], ""dev_type"": ""bcache""}],
              ""dev_type"": ""raid""}],
            ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb4"", 'name': 'vdb4', ""holders"":
            [{""device"": ""/sys/class/block/md0"", 'name': 'md0', ""holders"":
              [{""device"": ""/sys/class/block/bcache1"", 'name': 'bcache1',
                ""holders"": [], ""dev_type"": ""bcache""}],
              ""dev_type"": ""raid""}],
            ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb5"", 'name': 'vdb5', ""holders"":
            [{""device"": ""/sys/class/block/md0"", 'name': 'md0', ""holders"":
              [{""device"": ""/sys/class/block/bcache1"", 'name': 'bcache1',
                ""holders"": [], ""dev_type"": ""bcache""}],
              ""dev_type"": ""raid""}],
            ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb6"", 'name': 'vdb6', ""holders"":
            [{""device"": ""/sys/class/block/bcache1"", 'name': 'bcache1',
              ""holders"": [], ""dev_type"": ""bcache""},
             {""device"": ""/sys/class/block/bcache2"", 'name': 'bcache2',
              ""holders"": [], ""dev_type"": ""bcache""}],
            ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb7"", 'name': 'vdb7', ""holders"":
            [{""device"": ""/sys/class/block/bcache2"", 'name': 'bcache2',
              ""holders"": [], ""dev_type"": ""bcache""}],
            ""dev_type"": ""partition""},
           {""device"": ""/sys/class/block/vdb/vdb8"", 'name': 'vdb8',
            ""holders"": [], ""dev_type"": ""partition""}],
          ""dev_type"": ""disk""},
         {""device"": ""/sys/class/block/vdc"", 'name': 'vdc', ""holders"": [],
          ""dev_type"": ""disk""},
         {""device"": ""/sys/class/block/vdd"", 'name': 'vdd', ""holders"":
          [{""device"": ""/sys/class/block/vdd/vdd1"", 'name': 'vdd1',
            ""holders"": [], ""dev_type"": ""partition""}],
          ""dev_type"": ""disk""}],
    ]

    @mock.patch('curtin.block.clear_holders.block')
    @mock.patch('curtin.block.clear_holders.util')
    def test_get_dmsetup_uuid(self, mock_util, mock_block):
        """"""ensure that clear_holders.get_dmsetup_uuid works as expected""""""
        uuid = ""CRYPT-LUKS1-fe335a74374e4649af9776c1699676f8-sdb5_crypt""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_util.subp.return_value = (' ' + uuid + '\n', None)
        res = clear_holders.get_dmsetup_uuid(self.test_syspath)
        mock_util.subp.assert_called_with(
            ['dmsetup', 'info', self.test_blockdev, '-C', '-o',
             'uuid', '--noheadings'], capture=True)
        self.assertEqual(res, uuid)
        mock_block.sysfs_to_devpath.assert_called_with(self.test_syspath)

    @mock.patch('curtin.block.clear_holders.block')
    @mock.patch('curtin.block.clear_holders.os')
    def test_get_bcache_using_dev(self, mock_os, mock_block):
        """"""Ensure that get_bcache_using_dev works""""""
        fake_bcache = '/sys/fs/bcache/fake'
        mock_os.path.join.side_effect = os.path.join
        mock_block.sys_block_path.return_value = self.test_syspath
        mock_os.path.realpath.return_value = fake_bcache

        bcache_dir = clear_holders.get_bcache_using_dev(self.test_blockdev)
        mock_os.path.realpath.assert_called_with(self.test_syspath +
                                                 '/bcache/cache')
        self.assertEqual(bcache_dir, fake_bcache)

    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.block')
    def test_get_bcache_sys_path(self, mock_block, mock_os):
        fake_backing = '/sys/class/block/fake'
        mock_block.sys_block_path.return_value = fake_backing
        mock_os.path.join.side_effect = os.path.join
        mock_os.path.exists.return_value = True
        bcache_dir = clear_holders.get_bcache_sys_path(""/dev/fake"")
        self.assertEqual(bcache_dir, fake_backing + ""/bcache"")

    @mock.patch('curtin.block.clear_holders.get_dmsetup_uuid')
    @mock.patch('curtin.block.clear_holders.block')
    def test_differentiate_lvm_and_crypt(
            self, mock_block, mock_get_dmsetup_uuid):
        """"""test clear_holders.identify_lvm and clear_holders.identify_crypt""""""
        for (kname, dm_uuid, is_lvm, is_crypt) in [
                ('dm-0', 'LVM-abcdefg', True, False),
                ('sda', 'LVM-abcdefg', False, False),
                ('sda', 'CRYPT-abcdefg', False, False),
                ('dm-0', 'CRYPT-abcdefg', False, True),
                ('dm-1', 'invalid', False, False)]:
            mock_block.path_to_kname.return_value = kname
            mock_get_dmsetup_uuid.return_value = dm_uuid
            self.assertEqual(
                is_lvm, clear_holders.identify_lvm(self.test_syspath))
            self.assertEqual(
                is_crypt, clear_holders.identify_crypt(self.test_syspath))
            mock_block.path_to_kname.assert_called_with(self.test_syspath)
            mock_get_dmsetup_uuid.assert_called_with(self.test_syspath)

    @mock.patch('curtin.block.clear_holders.udev.udevadm_settle')
    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache(self, mock_get_bcache, mock_log, mock_os,
                             mock_util, mock_get_bcache_block,
                             mock_udevadm_settle):
        """"""test clear_holders.shutdown_bcache""""""
        #
        # pass in a sysfs path to a bcache block device,
        # determine the bcache cset it is part of (or not)
        # 1) stop the cset device (if it's enabled)
        # 2) wait on cset to be removed if it was present
        # 3) stop the block device (if it's still present after stopping cset)
        # 4) wait on bcache block device to be removed
        #

        device = self.test_syspath
        bcache_cset_uuid = 'c08ae789-a964-46fb-a66e-650f0ae78f94'

        mock_os.path.exists.return_value = True
        mock_os.path.join.side_effect = os.path.join
        # os.path.realpath on symlink of /sys/class/block/null/bcache/cache ->
        # to /sys/fs/bcache/cset_UUID
        mock_get_bcache.return_value = '/sys/fs/bcache/' + bcache_cset_uuid
        mock_get_bcache_block.return_value = device + '/bcache'

        clear_holders.shutdown_bcache(device)

        mock_get_bcache.assert_called_with(device, strict=False)
        mock_get_bcache_block.assert_called_with(device, strict=False)

        self.assertTrue(mock_log.info.called)
        self.assertFalse(mock_log.warn.called)
        mock_util.wait_for_removal.assert_has_calls([
                mock.call('/sys/fs/bcache/' + bcache_cset_uuid,
                          retries=self.remove_retries),
                mock.call(device, retries=self.remove_retries)])

        mock_util.write_file.assert_has_calls([
                mock.call('/sys/fs/bcache/%s/stop' % bcache_cset_uuid,
                          '1', mode=None),
                mock.call(device + '/bcache/stop',
                          '1', mode=None)])

    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_non_sysfs_device(self, mock_get_bcache, mock_log,
                                              mock_os, mock_util,
                                              mock_get_bcache_block):
        device = ""/dev/fakenull""
        with self.assertRaises(ValueError):
            clear_holders.shutdown_bcache(device)

        self.assertEqual(0, len(mock_get_bcache.call_args_list))
        self.assertEqual(0, len(mock_log.call_args_list))
        self.assertEqual(0, len(mock_os.call_args_list))
        self.assertEqual(0, len(mock_util.call_args_list))
        self.assertEqual(0, len(mock_get_bcache_block.call_args_list))

    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_no_device(self, mock_get_bcache, mock_log,
                                       mock_os, mock_util,
                                       mock_get_bcache_block):
        device = ""/sys/class/block/null""
        mock_os.path.exists.return_value = False

        clear_holders.shutdown_bcache(device)

        self.assertEqual(1, len(mock_log.info.call_args_list))
        self.assertEqual(1, len(mock_os.path.exists.call_args_list))
        self.assertEqual(0, len(mock_get_bcache.call_args_list))
        self.assertEqual(0, len(mock_util.call_args_list))
        self.assertEqual(0, len(mock_get_bcache_block.call_args_list))

    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_no_cset(self, mock_get_bcache, mock_log,
                                     mock_os, mock_util,
                                     mock_get_bcache_block):
        device = ""/sys/class/block/null""
        mock_os.path.exists.side_effect = iter([
                True,   # backing device exists
                False,  # cset device not present (already removed)
                True,   # backing device (still) exists
        ])
        mock_get_bcache.return_value = '/sys/fs/bcache/fake'
        mock_get_bcache_block.return_value = device + '/bcache'
        mock_os.path.join.side_effect = os.path.join

        clear_holders.shutdown_bcache(device)

        self.assertEqual(2, len(mock_log.info.call_args_list))
        self.assertEqual(3, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_get_bcache.call_args_list))
        self.assertEqual(1, len(mock_get_bcache_block.call_args_list))
        self.assertEqual(1, len(mock_util.write_file.call_args_list))
        self.assertEqual(2, len(mock_util.wait_for_removal.call_args_list))

        mock_get_bcache.assert_called_with(device, strict=False)
        mock_get_bcache_block.assert_called_with(device, strict=False)
        mock_util.write_file.assert_called_with(device + '/bcache/stop',
                                                '1', mode=None)
        retries = self.remove_retries
        mock_util.wait_for_removal.assert_has_calls([
            mock.call(device, retries=retries),
            mock.call(device + '/bcache', retries=retries)])

    @mock.patch('curtin.block.clear_holders.udev.udevadm_settle')
    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_delete_cset_and_backing(self, mock_get_bcache,
                                                     mock_log, mock_os,
                                                     mock_util,
                                                     mock_get_bcache_block,
                                                     mock_udevadm_settle):
        device = ""/sys/class/block/null""
        mock_os.path.exists.side_effect = iter([
                True,  # backing device exists
                True,  # cset device not present (already removed)
                True,  # backing device (still) exists
        ])
        cset = '/sys/fs/bcache/fake'
        mock_get_bcache.return_value = cset
        mock_get_bcache_block.return_value = device + '/bcache'
        mock_os.path.join.side_effect = os.path.join

        clear_holders.shutdown_bcache(device)

        self.assertEqual(2, len(mock_log.info.call_args_list))
        self.assertEqual(3, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_get_bcache.call_args_list))
        self.assertEqual(1, len(mock_get_bcache_block.call_args_list))
        self.assertEqual(2, len(mock_util.write_file.call_args_list))
        self.assertEqual(3, len(mock_util.wait_for_removal.call_args_list))

        mock_get_bcache.assert_called_with(device, strict=False)
        mock_get_bcache_block.assert_called_with(device, strict=False)
        mock_util.write_file.assert_has_calls([
            mock.call(cset + '/stop', '1', mode=None),
            mock.call(device + '/bcache/stop', '1', mode=None)])
        mock_util.wait_for_removal.assert_has_calls([
            mock.call(cset, retries=self.remove_retries),
            mock.call(device, retries=self.remove_retries)
        ])

    @mock.patch('curtin.block.clear_holders.udev.udevadm_settle')
    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_delete_cset_no_backing(self, mock_get_bcache,
                                                    mock_log, mock_os,
                                                    mock_util,
                                                    mock_get_bcache_block,
                                                    mock_udevadm_settle):
        device = ""/sys/class/block/null""
        mock_os.path.exists.side_effect = iter([
                True,   # backing device exists
                True,   # cset device not present (already removed)
                False,  # backing device is removed with cset
        ])
        cset = '/sys/fs/bcache/fake'
        mock_get_bcache.return_value = cset
        mock_get_bcache_block.return_value = device + '/bcache'
        mock_os.path.join.side_effect = os.path.join

        clear_holders.shutdown_bcache(device)

        self.assertEqual(2, len(mock_log.info.call_args_list))
        self.assertEqual(3, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_get_bcache.call_args_list))
        self.assertEqual(1, len(mock_get_bcache_block.call_args_list))
        self.assertEqual(1, len(mock_util.write_file.call_args_list))
        self.assertEqual(1, len(mock_util.wait_for_removal.call_args_list))

        mock_get_bcache.assert_called_with(device, strict=False)
        mock_util.write_file.assert_has_calls([
            mock.call(cset + '/stop', '1', mode=None),
        ])
        mock_util.wait_for_removal.assert_has_calls([
            mock.call(cset, retries=self.remove_retries)
        ])

    # test bcache shutdown with 'stop' sysfs write failure
    @mock.patch('curtin.block.clear_holders.udev.udevadm_settle')
    @mock.patch('curtin.block.clear_holders.get_bcache_sys_path')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.get_bcache_using_dev')
    def test_shutdown_bcache_stop_sysfs_write_fails(self, mock_get_bcache,
                                                    mock_log, mock_os,
                                                    mock_util,
                                                    mock_get_bcache_block,
                                                    mock_udevadm_settle):
        """"""Test writes sysfs write failures pass if file not present""""""
        device = ""/sys/class/block/null""
        mock_os.path.exists.side_effect = iter([
                True,   # backing device exists
                True,   # cset device not present (already removed)
                False,  # backing device is removed with cset
                False,  # bcache/stop sysfs is missing (already removed)
        ])
        cset = '/sys/fs/bcache/fake'
        mock_get_bcache.return_value = cset
        mock_get_bcache_block.return_value = device + '/bcache'
        mock_os.path.join.side_effect = os.path.join

        # make writes to sysfs fail
        mock_util.write_file.side_effect = IOError(errno.ENOENT,
                                                   ""File not found"")

        clear_holders.shutdown_bcache(device)

        self.assertEqual(2, len(mock_log.info.call_args_list))
        self.assertEqual(3, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_get_bcache.call_args_list))
        self.assertEqual(1, len(mock_get_bcache_block.call_args_list))
        self.assertEqual(1, len(mock_util.write_file.call_args_list))
        self.assertEqual(1, len(mock_util.wait_for_removal.call_args_list))

        mock_get_bcache.assert_called_with(device, strict=False)
        mock_util.write_file.assert_has_calls([
            mock.call(cset + '/stop', '1', mode=None),
        ])
        mock_util.wait_for_removal.assert_has_calls([
            mock.call(cset, retries=self.remove_retries)
        ])

    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.block.sys_block_path')
    @mock.patch('curtin.block.clear_holders.lvm')
    @mock.patch('curtin.block.clear_holders.util')
    def test_shutdown_lvm(self, mock_util, mock_lvm, mock_syspath, mock_log):
        """"""test clear_holders.shutdown_lvm""""""
        vg_name = 'volgroup1'
        lv_name = 'lvol1'
        mock_syspath.return_value = self.test_blockdev
        mock_util.load_file.return_value = '-'.join((vg_name, lv_name))
        mock_lvm.split_lvm_name.return_value = (vg_name, lv_name)
        mock_lvm.get_lvols_in_volgroup.return_value = ['lvol2']
        clear_holders.shutdown_lvm(self.test_blockdev)
        mock_syspath.assert_called_with(self.test_blockdev)
        mock_util.load_file.assert_called_with(self.test_blockdev + '/dm/name')
        mock_lvm.split_lvm_name.assert_called_with(
            '-'.join((vg_name, lv_name)))
        self.assertTrue(mock_log.debug.called)
        mock_util.subp.assert_called_with(
            ['lvremove', '--force', '--force', '/'.join((vg_name, lv_name))],
            rcs=[0, 5])
        mock_lvm.get_lvols_in_volgroup.assert_called_with(vg_name)
        self.assertEqual(len(mock_util.subp.call_args_list), 1)
        self.assertTrue(mock_lvm.lvm_scan.called)
        mock_lvm.get_lvols_in_volgroup.return_value = []
        clear_holders.shutdown_lvm(self.test_blockdev)
        mock_util.subp.assert_called_with(
            ['vgremove', '--force', '--force', vg_name], rcs=[0, 5])

    @mock.patch('curtin.block.clear_holders.block')
    @mock.patch('curtin.block.clear_holders.util')
    def test_shutdown_crypt(self, mock_util, mock_block):
        """"""test clear_holders.shutdown_crypt""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        clear_holders.shutdown_crypt(self.test_syspath)
        mock_block.sysfs_to_devpath.assert_called_with(self.test_syspath)
        mock_util.subp.assert_called_with(
            ['cryptsetup', 'remove', self.test_blockdev], capture=True)

    @mock.patch('curtin.block.clear_holders.time')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.mdadm')
    @mock.patch('curtin.block.clear_holders.block')
    def test_shutdown_mdadm(self, mock_block, mock_mdadm, mock_log, mock_util,
                            mock_time):
        """"""test clear_holders.shutdown_mdadm""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_block.path_to_kname.return_value = self.test_blockdev
        mock_mdadm.md_present.return_value = False
        clear_holders.shutdown_mdadm(self.test_syspath)
        mock_mdadm.mdadm_stop.assert_called_with(self.test_blockdev)
        mock_mdadm.md_present.assert_called_with(self.test_blockdev)
        self.assertTrue(mock_log.debug.called)

    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.time')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.mdadm')
    @mock.patch('curtin.block.clear_holders.block')
    def test_shutdown_mdadm_fail_raises_oserror(self, mock_block, mock_mdadm,
                                                mock_log, mock_util, mock_time,
                                                mock_os):
        """"""test clear_holders.shutdown_mdadm raises OSError on failure""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_block.path_to_kname.return_value = self.test_blockdev
        mock_mdadm.md_present.return_value = True
        mock_util.subp.return_value = ("""", """")
        mock_os.path.exists.return_value = True

        with self.assertRaises(OSError):
            clear_holders.shutdown_mdadm(self.test_syspath)

        mock_mdadm.mdadm_stop.assert_called_with(self.test_blockdev)
        mock_mdadm.md_present.assert_called_with(self.test_blockdev)
        mock_util.load_file.assert_called_with('/proc/mdstat')
        self.assertTrue(mock_log.debug.called)
        self.assertTrue(mock_log.critical.called)

    @mock.patch('curtin.block.clear_holders.os')
    @mock.patch('curtin.block.clear_holders.time')
    @mock.patch('curtin.block.clear_holders.util')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.mdadm')
    @mock.patch('curtin.block.clear_holders.block')
    def test_shutdown_mdadm_fails_no_proc_mdstat(self, mock_block, mock_mdadm,
                                                 mock_log, mock_util,
                                                 mock_time, mock_os):
        """"""test clear_holders.shutdown_mdadm handles no /proc/mdstat""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_block.path_to_kname.return_value = self.test_blockdev
        mock_mdadm.md_present.return_value = True
        mock_os.path.exists.return_value = False

        with self.assertRaises(OSError):
            clear_holders.shutdown_mdadm(self.test_syspath)

        mock_mdadm.mdadm_stop.assert_called_with(self.test_blockdev)
        mock_mdadm.md_present.assert_called_with(self.test_blockdev)
        self.assertEqual([], mock_util.subp.call_args_list)
        self.assertTrue(mock_log.debug.called)
        self.assertTrue(mock_log.critical.called)

    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.block')
    def test_clear_holders_wipe_superblock(self, mock_block, mock_log):
        """"""test clear_holders.wipe_superblock handles errors right""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_block.is_extended_partition.return_value = True
        clear_holders.wipe_superblock(self.test_syspath)
        self.assertFalse(mock_block.wipe_volume.called)
        mock_block.is_extended_partition.return_value = False
        mock_block.is_zfs_member.return_value = False
        clear_holders.wipe_superblock(self.test_syspath)
        mock_block.sysfs_to_devpath.assert_called_with(self.test_syspath)
        mock_block.wipe_volume.assert_called_with(
            self.test_blockdev, mode='superblock')

    @mock.patch('curtin.block.clear_holders.zfs')
    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.block')
    def test_clear_holders_wipe_superblock_zfs(self, mock_block, mock_log,
                                               mock_zfs):
        """"""test clear_holders.wipe_superblock handles zfs member""""""
        mock_block.sysfs_to_devpath.return_value = self.test_blockdev
        mock_block.is_extended_partition.return_value = True
        clear_holders.wipe_superblock(self.test_syspath)
        self.assertFalse(mock_block.wipe_volume.called)
        mock_block.is_extended_partition.return_value = False
        mock_block.is_zfs_member.return_value = True
        mock_zfs.device_to_poolname.return_value = 'fake_pool'
        clear_holders.wipe_superblock(self.test_syspath)
        mock_block.sysfs_to_devpath.assert_called_with(self.test_syspath)
        mock_zfs.zpool_export.assert_called_with('fake_pool')
        mock_block.wipe_volume.assert_called_with(
            self.test_blockdev, mode='superblock')

    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.block')
    @mock.patch('curtin.block.clear_holders.os')
    def test_get_holders(self, mock_os, mock_block, mock_log):
        """"""test clear_holders.get_holders""""""
        mock_block.sys_block_path.return_value = self.test_syspath
        mock_os.path.join.side_effect = os.path.join
        clear_holders.get_holders(self.test_blockdev)
        mock_block.sys_block_path.assert_called_with(self.test_blockdev)
        mock_os.path.join.assert_called_with(self.test_syspath, 'holders')
        self.assertTrue(mock_log.debug.called)
        mock_os.listdir.assert_called_with(
            os.path.join(self.test_syspath, 'holders'))

    def test_plan_shutdown_holders_trees(self):
        """"""
        make sure clear_holdrs.plan_shutdown_holders_tree orders shutdown
        functions correctly and uses the appropriate shutdown function for each
        dev type
        """"""
        # trees that have been generated, checked for correctness,
        # and the order that they should be shut down in (by level)
        test_trees_and_orders = [
            (self.example_holders_trees[0][0],
             ({'dm-3'}, {'dm-1', 'dm-2'}, {'dm-0'}, {'sda5', 'sda2', 'sda1'},
              {'sda'})),
            (self.example_holders_trees[1],
             ({'bcache1'}, {'bcache2', 'md0'},
              {'vdb1', 'vdb2', 'vdb3', 'vdb4', 'vdb5', 'vdb6', 'vdb7', 'vdb8',
               'vdd1'},
              {'vdb', 'vdc', 'vdd'}))
        ]
        for tree, correct_order in test_trees_and_orders:
            res = clear_holders.plan_shutdown_holder_trees(tree)
            for level in correct_order:
                self.assertEqual({os.path.basename(e['device'])
                                  for e in res[:len(level)]}, level)
                res = res[len(level):]

    def test_format_holders_tree(self):
        """"""test output of clear_holders.format_holders_tree""""""
        test_trees_and_results = [
            (self.example_holders_trees[0][0],
             textwrap.dedent(""""""
                 sda
                 |-- sda1
                 |-- sda2
                 `-- sda5
                     `-- dm-0
                         |-- dm-1
                         `-- dm-2
                             `-- dm-3
                 """""").strip()),
            (self.example_holders_trees[1][0],
             textwrap.dedent(""""""
                 vdb
                 |-- vdb1
                 |-- vdb2
                 |-- vdb3
                 |   `-- md0
                 |       `-- bcache1
                 |-- vdb4
                 |   `-- md0
                 |       `-- bcache1
                 |-- vdb5
                 |   `-- md0
                 |       `-- bcache1
                 |-- vdb6
                 |   |-- bcache1
                 |   `-- bcache2
                 |-- vdb7
                 |   `-- bcache2
                 `-- vdb8
                 """""").strip()),
            (self.example_holders_trees[1][1], 'vdc'),
            (self.example_holders_trees[1][2],
             textwrap.dedent(""""""
                 vdd
                 `-- vdd1
                 """""").strip())
        ]
        for tree, result in test_trees_and_results:
            self.assertEqual(clear_holders.format_holders_tree(tree), result)

    @mock.patch('curtin.block.clear_holders.util.write_file')
    def test_maybe_stop_bcache_device_raises_errors(self, m_write_file):
        """"""Non-IO/OS exceptions are raised by maybe_stop_bcache_device.""""""
        m_write_file.side_effect = ValueError('Crazy Value Error')
        with self.assertRaises(ValueError) as cm:
            clear_holders.maybe_stop_bcache_device('does/not/matter')
        self.assertEqual('Crazy Value Error', str(cm.exception))
        self.assertEqual(
            mock.call('does/not/matter/stop', '1', mode=None),
            m_write_file.call_args)

    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.util.write_file')
    def test_maybe_stop_bcache_device_handles_oserror(self, m_write_file,
                                                      m_log):
        """"""When OSError.NOENT is raised, log the condition and move on.""""""
        m_write_file.side_effect = OSError(errno.ENOENT, 'Expected oserror')
        clear_holders.maybe_stop_bcache_device('does/not/matter')
        self.assertEqual(
            'Error writing to bcache stop file %s, device removed: %s',
            m_log.debug.call_args[0][0])
        self.assertEqual('does/not/matter/stop', m_log.debug.call_args[0][1])

    @mock.patch('curtin.block.clear_holders.LOG')
    @mock.patch('curtin.block.clear_holders.util.write_file')
    def test_maybe_stop_bcache_device_handles_ioerror(self, m_write_file,
                                                      m_log):
        """"""When IOError.NOENT is raised, log the condition and move on.""""""
        m_write_file.side_effect = IOError(errno.ENOENT, 'Expected ioerror')
        clear_holders.maybe_stop_bcache_device('does/not/matter')
        self.assertEqual(
            'Error writing to bcache stop file %s, device removed: %s',
            m_log.debug.call_args[0][0])
        self.assertEqual('does/not/matter/stop', m_log.debug.call_args[0][1])

    def test_get_holder_types(self):
        """"""test clear_holders.get_holder_types""""""
        test_trees_and_results = [
            (self.example_holders_trees[0][0],
             {('disk', '/sys/class/block/sda'),
              ('partition', '/sys/class/block/sda/sda1'),
              ('partition', '/sys/class/block/sda/sda2'),
              ('partition', '/sys/class/block/sda/sda5'),
              ('crypt', '/sys/class/block/dm-0'),
              ('lvm', '/sys/class/block/dm-1'),
              ('lvm', '/sys/class/block/dm-2'),
              ('crypt', '/sys/class/block/dm-3')}),
            (self.example_holders_trees[1][0],
             {('disk', '/sys/class/block/vdb'),
              ('partition', '/sys/class/block/vdb/vdb1'),
              ('partition', '/sys/class/block/vdb/vdb2'),
              ('partition', '/sys/class/block/vdb/vdb3'),
              ('partition', '/sys/class/block/vdb/vdb4'),
              ('partition', '/sys/class/block/vdb/vdb5'),
              ('partition', '/sys/class/block/vdb/vdb6'),
              ('partition', '/sys/class/block/vdb/vdb7'),
              ('partition', '/sys/class/block/vdb/vdb8'),
              ('raid', '/sys/class/block/md0'),
              ('bcache', '/sys/class/block/bcache1'),
              ('bcache', '/sys/class/block/bcache2')})
        ]
        for tree, result in test_trees_and_results:
            self.assertEqual(clear_holders.get_holder_types(tree), result)

    @mock.patch('curtin.block.clear_holders.block.sys_block_path')
    @mock.patch('curtin.block.clear_holders.gen_holders_tree')
    def test_assert_clear(self, mock_gen_holders_tree, mock_syspath):
        mock_gen_holders_tree.return_value = self.example_holders_trees[0][0]
        mock_syspath.side_effect = lambda x: x
        device = '/dev/null'
        with self.assertRaises(OSError):
            clear_holders.assert_clear(device)
            mock_gen_holders_tree.assert_called_with(device)
        mock_gen_holders_tree.return_value = self.example_holders_trees[1][1]
        clear_holders.assert_clear(device)

    @mock.patch('curtin.block.clear_holders.mdadm')
    @mock.patch('curtin.block.clear_holders.util')
    def test_start_clear_holders_deps(self, mock_util, mock_mdadm):
        clear_holders.start_clear_holders_deps()
        mock_mdadm.mdadm_assemble.assert_called_with(
            scan=True, ignore_errors=True)
        mock_util.load_kernel_module.has_calls(
                mock.call('bcache'), mock.call('zfs'))
# vi: ts=4 expandtab syntax=python
/n/n/ntests/unittests/test_util.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

from unittest import skipIf
import mock
import os
import stat
from textwrap import dedent

from curtin import util
from .helpers import CiTestCase, simple_mocked_open


class TestLogTimer(CiTestCase):
    def test_logger_called(self):
        data = {}

        def mylog(msg):
            data['msg'] = msg

        with util.LogTimer(mylog, ""mymessage""):
            pass

        self.assertIn(""msg"", data)
        self.assertIn(""mymessage"", data['msg'])


class TestDisableDaemons(CiTestCase):
    prcpath = ""usr/sbin/policy-rc.d""

    def setUp(self):
        super(TestDisableDaemons, self).setUp()
        self.target = self.tmp_dir()
        self.temp_prc = os.path.join(self.target, self.prcpath)

    def test_disable_daemons_in_root_works(self):
        ret = util.disable_daemons_in_root(self.target)
        self.assertTrue(ret)
        self.assertTrue(os.path.exists(self.temp_prc))

        ret = util.undisable_daemons_in_root(self.target)

        # return should have been true (it removed) and file should be gone
        self.assertTrue(ret)
        self.assertFalse(os.path.exists(self.temp_prc))

    def test_disable_daemons_with_existing_is_false(self):
        util.write_file(os.path.join(self.target, self.prcpath), ""foo"")
        ret = util.disable_daemons_in_root(self.target)

        # the return should have been false (it did not create)
        # but the file should still exist
        self.assertFalse(ret)
        self.assertTrue(os.path.exists(self.temp_prc))


class TestWhich(CiTestCase):

    def setUp(self):
        super(TestWhich, self).setUp()
        self.orig_is_exe = util.is_exe
        util.is_exe = self.my_is_exe
        self.orig_path = os.environ.get(""PATH"")
        os.environ[""PATH""] = ""/usr/bin:/usr/sbin:/bin:/sbin""

    def tearDown(self):
        if self.orig_path is None:
            del os.environ[""PATH""]
        else:
            os.environ[""PATH""] = self.orig_path

        util.is_exe = self.orig_is_exe
        self.exe_list = []

    def my_is_exe(self, fpath):
        return os.path.abspath(fpath) in self.exe_list

    def test_target_none(self):
        self.exe_list = [""/usr/bin/ls""]
        self.assertEqual(util.which(""ls""), ""/usr/bin/ls"")

    def test_no_program_target_none(self):
        self.exe_list = []
        self.assertEqual(util.which(""fuzz""), None)

    def test_target_set(self):
        self.exe_list = [""/foo/bin/ls""]
        self.assertEqual(util.which(""ls"", target=""/foo""), ""/bin/ls"")

    def test_no_program_target_set(self):
        self.exe_list = [""/usr/bin/ls""]
        self.assertEqual(util.which(""fuzz""), None)

    def test_custom_path_target_unset(self):
        self.exe_list = [""/usr/bin2/fuzz""]
        self.assertEqual(
            util.which(""fuzz"", search=[""/bin1"", ""/usr/bin2""]),
            ""/usr/bin2/fuzz"")

    def test_custom_path_target_set(self):
        self.exe_list = [""/target/usr/bin2/fuzz""]
        found = util.which(""fuzz"", search=[""/bin1"", ""/usr/bin2""],
                           target=""/target"")
        self.assertEqual(found, ""/usr/bin2/fuzz"")


class TestLsbRelease(CiTestCase):

    def setUp(self):
        super(TestLsbRelease, self).setUp()
        self._reset_cache()

    def _reset_cache(self):
        keys = [k for k in util._LSB_RELEASE.keys()]
        for d in keys:
            del util._LSB_RELEASE[d]

    @mock.patch(""curtin.util.subp"")
    def test_lsb_release_functional(self, mock_subp):
        output = '\n'.join([
            ""Distributor ID: Ubuntu"",
            ""Description:    Ubuntu 14.04.2 LTS"",
            ""Release:    14.04"",
            ""Codename:   trusty"",
        ])
        rdata = {'id': 'Ubuntu', 'description': 'Ubuntu 14.04.2 LTS',
                 'codename': 'trusty', 'release': '14.04'}

        def fake_subp(cmd, capture=False, target=None):
            return output, 'No LSB modules are available.'

        mock_subp.side_effect = fake_subp
        found = util.lsb_release()
        mock_subp.assert_called_with(
            ['lsb_release', '--all'], capture=True, target=None)
        self.assertEqual(found, rdata)

    @mock.patch(""curtin.util.subp"")
    def test_lsb_release_unavailable(self, mock_subp):
        def doraise(*args, **kwargs):
            raise util.ProcessExecutionError(""foo"")
        mock_subp.side_effect = doraise

        expected = {k: ""UNAVAILABLE"" for k in
                    ('id', 'description', 'codename', 'release')}
        self.assertEqual(util.lsb_release(), expected)


class TestSubp(CiTestCase):

    stdin2err = ['bash', '-c', 'cat >&2']
    stdin2out = ['cat']
    bin_true = ['bash', '-c', ':']
    exit_with_value = ['bash', '-c', 'exit ${1:-0}', 'test_subp_exit_val']
    utf8_invalid = b'ab\xaadef'
    utf8_valid = b'start \xc3\xa9 end'
    utf8_valid_2 = b'd\xc3\xa9j\xc8\xa7'

    try:
        decode_type = unicode
        nodecode_type = str
    except NameError:
        decode_type = str
        nodecode_type = bytes

    def setUp(self):
        super(TestSubp, self).setUp()
        self.add_patch(
            'curtin.util._get_unshare_pid_args', 'mock_get_unshare_pid_args',
            return_value=[])

    def printf_cmd(self, *args):
        # bash's printf supports \xaa.  So does /usr/bin/printf
        # but by using bash, we remove dependency on another program.
        return(['bash', '-c', 'printf ""$@""', 'printf'] + list(args))

    def test_subp_handles_utf8(self):
        # The given bytes contain utf-8 accented characters as seen in e.g.
        # the ""deja dup"" package in Ubuntu.
        cmd = self.printf_cmd(self.utf8_valid_2)
        (out, _err) = util.subp(cmd, capture=True)
        self.assertEqual(out, self.utf8_valid_2.decode('utf-8'))

    def test_subp_target_as_different_forms_of_slash_works(self):
        # passing target=/ in any form should work.

        # it is assumed that if chroot was used, then test case would
        # fail unless user was root ('chroot /' is still priviledged)
        util.subp(self.bin_true, target=""/"")
        util.subp(self.bin_true, target=""//"")
        util.subp(self.bin_true, target=""///"")
        util.subp(self.bin_true, target=""//etc/..//"")

    def test_subp_exit_nonzero_raises(self):
        exc = None
        try:
            util.subp(self.exit_with_value + [""9""])
        except util.ProcessExecutionError as e:
            self.assertEqual(9, e.exit_code)
            exc = e

        self.assertNotEqual(exc, None)

    def test_rcs_not_in_list_raise(self):
        exc = None
        try:
            util.subp(self.exit_with_value + [""9""], rcs=[""8"", ""0""])
        except util.ProcessExecutionError as e:
            self.assertEqual(9, e.exit_code)
            exc = e
        self.assertNotEqual(exc, None)

    def test_rcs_other_than_zero_work(self):
        _out, _err = util.subp(self.exit_with_value + [""9""], rcs=[9])

    def test_subp_respects_decode_false(self):
        (out, err) = util.subp(self.stdin2out, capture=True, decode=False,
                               data=self.utf8_valid)
        self.assertTrue(isinstance(out, self.nodecode_type))
        self.assertTrue(isinstance(err, self.nodecode_type))
        self.assertEqual(out, self.utf8_valid)

    def test_subp_decode_ignore(self):
        # this executes a string that writes invalid utf-8 to stdout
        (out, err) = util.subp(self.printf_cmd('abc\\xaadef'),
                               capture=True, decode='ignore')
        self.assertTrue(isinstance(out, self.decode_type))
        self.assertTrue(isinstance(err, self.decode_type))
        self.assertEqual(out, 'abcdef')

    def test_subp_decode_strict_valid_utf8(self):
        (out, err) = util.subp(self.stdin2out, capture=True,
                               decode='strict', data=self.utf8_valid)
        self.assertEqual(out, self.utf8_valid.decode('utf-8'))
        self.assertTrue(isinstance(out, self.decode_type))
        self.assertTrue(isinstance(err, self.decode_type))

    def test_subp_decode_invalid_utf8_replaces(self):
        (out, err) = util.subp(self.stdin2out, capture=True,
                               data=self.utf8_invalid)
        expected = self.utf8_invalid.decode('utf-8', errors='replace')
        self.assertTrue(isinstance(out, self.decode_type))
        self.assertTrue(isinstance(err, self.decode_type))
        self.assertEqual(out, expected)

    def test_subp_decode_strict_raises(self):
        args = []
        kwargs = {'args': self.stdin2out, 'capture': True,
                  'decode': 'strict', 'data': self.utf8_invalid}
        self.assertRaises(UnicodeDecodeError, util.subp, *args, **kwargs)

    def test_subp_capture_stderr(self):
        data = b'hello world'
        (out, err) = util.subp(self.stdin2err, capture=True,
                               decode=False, data=data)
        self.assertEqual(err, data)
        self.assertEqual(out, b'')

    def test_subp_combined_stderr_stdout(self):
        """"""Providing combine_capture as True redirects stderr to stdout.""""""
        data = b'hello world'
        (out, err) = util.subp(self.stdin2err, combine_capture=True,
                               decode=False, data=data)
        self.assertIsNone(err)
        self.assertEqual(out, data)

    def test_returns_none_if_no_capture(self):
        (out, err) = util.subp(self.stdin2out, data=b'')
        self.assertEqual(err, None)
        self.assertEqual(out, None)

    def _subp_wrap_popen(self, cmd, kwargs,
                         stdout=b'', stderr=b'', returncodes=None):
        # mocks the subprocess.Popen as expected from subp
        # checks that subp returned the output of 'communicate' and
        # returns the (args, kwargs) that Popen() was called with.
        # returncodes is a list to cover, one for each expected call

        if returncodes is None:
            returncodes = [0]

        capture = kwargs.get('capture')

        mreturncodes = mock.PropertyMock(side_effect=iter(returncodes))
        with mock.patch(""curtin.util.subprocess.Popen"") as m_popen:
            sp = mock.Mock()
            m_popen.return_value = sp
            if capture:
                sp.communicate.return_value = (stdout, stderr)
            else:
                sp.communicate.return_value = (None, None)
            type(sp).returncode = mreturncodes
            ret = util.subp(cmd, **kwargs)

        # popen may be called once or > 1 for retries, but must be called.
        self.assertTrue(m_popen.called)
        # communicate() needs to have been called.
        self.assertTrue(sp.communicate.called)

        if capture:
            # capture response is decoded if decode is not False
            decode = kwargs.get('decode', ""replace"")
            if decode is False:
                self.assertEqual(stdout.decode(stdout, stderr), ret)
            else:
                self.assertEqual((stdout.decode(errors=decode),
                                  stderr.decode(errors=decode)), ret)
        else:
            # if capture is false, then return is None, None
            self.assertEqual((None, None), ret)

        # if target is not provided or is /, chroot should not be used
        calls = m_popen.call_args_list
        popen_args, popen_kwargs = calls[-1]
        target = util.target_path(kwargs.get('target', None))
        unshcmd = self.mock_get_unshare_pid_args.return_value
        if target == ""/"":
            self.assertEqual(unshcmd + list(cmd), popen_args[0])
        else:
            self.assertEqual(unshcmd + ['chroot', target] + list(cmd),
                             popen_args[0])
        return calls

    def test_args_can_be_a_tuple(self):
        """"""subp can take a tuple for cmd rather than a list.""""""
        my_cmd = tuple(['echo', 'hi', 'mom'])
        calls = self._subp_wrap_popen(my_cmd, {})
        args, kwargs = calls[0]
        # subp was called with cmd as a tuple.  That may get converted to
        # a list before subprocess.popen.  So only compare as lists.
        self.assertEqual(1, len(calls))
        self.assertEqual(list(my_cmd), list(args[0]))

    def test_args_can_be_a_string(self):
        """"""subp(""cat"") is acceptable, as suprocess.call(""cat"") works fine.""""""
        out, err = util.subp(""cat"", data=b'hi mom', capture=True, decode=False)
        self.assertEqual(b'hi mom', out)

    def test_with_target_gets_chroot(self):
        args, kwargs = self._subp_wrap_popen([""my-command""],
                                             {'target': ""/mytarget""})[0]
        self.assertIn('chroot', args[0])

    def test_with_target_as_slash_does_not_chroot(self):
        args, kwargs = self._subp_wrap_popen(
            ['whatever'], {'capture': True, 'target': ""/""})[0]
        self.assertNotIn('chroot', args[0])

    def test_with_no_target_does_not_chroot(self):
        r = self._subp_wrap_popen(['whatever'], {'capture': True})
        args, kwargs = r[0]
        # note this path is reasonably tested with all of the above
        # tests that do not mock Popen as if we did try to chroot the
        # unit tests would fail unless they were run as root.
        self.assertNotIn('chroot', args[0])

    def test_retry_none_does_not_retry(self):
        rcfail = 7
        try:
            self._subp_wrap_popen(
                ['succeeds-second-time'], {'capture': True, 'retries': None},
                returncodes=[rcfail, 0])
            raise Exception(""did not raise a ProcessExecutionError!"")
        except util.ProcessExecutionError as e:
            self.assertEqual(e.exit_code, rcfail)

    def test_retry_does_retry(self):
        # test subp with retries does retry
        rcs = [7, 8, 9, 0]
        # these are our very short sleeps
        retries = [0] * len(rcs)
        r = self._subp_wrap_popen(
            ['succeeds-eventually'], {'capture': True, 'retries': retries},
            returncodes=rcs)
        # r is a list of all args, kwargs to Popen that happend.
        # since we fail a few times, it needs to have been called again.
        self.assertEqual(len(r), len(rcs))

    def test_unshare_pid_return_is_used(self):
        """"""The return of _get_unshare_pid_return needs to be in command.""""""
        my_unshare_cmd = ['do-unshare-command', 'arg0', 'arg1', '--']
        self.mock_get_unshare_pid_args.return_value = my_unshare_cmd
        my_kwargs = {'target': '/target', 'unshare_pid': True}
        r = self._subp_wrap_popen(['apt-get', 'install'], my_kwargs)
        self.assertEqual(1, len(r))
        args, kwargs = r[0]
        self.assertEqual(
            [mock.call(my_kwargs['unshare_pid'], my_kwargs['target'])],
            self.mock_get_unshare_pid_args.call_args_list)
        expected = (my_unshare_cmd + ['chroot', '/target'] +
                    ['apt-get', 'install'])
        self.assertEqual(expected, args[0])


class TestGetUnsharePidArgs(CiTestCase):
    """"""Test the internal implementation for when to unshare.""""""

    def setUp(self):
        super(TestGetUnsharePidArgs, self).setUp()
        self.add_patch('curtin.util._has_unshare_pid', 'mock_has_unshare_pid',
                       return_value=True)
        # our trusty tox environment with mock 1.0.1 will stack trace
        # if autospec is not disabled here.
        self.add_patch('curtin.util.os.geteuid', 'mock_geteuid',
                       autospec=False, return_value=0)

    def assertOff(self, result):
        self.assertEqual([], result)

    def assertOn(self, result):
        self.assertEqual(['unshare', '--fork', '--pid', '--'], result)

    def test_unshare_pid_none_and_not_root_means_off(self):
        """"""If not root, then expect off.""""""
        self.assertOff(util._get_unshare_pid_args(None, ""/foo"", 500))
        self.assertOff(util._get_unshare_pid_args(None, ""/"", 500))

        self.mock_geteuid.return_value = 500
        self.assertOff(util._get_unshare_pid_args(None, ""/""))
        self.assertOff(
            util._get_unshare_pid_args(unshare_pid=None, target=""/foo""))

    def test_unshare_pid_none_and_no_unshare_pid_means_off(self):
        """"""No unshare support and unshare_pid is None means off.""""""
        self.mock_has_unshare_pid.return_value = False
        self.assertOff(util._get_unshare_pid_args(None, ""/target"", 0))

    def test_unshare_pid_true_and_no_unshare_pid_raises(self):
        """"""Passing unshare_pid in as True and no command should raise.""""""
        self.mock_has_unshare_pid.return_value = False
        expected_msg = 'no unshare command'
        with self.assertRaisesRegexp(RuntimeError, expected_msg):
            util._get_unshare_pid_args(True)

        with self.assertRaisesRegexp(RuntimeError, expected_msg):
            util._get_unshare_pid_args(True, ""/foo"", 0)

    def test_unshare_pid_true_and_not_root_raises(self):
        """"""When unshare_pid is True for non-root an error is raised.""""""
        expected_msg = 'euid.* != 0'
        with self.assertRaisesRegexp(RuntimeError, expected_msg):
            util._get_unshare_pid_args(True, ""/foo"", 500)

        self.mock_geteuid.return_value = 500
        with self.assertRaisesRegexp(RuntimeError, expected_msg):
            util._get_unshare_pid_args(True)

    def test_euid0_target_not_slash(self):
        """"""If root and target is not /, then expect on.""""""
        self.assertOn(util._get_unshare_pid_args(None, target=""/foo"", euid=0))

    def test_euid0_target_slash(self):
        """"""If root and target is /, then expect off.""""""
        self.assertOff(util._get_unshare_pid_args(None, ""/"", 0))
        self.assertOff(util._get_unshare_pid_args(None, target=None, euid=0))

    def test_unshare_pid_of_false_means_off(self):
        """"""Any unshare_pid value false-ish other than None means no unshare.""""""
        self.assertOff(
            util._get_unshare_pid_args(unshare_pid=False, target=None))
        self.assertOff(util._get_unshare_pid_args(False, ""/target"", 1))
        self.assertOff(util._get_unshare_pid_args(False, ""/"", 0))
        self.assertOff(util._get_unshare_pid_args("""", ""/target"", 0))


class TestHuman2Bytes(CiTestCase):
    GB = 1024 * 1024 * 1024
    MB = 1024 * 1024

    def test_float_equal_int_is_allowed(self):
        self.assertEqual(1000, util.human2bytes(1000.0))

    def test_float_in_string_nonequal_int_raises_type_error(self):
        self.assertRaises(ValueError, util.human2bytes, ""1000.4B"")

    def test_float_nonequal_int_raises_type_error(self):
        self.assertRaises(ValueError, util.human2bytes, 1000.4)

    def test_int_gets_int(self):
        self.assertEqual(100, util.human2bytes(100))

    def test_no_suffix_is_bytes(self):
        self.assertEqual(100, util.human2bytes(""100""))

    def test_suffix_M(self):
        self.assertEqual(100 * self.MB, util.human2bytes(""100M""))

    def test_suffix_B(self):
        self.assertEqual(100, util.human2bytes(""100B""))

    def test_suffix_G(self):
        self.assertEqual(int(10 * self.GB), util.human2bytes(""10G""))

    def test_float_in_string(self):
        self.assertEqual(int(3.5 * self.GB), util.human2bytes(""3.5G""))

    def test_GB_equals_G(self):
        self.assertEqual(util.human2bytes(""3GB""), util.human2bytes(""3G""))

    def test_b2h_errors(self):
        self.assertRaises(ValueError, util.bytes2human, 10.4)
        self.assertRaises(ValueError, util.bytes2human, 'notint')
        self.assertRaises(ValueError, util.bytes2human, -1)
        self.assertRaises(ValueError, util.bytes2human, -1.0)

    def test_b2h_values(self):
        self.assertEqual('10G', util.bytes2human(10 * self.GB))
        self.assertEqual('10M', util.bytes2human(10 * self.MB))
        self.assertEqual('1000B', util.bytes2human(1000))
        self.assertEqual('1K', util.bytes2human(1024))
        self.assertEqual('1K', util.bytes2human(1024.0))
        self.assertEqual('1T', util.bytes2human(float(1024 * self.GB)))

    def test_h2b_b2b(self):
        for size_str in ['10G', '20G', '2T', '12K', '1M', '1023K']:
            self.assertEqual(
                util.bytes2human(util.human2bytes(size_str)), size_str)


class TestSetUnExecutable(CiTestCase):
    tmpf = None
    tmpd = None

    def setUp(self):
        super(CiTestCase, self).setUp()
        self.tmpd = self.tmp_dir()

    def test_change_needed_returns_original_mode(self):
        tmpf = self.tmp_path('testfile')
        util.write_file(tmpf, '')
        os.chmod(tmpf, 0o755)
        ret = util.set_unexecutable(tmpf)
        self.assertEqual(ret, 0o0755)

    def test_no_change_needed_returns_none(self):
        tmpf = self.tmp_path('testfile')
        util.write_file(tmpf, '')
        os.chmod(tmpf, 0o600)
        ret = util.set_unexecutable(tmpf)
        self.assertEqual(ret, None)

    def test_change_does_as_expected(self):
        tmpf = self.tmp_path('testfile')
        util.write_file(tmpf, '')
        os.chmod(tmpf, 0o755)
        ret = util.set_unexecutable(tmpf)
        self.assertEqual(ret, 0o0755)
        self.assertEqual(stat.S_IMODE(os.stat(tmpf).st_mode), 0o0644)

    def test_strict_no_exists_raises_exception(self):
        bogus = os.path.join(self.tmpd, 'bogus')
        self.assertRaises(ValueError, util.set_unexecutable, bogus, True)


class TestTargetPath(CiTestCase):
    def test_target_empty_string(self):
        self.assertEqual(""/etc/passwd"", util.target_path("""", ""/etc/passwd""))

    def test_target_non_string_raises(self):
        self.assertRaises(ValueError, util.target_path, False)
        self.assertRaises(ValueError, util.target_path, 9)
        self.assertRaises(ValueError, util.target_path, True)

    def test_lots_of_slashes_is_slash(self):
        self.assertEqual(""/"", util.target_path(""/""))
        self.assertEqual(""/"", util.target_path(""//""))
        self.assertEqual(""/"", util.target_path(""///""))
        self.assertEqual(""/"", util.target_path(""////""))

    def test_empty_string_is_slash(self):
        self.assertEqual(""/"", util.target_path(""""))

    def test_recognizes_relative(self):
        self.assertEqual(""/"", util.target_path(""/foo/../""))
        self.assertEqual(""/"", util.target_path(""/foo//bar/../../""))

    def test_no_path(self):
        self.assertEqual(""/my/target"", util.target_path(""/my/target""))

    def test_no_target_no_path(self):
        self.assertEqual(""/"", util.target_path(None))

    def test_no_target_with_path(self):
        self.assertEqual(""/my/path"", util.target_path(None, ""/my/path""))

    def test_trailing_slash(self):
        self.assertEqual(""/my/target/my/path"",
                         util.target_path(""/my/target/"", ""/my/path""))

    def test_bunch_of_slashes_in_path(self):
        self.assertEqual(""/target/my/path/"",
                         util.target_path(""/target/"", ""//my/path/""))
        self.assertEqual(""/target/my/path/"",
                         util.target_path(""/target/"", ""///my/path/""))


class TestRunInChroot(CiTestCase):
    """"""Test the legacy 'RunInChroot'.

    The test works by mocking ChrootableTarget's __enter__ to do nothing.
    The assumptions made are:
      a.) RunInChroot is a subclass of ChrootableTarget
      b.) ChrootableTarget's __exit__ only un-does work that its __enter__
          did.  Meaning for our mocked case, it does nothing.""""""

    @mock.patch.object(util.ChrootableTarget, ""__enter__"", new=lambda a: a)
    def test_run_in_chroot_with_target_slash(self):
        with util.RunInChroot(""/"") as i:
            out, err = i(['echo', 'HI MOM'], capture=True)
        self.assertEqual('HI MOM\n', out)

    @mock.patch.object(util.ChrootableTarget, ""__enter__"", new=lambda a: a)
    @mock.patch(""curtin.util.subp"")
    def test_run_in_chroot_with_target(self, m_subp):
        my_stdout = ""my output""
        my_stderr = ""my stderr""
        cmd = ['echo', 'HI MOM']
        target = ""/foo""
        m_subp.return_value = (my_stdout, my_stderr)
        with util.RunInChroot(target) as i:
            out, err = i(cmd)
        self.assertEqual(my_stdout, out)
        self.assertEqual(my_stderr, err)
        m_subp.assert_called_with(cmd, target=target)


class TestLoadFile(CiTestCase):
    """"""Test utility 'load_file'""""""

    def test_load_file_simple(self):
        fname = 'test.cfg'
        contents = ""#curtin-config""
        with simple_mocked_open(content=contents) as m_open:
            loaded_contents = util.load_file(fname, decode=False)
            self.assertEqual(contents, loaded_contents)
            m_open.assert_called_with(fname, 'rb')

    @skipIf(mock.__version__ < '2.0.0', ""mock version < 2.0.0"")
    def test_load_file_handles_utf8(self):
        fname = 'test.cfg'
        contents = b'd\xc3\xa9j\xc8\xa7'
        with simple_mocked_open(content=contents) as m_open:
            with open(fname, 'rb') as f:
                self.assertEqual(f.read(), contents)
            m_open.assert_called_with(fname, 'rb')

    @skipIf(mock.__version__ < '2.0.0', ""mock version < 2.0.0"")
    @mock.patch('curtin.util.decode_binary')
    def test_load_file_respects_decode_false(self, mock_decode):
        fname = 'test.cfg'
        contents = b'start \xc3\xa9 end'
        with simple_mocked_open(contents):
            loaded_contents = util.load_file(fname, decode=False)
            self.assertEqual(type(loaded_contents), bytes)
            self.assertEqual(loaded_contents, contents)


class TestIpAddress(CiTestCase):
    """"""Test utility 'is_valid_ip{,v4,v6}_address'""""""

    def test_is_valid_ipv6_address(self):
        self.assertFalse(util.is_valid_ipv6_address('192.168'))
        self.assertFalse(util.is_valid_ipv6_address('69.89.31.226'))
        self.assertFalse(util.is_valid_ipv6_address('254.254.254.254'))
        self.assertTrue(util.is_valid_ipv6_address('2001:db8::1'))
        self.assertTrue(util.is_valid_ipv6_address('::1'))
        self.assertTrue(util.is_valid_ipv6_address(
            '1200:0000:AB00:1234:0000:2552:7777:1313'))
        self.assertFalse(util.is_valid_ipv6_address(
            '1200::AB00:1234::2552:7777:1313'))
        self.assertTrue(util.is_valid_ipv6_address(
            '21DA:D3:0:2F3B:2AA:FF:FE28:9C5A'))
        self.assertFalse(util.is_valid_ipv6_address(
            '1200:0000:AB00:1234:O000:2552:7777:1313'))
        self.assertTrue(util.is_valid_ipv6_address(
            '2002:4559:1FE2::4559:1FE2'))
        self.assertTrue(util.is_valid_ipv6_address(
            '2002:4559:1fe2:0:0:0:4559:1fe2'))
        self.assertTrue(util.is_valid_ipv6_address(
            '2002:4559:1FE2:0000:0000:0000:4559:1FE2'))


class TestLoadCommandEnvironment(CiTestCase):

    def setUp(self):
        super(TestLoadCommandEnvironment, self).setUp()
        self.tmpd = self.tmp_dir()
        all_names = {
            'CONFIG',
            'OUTPUT_FSTAB',
            'OUTPUT_INTERFACES',
            'OUTPUT_NETWORK_CONFIG',
            'OUTPUT_NETWORK_STATE',
            'CURTIN_REPORTSTACK',
            'WORKING_DIR',
            'TARGET_MOUNT_POINT',
        }
        self.full_env = {v: os.path.join(self.tmpd, v.lower())
                         for v in all_names}

    def test_strict_with_missing(self):
        my_env = self.full_env.copy()
        del my_env['OUTPUT_FSTAB']
        del my_env['WORKING_DIR']
        exc = None
        try:
            util.load_command_environment(my_env, strict=True)
        except KeyError as e:
            self.assertIn(""OUTPUT_FSTAB"", str(e))
            self.assertIn(""WORKING_DIR"", str(e))
            exc = e

        self.assertTrue(exc)

    def test_nostrict_with_missing(self):
        my_env = self.full_env.copy()
        del my_env['OUTPUT_FSTAB']
        try:
            util.load_command_environment(my_env, strict=False)
        except KeyError as e:
            self.fail(""unexpected key error raised: %s"" % e)

    def test_full_and_strict(self):
        try:
            util.load_command_environment(self.full_env, strict=False)
        except KeyError as e:
            self.fail(""unexpected key error raised: %s"" % e)


class TestWaitForRemoval(CiTestCase):
    def test_wait_for_removal_missing_path(self):
        with self.assertRaises(ValueError):
            util.wait_for_removal(None)

    @mock.patch('curtin.util.time')
    @mock.patch('curtin.util.os')
    def test_wait_for_removal(self, mock_os, mock_time):
        path = ""/file/to/remove""
        mock_os.path.exists.side_effect = iter([
            True,    # File is not yet removed
            False,   # File has  been removed
        ])

        util.wait_for_removal(path)

        self.assertEqual(2, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_time.sleep.call_args_list))
        mock_os.path.exists.assert_has_calls([
            mock.call(path),
            mock.call(path),
        ])
        mock_time.sleep.assert_has_calls([
            mock.call(1),
        ])

    @mock.patch('curtin.util.time')
    @mock.patch('curtin.util.os')
    def test_wait_for_removal_timesout(self, mock_os, mock_time):
        path = ""/file/to/remove""
        mock_os.path.exists.return_value = True

        with self.assertRaises(OSError):
            util.wait_for_removal(path)

        self.assertEqual(5, len(mock_os.path.exists.call_args_list))
        self.assertEqual(4, len(mock_time.sleep.call_args_list))
        mock_os.path.exists.assert_has_calls(5 * [mock.call(path)])
        mock_time.sleep.assert_has_calls([
            mock.call(1),
            mock.call(3),
            mock.call(5),
            mock.call(7),
        ])

    @mock.patch('curtin.util.time')
    @mock.patch('curtin.util.os')
    def test_wait_for_removal_custom_retry(self, mock_os, mock_time):
        path = ""/file/to/remove""
        timeout = 100
        mock_os.path.exists.side_effect = iter([
            True,    # File is not yet removed
            False,   # File has  been removed
        ])

        util.wait_for_removal(path, retries=[timeout])

        self.assertEqual(2, len(mock_os.path.exists.call_args_list))
        self.assertEqual(1, len(mock_time.sleep.call_args_list))
        mock_os.path.exists.assert_has_calls([
            mock.call(path),
            mock.call(path),
        ])
        mock_time.sleep.assert_has_calls([
            mock.call(timeout),
        ])


class TestGetEFIBootMGR(CiTestCase):

    def setUp(self):
        super(TestGetEFIBootMGR, self).setUp()
        self.add_patch(
            'curtin.util.ChrootableTarget', 'mock_chroot', autospec=False)
        self.mock_in_chroot = mock.MagicMock()
        self.mock_in_chroot.__enter__.return_value = self.mock_in_chroot
        self.in_chroot_subp_output = []
        self.mock_in_chroot_subp = self.mock_in_chroot.subp
        self.mock_in_chroot_subp.side_effect = self.in_chroot_subp_output
        self.mock_chroot.return_value = self.mock_in_chroot

    def test_calls_efibootmgr_verbose(self):
        self.in_chroot_subp_output.append(('', ''))
        util.get_efibootmgr('target')
        self.assertEquals(
            (['efibootmgr', '-v'],),
            self.mock_in_chroot_subp.call_args_list[0][0])

    def test_parses_output(self):
        self.in_chroot_subp_output.append((dedent(
            """"""\
            BootCurrent: 0000
            Timeout: 1 seconds
            BootOrder: 0000,0002,0001,0003,0004,0005
            Boot0000* ubuntu	HD(1,GPT)/File(\\EFI\\ubuntu\\shimx64.efi)
            Boot0001* CD/DVD Drive 	BBS(CDROM,,0x0)
            Boot0002* Hard Drive 	BBS(HD,,0x0)
            Boot0003* UEFI:CD/DVD Drive	BBS(129,,0x0)
            Boot0004* UEFI:Removable Device	BBS(130,,0x0)
            Boot0005* UEFI:Network Device	BBS(131,,0x0)
            """"""), ''))
        observed = util.get_efibootmgr('target')
        self.assertEquals({
            'current': '0000',
            'timeout': '1 seconds',
            'order': ['0000', '0002', '0001', '0003', '0004', '0005'],
            'entries': {
                '0000': {
                    'name': 'ubuntu',
                    'path': 'HD(1,GPT)/File(\\EFI\\ubuntu\\shimx64.efi)',
                },
                '0001': {
                    'name': 'CD/DVD Drive',
                    'path': 'BBS(CDROM,,0x0)',
                },
                '0002': {
                    'name': 'Hard Drive',
                    'path': 'BBS(HD,,0x0)',
                },
                '0003': {
                    'name': 'UEFI:CD/DVD Drive',
                    'path': 'BBS(129,,0x0)',
                },
                '0004': {
                    'name': 'UEFI:Removable Device',
                    'path': 'BBS(130,,0x0)',
                },
                '0005': {
                    'name': 'UEFI:Network Device',
                    'path': 'BBS(131,,0x0)',
                },
            }
        }, observed)


class TestUsesSystemd(CiTestCase):

    def setUp(self):
        super(TestUsesSystemd, self).setUp()
        self._reset_cache()
        self.add_patch('curtin.util.os.path.isdir', 'mock_isdir')

    def _reset_cache(self):
        util._USES_SYSTEMD = None

    def test_uses_systemd_on_systemd(self):
        """""" Test that uses_systemd returns True if sdpath is a dir """"""
        # systemd_enabled
        self.mock_isdir.return_value = True
        result = util.uses_systemd()
        self.assertEqual(True, result)
        self.assertEqual(1, len(self.mock_isdir.call_args_list))

    def test_uses_systemd_cached(self):
        """"""Test that we cache the uses_systemd result""""""

        # reset_cache should ensure it's unset
        self.assertEqual(None, util._USES_SYSTEMD)

        # systemd enabled
        self.mock_isdir.return_value = True

        # first time
        first_result = util.uses_systemd()

        # check the cache value
        self.assertEqual(first_result, util._USES_SYSTEMD)

        # second time
        second_result = util.uses_systemd()

        # results should match between tries
        self.assertEqual(True, first_result)
        self.assertEqual(True, second_result)

        # isdir should only be called once
        self.assertEqual(1, len(self.mock_isdir.call_args_list))

    def test_uses_systemd_on_non_systemd(self):
        """""" Test that uses_systemd returns False if sdpath is not a dir """"""
        # systemd not available
        self.mock_isdir.return_value = False
        result = util.uses_systemd()
        self.assertEqual(False, result)


class TestIsKmodLoaded(CiTestCase):

    def setUp(self):
        super(TestIsKmodLoaded, self).setUp()
        self.add_patch('curtin.util.os.path.isdir', 'm_path_isdir')
        self.modname = 'fake_module'

    def test_is_kmod_loaded_invalid_module(self):
        """"""test raise ValueError on invalid module parameter""""""
        for module_name in ['', None]:
            with self.assertRaises(ValueError):
                util.is_kmod_loaded(module_name)

    def test_is_kmod_loaded_path_checked(self):
        """""" test /sys/modules/<modname> path is checked """"""
        util.is_kmod_loaded(self.modname)
        self.m_path_isdir.assert_called_with('/sys/module/%s' % self.modname)

    def test_is_kmod_loaded_already_loaded(self):
        """""" test returns True if /sys/module/modname exists """"""
        self.m_path_isdir.return_value = True
        is_loaded = util.is_kmod_loaded(self.modname)
        self.assertTrue(is_loaded)
        self.m_path_isdir.assert_called_with('/sys/module/%s' % self.modname)

    def test_is_kmod_loaded_not_loaded(self):
        """""" test returns False if /sys/module/modname does not exist """"""
        self.m_path_isdir.return_value = False
        is_loaded = util.is_kmod_loaded(self.modname)
        self.assertFalse(is_loaded)
        self.m_path_isdir.assert_called_with('/sys/module/%s' % self.modname)


class TestLoadKernelModule(CiTestCase):

    def setUp(self):
        super(TestLoadKernelModule, self).setUp()
        self.add_patch('curtin.util.is_kmod_loaded', 'm_is_kmod_loaded')
        self.add_patch('curtin.util.subp', 'm_subp')
        self.modname = 'fake_module'

    def test_load_kernel_module_invalid_module(self):
        """""" test raise ValueError on invalid module parameter""""""
        for module_name in ['', None]:
            with self.assertRaises(ValueError):
                util.load_kernel_module(module_name)

    def test_load_kernel_module_unloaded(self):
        """""" test unloaded kmod is loaded via call to modprobe""""""
        self.m_is_kmod_loaded.return_value = False

        util.load_kernel_module(self.modname)

        self.m_is_kmod_loaded.assert_called_with(self.modname)
        self.m_subp.assert_called_with(['modprobe', '--use-blacklist',
                                        self.modname])

    def test_load_kernel_module_loaded(self):
        """""" test modprobe called with check_loaded=False""""""
        self.m_is_kmod_loaded.return_value = True
        util.load_kernel_module(self.modname, check_loaded=False)

        self.assertEqual(0, self.m_is_kmod_loaded.call_count)
        self.m_subp.assert_called_with(['modprobe', '--use-blacklist',
                                        self.modname])

    def test_load_kernel_module_skips_modprobe_if_loaded(self):
        """""" test modprobe skipped if module already loaded""""""
        self.m_is_kmod_loaded.return_value = True
        util.load_kernel_module(self.modname)

        self.assertEqual(1, self.m_is_kmod_loaded.call_count)
        self.m_is_kmod_loaded.assert_called_with(self.modname)
        self.assertEqual(0, self.m_subp.call_count)


# vi: ts=4 expandtab syntax=python
/n/n/ntests/vmtests/test_zfsroot.py/n/nfrom . import VMBaseClass
from .releases import base_vm_classes as relbase

import textwrap


class TestZfsRootAbs(VMBaseClass):
    interactive = False
    nr_cpus = 2
    dirty_disks = True
    conf_file = ""examples/tests/zfsroot.yaml""
    extra_disks = []
    collect_scripts = VMBaseClass.collect_scripts + [
        textwrap.dedent(""""""
            cd OUTPUT_COLLECT_D
            blkid -o export /dev/vda > blkid_output_vda
            blkid -o export /dev/vda1 > blkid_output_vda1
            blkid -o export /dev/vda2 > blkid_output_vda2
            zfs list > zfs_list
            zpool list > zpool_list
            zpool status > zpool_status
            cat /proc/partitions > proc_partitions
            cat /proc/mounts > proc_mounts
            cat /proc/cmdline > proc_cmdline
            ls -al /dev/disk/by-uuid/ > ls_uuid
            cat /etc/fstab > fstab
            mkdir -p /dev/disk/by-dname
            ls /dev/disk/by-dname/ > ls_dname
            find /etc/network/interfaces.d > find_interfacesd
            v=""""
            out=$(apt-config shell v Acquire::HTTP::Proxy)
            eval ""$out""
            echo ""$v"" > apt-proxy
            cp /etc/environment etc_environment
        """""")]

    def test_output_files_exist(self):
        self.output_files_exist(
            [""blkid_output_vda"", ""blkid_output_vda1"", ""blkid_output_vda2"",
             ""fstab"", ""ls_dname"", ""ls_uuid"",
             ""proc_partitions"",
             ""root/curtin-install.log"", ""root/curtin-install-cfg.yaml""])

    def test_ptable(self):
        blkid_info = self.get_blkid_data(""blkid_output_vda"")
        self.assertEquals(blkid_info[""PTTYPE""], ""gpt"")

    def test_zfs_list(self):
        """"""Check rpoot/ROOT/ubuntu is mounted at slash""""""
        self.output_files_exist(['zfs_list'])
        self.check_file_regex('zfs_list', r""rpool/ROOT/ubuntu.*/\n"")

    def test_env_has_zpool_vdev_name_path(self):
        """"""Target env has ZPOOL_VDEV_NAME_PATH=1 set""""""
        self.output_files_exist(['etc_environment'])
        self.check_file_regex('etc_environment', r'ZPOOL_VDEV_NAME_PATH=""1""')

    def test_proc_cmdline_has_root_zfs(self):
        """"""Check /proc/cmdline has root=ZFS=<pool>""""""
        self.output_files_exist(['proc_cmdline'])
        self.check_file_regex('proc_cmdline', r""root=ZFS=rpool/ROOT/ubuntu"")


class XenialGATestZfsRoot(relbase.xenial_ga, TestZfsRootAbs):
    __test__ = True


class XenialHWETestZfsRoot(relbase.xenial_hwe, TestZfsRootAbs):
    __test__ = True


class XenialEdgeTestZfsRoot(relbase.xenial_edge, TestZfsRootAbs):
    __test__ = True


class ArtfulTestZfsRoot(relbase.artful, TestZfsRootAbs):
    __test__ = True


class BionicTestZfsRoot(relbase.bionic, TestZfsRootAbs):
    __test__ = True
/n/n/n",0
39,39,f4ac975d776092dfe881ec63fd398c1ab851365b,"/curtin/block/clear_holders.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

""""""
This module provides a mechanism for shutting down virtual storage layers on
top of a block device, making it possible to reuse the block device without
having to reboot the system
""""""

import errno
import os
import time

from curtin import (block, udev, util)
from curtin.block import lvm
from curtin.block import mdadm
from curtin.log import LOG

# poll frequenty, but wait up to 60 seconds total
MDADM_RELEASE_RETRIES = [0.4] * 150


def _define_handlers_registry():
    """"""
    returns instantiated dev_types
    """"""
    return {
        'partition': {'shutdown': wipe_superblock,
                      'ident': identify_partition},
        'lvm': {'shutdown': shutdown_lvm, 'ident': identify_lvm},
        'crypt': {'shutdown': shutdown_crypt, 'ident': identify_crypt},
        'raid': {'shutdown': shutdown_mdadm, 'ident': identify_mdadm},
        'bcache': {'shutdown': shutdown_bcache, 'ident': identify_bcache},
        'disk': {'ident': lambda x: False, 'shutdown': wipe_superblock},
    }


def get_dmsetup_uuid(device):
    """"""
    get the dm uuid for a specified dmsetup device
    """"""
    blockdev = block.sysfs_to_devpath(device)
    (out, _) = util.subp(['dmsetup', 'info', blockdev, '-C', '-o', 'uuid',
                          '--noheadings'], capture=True)
    return out.strip()


def get_bcache_using_dev(device, strict=True):
    """"""
    Get the /sys/fs/bcache/ path of the bcache cache device bound to
    specified device
    """"""
    # FIXME: when block.bcache is written this should be moved there
    sysfs_path = block.sys_block_path(device)
    path = os.path.realpath(os.path.join(sysfs_path, 'bcache', 'cache'))
    if strict and not os.path.exists(path):
        err = OSError(
            ""device '{}' did not have existing syspath '{}'"".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def get_bcache_sys_path(device, strict=True):
    """"""
    Get the /sys/class/block/<device>/bcache path
    """"""
    sysfs_path = block.sys_block_path(device, strict=strict)
    path = os.path.join(sysfs_path, 'bcache')
    if strict and not os.path.exists(path):
        err = OSError(
            ""device '{}' did not have existing syspath '{}'"".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def maybe_stop_bcache_device(device):
    """"""Attempt to stop the provided device_path or raise unexpected errors.""""""
    bcache_stop = os.path.join(device, 'stop')
    try:
        util.write_file(bcache_stop, '1', mode=None)
    except (IOError, OSError) as e:
        # Note: if we get any exceptions in the above exception classes
        # it is a result of attempting to write ""1"" into the sysfs path
        # The range of errors changes depending on when we race with
        # the kernel asynchronously removing the sysfs path. Therefore
        # we log the exception errno we got, but do not re-raise as
        # the calling process is watching whether the same sysfs path
        # is being removed;  if it fails to go away then we'll have
        # a log of the exceptions to debug.
        LOG.debug('Error writing to bcache stop file %s, device removed: %s',
                  bcache_stop, e)


def shutdown_bcache(device):
    """"""
    Shut down bcache for specified bcache device

    1. Stop the cacheset that `device` is connected to
    2. Stop the 'device'
    """"""
    if not device.startswith('/sys/class/block'):
        raise ValueError('Invalid Device (%s): '
                         'Device path must start with /sys/class/block/',
                         device)

    # bcache device removal should be fast but in an extreme
    # case, might require the cache device to flush large
    # amounts of data to a backing device.  The strategy here
    # is to wait for approximately 30 seconds but to check
    # frequently since curtin cannot proceed until devices
    # cleared.
    removal_retries = [0.2] * 150  # 30 seconds total
    bcache_shutdown_message = ('shutdown_bcache running on {} has determined '
                               'that the device has already been shut down '
                               'during handling of another bcache dev. '
                               'skipping'.format(device))

    if not os.path.exists(device):
        LOG.info(bcache_shutdown_message)
        return

    # get slaves [vdb1, vdc], allow for slaves to not have bcache dir
    slave_paths = [get_bcache_sys_path(k, strict=False) for k in
                   os.listdir(os.path.join(device, 'slaves'))]

    # stop cacheset if it exists
    bcache_cache_sysfs = get_bcache_using_dev(device, strict=False)
    if not os.path.exists(bcache_cache_sysfs):
        LOG.info('bcache cacheset already removed: %s',
                 os.path.basename(bcache_cache_sysfs))
    else:
        LOG.info('stopping bcache cacheset at: %s', bcache_cache_sysfs)
        maybe_stop_bcache_device(bcache_cache_sysfs)
        try:
            util.wait_for_removal(bcache_cache_sysfs, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache cacheset %s', bcache_cache_sysfs)
            raise

        # let kernel settle before the next remove
        udev.udevadm_settle()

    # after stopping cache set, we may need to stop the device
    # both the dev and sysfs entry should be gone.

    # we know the bcacheN device is really gone when we've removed:
    #  /sys/class/block/{bcacheN}
    #  /sys/class/block/slaveN1/bcache
    #  /sys/class/block/slaveN2/bcache
    bcache_block_sysfs = get_bcache_sys_path(device, strict=False)
    to_check = [device] + slave_paths
    found_devs = [os.path.exists(p) for p in to_check]
    LOG.debug('os.path.exists on blockdevs:\n%s',
              list(zip(to_check, found_devs)))
    if not any(found_devs):
        LOG.info('bcache backing device already removed: %s (%s)',
                 bcache_block_sysfs, device)
        LOG.debug('bcache slave paths checked: %s', slave_paths)
        return
    else:
        LOG.info('stopping bcache backing device at: %s', bcache_block_sysfs)
        maybe_stop_bcache_device(bcache_block_sysfs)
        try:
            # wait for them all to go away
            for dev in [device, bcache_block_sysfs] + slave_paths:
                util.wait_for_removal(dev, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache backing device %s',
                     bcache_block_sysfs)
            raise

    return


def shutdown_lvm(device):
    """"""
    Shutdown specified lvm device.
    """"""
    device = block.sys_block_path(device)
    # lvm devices have a dm directory that containes a file 'name' containing
    # '{volume group}-{logical volume}'. The volume can be freed using lvremove
    name_file = os.path.join(device, 'dm', 'name')
    (vg_name, lv_name) = lvm.split_lvm_name(util.load_file(name_file))
    # use two --force flags here in case the volume group that this lv is
    # attached two has been damaged
    LOG.debug('running lvremove on %s/%s', vg_name, lv_name)
    util.subp(['lvremove', '--force', '--force',
               '{}/{}'.format(vg_name, lv_name)], rcs=[0, 5])
    # if that was the last lvol in the volgroup, get rid of volgroup
    if len(lvm.get_lvols_in_volgroup(vg_name)) == 0:
        util.subp(['vgremove', '--force', '--force', vg_name], rcs=[0, 5])
    # refresh lvmetad
    lvm.lvm_scan()


def shutdown_crypt(device):
    """"""
    Shutdown specified cryptsetup device
    """"""
    blockdev = block.sysfs_to_devpath(device)
    util.subp(['cryptsetup', 'remove', blockdev], capture=True)


def shutdown_mdadm(device):
    """"""
    Shutdown specified mdadm device.
    """"""
    blockdev = block.sysfs_to_devpath(device)
    LOG.debug('using mdadm.mdadm_stop on dev: %s', blockdev)
    mdadm.mdadm_stop(blockdev)

    # mdadm stop operation is asynchronous so we must wait for the kernel to
    # release resources. For more details see  LP: #1682456
    try:
        for wait in MDADM_RELEASE_RETRIES:
            if mdadm.md_present(block.path_to_kname(blockdev)):
                time.sleep(wait)
            else:
                LOG.debug('%s has been removed', blockdev)
                break

        if mdadm.md_present(block.path_to_kname(blockdev)):
            raise OSError('Timeout exceeded for removal of %s', blockdev)

    except OSError:
        LOG.critical('Failed to stop mdadm device %s', device)
        if os.path.exists('/proc/mdstat'):
            LOG.critical(""/proc/mdstat:\n%s"", util.load_file('/proc/mdstat'))
        raise


def wipe_superblock(device):
    """"""
    Wrapper for block.wipe_volume compatible with shutdown function interface
    """"""
    blockdev = block.sysfs_to_devpath(device)
    # when operating on a disk that used to have a dos part table with an
    # extended partition, attempting to wipe the extended partition will fail
    if block.is_extended_partition(blockdev):
        LOG.info(""extended partitions do not need wiping, so skipping: '%s'"",
                 blockdev)
    else:
        # some volumes will be claimed by the bcache layer but do not surface
        # an actual /dev/bcacheN device which owns the parts (backing, cache)
        # The result is that some volumes cannot be wiped while bcache claims
        # the device.  Resolve this by stopping bcache layer on those volumes
        # if present.
        for bcache_path in ['bcache', 'bcache/set']:
            stop_path = os.path.join(device, bcache_path)
            if os.path.exists(stop_path):
                LOG.debug('Attempting to release bcache layer from device: %s',
                          device)
                maybe_stop_bcache_device(stop_path)
                continue

        retries = [1, 3, 5, 7]
        LOG.info('wiping superblock on %s', blockdev)
        for attempt, wait in enumerate(retries):
            LOG.debug('wiping %s attempt %s/%s',
                      blockdev, attempt + 1, len(retries))
            try:
                block.wipe_volume(blockdev, mode='superblock')
                LOG.debug('successfully wiped device %s on attempt %s/%s',
                          blockdev, attempt + 1, len(retries))
                return
            except OSError:
                if attempt + 1 >= len(retries):
                    raise
                else:
                    LOG.debug(""wiping device '%s' failed on attempt""
                              "" %s/%s.  sleeping %ss before retry"",
                              blockdev, attempt + 1, len(retries), wait)
                    time.sleep(wait)


def identify_lvm(device):
    """"""
    determine if specified device is a lvm device
    """"""
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('LVM'))


def identify_crypt(device):
    """"""
    determine if specified device is dm-crypt device
    """"""
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('CRYPT'))


def identify_mdadm(device):
    """"""
    determine if specified device is a mdadm device
    """"""
    return block.path_to_kname(device).startswith('md')


def identify_bcache(device):
    """"""
    determine if specified device is a bcache device
    """"""
    return block.path_to_kname(device).startswith('bcache')


def identify_partition(device):
    """"""
    determine if specified device is a partition
    """"""
    path = os.path.join(block.sys_block_path(device), 'partition')
    return os.path.exists(path)


def get_holders(device):
    """"""
    Look up any block device holders, return list of knames
    """"""
    # block.sys_block_path works when given a /sys or /dev path
    sysfs_path = block.sys_block_path(device)
    # get holders
    holders = os.listdir(os.path.join(sysfs_path, 'holders'))
    LOG.debug(""devname '%s' had holders: %s"", device, holders)
    return holders


def gen_holders_tree(device):
    """"""
    generate a tree representing the current storage hirearchy above 'device'
    """"""
    device = block.sys_block_path(device)
    dev_name = block.path_to_kname(device)
    # the holders for a device should consist of the devices in the holders/
    # dir in sysfs and any partitions on the device. this ensures that a
    # storage tree starting from a disk will include all devices holding the
    # disk's partitions
    holder_paths = ([block.sys_block_path(h) for h in get_holders(device)] +
                    block.get_sysfs_partitions(device))
    # the DEV_TYPE registry contains a function under the key 'ident' for each
    # device type entry that returns true if the device passed to it is of the
    # correct type. there should never be a situation in which multiple
    # identify functions return true. therefore, it will always work to take
    # the device type with the first identify function that returns true as the
    # device type for the current device. in the event that no identify
    # functions return true, the device will be treated as a disk
    # (DEFAULT_DEV_TYPE). the identify function for disk never returns true.
    # the next() builtin in python will not raise a StopIteration exception if
    # there is a default value defined
    dev_type = next((k for k, v in DEV_TYPES.items() if v['ident'](device)),
                    DEFAULT_DEV_TYPE)
    return {
        'device': device, 'dev_type': dev_type, 'name': dev_name,
        'holders': [gen_holders_tree(h) for h in holder_paths],
    }


def plan_shutdown_holder_trees(holders_trees):
    """"""
    plan best order to shut down holders in, taking into account high level
    storage layers that may have many devices below them

    returns a sorted list of descriptions of storage config entries including
    their path in /sys/block and their dev type

    can accept either a single storage tree or a list of storage trees assumed
    to start at an equal place in storage hirearchy (i.e. a list of trees
    starting from disk)
    """"""
    # holds a temporary registry of holders to allow cross references
    # key = device sysfs path, value = {} of priority level, shutdown function
    reg = {}

    # normalize to list of trees
    if not isinstance(holders_trees, (list, tuple)):
        holders_trees = [holders_trees]

    def flatten_holders_tree(tree, level=0):
        """"""
        add entries from holders tree to registry with level key corresponding
        to how many layers from raw disks the current device is at
        """"""
        device = tree['device']

        # always go with highest level if current device has been
        # encountered already. since the device and everything above it is
        # re-added to the registry it ensures that any increase of level
        # required here will propagate down the tree
        # this handles a scenario like mdadm + bcache, where the backing
        # device for bcache is a 3nd level item like mdadm, but the cache
        # device is 1st level (disk) or second level (partition), ensuring
        # that the bcache item is always considered higher level than
        # anything else regardless of whether it was added to the tree via
        # the cache device or backing device first
        if device in reg:
            level = max(reg[device]['level'], level)

        reg[device] = {'level': level, 'device': device,
                       'dev_type': tree['dev_type']}

        # handle holders above this level
        for holder in tree['holders']:
            flatten_holders_tree(holder, level=level + 1)

    # flatten the holders tree into the registry
    for holders_tree in holders_trees:
        flatten_holders_tree(holders_tree)

    # return list of entry dicts with highest level first
    return [reg[k] for k in sorted(reg, key=lambda x: reg[x]['level'] * -1)]


def format_holders_tree(holders_tree):
    """"""
    draw a nice dirgram of the holders tree
    """"""
    # spacer styles based on output of 'tree --charset=ascii'
    spacers = (('`-- ', ' ' * 4), ('|-- ', '|' + ' ' * 3))

    def format_tree(tree):
        """"""
        format entry and any subentries
        """"""
        result = [tree['name']]
        holders = tree['holders']
        for (holder_no, holder) in enumerate(holders):
            spacer_style = spacers[min(len(holders) - (holder_no + 1), 1)]
            subtree_lines = format_tree(holder)
            for (line_no, line) in enumerate(subtree_lines):
                result.append(spacer_style[min(line_no, 1)] + line)
        return result

    return '\n'.join(format_tree(holders_tree))


def get_holder_types(tree):
    """"""
    get flattened list of types of holders in holders tree and the devices
    they correspond to
    """"""
    types = {(tree['dev_type'], tree['device'])}
    for holder in tree['holders']:
        types.update(get_holder_types(holder))
    return types


def assert_clear(base_paths):
    """"""
    Check if all paths in base_paths are clear to use
    """"""
    valid = ('disk', 'partition')
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]
    base_paths = [block.sys_block_path(path) for path in base_paths]
    for holders_tree in [gen_holders_tree(p) for p in base_paths]:
        if any(holder_type not in valid and path not in base_paths
               for (holder_type, path) in get_holder_types(holders_tree)):
            raise OSError('Storage not clear, remaining:\n{}'
                          .format(format_holders_tree(holders_tree)))


def clear_holders(base_paths, try_preserve=False):
    """"""
    Clear all storage layers depending on the devices specified in 'base_paths'
    A single device or list of devices can be specified.
    Device paths can be specified either as paths in /dev or /sys/block
    Will throw OSError if any holders could not be shut down
    """"""
    # handle single path
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]

    # get current holders and plan how to shut them down
    holder_trees = [gen_holders_tree(path) for path in base_paths]
    LOG.info('Current device storage tree:\n%s',
             '\n'.join(format_holders_tree(tree) for tree in holder_trees))
    ordered_devs = plan_shutdown_holder_trees(holder_trees)

    # run shutdown functions
    for dev_info in ordered_devs:
        dev_type = DEV_TYPES.get(dev_info['dev_type'])
        shutdown_function = dev_type.get('shutdown')
        if not shutdown_function:
            continue
        if try_preserve and shutdown_function in DATA_DESTROYING_HANDLERS:
            LOG.info('shutdown function for holder type: %s is destructive. '
                     'attempting to preserve data, so not skipping' %
                     dev_info['dev_type'])
            continue
        LOG.info(""shutdown running on holder type: '%s' syspath: '%s'"",
                 dev_info['dev_type'], dev_info['device'])
        shutdown_function(dev_info['device'])
        udev.udevadm_settle()


def start_clear_holders_deps():
    """"""
    prepare system for clear holders to be able to scan old devices
    """"""
    # a mdadm scan has to be started in case there is a md device that needs to
    # be detected. if the scan fails, it is either because there are no mdadm
    # devices on the system, or because there is a mdadm device in a damaged
    # state that could not be started. due to the nature of mdadm tools, it is
    # difficult to know which is the case. if any errors did occur, then ignore
    # them, since no action needs to be taken if there were no mdadm devices on
    # the system, and in the case where there is some mdadm metadata on a disk,
    # but there was not enough to start the array, the call to wipe_volume on
    # all disks and partitions should be sufficient to remove the mdadm
    # metadata
    mdadm.mdadm_assemble(scan=True, ignore_errors=True)
    # the bcache module needs to be present to properly detect bcache devs
    # on some systems (precise without hwe kernel) it may not be possible to
    # lad the bcache module bcause it is not present in the kernel. if this
    # happens then there is no need to halt installation, as the bcache devices
    # will never appear and will never prevent the disk from being reformatted
    util.subp(['modprobe', 'bcache'], rcs=[0, 1])


# anything that is not identified can assumed to be a 'disk' or similar
DEFAULT_DEV_TYPE = 'disk'
# handlers that should not be run if an attempt is being made to preserve data
DATA_DESTROYING_HANDLERS = [wipe_superblock]
# types of devices that could be encountered by clear holders and functions to
# identify them and shut them down
DEV_TYPES = _define_handlers_registry()

# vi: ts=4 expandtab syntax=python
/n/n/n/curtin/deps/__init__.py/n/n# This file is part of curtin. See LICENSE file for copyright and license info.

import os
import sys

from curtin.util import (
    ProcessExecutionError,
    get_architecture,
    install_packages,
    is_uefi_bootable,
    lsb_release,
    which,
)

REQUIRED_IMPORTS = [
    # import string to execute, python2 package, python3 package
    ('import yaml', 'python-yaml', 'python3-yaml'),
]

REQUIRED_EXECUTABLES = [
    # executable in PATH, package
    ('file', 'file'),
    ('lvcreate', 'lvm2'),
    ('mdadm', 'mdadm'),
    ('mkfs.vfat', 'dosfstools'),
    ('mkfs.btrfs', 'btrfs-tools'),
    ('mkfs.ext4', 'e2fsprogs'),
    ('mkfs.xfs', 'xfsprogs'),
    ('partprobe', 'parted'),
    ('sgdisk', 'gdisk'),
    ('udevadm', 'udev'),
    ('make-bcache', 'bcache-tools'),
    ('iscsiadm', 'open-iscsi'),
]

if lsb_release()['codename'] == ""precise"":
    REQUIRED_IMPORTS.append(
        ('import oauth.oauth', 'python-oauth', None),)
else:
    REQUIRED_IMPORTS.append(
        ('import oauthlib.oauth1', 'python-oauthlib', 'python3-oauthlib'),)

if not is_uefi_bootable() and 'arm' in get_architecture():
    REQUIRED_EXECUTABLES.append(('flash-kernel', 'flash-kernel'))


class MissingDeps(Exception):
    def __init__(self, message, deps):
        self.message = message
        if isinstance(deps, str) or deps is None:
            deps = [deps]
        self.deps = [d for d in deps if d is not None]
        self.fatal = None in deps

    def __str__(self):
        if self.fatal:
            if not len(self.deps):
                return self.message + "" Unresolvable.""
            return (self.message +
                    "" Unresolvable.  Partially resolvable with packages: %s"" %
                    ' '.join(self.deps))
        else:
            return self.message + "" Install packages: %s"" % ' '.join(self.deps)


def check_import(imports, py2pkgs, py3pkgs, message=None):
    import_group = imports
    if isinstance(import_group, str):
        import_group = [import_group]

    for istr in import_group:
        try:
            exec(istr)
            return
        except ImportError:
            pass

    if not message:
        if isinstance(imports, str):
            message = ""Failed '%s'."" % imports
        else:
            message = ""Unable to do any of %s."" % import_group

    if sys.version_info[0] == 2:
        pkgs = py2pkgs
    else:
        pkgs = py3pkgs

    raise MissingDeps(message, pkgs)


def check_executable(cmdname, pkg):
    if not which(cmdname):
        raise MissingDeps(""Missing program '%s'."" % cmdname, pkg)


def check_executables(executables=None):
    if executables is None:
        executables = REQUIRED_EXECUTABLES
    mdeps = []
    for exe, pkg in executables:
        try:
            check_executable(exe, pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def check_imports(imports=None):
    if imports is None:
        imports = REQUIRED_IMPORTS

    mdeps = []
    for import_str, py2pkg, py3pkg in imports:
        try:
            check_import(import_str, py2pkg, py3pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def find_missing_deps():
    return check_executables() + check_imports()


def install_deps(verbosity=False, dry_run=False, allow_daemons=True):
    errors = find_missing_deps()
    if len(errors) == 0:
        if verbosity:
            sys.stderr.write(""No missing dependencies\n"")
        return 0

    missing_pkgs = []
    for e in errors:
        missing_pkgs += e.deps

    deps_string = ' '.join(sorted(missing_pkgs))

    if dry_run:
        sys.stderr.write(""Missing dependencies: %s\n"" % deps_string)
        return 0

    if os.geteuid() != 0:
        sys.stderr.write(""Missing dependencies: %s\n"" % deps_string)
        sys.stderr.write(""Package installation is not possible as non-root.\n"")
        return 2

    if verbosity:
        sys.stderr.write(""Installing %s\n"" % deps_string)

    ret = 0
    try:
        install_packages(missing_pkgs, allow_daemons=allow_daemons,
                         aptopts=[""--no-install-recommends""])
    except ProcessExecutionError as e:
        sys.stderr.write(""%s\n"" % e)
        ret = e.exit_code

    return ret

# vi: ts=4 expandtab syntax=python
/n/n/n",1
46,46,adb44011274c2ee193f56fb39f1f3d3b3f22fa82,"pautomate/common/git.py/n/n# -*- coding: utf-8 *-
""""""
Run Git commands in separate processes
""""""
import subprocess
from os import path
from typing import Dict

from .printing import print_green
from .printing import print_yellow


def shell(command: str) -> str:
    """"""Execute shell command

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- output of the shell
    """"""
    out, err = subprocess.Popen(
        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
    ).communicate()
    stdout, stderr = out.decode('utf-8'), err.decode('utf-8')
    output_lines = f'{stdout}\n{stderr}'.split('\n')
    for index, line in enumerate(output_lines):
        if '*' in line:
            output_lines[index] = f'\033[93m{line}\033[0m'
    return '\n'.join(output_lines)


def shell_first(command: str) -> str:
    """"""Execute in shell

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- first line of stdout
    """"""
    out, _ = subprocess.Popen(
        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
    ).communicate()
    return out.decode('utf-8').split('\n')[0]


def hard_reset(repo_path: str) -> str:
    """"""reset --hard

    Arguments:
        repo_path {str} -- path to repo to reset
    """"""
    return shell(f'git -C {repo_path} reset --hard')


def get_branches_info(repo_path: str) -> str:
    """"""git branch -a

    Arguments:
        repo_path {str} -- path to repo
    """"""
    return shell(f'git -C {repo_path} branch -a')


def fetch_repo(working_directory: str, name: str, url: str, summery_info: Dict[str, str]) -> None:
    """"""Clone / Fetch repo

    Arguments:
        working_directory {str} -- target directory
        name {str} -- repo name
        url {str} -- repo url in gitlab
        summery_info {Dict[str, str]} -- the result of the cloning/fetching
    """"""
    repo_path = path.join(working_directory, name)
    if path.isdir(repo_path):
        print_green(f'Fetching {name}')
        shell_first(f'git -C {repo_path} fetch')
        remote_banches = shell_first(f'git -C {repo_path} ls-remote --heads')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
        if f'refs/heads/{current_branch}' in remote_banches:
            shell_first(
                f'git -C {repo_path} fetch -u origin {current_branch}:{current_branch}',
            )
        else:
            print_yellow(f'{current_branch} does not exist on remote')

        if ('refs/heads/develop' in remote_banches and current_branch != 'develop'):
            shell_first(f'git -C {repo_path} fetch origin develop:develop')
    else:
        print_green(f'Cloning {name}')
        shell_first(f'git clone {url} {name}')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
    summery_info.update({name: current_branch})
/n/n/n",0
47,47,adb44011274c2ee193f56fb39f1f3d3b3f22fa82,"/pautomate/common/git.py/n/n# -*- coding: utf-8 *-
""""""
Run Git commands in separate processes
""""""
import shlex
import subprocess
from os import path
from typing import Dict

from .printing import print_green
from .printing import print_yellow


def shell(command: str) -> str:
    """"""Execute shell command

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- output of the shell
    """"""
    cmd = shlex.split(command)
    output_lines = subprocess.check_output(cmd).decode('utf-8').split('\n')
    for index, line in enumerate(output_lines):
        if '*' in line:
            output_lines[index] = f'\033[93m{line}\033[0m'
    return '\n'.join(output_lines)


def shell_first(command: str) -> str:
    """"""Execute in shell

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- first line of output
    """"""
    cmd = shlex.split(command)
    return subprocess.check_output(cmd).decode('utf-8').split('\n')[0]


def hard_reset(repo_path: str) -> str:
    """"""reset --hard

    Arguments:
        repo_path {str} -- path to repo to reset
    """"""
    return shell(f'git -C {repo_path} reset --hard')


def get_branches_info(repo_path: str) -> str:
    """"""git branch -a

    Arguments:
        repo_path {str} -- path to repo
    """"""
    return shell(f'git -C {repo_path} branch -a')


def fetch_repo(working_directory: str, name: str, url: str, summery_info: Dict[str, str]) -> None:
    """"""Clone / Fetch repo

    Arguments:
        working_directory {str} -- target directory
        name {str} -- repo name
        url {str} -- repo url in gitlab
        summery_info {Dict[str, str]} -- the result of the cloning/fetching
    """"""
    repo_path = path.join(working_directory, name)
    if path.isdir(repo_path):
        print_green(f'Fetching {name}')
        shell_first(f'git -C {repo_path} fetch')
        remote_banches = shell_first(f'git -C {repo_path} ls-remote --heads')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
        if f'refs/heads/{current_branch}' in remote_banches:
            shell_first(
                f'git -C {repo_path} fetch -u origin {current_branch}:{current_branch}',
            )
        else:
            print_yellow(f'{current_branch} does not exist on remote')

        if ('refs/heads/develop' in remote_banches and current_branch != 'develop'):
            shell_first(f'git -C {repo_path} fetch origin develop:develop')
    else:
        print_green(f'Cloning {name}')
        shell_first(f'git clone {url} {name}')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
    summery_info.update({name: current_branch})
/n/n/n",1
50,50,77244085365be31440c7e7909fa0cce0728d9cfc,"scoring/engine/polling/poll_rdp.py/n/nimport subprocess

from .poller import PollInput, PollResult, Poller

class RdpPollInput(PollInput):

    def __init__(self, server=None, port=None):
        super(RdpPollInput, self).__init__(server, port)

class RdpPollResult(PollResult):

    def __init__(self, authenticated, exceptions=None):
        super(RdpPollResult, self).__init__(exceptions)
        self.authenticated = authenticated

class RdpPoller(Poller):

    def poll(self, poll_input):
        username = poll_input.credentials.username
        password = poll_input.credentials.password
        domain = poll_input.credentials.domain
        cmd = ['xfreerdp', '--ignore-certificate', '--authonly', '-u', username, '-p', password]
        if not domain is None:
            cmd.extend(['-d', domain])
            opt_str = '--ignore-certificate --authonly -u \'{}\' -p \'{}\' {}:{}'
        cmd.append('{}:{}'.format(poll_input.server, poll_input.port))

        try:
            output = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
            result = RdpPollResult(True)
            return result
        except Exception as e:
#            if e.returncode == 131 and 'negotiation' in str(e.output) and not 'Connection reset by peer' in str(e.output):
#                result = RdpPollResult(True)
#                return result
            #print(""{{{{%s}}}}"" % e.output)
            result = RdpPollResult(False, e.output)
            return result
/n/n/n",0
51,51,77244085365be31440c7e7909fa0cce0728d9cfc,"/scoring/engine/polling/poll_rdp.py/n/nimport subprocess

from .poller import PollInput, PollResult, Poller

class RdpPollInput(PollInput):

    def __init__(self, server=None, port=None):
        super(RdpPollInput, self).__init__(server, port)

class RdpPollResult(PollResult):

    def __init__(self, authenticated, exceptions=None):
        super(RdpPollResult, self).__init__(exceptions)
        self.authenticated = authenticated

class RdpPoller(Poller):

    def poll(self, poll_input):
        username = poll_input.credentials.username
        password = poll_input.credentials.password
        domain = poll_input.credentials.domain
        
        if domain is None:
            opt_str = '--ignore-certificate --authonly -u \'{}\' -p \'{}\' {}:{}'
            options = opt_str.format(
                    username, password,
                    poll_input.server, poll_input.port)
        else:
            opt_str = '--ignore-certificate --authonly -d {} -u \'{}\' -p \'{}\' {}:{}'
            options = opt_str.format(
                    domain.domain, username, password,
                    poll_input.server, poll_input.port)

        try:
            output = subprocess.check_output('xfreerdp {}'.format(options), shell=True, stderr=subprocess.STDOUT)
            result = RdpPollResult(True)
            return result
        except Exception as e:
            if ('connected to' in str(e.output) and 'Authentication failure' not in str(e.output)) or (e.returncode == 131 and 'negotiation' in str(e.output)):
                result = RdpPollResult(True)
                return result
            print(""{{{{%s}}}}"" % e.output)
            result = RdpPollResult(False, e)
            return result
/n/n/n",1
118,118,3862e8ba359040fbdd6e1a6d36e51d07cda8e1ee,"mobly/controllers/android_device.py/n/n#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from builtins import str
from builtins import open
from past.builtins import basestring

import contextlib
import logging
import os
import time

from mobly import logger as mobly_logger
from mobly import signals
from mobly import utils
from mobly.controllers.android_device_lib import adb
from mobly.controllers.android_device_lib import event_dispatcher
from mobly.controllers.android_device_lib import fastboot
from mobly.controllers.android_device_lib import jsonrpc_client_base
from mobly.controllers.android_device_lib import sl4a_client
from mobly.controllers.android_device_lib import snippet_client

MOBLY_CONTROLLER_CONFIG_NAME = 'AndroidDevice'

ANDROID_DEVICE_PICK_ALL_TOKEN = '*'
_DEBUG_PREFIX_TEMPLATE = '[AndroidDevice|%s] %s'

# Key name for adb logcat extra params in config file.
ANDROID_DEVICE_ADB_LOGCAT_PARAM_KEY = 'adb_logcat_param'
ANDROID_DEVICE_EMPTY_CONFIG_MSG = 'Configuration is empty, abort!'
ANDROID_DEVICE_NOT_LIST_CONFIG_MSG = 'Configuration should be a list, abort!'

# Keys for attributes in configs that alternate the controller module behavior.
KEY_DEVICE_REQUIRED = 'required'


class Error(signals.ControllerError):
    pass


class DeviceError(Error):
    """"""Raised for errors from within an AndroidDevice object.""""""

    def __init__(self, ad, msg):
        new_msg = '%s %s' % (repr(ad), msg)
        super(DeviceError, self).__init__(new_msg)


class SnippetError(DeviceError):
    """"""Raised for errors related to Mobly snippet.""""""


def create(configs):
    """"""Creates AndroidDevice controller objects.

    Args:
        configs: A list of dicts, each representing a configuration for an
                 Android device.

    Returns:
        A list of AndroidDevice objects.
    """"""
    if not configs:
        raise Error(ANDROID_DEVICE_EMPTY_CONFIG_MSG)
    elif configs == ANDROID_DEVICE_PICK_ALL_TOKEN:
        ads = get_all_instances()
    elif not isinstance(configs, list):
        raise Error(ANDROID_DEVICE_NOT_LIST_CONFIG_MSG)
    elif isinstance(configs[0], dict):
        # Configs is a list of dicts.
        ads = get_instances_with_configs(configs)
    elif isinstance(configs[0], basestring):
        # Configs is a list of strings representing serials.
        ads = get_instances(configs)
    else:
        raise Error(""No valid config found in: %s"" % configs)
    connected_ads = list_adb_devices()

    for ad in ads:
        if ad.serial not in connected_ads:
            raise DeviceError(ad, 'Android device is specified in config but'
                              ' is not attached.')
    _start_services_on_ads(ads)
    return ads


def destroy(ads):
    """"""Cleans up AndroidDevice objects.

    Args:
        ads: A list of AndroidDevice objects.
    """"""
    for ad in ads:
        try:
            ad.stop_services()
        except:
            ad.log.exception('Failed to clean up properly.')


def get_info(ads):
    """"""Get information on a list of AndroidDevice objects.

    Args:
        ads: A list of AndroidDevice objects.

    Returns:
        A list of dict, each representing info for an AndroidDevice objects.
    """"""
    device_info = []
    for ad in ads:
        info = {'serial': ad.serial, 'model': ad.model}
        info.update(ad.build_info)
        device_info.append(info)
    return device_info


def _start_services_on_ads(ads):
    """"""Starts long running services on multiple AndroidDevice objects.

    If any one AndroidDevice object fails to start services, cleans up all
    existing AndroidDevice objects and their services.

    Args:
        ads: A list of AndroidDevice objects whose services to start.
    """"""
    running_ads = []
    for ad in ads:
        running_ads.append(ad)
        try:
            ad.start_services()
        except Exception:
            is_required = getattr(ad, KEY_DEVICE_REQUIRED, True)
            if is_required:
                ad.log.exception('Failed to start some services, abort!')
                destroy(running_ads)
                raise
            else:
                ad.log.exception('Skipping this optional device because some '
                                 'services failed to start.')


def _parse_device_list(device_list_str, key):
    """"""Parses a byte string representing a list of devices. The string is
    generated by calling either adb or fastboot.

    Args:
        device_list_str: Output of adb or fastboot.
        key: The token that signifies a device in device_list_str.

    Returns:
        A list of android device serial numbers.
    """"""
    clean_lines = str(device_list_str, 'utf-8').strip().split('\n')
    results = []
    for line in clean_lines:
        tokens = line.strip().split('\t')
        if len(tokens) == 2 and tokens[1] == key:
            results.append(tokens[0])
    return results


def list_adb_devices():
    """"""List all android devices connected to the computer that are detected by
    adb.

    Returns:
        A list of android device serials. Empty if there's none.
    """"""
    out = adb.AdbProxy().devices()
    return _parse_device_list(out, 'device')


def list_fastboot_devices():
    """"""List all android devices connected to the computer that are in in
    fastboot mode. These are detected by fastboot.

    Returns:
        A list of android device serials. Empty if there's none.
    """"""
    out = fastboot.FastbootProxy().devices()
    return _parse_device_list(out, 'fastboot')


def get_instances(serials):
    """"""Create AndroidDevice instances from a list of serials.

    Args:
        serials: A list of android device serials.

    Returns:
        A list of AndroidDevice objects.
    """"""
    results = []
    for s in serials:
        results.append(AndroidDevice(s))
    return results


def get_instances_with_configs(configs):
    """"""Create AndroidDevice instances from a list of dict configs.

    Each config should have the required key-value pair 'serial'.

    Args:
        configs: A list of dicts each representing the configuration of one
            android device.

    Returns:
        A list of AndroidDevice objects.
    """"""
    results = []
    for c in configs:
        try:
            serial = c.pop('serial')
        except KeyError:
            raise Error(
                'Required value ""serial"" is missing in AndroidDevice config %s.'
                % c)
        is_required = c.get(KEY_DEVICE_REQUIRED, True)
        try:
            ad = AndroidDevice(serial)
            ad.load_config(c)
        except Exception:
            if is_required:
                raise
            ad.log.exception('Skipping this optional device due to error.')
            continue
        results.append(ad)
    return results


def get_all_instances(include_fastboot=False):
    """"""Create AndroidDevice instances for all attached android devices.

    Args:
        include_fastboot: Whether to include devices in bootloader mode or not.

    Returns:
        A list of AndroidDevice objects each representing an android device
        attached to the computer.
    """"""
    if include_fastboot:
        serial_list = list_adb_devices() + list_fastboot_devices()
        return get_instances(serial_list)
    return get_instances(list_adb_devices())


def filter_devices(ads, func):
    """"""Finds the AndroidDevice instances from a list that match certain
    conditions.

    Args:
        ads: A list of AndroidDevice instances.
        func: A function that takes an AndroidDevice object and returns True
            if the device satisfies the filter condition.

    Returns:
        A list of AndroidDevice instances that satisfy the filter condition.
    """"""
    results = []
    for ad in ads:
        if func(ad):
            results.append(ad)
    return results


def get_device(ads, **kwargs):
    """"""Finds a unique AndroidDevice instance from a list that has specific
    attributes of certain values.

    Example:
        get_device(android_devices, label='foo', phone_number='1234567890')
        get_device(android_devices, model='angler')

    Args:
        ads: A list of AndroidDevice instances.
        kwargs: keyword arguments used to filter AndroidDevice instances.

    Returns:
        The target AndroidDevice instance.

    Raises:
        Error is raised if none or more than one device is
        matched.
    """"""

    def _get_device_filter(ad):
        for k, v in kwargs.items():
            if not hasattr(ad, k):
                return False
            elif getattr(ad, k) != v:
                return False
        return True

    filtered = filter_devices(ads, _get_device_filter)
    if not filtered:
        raise Error(
            'Could not find a target device that matches condition: %s.' %
            kwargs)
    elif len(filtered) == 1:
        return filtered[0]
    else:
        serials = [ad.serial for ad in filtered]
        raise Error('More than one device matched: %s' % serials)


def take_bug_reports(ads, test_name, begin_time):
    """"""Takes bug reports on a list of android devices.

    If you want to take a bug report, call this function with a list of
    android_device objects in on_fail. But reports will be taken on all the
    devices in the list concurrently. Bug report takes a relative long
    time to take, so use this cautiously.

    Args:
        ads: A list of AndroidDevice instances.
        test_name: Name of the test case that triggered this bug report.
        begin_time: Logline format timestamp taken when the test started.
    """"""
    begin_time = mobly_logger.normalize_log_line_timestamp(begin_time)

    def take_br(test_name, begin_time, ad):
        ad.take_bug_report(test_name, begin_time)

    args = [(test_name, begin_time, ad) for ad in ads]
    utils.concurrent_exec(take_br, args)


class AndroidDevice(object):
    """"""Class representing an android device.

    Each object of this class represents one Android device in Mobly. This class
    provides various ways, like adb, fastboot, sl4a, and snippets, to control an
    Android device, whether it's a real device or an emulator instance.

    Attributes:
        serial: A string that's the serial number of the Androi device.
        log_path: A string that is the path where all logs collected on this
                  android device should be stored.
        log: A logger adapted from root logger with an added prefix specific
             to an AndroidDevice instance. The default prefix is
             [AndroidDevice|<serial>]. Use self.set_debug_tag to use a
             different tag in the prefix.
        adb_logcat_file_path: A string that's the full path to the adb logcat
                              file collected, if any.
        adb: An AdbProxy object used for interacting with the device via adb.
        fastboot: A FastbootProxy object used for interacting with the device
                  via fastboot.
    """"""

    def __init__(self, serial=''):
        self.serial = serial
        # logging.log_path only exists when this is used in an Mobly test run.
        log_path_base = getattr(logging, 'log_path', '/tmp/logs')
        self.log_path = os.path.join(log_path_base, 'AndroidDevice%s' % serial)
        self._debug_tag = self.serial
        self.log = AndroidDeviceLoggerAdapter(logging.getLogger(),
                                              {'tag': self.debug_tag})
        self.sl4a = None
        self.ed = None
        self._adb_logcat_process = None
        self.adb_logcat_file_path = None
        self.adb = adb.AdbProxy(serial)
        self.fastboot = fastboot.FastbootProxy(serial)
        if not self.is_bootloader and self.is_rootable:
            self.root_adb()
        # A dict for tracking snippet clients. Keys are clients' attribute
        # names, values are the clients: {<attr name string>: <client object>}.
        self._snippet_clients = {}

    def __repr__(self):
        return ""<AndroidDevice|%s>"" % self.debug_tag

    @property
    def debug_tag(self):
        """"""A string that represents a device object in debug info. Default value
        is the device serial.

        This will be used as part of the prefix of debugging messages emitted by
        this device object, like log lines and the message of DeviceError.
        """"""
        return self._debug_tag

    @debug_tag.setter
    def debug_tag(self, tag):
        """"""Setter for the debug tag.

        By default, the tag is the serial of the device, but sometimes it may
        be more descriptive to use a different tag of the user's choice.

        Changing debug tag changes part of the prefix of debug info emitted by
        this object, like log lines and the message of DeviceError.

        Example:
            By default, the device's serial number is used:
                'INFO [AndroidDevice|abcdefg12345] One pending call ringing.'
            The tag can be customized with `ad.debug_tag = 'Caller'`:
                'INFO [AndroidDevice|Caller] One pending call ringing.'
        """"""
        self._debug_tag = tag
        self.log.extra['tag'] = tag

    def start_services(self):
        """"""Starts long running services on the android device, like adb logcat
        capture.
        """"""
        try:
            self.start_adb_logcat()
        except:
            self.log.exception('Failed to start adb logcat!')
            raise

    def stop_services(self):
        """"""Stops long running services on the Android device.

        Stop adb logcat, terminate sl4a sessions if exist, terminate all
        snippet clients.

        Returns:
            A dict containing information on the running services before they
            are torn down. This can be used to restore these services, which
            includes snippets and sl4a.
        """"""
        service_info = {}
        service_info['snippet_info'] = self._get_active_snippet_info()
        service_info['use_sl4a'] = self.sl4a is not None
        self._terminate_sl4a()
        for name, client in self._snippet_clients.items():
            self._terminate_jsonrpc_client(client)
            delattr(self, name)
        self._snippet_clients = {}
        if self._adb_logcat_process:
            try:
                self.stop_adb_logcat()
            except:
                self.log.exception('Failed to stop adb logcat.')
        return service_info

    @contextlib.contextmanager
    def handle_device_disconnect(self):
        """"""Properly manage the service life cycle when the device needs to
        temporarily disconnect.

        The device can temporarily lose adb connection due to user-triggered
        reboot or power measurement. Use this function to make sure the services
        started by Mobly are properly stopped and restored afterwards.

        For sample usage, see self.reboot().
        """"""
        service_info = self.stop_services()
        try:
            yield
        finally:
            self._restore_services(service_info)

    def _restore_services(self, service_info):
        """"""Restores services after a device has come back from temporary
        being offline.

        Args:
            service_info: A dict containing information on the services to
                          restore, which could include snippet and sl4a.
        """"""
        self.wait_for_boot_completion()
        if self.is_rootable:
            self.root_adb()
        self.start_services()
        # Restore snippets.
        snippet_info = service_info['snippet_info']
        for attr_name, package_name in snippet_info:
            self.load_snippet(attr_name, package_name)
        # Restore sl4a if needed.
        if service_info['use_sl4a']:
            self.load_sl4a()

    @property
    def build_info(self):
        """"""Get the build info of this Android device, including build id and
        build type.

        This is not available if the device is in bootloader mode.

        Returns:
            A dict with the build info of this Android device, or None if the
            device is in bootloader mode.
        """"""
        if self.is_bootloader:
            self.log.error('Device is in fastboot mode, could not get build '
                           'info.')
            return
        info = {}
        info['build_id'] = self.adb.getprop('ro.build.id')
        info['build_type'] = self.adb.getprop('ro.build.type')
        return info

    @property
    def is_bootloader(self):
        """"""True if the device is in bootloader mode.
        """"""
        return self.serial in list_fastboot_devices()

    @property
    def is_adb_root(self):
        """"""True if adb is running as root for this device.
        """"""
        try:
            return '0' == self.adb.shell('id -u').decode('utf-8').strip()
        except adb.AdbError:
            # Wait a bit and retry to work around adb flakiness for this cmd.
            time.sleep(0.2)
            return '0' == self.adb.shell('id -u').decode('utf-8').strip()

    @property
    def is_rootable(self):
        """"""If the build type is 'user', the device is not rootable.

        Other possible build types are 'userdebug' and 'eng', both are rootable.
        We are checking the last four chars of the clean stdout because the
        stdout of the adb command could be polluted with other info like adb
        server startup message.
        """"""
        build_type_output = self.adb.getprop('ro.build.type').lower()
        return build_type_output[-4:] != 'user'

    @property
    def model(self):
        """"""The Android code name for the device.
        """"""
        # If device is in bootloader mode, get mode name from fastboot.
        if self.is_bootloader:
            out = self.fastboot.getvar('product').strip()
            # 'out' is never empty because of the 'total time' message fastboot
            # writes to stderr.
            lines = out.decode('utf-8').split('\n', 1)
            if lines:
                tokens = lines[0].split(' ')
                if len(tokens) > 1:
                    return tokens[1].lower()
            return None
        model = self.adb.getprop('ro.build.product').lower()
        if model == 'sprout':
            return model
        else:
            return self.adb.getprop('ro.product.name').lower()

    def load_config(self, config):
        """"""Add attributes to the AndroidDevice object based on config.

        Args:
            config: A dictionary representing the configs.

        Raises:
            Error is raised if the config is trying to overwrite
            an existing attribute.
        """"""
        for k, v in config.items():
            if hasattr(self, k):
                raise DeviceError(self, (
                    'Attribute %s already exists with value %s, cannot set '
                    'again.') % (k, getattr(self, k)))
            setattr(self, k, v)

    def root_adb(self):
        """"""Change adb to root mode for this device if allowed.

        If executed on a production build, adb will not be switched to root
        mode per security restrictions.
        """"""
        self.adb.root()
        self.adb.wait_for_device()

    def load_snippet(self, name, package):
        """"""Starts the snippet apk with the given package name and connects.

        Examples:
            >>> ad = AndroidDevice()
            >>> ad.load_snippet(
                    name='maps', package='com.google.maps.snippets')
            >>> ad.maps.activateZoom('3')

        Args:
            name: The attribute name to which to attach the snippet server.
                  e.g. name='maps' will attach the snippet server to ad.maps.
            package: The package name defined in AndroidManifest.xml of the
                     snippet apk.

        Raises:
            SnippetError is raised if illegal load operations are attempted.
        """"""
        # Should not load snippet with the same attribute more than once.
        if name in self._snippet_clients:
            raise SnippetError(
                self,
                'Attribute ""%s"" is already registered with package ""%s"", it '
                'cannot be used again.' %
                (name, self._snippet_clients[name].package))
        # Should not load snippet with an existing attribute.
        if hasattr(self, name):
            raise SnippetError(
                self,
                'Attribute ""%s"" already exists, please use a different name.' %
                name)
        # Should not load the same snippet package more than once.
        for client_name, client in self._snippet_clients.items():
            if package == client.package:
                raise SnippetError(
                    self,
                    'Snippet package ""%s"" has already been loaded under name'
                    ' ""%s"".' % (package, client_name))
        host_port = utils.get_available_host_port()
        client = snippet_client.SnippetClient(
            package=package,
            host_port=host_port,
            adb_proxy=self.adb,
            log=self.log)
        self._start_jsonrpc_client(client)
        self._snippet_clients[name] = client
        setattr(self, name, client)

    def load_sl4a(self):
        """"""Start sl4a service on the Android device.

        Launch sl4a server if not already running, spin up a session on the
        server, and two connections to this session.

        Creates an sl4a client (self.sl4a) with one connection, and one
        EventDispatcher obj (self.ed) with the other connection.
        """"""
        host_port = utils.get_available_host_port()
        self.sl4a = sl4a_client.Sl4aClient(
            host_port=host_port, adb_proxy=self.adb)
        self._start_jsonrpc_client(self.sl4a)

        # Start an EventDispatcher for the current sl4a session
        event_client = sl4a_client.Sl4aClient(
            host_port=host_port, adb_proxy=self.adb)
        event_client.connect(
            uid=self.sl4a.uid, cmd=jsonrpc_client_base.JsonRpcCommand.CONTINUE)
        self.ed = event_dispatcher.EventDispatcher(event_client)
        self.ed.start()

    def _start_jsonrpc_client(self, client):
        """"""Create a connection to a jsonrpc server running on the device.

        If the connection cannot be made, tries to restart it.
        """"""
        client.check_app_installed()
        self.adb.tcp_forward(client.host_port, client.device_port)
        try:
            client.connect()
        except:
            try:
                client.stop_app()
            except Exception as e:
                self.log.warning(e)
            client.start_app()
            client.connect()

    def _terminate_jsonrpc_client(self, client):
        try:
            client.closeSl4aSession()
            client.close()
            client.stop_app()
        except:
            self.log.exception('Failed to stop Rpc client for %s.',
                               client.app_name)
        finally:
            # Always clean up the adb port
            self.adb.forward('--remove tcp:%d' % client.host_port)

    def _is_timestamp_in_range(self, target, begin_time, end_time):
        low = mobly_logger.logline_timestamp_comparator(begin_time,
                                                        target) <= 0
        high = mobly_logger.logline_timestamp_comparator(end_time, target) >= 0
        return low and high

    def cat_adb_log(self, tag, begin_time):
        """"""Takes an excerpt of the adb logcat log from a certain time point to
        current time.

        Args:
            tag: An identifier of the time period, usualy the name of a test.
            begin_time: Logline format timestamp of the beginning of the time
                period.
        """"""
        if not self.adb_logcat_file_path:
            raise DeviceError(
                self,
                'Attempting to cat adb log when none has been collected.')
        end_time = mobly_logger.get_log_line_timestamp()
        self.log.debug('Extracting adb log from logcat.')
        adb_excerpt_path = os.path.join(self.log_path, 'AdbLogExcerpts')
        utils.create_dir(adb_excerpt_path)
        f_name = os.path.basename(self.adb_logcat_file_path)
        out_name = f_name.replace('adblog,', '').replace('.txt', '')
        out_name = ',%s,%s.txt' % (begin_time, out_name)
        tag_len = utils.MAX_FILENAME_LEN - len(out_name)
        tag = tag[:tag_len]
        out_name = tag + out_name
        full_adblog_path = os.path.join(adb_excerpt_path, out_name)
        with open(full_adblog_path, 'w', encoding='utf-8') as out:
            in_file = self.adb_logcat_file_path
            with open(in_file, 'r', encoding='utf-8', errors='replace') as f:
                in_range = False
                while True:
                    line = None
                    try:
                        line = f.readline()
                        if not line:
                            break
                    except:
                        continue
                    line_time = line[:mobly_logger.log_line_timestamp_len]
                    if not mobly_logger.is_valid_logline_timestamp(line_time):
                        continue
                    if self._is_timestamp_in_range(line_time, begin_time,
                                                   end_time):
                        in_range = True
                        if not line.endswith('\n'):
                            line += '\n'
                        out.write(line)
                    else:
                        if in_range:
                            break

    def start_adb_logcat(self):
        """"""Starts a standing adb logcat collection in separate subprocesses and
        save the logcat in a file.
        """"""
        if self._adb_logcat_process:
            raise DeviceError(
                self,
                'Logcat thread is already running, cannot start another one.')
        # Disable adb log spam filter for rootable. Have to stop and clear
        # settings first because 'start' doesn't support --clear option before
        # Android N.
        if self.is_rootable:
            self.adb.shell('logpersist.stop --clear')
            self.adb.shell('logpersist.start')
        f_name = 'adblog,%s,%s.txt' % (self.model, self.serial)
        utils.create_dir(self.log_path)
        logcat_file_path = os.path.join(self.log_path, f_name)
        try:
            extra_params = self.adb_logcat_param
        except AttributeError:
            extra_params = '-b all'
        cmd = 'adb -s %s logcat -v threadtime %s >> %s' % (
            self.serial, extra_params, logcat_file_path)
        self._adb_logcat_process = utils.start_standing_subprocess(cmd)
        self.adb_logcat_file_path = logcat_file_path

    def stop_adb_logcat(self):
        """"""Stops the adb logcat collection subprocess.

        Raises:
            DeviceError: raised if there's no adb logcat collection going on.
        """"""
        if not self._adb_logcat_process:
            raise DeviceError(self, 'No ongoing adb logcat collection found.')
        utils.stop_standing_subprocess(self._adb_logcat_process)
        self._adb_logcat_process = None

    def take_bug_report(self, test_name, begin_time):
        """"""Takes a bug report on the device and stores it in a file.

        Args:
            test_name: Name of the test case that triggered this bug report.
            begin_time: Logline format timestamp taken when the test started.
        """"""
        new_br = True
        try:
            stdout = self.adb.shell('bugreportz -v').decode('utf-8')
            # This check is necessary for builds before N, where adb shell's ret
            # code and stderr are not propagated properly.
            if 'not found' in stdout:
                new_br = False
        except adb.AdbError:
            new_br = False
        br_path = os.path.join(self.log_path, 'BugReports')
        utils.create_dir(br_path)
        base_name = ',%s,%s.txt' % (begin_time, self.serial)
        if new_br:
            base_name = base_name.replace('.txt', '.zip')
        test_name_len = utils.MAX_FILENAME_LEN - len(base_name)
        out_name = test_name[:test_name_len] + base_name
        full_out_path = os.path.join(br_path, out_name.replace(' ', r'\ '))
        # in case device restarted, wait for adb interface to return
        self.wait_for_boot_completion()
        self.log.info('Taking bugreport for %s.', test_name)
        if new_br:
            out = self.adb.shell('bugreportz').decode('utf-8')
            if not out.startswith('OK'):
                raise DeviceError(self, 'Failed to take bugreport: %s' % out)
            br_out_path = out.split(':')[1].strip()
            self.adb.pull([br_out_path, full_out_path])
        else:
            # shell=True as this command redirects the stdout to a local file
            # using shell redirection.
            self.adb.bugreport(' > %s' % full_out_path, shell=True)
        self.log.info('Bugreport for %s taken at %s.', test_name,
                      full_out_path)

    def _terminate_sl4a(self):
        """"""Terminate the current sl4a session.

        Send terminate signal to sl4a server; stop dispatcher associated with
        the session. Clear corresponding droids and dispatchers from cache.
        """"""
        if self.sl4a:
            self._terminate_jsonrpc_client(self.sl4a)
            self.sl4a = None
        if self.ed:
            try:
                self.ed.clean_up()
            except:
                self.log.exception('Failed to shutdown sl4a event dispatcher.')
            self.ed = None

    def run_iperf_client(self, server_host, extra_args=''):
        """"""Start iperf client on the device.

        Return status as true if iperf client start successfully.
        And data flow information as results.

        Args:
            server_host: Address of the iperf server.
            extra_args: A string representing extra arguments for iperf client,
                e.g. '-i 1 -t 30'.

        Returns:
            status: true if iperf client start successfully.
            results: results have data flow information
        """"""
        out = self.adb.shell('iperf3 -c %s %s' % (server_host, extra_args))
        clean_out = str(out, 'utf-8').strip().split('\n')
        if 'error' in clean_out[0].lower():
            return False, clean_out
        return True, clean_out

    def wait_for_boot_completion(self):
        """"""Waits for Android framework to broadcast ACTION_BOOT_COMPLETED.

        This function times out after 15 minutes.
        """"""
        timeout_start = time.time()
        timeout = 15 * 60

        self.adb.wait_for_device()
        while time.time() < timeout_start + timeout:
            try:
                completed = self.adb.getprop('sys.boot_completed')
                if completed == '1':
                    return
            except adb.AdbError:
                # adb shell calls may fail during certain period of booting
                # process, which is normal. Ignoring these errors.
                pass
            time.sleep(5)
        raise DeviceError(self, 'Booting process timed out.')

    def _get_active_snippet_info(self):
        """"""Collects information on currently active snippet clients.

        The info is used for restoring the snippet clients after rebooting the
        device.

        Returns:
            A list of tuples, each tuple's first element is the name of the
            snippet client's attribute, the second element is the package name
            of the snippet.
        """"""
        snippet_info = []
        for attr_name, client in self._snippet_clients.items():
            snippet_info.append((attr_name, client.package))
        return snippet_info

    def reboot(self):
        """"""Reboots the device.

        Terminate all sl4a sessions, reboot the device, wait for device to
        complete booting, and restart an sl4a session.

        This is a blocking method.

        This is probably going to print some error messages in console. Only
        use if there's no other option.

        Raises:
            Error is raised if waiting for completion timed out.
        """"""
        if self.is_bootloader:
            self.fastboot.reboot()
            return
        with self.handle_device_disconnect():
            self.adb.reboot()


class AndroidDeviceLoggerAdapter(logging.LoggerAdapter):
    """"""A wrapper class that adds a prefix to each log line.

    Usage:
        my_log = AndroidDeviceLoggerAdapter(logging.getLogger(), {
            'tag': <custom tag>
        })

        Then each log line added by my_log will have a prefix
        '[AndroidDevice|<tag>]'
    """"""

    def process(self, msg, kwargs):
        msg = _DEBUG_PREFIX_TEMPLATE % (self.extra['tag'], msg)
        return (msg, kwargs)
/n/n/nmobly/controllers/android_device_lib/adb.py/n/n#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from builtins import str
from past.builtins import basestring

import logging
import subprocess


class AdbError(Exception):
    """"""Raised when there is an error in adb operations.""""""

    def __init__(self, cmd, stdout, stderr, ret_code):
        self.cmd = cmd
        self.stdout = stdout
        self.stderr = stderr
        self.ret_code = ret_code

    def __str__(self):
        return ('Error executing adb cmd ""%s"". ret: %d, stdout: %s, stderr: %s'
                ) % (self.cmd, self.ret_code, self.stdout, self.stderr)


def list_occupied_adb_ports():
    """"""Lists all the host ports occupied by adb forward.

    This is useful because adb will silently override the binding if an attempt
    to bind to a port already used by adb was made, instead of throwing binding
    error. So one should always check what ports adb is using before trying to
    bind to a port with adb.

    Returns:
        A list of integers representing occupied host ports.
    """"""
    out = AdbProxy().forward('--list')
    clean_lines = str(out, 'utf-8').strip().split('\n')
    used_ports = []
    for line in clean_lines:
        tokens = line.split(' tcp:')
        if len(tokens) != 3:
            continue
        used_ports.append(int(tokens[1]))
    return used_ports


class AdbProxy(object):
    """"""Proxy class for ADB.

    For syntactic reasons, the '-' in adb commands need to be replaced with
    '_'. Can directly execute adb commands on an object:
    >> adb = AdbProxy(<serial>)
    >> adb.start_server()
    >> adb.devices() # will return the console output of ""adb devices"".

    By default, command args are expected to be an iterable which is passed
    directly to subprocess.Popen():
    >> adb.shell(['echo', 'a', 'b'])

    This way of launching commands is recommended by the subprocess
    documentation to avoid shell injection vulnerabilities and avoid having to
    deal with multiple layers of shell quoting and different shell environments
    between different OSes.

    If you really want to run the command through the system shell, this is
    possible by supplying shell=True, but try to avoid this if possible:
    >> adb.shell('cat /foo > /tmp/file', shell=True)
    """"""

    def __init__(self, serial=''):
        self.serial = serial

    def _exec_cmd(self, args, shell):
        """"""Executes adb commands.

        Args:
            args: string or list of strings, program arguments.
                See subprocess.Popen() documentation.
            shell: bool, True to run this command through the system shell,
                False to invoke it directly. See subprocess.Popen() docs.

        Returns:
            The output of the adb command run if exit code is 0.

        Raises:
            AdbError is raised if the adb command exit code is not 0.
        """"""
        proc = subprocess.Popen(
            args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=shell)
        (out, err) = proc.communicate()
        ret = proc.returncode
        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s', args, out,
                      err, ret)
        if ret == 0:
            return out
        else:
            raise AdbError(cmd=args, stdout=out, stderr=err, ret_code=ret)

    def _exec_adb_cmd(self, name, args, shell):
        if shell:
            if self.serial:
                adb_cmd = 'adb -s ""%s"" %s %s' % (self.serial, name, args)
            else:
                adb_cmd = 'adb %s %s' % (name, args)
        else:
            adb_cmd = ['adb']
            if self.serial:
                adb_cmd.extend(['-s', self.serial])
            adb_cmd.append(name)
            if args:
                if isinstance(args, basestring):
                    adb_cmd.append(args)
                else:
                    adb_cmd.extend(args)
        return self._exec_cmd(adb_cmd, shell=shell)

    def tcp_forward(self, host_port, device_port):
        """"""Starts tcp forwarding.

        Args:
            host_port: Port number to use on the computer.
            device_port: Port number to use on the android device.
        """"""
        self.forward(['tcp:%d' % host_port, 'tcp:%d' % device_port])

    def getprop(self, prop_name):
        """"""Get a property of the device.

        This is a convenience wrapper for ""adb shell getprop xxx"".

        Args:
            prop_name: A string that is the name of the property to get.

        Returns:
            A string that is the value of the property, or None if the property
            doesn't exist.
        """"""
        return self.shell('getprop %s' % prop_name).decode('utf-8').strip()

    def __getattr__(self, name):
        def adb_call(args=None, shell=False):
            """"""Wrapper for an ADB command.

            Args:
                args: string or list of strings, arguments to the adb command.
                    See subprocess.Proc() documentation.
                shell: bool, True to run this command through the system shell,
                    False to invoke it directly. See subprocess.Proc() docs.

            Returns:
                The output of the adb command run if exit code is 0.
            """"""
            args = args or ''
            clean_name = name.replace('_', '-')
            return self._exec_adb_cmd(clean_name, args, shell=shell)

        return adb_call
/n/n/nmobly/controllers/android_device_lib/jsonrpc_client_base.py/n/n#/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Base class for clients that communicate with apps over a JSON RPC interface.

The JSON protocol expected by this module is:

Request:
{
    ""id"": <monotonically increasing integer containing the ID of this request>
    ""method"": <string containing the name of the method to execute>
    ""params"": <JSON array containing the arguments to the method>
}

Response:
{
    ""id"": <int id of request that this response maps to>,
    ""result"": <Arbitrary JSON object containing the result of executing the
               method. If the method could not be executed or returned void,
               contains 'null'.>,
    ""error"": <String containing the error thrown by executing the method.
              If no error occurred, contains 'null'.>
    ""callback"": <String that represents a callback ID used to identify events
                 associated with a particular CallbackHandler object.>
""""""

from builtins import str

import json
import logging
import socket
import threading
import time

from mobly.controllers.android_device_lib import adb
from mobly.controllers.android_device_lib import callback_handler

# Maximum time to wait for the app to start on the device.
APP_START_WAIT_TIME = 15

# UID of the 'unknown' jsonrpc session. Will cause creation of a new session.
UNKNOWN_UID = -1

# Maximum time to wait for the socket to open on the device.
_SOCKET_CONNECTION_TIMEOUT = 60

# Maximum time to wait for a response message on the socket.
_SOCKET_READ_TIMEOUT = callback_handler.MAX_TIMEOUT


class Error(Exception):
    pass


class AppStartError(Error):
    """"""Raised when the app is not able to be started.""""""


class ApiError(Error):
    """"""Raised when remote API reports an error.""""""


class ProtocolError(Error):
    """"""Raised when there is some error in exchanging data with server.""""""
    NO_RESPONSE_FROM_HANDSHAKE = 'No response from handshake.'
    NO_RESPONSE_FROM_SERVER = 'No response from server.'
    MISMATCHED_API_ID = 'Mismatched API id.'


class JsonRpcCommand(object):
    """"""Commands that can be invoked on all jsonrpc clients.

    INIT: Initializes a new session.
    CONTINUE: Creates a connection.
    """"""
    INIT = 'initiate'
    CONTINUE = 'continue'


class JsonRpcClientBase(object):
    """"""Base class for jsonrpc clients that connect to remote servers.

    Connects to a remote device running a jsonrpc-compatible app. Before opening
    a connection a port forward must be setup to go over usb. This be done using
    adb.tcp_forward(). This calls the shell command adb forward <local> remote>.
    Once the port has been forwarded it can be used in this object as the port
    of communication.

    Attributes:
        host_port: (int) The host port of this RPC client.
        device_port: (int) The device port of this RPC client.
        app_name: (str) The user-visible name of the app being communicated
                  with.
        uid: (int) The uid of this session.
    """"""

    def __init__(self,
                 host_port,
                 device_port,
                 app_name,
                 adb_proxy,
                 log=logging.getLogger()):
        """"""
        Args:
            host_port: (int) The host port of this RPC client.
            device_port: (int) The device port of this RPC client.
            app_name: (str) The user-visible name of the app being communicated
                      with.
            adb_proxy: (adb.AdbProxy) The adb proxy to use to start the app.
        """"""
        self.host_port = host_port
        self.device_port = device_port
        self.app_name = app_name
        self.uid = None
        self._adb = adb_proxy
        self._client = None  # prevent close errors on connect failure
        self._conn = None
        self._counter = None
        self._lock = threading.Lock()
        self._event_client = None
        self._log = log

    def __del__(self):
        self.close()

    # Methods to be implemented by subclasses.

    def _do_start_app(self):
        """"""Starts the server app on the android device.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    def _start_event_client(self):
        """"""Starts a separate JsonRpc client to the same session for propagating
        events.

        This is an optional function that should only implement if the client
        utilizes the snippet event mechanism.

        Returns:
            A JsonRpc Client object that connects to the same session as the
            one on which this function is called.
        """"""
        raise NotImplementedError()

    def stop_app(self):
        """"""Kills any running instance of the app.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    def check_app_installed(self):
        """"""Checks if app is installed.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    # Rest of the client methods.

    def start_app(self, wait_time=APP_START_WAIT_TIME):
        """"""Starts the server app on the android device.

        Args:
            wait_time: float, The time to wait for the app to come up before
                       raising an error.

        Raises:
            AppStartError: When the app was not able to be started.
        """"""
        self.check_app_installed()
        self._do_start_app()
        for _ in range(wait_time):
            time.sleep(1)
            if self._is_app_running():
                self._log.debug('Successfully started %s', self.app_name)
                return
        raise AppStartError('%s failed to start on %s.' %
                            (self.app_name, self._adb.serial))

    def connect(self, uid=UNKNOWN_UID, cmd=JsonRpcCommand.INIT):
        """"""Opens a connection to a JSON RPC server.

        Opens a connection to a remote client. The connection attempt will time
        out if it takes longer than _SOCKET_CONNECTION_TIMEOUT seconds. Each
        subsequent operation over this socket will time out after
        _SOCKET_READ_TIMEOUT seconds as well.

        Args:
            uid: int, The uid of the session to join, or UNKNOWN_UID to start a
                 new session.
            cmd: JsonRpcCommand, The command to use for creating the connection.

        Raises:
            IOError: Raised when the socket times out from io error
            socket.timeout: Raised when the socket waits to long for connection.
            ProtocolError: Raised when there is an error in the protocol.
        """"""
        self._counter = self._id_counter()
        self._conn = socket.create_connection(('127.0.0.1', self.host_port),
                                              _SOCKET_CONNECTION_TIMEOUT)
        self._conn.settimeout(_SOCKET_READ_TIMEOUT)
        self._client = self._conn.makefile(mode='brw')

        resp = self._cmd(cmd, uid)
        if not resp:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_HANDSHAKE)
        result = json.loads(str(resp, encoding='utf8'))
        if result['status']:
            self.uid = result['uid']
        else:
            self.uid = UNKNOWN_UID

    def close(self):
        """"""Close the connection to the remote client.""""""
        if self._conn:
            self._conn.close()
            self._conn = None

    def _adb_grep_wrapper(self, adb_shell_cmd):
        """"""A wrapper for the specific usage of adb shell grep in this class.

        This surpresses AdbError if the grep fails to find anything.

        Args:
            adb_shell_cmd: string, a grep command to execute on the device.

        Returns:
            The stdout of the grep result if the grep found something, False
            otherwise.
        """"""
        try:
            return self._adb.shell(adb_shell_cmd).decode('utf-8').rstrip()
        except adb.AdbError as e:
            if (e.ret_code == 1) and (not e.stdout) and (not e.stderr):
                return False
            raise

    def _cmd(self, command, uid=None):
        """"""Send a command to the server.

        Args:
            command: str, The name of the command to execute.
            uid: int, the uid of the session to send the command to.

        Returns:
            The line that was written back.
        """"""
        if not uid:
            uid = self.uid
        self._client.write(
            json.dumps({
                'cmd': command,
                'uid': uid
            }).encode(""utf8"") + b'\n')
        self._client.flush()
        return self._client.readline()

    def _rpc(self, method, *args):
        """"""Sends an rpc to the app.

        Args:
            method: str, The name of the method to execute.
            args: any, The args of the method.

        Returns:
            The result of the rpc.

        Raises:
            ProtocolError: Something went wrong with the protocol.
            ApiError: The rpc went through, however executed with errors.
        """"""
        with self._lock:
            apiid = next(self._counter)
            data = {'id': apiid, 'method': method, 'params': args}
            request = json.dumps(data)
            self._client.write(request.encode(""utf8"") + b'\n')
            self._client.flush()
            response = self._client.readline()
        if not response:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_SERVER)
        result = json.loads(str(response, encoding=""utf8""))
        if result['error']:
            raise ApiError(result['error'])
        if result['id'] != apiid:
            raise ProtocolError(ProtocolError.MISMATCHED_API_ID)
        if result.get('callback') is not None:
            if self._event_client is None:
                self._event_client = self._start_event_client()
            return callback_handler.CallbackHandler(
                callback_id=result['callback'],
                event_client=self._event_client,
                ret_value=result['result'],
                method_name=method)
        return result['result']

    def _is_app_running(self):
        """"""Checks if the app is currently running on an android device.

        May be overridden by subclasses with custom sanity checks.
        """"""
        running = False
        try:
            self.connect()
            running = True
        finally:
            self.close()
            # This 'return' squashes exceptions from connect()
            return running

    def __getattr__(self, name):
        """"""Wrapper for python magic to turn method calls into RPC calls.""""""

        def rpc_call(*args):
            return self._rpc(name, *args)

        return rpc_call

    def _id_counter(self):
        i = 0
        while True:
            yield i
            i += 1
/n/n/ntests/lib/mock_android_device.py/n/n#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This module has common mock objects and functions used in unit tests for
# mobly.controllers.android_device module.

import logging
import mock
import os


class Error(Exception):
    pass


def get_mock_ads(num):
    """"""Generates a list of mock AndroidDevice objects.

    The serial number of each device will be integer 0 through num - 1.

    Args:
        num: An integer that is the number of mock AndroidDevice objects to
            create.
    """"""
    ads = []
    for i in range(num):
        ad = mock.MagicMock(name=""AndroidDevice"", serial=str(i), h_port=None)
        ads.append(ad)
    return ads


def get_all_instances():
    return get_mock_ads(5)


def get_instances(serials):
    ads = []
    for serial in serials:
        ad = mock.MagicMock(name=""AndroidDevice"", serial=serial, h_port=None)
        ads.append(ad)
    return ads


def get_instances_with_configs(dicts):
    return get_instances([d['serial'] for d in dicts])


def list_adb_devices():
    return [ad.serial for ad in get_mock_ads(5)]


class MockAdbProxy(object):
    """"""Mock class that swaps out calls to adb with mock calls.""""""

    def __init__(self, serial, fail_br=False, fail_br_before_N=False):
        self.serial = serial
        self.fail_br = fail_br
        self.fail_br_before_N = fail_br_before_N

    def shell(self, params):
        if params == ""id -u"":
            return b""root""
        elif params == ""bugreportz"":
            if self.fail_br:
                return b""OMG I died!\n""
            return b'OK:/path/bugreport.zip\n'
        elif params == ""bugreportz -v"":
            if self.fail_br_before_N:
                return b""/system/bin/sh: bugreportz: not found""
            return b'1.1'

    def getprop(self, params):
        if params == ""ro.build.id"":
            return ""AB42""
        elif params == ""ro.build.type"":
            return ""userdebug""
        elif params == ""ro.build.product"" or params == ""ro.product.name"":
            return ""FakeModel""
        elif params == ""sys.boot_completed"":
            return ""1""

    def bugreport(self, args, shell=False):
        expected = os.path.join(
            logging.log_path,
            'AndroidDevice%s' % self.serial,
            'BugReports',
            'test_something,sometime,%s' % self.serial)
        if expected not in args:
            raise Error('""Expected ""%s"", got ""%s""' % (expected, args))

    def __getattr__(self, name):
        """"""All calls to the none-existent functions in adb proxy would
        simply return the adb command string.
        """"""
        def adb_call(*args):
            arg_str = ' '.join(str(elem) for elem in args)
            return arg_str

        return adb_call


class MockFastbootProxy(object):
    """"""Mock class that swaps out calls to adb with mock calls.""""""

    def __init__(self, serial):
        self.serial = serial

    def devices(self):
        return b""xxxx device\nyyyy device""

    def __getattr__(self, name):
        def fastboot_call(*args):
            arg_str = ' '.join(str(elem) for elem in args)
            return arg_str
        return fastboot_call
/n/n/n",0
119,119,3862e8ba359040fbdd6e1a6d36e51d07cda8e1ee,"/mobly/controllers/android_device_lib/adb.py/n/n#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from builtins import str

import logging
import random
import socket
import subprocess
import time


class AdbError(Exception):
    """"""Raised when there is an error in adb operations.""""""

    def __init__(self, cmd, stdout, stderr, ret_code):
        self.cmd = cmd
        self.stdout = stdout
        self.stderr = stderr
        self.ret_code = ret_code

    def __str__(self):
        return ('Error executing adb cmd ""%s"". ret: %d, stdout: %s, stderr: %s'
                ) % (self.cmd, self.ret_code, self.stdout, self.stderr)


def list_occupied_adb_ports():
    """"""Lists all the host ports occupied by adb forward.

    This is useful because adb will silently override the binding if an attempt
    to bind to a port already used by adb was made, instead of throwing binding
    error. So one should always check what ports adb is using before trying to
    bind to a port with adb.

    Returns:
        A list of integers representing occupied host ports.
    """"""
    out = AdbProxy().forward('--list')
    clean_lines = str(out, 'utf-8').strip().split('\n')
    used_ports = []
    for line in clean_lines:
        tokens = line.split(' tcp:')
        if len(tokens) != 3:
            continue
        used_ports.append(int(tokens[1]))
    return used_ports


class AdbProxy():
    """"""Proxy class for ADB.

    For syntactic reasons, the '-' in adb commands need to be replaced with
    '_'. Can directly execute adb commands on an object:
    >> adb = AdbProxy(<serial>)
    >> adb.start_server()
    >> adb.devices() # will return the console output of ""adb devices"".
    """"""

    def __init__(self, serial=''):
        self.serial = serial
        if serial:
            self.adb_str = 'adb -s %s' % serial
        else:
            self.adb_str = 'adb'

    def _exec_cmd(self, cmd):
        """"""Executes adb commands in a new shell.

        This is specific to executing adb binary because stderr is not a good
        indicator of cmd execution status.

        Args:
            cmds: A string that is the adb command to execute.

        Returns:
            The output of the adb command run if exit code is 0.

        Raises:
            AdbError is raised if the adb command exit code is not 0.
        """"""
        proc = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
        (out, err) = proc.communicate()
        ret = proc.returncode
        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s', cmd, out,
                      err, ret)
        if ret == 0:
            return out
        else:
            raise AdbError(cmd=cmd, stdout=out, stderr=err, ret_code=ret)

    def _exec_adb_cmd(self, name, arg_str):
        return self._exec_cmd(' '.join((self.adb_str, name, arg_str)))

    def tcp_forward(self, host_port, device_port):
        """"""Starts tcp forwarding.

        Args:
            host_port: Port number to use on the computer.
            device_port: Port number to use on the android device.
        """"""
        self.forward('tcp:%d tcp:%d' % (host_port, device_port))

    def getprop(self, prop_name):
        """"""Get a property of the device.

        This is a convenience wrapper for ""adb shell getprop xxx"".

        Args:
            prop_name: A string that is the name of the property to get.

        Returns:
            A string that is the value of the property, or None if the property
            doesn't exist.
        """"""
        return self.shell('getprop %s' % prop_name).decode('utf-8').strip()

    def __getattr__(self, name):
        def adb_call(*args):
            clean_name = name.replace('_', '-')
            arg_str = ' '.join(str(elem) for elem in args)
            return self._exec_adb_cmd(clean_name, arg_str)

        return adb_call
/n/n/n/mobly/controllers/android_device_lib/jsonrpc_client_base.py/n/n#/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Base class for clients that communicate with apps over a JSON RPC interface.

The JSON protocol expected by this module is:

Request:
{
    ""id"": <monotonically increasing integer containing the ID of this request>
    ""method"": <string containing the name of the method to execute>
    ""params"": <JSON array containing the arguments to the method>
}

Response:
{
    ""id"": <int id of request that this response maps to>,
    ""result"": <Arbitrary JSON object containing the result of executing the
               method. If the method could not be executed or returned void,
               contains 'null'.>,
    ""error"": <String containing the error thrown by executing the method.
              If no error occurred, contains 'null'.>
    ""callback"": <String that represents a callback ID used to identify events
                 associated with a particular CallbackHandler object.>
""""""

from builtins import str

import json
import logging
import socket
import threading
import time

from mobly.controllers.android_device_lib import adb
from mobly.controllers.android_device_lib import callback_handler

# Maximum time to wait for the app to start on the device.
APP_START_WAIT_TIME = 15

# UID of the 'unknown' jsonrpc session. Will cause creation of a new session.
UNKNOWN_UID = -1

# Maximum time to wait for the socket to open on the device.
_SOCKET_CONNECTION_TIMEOUT = 60

# Maximum time to wait for a response message on the socket.
_SOCKET_READ_TIMEOUT = callback_handler.MAX_TIMEOUT


class Error(Exception):
    pass


class AppStartError(Error):
    """"""Raised when the app is not able to be started.""""""


class ApiError(Error):
    """"""Raised when remote API reports an error.""""""


class ProtocolError(Error):
    """"""Raised when there is some error in exchanging data with server.""""""
    NO_RESPONSE_FROM_HANDSHAKE = 'No response from handshake.'
    NO_RESPONSE_FROM_SERVER = 'No response from server.'
    MISMATCHED_API_ID = 'Mismatched API id.'


class JsonRpcCommand(object):
    """"""Commands that can be invoked on all jsonrpc clients.

    INIT: Initializes a new session.
    CONTINUE: Creates a connection.
    """"""
    INIT = 'initiate'
    CONTINUE = 'continue'


class JsonRpcClientBase(object):
    """"""Base class for jsonrpc clients that connect to remote servers.

    Connects to a remote device running a jsonrpc-compatible app. Before opening
    a connection a port forward must be setup to go over usb. This be done using
    adb.tcp_forward(). This calls the shell command adb forward <local> remote>.
    Once the port has been forwarded it can be used in this object as the port
    of communication.

    Attributes:
        host_port: (int) The host port of this RPC client.
        device_port: (int) The device port of this RPC client.
        app_name: (str) The user-visible name of the app being communicated
                  with.
        uid: (int) The uid of this session.
    """"""

    def __init__(self,
                 host_port,
                 device_port,
                 app_name,
                 adb_proxy,
                 log=logging.getLogger()):
        """"""
        Args:
            host_port: (int) The host port of this RPC client.
            device_port: (int) The device port of this RPC client.
            app_name: (str) The user-visible name of the app being communicated
                      with.
            adb_proxy: (adb.AdbProxy) The adb proxy to use to start the app.
        """"""
        self.host_port = host_port
        self.device_port = device_port
        self.app_name = app_name
        self.uid = None
        self._adb = adb_proxy
        self._client = None  # prevent close errors on connect failure
        self._conn = None
        self._counter = None
        self._lock = threading.Lock()
        self._event_client = None
        self._log = log

    def __del__(self):
        self.close()

    # Methods to be implemented by subclasses.

    def _do_start_app(self):
        """"""Starts the server app on the android device.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    def _start_event_client(self):
        """"""Starts a separate JsonRpc client to the same session for propagating
        events.

        This is an optional function that should only implement if the client
        utilizes the snippet event mechanism.

        Returns:
            A JsonRpc Client object that connects to the same session as the
            one on which this function is called.
        """"""
        raise NotImplementedError()

    def stop_app(self):
        """"""Kills any running instance of the app.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    def check_app_installed(self):
        """"""Checks if app is installed.

        Must be implemented by subclasses.
        """"""
        raise NotImplementedError()

    # Rest of the client methods.

    def start_app(self, wait_time=APP_START_WAIT_TIME):
        """"""Starts the server app on the android device.

        Args:
            wait_time: float, The time to wait for the app to come up before
                       raising an error.

        Raises:
            AppStartError: When the app was not able to be started.
        """"""
        self.check_app_installed()
        self._do_start_app()
        for _ in range(wait_time):
            time.sleep(1)
            if self._is_app_running():
                self._log.debug('Successfully started %s', self.app_name)
                return
        raise AppStartError('%s failed to start on %s.' %
                            (self.app_name, self._adb.serial))

    def connect(self, uid=UNKNOWN_UID, cmd=JsonRpcCommand.INIT):
        """"""Opens a connection to a JSON RPC server.

        Opens a connection to a remote client. The connection attempt will time
        out if it takes longer than _SOCKET_CONNECTION_TIMEOUT seconds. Each
        subsequent operation over this socket will time out after
        _SOCKET_READ_TIMEOUT seconds as well.

        Args:
            uid: int, The uid of the session to join, or UNKNOWN_UID to start a
                 new session.
            cmd: JsonRpcCommand, The command to use for creating the connection.

        Raises:
            IOError: Raised when the socket times out from io error
            socket.timeout: Raised when the socket waits to long for connection.
            ProtocolError: Raised when there is an error in the protocol.
        """"""
        self._counter = self._id_counter()
        self._conn = socket.create_connection(('127.0.0.1', self.host_port),
                                              _SOCKET_CONNECTION_TIMEOUT)
        self._conn.settimeout(_SOCKET_READ_TIMEOUT)
        self._client = self._conn.makefile(mode='brw')

        resp = self._cmd(cmd, uid)
        if not resp:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_HANDSHAKE)
        result = json.loads(str(resp, encoding='utf8'))
        if result['status']:
            self.uid = result['uid']
        else:
            self.uid = UNKNOWN_UID

    def close(self):
        """"""Close the connection to the remote client.""""""
        if self._conn:
            self._conn.close()
            self._conn = None

    def _adb_grep_wrapper(self, adb_shell_cmd):
        """"""A wrapper for the specific usage of adb shell grep in this class.

        This surpresses AdbError if the grep fails to find anything.

        Args:
            adb_shell_cmd: A string that is an adb shell cmd with grep.

        Returns:
            The stdout of the grep result if the grep found something, False
            otherwise.
        """"""
        try:
            return self._adb.shell(adb_shell_cmd).decode('utf-8')
        except adb.AdbError as e:
            if (e.ret_code == 1) and (not e.stdout) and (not e.stderr):
                return False
            raise

    def _cmd(self, command, uid=None):
        """"""Send a command to the server.

        Args:
            command: str, The name of the command to execute.
            uid: int, the uid of the session to send the command to.

        Returns:
            The line that was written back.
        """"""
        if not uid:
            uid = self.uid
        self._client.write(
            json.dumps({
                'cmd': command,
                'uid': uid
            }).encode(""utf8"") + b'\n')
        self._client.flush()
        return self._client.readline()

    def _rpc(self, method, *args):
        """"""Sends an rpc to the app.

        Args:
            method: str, The name of the method to execute.
            args: any, The args of the method.

        Returns:
            The result of the rpc.

        Raises:
            ProtocolError: Something went wrong with the protocol.
            ApiError: The rpc went through, however executed with errors.
        """"""
        with self._lock:
            apiid = next(self._counter)
            data = {'id': apiid, 'method': method, 'params': args}
            request = json.dumps(data)
            self._client.write(request.encode(""utf8"") + b'\n')
            self._client.flush()
            response = self._client.readline()
        if not response:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_SERVER)
        result = json.loads(str(response, encoding=""utf8""))
        if result['error']:
            raise ApiError(result['error'])
        if result['id'] != apiid:
            raise ProtocolError(ProtocolError.MISMATCHED_API_ID)
        if result.get('callback') is not None:
            if self._event_client is None:
                self._event_client = self._start_event_client()
            return callback_handler.CallbackHandler(
                callback_id=result['callback'],
                event_client=self._event_client,
                ret_value=result['result'],
                method_name=method)
        return result['result']

    def _is_app_running(self):
        """"""Checks if the app is currently running on an android device.

        May be overridden by subclasses with custom sanity checks.
        """"""
        running = False
        try:
            self.connect()
            running = True
        finally:
            self.close()
            # This 'return' squashes exceptions from connect()
            return running

    def __getattr__(self, name):
        """"""Wrapper for python magic to turn method calls into RPC calls.""""""

        def rpc_call(*args):
            return self._rpc(name, *args)

        return rpc_call

    def _id_counter(self):
        i = 0
        while True:
            yield i
            i += 1
/n/n/n/tests/lib/mock_android_device.py/n/n#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This module has common mock objects and functions used in unit tests for
# mobly.controllers.android_device module.

import logging
import mock
import os


def get_mock_ads(num):
    """"""Generates a list of mock AndroidDevice objects.

    The serial number of each device will be integer 0 through num - 1.

    Args:
        num: An integer that is the number of mock AndroidDevice objects to
            create.
    """"""
    ads = []
    for i in range(num):
        ad = mock.MagicMock(name=""AndroidDevice"", serial=str(i), h_port=None)
        ads.append(ad)
    return ads


def get_all_instances():
    return get_mock_ads(5)


def get_instances(serials):
    ads = []
    for serial in serials:
        ad = mock.MagicMock(name=""AndroidDevice"", serial=serial, h_port=None)
        ads.append(ad)
    return ads


def get_instances_with_configs(dicts):
    return get_instances([d['serial'] for d in dicts])


def list_adb_devices():
    return [ad.serial for ad in get_mock_ads(5)]


class MockAdbProxy(object):
    """"""Mock class that swaps out calls to adb with mock calls.""""""

    def __init__(self, serial, fail_br=False, fail_br_before_N=False):
        self.serial = serial
        self.fail_br = fail_br
        self.fail_br_before_N = fail_br_before_N

    def shell(self, params):
        if params == ""id -u"":
            return b""root""
        elif params == ""bugreportz"":
            if self.fail_br:
                return b""OMG I died!\n""
            return b'OK:/path/bugreport.zip\n'
        elif params == ""bugreportz -v"":
            if self.fail_br_before_N:
                return b""/system/bin/sh: bugreportz: not found""
            return b'1.1'

    def getprop(self, params):
        if params == ""ro.build.id"":
            return ""AB42""
        elif params == ""ro.build.type"":
            return ""userdebug""
        elif params == ""ro.build.product"" or params == ""ro.product.name"":
            return ""FakeModel""
        elif params == ""sys.boot_completed"":
            return ""1""

    def bugreport(self, params):
        expected = os.path.join(logging.log_path,
                                ""AndroidDevice%s"" % self.serial, ""BugReports"",
                                ""test_something,sometime,%s"" % (self.serial))
        assert expected in params, ""Expected '%s', got '%s'."" % (expected,
                                                                 params)

    def __getattr__(self, name):
        """"""All calls to the none-existent functions in adb proxy would
        simply return the adb command string.
        """"""
        def adb_call(*args):
            arg_str = ' '.join(str(elem) for elem in args)
            return arg_str

        return adb_call


class MockFastbootProxy(object):
    """"""Mock class that swaps out calls to adb with mock calls.""""""

    def __init__(self, serial):
        self.serial = serial

    def devices(self):
        return b""xxxx device\nyyyy device""

    def __getattr__(self, name):
        def fastboot_call(*args):
            arg_str = ' '.join(str(elem) for elem in args)
            return arg_str
        return fastboot_call
/n/n/n",1
114,114,b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb,"repack_rust.py/n/n#!/bin/env python
'''
This script downloads and repacks official rust language builds
with the necessary tool and target support for the Firefox
build environment.
'''

import os.path
import requests
import subprocess
import toml

def fetch_file(url):
  '''Download a file from the given url if it's not already present.'''
  filename = os.path.basename(url)
  if os.path.exists(filename):
    return
  r = requests.get(url, stream=True)
  r.raise_for_status()
  with open(filename, 'wb') as fd:
    for chunk in r.iter_content(4096):
      fd.write(chunk)

def fetch(url):
  '''Download and verify a package url.'''
  base = os.path.basename(url)
  print('Fetching %s...' % base)
  fetch_file(url + '.asc')
  fetch_file(url)
  fetch_file(url + '.sha256')
  fetch_file(url + '.asc.sha256')
  print('Verifying %s...' % base)
  # TODO: check for verification failure.
  subprocess.check_call(['shasum', '-c', base + '.sha256'])
  subprocess.check_call(['shasum', '-c', base + '.asc.sha256'])
  subprocess.check_call(['gpg', '--verify', base + '.asc', base])
  subprocess.check_call(['keybase', 'verify', base + '.asc'])

def install(filename, target):
  '''Run a package's installer script against the given target directory.'''
  print(' Unpacking %s...' % filename)
  subprocess.check_call(['tar', 'xf', filename])
  basename = filename.split('.tar')[0]
  print(' Installing %s...' % basename)
  install_cmd = [os.path.join(basename, 'install.sh')]
  install_cmd += ['--prefix=' + os.path.abspath(target)]
  install_cmd += ['--disable-ldconfig']
  subprocess.check_call(install_cmd)
  print(' Cleaning %s...' % basename)
  subprocess.check_call(['rm', '-rf', basename])

def package(manifest, pkg, target):
  '''Pull out the package dict for a particular package and target
  from the given manifest.'''
  version = manifest['pkg'][pkg]['version']
  info = manifest['pkg'][pkg]['target'][target]
  return (version, info)

def repack(host, targets, channel='stable'):
  print(""Repacking rust for %s..."" % host)
  url = 'https://static.rust-lang.org/dist/channel-rust-' + channel + '.toml'
  req = requests.get(url)
  req.raise_for_status()
  manifest = toml.loads(req.content)
  if manifest['manifest-version'] != '2':
    print('ERROR: unrecognized manifest version %s.' % manifest['manifest-version'])
    return
  print('Using manifest for rust %s as of %s.' % (channel, manifest['date']))
  rustc_version, rustc = package(manifest, 'rustc', host)
  if rustc['available']:
    print('rustc %s\n  %s\n  %s' % (rustc_version, rustc['url'], rustc['hash']))
    fetch(rustc['url'])
  cargo_version, cargo = package(manifest, 'cargo', host)
  if cargo['available']:
    print('cargo %s\n  %s\n  %s' % (cargo_version, cargo['url'], cargo['hash']))
    fetch(cargo['url'])
  stds = []
  for target in targets:
      version, info = package(manifest, 'rust-std', target)
      if info['available']:
        print('rust-std %s\n  %s\n  %s' % (version, info['url'], info['hash']))
        fetch(info['url'])
        stds.append(info)
  print('Installing packages...')
  tar_basename = 'rustc-%s-repack' % host
  install_dir = 'rustc'
  subprocess.check_call(['rm', '-rf', install_dir])
  install(os.path.basename(rustc['url']), install_dir)
  install(os.path.basename(cargo['url']), install_dir)
  for std in stds:
    install(os.path.basename(std['url']), install_dir)
  print('Tarring %s...' % tar_basename)
  subprocess.check_call(['tar', 'cjf', tar_basename + '.tar.bz2', install_dir])
  subprocess.check_call(['rm', '-rf', install_dir])

# rust platform triples
android=""arm-linux-androideabi""
linux64=""x86_64-unknown-linux-gnu""
linux32=""i686-unknown-linux-gnu""
mac64=""x86_64-apple-darwin""
mac32=""i686-apple-darwin""
win64=""x86_64-pc-windows-msvc""
win32=""i686-pc-windows-msvc""

if __name__ == '__main__':
  repack(mac64, [mac64, mac32])
  repack(win32, [win32])
  repack(win64, [win64])
  repack(linux64, [linux64, linux32])

'''
install_rustc() {
  pkg=$(cat ${IDX} | grep ^rustc | grep $1)
  base=${pkg%%.tar.*}
  echo ""Installing $base...""
  tar xf ${pkg}
  ${base}/install.sh ${INSTALL_OPTS}
  rm -rf ${base}
}

install_std() {
  for arch in $@; do
    for pkg in $(cat ${IDX} | grep rust-std | grep $arch); do
      base=${pkg%%.tar.*}
      echo ""Installing $base...""
      tar xf ${pkg}
      ${base}/install.sh ${INSTALL_OPTS}
      rm -rf ${base}
    done
  done
}

check() {
  if test -x ${TARGET}/bin/rustc; then
    file ${TARGET}/bin/rustc
    ${TARGET}/bin/rustc --version
  elif test -x ${TARGET}/bin/rustc.exe; then
    file ${TARGET}/bin/rustc.exe
    ${TARGET}/bin/rustc.exe --version
  else
    die ""ERROR: Couldn't fine rustc executable""
  fi
  echo ""Installed components:""
  for component in $(cat ${TARGET}/lib/rustlib/components); do
    echo ""  $component""
  done
  echo
}

test -n ""$TASK_ID"" && set -v

linux64=""x86_64-unknown-linux-gnu""
linux32=""i686-unknown-linux-gnu""

android=""arm-linux-androideabi""

mac64=""x86_64-apple-darwin""
mac32=""i686-apple-darwin""

win64=""x86_64-pc-windows-msvc""
win32=""i686-pc-windows-msvc""
win32_i586=""i586-pc-windows-msvc""

# Fetch the manifest

IDX=channel-rustc-${RUST_CHANNEL}

fetch ${IDX}
verify ${IDX}

TARGET=rustc
INSTALL_OPTS=""--prefix=${PWD}/${TARGET} --disable-ldconfig""

# Repack the linux64 builds.
repack_linux64() {
  fetch_rustc $linux64
  fetch_std $linux64 $linux32

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $linux32

  tar cJf rustc-$linux64-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

# Repack the win64 builds.
repack_win64() {
  fetch_rustc $win64
  fetch_std $win64

  rm -rf ${TARGET}

  install_rustc $win64
  install_std $win64

  tar cjf rustc-$win64-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the win32 builds.
repack_win32() {
  fetch_rustc $win32
  fetch_std $win32

  rm -rf ${TARGET}

  install_rustc $win32
  install_std $win32

  tar cjf rustc-$win32-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the mac builds.
repack_mac() {
  fetch_rustc $mac64
  fetch_std $mac64 $mac32

  rm -rf ${TARGET}

  install_rustc $mac64
  install_std $mac64 $mac32

  tar cjf rustc-mac-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack mac cross build.
repack_mac_cross() {
  fetch_rustc $linux64
  fetch_std $linux64

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $mac64 $mac32

  tar cJf rustc-mac-cross-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

repack_win32
repack_win64
repack_linux64
repack_mac
repack_mac_cross

rm -rf ${TARGET}
'''
/n/n/n",0
115,115,b8af51e5811fcb35eff9e1e3e91c98490e7a7dcb,"/repack_rust.py/n/n#!/bin/env python
'''
This script downloads and repacks official rust language builds
with the necessary tool and target support for the Firefox
build environment.
'''

import requests
import toml
import os

def fetch_file(url):
  '''Download a file from the given url if it's not already present.'''
  filename = os.path.basename(url)
  if os.path.exists(filename):
    return
  r = requests.get(url, stream=True)
  r.raise_for_status()
  with open(filename, 'wb') as fd:
    for chunk in r.iter_content(4096):
      fd.write(chunk)

def fetch(url):
  '''Download and verify a package url.'''
  base = os.path.basename(url)
  print('Fetching %s...' % base)
  fetch_file(url + '.asc')
  fetch_file(url)
  fetch_file(url + '.sha256')
  fetch_file(url + '.asc.sha256')
  print('Verifying %s...' % base)
  # TODO: check for verification failure.
  os.system('shasum -c %s.sha256' % base)
  os.system('shasum -c %s.asc.sha256' % base)
  os.system('gpg --verify %s.asc %s' % (base, base))
  os.system('keybase verify %s.asc' % base)

def install(filename, target):
  '''Run a package's installer script against the given target directory.'''
  print(' Unpacking %s...' % filename)
  os.system('tar xf ' + filename)
  basename = filename.split('.tar')[0]
  print(' Installing %s...' % basename)
  install_opts = '--prefix=${PWD}/%s --disable-ldconfig' % target
  os.system('%s/install.sh %s' % (basename, install_opts))
  print(' Cleaning %s...' % basename)
  os.system('rm -rf %s' % basename)

def package(manifest, pkg, target):
  '''Pull out the package dict for a particular package and target
  from the given manifest.'''
  version = manifest['pkg'][pkg]['version']
  info = manifest['pkg'][pkg]['target'][target]
  return (version, info)

def repack(host, targets, channel='stable'):
  url = 'https://static.rust-lang.org/dist/channel-rust-' + channel + '.toml'
  req = requests.get(url)
  req.raise_for_status()
  manifest = toml.loads(req.content)
  if manifest['manifest-version'] != '2':
    print('ERROR: unrecognized manifest version %s.' % manifest['manifest-version'])
    return
  print('Using manifest for rust %s as of %s.' % (channel, manifest['date']))
  rustc_version, rustc = package(manifest, 'rustc', host)
  if rustc['available']:
    print('rustc %s\n  %s\n  %s' % (rustc_version, rustc['url'], rustc['hash']))
    fetch(rustc['url'])
  cargo_version, cargo = package(manifest, 'cargo', host)
  if cargo['available']:
    print('cargo %s\n  %s\n  %s' % (cargo_version, cargo['url'], cargo['hash']))
    fetch(cargo['url'])
  stds = []
  for target in targets:
      version, info = package(manifest, 'rust-std', target)
      if info['available']:
        print('rust-std %s\n  %s\n  %s' % (version, info['url'], info['hash']))
        fetch(info['url'])
        stds.append(info)
  print('Installing packages...')
  tar_basename = 'rustc-%s-repack' % host
  install_dir = 'rustc'
  os.system('rm -rf %s' % install_dir)
  install(os.path.basename(rustc['url']), install_dir)
  install(os.path.basename(cargo['url']), install_dir)
  for std in stds:
    install(os.path.basename(std['url']), install_dir)
  print('Tarring %s...' % tar_basename)
  os.system('tar cjf %s.tar.bz2 %s/*' % (tar_basename, install_dir))
  os.system('rm -rf %s' % install_dir)

# rust platform triples
android=""arm-linux-androideabi""
linux64=""x86_64-unknown-linux-gnu""
linux32=""i686-unknown-linux-gnu""
mac64=""x86_64-apple-darwin""
mac32=""i686-apple-darwin""
win64=""x86_64-pc-windows-msvc""
win32=""i686-pc-windows-msvc""

if __name__ == '__main__':
  repack(mac64, [mac64, mac32])
  repack(win32, [win32])
  repack(win64, [win64])
  repack(linux64, [linux64, linux32])

'''
install_rustc() {
  pkg=$(cat ${IDX} | grep ^rustc | grep $1)
  base=${pkg%%.tar.*}
  echo ""Installing $base...""
  tar xf ${pkg}
  ${base}/install.sh ${INSTALL_OPTS}
  rm -rf ${base}
}

install_std() {
  for arch in $@; do
    for pkg in $(cat ${IDX} | grep rust-std | grep $arch); do
      base=${pkg%%.tar.*}
      echo ""Installing $base...""
      tar xf ${pkg}
      ${base}/install.sh ${INSTALL_OPTS}
      rm -rf ${base}
    done
  done
}

check() {
  if test -x ${TARGET}/bin/rustc; then
    file ${TARGET}/bin/rustc
    ${TARGET}/bin/rustc --version
  elif test -x ${TARGET}/bin/rustc.exe; then
    file ${TARGET}/bin/rustc.exe
    ${TARGET}/bin/rustc.exe --version
  else
    die ""ERROR: Couldn't fine rustc executable""
  fi
  echo ""Installed components:""
  for component in $(cat ${TARGET}/lib/rustlib/components); do
    echo ""  $component""
  done
  echo
}

test -n ""$TASK_ID"" && set -v

linux64=""x86_64-unknown-linux-gnu""
linux32=""i686-unknown-linux-gnu""

android=""arm-linux-androideabi""

mac64=""x86_64-apple-darwin""
mac32=""i686-apple-darwin""

win64=""x86_64-pc-windows-msvc""
win32=""i686-pc-windows-msvc""
win32_i586=""i586-pc-windows-msvc""

# Fetch the manifest

IDX=channel-rustc-${RUST_CHANNEL}

fetch ${IDX}
verify ${IDX}

TARGET=rustc
INSTALL_OPTS=""--prefix=${PWD}/${TARGET} --disable-ldconfig""

# Repack the linux64 builds.
repack_linux64() {
  fetch_rustc $linux64
  fetch_std $linux64 $linux32

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $linux32

  tar cJf rustc-$linux64-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

# Repack the win64 builds.
repack_win64() {
  fetch_rustc $win64
  fetch_std $win64

  rm -rf ${TARGET}

  install_rustc $win64
  install_std $win64

  tar cjf rustc-$win64-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the win32 builds.
repack_win32() {
  fetch_rustc $win32
  fetch_std $win32

  rm -rf ${TARGET}

  install_rustc $win32
  install_std $win32

  tar cjf rustc-$win32-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the mac builds.
repack_mac() {
  fetch_rustc $mac64
  fetch_std $mac64 $mac32

  rm -rf ${TARGET}

  install_rustc $mac64
  install_std $mac64 $mac32

  tar cjf rustc-mac-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack mac cross build.
repack_mac_cross() {
  fetch_rustc $linux64
  fetch_std $linux64

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $mac64 $mac32

  tar cJf rustc-mac-cross-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

repack_win32
repack_win64
repack_linux64
repack_mac
repack_mac_cross

rm -rf ${TARGET}
'''
/n/n/n",1
126,126,9115beba55f93e4822e530372cc972ffa86c0245,"src/pdm/workqueue/Worker.py/n/n#!/usr/bin/env python
""""""Worker script.""""""
import os
import time
import uuid
import random
# import json
# import shlex
import socket
import subprocess
from urlparse import urlsplit, urlunsplit
from tempfile import NamedTemporaryFile
from contextlib import contextmanager

from requests.exceptions import Timeout

from pdm.framework.RESTClient import RESTClient, RESTException
from pdm.cred.CredClient import CredClient
from pdm.endpoint.EndpointClient import EndpointClient
from pdm.utils.daemon import Daemon
from pdm.utils.config import getConfig

from .WorkqueueDB import COMMANDMAP, PROTOCOLMAP, JobType


@contextmanager
def TempX509Files(token):
    """"""Create temporary grid credential files.""""""
    cert, key = CredClient().get_cred(token)
    with NamedTemporaryFile() as proxyfile:
        proxyfile.write(key)
        proxyfile.write(cert)
        proxyfile.flush()
        os.fsync(proxyfile.fileno())
        yield proxyfile


class Worker(RESTClient, Daemon):
    """"""Worker Daemon.""""""

    def __init__(self, debug=False, one_shot=False):
        """"""Initialisation.""""""
        RESTClient.__init__(self, 'workqueue')
        conf = getConfig('worker')
        self._uid = uuid.uuid4()
        Daemon.__init__(self,
                        pidfile='/tmp/worker-%s.pid' % self._uid,
                        logfile='/tmp/worker-%s.log' % self._uid,
                        target=self.run,
                        debug=debug)
        self._one_shot = one_shot
        self._types = [JobType[type_.upper()] for type_ in  # pylint: disable=unsubscriptable-object
                       conf.pop('types', ('LIST', 'COPY', 'REMOVE'))]
        self._interpoll_sleep_time = conf.pop('poll_time', 2)
        self._script_path = conf.pop('script_path',
                                     os.path.join(os.path.dirname(__file__), 'scripts'))
        self._script_path = os.path.abspath(self._script_path)
        self._logger.info(""Script search path is: %r"", self._script_path)
        self._current_process = None

        # Check for unused config options
        if conf:
            raise ValueError(""Unused worker config params: '%s'"" % ', '.join(conf.keys()))

    def terminate(self, *_):
        """"""Terminate worker daemon.""""""
        Daemon.terminate(self, *_)
        if self._current_process is not None:
            self._current_process.terminate()

    def _abort(self, job_id, message):
        """"""Abort job cycle.""""""
        self._logger.error(""Error with job %d: %s"", job_id, message)
        try:
            self.put('worker/%s' % job_id,
                     data={'log': message,
                           'returncode': 1,
                           'host': socket.gethostbyaddr(socket.getfqdn())})
        except RESTException:
            self._logger.exception(""Error trying to PUT back abort message"")

    def run(self):
        """"""Daemon main method.""""""
        endpoint_client = EndpointClient()
        run = True
        while run:
            if self._one_shot:
                run = False
            try:
                response = self.post('worker', data={'types': self._types})
            except Timeout:
                self._logger.warning(""Timed out contacting the WorkqueueService."")
                continue
            except RESTException as err:
                if err.code == 404:
                    self._logger.debug(""No work to pick up."")
                    time.sleep(self._interpoll_sleep_time)
                else:
                    self._logger.exception(""Error trying to get job from WorkqueueService."")
                continue
            job, token = response
#            try:
#                job, token = json.loads(response.data())
#            except ValueError:
#                self._logger.exception(""Error decoding JSON job."")
#                continue
            src_site = endpoint_client.get_site(job['src_siteid'])
            src_endpoints = [urlsplit(site) for site
                             in src_site['endpoints'].itervalues()]
            src = [urlunsplit(site._replace(path=job['src_filepath'])) for site in src_endpoints
                   if site.scheme == PROTOCOLMAP[job['protocol']]]
            if not src:
                self._abort(job['id'], ""Protocol '%s' not supported at src site with id %d""
                            % (job['protocol'], job['src_siteid']))
                continue
            script_env = dict(os.environ,
                              PATH=self._script_path,
                              SRC_PATH=random.choice(src))

            if job['type'] == JobType.COPY:
                if job['dst_siteid'] is None:
                    self._abort(job['id'], ""No dst site id set for copy operation"")
                    continue
                if job['dst_filepath'] is None:
                    self._abort(job['id'], ""No dst site filepath set for copy operation"")
                    continue

                dst_site = endpoint_client.get_site(job['dst_siteid'])
                dst_endpoints = [urlsplit(site) for site
                                 in dst_site['endpoints'].itervalues()]
                dst = [urlunsplit(site._replace(path=job['dst_filepath'])) for site in dst_endpoints
                       if site.scheme == PROTOCOLMAP[job['protocol']]]
                if not dst:
                    self._abort(job['id'], ""Protocol '%s' not supported at dst site with id %d""
                                % (job['protocol'], job['dst_siteid']))
                    continue
                script_env['DST_PATH'] = random.choice(dst)

            command = COMMANDMAP[job['type']][job['protocol']]
            with TempX509Files(job['credentials']) as proxyfile:
                script_env['X509_USER_PROXY'] = proxyfile.name
                self._current_process = subprocess.Popen('(set -x && %s)' % command,
                                                         shell=True,
                                                         stdout=subprocess.PIPE,
                                                         stderr=subprocess.STDOUT,
                                                         env=script_env)
                log, _ = self._current_process.communicate()
                self.set_token(token)
                try:
                    self.put('worker/%s' % job['id'],
                             data={'log': log,
                                   'returncode': self._current_process.returncode,
                                   'host': socket.gethostbyaddr(socket.getfqdn())})
                except RESTException:
                    self._logger.exception(""Error trying to PUT back output from subcommand."")
                finally:
                    self.set_token(None)

/n/n/n",0
127,127,9115beba55f93e4822e530372cc972ffa86c0245,"/src/pdm/workqueue/Worker.py/n/n#!/usr/bin/env python
""""""Worker script.""""""
import os
import time
import uuid
import random
# import json
# import shlex
import socket
import subprocess
from urlparse import urlsplit, urlunsplit
from tempfile import NamedTemporaryFile
from contextlib import contextmanager

from requests.exceptions import Timeout

from pdm.framework.RESTClient import RESTClient, RESTException
from pdm.cred.CredClient import CredClient
from pdm.endpoint.EndpointClient import EndpointClient
from pdm.utils.daemon import Daemon
from pdm.utils.config import getConfig

from .WorkqueueDB import COMMANDMAP, PROTOCOLMAP, JobType


@contextmanager
def TempX509Files(token):
    """"""Create temporary grid credential files.""""""
    cert, key = CredClient().get_cred(token)
    with NamedTemporaryFile() as proxyfile:
        proxyfile.write(key)
        proxyfile.write(cert)
        proxyfile.flush()
        os.fsync(proxyfile.fileno())
        yield proxyfile


class Worker(RESTClient, Daemon):
    """"""Worker Daemon.""""""

    def __init__(self, debug=False, one_shot=False):
        """"""Initialisation.""""""
        RESTClient.__init__(self, 'workqueue')
        conf = getConfig('worker')
        self._uid = uuid.uuid4()
        Daemon.__init__(self,
                        pidfile='/tmp/worker-%s.pid' % self._uid,
                        logfile='/tmp/worker-%s.log' % self._uid,
                        target=self.run,
                        debug=debug)
        self._one_shot = one_shot
        self._types = [JobType[type_.upper()] for type_ in  # pylint: disable=unsubscriptable-object
                       conf.pop('types', ('LIST', 'COPY', 'REMOVE'))]
        self._interpoll_sleep_time = conf.pop('poll_time', 2)
        self._script_path = conf.pop('script_path', None)
        if self._script_path:
            self._script_path = os.path.abspath(self._script_path)
        else:
            code_path = os.path.abspath(os.path.dirname(__file__))
            self._script_path = os.path.join(code_path, 'scripts')
        self._logger.info(""Script search path is: %s"", self._script_path)
        self._current_process = None
        # Check for unused config options
        if conf:
            keys = ', '.join(conf.keys())
            raise ValueError(""Unused worker config params: '%s'"" % keys)

    def terminate(self, *_):
        """"""Terminate worker daemon.""""""
        Daemon.terminate(self, *_)
        if self._current_process is not None:
            self._current_process.terminate()

    def _abort(self, job_id, message):
        """"""Abort job cycle.""""""
        self._logger.error(""Error with job %d: %s"", job_id, message)
        try:
            self.put('worker/%s' % job_id,
                     data={'log': message,
                           'returncode': 1,
                           'host': socket.gethostbyaddr(socket.getfqdn())})
        except RESTException:
            self._logger.exception(""Error trying to PUT back abort message"")

    def run(self):
        """"""Daemon main method.""""""
        endpoint_client = EndpointClient()
        run = True
        while run:
            if self._one_shot:
                run = False
            try:
                response = self.post('worker', data={'types': self._types})
            except Timeout:
                self._logger.warning(""Timed out contacting the WorkqueueService."")
                continue
            except RESTException as err:
                if err.code == 404:
                    self._logger.debug(""No work to pick up."")
                    time.sleep(self._interpoll_sleep_time)
                else:
                    self._logger.exception(""Error trying to get job from WorkqueueService."")
                continue
            job, token = response
#            try:
#                job, token = json.loads(response.data())
#            except ValueError:
#                self._logger.exception(""Error decoding JSON job."")
#                continue
            src_site = endpoint_client.get_site(job['src_siteid'])
            src_endpoints = [urlsplit(site) for site
                             in src_site['endpoints'].itervalues()]
            src = [urlunsplit(site._replace(path=job['src_filepath'])) for site in src_endpoints
                   if site.scheme == PROTOCOLMAP[job['protocol']]]
            if not src:
                self._abort(job['id'], ""Protocol '%s' not supported at src site with id %d""
                            % (job['protocol'], job['src_siteid']))
                continue
            command = ""%s %s"" % (COMMANDMAP[job['type']][job['protocol']], random.choice(src))

            if job['type'] == JobType.COPY:
                if job['dst_siteid'] is None:
                    self._abort(job['id'], ""No dst site id set for copy operation"")
                    continue
                if job['dst_filepath'] is None:
                    self._abort(job['id'], ""No dst site filepath set for copy operation"")
                    continue

                dst_site = endpoint_client.get_site(job['dst_siteid'])
                dst_endpoints = [urlsplit(site) for site
                                 in dst_site['endpoints'].itervalues()]
                dst = [urlunsplit(site._replace(path=job['dst_filepath'])) for site in dst_endpoints
                       if site.scheme == PROTOCOLMAP[job['protocol']]]
                if not dst:
                    self._abort(job['id'], ""Protocol '%s' not supported at dst site with id %d""
                                % (job['protocol'], job['dst_siteid']))
                    continue
                command += "" %s"" % random.choice(dst)

            with TempX509Files(job['credentials']) as proxyfile:
                self._current_process = subprocess.Popen('(set -x && %s)' % command,
                                                         shell=True,
                                                         stdout=subprocess.PIPE,
                                                         stderr=subprocess.STDOUT,
                                                         env=dict(os.environ,
                                                                  PATH=self._script_path,
                                                                  X509_USER_PROXY=proxyfile.name))
                log, _ = self._current_process.communicate()
                self.set_token(token)
                try:
                    self.put('worker/%s' % job['id'],
                             data={'log': log,
                                   'returncode': self._current_process.returncode,
                                   'host': socket.gethostbyaddr(socket.getfqdn())})
                except RESTException:
                    self._logger.exception(""Error trying to PUT back output from subcommand."")
                finally:
                    self.set_token(None)

/n/n/n",1
62,62,f752302d181583a95cf44354aea607ce9d9283f4,"cinder/exception.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Cinder base exception handling.

Includes decorator for re-raising Cinder-type exceptions.

SHOULD include dedicated exception logging.

""""""

import sys

from oslo.config import cfg
import webob.exc

from cinder.openstack.common import exception as com_exception
from cinder.openstack.common import log as logging


LOG = logging.getLogger(__name__)

exc_log_opts = [
    cfg.BoolOpt('fatal_exception_format_errors',
                default=False,
                help='make exception message format errors fatal'),
]

CONF = cfg.CONF
CONF.register_opts(exc_log_opts)


class ConvertedException(webob.exc.WSGIHTTPException):
    def __init__(self, code=0, title="""", explanation=""""):
        self.code = code
        self.title = title
        self.explanation = explanation
        super(ConvertedException, self).__init__()


class ProcessExecutionError(IOError):
    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,
                 description=None):
        self.exit_code = exit_code
        self.stderr = stderr
        self.stdout = stdout
        self.cmd = cmd
        self.description = description

        if description is None:
            description = _('Unexpected error while running command.')
        if exit_code is None:
            exit_code = '-'
        message = _('%(description)s\nCommand: %(cmd)s\n'
                    'Exit code: %(exit_code)s\nStdout: %(stdout)r\n'
                    'Stderr: %(stderr)r') % {
                        'description': description,
                        'cmd': cmd,
                        'exit_code': exit_code,
                        'stdout': stdout,
                        'stderr': stderr,
                    }
        IOError.__init__(self, message)


Error = com_exception.Error


class CinderException(Exception):
    """"""Base Cinder Exception

    To correctly use this class, inherit from it and define
    a 'message' property. That message will get printf'd
    with the keyword arguments provided to the constructor.

    """"""
    message = _(""An unknown exception occurred."")
    code = 500
    headers = {}
    safe = False

    def __init__(self, message=None, **kwargs):
        self.kwargs = kwargs

        if 'code' not in self.kwargs:
            try:
                self.kwargs['code'] = self.code
            except AttributeError:
                pass

        if not message:
            try:
                message = self.message % kwargs

            except Exception:
                exc_info = sys.exc_info()
                # kwargs doesn't match a variable in the message
                # log the issue and the kwargs
                LOG.exception(_('Exception in string format operation'))
                for name, value in kwargs.iteritems():
                    LOG.error(""%s: %s"" % (name, value))
                if CONF.fatal_exception_format_errors:
                    raise exc_info[0], exc_info[1], exc_info[2]
                # at least get the core message out if something happened
                message = self.message

        super(CinderException, self).__init__(message)


class GlanceConnectionFailed(CinderException):
    message = _(""Connection to glance failed"") + "": %(reason)s""


class NotAuthorized(CinderException):
    message = _(""Not authorized."")
    code = 403


class AdminRequired(NotAuthorized):
    message = _(""User does not have admin privileges"")


class PolicyNotAuthorized(NotAuthorized):
    message = _(""Policy doesn't allow %(action)s to be performed."")


class ImageNotAuthorized(CinderException):
    message = _(""Not authorized for image %(image_id)s."")


class Invalid(CinderException):
    message = _(""Unacceptable parameters."")
    code = 400


class InvalidSnapshot(Invalid):
    message = _(""Invalid snapshot"") + "": %(reason)s""


class InvalidSourceVolume(Invalid):
    message = _(""Invalid source volume %(reason)s."")


class VolumeAttached(Invalid):
    message = _(""Volume %(volume_id)s is still attached, detach volume first."")


class SfJsonEncodeFailure(CinderException):
    message = _(""Failed to load data into json format"")


class InvalidRequest(Invalid):
    message = _(""The request is invalid."")


class InvalidResults(Invalid):
    message = _(""The results are invalid."")


class InvalidInput(Invalid):
    message = _(""Invalid input received"") + "": %(reason)s""


class InvalidVolumeType(Invalid):
    message = _(""Invalid volume type"") + "": %(reason)s""


class InvalidVolume(Invalid):
    message = _(""Invalid volume"") + "": %(reason)s""


class InvalidContentType(Invalid):
    message = _(""Invalid content type %(content_type)s."")


class InvalidHost(Invalid):
    message = _(""Invalid host"") + "": %(reason)s""


# Cannot be templated as the error syntax varies.
# msg needs to be constructed when raised.
class InvalidParameterValue(Invalid):
    message = _(""%(err)s"")


class InvalidAuthKey(Invalid):
    message = _(""Invalid auth key"") + "": %(reason)s""


class ServiceUnavailable(Invalid):
    message = _(""Service is unavailable at this time."")


class ImageUnacceptable(Invalid):
    message = _(""Image %(image_id)s is unacceptable: %(reason)s"")


class DeviceUnavailable(Invalid):
    message = _(""The device in the path %(path)s is unavailable: %(reason)s"")


class InvalidUUID(Invalid):
    message = _(""Expected a uuid but received %(uuid)s."")


class NotFound(CinderException):
    message = _(""Resource could not be found."")
    code = 404
    safe = True


class PersistentVolumeFileNotFound(NotFound):
    message = _(""Volume %(volume_id)s persistence file could not be found."")


class VolumeNotFound(NotFound):
    message = _(""Volume %(volume_id)s could not be found."")


class SfAccountNotFound(NotFound):
    message = _(""Unable to locate account %(account_name)s on ""
                ""Solidfire device"")


class VolumeNotFoundForInstance(VolumeNotFound):
    message = _(""Volume not found for instance %(instance_id)s."")


class VolumeMetadataNotFound(NotFound):
    message = _(""Volume %(volume_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidVolumeMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidVolumeMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class SnapshotMetadataNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidSnapshotMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidSnapshotMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class VolumeTypeNotFound(NotFound):
    message = _(""Volume type %(volume_type_id)s could not be found."")


class VolumeTypeNotFoundByName(VolumeTypeNotFound):
    message = _(""Volume type with name %(volume_type_name)s ""
                ""could not be found."")


class VolumeTypeExtraSpecsNotFound(NotFound):
    message = _(""Volume Type %(volume_type_id)s has no extra specs with ""
                ""key %(extra_specs_key)s."")


class SnapshotNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s could not be found."")


class VolumeIsBusy(CinderException):
    message = _(""deleting volume %(volume_name)s that has snapshot"")


class SnapshotIsBusy(CinderException):
    message = _(""deleting snapshot %(snapshot_name)s that has ""
                ""dependent volumes"")


class ISCSITargetNotFoundForVolume(NotFound):
    message = _(""No target id found for volume %(volume_id)s."")


class ISCSITargetCreateFailed(CinderException):
    message = _(""Failed to create iscsi target for volume %(volume_id)s."")


class ISCSITargetAttachFailed(CinderException):
    message = _(""Failed to attach iSCSI target for volume %(volume_id)s."")


class ISCSITargetRemoveFailed(CinderException):
    message = _(""Failed to remove iscsi target for volume %(volume_id)s."")


class DiskNotFound(NotFound):
    message = _(""No disk at %(location)s"")


class InvalidImageRef(Invalid):
    message = _(""Invalid image href %(image_href)s."")


class ImageNotFound(NotFound):
    message = _(""Image %(image_id)s could not be found."")


class ServiceNotFound(NotFound):
    message = _(""Service %(service_id)s could not be found."")


class HostNotFound(NotFound):
    message = _(""Host %(host)s could not be found."")


class SchedulerHostFilterNotFound(NotFound):
    message = _(""Scheduler Host Filter %(filter_name)s could not be found."")


class SchedulerHostWeigherNotFound(NotFound):
    message = _(""Scheduler Host Weigher %(weigher_name)s could not be found."")


class HostBinaryNotFound(NotFound):
    message = _(""Could not find binary %(binary)s on host %(host)s."")


class InvalidReservationExpiration(Invalid):
    message = _(""Invalid reservation expiration %(expire)s."")


class InvalidQuotaValue(Invalid):
    message = _(""Change would make usage less than 0 for the following ""
                ""resources: %(unders)s"")


class QuotaNotFound(NotFound):
    message = _(""Quota could not be found"")


class QuotaResourceUnknown(QuotaNotFound):
    message = _(""Unknown quota resources %(unknown)s."")


class ProjectQuotaNotFound(QuotaNotFound):
    message = _(""Quota for project %(project_id)s could not be found."")


class QuotaClassNotFound(QuotaNotFound):
    message = _(""Quota class %(class_name)s could not be found."")


class QuotaUsageNotFound(QuotaNotFound):
    message = _(""Quota usage for project %(project_id)s could not be found."")


class ReservationNotFound(QuotaNotFound):
    message = _(""Quota reservation %(uuid)s could not be found."")


class OverQuota(CinderException):
    message = _(""Quota exceeded for resources: %(overs)s"")


class MigrationNotFound(NotFound):
    message = _(""Migration %(migration_id)s could not be found."")


class MigrationNotFoundByStatus(MigrationNotFound):
    message = _(""Migration not found for instance %(instance_id)s ""
                ""with status %(status)s."")


class FileNotFound(NotFound):
    message = _(""File %(file_path)s could not be found."")


class ClassNotFound(NotFound):
    message = _(""Class %(class_name)s could not be found: %(exception)s"")


class NotAllowed(CinderException):
    message = _(""Action not allowed."")


#TODO(bcwaldon): EOL this exception!
class Duplicate(CinderException):
    pass


class KeyPairExists(Duplicate):
    message = _(""Key pair %(key_name)s already exists."")


class VolumeTypeExists(Duplicate):
    message = _(""Volume Type %(id)s already exists."")


class MigrationError(CinderException):
    message = _(""Migration error"") + "": %(reason)s""


class MalformedRequestBody(CinderException):
    message = _(""Malformed message body: %(reason)s"")


class ConfigNotFound(NotFound):
    message = _(""Could not find config at %(path)s"")


class ParameterNotFound(NotFound):
    message = _(""Could not find parameter %(param)s"")


class PasteAppNotFound(NotFound):
    message = _(""Could not load paste app '%(name)s' from %(path)s"")


class NoValidHost(CinderException):
    message = _(""No valid host was found. %(reason)s"")


class WillNotSchedule(CinderException):
    message = _(""Host %(host)s is not up or doesn't exist."")


class QuotaError(CinderException):
    message = _(""Quota exceeded"") + "": code=%(code)s""
    code = 413
    headers = {'Retry-After': 0}
    safe = True


class VolumeSizeExceedsAvailableQuota(QuotaError):
    message = _(""Requested volume or snapshot exceeds ""
                ""allowed Gigabytes quota"")


class VolumeSizeExceedsQuota(QuotaError):
    message = _(""Maximum volume/snapshot size exceeded"")


class VolumeLimitExceeded(QuotaError):
    message = _(""Maximum number of volumes allowed (%(allowed)d) exceeded"")


class SnapshotLimitExceeded(QuotaError):
    message = _(""Maximum number of snapshots allowed (%(allowed)d) exceeded"")


class DuplicateSfVolumeNames(Duplicate):
    message = _(""Detected more than one volume with name %(vol_name)s"")


class Duplicate3PARHost(CinderException):
    message = _(""3PAR Host already exists: %(err)s.  %(info)s"")


class Invalid3PARDomain(CinderException):
    message = _(""Invalid 3PAR Domain: %(err)s"")


class VolumeTypeCreateFailed(CinderException):
    message = _(""Cannot create volume_type with ""
                ""name %(name)s and specs %(extra_specs)s"")


class SolidFireAPIException(CinderException):
    message = _(""Bad response from SolidFire API"")


class SolidFireAPIDataException(SolidFireAPIException):
    message = _(""Error in SolidFire API response: data=%(data)s"")


class UnknownCmd(Invalid):
    message = _(""Unknown or unsupported command %(cmd)s"")


class MalformedResponse(Invalid):
    message = _(""Malformed response to command %(cmd)s: %(reason)s"")


class BadHTTPResponseStatus(CinderException):
    message = _(""Bad HTTP response status %(status)s"")


class FailedCmdWithDump(CinderException):
    message = _(""Operation failed with status=%(status)s. Full dump: %(data)s"")


class ZadaraServerCreateFailure(CinderException):
    message = _(""Unable to create server object for initiator %(name)s"")


class ZadaraServerNotFound(NotFound):
    message = _(""Unable to find server object for initiator %(name)s"")


class ZadaraVPSANoActiveController(CinderException):
    message = _(""Unable to find any active VPSA controller"")


class ZadaraAttachmentsNotFound(NotFound):
    message = _(""Failed to retrieve attachments for volume %(name)s"")


class ZadaraInvalidAttachmentInfo(Invalid):
    message = _(""Invalid attachment info for volume %(name)s: %(reason)s"")


class InstanceNotFound(NotFound):
    message = _(""Instance %(instance_id)s could not be found."")


class VolumeBackendAPIException(CinderException):
    message = _(""Bad or unexpected response from the storage volume ""
                ""backend API: %(data)s"")


class NfsException(CinderException):
    message = _(""Unknown NFS exception"")


class NfsNoSharesMounted(NotFound):
    message = _(""No mounted NFS shares found"")


class NfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlusterfsException(CinderException):
    message = _(""Unknown Gluster exception"")


class GlusterfsNoSharesMounted(NotFound):
    message = _(""No mounted Gluster shares found"")


class GlusterfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlanceMetadataExists(Invalid):
    message = _(""Glance metadata cannot be updated, key %(key)s""
                "" exists for volume id %(volume_id)s"")


class ImageCopyFailure(Invalid):
    message = _(""Failed to copy image to volume: %(reason)s"")


class BackupInvalidCephArgs(Invalid):
    message = _(""Invalid Ceph args provided for backup rbd operation"")


class BackupOperationError(Invalid):
    message = _(""An error has occurred during backup operation"")


class BackupRBDOperationFailed(Invalid):
    message = _(""Backup RBD operation failed"")


class BackupVolumeInvalidType(Invalid):
    message = _(""Backup volume %(volume_id)s type not recognised."")


class BackupNotFound(NotFound):
    message = _(""Backup %(backup_id)s could not be found."")


class InvalidBackup(Invalid):
    message = _(""Invalid backup: %(reason)s"")


class SwiftConnectionFailed(CinderException):
    message = _(""Connection to swift failed"") + "": %(reason)s""


class TransferNotFound(NotFound):
    message = _(""Transfer %(transfer_id)s could not be found."")


class VolumeMigrationFailed(CinderException):
    message = _(""Volume migration failed"") + "": %(reason)s""


class ProtocolNotSupported(CinderException):
    message = _(""Connect to volume via protocol %(protocol)s not supported."")


class SSHInjectionThreat(CinderException):
    message = _(""SSH command injection detected"") + "": %(command)s""
/n/n/ncinder/tests/test_storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Tests for the IBM Storwize family and SVC volume driver.
""""""


import random
import re
import socket

from cinder.brick.initiator import connector
from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import test
from cinder import units
from cinder import utils
from cinder.volume import configuration as conf
from cinder.volume.drivers import storwize_svc
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)


class StorwizeSVCFakeDB:
    def __init__(self):
        self.volume = None

    def volume_get(self, context, vol_id):
        return self.volume

    def volume_set(self, vol):
        self.volume = vol


class StorwizeSVCManagementSimulator:
    def __init__(self, pool_name):
        self._flags = {'storwize_svc_volpool_name': pool_name}
        self._volumes_list = {}
        self._hosts_list = {}
        self._mappings_list = {}
        self._fcmappings_list = {}
        self._next_cmd_error = {
            'lsportip': '',
            'lsfabric': '',
            'lsiscsiauth': '',
            'lsnodecanister': '',
            'mkvdisk': '',
            'lsvdisk': '',
            'lsfcmap': '',
            'prestartfcmap': '',
            'startfcmap': '',
            'rmfcmap': '',
            'lslicense': '',
        }
        self._errors = {
            'CMMVC5701E': ('', 'CMMVC5701E No object ID was specified.'),
            'CMMVC6035E': ('', 'CMMVC6035E The action failed as the '
                               'object already exists.'),
            'CMMVC5753E': ('', 'CMMVC5753E The specified object does not '
                               'exist or is not a suitable candidate.'),
            'CMMVC5707E': ('', 'CMMVC5707E Required parameters are missing.'),
            'CMMVC6581E': ('', 'CMMVC6581E The command has failed because '
                               'the maximum number of allowed iSCSI '
                               'qualified names (IQNs) has been reached, '
                               'or the IQN is already assigned or is not '
                               'valid.'),
            'CMMVC5754E': ('', 'CMMVC5754E The specified object does not '
                               'exist, or the name supplied does not meet '
                               'the naming rules.'),
            'CMMVC6071E': ('', 'CMMVC6071E The VDisk-to-host mapping was '
                               'not created because the VDisk is already '
                               'mapped to a host.'),
            'CMMVC5879E': ('', 'CMMVC5879E The VDisk-to-host mapping was '
                               'not created because a VDisk is already '
                               'mapped to this host with this SCSI LUN.'),
            'CMMVC5840E': ('', 'CMMVC5840E The virtual disk (VDisk) was '
                               'not deleted because it is mapped to a '
                               'host or because it is part of a FlashCopy '
                               'or Remote Copy mapping, or is involved in '
                               'an image mode migrate.'),
            'CMMVC6527E': ('', 'CMMVC6527E The name that you have entered '
                               'is not valid. The name can contain letters, '
                               'numbers, spaces, periods, dashes, and '
                               'underscores. The name must begin with a '
                               'letter or an underscore. The name must not '
                               'begin or end with a space.'),
            'CMMVC5871E': ('', 'CMMVC5871E The action failed because one or '
                               'more of the configured port names is in a '
                               'mapping.'),
            'CMMVC5924E': ('', 'CMMVC5924E The FlashCopy mapping was not '
                               'created because the source and target '
                               'virtual disks (VDisks) are different sizes.'),
            'CMMVC6303E': ('', 'CMMVC6303E The create failed because the '
                               'source and target VDisks are the same.'),
            'CMMVC7050E': ('', 'CMMVC7050E The command failed because at '
                               'least one node in the I/O group does not '
                               'support compressed VDisks.'),
            # Catch-all for invalid state transitions:
            'CMMVC5903E': ('', 'CMMVC5903E The FlashCopy mapping was not '
                               'changed because the mapping or consistency '
                               'group is another state.'),
        }
        self._transitions = {'begin': {'make': 'idle_or_copied'},
                             'idle_or_copied': {'prepare': 'preparing',
                                                'delete': 'end',
                                                'delete_force': 'end'},
                             'preparing': {'flush_failed': 'stopped',
                                           'wait': 'prepared'},
                             'end': None,
                             'stopped': {'prepare': 'preparing',
                                         'delete_force': 'end'},
                             'prepared': {'stop': 'stopped',
                                          'start': 'copying'},
                             'copying': {'wait': 'idle_or_copied',
                                         'stop': 'stopping'},
                             # Assume the worst case where stopping->stopped
                             # rather than stopping idle_or_copied
                             'stopping': {'wait': 'stopped'},
                             }

    def _state_transition(self, function, fcmap):
        if (function == 'wait' and
                'wait' not in self._transitions[fcmap['status']]):
            return ('', '')

        if fcmap['status'] == 'copying' and function == 'wait':
            if fcmap['copyrate'] != '0':
                if fcmap['progress'] == '0':
                    fcmap['progress'] = '50'
                else:
                    fcmap['progress'] = '100'
                    fcmap['status'] = 'idle_or_copied'
            return ('', '')
        else:
            try:
                curr_state = fcmap['status']
                fcmap['status'] = self._transitions[curr_state][function]
                return ('', '')
            except Exception:
                return self._errors['CMMVC5903E']

    # Find an unused ID
    def _find_unused_id(self, d):
        ids = []
        for k, v in d.iteritems():
            ids.append(int(v['id']))
        ids.sort()
        for index, n in enumerate(ids):
            if n > index:
                return str(index)
        return str(len(ids))

    # Check if name is valid
    def _is_invalid_name(self, name):
        if re.match(""^[a-zA-Z_][\w ._-]*$"", name):
            return False
        return True

    # Convert argument string to dictionary
    def _cmd_to_dict(self, arg_list):
        no_param_args = [
            'autodelete',
            'autoexpand',
            'bytes',
            'compressed',
            'force',
            'nohdr',
        ]
        one_param_args = [
            'chapsecret',
            'cleanrate',
            'copyrate',
            'delim',
            'filtervalue',
            'grainsize',
            'hbawwpn',
            'host',
            'iogrp',
            'iscsiname',
            'mdiskgrp',
            'name',
            'rsize',
            'scsi',
            'size',
            'source',
            'target',
            'unit',
            'easytier',
            'warning',
            'wwpn',
        ]

        # Handle the special case of lsnode which is a two-word command
        # Use the one word version of the command internally
        if arg_list[0] in ('svcinfo', 'svctask'):
            if arg_list[1] == 'lsnode':
                if len(arg_list) > 4:  # e.g. svcinfo lsnode -delim ! <node id>
                    ret = {'cmd': 'lsnode', 'node_id': arg_list[-1]}
                else:
                    ret = {'cmd': 'lsnodecanister'}
            else:
                ret = {'cmd': arg_list[1]}
            arg_list.pop(0)
        else:
            ret = {'cmd': arg_list[0]}

        skip = False
        for i in range(1, len(arg_list)):
            if skip:
                skip = False
                continue
            if arg_list[i][0] == '-':
                if arg_list[i][1:] in no_param_args:
                    ret[arg_list[i][1:]] = True
                elif arg_list[i][1:] in one_param_args:
                    ret[arg_list[i][1:]] = arg_list[i + 1]
                    skip = True
                else:
                    raise exception.InvalidInput(
                        reason=_('unrecognized argument %s') % arg_list[i])
            else:
                ret['obj'] = arg_list[i]
        return ret

    def _print_info_cmd(self, rows, delim=' ', nohdr=False, **kwargs):
        """"""Generic function for printing information.""""""
        if nohdr:
            del rows[0]

        for index in range(len(rows)):
            rows[index] = delim.join(rows[index])
        return ('%s' % '\n'.join(rows), '')

    def _print_info_obj_cmd(self, header, row, delim=' ', nohdr=False):
        """"""Generic function for printing information for a specific object.""""""
        objrows = []
        for idx, val in enumerate(header):
            objrows.append([val, row[idx]])

        if nohdr:
            for index in range(len(objrows)):
                objrows[index] = ' '.join(objrows[index][1:])
        for index in range(len(objrows)):
            objrows[index] = delim.join(objrows[index])
        return ('%s' % '\n'.join(objrows), '')

    def _convert_bytes_units(self, bytestr):
        num = int(bytestr)
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while num > 1024:
            num = num / 1024
            unit_index += 1

        return '%d%s' % (num, unit_array[unit_index])

    def _convert_units_bytes(self, num, unit):
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while unit.lower() != unit_array[unit_index].lower():
            num = num * 1024
            unit_index += 1

        return str(num)

    def _cmd_lslicense(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['used_compression_capacity', '0.08']
        rows[1] = ['license_compression_capacity', '0']
        if self._next_cmd_error['lslicense'] == 'no_compression':
            self._next_cmd_error['lslicense'] = ''
            rows[2] = ['license_compression_enclosures', '0']
        else:
            rows[2] = ['license_compression_enclosures', '1']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lssystem(self, **kwargs):
        rows = [None] * 2
        rows[0] = ['id', '0123456789ABCDEF']
        rows[1] = ['name', 'storwize-svc-sim']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax, assume -bytes passed
    def _cmd_lsmdiskgrp(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'status', 'mdisk_count',
                   'vdisk_count', 'capacity', 'extent_size',
                   'free_capacity', 'virtual_capacity', 'used_capacity',
                   'real_capacity', 'overallocation', 'warning',
                   'easy_tier', 'easy_tier_status']
        rows[1] = ['1', self._flags['storwize_svc_volpool_name'], 'online',
                   '1', str(len(self._volumes_list)), '3573412790272',
                   '256', '3529926246400', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        rows[2] = ['2', 'volpool2', 'online',
                   '1', '0', '3573412790272', '256',
                   '3529432325160', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] == self._flags['storwize_svc_volpool_name']:
                row = rows[1]
            elif kwargs['obj'] == 'volpool2':
                row = rows[2]
            else:
                return self._errors['CMMVC5754E']

            objrows = []
            for idx, val in enumerate(rows[0]):
                objrows.append([val, row[idx]])

            if 'nohdr' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = ' '.join(objrows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = kwargs['delim'].join(objrows[index])

            return ('%s' % '\n'.join(objrows), '')

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsnodecanister(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'UPS_serial_number', 'WWNN', 'status',
                   'IO_group_id', 'IO_group_name', 'config_node',
                   'UPS_unique_id', 'hardware', 'iscsi_name', 'iscsi_alias',
                   'panel_name', 'enclosure_id', 'canister_id',
                   'enclosure_serial_number']
        rows[1] = ['1', 'node1', '', '123456789ABCDEF0', 'online', '0',
                   'io_grp0',
                   'yes', '123456789ABCDEF0', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node1', '', '01-1', '1', '1',
                   '0123ABC']
        rows[2] = ['2', 'node2', '', '123456789ABCDEF1', 'online', '0',
                   'io_grp0',
                   'no', '123456789ABCDEF1', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node2', '', '01-2', '1', '2',
                   '0123ABC']

        if self._next_cmd_error['lsnodecanister'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsnodecanister'] = ''
        if self._next_cmd_error['lsnodecanister'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsnodecanister'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    # Print information of every single node of SVC
    def _cmd_lsnode(self, **kwargs):
        node_infos = dict()
        node_infos['1'] = r'''id!1
name!node1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680220C744
port_status!active
port_speed!8Gb
'''
        node_infos['2'] = r'''id!2
name!node2
port_id!500507680220C745
port_status!active
port_speed!8Gb
port_id!500507680230C745
port_status!inactive
port_speed!N/A
'''
        node_id = kwargs.get('node_id', None)
        stdout = node_infos.get(node_id, '')
        return stdout, ''

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsportip(self, **kwargs):
        if self._next_cmd_error['lsportip'] == 'ip_no_config':
            self._next_cmd_error['lsportip'] = ''
            ip_addr1 = ''
            ip_addr2 = ''
            gw = ''
        else:
            ip_addr1 = '1.234.56.78'
            ip_addr2 = '1.234.56.79'
            gw = '1.234.56.1'

        rows = [None] * 17
        rows[0] = ['id', 'node_id', 'node_name', 'IP_address', 'mask',
                   'gateway', 'IP_address_6', 'prefix_6', 'gateway_6', 'MAC',
                   'duplex', 'state', 'speed', 'failover']
        rows[1] = ['1', '1', 'node1', ip_addr1, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:00', 'Full',
                   'online', '1Gb/s', 'no']
        rows[2] = ['1', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:00', 'Full', 'online', '1Gb/s', 'yes']
        rows[3] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[4] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'yes']
        rows[5] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[6] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[7] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[8] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[9] = ['1', '2', 'node2', ip_addr2, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:02', 'Full',
                   'online', '1Gb/s', 'no']
        rows[10] = ['1', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:02', 'Full', 'online', '1Gb/s', 'yes']
        rows[11] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[12] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s',
                    'yes']
        rows[13] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[14] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']
        rows[15] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[16] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']

        if self._next_cmd_error['lsportip'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsportip'] = ''
        if self._next_cmd_error['lsportip'] == 'remove_field':
            for row in rows:
                row.pop(1)
            self._next_cmd_error['lsportip'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_lsfabric(self, **kwargs):
        host_name = kwargs['host'] if 'host' in kwargs else None
        target_wwpn = kwargs['wwpn'] if 'wwpn' in kwargs else None
        host_infos = []

        for hk, hv in self._hosts_list.iteritems():
            if not host_name or hv['host_name'] == host_name:
                for mk, mv in self._mappings_list.iteritems():
                    if mv['host'] == hv['host_name']:
                        if not target_wwpn or target_wwpn in hv['wwpns']:
                            host_infos.append(hv)
                            break

        if not len(host_infos):
            return ('', '')

        rows = []
        rows.append(['remote_wwpn', 'remote_nportid', 'id', 'node_name',
                     'local_wwpn', 'local_port', 'local_nportid', 'state',
                     'name', 'cluster_name', 'type'])
        for host_info in host_infos:
            for wwpn in host_info['wwpns']:
                rows.append([wwpn, '123456', host_info['id'], 'nodeN',
                            'AABBCCDDEEFF0011', '1', '0123ABC', 'active',
                            host_info['host_name'], '', 'host'])

        if self._next_cmd_error['lsfabric'] == 'header_mismatch':
            rows[0].pop(0)
            self._next_cmd_error['lsfabric'] = ''
        if self._next_cmd_error['lsfabric'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsfabric'] = ''
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk
    def _cmd_mkvdisk(self, **kwargs):
        # We only save the id/uid, name, and size - all else will be made up
        volume_info = {}
        volume_info['id'] = self._find_unused_id(self._volumes_list)
        volume_info['uid'] = ('ABCDEF' * 3) + ('0' * 14) + volume_info['id']

        if 'name' in kwargs:
            volume_info['name'] = kwargs['name'].strip('\'\'')
        else:
            volume_info['name'] = 'vdisk' + volume_info['id']

        # Assume size and unit are given, store it in bytes
        capacity = int(kwargs['size'])
        unit = kwargs['unit']
        volume_info['capacity'] = self._convert_units_bytes(capacity, unit)

        if 'easytier' in kwargs:
            if kwargs['easytier'] == 'on':
                volume_info['easy_tier'] = 'on'
            else:
                volume_info['easy_tier'] = 'off'

        if 'rsize' in kwargs:
            # Fake numbers
            volume_info['used_capacity'] = '786432'
            volume_info['real_capacity'] = '21474816'
            volume_info['free_capacity'] = '38219264'
            if 'warning' in kwargs:
                volume_info['warning'] = kwargs['warning'].rstrip('%')
            else:
                volume_info['warning'] = '80'
            if 'autoexpand' in kwargs:
                volume_info['autoexpand'] = 'on'
            else:
                volume_info['autoexpand'] = 'off'
            if 'grainsize' in kwargs:
                volume_info['grainsize'] = kwargs['grainsize']
            else:
                volume_info['grainsize'] = '32'
            if 'compressed' in kwargs:
                volume_info['compressed_copy'] = 'yes'
            else:
                volume_info['compressed_copy'] = 'no'
        else:
            volume_info['used_capacity'] = volume_info['capacity']
            volume_info['real_capacity'] = volume_info['capacity']
            volume_info['free_capacity'] = '0'
            volume_info['warning'] = ''
            volume_info['autoexpand'] = ''
            volume_info['grainsize'] = ''
            volume_info['compressed_copy'] = 'no'

        if volume_info['name'] in self._volumes_list:
            return self._errors['CMMVC6035E']
        else:
            self._volumes_list[volume_info['name']] = volume_info
            return ('Virtual Disk, id [%s], successfully created' %
                    (volume_info['id']), '')

    # Delete a vdisk
    def _cmd_rmvdisk(self, **kwargs):
        force = True if 'force' in kwargs else False

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if not force:
            for k, mapping in self._mappings_list.iteritems():
                if mapping['vol'] == vol_name:
                    return self._errors['CMMVC5840E']
            for k, fcmap in self._fcmappings_list.iteritems():
                if ((fcmap['source'] == vol_name) or
                        (fcmap['target'] == vol_name)):
                    return self._errors['CMMVC5840E']

        del self._volumes_list[vol_name]
        return ('', '')

    def _cmd_expandvdisksize(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        # Assume unit is gb
        if 'size' not in kwargs:
            return self._errors['CMMVC5707E']
        size = int(kwargs['size'])

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        curr_size = int(self._volumes_list[vol_name]['capacity'])
        addition = size * units.GiB
        self._volumes_list[vol_name]['capacity'] = str(curr_size + addition)
        return ('', '')

    def _get_fcmap_info(self, vol_name):
        ret_vals = {
            'fc_id': '',
            'fc_name': '',
            'fc_map_count': '0',
        }
        for k, fcmap in self._fcmappings_list.iteritems():
            if ((fcmap['source'] == vol_name) or
                    (fcmap['target'] == vol_name)):
                ret_vals['fc_id'] = fcmap['id']
                ret_vals['fc_name'] = fcmap['name']
                ret_vals['fc_map_count'] = '1'
        return ret_vals

    # List information about vdisks
    def _cmd_lsvdisk(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'IO_group_id', 'IO_group_name',
                     'status', 'mdisk_grp_id', 'mdisk_grp_name',
                     'capacity', 'type', 'FC_id', 'FC_name', 'RC_id',
                     'RC_name', 'vdisk_UID', 'fc_map_count', 'copy_count',
                     'fast_write_state', 'se_copy_count', 'RC_change'])

        for k, vol in self._volumes_list.iteritems():
            if (('filtervalue' not in kwargs) or
                    (kwargs['filtervalue'] == 'name=' + vol['name'])):
                fcmap_info = self._get_fcmap_info(vol['name'])

                if 'bytes' in kwargs:
                    cap = self._convert_bytes_units(vol['capacity'])
                else:
                    cap = vol['capacity']
                rows.append([str(vol['id']), vol['name'], '0', 'io_grp0',
                            'online', '0',
                            self._flags['storwize_svc_volpool_name'],
                            cap, 'striped',
                            fcmap_info['fc_id'], fcmap_info['fc_name'],
                            '', '', vol['uid'],
                            fcmap_info['fc_map_count'], '1', 'empty',
                            '1', 'no'])

        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] not in self._volumes_list:
                return self._errors['CMMVC5754E']
            vol = self._volumes_list[kwargs['obj']]
            fcmap_info = self._get_fcmap_info(vol['name'])
            cap = vol['capacity']
            cap_u = vol['used_capacity']
            cap_r = vol['real_capacity']
            cap_f = vol['free_capacity']
            if 'bytes' not in kwargs:
                for item in [cap, cap_u, cap_r, cap_f]:
                    item = self._convert_bytes_units(item)
            rows = []

            rows.append(['id', str(vol['id'])])
            rows.append(['name', vol['name']])
            rows.append(['IO_group_id', '0'])
            rows.append(['IO_group_name', 'io_grp0'])
            rows.append(['status', 'online'])
            rows.append(['mdisk_grp_id', '0'])
            rows.append([
                'mdisk_grp_name',
                self._flags['storwize_svc_volpool_name']])
            rows.append(['capacity', cap])
            rows.append(['type', 'striped'])
            rows.append(['formatted', 'no'])
            rows.append(['mdisk_id', ''])
            rows.append(['mdisk_name', ''])
            rows.append(['FC_id', fcmap_info['fc_id']])
            rows.append(['FC_name', fcmap_info['fc_name']])
            rows.append(['RC_id', ''])
            rows.append(['RC_name', ''])
            rows.append(['vdisk_UID', vol['uid']])
            rows.append(['throttling', '0'])

            if self._next_cmd_error['lsvdisk'] == 'blank_pref_node':
                rows.append(['preferred_node_id', ''])
                self._next_cmd_error['lsvdisk'] = ''
            elif self._next_cmd_error['lsvdisk'] == 'no_pref_node':
                self._next_cmd_error['lsvdisk'] = ''
            else:
                rows.append(['preferred_node_id', '1'])
            rows.append(['fast_write_state', 'empty'])
            rows.append(['cache', 'readwrite'])
            rows.append(['udid', ''])
            rows.append(['fc_map_count', fcmap_info['fc_map_count']])
            rows.append(['sync_rate', '50'])
            rows.append(['copy_count', '1'])
            rows.append(['se_copy_count', '0'])
            rows.append(['mirror_write_priority', 'latency'])
            rows.append(['RC_change', 'no'])
            rows.append(['used_capacity', cap_u])
            rows.append(['real_capacity', cap_r])
            rows.append(['free_capacity', cap_f])
            rows.append(['autoexpand', vol['autoexpand']])
            rows.append(['warning', vol['warning']])
            rows.append(['grainsize', vol['grainsize']])
            rows.append(['easy_tier', vol['easy_tier']])
            rows.append(['compressed_copy', vol['compressed_copy']])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    def _add_port_to_host(self, host_info, **kwargs):
        if 'iscsiname' in kwargs:
            added_key = 'iscsi_names'
            added_val = kwargs['iscsiname'].strip('\'\""')
        elif 'hbawwpn' in kwargs:
            added_key = 'wwpns'
            added_val = kwargs['hbawwpn'].strip('\'\""')
        else:
            return self._errors['CMMVC5707E']

        host_info[added_key].append(added_val)

        for k, v in self._hosts_list.iteritems():
            if v['id'] == host_info['id']:
                continue
            for port in v[added_key]:
                if port == added_val:
                    return self._errors['CMMVC6581E']
        return ('', '')

    # Make a host
    def _cmd_mkhost(self, **kwargs):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)

        if 'name' in kwargs:
            host_name = kwargs['name'].strip('\'\""')
        else:
            host_name = 'host' + str(host_info['id'])

        if self._is_invalid_name(host_name):
            return self._errors['CMMVC6527E']

        if host_name in self._hosts_list:
            return self._errors['CMMVC6035E']

        host_info['host_name'] = host_name
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []

        out, err = self._add_port_to_host(host_info, **kwargs)
        if not len(err):
            self._hosts_list[host_name] = host_info
            return ('Host, id [%s], successfully created' %
                    (host_info['id']), '')
        else:
            return (out, err)

    # Add ports to an existing host
    def _cmd_addhostport(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        host_info = self._hosts_list[host_name]
        return self._add_port_to_host(host_info, **kwargs)

    # Change host properties
    def _cmd_chhost(self, **kwargs):
        if 'chapsecret' not in kwargs:
            return self._errors['CMMVC5707E']
        secret = kwargs['obj'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        self._hosts_list[host_name]['chapsecret'] = secret
        return ('', '')

    # Remove a host
    def _cmd_rmhost(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']

        host_name = kwargs['obj'].strip('\'\'')
        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        for k, v in self._mappings_list.iteritems():
            if (v['host'] == host_name):
                return self._errors['CMMVC5871E']

        del self._hosts_list[host_name]
        return ('', '')

    # List information about hosts
    def _cmd_lshost(self, **kwargs):
        if 'obj' not in kwargs:
            rows = []
            rows.append(['id', 'name', 'port_count', 'iogrp_count', 'status'])

            found = False
            for k, host in self._hosts_list.iteritems():
                filterstr = 'name=' + host['host_name']
                if (('filtervalue' not in kwargs) or
                        (kwargs['filtervalue'] == filterstr)):
                    rows.append([host['id'], host['host_name'], '1', '4',
                                'offline'])
                    found = True
            if found:
                return self._print_info_cmd(rows=rows, **kwargs)
            else:
                return ('', '')
        else:
            if kwargs['obj'] not in self._hosts_list:
                return self._errors['CMMVC5754E']
            host = self._hosts_list[kwargs['obj']]
            rows = []
            rows.append(['id', host['id']])
            rows.append(['name', host['host_name']])
            rows.append(['port_count', '1'])
            rows.append(['type', 'generic'])
            rows.append(['mask', '1111'])
            rows.append(['iogrp_count', '4'])
            rows.append(['status', 'online'])
            for port in host['iscsi_names']:
                rows.append(['iscsi_name', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'offline'])
            for port in host['wwpns']:
                rows.append(['WWPN', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'active'])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    # List iSCSI authorization information about hosts
    def _cmd_lsiscsiauth(self, **kwargs):
        if self._next_cmd_error['lsiscsiauth'] == 'no_info':
            self._next_cmd_error['lsiscsiauth'] = ''
            return ('', '')
        rows = []
        rows.append(['type', 'id', 'name', 'iscsi_auth_method',
                     'iscsi_chap_secret'])

        for k, host in self._hosts_list.iteritems():
            method = 'none'
            secret = ''
            if 'chapsecret' in host:
                method = 'chap'
                secret = host['chapsecret']
            rows.append(['host', host['id'], host['host_name'], method,
                         secret])
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk-host mapping
    def _cmd_mkvdiskhostmap(self, **kwargs):
        mapping_info = {}
        mapping_info['id'] = self._find_unused_id(self._mappings_list)

        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['host'] = kwargs['host'].strip('\'\'')

        if 'scsi' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['lun'] = kwargs['scsi'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['vol'] = kwargs['obj'].strip('\'\'')

        if mapping_info['vol'] not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if mapping_info['host'] not in self._hosts_list:
            return self._errors['CMMVC5754E']

        if mapping_info['vol'] in self._mappings_list:
            return self._errors['CMMVC6071E']

        for k, v in self._mappings_list.iteritems():
            if ((v['host'] == mapping_info['host']) and
                    (v['lun'] == mapping_info['lun'])):
                return self._errors['CMMVC5879E']

        for k, v in self._mappings_list.iteritems():
            if (v['lun'] == mapping_info['lun']) and ('force' not in kwargs):
                return self._errors['CMMVC6071E']

        self._mappings_list[mapping_info['id']] = mapping_info
        return ('Virtual Disk to Host map, id [%s], successfully created'
                % (mapping_info['id']), '')

    # Delete a vdisk-host mapping
    def _cmd_rmvdiskhostmap(self, **kwargs):
        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        host = kwargs['host'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol = kwargs['obj'].strip('\'\'')

        mapping_ids = []
        for k, v in self._mappings_list.iteritems():
            if v['vol'] == vol:
                mapping_ids.append(v['id'])
        if not mapping_ids:
            return self._errors['CMMVC5753E']

        this_mapping = None
        for mapping_id in mapping_ids:
            if self._mappings_list[mapping_id]['host'] == host:
                this_mapping = mapping_id
        if this_mapping == None:
            return self._errors['CMMVC5753E']

        del self._mappings_list[this_mapping]
        return ('', '')

    # List information about vdisk-host mappings
    def _cmd_lshostvdiskmap(self, **kwargs):
        index = 1
        no_hdr = 0
        delimeter = ''
        host_name = kwargs['obj']

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5754E']

        rows = []
        rows.append(['id', 'name', 'SCSI_id', 'vdisk_id', 'vdisk_name',
                     'vdisk_UID'])

        for k, mapping in self._mappings_list.iteritems():
            if (host_name == '') or (mapping['host'] == host_name):
                volume = self._volumes_list[mapping['vol']]
                rows.append([mapping['id'], mapping['host'],
                            mapping['lun'], volume['id'],
                            volume['name'], volume['uid']])

        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a FlashCopy mapping
    def _cmd_mkfcmap(self, **kwargs):
        source = ''
        target = ''
        copyrate = kwargs['copyrate'] if 'copyrate' in kwargs else '50'

        if 'source' not in kwargs:
            return self._errors['CMMVC5707E']
        source = kwargs['source'].strip('\'\'')
        if source not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if 'target' not in kwargs:
            return self._errors['CMMVC5707E']
        target = kwargs['target'].strip('\'\'')
        if target not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if source == target:
            return self._errors['CMMVC6303E']

        if (self._volumes_list[source]['capacity'] !=
                self._volumes_list[target]['capacity']):
            return self._errors['CMMVC5924E']

        fcmap_info = {}
        fcmap_info['source'] = source
        fcmap_info['target'] = target
        fcmap_info['id'] = self._find_unused_id(self._fcmappings_list)
        fcmap_info['name'] = 'fcmap' + fcmap_info['id']
        fcmap_info['copyrate'] = copyrate
        fcmap_info['progress'] = '0'
        fcmap_info['autodelete'] = True if 'autodelete' in kwargs else False
        fcmap_info['status'] = 'idle_or_copied'
        self._fcmappings_list[fcmap_info['id']] = fcmap_info

        return('FlashCopy Mapping, id [' + fcmap_info['id'] +
               '], successfully created', '')

    def _cmd_gen_prestartfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['prestartfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['prestartfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('prepare', fcmap)

    def _cmd_gen_startfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['startfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['startfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('start', fcmap)

    def _cmd_stopfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('stop', fcmap)

    def _cmd_rmfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']
        force = True if 'force' in kwargs else False

        if self._next_cmd_error['rmfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['rmfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        function = 'delete_force' if force else 'delete'
        ret = self._state_transition(function, fcmap)
        if fcmap['status'] == 'end':
            del self._fcmappings_list[id_num]
        return ret

    def _cmd_lsvdiskfcmappings(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        vdisk = kwargs['obj']
        rows = []
        rows.append(['id', 'name'])
        for k, v in self._fcmappings_list.iteritems():
            if v['source'] == vdisk or v['target'] == vdisk:
                rows.append([v['id'], v['name']])
        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_chfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        for key in ['name', 'copyrate', 'autodelete']:
            if key in kwargs:
                fcmap[key] = kwargs[key]
        return ('', '')

    def _cmd_lsfcmap(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'source_vdisk_id', 'source_vdisk_name',
                     'target_vdisk_id', 'target_vdisk_name', 'group_id',
                     'group_name', 'status', 'progress', 'copy_rate',
                     'clean_progress', 'incremental', 'partner_FC_id',
                     'partner_FC_name', 'restoring', 'start_time',
                     'rc_controlled'])

        # Assume we always get a filtervalue argument
        filter_key = kwargs['filtervalue'].split('=')[0]
        filter_value = kwargs['filtervalue'].split('=')[1]
        to_delete = []
        for k, v in self._fcmappings_list.iteritems():
            if str(v[filter_key]) == filter_value:
                source = self._volumes_list[v['source']]
                target = self._volumes_list[v['target']]
                self._state_transition('wait', v)

                if self._next_cmd_error['lsfcmap'] == 'speed_up':
                    self._next_cmd_error['lsfcmap'] = ''
                    curr_state = v['status']
                    while self._state_transition('wait', v) == ("""", """"):
                        if curr_state == v['status']:
                            break
                        curr_state = v['status']

                if ((v['status'] == 'idle_or_copied' and v['autodelete'] and
                     v['progress'] == '100') or (v['status'] == 'end')):
                    to_delete.append(k)
                else:
                    rows.append([v['id'], v['name'], source['id'],
                                source['name'], target['id'], target['name'],
                                '', '', v['status'], v['progress'],
                                v['copyrate'], '100', 'off', '', '', 'no', '',
                                'no'])

        for d in to_delete:
            del self._fcmappings_list[k]

        return self._print_info_cmd(rows=rows, **kwargs)

    # Add host to list
    def _add_host_to_list(self, connector):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)
        host_info['host_name'] = connector['host']
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []
        if 'initiator' in connector:
            host_info['iscsi_names'].append(connector['initiator'])
        if 'wwpns' in connector:
            host_info['wwpns'] = host_info['wwpns'] + connector['wwpns']
        self._hosts_list[connector['host']] = host_info

    # The main function to run commands on the management simulator
    def execute_command(self, cmd, check_exit_code=True):
        try:
            kwargs = self._cmd_to_dict(cmd)
        except IndexError:
            return self._errors['CMMVC5707E']

        command = kwargs['cmd']
        del kwargs['cmd']

        if command == 'lsmdiskgrp':
            out, err = self._cmd_lsmdiskgrp(**kwargs)
        elif command == 'lslicense':
            out, err = self._cmd_lslicense(**kwargs)
        elif command == 'lssystem':
            out, err = self._cmd_lssystem(**kwargs)
        elif command == 'lsnodecanister':
            out, err = self._cmd_lsnodecanister(**kwargs)
        elif command == 'lsnode':
            out, err = self._cmd_lsnode(**kwargs)
        elif command == 'lsportip':
            out, err = self._cmd_lsportip(**kwargs)
        elif command == 'lsfabric':
            out, err = self._cmd_lsfabric(**kwargs)
        elif command == 'mkvdisk':
            out, err = self._cmd_mkvdisk(**kwargs)
        elif command == 'rmvdisk':
            out, err = self._cmd_rmvdisk(**kwargs)
        elif command == 'expandvdisksize':
            out, err = self._cmd_expandvdisksize(**kwargs)
        elif command == 'lsvdisk':
            out, err = self._cmd_lsvdisk(**kwargs)
        elif command == 'mkhost':
            out, err = self._cmd_mkhost(**kwargs)
        elif command == 'addhostport':
            out, err = self._cmd_addhostport(**kwargs)
        elif command == 'chhost':
            out, err = self._cmd_chhost(**kwargs)
        elif command == 'rmhost':
            out, err = self._cmd_rmhost(**kwargs)
        elif command == 'lshost':
            out, err = self._cmd_lshost(**kwargs)
        elif command == 'lsiscsiauth':
            out, err = self._cmd_lsiscsiauth(**kwargs)
        elif command == 'mkvdiskhostmap':
            out, err = self._cmd_mkvdiskhostmap(**kwargs)
        elif command == 'rmvdiskhostmap':
            out, err = self._cmd_rmvdiskhostmap(**kwargs)
        elif command == 'lshostvdiskmap':
            out, err = self._cmd_lshostvdiskmap(**kwargs)
        elif command == 'mkfcmap':
            out, err = self._cmd_mkfcmap(**kwargs)
        elif command == 'prestartfcmap':
            out, err = self._cmd_gen_prestartfcmap(**kwargs)
        elif command == 'startfcmap':
            out, err = self._cmd_gen_startfcmap(**kwargs)
        elif command == 'stopfcmap':
            out, err = self._cmd_stopfcmap(**kwargs)
        elif command == 'rmfcmap':
            out, err = self._cmd_rmfcmap(**kwargs)
        elif command == 'chfcmap':
            out, err = self._cmd_chfcmap(**kwargs)
        elif command == 'lsfcmap':
            out, err = self._cmd_lsfcmap(**kwargs)
        elif command == 'lsvdiskfcmappings':
            out, err = self._cmd_lsvdiskfcmappings(**kwargs)
        else:
            out, err = ('', 'ERROR: Unsupported command')

        if (check_exit_code) and (len(err) != 0):
            raise exception.ProcessExecutionError(exit_code=1,
                                                  stdout=out,
                                                  stderr=err,
                                                  cmd=' '.join(cmd))

        return (out, err)

    # After calling this function, the next call to the specified command will
    # result in in the error specified
    def error_injection(self, cmd, error):
        self._next_cmd_error[cmd] = error


class StorwizeSVCFakeDriver(storwize_svc.StorwizeSVCDriver):
    def __init__(self, *args, **kwargs):
        super(StorwizeSVCFakeDriver, self).__init__(*args, **kwargs)

    def set_fake_storage(self, fake):
        self.fake_storage = fake

    def _run_ssh(self, cmd, check_exit_code=True):
        try:
            LOG.debug(_('Run CLI command: %s') % cmd)
            ret = self.fake_storage.execute_command(cmd, check_exit_code)
            (stdout, stderr) = ret
            LOG.debug(_('CLI output:\n stdout: %(stdout)s\n stderr: '
                        '%(stderr)s') % {'stdout': stdout, 'stderr': stderr})

        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.debug(_('CLI Exception output:\n stdout: %(out)s\n '
                            'stderr: %(err)s') % {'out': e.stdout,
                                                  'err': e.stderr})

        return ret


class StorwizeSVCFakeSock:
    def settimeout(self, time):
        return


class StorwizeSVCDriverTestCase(test.TestCase):
    def setUp(self):
        super(StorwizeSVCDriverTestCase, self).setUp()
        self.USESIM = True
        if self.USESIM:
            self.driver = StorwizeSVCFakeDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': 'hostname',
                               'san_login': 'user',
                               'san_password': 'pass',
                               'storwize_svc_flashcopy_timeout': 20,
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False}
            wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                     str(random.randint(0, 9999999999999999)).zfill(16)]
            initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
            self._connector = {'ip': '1.234.56.78',
                               'host': 'storwize-svc-test',
                               'wwpns': wwpns,
                               'initiator': initiator}
            self.sim = StorwizeSVCManagementSimulator('volpool')

            self.driver.set_fake_storage(self.sim)
        else:
            self.driver = storwize_svc.StorwizeSVCDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': '1.111.11.11',
                               'san_login': 'user',
                               'san_password': 'password',
                               'storwize_svc_volpool_name': 'openstack',
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False,
                               'ssh_conn_timeout': 0}
            config_group = self.driver.configuration.config_group
            self.driver.configuration.set_override('rootwrap_config',
                                                   '/etc/cinder/rootwrap.conf',
                                                   config_group)
            self._connector = connector.get_connector_properties()

        self._reset_flags()
        self.driver.db = StorwizeSVCFakeDB()
        self.driver.do_setup(None)
        self.driver.check_for_setup_error()
        self.stubs.Set(storwize_svc.time, 'sleep', lambda s: None)

    def _set_flag(self, flag, value):
        group = self.driver.configuration.config_group
        self.driver.configuration.set_override(flag, value, group)

    def _reset_flags(self):
        self.driver.configuration.local_conf.reset()
        for k, v in self._def_flags.iteritems():
            self._set_flag(k, v)

    def _assert_vol_exists(self, name, exists):
        is_vol_defined = self.driver._is_vdisk_defined(name)
        self.assertEqual(is_vol_defined, exists)

    def test_storwize_svc_connectivity(self):
        # Make sure we detect if the pool doesn't exist
        no_exist_pool = 'i-dont-exist-%s' % random.randint(10000, 99999)
        self._set_flag('storwize_svc_volpool_name', no_exist_pool)
        self.assertRaises(exception.InvalidInput,
                          self.driver.do_setup, None)
        self._reset_flags()

        # Check the case where the user didn't configure IP addresses
        # as well as receiving unexpected results from the storage
        if self.USESIM:
            self.sim.error_injection('lsnodecanister', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsnodecanister', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)

        # Check with bad parameters
        self._set_flag('san_ip', '')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('san_password', None)
        self._set_flag('san_private_key', None)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_rsize', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_warning', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_grainsize', 42)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_flashcopy_timeout', 601)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_compression', True)
        self._set_flag('storwize_svc_vol_rsize', -1)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'foo')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self._set_flag('storwize_svc_multipath_enabled', True)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        if self.USESIM:
            self.sim.error_injection('lslicense', 'no_compression')
            self._set_flag('storwize_svc_vol_compression', True)
            self.driver.do_setup(None)
            self.assertRaises(exception.InvalidInput,
                              self.driver.check_for_setup_error)
            self._reset_flags()

        # Finally, check with good parameters
        self.driver.do_setup(None)

    def _generate_vol_info(self, vol_name, vol_id):
        rand_id = str(random.randint(10000, 99999))
        if vol_name:
            return {'name': 'snap_volume%s' % rand_id,
                    'volume_name': vol_name,
                    'id': rand_id,
                    'volume_id': vol_id,
                    'volume_size': 10}
        else:
            return {'name': 'test_volume%s' % rand_id,
                    'size': 10,
                    'id': '%s' % rand_id,
                    'volume_type_id': None}

    def _create_test_vol(self, opts):
        ctxt = context.get_admin_context()
        type_ref = volume_types.create(ctxt, 'testtype', opts)
        volume = self._generate_vol_info(None, None)
        volume['volume_type_id'] = type_ref['id']
        self.driver.create_volume(volume)

        attrs = self.driver._get_vdisk_attributes(volume['name'])
        self.driver.delete_volume(volume)
        volume_types.destroy(ctxt, type_ref['id'])
        return attrs

    def _fail_prepare_fc_map(self, fc_map_id, source, target):
        raise exception.ProcessExecutionError(exit_code=1,
                                              stdout='',
                                              stderr='unit-test-fail',
                                              cmd='prestartfcmap id')

    def test_storwize_svc_snapshots(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])

        # Test timeout and volume cleanup
        self._set_flag('storwize_svc_flashcopy_timeout', 1)
        self.assertRaises(exception.InvalidSnapshot,
                          self.driver.create_snapshot, snap1)
        self._assert_vol_exists(snap1['name'], False)
        self._reset_flags()

        # Test prestartfcmap, startfcmap, and rmfcmap failing
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_snapshot, snap1)
        self.driver._call_prepare_fc_map = orig

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)
            self.sim.error_injection('prestartfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)

        # Test successful snapshot
        self.driver.create_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], True)

        # Try to create a snapshot from an non-existing volume - should fail
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        self.assertRaises(exception.VolumeNotFound,
                          self.driver.create_snapshot,
                          snap_novol)

        # We support deleting a volume that has snapshots, so delete the volume
        # first
        self.driver.delete_volume(vol1)
        self.driver.delete_snapshot(snap1)

    def test_storwize_svc_create_volfromsnap_clone(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])
        self.driver.create_snapshot(snap1)
        vol2 = self._generate_vol_info(None, None)
        vol3 = self._generate_vol_info(None, None)

        # Try to create a volume from a non-existing snapshot
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        vol_novol = self._generate_vol_info(None, None)
        self.assertRaises(exception.SnapshotNotFound,
                          self.driver.create_volume_from_snapshot,
                          vol_novol,
                          snap_novol)

        # Fail the snapshot
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self.driver._call_prepare_fc_map = orig
        self._assert_vol_exists(vol2['name'], False)

        # Try to create where source size != target size
        vol2['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self._assert_vol_exists(vol2['name'], False)
        vol2['size'] -= 1

        # Succeed
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(vol2, snap1)
        self._assert_vol_exists(vol2['name'], True)

        # Try to clone where source size != target size
        vol3['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_cloned_volume,
                          vol3, vol2)
        self._assert_vol_exists(vol3['name'], False)
        vol3['size'] -= 1

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(vol3, vol2)
        self._assert_vol_exists(vol3['name'], True)

        # Delete in the 'opposite' order to make sure it works
        self.driver.delete_volume(vol3)
        self._assert_vol_exists(vol3['name'], False)
        self.driver.delete_volume(vol2)
        self._assert_vol_exists(vol2['name'], False)
        self.driver.delete_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], False)
        self.driver.delete_volume(vol1)
        self._assert_vol_exists(vol1['name'], False)

    def test_storwize_svc_volumes(self):
        # Create a first volume
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        self.driver.ensure_export(None, volume)

        # Do nothing
        self.driver.create_export(None, volume)
        self.driver.remove_export(None, volume)

        # Make sure volume attributes are as they should be
        attributes = self.driver._get_vdisk_attributes(volume['name'])
        attr_size = float(attributes['capacity']) / (1024 ** 3)  # bytes to GB
        self.assertEqual(attr_size, float(volume['size']))
        pool = self.driver.configuration.local_conf.storwize_svc_volpool_name
        self.assertEqual(attributes['mdisk_grp_name'], pool)

        # Try to create the volume again (should fail)
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume,
                          volume)

        # Try to delete a volume that doesn't exist (should not fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.delete_volume(vol_no_exist)
        # Ensure export for volume that doesn't exist (should not fail)
        self.driver.ensure_export(None, vol_no_exist)

        # Delete the volume
        self.driver.delete_volume(volume)

    def test_storwize_svc_volume_params(self):
        # Option test matrix
        # Option        Value   Covered by test #
        # rsize         -1      1
        # rsize         2       2,3
        # warning       0       2
        # warning       80      3
        # autoexpand    True    2
        # autoexpand    False   3
        # grainsize     32      2
        # grainsize     256     3
        # compression   True    4
        # compression   False   2,3
        # easytier      True    1,3
        # easytier      False   2

        opts_list = []
        chck_list = []
        opts_list.append({'rsize': -1, 'easytier': True})
        chck_list.append({'free_capacity': '0', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 0,
                          'autoexpand': True, 'grainsize': 32,
                          'easytier': False})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '0', 'autoexpand': 'on',
                          'grainsize': '32', 'easy_tier': 'off'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 80,
                          'autoexpand': False, 'grainsize': 256,
                          'easytier': True})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '80', 'autoexpand': 'off',
                          'grainsize': '256', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': True})
        chck_list.append({'-free_capacity': '0',
                          'compressed_copy': 'yes'})

        for idx in range(len(opts_list)):
            attrs = self._create_test_vol(opts_list[idx])
            for k, v in chck_list[idx].iteritems():
                try:
                    if k[0] == '-':
                        k = k[1:]
                        self.assertNotEqual(attrs[k], v)
                    else:
                        self.assertEqual(attrs[k], v)
                except exception.ProcessExecutionError as e:
                    if 'CMMVC7050E' not in e.stderr:
                        raise

    def test_storwize_svc_unicode_host_and_volume_names(self):
        # We'll check with iSCSI only - nothing protocol-dependednt here
        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self.driver.do_setup(None)

        rand_id = random.randint(10000, 99999)
        volume1 = {'name': u'unicode1_volume%s' % rand_id,
                   'size': 2,
                   'id': 1,
                   'volume_type_id': None}
        self.driver.create_volume(volume1)
        self._assert_vol_exists(volume1['name'], True)

        self.assertRaises(exception.NoValidHost,
                          self.driver._connector_to_hostname_prefix,
                          {'host': 12345})

        # Add a a host first to make life interesting (this host and
        # conn['host'] should be translated to the same prefix, and the
        # initiator should differentiate
        tmpconn1 = {'initiator': u'unicode:initiator1.%s' % rand_id,
                    'ip': '10.10.10.10',
                    'host': u'unicode.foo}.bar{.baz-%s' % rand_id}
        self.driver._create_host(tmpconn1)

        # Add a host with a different prefix
        tmpconn2 = {'initiator': u'unicode:initiator2.%s' % rand_id,
                    'ip': '10.10.10.11',
                    'host': u'unicode.hello.world-%s' % rand_id}
        self.driver._create_host(tmpconn2)

        conn = {'initiator': u'unicode:initiator3.%s' % rand_id,
                'ip': '10.10.10.12',
                'host': u'unicode.foo}.bar}.baz-%s' % rand_id}
        self.driver.initialize_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertNotEqual(host_name, None)
        self.driver.terminate_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertEqual(host_name, None)
        self.driver.delete_volume(volume1)

        # Clean up temporary hosts
        for tmpconn in [tmpconn1, tmpconn2]:
            host_name = self.driver._get_host_from_connector(tmpconn)
            self.assertNotEqual(host_name, None)
            self.driver._delete_host(host_name)

    def test_storwize_svc_validate_connector(self):
        conn_neither = {'host': 'host'}
        conn_iscsi = {'host': 'host', 'initiator': 'foo'}
        conn_fc = {'host': 'host', 'wwpns': 'bar'}
        conn_both = {'host': 'host', 'initiator': 'foo', 'wwpns': 'bar'}

        self.driver._enabled_protocols = set(['iSCSI'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_fc)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['FC'])
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_iscsi)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['iSCSI', 'FC'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

    def test_storwize_svc_host_maps(self):
        # Create two volumes to be used in mappings

        ctxt = context.get_admin_context()
        volume1 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume1)
        volume2 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume2)

        # Create volume types that we created
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        for protocol in ['FC', 'iSCSI']:
            volume1['volume_type_id'] = types[protocol]['id']
            volume2['volume_type_id'] = types[protocol]['id']

            # Check case where no hosts exist
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

            # Make sure that the volumes have been created
            self._assert_vol_exists(volume1['name'], True)
            self._assert_vol_exists(volume2['name'], True)

            # Initialize connection from the first volume to a host
            self.driver.initialize_connection(volume1, self._connector)

            # Initialize again, should notice it and do nothing
            self.driver.initialize_connection(volume1, self._connector)

            # Try to delete the 1st volume (should fail because it is mapped)
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.delete_volume,
                              volume1)

            # Check bad output from lsfabric for the 2nd volume
            if protocol == 'FC' and self.USESIM:
                for error in ['remove_field', 'header_mismatch']:
                    self.sim.error_injection('lsfabric', error)
                    self.assertRaises(exception.VolumeBackendAPIException,
                                      self.driver.initialize_connection,
                                      volume2, self._connector)

            self.driver.terminate_connection(volume1, self._connector)
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

        # Check cases with no auth set for host
        if self.USESIM:
            for case in ['no_info', 'no_auth_set']:
                conn_na = {'initiator': 'test:init:%s' %
                                        random.randint(10000, 99999),
                           'ip': '11.11.11.11',
                           'host': 'host-%s' % case}
                self.sim._add_host_to_list(conn_na)
                volume1['volume_type_id'] = types['iSCSI']['id']
                if case == 'no_info':
                    self.sim.error_injection('lsiscsiauth', 'no_info')
                self.driver.initialize_connection(volume1, conn_na)
                ret = self.driver._get_chap_secret_for_host(conn_na['host'])
                self.assertNotEqual(ret, None)
                self.driver.terminate_connection(volume1, conn_na)

        # Test no preferred node
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'no_pref_node')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.initialize_connection,
                              volume1, self._connector)

        # Initialize connection from the second volume to the host with no
        # preferred node set if in simulation mode, otherwise, just
        # another initialize connection.
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'blank_pref_node')
        self.driver.initialize_connection(volume2, self._connector)

        # Try to remove connection from host that doesn't exist (should fail)
        conn_no_exist = self._connector.copy()
        conn_no_exist['initiator'] = 'i_dont_exist'
        conn_no_exist['wwpns'] = ['0000000000000000']
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.terminate_connection,
                          volume1,
                          conn_no_exist)

        # Try to remove connection from volume that isn't mapped (should print
        # message but NOT fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.terminate_connection(vol_no_exist, self._connector)

        # Remove the mapping from the 1st volume and delete it
        self.driver.terminate_connection(volume1, self._connector)
        self.driver.delete_volume(volume1)
        self._assert_vol_exists(volume1['name'], False)

        # Make sure our host still exists
        host_name = self.driver._get_host_from_connector(self._connector)
        self.assertNotEqual(host_name, None)

        # Remove the mapping from the 2nd volume and delete it. The host should
        # be automatically removed because there are no more mappings.
        self.driver.terminate_connection(volume2, self._connector)
        self.driver.delete_volume(volume2)
        self._assert_vol_exists(volume2['name'], False)

        # Delete volume types that we created
        for protocol in ['FC', 'iSCSI']:
            volume_types.destroy(ctxt, types[protocol]['id'])

        # Check if our host still exists (it should not)
        if self.USESIM:
            ret = self.driver._get_host_from_connector(self._connector)
            self.assertEqual(ret, None)

    def test_storwize_svc_multi_host_maps(self):
        # We can't test connecting to multiple hosts from a single host when
        # using real storage
        if not self.USESIM:
            return

        # Create a volume to be used in mappings
        ctxt = context.get_admin_context()
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        # Create volume types for protocols
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        # Create a connector for the second 'host'
        wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                 str(random.randint(0, 9999999999999999)).zfill(16)]
        initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
        conn2 = {'ip': '1.234.56.79',
                 'host': 'storwize-svc-test2',
                 'wwpns': wwpns,
                 'initiator': initiator}

        for protocol in ['FC', 'iSCSI']:
            volume['volume_type_id'] = types[protocol]['id']

            # Make sure that the volume has been created
            self._assert_vol_exists(volume['name'], True)

            self.driver.initialize_connection(volume, self._connector)

            self._set_flag('storwize_svc_multihostmap_enabled', False)
            self.assertRaises(exception.CinderException,
                              self.driver.initialize_connection, volume, conn2)

            self._set_flag('storwize_svc_multihostmap_enabled', True)
            self.driver.initialize_connection(volume, conn2)

            self.driver.terminate_connection(volume, conn2)
            self.driver.terminate_connection(volume, self._connector)

    def test_storwize_svc_delete_volume_snapshots(self):
        # Create a volume with two snapshots
        master = self._generate_vol_info(None, None)
        self.driver.create_volume(master)
        self.driver.db.volume_set(master)

        # Fail creating a snapshot - will force delete the snapshot
        if self.USESIM and False:
            snap = self._generate_vol_info(master['name'], master['id'])
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap)
            self._assert_vol_exists(snap['name'], False)

        # Delete a snapshot
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Delete a volume with snapshots (regular)
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_volume(master)
        self._assert_vol_exists(master['name'], False)

        # Fail create volume from snapshot - will force delete the volume
        if self.USESIM:
            volfs = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_volume_from_snapshot,
                              volfs, snap)
            self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete it
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self._assert_vol_exists(volfs['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete the snapshot
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Fail create clone - will force delete the target volume
        if self.USESIM:
            clone = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_cloned_volume,
                              clone, volfs)
            self._assert_vol_exists(clone['name'], False)

        # Create the clone, delete the source and target
        clone = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(clone, volfs)
        self._assert_vol_exists(clone['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)
        self.driver.delete_volume(clone)
        self._assert_vol_exists(clone['name'], False)

    # Note defined in python 2.6, so define here...
    def assertLessEqual(self, a, b, msg=None):
        if not a <= b:
            self.fail('%s not less than or equal to %s' % (repr(a), repr(b)))

    def test_storwize_svc_get_volume_stats(self):
        stats = self.driver.get_volume_stats()
        self.assertLessEqual(stats['free_capacity_gb'],
                             stats['total_capacity_gb'])
        if self.USESIM:
            self.assertEqual(stats['volume_backend_name'],
                             'storwize-svc-sim_volpool')
            self.assertAlmostEqual(stats['total_capacity_gb'], 3328.0)
            self.assertAlmostEqual(stats['free_capacity_gb'], 3287.5)

    def test_storwize_svc_extend_volume(self):
        volume = self._generate_vol_info(None, None)
        self.driver.db.volume_set(volume)
        self.driver.create_volume(volume)
        stats = self.driver.extend_volume(volume, '13')
        attrs = self.driver._get_vdisk_attributes(volume['name'])
        vol_size = int(attrs['capacity']) / units.GiB
        self.assertAlmostEqual(vol_size, 13)

        snap = self._generate_vol_info(volume['name'], volume['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.extend_volume, volume, '16')

        self.driver.delete_snapshot(snap)
        self.driver.delete_volume(volume)


class CLIResponseTestCase(test.TestCase):
    def test_empty(self):
        self.assertEqual(0, len(storwize_svc.CLIResponse('')))
        self.assertEqual(0, len(storwize_svc.CLIResponse(('', 'stderr'))))

    def test_header(self):
        raw = r'''id!name
1!node1
2!node2
'''
        resp = storwize_svc.CLIResponse(raw, with_header=True)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('2', resp[1]['id'])

    def test_select(self):
        raw = r'''id!123
name!Bill
name!Bill2
age!30
home address!s1
home address!s2

id! 7
name!John
name!John2
age!40
home address!s3
home address!s4
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(list(resp.select('home address', 'name',
                                          'home address')),
                         [('s1', 'Bill', 's1'), ('s2', 'Bill2', 's2'),
                          ('s3', 'John', 's3'), ('s4', 'John2', 's4')])

    def test_lsnode_all(self):
        raw = r'''id!name!UPS_serial_number!WWNN!status
1!node1!!500507680200C744!online
2!node2!!500507680200C745!online
'''
        resp = storwize_svc.CLIResponse(raw)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('500507680200C744', resp[0]['WWNN'])
        self.assertEqual('2', resp[1]['id'])
        self.assertEqual('500507680200C745', resp[1]['WWNN'])

    def test_lsnode_single(self):
        raw = r'''id!1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680240C744
port_status!inactive
port_speed!8Gb
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(1, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual(list(resp.select('port_id', 'port_status')),
                         [('500507680210C744', 'active'),
                          ('500507680240C744', 'inactive')])
/n/n/ncinder/utils.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Utilities and helper functions.""""""


import contextlib
import datetime
import functools
import hashlib
import inspect
import os
import paramiko
import pyclbr
import random
import re
import shutil
import sys
import tempfile
import time
from xml.dom import minidom
from xml.parsers import expat
from xml import sax
from xml.sax import expatreader
from xml.sax import saxutils

from eventlet import event
from eventlet import greenthread
from eventlet import pools

from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import importutils
from cinder.openstack.common import lockutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder.openstack.common import timeutils


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
ISO_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S""
PERFECT_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S.%f""

synchronized = lockutils.synchronized_with_prefix('cinder-')


def find_config(config_path):
    """"""Find a configuration file using the given hint.

    :param config_path: Full or relative path to the config.
    :returns: Full path of the config, if it exists.
    :raises: `cinder.exception.ConfigNotFound`

    """"""
    possible_locations = [
        config_path,
        os.path.join(CONF.state_path, ""etc"", ""cinder"", config_path),
        os.path.join(CONF.state_path, ""etc"", config_path),
        os.path.join(CONF.state_path, config_path),
        ""/etc/cinder/%s"" % config_path,
    ]

    for path in possible_locations:
        if os.path.exists(path):
            return os.path.abspath(path)

    raise exception.ConfigNotFound(path=os.path.abspath(config_path))


def fetchfile(url, target):
    LOG.debug(_('Fetching %s') % url)
    execute('curl', '--fail', url, '-o', target)


def execute(*cmd, **kwargs):
    """"""Convenience wrapper around oslo's execute() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.execute(*cmd, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def trycmd(*args, **kwargs):
    """"""Convenience wrapper around oslo's trycmd() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.trycmd(*args, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def check_ssh_injection(cmd_list):
    ssh_injection_pattern = ['`', '$', '|', '||', ';', '&', '&&', '>', '>>',
                             '<']

    # Check whether injection attacks exist
    for arg in cmd_list:
        arg = arg.strip()
        # First, check no space in the middle of arg
        arg_len = len(arg.split())
        if arg_len > 1:
            raise exception.SSHInjectionThreat(command=str(cmd_list))

        # Second, check whether danger character in command. So the shell
        # special operator must be a single argument.
        for c in ssh_injection_pattern:
            if arg == c:
                continue

            result = arg.find(c)
            if not result == -1:
                if result == 0 or not arg[result - 1] == '\\':
                    raise exception.SSHInjectionThreat(command=cmd_list)


def ssh_execute(ssh, cmd, process_input=None,
                addl_env=None, check_exit_code=True):
    LOG.debug(_('Running cmd (SSH): %s'), cmd)
    if addl_env:
        raise exception.Error(_('Environment not supported over SSH'))

    if process_input:
        # This is (probably) fixable if we need it...
        raise exception.Error(_('process_input not supported over SSH'))

    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)
    channel = stdout_stream.channel

    #stdin.write('process_input would go here')
    #stdin.flush()

    # NOTE(justinsb): This seems suspicious...
    # ...other SSH clients have buffering issues with this approach
    stdout = stdout_stream.read()
    stderr = stderr_stream.read()
    stdin_stream.close()
    stdout_stream.close()
    stderr_stream.close()

    exit_status = channel.recv_exit_status()

    # exit_status == -1 if no exit code was returned
    if exit_status != -1:
        LOG.debug(_('Result was %s') % exit_status)
        if check_exit_code and exit_status != 0:
            raise exception.ProcessExecutionError(exit_code=exit_status,
                                                  stdout=stdout,
                                                  stderr=stderr,
                                                  cmd=cmd)
    channel.close()
    return (stdout, stderr)


def create_channel(client, width, height):
    """"""Invoke an interactive shell session on server.""""""
    channel = client.invoke_shell()
    channel.resize_pty(width, height)
    return channel


class SSHPool(pools.Pool):
    """"""A simple eventlet pool to hold ssh connections.""""""

    def __init__(self, ip, port, conn_timeout, login, password=None,
                 privatekey=None, *args, **kwargs):
        self.ip = ip
        self.port = port
        self.login = login
        self.password = password
        self.conn_timeout = conn_timeout if conn_timeout else None
        self.privatekey = privatekey
        super(SSHPool, self).__init__(*args, **kwargs)

    def create(self):
        try:
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            if self.password:
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            password=self.password,
                            timeout=self.conn_timeout)
            elif self.privatekey:
                pkfile = os.path.expanduser(self.privatekey)
                privatekey = paramiko.RSAKey.from_private_key_file(pkfile)
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            pkey=privatekey,
                            timeout=self.conn_timeout)
            else:
                msg = _(""Specify a password or private_key"")
                raise exception.CinderException(msg)

            # Paramiko by default sets the socket timeout to 0.1 seconds,
            # ignoring what we set thru the sshclient. This doesn't help for
            # keeping long lived connections. Hence we have to bypass it, by
            # overriding it after the transport is initialized. We are setting
            # the sockettimeout to None and setting a keepalive packet so that,
            # the server will keep the connection open. All that does is send
            # a keepalive packet every ssh_conn_timeout seconds.
            if self.conn_timeout:
                transport = ssh.get_transport()
                transport.sock.settimeout(None)
                transport.set_keepalive(self.conn_timeout)
            return ssh
        except Exception as e:
            msg = _(""Error connecting via ssh: %s"") % e
            LOG.error(msg)
            raise paramiko.SSHException(msg)

    def get(self):
        """"""
        Return an item from the pool, when one is available.  This may
        cause the calling greenthread to block. Check if a connection is active
        before returning it. For dead connections create and return a new
        connection.
        """"""
        conn = super(SSHPool, self).get()
        if conn:
            if conn.get_transport().is_active():
                return conn
            else:
                conn.close()
        return self.create()

    def remove(self, ssh):
        """"""Close an ssh client and remove it from free_items.""""""
        ssh.close()
        ssh = None
        if ssh in self.free_items:
            self.free_items.pop(ssh)
        if self.current_size > 0:
            self.current_size -= 1


def cinderdir():
    import cinder
    return os.path.abspath(cinder.__file__).split('cinder/__init__.py')[0]


def debug(arg):
    LOG.debug(_('debug in callback: %s'), arg)
    return arg


def generate_uid(topic, size=8):
    characters = '01234567890abcdefghijklmnopqrstuvwxyz'
    choices = [random.choice(characters) for x in xrange(size)]
    return '%s-%s' % (topic, ''.join(choices))


# Default symbols to use for passwords. Avoids visually confusing characters.
# ~6 bits per symbol
DEFAULT_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0,1
                            'ABCDEFGHJKLMNPQRSTUVWXYZ',   # Removed: I, O
                            'abcdefghijkmnopqrstuvwxyz')  # Removed: l


# ~5 bits per symbol
EASIER_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0, 1
                           'ABCDEFGHJKLMNPQRSTUVWXYZ')  # Removed: I, O


def last_completed_audit_period(unit=None):
    """"""This method gives you the most recently *completed* audit period.

    arguments:
            units: string, one of 'hour', 'day', 'month', 'year'
                    Periods normally begin at the beginning (UTC) of the
                    period unit (So a 'day' period begins at midnight UTC,
                    a 'month' unit on the 1st, a 'year' on Jan, 1)
                    unit string may be appended with an optional offset
                    like so:  'day@18'  This will begin the period at 18:00
                    UTC.  'month@15' starts a monthly period on the 15th,
                    and year@3 begins a yearly one on March 1st.


    returns:  2 tuple of datetimes (begin, end)
              The begin timestamp of this audit period is the same as the
              end of the previous.
    """"""
    if not unit:
        unit = CONF.volume_usage_audit_period

    offset = 0
    if '@' in unit:
        unit, offset = unit.split(""@"", 1)
        offset = int(offset)

    rightnow = timeutils.utcnow()
    if unit not in ('month', 'day', 'year', 'hour'):
        raise ValueError('Time period must be hour, day, month or year')
    if unit == 'month':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=offset,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            year = rightnow.year
            if 1 >= rightnow.month:
                year -= 1
                month = 12 + (rightnow.month - 1)
            else:
                month = rightnow.month - 1
            end = datetime.datetime(day=offset,
                                    month=month,
                                    year=year)
        year = end.year
        if 1 >= end.month:
            year -= 1
            month = 12 + (end.month - 1)
        else:
            month = end.month - 1
        begin = datetime.datetime(day=offset, month=month, year=year)

    elif unit == 'year':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=1, month=offset, year=rightnow.year)
        if end >= rightnow:
            end = datetime.datetime(day=1,
                                    month=offset,
                                    year=rightnow.year - 1)
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 2)
        else:
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 1)

    elif unit == 'day':
        end = datetime.datetime(hour=offset,
                                day=rightnow.day,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            end = end - datetime.timedelta(days=1)
        begin = end - datetime.timedelta(days=1)

    elif unit == 'hour':
        end = rightnow.replace(minute=offset, second=0, microsecond=0)
        if end >= rightnow:
            end = end - datetime.timedelta(hours=1)
        begin = end - datetime.timedelta(hours=1)

    return (begin, end)


def generate_password(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    """"""Generate a random password from the supplied symbol groups.

    At least one symbol from each group will be included. Unpredictable
    results if length is less than the number of symbol groups.

    Believed to be reasonably secure (with a reasonable password length!)

    """"""
    r = random.SystemRandom()

    # NOTE(jerdfelt): Some password policies require at least one character
    # from each group of symbols, so start off with one random character
    # from each symbol group
    password = [r.choice(s) for s in symbolgroups]
    # If length < len(symbolgroups), the leading characters will only
    # be from the first length groups. Try our best to not be predictable
    # by shuffling and then truncating.
    r.shuffle(password)
    password = password[:length]
    length -= len(password)

    # then fill with random characters from all symbol groups
    symbols = ''.join(symbolgroups)
    password.extend([r.choice(symbols) for _i in xrange(length)])

    # finally shuffle to ensure first x characters aren't from a
    # predictable group
    r.shuffle(password)

    return ''.join(password)


def generate_username(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    # Use the same implementation as the password generation.
    return generate_password(length, symbolgroups)


def last_octet(address):
    return int(address.split('.')[-1])


def get_my_linklocal(interface):
    try:
        if_str = execute('ip', '-f', 'inet6', '-o', 'addr', 'show', interface)
        condition = '\s+inet6\s+([0-9a-f:]+)/\d+\s+scope\s+link'
        links = [re.search(condition, x) for x in if_str[0].split('\n')]
        address = [w.group(1) for w in links if w is not None]
        if address[0] is not None:
            return address[0]
        else:
            raise exception.Error(_('Link Local address is not found.:%s')
                                  % if_str)
    except Exception as ex:
        raise exception.Error(_(""Couldn't get Link Local IP of %(interface)s""
                                "" :%(ex)s"") %
                              {'interface': interface, 'ex': ex, })


def parse_mailmap(mailmap='.mailmap'):
    mapping = {}
    if os.path.exists(mailmap):
        fp = open(mailmap, 'r')
        for l in fp:
            l = l.strip()
            if not l.startswith('#') and ' ' in l:
                canonical_email, alias = l.split(' ')
                mapping[alias.lower()] = canonical_email.lower()
    return mapping


def str_dict_replace(s, mapping):
    for s1, s2 in mapping.iteritems():
        s = s.replace(s1, s2)
    return s


class LazyPluggable(object):
    """"""A pluggable backend loaded lazily based on some value.""""""

    def __init__(self, pivot, **backends):
        self.__backends = backends
        self.__pivot = pivot
        self.__backend = None

    def __get_backend(self):
        if not self.__backend:
            backend_name = CONF[self.__pivot]
            if backend_name not in self.__backends:
                raise exception.Error(_('Invalid backend: %s') % backend_name)

            backend = self.__backends[backend_name]
            if isinstance(backend, tuple):
                name = backend[0]
                fromlist = backend[1]
            else:
                name = backend
                fromlist = backend

            self.__backend = __import__(name, None, None, fromlist)
            LOG.debug(_('backend %s'), self.__backend)
        return self.__backend

    def __getattr__(self, key):
        backend = self.__get_backend()
        return getattr(backend, key)


class LoopingCallDone(Exception):
    """"""Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """"""

    def __init__(self, retvalue=True):
        """""":param retvalue: Value that LoopingCall.wait() should return.""""""
        self.retvalue = retvalue


class LoopingCall(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    self.f(*self.args, **self.kw)
                    if not self._running:
                        break
                    greenthread.sleep(interval)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class ProtectedExpatParser(expatreader.ExpatParser):
    """"""An expat parser which disables DTD's and entities by default.""""""

    def __init__(self, forbid_dtd=True, forbid_entities=True,
                 *args, **kwargs):
        # Python 2.x old style class
        expatreader.ExpatParser.__init__(self, *args, **kwargs)
        self.forbid_dtd = forbid_dtd
        self.forbid_entities = forbid_entities

    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):
        raise ValueError(""Inline DTD forbidden"")

    def entity_decl(self, entityName, is_parameter_entity, value, base,
                    systemId, publicId, notationName):
        raise ValueError(""<!ENTITY> forbidden"")

    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):
        # expat 1.2
        raise ValueError(""<!ENTITY> forbidden"")

    def reset(self):
        expatreader.ExpatParser.reset(self)
        if self.forbid_dtd:
            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl
        if self.forbid_entities:
            self._parser.EntityDeclHandler = self.entity_decl
            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl


def safe_minidom_parse_string(xml_string):
    """"""Parse an XML string using minidom safely.

    """"""
    try:
        return minidom.parseString(xml_string, parser=ProtectedExpatParser())
    except sax.SAXParseException as se:
        raise expat.ExpatError()


def xhtml_escape(value):
    """"""Escapes a string so it is valid within XML or XHTML.

    """"""
    return saxutils.escape(value, {'""': '&quot;', ""'"": '&apos;'})


def utf8(value):
    """"""Try to turn a string into utf-8 if possible.

    """"""
    if isinstance(value, unicode):
        return value.encode('utf-8')
    elif isinstance(value, str):
        return value
    else:
        raise ValueError(""%s is not a string"" % value)


def get_from_path(items, path):
    """"""Returns a list of items matching the specified path.

    Takes an XPath-like expression e.g. prop1/prop2/prop3, and for each item
    in items, looks up items[prop1][prop2][prop3]. Like XPath, if any of the
    intermediate results are lists it will treat each list item individually.
    A 'None' in items or any child expressions will be ignored, this function
    will not throw because of None (anywhere) in items.  The returned list
    will contain no None values.

    """"""
    if path is None:
        raise exception.Error('Invalid mini_xpath')

    (first_token, sep, remainder) = path.partition('/')

    if first_token == '':
        raise exception.Error('Invalid mini_xpath')

    results = []

    if items is None:
        return results

    if not isinstance(items, list):
        # Wrap single objects in a list
        items = [items]

    for item in items:
        if item is None:
            continue
        get_method = getattr(item, 'get', None)
        if get_method is None:
            continue
        child = get_method(first_token)
        if child is None:
            continue
        if isinstance(child, list):
            # Flatten intermediate lists
            for x in child:
                results.append(x)
        else:
            results.append(child)

    if not sep:
        # No more tokens
        return results
    else:
        return get_from_path(results, remainder)


def flatten_dict(dict_, flattened=None):
    """"""Recursively flatten a nested dictionary.""""""
    flattened = flattened or {}
    for key, value in dict_.iteritems():
        if hasattr(value, 'iteritems'):
            flatten_dict(value, flattened)
        else:
            flattened[key] = value
    return flattened


def partition_dict(dict_, keys):
    """"""Return two dicts, one with `keys` the other with everything else.""""""
    intersection = {}
    difference = {}
    for key, value in dict_.iteritems():
        if key in keys:
            intersection[key] = value
        else:
            difference[key] = value
    return intersection, difference


def map_dict_keys(dict_, key_map):
    """"""Return a dict in which the dictionaries keys are mapped to new keys.""""""
    mapped = {}
    for key, value in dict_.iteritems():
        mapped_key = key_map[key] if key in key_map else key
        mapped[mapped_key] = value
    return mapped


def subset_dict(dict_, keys):
    """"""Return a dict that only contains a subset of keys.""""""
    subset = partition_dict(dict_, keys)[0]
    return subset


def check_isinstance(obj, cls):
    """"""Checks that obj is of type cls, and lets PyLint infer types.""""""
    if isinstance(obj, cls):
        return obj
    raise Exception(_('Expected object of type: %s') % (str(cls)))
    # TODO(justinsb): Can we make this better??
    return cls()  # Ugly PyLint hack


def is_valid_boolstr(val):
    """"""Check if the provided string is a valid bool string or not.""""""
    val = str(val).lower()
    return (val == 'true' or val == 'false' or
            val == 'yes' or val == 'no' or
            val == 'y' or val == 'n' or
            val == '1' or val == '0')


def is_valid_ipv4(address):
    """"""valid the address strictly as per format xxx.xxx.xxx.xxx.
    where xxx is a value between 0 and 255.
    """"""
    parts = address.split(""."")
    if len(parts) != 4:
        return False
    for item in parts:
        try:
            if not 0 <= int(item) <= 255:
                return False
        except ValueError:
            return False
    return True


def monkey_patch():
    """"""If the CONF.monkey_patch set as True,
    this function patches a decorator
    for all functions in specified modules.

    You can set decorators for each modules
    using CONF.monkey_patch_modules.
    The format is ""Module path:Decorator function"".
    Example: 'cinder.api.ec2.cloud:' \
     cinder.openstack.common.notifier.api.notify_decorator'

    Parameters of the decorator is as follows.
    (See cinder.openstack.common.notifier.api.notify_decorator)

    name - name of the function
    function - object of the function
    """"""
    # If CONF.monkey_patch is not True, this function do nothing.
    if not CONF.monkey_patch:
        return
    # Get list of modules and decorators
    for module_and_decorator in CONF.monkey_patch_modules:
        module, decorator_name = module_and_decorator.split(':')
        # import decorator function
        decorator = importutils.import_class(decorator_name)
        __import__(module)
        # Retrieve module information using pyclbr
        module_data = pyclbr.readmodule_ex(module)
        for key in module_data.keys():
            # set the decorator for the class methods
            if isinstance(module_data[key], pyclbr.Class):
                clz = importutils.import_class(""%s.%s"" % (module, key))
                for method, func in inspect.getmembers(clz, inspect.ismethod):
                    setattr(
                        clz, method,
                        decorator(""%s.%s.%s"" % (module, key, method), func))
            # set the decorator for the function
            if isinstance(module_data[key], pyclbr.Function):
                func = importutils.import_class(""%s.%s"" % (module, key))
                setattr(sys.modules[module], key,
                        decorator(""%s.%s"" % (module, key), func))


def convert_to_list_dict(lst, label):
    """"""Convert a value or list into a list of dicts""""""
    if not lst:
        return None
    if not isinstance(lst, list):
        lst = [lst]
    return [{label: x} for x in lst]


def timefunc(func):
    """"""Decorator that logs how long a particular function took to execute""""""
    @functools.wraps(func)
    def inner(*args, **kwargs):
        start_time = time.time()
        try:
            return func(*args, **kwargs)
        finally:
            total_time = time.time() - start_time
            LOG.debug(_(""timefunc: '%(name)s' took %(total_time).2f secs"") %
                      dict(name=func.__name__, total_time=total_time))
    return inner


def generate_glance_url():
    """"""Generate the URL to glance.""""""
    # TODO(jk0): This will eventually need to take SSL into consideration
    # when supported in glance.
    return ""http://%s:%d"" % (CONF.glance_host, CONF.glance_port)


@contextlib.contextmanager
def logging_error(message):
    """"""Catches exception, write message to the log, re-raise.
    This is a common refinement of save_and_reraise that writes a specific
    message to the log.
    """"""
    try:
        yield
    except Exception as error:
        with excutils.save_and_reraise_exception():
            LOG.exception(message)


def make_dev_path(dev, partition=None, base='/dev'):
    """"""Return a path to a particular device.

    >>> make_dev_path('xvdc')
    /dev/xvdc

    >>> make_dev_path('xvdc', 1)
    /dev/xvdc1
    """"""
    path = os.path.join(base, dev)
    if partition:
        path += str(partition)
    return path


def total_seconds(td):
    """"""Local total_seconds implementation for compatibility with python 2.6""""""
    if hasattr(td, 'total_seconds'):
        return td.total_seconds()
    else:
        return ((td.days * 86400 + td.seconds) * 10 ** 6 +
                td.microseconds) / 10.0 ** 6


def sanitize_hostname(hostname):
    """"""Return a hostname which conforms to RFC-952 and RFC-1123 specs.""""""
    if isinstance(hostname, unicode):
        hostname = hostname.encode('latin-1', 'ignore')

    hostname = re.sub('[ _]', '-', hostname)
    hostname = re.sub('[^\w.-]+', '', hostname)
    hostname = hostname.lower()
    hostname = hostname.strip('.-')

    return hostname


def read_cached_file(filename, cache_info, reload_func=None):
    """"""Read from a file if it has been modified.

    :param cache_info: dictionary to hold opaque cache.
    :param reload_func: optional function to be called with data when
                        file is reloaded due to a modification.

    :returns: data from file

    """"""
    mtime = os.path.getmtime(filename)
    if not cache_info or mtime != cache_info.get('mtime'):
        with open(filename) as fap:
            cache_info['data'] = fap.read()
        cache_info['mtime'] = mtime
        if reload_func:
            reload_func(cache_info['data'])
    return cache_info['data']


def hash_file(file_like_object):
    """"""Generate a hash for the contents of a file.""""""
    checksum = hashlib.sha1()
    any(map(checksum.update, iter(lambda: file_like_object.read(32768), '')))
    return checksum.hexdigest()


@contextlib.contextmanager
def temporary_mutation(obj, **kwargs):
    """"""Temporarily set the attr on a particular object to a given value then
    revert when finished.

    One use of this is to temporarily set the read_deleted flag on a context
    object:

        with temporary_mutation(context, read_deleted=""yes""):
            do_something_that_needed_deleted_objects()
    """"""
    NOT_PRESENT = object()

    old_values = {}
    for attr, new_value in kwargs.items():
        old_values[attr] = getattr(obj, attr, NOT_PRESENT)
        setattr(obj, attr, new_value)

    try:
        yield
    finally:
        for attr, old_value in old_values.items():
            if old_value is NOT_PRESENT:
                del obj[attr]
            else:
                setattr(obj, attr, old_value)


def service_is_up(service):
    """"""Check whether a service is up based on last heartbeat.""""""
    last_heartbeat = service['updated_at'] or service['created_at']
    # Timestamps in DB are UTC.
    elapsed = total_seconds(timeutils.utcnow() - last_heartbeat)
    return abs(elapsed) <= CONF.service_down_time


def generate_mac_address():
    """"""Generate an Ethernet MAC address.""""""
    # NOTE(vish): We would prefer to use 0xfe here to ensure that linux
    #             bridge mac addresses don't change, but it appears to
    #             conflict with libvirt, so we use the next highest octet
    #             that has the unicast and locally administered bits set
    #             properly: 0xfa.
    #             Discussion: https://bugs.launchpad.net/cinder/+bug/921838
    mac = [0xfa, 0x16, 0x3e,
           random.randint(0x00, 0x7f),
           random.randint(0x00, 0xff),
           random.randint(0x00, 0xff)]
    return ':'.join(map(lambda x: ""%02x"" % x, mac))


def read_file_as_root(file_path):
    """"""Secure helper to read file as root.""""""
    try:
        out, _err = execute('cat', file_path, run_as_root=True)
        return out
    except exception.ProcessExecutionError:
        raise exception.FileNotFound(file_path=file_path)


@contextlib.contextmanager
def temporary_chown(path, owner_uid=None):
    """"""Temporarily chown a path.

    :params owner_uid: UID of temporary owner (defaults to current user)
    """"""
    if owner_uid is None:
        owner_uid = os.getuid()

    orig_uid = os.stat(path).st_uid

    if orig_uid != owner_uid:
        execute('chown', owner_uid, path, run_as_root=True)
    try:
        yield
    finally:
        if orig_uid != owner_uid:
            execute('chown', orig_uid, path, run_as_root=True)


@contextlib.contextmanager
def tempdir(**kwargs):
    tmpdir = tempfile.mkdtemp(**kwargs)
    try:
        yield tmpdir
    finally:
        try:
            shutil.rmtree(tmpdir)
        except OSError as e:
            LOG.debug(_('Could not remove tmpdir: %s'), str(e))


def strcmp_const_time(s1, s2):
    """"""Constant-time string comparison.

    :params s1: the first string
    :params s2: the second string

    :return: True if the strings are equal.

    This function takes two strings and compares them.  It is intended to be
    used when doing a comparison for authentication purposes to help guard
    against timing attacks.
    """"""
    if len(s1) != len(s2):
        return False
    result = 0
    for (a, b) in zip(s1, s2):
        result |= ord(a) ^ ord(b)
    return result == 0


def walk_class_hierarchy(clazz, encountered=None):
    """"""Walk class hierarchy, yielding most derived classes first""""""
    if not encountered:
        encountered = []
    for subclass in clazz.__subclasses__():
        if subclass not in encountered:
            encountered.append(subclass)
            # drill down to leaves first
            for subsubclass in walk_class_hierarchy(subclass, encountered):
                yield subsubclass
            yield subclass


class UndoManager(object):
    """"""Provides a mechanism to facilitate rolling back a series of actions
    when an exception is raised.
    """"""
    def __init__(self):
        self.undo_stack = []

    def undo_with(self, undo_func):
        self.undo_stack.append(undo_func)

    def _rollback(self):
        for undo_func in reversed(self.undo_stack):
            undo_func()

    def rollback_and_reraise(self, msg=None, **kwargs):
        """"""Rollback a series of actions then re-raise the exception.

        .. note:: (sirp) This should only be called within an
                  exception handler.
        """"""
        with excutils.save_and_reraise_exception():
            if msg:
                LOG.exception(msg, **kwargs)

            self._rollback()
/n/n/ncinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, cmd_list, check_exit_code=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/ncinder/volume/drivers/storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Volume driver for IBM Storwize family and SVC storage systems.

Notes:
1. If you specify both a password and a key file, this driver will use the
   key file only.
2. When using a key file for authentication, it is up to the user or
   system administrator to store the private key in a safe manner.
3. The defaults for creating volumes are ""-rsize 2% -autoexpand
   -grainsize 256 -warning 0"".  These can be changed in the configuration
   file or by using volume types(recommended only for advanced users).

Limitations:
1. The driver expects CLI output in English, error messages may be in a
   localized format.
2. Clones and creating volumes from snapshots, where the source and target
   are of different sizes, is not supported.

""""""

import random
import re
import string
import time

from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import strutils
from cinder import utils
from cinder.volume.drivers.san import san
from cinder.volume import volume_types

VERSION = 1.1
LOG = logging.getLogger(__name__)

storwize_svc_opts = [
    cfg.StrOpt('storwize_svc_volpool_name',
               default='volpool',
               help='Storage system storage pool for volumes'),
    cfg.IntOpt('storwize_svc_vol_rsize',
               default=2,
               help='Storage system space-efficiency parameter for volumes '
                    '(percentage)'),
    cfg.IntOpt('storwize_svc_vol_warning',
               default=0,
               help='Storage system threshold for volume capacity warnings '
                    '(percentage)'),
    cfg.BoolOpt('storwize_svc_vol_autoexpand',
                default=True,
                help='Storage system autoexpand parameter for volumes '
                     '(True/False)'),
    cfg.IntOpt('storwize_svc_vol_grainsize',
               default=256,
               help='Storage system grain size parameter for volumes '
                    '(32/64/128/256)'),
    cfg.BoolOpt('storwize_svc_vol_compression',
                default=False,
                help='Storage system compression option for volumes'),
    cfg.BoolOpt('storwize_svc_vol_easytier',
                default=True,
                help='Enable Easy Tier for volumes'),
    cfg.IntOpt('storwize_svc_flashcopy_timeout',
               default=120,
               help='Maximum number of seconds to wait for FlashCopy to be '
                    'prepared. Maximum value is 600 seconds (10 minutes).'),
    cfg.StrOpt('storwize_svc_connection_protocol',
               default='iSCSI',
               help='Connection protocol (iSCSI/FC)'),
    cfg.BoolOpt('storwize_svc_multipath_enabled',
                default=False,
                help='Connect with multipath (currently FC-only)'),
    cfg.BoolOpt('storwize_svc_multihostmap_enabled',
                default=True,
                help='Allows vdisk to multi host mapping'),
]


CONF = cfg.CONF
CONF.register_opts(storwize_svc_opts)


class StorwizeSVCDriver(san.SanDriver):
    """"""IBM Storwize V7000 and SVC iSCSI/FC volume driver.

    Version history:
    1.0 - Initial driver
    1.1 - FC support, create_cloned_volume, volume type support,
          get_volume_stats, minor bug fixes

    """"""

    """"""=====================================================================""""""
    """""" SETUP                                                               """"""
    """"""=====================================================================""""""

    def __init__(self, *args, **kwargs):
        super(StorwizeSVCDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(storwize_svc_opts)
        self._storage_nodes = {}
        self._enabled_protocols = set()
        self._compression_enabled = False
        self._context = None

        # Build cleanup translation tables for host names
        invalid_ch_in_host = ''
        for num in range(0, 128):
            ch = str(chr(num))
            if (not ch.isalnum() and ch != ' ' and ch != '.'
                    and ch != '-' and ch != '_'):
                invalid_ch_in_host = invalid_ch_in_host + ch
        self._string_host_name_filter = string.maketrans(
            invalid_ch_in_host, '-' * len(invalid_ch_in_host))

        self._unicode_host_name_filter = dict((ord(unicode(char)), u'-')
                                              for char in invalid_ch_in_host)

    def _get_iscsi_ip_addrs(self):
        generator = self._port_conf_generator(['svcinfo', 'lsportip'])
        header = next(generator, None)
        if not header:
            return

        for port_data in generator:
            try:
                port_node_id = port_data['node_id']
                port_ipv4 = port_data['IP_address']
                port_ipv6 = port_data['IP_address_6']
                state = port_data['state']
            except KeyError:
                self._handle_keyerror('lsportip', header)

            if port_node_id in self._storage_nodes and (
                    state == 'configured' or state == 'online'):
                node = self._storage_nodes[port_node_id]
                if len(port_ipv4):
                    node['ipv4'].append(port_ipv4)
                if len(port_ipv6):
                    node['ipv6'].append(port_ipv6)

    def _get_fc_wwpns(self):
        for key in self._storage_nodes:
            node = self._storage_nodes[key]
            ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!', node['id']]
            raw = self._run_ssh(ssh_cmd)
            resp = CLIResponse(raw, delim='!', with_header=False)
            wwpns = set(node['WWPN'])
            for i, s in resp.select('port_id', 'port_status'):
                if 'unconfigured' != s:
                    wwpns.add(i)
            node['WWPN'] = list(wwpns)
            LOG.info(_('WWPN on node %(node)s: %(wwpn)s')
                     % {'node': node['id'], 'wwpn': node['WWPN']})

    def do_setup(self, ctxt):
        """"""Check that we have all configuration details from the storage.""""""

        LOG.debug(_('enter: do_setup'))
        self._context = ctxt

        # Validate that the pool exists
        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-delim', '!', '-nohdr']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)
        search_text = '!%s!' % self.configuration.storwize_svc_volpool_name
        if search_text not in out:
            raise exception.InvalidInput(
                reason=(_('pool %s doesn\'t exist')
                        % self.configuration.storwize_svc_volpool_name))

        # Check if compression is supported
        self._compression_enabled = False
        try:
            ssh_cmd = ['svcinfo', 'lslicense', '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)
            license_lines = out.strip().split('\n')
            for license_line in license_lines:
                name, foo, value = license_line.partition('!')
                if name in ('license_compression_enclosures',
                            'license_compression_capacity') and value != '0':
                    self._compression_enabled = True
                    break
        except exception.ProcessExecutionError:
            LOG.exception(_('Failed to get license information.'))

        # Get the iSCSI and FC names of the Storwize/SVC nodes
        ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)

        nodes = out.strip().split('\n')
        self._assert_ssh_return(len(nodes),
                                'do_setup', ssh_cmd, out, err)
        header = nodes.pop(0)
        for node_line in nodes:
            try:
                node_data = self._get_hdr_dic(header, node_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('do_setup',
                                               ssh_cmd, out, err)
            node = {}
            try:
                node['id'] = node_data['id']
                node['name'] = node_data['name']
                node['IO_group'] = node_data['IO_group_id']
                node['iscsi_name'] = node_data['iscsi_name']
                node['WWNN'] = node_data['WWNN']
                node['status'] = node_data['status']
                node['WWPN'] = []
                node['ipv4'] = []
                node['ipv6'] = []
                node['enabled_protocols'] = []
                if node['status'] == 'online':
                    self._storage_nodes[node['id']] = node
            except KeyError:
                self._handle_keyerror('lsnode', header)

        # Get the iSCSI IP addresses and WWPNs of the Storwize/SVC nodes
        self._get_iscsi_ip_addrs()
        self._get_fc_wwpns()

        # For each node, check what connection modes it supports.  Delete any
        # nodes that do not support any types (may be partially configured).
        to_delete = []
        for k, node in self._storage_nodes.iteritems():
            if ((len(node['ipv4']) or len(node['ipv6']))
                    and len(node['iscsi_name'])):
                node['enabled_protocols'].append('iSCSI')
                self._enabled_protocols.add('iSCSI')
            if len(node['WWPN']):
                node['enabled_protocols'].append('FC')
                self._enabled_protocols.add('FC')
            if not len(node['enabled_protocols']):
                to_delete.append(k)

        for delkey in to_delete:
            del self._storage_nodes[delkey]

        # Make sure we have at least one node configured
        self._driver_assert(len(self._storage_nodes),
                            _('do_setup: No configured nodes'))

        LOG.debug(_('leave: do_setup'))

    def _build_default_opts(self):
        # Ignore capitalization
        protocol = self.configuration.storwize_svc_connection_protocol
        if protocol.lower() == 'fc':
            protocol = 'FC'
        elif protocol.lower() == 'iscsi':
            protocol = 'iSCSI'

        opt = {'rsize': self.configuration.storwize_svc_vol_rsize,
               'warning': self.configuration.storwize_svc_vol_warning,
               'autoexpand': self.configuration.storwize_svc_vol_autoexpand,
               'grainsize': self.configuration.storwize_svc_vol_grainsize,
               'compression': self.configuration.storwize_svc_vol_compression,
               'easytier': self.configuration.storwize_svc_vol_easytier,
               'protocol': protocol,
               'multipath': self.configuration.storwize_svc_multipath_enabled}
        return opt

    def check_for_setup_error(self):
        """"""Ensure that the flags are set properly.""""""
        LOG.debug(_('enter: check_for_setup_error'))

        required_flags = ['san_ip', 'san_ssh_port', 'san_login',
                          'storwize_svc_volpool_name']
        for flag in required_flags:
            if not self.configuration.safe_get(flag):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

        # Ensure that either password or keyfile were set
        if not (self.configuration.san_password or
                self.configuration.san_private_key):
            raise exception.InvalidInput(
                reason=_('Password or SSH private key is required for '
                         'authentication: set either san_password or '
                         'san_private_key option'))

        # Check that flashcopy_timeout is not more than 10 minutes
        flashcopy_timeout = self.configuration.storwize_svc_flashcopy_timeout
        if not (flashcopy_timeout > 0 and flashcopy_timeout <= 600):
            raise exception.InvalidInput(
                reason=_('Illegal value %d specified for '
                         'storwize_svc_flashcopy_timeout: '
                         'valid values are between 0 and 600')
                % flashcopy_timeout)

        opts = self._build_default_opts()
        self._check_vdisk_opts(opts)

        LOG.debug(_('leave: check_for_setup_error'))

    """"""=====================================================================""""""
    """""" INITIALIZE/TERMINATE CONNECTIONS                                    """"""
    """"""=====================================================================""""""

    def ensure_export(self, ctxt, volume):
        """"""Check that the volume exists on the storage.

        The system does not ""export"" volumes as a Linux iSCSI target does,
        and therefore we just check that the volume exists on the storage.
        """"""
        volume_defined = self._is_vdisk_defined(volume['name'])
        if not volume_defined:
            LOG.error(_('ensure_export: Volume %s not found on storage')
                      % volume['name'])

    def create_export(self, ctxt, volume):
        model_update = None
        return model_update

    def remove_export(self, ctxt, volume):
        pass

    def _add_chapsecret_to_host(self, host_name):
        """"""Generate and store a randomly-generated CHAP secret for the host.""""""

        chap_secret = utils.generate_password()
        ssh_cmd = ['svctask', 'chhost', '-chapsecret', chap_secret, host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from chhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_add_chapsecret_to_host', ssh_cmd, out, err)
        return chap_secret

    def _get_chap_secret_for_host(self, host_name):
        """"""Return the CHAP secret for the given host.""""""

        LOG.debug(_('enter: _get_chap_secret_for_host: host name %s')
                  % host_name)

        ssh_cmd = ['svcinfo', 'lsiscsiauth', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        host_lines = out.strip().split('\n')
        self._assert_ssh_return(len(host_lines), '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        header = host_lines.pop(0).split('!')
        self._assert_ssh_return('name' in header, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_auth_method' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_chap_secret' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        name_index = header.index('name')
        method_index = header.index('iscsi_auth_method')
        secret_index = header.index('iscsi_chap_secret')

        chap_secret = None
        host_found = False
        for line in host_lines:
            info = line.split('!')
            if info[name_index] == host_name:
                host_found = True
                if info[method_index] == 'chap':
                    chap_secret = info[secret_index]

        self._assert_ssh_return(host_found, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        LOG.debug(_('leave: _get_chap_secret_for_host: host name '
                    '%(host_name)s with secret %(chap_secret)s')
                  % {'host_name': host_name, 'chap_secret': chap_secret})

        return chap_secret

    def _connector_to_hostname_prefix(self, connector):
        """"""Translate connector info to storage system host name.

        Translate a host's name and IP to the prefix of its hostname on the
        storage subsystem.  We create a host name host name from the host and
        IP address, replacing any invalid characters (at most 55 characters),
        and adding a random 8-character suffix to avoid collisions. The total
        length should be at most 63 characters.

        """"""

        host_name = connector['host']
        if isinstance(host_name, unicode):
            host_name = host_name.translate(self._unicode_host_name_filter)
        elif isinstance(host_name, str):
            host_name = host_name.translate(self._string_host_name_filter)
        else:
            msg = _('_create_host: Cannot clean host name. Host name '
                    'is not unicode or string')
            LOG.error(msg)
            raise exception.NoValidHost(reason=msg)

        host_name = str(host_name)
        return host_name[:55]

    def _find_host_from_wwpn(self, connector):
        for wwpn in connector['wwpns']:
            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)

            if not len(out.strip()):
                # This WWPN is not in use
                continue

            host_lines = out.strip().split('\n')
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('remote_wwpn' in header and
                                    'name' in header,
                                    '_find_host_from_wwpn',
                                    ssh_cmd, out, err)
            rmt_wwpn_idx = header.index('remote_wwpn')
            name_idx = header.index('name')

            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)

            if wwpn in wwpns:
                # All the wwpns will be the mapping for the same
                # host from this WWPN-based query. Just pick
                # the name from first line.
                hostname = host_lines[0].split('!')[name_idx]
                return hostname

        # Didn't find a host
        return None

    def _find_host_exhaustive(self, connector, hosts):
        for host in hosts:
            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]
            out, err = self._run_ssh(ssh_cmd)
            self._assert_ssh_return(len(out.strip()),
                                    '_find_host_exhaustive',
                                    ssh_cmd, out, err)
            for attr_line in out.split('\n'):
                # If '!' not found, return the string and two empty strings
                attr_name, foo, attr_val = attr_line.partition('!')
                if (attr_name == 'iscsi_name' and
                        'initiator' in connector and
                        attr_val == connector['initiator']):
                    return host
                elif (attr_name == 'WWPN' and
                      'wwpns' in connector and
                      attr_val.lower() in
                      map(str.lower, map(str, connector['wwpns']))):
                        return host
        return None

    def _get_host_from_connector(self, connector):
        """"""List the hosts defined in the storage.

        Return the host name with the given connection info, or None if there
        is no host fitting that information.

        """"""

        prefix = self._connector_to_hostname_prefix(connector)
        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)

        # Get list of host in the storage
        ssh_cmd = ['svcinfo', 'lshost', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        # If we have FC information, we have a faster lookup option
        hostname = None
        if 'wwpns' in connector:
            hostname = self._find_host_from_wwpn(connector)

        # If we don't have a hostname yet, try the long way
        if not hostname:
            host_lines = out.strip().split('\n')
            self._assert_ssh_return(len(host_lines),
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('name' in header,
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            name_index = header.index('name')
            hosts = map(lambda x: x.split('!')[name_index], host_lines)
            hostname = self._find_host_exhaustive(connector, hosts)

        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)

        return hostname

    def _create_host(self, connector):
        """"""Create a new host on the storage system.

        We create a host name and associate it with the given connection
        information.

        """"""

        LOG.debug(_('enter: _create_host: host %s') % connector['host'])

        rand_id = str(random.randint(0, 99999999)).zfill(8)
        host_name = '%s-%s' % (self._connector_to_hostname_prefix(connector),
                               rand_id)

        # Get all port information from the connector
        ports = []
        if 'initiator' in connector:
            ports.append('-iscsiname %s' % connector['initiator'])
        if 'wwpns' in connector:
            for wwpn in connector['wwpns']:
                ports.append('-hbawwpn %s' % wwpn)

        # When creating a host, we need one port
        self._driver_assert(len(ports), _('_create_host: No connector ports'))
        port1 = ports.pop(0)
        arg_name, arg_val = port1.split()
        ssh_cmd = ['svctask', 'mkhost', '-force', arg_name, arg_val, '-name',
                   '""%s""' % host_name]
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return('successfully created' in out,
                                '_create_host', ssh_cmd, out, err)

        # Add any additional ports to the host
        for port in ports:
            arg_name, arg_val = port.split()
            ssh_cmd = ['svctask', 'addhostport', '-force', arg_name, arg_val,
                       host_name]
            out, err = self._run_ssh(ssh_cmd)

        LOG.debug(_('leave: _create_host: host %(host)s - %(host_name)s') %
                  {'host': connector['host'], 'host_name': host_name})
        return host_name

    def _get_hostvdisk_mappings(self, host_name):
        """"""Return the defined storage mappings for a host.""""""

        return_data = {}
        ssh_cmd = ['svcinfo', 'lshostvdiskmap', '-delim', '!', host_name]
        out, err = self._run_ssh(ssh_cmd)

        mappings = out.strip().split('\n')
        if len(mappings):
            header = mappings.pop(0)
            for mapping_line in mappings:
                mapping_data = self._get_hdr_dic(header, mapping_line, '!')
                return_data[mapping_data['vdisk_name']] = mapping_data

        return return_data

    def _map_vol_to_host(self, volume_name, host_name):
        """"""Create a mapping between a volume to a host.""""""

        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '
                    'host %(host_name)s')
                  % {'volume_name': volume_name, 'host_name': host_name})

        # Check if this volume is already mapped to this host
        mapping_data = self._get_hostvdisk_mappings(host_name)

        mapped_flag = False
        result_lun = '-1'
        if volume_name in mapping_data:
            mapped_flag = True
            result_lun = mapping_data[volume_name]['SCSI_id']
        else:
            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]
            lun_used.sort()
            # Assume all luns are taken to this point, and then try to find
            # an unused one
            result_lun = str(len(lun_used))
            for index, n in enumerate(lun_used):
                if n > index:
                    result_lun = str(index)
                    break

        # Volume is not mapped to host, create a new LUN
        if not mapped_flag:
            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,
                       '-scsi', result_lun, volume_name]
            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)
            if err and err.startswith('CMMVC6071E'):
                if not self.configuration.storwize_svc_multihostmap_enabled:
                    LOG.error(_('storwize_svc_multihostmap_enabled is set '
                                'to False, Not allow multi host mapping'))
                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\
                                    'was not created because the VDisk is '\
                                    'already mapped to a host.\n""'
                    raise exception.CinderException(data=exception_msg)

                for i in range(len(ssh_cmd)):
                    if ssh_cmd[i] == 'mkvdiskhostmap':
                        ssh_cmd.insert(i + 1, '-force')

                # try to map one volume to multiple hosts
                out, err = self._run_ssh(ssh_cmd)
                LOG.warn(_('volume %s mapping to multi host') % volume_name)
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
            else:
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '
                    '%(volume_name)s, host %(host_name)s') %
                  {'result_lun': result_lun,
                   'volume_name': volume_name,
                   'host_name': host_name})
        return result_lun

    def _delete_host(self, host_name):
        """"""Delete a host on the storage system.""""""

        LOG.debug(_('enter: _delete_host: host %s ') % host_name)

        ssh_cmd = ['svctask', 'rmhost', host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_delete_host', ssh_cmd, out, err)

        LOG.debug(_('leave: _delete_host: host %s ') % host_name)

    def _get_conn_fc_wwpns(self, host_name):
        wwpns = []
        cmd = ['svcinfo', 'lsfabric', '-host', host_name]
        generator = self._port_conf_generator(cmd)
        header = next(generator, None)
        if not header:
            return wwpns

        for port_data in generator:
            try:
                wwpns.append(port_data['local_wwpn'])
            except KeyError as e:
                self._handle_keyerror('lsfabric', header)

        return wwpns

    def validate_connector(self, connector):
        """"""Check connector for at least one enabled protocol (iSCSI/FC).""""""
        valid = False
        if 'iSCSI' in self._enabled_protocols and 'initiator' in connector:
            valid = True
        if 'FC' in self._enabled_protocols and 'wwpns' in connector:
            valid = True
        if not valid:
            err_msg = (_('The connector does not contain the required '
                         'information.'))
            LOG.error(err_msg)
            raise exception.VolumeBackendAPIException(data=err_msg)

    def initialize_connection(self, volume, connector):
        """"""Perform the necessary work so that an iSCSI/FC connection can
        be made.

        To be able to create an iSCSI/FC connection from a given host to a
        volume, we must:
        1. Translate the given iSCSI name or WWNN to a host name
        2. Create new host on the storage system if it does not yet exist
        3. Map the volume to the host if it is not already done
        4. Return the connection information for relevant nodes (in the
           proper I/O group)

        """"""

        LOG.debug(_('enter: initialize_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_opts = self._get_vdisk_params(volume['volume_type_id'])
        host_name = connector['host']
        volume_name = volume['name']

        # Check if a host object is defined for this host name
        host_name = self._get_host_from_connector(connector)
        if host_name is None:
            # Host does not exist - add a new host to Storwize/SVC
            host_name = self._create_host(connector)
            # Verify that create_new_host succeeded
            self._driver_assert(
                host_name is not None,
                _('_create_host failed to return the host name.'))

        if vol_opts['protocol'] == 'iSCSI':
            chap_secret = self._get_chap_secret_for_host(host_name)
            if chap_secret is None:
                chap_secret = self._add_chapsecret_to_host(host_name)

        volume_attributes = self._get_vdisk_attributes(volume_name)
        lun_id = self._map_vol_to_host(volume_name, host_name)

        self._driver_assert(volume_attributes is not None,
                            _('initialize_connection: Failed to get attributes'
                              ' for volume %s') % volume_name)

        try:
            preferred_node = volume_attributes['preferred_node_id']
            IO_group = volume_attributes['IO_group_id']
        except KeyError as e:
                LOG.error(_('Did not find expected column name in '
                            'lsvdisk: %s') % str(e))
                exception_msg = (_('initialize_connection: Missing volume '
                                   'attribute for volume %s') % volume_name)
                raise exception.VolumeBackendAPIException(data=exception_msg)

        try:
            # Get preferred node and other nodes in I/O group
            preferred_node_entry = None
            io_group_nodes = []
            for k, node in self._storage_nodes.iteritems():
                if vol_opts['protocol'] not in node['enabled_protocols']:
                    continue
                if node['id'] == preferred_node:
                    preferred_node_entry = node
                if node['IO_group'] == IO_group:
                    io_group_nodes.append(node)

            if not len(io_group_nodes):
                exception_msg = (_('initialize_connection: No node found in '
                                   'I/O group %(gid)s for volume %(vol)s') %
                                 {'gid': IO_group, 'vol': volume_name})
                raise exception.VolumeBackendAPIException(data=exception_msg)

            if not preferred_node_entry and not vol_opts['multipath']:
                # Get 1st node in I/O group
                preferred_node_entry = io_group_nodes[0]
                LOG.warn(_('initialize_connection: Did not find a preferred '
                           'node for volume %s') % volume_name)

            properties = {}
            properties['target_discovered'] = False
            properties['target_lun'] = lun_id
            properties['volume_id'] = volume['id']
            if vol_opts['protocol'] == 'iSCSI':
                type_str = 'iscsi'
                # We take the first IP address for now. Ideally, OpenStack will
                # support iSCSI multipath for improved performance.
                if len(preferred_node_entry['ipv4']):
                    ipaddr = preferred_node_entry['ipv4'][0]
                else:
                    ipaddr = preferred_node_entry['ipv6'][0]
                properties['target_portal'] = '%s:%s' % (ipaddr, '3260')
                properties['target_iqn'] = preferred_node_entry['iscsi_name']
                properties['auth_method'] = 'CHAP'
                properties['auth_username'] = connector['initiator']
                properties['auth_password'] = chap_secret
            else:
                type_str = 'fibre_channel'
                conn_wwpns = self._get_conn_fc_wwpns(host_name)
                if not vol_opts['multipath']:
                    if preferred_node_entry['WWPN'] in conn_wwpns:
                        properties['target_wwn'] = preferred_node_entry['WWPN']
                    else:
                        properties['target_wwn'] = conn_wwpns[0]
                else:
                    properties['target_wwn'] = conn_wwpns
        except Exception:
            with excutils.save_and_reraise_exception():
                self.terminate_connection(volume, connector)
                LOG.error(_('initialize_connection: Failed to collect return '
                            'properties for volume %(vol)s and connector '
                            '%(conn)s.\n') % {'vol': str(volume),
                                              'conn': str(connector)})

        LOG.debug(_('leave: initialize_connection:\n volume: %(vol)s\n '
                    'connector %(conn)s\n properties: %(prop)s')
                  % {'vol': str(volume),
                     'conn': str(connector),
                     'prop': str(properties)})

        return {'driver_volume_type': type_str, 'data': properties, }

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Cleanup after an iSCSI connection has been terminated.

        When we clean up a terminated connection between a given connector
        and volume, we:
        1. Translate the given connector to a host name
        2. Remove the volume-to-host mapping if it exists
        3. Delete the host if it has no more mappings (hosts are created
           automatically by this driver when mappings are created)
        """"""
        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_name = volume['name']
        host_name = self._get_host_from_connector(connector)
        # Verify that _get_host_from_connector returned the host.
        # This should always succeed as we terminate an existing connection.
        self._driver_assert(
            host_name is not None,
            _('_get_host_from_connector failed to return the host name '
              'for connector'))

        # Check if vdisk-host mapping exists, remove if it does
        mapping_data = self._get_hostvdisk_mappings(host_name)
        if vol_name in mapping_data:
            ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name,
                       vol_name]
            out, err = self._run_ssh(ssh_cmd)
            # Verify CLI behaviour - no output is returned from
            # rmvdiskhostmap
            self._assert_ssh_return(len(out.strip()) == 0,
                                    'terminate_connection', ssh_cmd, out, err)
            del mapping_data[vol_name]
        else:
            LOG.error(_('terminate_connection: No mapping of volume '
                        '%(vol_name)s to host %(host_name)s found') %
                      {'vol_name': vol_name, 'host_name': host_name})

        # If this host has no more mappings, delete it
        if not mapping_data:
            self._delete_host(host_name)

        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

    """"""=====================================================================""""""
    """""" VOLUMES/SNAPSHOTS                                                   """"""
    """"""=====================================================================""""""

    def _get_vdisk_attributes(self, vdisk_name):
        """"""Return vdisk attributes, or None if vdisk does not exist

        Exception is raised if the information from system can not be
        parsed/matched to a single vdisk.
        """"""

        ssh_cmd = ['svcinfo', 'lsvdisk', '-bytes', '-delim', '!', vdisk_name]
        return self._execute_command_and_parse_attributes(ssh_cmd)

    def _get_vdisk_fc_mappings(self, vdisk_name):
        """"""Return FlashCopy mappings that this vdisk is associated with.""""""

        ssh_cmd = ['svcinfo', 'lsvdiskfcmappings', '-nohdr', vdisk_name]
        out, err = self._run_ssh(ssh_cmd)

        mapping_ids = []
        if (len(out.strip())):
            lines = out.strip().split('\n')
            mapping_ids = [line.split()[0] for line in lines]
        return mapping_ids

    def _get_vdisk_params(self, type_id):
        opts = self._build_default_opts()
        if type_id:
            ctxt = context.get_admin_context()
            volume_type = volume_types.get_volume_type(ctxt, type_id)
            specs = volume_type.get('extra_specs')
            for k, value in specs.iteritems():
                # Get the scope, if using scope format
                key_split = k.split(':')
                if len(key_split) == 1:
                    scope = None
                    key = key_split[0]
                else:
                    scope = key_split[0]
                    key = key_split[1]

                # We generally do not look at capabilities in the driver, but
                # protocol is a special case where the user asks for a given
                # protocol and we want both the scheduler and the driver to act
                # on the value.
                if scope == 'capabilities' and key == 'storage_protocol':
                    scope = None
                    key = 'protocol'
                    words = value.split()
                    self._driver_assert(words and
                                        len(words) == 2 and
                                        words[0] == '<in>',
                                        _('protocol must be specified as '
                                          '\'<in> iSCSI\' or \'<in> FC\''))
                    del words[0]
                    value = words[0]

                # Anything keys that the driver should look at should have the
                # 'drivers' scope.
                if scope and scope != ""drivers"":
                    continue

                if key in opts:
                    this_type = type(opts[key]).__name__
                    if this_type == 'int':
                        value = int(value)
                    elif this_type == 'bool':
                        value = strutils.bool_from_string(value)
                    opts[key] = value

        self._check_vdisk_opts(opts)
        return opts

    def _create_vdisk(self, name, size, units, opts):
        """"""Create a new vdisk.""""""

        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)

        model_update = None
        easytier = 'on' if opts['easytier'] else 'off'

        # Set space-efficient options
        if opts['rsize'] == -1:
            ssh_cmd_se_opt = []
        else:
            ssh_cmd_se_opt = ['-rsize', '%s%%' % str(opts['rsize']),
                              '-autoexpand', '-warning',
                              '%s%%' % str(opts['warning'])]
            if not opts['autoexpand']:
                ssh_cmd_se_opt.remove('-autoexpand')

            if opts['compression']:
                ssh_cmd_se_opt.append('-compressed')
            else:
                ssh_cmd_se_opt.extend(['-grainsize', str(opts['grainsize'])])

        ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp',
                   self.configuration.storwize_svc_volpool_name,
                   '-iogrp', '0', '-size', size, '-unit',
                   units, '-easytier', easytier] + ssh_cmd_se_opt
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), '_create_vdisk',
                                ssh_cmd, out, err)

        # Ensure that the output is as expected
        match_obj = re.search('Virtual Disk, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('_create_vdisk %(name)s - did not find '
              'success message in CLI output.\n '
              'stdout: %(out)s\n stderr: %(err)s')
            % {'name': name, 'out': str(out), 'err': str(err)})

        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)

    def _make_fc_map(self, source, target, full_copy):
        fc_map_cli_cmd = ['svctask', 'mkfcmap', '-source', source, '-target',
                          target, '-autodelete']
        if not full_copy:
            fc_map_cli_cmd.extend(['-copyrate', '0'])
        out, err = self._run_ssh(fc_map_cli_cmd)
        self._driver_assert(
            len(out.strip()),
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        # Ensure that the output is as expected
        match_obj = re.search('FlashCopy Mapping, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        try:
            fc_map_id = match_obj.group(1)
            self._driver_assert(
                fc_map_id is not None,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        except IndexError:
            self._driver_assert(
                False,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        return fc_map_id

    def _call_prepare_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'prestartfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_prepare_fc_map: Failed to prepare FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _prepare_fc_map(self, fc_map_id, source, target):
        self._call_prepare_fc_map(fc_map_id, source, target)
        mapping_ready = False
        wait_time = 5
        # Allow waiting of up to timeout (set as parameter)
        timeout = self.configuration.storwize_svc_flashcopy_timeout
        max_retries = (timeout / wait_time) + 1
        for try_number in range(1, max_retries):
            mapping_attrs = self._get_flashcopy_mapping_attributes(fc_map_id)
            if (mapping_attrs is None or
                    'status' not in mapping_attrs):
                break
            if mapping_attrs['status'] == 'prepared':
                mapping_ready = True
                break
            elif mapping_attrs['status'] == 'stopped':
                self._call_prepare_fc_map(fc_map_id, source, target)
            elif mapping_attrs['status'] != 'preparing':
                # Unexpected mapping status
                exception_msg = (_('Unexecpted mapping status %(status)s '
                                   'for mapping %(id)s. Attributes: '
                                   '%(attr)s')
                                 % {'status': mapping_attrs['status'],
                                    'id': fc_map_id,
                                    'attr': mapping_attrs})
                raise exception.VolumeBackendAPIException(data=exception_msg)
            # Need to wait for mapping to be prepared, wait a few seconds
            time.sleep(wait_time)

        if not mapping_ready:
            exception_msg = (_('Mapping %(id)s prepare failed to complete '
                               'within the allotted %(to)d seconds timeout. '
                               'Terminating.')
                             % {'id': fc_map_id,
                                'to': timeout})
            LOG.error(_('_prepare_fc_map: Failed to start FlashCopy '
                        'from %(source)s to %(target)s with '
                        'exception %(ex)s')
                      % {'source': source,
                         'target': target,
                         'ex': exception_msg})
            raise exception.InvalidSnapshot(
                reason=_('_prepare_fc_map: %s') % exception_msg)

    def _start_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'startfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_start_fc_map: Failed to start FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _run_flashcopy(self, source, target, full_copy=True):
        """"""Create a FlashCopy mapping from the source to the target.""""""

        LOG.debug(_('enter: _run_flashcopy: execute FlashCopy from source '
                    '%(source)s to target %(target)s') %
                  {'source': source, 'target': target})

        fc_map_id = self._make_fc_map(source, target, full_copy)
        try:
            self._prepare_fc_map(fc_map_id, source, target)
            self._start_fc_map(fc_map_id, source, target)
        except Exception:
            with excutils.save_and_reraise_exception():
                self._delete_vdisk(target, True)

        LOG.debug(_('leave: _run_flashcopy: FlashCopy started from '
                    '%(source)s to %(target)s') %
                  {'source': source, 'target': target})

    def _create_copy(self, src_vdisk, tgt_vdisk, full_copy, opts, src_id,
                     from_vol):
        """"""Create a new snapshot using FlashCopy.""""""

        LOG.debug(_('enter: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

        src_vdisk_attributes = self._get_vdisk_attributes(src_vdisk)
        if src_vdisk_attributes is None:
            exception_msg = (
                _('_create_copy: Source vdisk %s does not exist')
                % src_vdisk)
            LOG.error(exception_msg)
            if from_vol:
                raise exception.VolumeNotFound(exception_msg,
                                               volume_id=src_id)
            else:
                raise exception.SnapshotNotFound(exception_msg,
                                                 snapshot_id=src_id)

        self._driver_assert(
            'capacity' in src_vdisk_attributes,
            _('_create_copy: cannot get source vdisk '
              '%(src)s capacity from vdisk attributes '
              '%(attr)s')
            % {'src': src_vdisk,
               'attr': src_vdisk_attributes})

        src_vdisk_size = src_vdisk_attributes['capacity']
        self._create_vdisk(tgt_vdisk, src_vdisk_size, 'b', opts)
        self._run_flashcopy(src_vdisk, tgt_vdisk, full_copy)

        LOG.debug(_('leave: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

    def _get_flashcopy_mapping_attributes(self, fc_map_id):
        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')
                  % fc_map_id)

        fc_ls_map_cmd = ['svcinfo', 'lsfcmap', '-filtervalue',
                         'id=%s' % fc_map_id, '-delim', '!']
        out, err = self._run_ssh(fc_ls_map_cmd)
        if not len(out.strip()):
            return None

        # Get list of FlashCopy mappings
        # We expect zero or one line if mapping does not exist,
        # two lines if it does exist, otherwise error
        lines = out.strip().split('\n')
        self._assert_ssh_return(len(lines) <= 2,
                                '_get_flashcopy_mapping_attributes',
                                fc_ls_map_cmd, out, err)

        if len(lines) == 2:
            attributes = self._get_hdr_dic(lines[0], lines[1], '!')
        else:  # 0 or 1 lines
            attributes = None

        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '
                    '%(fc_map_id)s, attributes %(attributes)s') %
                  {'fc_map_id': fc_map_id, 'attributes': attributes})

        return attributes

    def _is_vdisk_defined(self, vdisk_name):
        """"""Check if vdisk is defined.""""""
        LOG.debug(_('enter: _is_vdisk_defined: vdisk %s ') % vdisk_name)
        vdisk_attributes = self._get_vdisk_attributes(vdisk_name)
        LOG.debug(_('leave: _is_vdisk_defined: vdisk %(vol)s with %(str)s ')
                  % {'vol': vdisk_name,
                     'str': vdisk_attributes is not None})
        if vdisk_attributes is None:
            return False
        else:
            return True

    def _ensure_vdisk_no_fc_mappings(self, name, allow_snaps=True):
        # Ensure vdisk has no FlashCopy mappings
        mapping_ids = self._get_vdisk_fc_mappings(name)
        while len(mapping_ids):
            wait_for_copy = False
            for map_id in mapping_ids:
                attrs = self._get_flashcopy_mapping_attributes(map_id)
                if not attrs:
                    continue
                source = attrs['source_vdisk_name']
                target = attrs['target_vdisk_name']
                copy_rate = attrs['copy_rate']
                status = attrs['status']

                if copy_rate == '0':
                    # Case #2: A vdisk that has snapshots
                    if source == name:
                        if not allow_snaps:
                            return False
                        ssh_cmd = ['svctask', 'chfcmap', '-copyrate', '50',
                                   '-autodelete', 'on', map_id]
                        out, err = self._run_ssh(ssh_cmd)
                        wait_for_copy = True
                    # Case #3: A snapshot
                    else:
                        msg = (_('Vdisk %(name)s not involved in '
                                 'mapping %(src)s -> %(tgt)s') %
                               {'name': name, 'src': source, 'tgt': target})
                        self._driver_assert(target == name, msg)
                        if status in ['copying', 'prepared']:
                            self._run_ssh(['svctask', 'stopfcmap', map_id])
                        elif status in ['stopping', 'preparing']:
                            wait_for_copy = True
                        else:
                            self._run_ssh(['svctask', 'rmfcmap', '-force',
                                           map_id])
                # Case 4: Copy in progress - wait and will autodelete
                else:
                    if status == 'prepared':
                        self._run_ssh(['svctask', 'stopfcmap', map_id])
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    elif status == 'idle_or_copied':
                        # Prepare failed
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    else:
                        wait_for_copy = True
            if wait_for_copy:
                time.sleep(5)
            mapping_ids = self._get_vdisk_fc_mappings(name)
        return True

    def _delete_vdisk(self, name, force):
        """"""Deletes existing vdisks.

        It is very important to properly take care of mappings before deleting
        the disk:
        1. If no mappings, then it was a vdisk, and can be deleted
        2. If it is the source of a flashcopy mapping and copy_rate is 0, then
           it is a vdisk that has a snapshot.  If the force flag is set,
           delete the mapping and the vdisk, otherwise set the mapping to
           copy and wait (this will allow users to delete vdisks that have
           snapshots if/when the upper layers allow it).
        3. If it is the target of a mapping and copy_rate is 0, it is a
           snapshot, and we should properly stop the mapping and delete.
        4. If it is the source/target of a mapping and copy_rate is not 0, it
           is a clone or vdisk created from a snapshot.  We wait for the copy
           to complete (the mapping will be autodeleted) and then delete the
           vdisk.

        """"""

        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)

        # Try to delete volume only if found on the storage
        vdisk_defined = self._is_vdisk_defined(name)
        if not vdisk_defined:
            LOG.info(_('warning: Tried to delete vdisk %s but it does not '
                       'exist.') % name)
            return

        self._ensure_vdisk_no_fc_mappings(name)

        ssh_cmd = ['svctask', 'rmvdisk', '-force', name]
        if not force:
            ssh_cmd.remove('-force')
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmvdisk
        self._assert_ssh_return(len(out.strip()) == 0,
                                ('_delete_vdisk %(name)s')
                                % {'name': name},
                                ssh_cmd, out, err)
        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)

    def create_volume(self, volume):
        opts = self._get_vdisk_params(volume['volume_type_id'])
        return self._create_vdisk(volume['name'], str(volume['size']), 'gb',
                                  opts)

    def delete_volume(self, volume):
        self._delete_vdisk(volume['name'], False)

    def create_snapshot(self, snapshot):
        source_vol = self.db.volume_get(self._context, snapshot['volume_id'])
        opts = self._get_vdisk_params(source_vol['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['volume_name'],
                          tgt_vdisk=snapshot['name'],
                          full_copy=False,
                          opts=opts,
                          src_id=snapshot['volume_id'],
                          from_vol=True)

    def delete_snapshot(self, snapshot):
        self._delete_vdisk(snapshot['name'], False)

    def create_volume_from_snapshot(self, volume, snapshot):
        if volume['size'] != snapshot['volume_size']:
            exception_message = (_('create_volume_from_snapshot: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(volume['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['name'],
                          tgt_vdisk=volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=snapshot['id'],
                          from_vol=False)

    def create_cloned_volume(self, tgt_volume, src_volume):
        if src_volume['size'] != tgt_volume['size']:
            exception_message = (_('create_cloned_volume: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(tgt_volume['volume_type_id'])
        self._create_copy(src_vdisk=src_volume['name'],
                          tgt_vdisk=tgt_volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=src_volume['id'],
                          from_vol=True)

    def extend_volume(self, volume, new_size):
        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])
        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],
                                                allow_snaps=False)
        if not ret:
            exception_message = (_('extend_volume: Extending a volume with '
                                   'snapshots is not supported.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        extend_amt = int(new_size) - volume['size']
        ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt),
                    '-unit', 'gb', volume['name']])
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from expandvdisksize
        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',
                                ssh_cmd, out, err)
        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])

    """"""=====================================================================""""""
    """""" MISC/HELPERS                                                        """"""
    """"""=====================================================================""""""

    def get_volume_stats(self, refresh=False):
        """"""Get volume stats.

        If we haven't gotten stats yet or 'refresh' is True,
        run update the stats first.
        """"""
        if not self._stats or refresh:
            self._update_volume_stats()

        return self._stats

    def _update_volume_stats(self):
        """"""Retrieve stats info from volume group.""""""

        LOG.debug(_(""Updating volume stats""))
        data = {}

        data['vendor_name'] = 'IBM'
        data['driver_version'] = '1.1'
        data['storage_protocol'] = list(self._enabled_protocols)

        data['total_capacity_gb'] = 0  # To be overwritten
        data['free_capacity_gb'] = 0   # To be overwritten
        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        pool = self.configuration.storwize_svc_volpool_name
        #Get storage system name
        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes or not attributes['name']:
            exception_message = (_('_update_volume_stats: '
                                   'Could not get system name'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        backend_name = self.configuration.safe_get('volume_backend_name')
        if not backend_name:
            backend_name = '%s_%s' % (attributes['name'], pool)
        data['volume_backend_name'] = backend_name

        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes:
            LOG.error(_('Could not get pool data from the storage'))
            exception_message = (_('_update_volume_stats: '
                                   'Could not get storage pool data'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        data['total_capacity_gb'] = (float(attributes['capacity']) /
                                    (1024 ** 3))
        data['free_capacity_gb'] = (float(attributes['free_capacity']) /
                                    (1024 ** 3))
        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']
        data['compression_support'] = self._compression_enabled

        self._stats = data

    def _port_conf_generator(self, cmd):
        ssh_cmd = cmd + ['-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return
        port_lines = out.strip().split('\n')
        if not len(port_lines):
            return

        header = port_lines.pop(0)
        yield header
        for portip_line in port_lines:
            try:
                port_data = self._get_hdr_dic(header, portip_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('_port_conf_generator',
                                               ssh_cmd, out, err)
            yield port_data

    def _check_vdisk_opts(self, opts):
        # Check that rsize is either -1 or between 0 and 100
        if not (opts['rsize'] >= -1 and opts['rsize'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for storwize_svc_vol_rsize: '
                         'set to either a percentage (0-100) or -1'))

        # Check that warning is either -1 or between 0 and 100
        if not (opts['warning'] >= -1 and opts['warning'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_warning: '
                         'set to a percentage (0-100)'))

        # Check that grainsize is 32/64/128/256
        if opts['grainsize'] not in [32, 64, 128, 256]:
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_grainsize: set to either '
                         '32, 64, 128, or 256'))

        # Check that compression is supported
        if opts['compression'] and not self._compression_enabled:
            raise exception.InvalidInput(
                reason=_('System does not support compression'))

        # Check that rsize is set if compression is set
        if opts['compression'] and opts['rsize'] == -1:
            raise exception.InvalidInput(
                reason=_('If compression is set to True, rsize must '
                         'also be set (not equal to -1)'))

        # Check that the requested protocol is enabled
        if opts['protocol'] not in self._enabled_protocols:
            raise exception.InvalidInput(
                reason=_('Illegal value %(prot)s specified for '
                         'storwize_svc_connection_protocol: '
                         'valid values are %(enabled)s')
                % {'prot': opts['protocol'],
                   'enabled': ','.join(self._enabled_protocols)})

        # Check that multipath is only enabled for fc
        if opts['protocol'] != 'FC' and opts['multipath']:
            raise exception.InvalidInput(
                reason=_('Multipath is currently only supported for FC '
                         'connections and not iSCSI.  (This is a Nova '
                         'limitation.)'))

    def _execute_command_and_parse_attributes(self, ssh_cmd):
        """"""Execute command on the Storwize/SVC and parse attributes.

        Exception is raised if the information from the system
        can not be obtained.

        """"""

        LOG.debug(_('enter: _execute_command_and_parse_attributes: '
                    ' command %s') % str(ssh_cmd))

        try:
            out, err = self._run_ssh(ssh_cmd)
        except exception.ProcessExecutionError as e:
            # Didn't get details from the storage, return None
            LOG.error(_('CLI Exception output:\n command: %(cmd)s\n '
                        'stdout: %(out)s\n stderr: %(err)s') %
                      {'cmd': ssh_cmd,
                       'out': e.stdout,
                       'err': e.stderr})
            return None

        self._assert_ssh_return(len(out),
                                '_execute_command_and_parse_attributes',
                                ssh_cmd, out, err)
        attributes = {}
        for attrib_line in out.split('\n'):
            # If '!' not found, return the string and two empty strings
            attrib_name, foo, attrib_value = attrib_line.partition('!')
            if attrib_name is not None and len(attrib_name.strip()):
                attributes[attrib_name] = attrib_value

        LOG.debug(_('leave: _execute_command_and_parse_attributes:\n'
                    'command: %(cmd)s\n'
                    'attributes: %(attr)s')
                  % {'cmd': str(ssh_cmd),
                     'attr': str(attributes)})

        return attributes

    def _get_hdr_dic(self, header, row, delim):
        """"""Return CLI row data as a dictionary indexed by names from header.
        string. The strings are converted to columns using the delimiter in
        delim.
        """"""

        attributes = header.split(delim)
        values = row.split(delim)
        self._driver_assert(
            len(values) ==
            len(attributes),
            _('_get_hdr_dic: attribute headers and values do not match.\n '
              'Headers: %(header)s\n Values: %(row)s')
            % {'header': str(header),
               'row': str(row)})
        dic = dict((a, v) for a, v in map(None, attributes, values))
        return dic

    def _log_cli_output_error(self, function, cmd, out, err):
        LOG.error(_('%(fun)s: Failed with unexpected CLI output.\n '
                    'Command: %(cmd)s\nstdout: %(out)s\nstderr: %(err)s\n')
                  % {'fun': function, 'cmd': cmd,
                     'out': str(out), 'err': str(err)})

    def _driver_assert(self, assert_condition, exception_message):
        """"""Internal assertion mechanism for CLI output.""""""
        if not assert_condition:
            LOG.error(exception_message)
            raise exception.VolumeBackendAPIException(data=exception_message)

    def _assert_ssh_return(self, test, fun, ssh_cmd, out, err):
        self._driver_assert(
            test,
            _('%(fun)s: Failed with unexpected CLI output.\n '
              'Command: %(cmd)s\n stdout: %(out)s\n stderr: %(err)s')
            % {'fun': fun,
               'cmd': ssh_cmd,
               'out': str(out),
               'err': str(err)})

    def _handle_keyerror(self, function, header):
        msg = (_('Did not find expected column in %(fun)s: %(hdr)s') %
               {'fun': function, 'hdr': header})
        LOG.error(msg)
        raise exception.VolumeBackendAPIException(
            data=msg)


class CLIResponse(object):
    '''Parse SVC CLI output and generate iterable'''

    def __init__(self, raw, delim='!', with_header=True):
        super(CLIResponse, self).__init__()
        self.raw = raw
        self.delim = delim
        self.with_header = with_header
        self.result = self._parse()

    def select(self, *keys):
        for a in self.result:
            vs = []
            for k in keys:
                v = a.get(k, None)
                if isinstance(v, basestring):
                    v = [v]
                if isinstance(v, list):
                    vs.append(v)
            for item in zip(*vs):
                yield item

    def __getitem__(self, key):
        return self.result[key]

    def __iter__(self):
        for a in self.result:
            yield a

    def __len__(self):
        return len(self.result)

    def _parse(self):
        def get_reader(content, delim):
            for line in content.lstrip().splitlines():
                line = line.strip()
                if line:
                    yield line.split(delim)
                else:
                    yield []

        if isinstance(self.raw, basestring):
            stdout, stderr = self.raw, ''
        else:
            stdout, stderr = self.raw
        reader = get_reader(stdout, self.delim)
        result = []

        if self.with_header:
            hds = tuple()
            for row in reader:
                hds = row
                break
            for row in reader:
                cur = dict()
                for k, v in zip(hds, row):
                    CLIResponse.append_dict(cur, k, v)
                result.append(cur)
        else:
            cur = dict()
            for row in reader:
                if row:
                    CLIResponse.append_dict(cur, row[0], ' '.join(row[1:]))
                elif cur:  # start new section
                    result.append(cur)
                    cur = dict()
            if cur:
                result.append(cur)
        return result

    @staticmethod
    def append_dict(dict_, key, value):
        key, value = key.strip(), value.strip()
        obj = dict_.get(key, None)
        if obj is None:
            dict_[key] = value
        elif isinstance(obj, list):
            obj.append(value)
            dict_[key] = obj
        else:
            dict_[key] = [obj, value]
        return dict_
/n/n/n",0
63,63,f752302d181583a95cf44354aea607ce9d9283f4,"/cinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/n",1
